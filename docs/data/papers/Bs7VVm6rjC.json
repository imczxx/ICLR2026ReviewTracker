{"id": "Bs7VVm6rjC", "number": 17967, "cdate": 1758282504389, "mdate": 1759897142152, "content": {"title": "Network Shape Automata: a brain network inspired collaborative filter for link prediction in bipartite complex networks and recommendation systems", "abstract": "In recommendation systems, representing user-item interactions as a bipartite network is a fundamental approach that provides a structured way to model relationships between users and items, allowing for efficient predictions via network science. Collaborative filtering is one of the most widely used and actively researched techniques for recommendation systems, its rationale is to predict user preferences based on shared patterns in user interactions, and vice versa. Memory-based collaborative filtering relies on directly analyzing user-item interactions to provide recommendations using similarity measures, and differs from model-based collaborative filtering which builds a predictive model using machine learning techniques such as neural networks. With the rise of machine learning, memory-based collaborative filtering has often been overshadowed by model-based approaches. However, the recent success of SSCF, a newly proposed memory-based method, has renewed interest in the potential of memory-based approaches. In this paper, we propose Network Shape Automata (NSA), a memory-based collaborative filtering method grounded in the connectivity shape of the bipartite network topology. NSA leverages the Cannistraci-Hebb theory proposed in network science to define brain-inspired network automata, using this paradigm as the foundation for its similarity measure. We evaluate NSA against a range of advanced collaborative filtering methods, both memory-based and model-based, across 13 middle-scale bipartite network datasets spanning complex systems domains such as social networks and biological networks and 3 classical large-scale recommentation datasets including Gowalla, Yelp2018, Amazon-book. Results show that NSA consistently achieves strong performance across diverse datasets and evaluation metrics, ranking most often first on average. Notably, NSA demonstrates strong robustness to network sparsity, while preserving the simplicity, interpretability, and training-free nature of memory-based methods. As a pioneering effort to bridge link prediction and recommendation tasks, NSA not only highlights the untapped potential of memory-based collaborative filtering but also demonstrates the effectiveness of the Cannistraci-Hebb theory in modeling network evolution within recommendation systems.", "tldr": "", "keywords": ["collaborative filtering", "recommendation systems", "bipartite network", "link prediction"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/404e559cd3645a3936ec4ff33ce5363636fbb8d1.pdf", "supplementary_material": "/attachment/e1c73818510012e1d0d705cebee57005a61109e0.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Network Shape Automata (NSA), a memory-based collaborative filtering method that incorporates the Cannistraci-Hebb (CH) theory from network science into similarity computation for link prediction and recommendation in bipartite graphs. The authors conduct extensive evaluations on 13 medium-scale and 3 large-scale recommendation datasets."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Introduces the relatively novel CH theory from network science to the recommender systems community."}, "weaknesses": {"value": "- [Mandatory] SCCF was published in KBS 2023; it is neither advanced nor influential, and thus unconvincing as a research motivation. Moreover, its performance is similar to LightGCN and is significantly uncompetitive compared to recent CL-based methods.\n\n- [Mandatory] Table 1 is not cited in the main text.\n\n- [Mandatory] The most recent baselines used are from 2023. Please add 2024–2025 baselines, such as BIGCF [1], RecDCL [2], WeightGCL [3], and NLGCL [4].\n\n- [Mandatory] Please update the related work; the discussion lacks the latest studies.\n\n- [Mandatory] There is a lack of detailed introduction to CH3.1-L2.\n\n- [Mandatory] There is a lack of theoretical analysis or intuitive interpretation of NSA itself. The physical meaning of CH index components (di_k, de_k) in the context of recommendation (e.g., what user/item behaviors they correspond to) is not clarified, making the method resemble a \"black-box\" similarity function and undermining its claimed \"interpretability.\"\n\n- [Mandatory] The time complexity analysis (Appendix J) indicates a complexity of O((U+I)^3) for heterogeneous sparse networks, which is prohibitive for large-scale recommendation. The paper fails to adequately discuss this potential bottleneck or propose effective mitigation strategies.\n\n\nRefs:\n\n[1] Exploring the individuality and collectivity of intents behind interactions for graph collaborative filtering, SIGIR 2024.\n\n[2] RecDCL: Dual Contrastive Learning for Recommendation, WWW 2024.\n\n[3] Squeeze and Excitation: A Weighted Graph Contrastive Learning for Collaborative Filtering, SIGIR 2025.\n\n[4] NLGCL: Naturally Existing Neighbor Layers Graph Contrastive Learning for Recommendation, RecSys 2025."}, "questions": {"value": "Please refer to Weaknesses. Btw, I have some optional questions:\n\n- [Optional] Given the high time complexity in power-law networks, are there feasible approximation computations or sampling strategies to make NSA applicable to ultra-large-scale scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AM8nGOal2o", "forum": "Bs7VVm6rjC", "replyto": "Bs7VVm6rjC", "signatures": ["ICLR.cc/2026/Conference/Submission17967/Reviewer_mqXY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17967/Reviewer_mqXY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17967/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760696492316, "cdate": 1760696492316, "tmdate": 1762927762298, "mdate": 1762927762298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel memory-based collaborative filtering (CF) method called \"Network Shape Automata\" (NSA) for link prediction in bipartite networks. This work contrasts with the current mainstream trend of using deep learning and Graph Neural Networks (GNNs) for recommendation.\n\nThe core contribution of NSA is a novel, brain-inspired similarity metric derived from the Cannistraci-Hebb (CH) theory in network science. This metric (e.g., the CH3-L2 index) goes beyond simple common neighbors, instead evaluating the local community topology of node pairs to compute similarity.\n\nThe entire NSA method is a 4-stage, training-free process:\n\n1. **CH Scoring:** Computes pairwise node similarity for users and items separately, using a CH index, a denominator for scaling, and an exponential parameter.\n2. **Monopartite Projection:** Projects these similarities as weights onto user-user and item-item graphs.\n3. **Bipartite Scoring:** Aggregates scores from the monopartite graphs to predict scores for unobserved user-item links.\n4. **Mixing:** Linearly combines the user-based and item-based scores using a mixing parameter $\\lambda$.\n\nThe authors conduct an extremely rigorous and comprehensive evaluation on 13 medium-sized bipartite networks from diverse domains (social and biological) and 3 large-scale recommendation datasets (Gowalla, Yelp2018, Amazon-book). The results show that the training-free NSA consistently matches or surpasses various state-of-the-art (SOTA) memory-based (SSCF) and model-based (e.g., LightGCN, BSPM, LT-OCF) baselines in performance.\n\nThe paper's main contribution is bridging network science theory with recommendation systems, providing a powerful, interpretable, and highly competitive non-learning baseline that challenges the necessity of using complex learned representations for this task."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Exceptional Empirical Performance: The method's primary strength is its performance. On 13 diverse datasets, NSA consistently achieves the 1st or 2nd average rank against 9 strong baselines (Figure 2) and remains highly competitive on the 3 large-scale benchmarks (Table 2).\n2. Methodological Rigor: The experimental design is a model of good scientific practice. The use of 13+3 datasets, multiple realizations, and an extensive and fair hyperparameter search for *all* methods makes the results highly credible.\n3. High Originality and Novel Perspective: This paper brings a genuinely new idea to the CF community by introducing the Cannistraci-Hebb (CH) theory from network science. This moves beyond standard heuristics and provides a brain-inspired, topologically-aware theoretical grounding for the similarity metric.\n4. Interpretability and Simplicity: As a memory-based, training-free method, NSA is far simpler and more interpretable than its deep learning competitors. Its success demonstrates the immense predictive power of well-engineered topological features over end-to-end learned representations.\n5. Robustness: The method shows strong performance in sparse network conditions. Furthermore, the ablation in Appendix H shows that even a simplified version (exponent parameter fixed to 1) is still one of the top-performing methods, highlighting the robustness of its core CH index."}, "weaknesses": {"value": "1. Scalability Concerns: The most significant weakness (which the authors acknowledge) is computational complexity. The time complexity analysis in Appendix J shows $\\mathcal{O}((U+I)^3)$ for sparse heterogeneous networks (dominated by CH scoring) or $\\mathcal{O}(U^2I + UI^2)$ for homogeneous networks (dominated by CF aggregation). This is polynomially worse than GNNs at $\\mathcal{O}(T \\times E \\times D)$. For truly industrial-scale graphs with billions of nodes, this approach may be infeasible. NSA is at a computational disadvantage compared to GNN methods like LightGCN which have lower complexity.\n2. Limited Exploration of CH Theory: The paper only uses L2 (path length 2) indices (CH3-L2, CH3.1-L2). While these are sophisticated variants of common neighbors, the CH theory also includes multi-scale (Ln) indices. The paper does not explore L3 or higher-order paths. This feels like a missed opportunity, as it would offer a more direct point of comparison to multi-layer GNNs that aggregate multi-hop neighborhood information.\n3. \"Training-Free\" vs. \"Expensive Validation\": The paper correctly claims the final model is \"training-free.\" However, it relies on a massive hyperparameter search (claiming over 105,300 model evaluations) to find the optimal combination of CH index, denominator, exponent, scoring rule, and $\\lambda$ mixer. This exhaustive validation search is a form of model selection, and its computational cost may be comparable to or *greater* than training one GNN. The paper claims it is \"training-free\" but relies on an extremely expensive, computationally intensive hyperparameter search space. The authors claim \"over 105,300 model evaluations.\" This is not \"simplicity\"; it is a computationally expensive brute-force search. This form of \"meta-training\" might be computationally more expensive than training a GNN, making the \"training-free\" advantage almost moot. Is the grid search time factored into the time comparisons?\n4. The ablation in Appendix N only compares the CH index against the most basic \"Common Neighbors\" (CN). This is a weak baseline. A comparison against more advanced, yet still simple, network science heuristics like Adamic-Adar (AA) or Resource Allocation (RA) would be more informative. A sophisticated variant of CH index and RA are similar, which is also worth considering."}, "questions": {"value": "1. Practical Scalability: Given the polynomial complexity, how do the authors envision this method being practically applied to web-scale recommendation systems? The authors briefly mention \"sampling strategies\" in future work. Could they elaborate? For example, can the CH index be effectively approximated? Or could NSA be applied to sampled subgraphs while retaining its performance?\n2. Higher-Order Paths (L3+): The CH-L2 indices are clearly a powerful enhancement over standard common neighbors. Did the authors experiment with or consider the L3 (path length 3) indices from CH theory? This seems like a natural extension to capture higher-order topological information and would be a good comparison point to a 3-layer GNN.\n3. Cost of Validation Search: Can the authors provide a more direct comparison of the total wall-clock time required to run the *full hyperparameter validation search* for NSA (e.g., on the Yelp2018 dataset) versus the *total training and validation time* for a key baseline like LightGCN? Table 5 compares single runs, but the search space for NSA (index, denominator, exponent, rule, $\\lambda$) seems very large. It is important to know the full cost of finding the \"optimal\" training-free model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MtrViTDDxl", "forum": "Bs7VVm6rjC", "replyto": "Bs7VVm6rjC", "signatures": ["ICLR.cc/2026/Conference/Submission17967/Reviewer_yxjj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17967/Reviewer_yxjj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17967/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761309504941, "cdate": 1761309504941, "tmdate": 1762927761942, "mdate": 1762927761942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NSA, a memory-based collaborative filtering method that applies Cannistraci-Hebb theory from network science to bipartite recommendation networks. The authors show competitive performance on multiple datasets while maintaining interpretability and training-free inference."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper conducts an extensive empirical study on 16 datasets. It includes rigorous hyperparameter tuning with 10-fold validation for each realization, which strengthens the reliability of the results.\n\n2. Integrating Cannistraci-Hebb theory from network science provides a principled and interpretable approach to similarity computation.\n\n3. NSA requires no training phase and maintains full interpretability, which is valuable for deployment scenarios where model transparency and computational efficiency are important."}, "weaknesses": {"value": "1. While the paper's core motivation emphasizes leveraging brain-inspired CH theory, the contribution appears closer to an engineering combination of existing components.\n2. The SSCF and LT-OCF models used for comparison are from 2023 and 2021, respectively. Table 2 needs to include comparisons with more recent state-of-the-art baselines.\n3. The paper presents only average rankings rather than full results across the 13 datasets used, making it difficult for readers to assess the actual performance contributions claimed by the authors.\n4. For Amazon-Book, the evaluation uses an inconsistent methodology by testing only the CH3-L2 variant (NSA3) instead of the full NSA model.\n5. Statistical significance testing appears necessary to adequately compare NSA against other baselines.\n6. The term \"automata\" in the title is misleading. Network automata typically refer to dynamic systems with update rules, whereas NSA is a static similarity-based scoring function.\n7. The notation is inconsistent. Section 3 uses U (users) and Γ (items) but later switches to U and I."}, "questions": {"value": "1. Why does Table 5 show that the NSA runtime for amazon-product takes longer than BSPM?\n2. Can you provide full results across all 13 datasets used, rather than just average rankings?\n3. Given that SSCF and LT-OCF are models from 2023 and 2021, can you include comparisons with more recent baselines in Table 2?\n4. Table 2 shows NSA3 (simplified NSA) used for Amazon-Book, but full NSA for Gowalla and Yelp2018. Why was the full NSA not tested on Amazon-Book?\n5. What is the conceptual connection regarding CH theory between brain neural networks and user-item recommendation networks? Is this a superficial analogy rather than a deep theoretical connection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BkuwSUfjzX", "forum": "Bs7VVm6rjC", "replyto": "Bs7VVm6rjC", "signatures": ["ICLR.cc/2026/Conference/Submission17967/Reviewer_p9Mu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17967/Reviewer_p9Mu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17967/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984794010, "cdate": 1761984794010, "tmdate": 1762927760991, "mdate": 1762927760991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}