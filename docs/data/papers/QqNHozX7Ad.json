{"id": "QqNHozX7Ad", "number": 8795, "cdate": 1758098518843, "mdate": 1762925214284, "content": {"title": "What Makes Large Language Models Undistillable?", "abstract": "Knowledge Distillation (KD) has been a cornerstone technique for accelerating Large Language Model (LLM) development by transferring knowledge from powerful teacher models to lightweight students. However, the efficacy of KD is not always guaranteed. Certain combinations of models and datasets have led to unexpected KD failure, which remains poorly understood. In this paper, we take a first step toward answering the fundamental question underlying these failures: What makes LLM undistillable? To this end, our first contribution is to identify and formalize the phenomenon we term as “distillation trap”, where teacher LLMs generate outputs that, despite being linguistically coherent, are nonsensical and misguide students during training. We further provide a theoretical motivation between this trap and KD dynamics of Kullback-Leibler (KL) divergence, the loss function central to most distillation protocols. Beyond elucidating the causes of KD failures, our second contribution is a control mechanism for LLMs’ distillability. We propose a novel methodology using Reinforcement Fine-tuning (RFT) to optimize a composite reward function. The reward function balances the teacher’s task capability with a confusion-based reward, which can be applied positively or negatively to either suppress or enhance the model’s amenability to distillation. By maximizing confusion reward, we deliberately construct “undistillable teachers”, effectively turning latent distillation traps into protective guards of model intellectual property (IP). Extensive experiments across four model pairs and four datasets demonstrate this approach’s effectiveness: our undistillable teachers retain their original performance while causing a catastrophic performance collapse (over 80% accuracy loss) in students trained with state-of-the-art distillation protocols. Our code can be found in supplementary material.", "tldr": "", "keywords": ["Knowledge Distillation", "Reinforcement Fine-tuning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/4dfd68400da893a174d6c177b31203f5368bc6a1.pdf", "supplementary_material": "/attachment/9b8677e41c490614e173da3126090eaa40c0cc0b.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the phenomenon of Knowledge Distillation failure in LLMs, seeking to answer the question: \"What makes an LLM undistillable?\" The authors introduce the concept of a \"distillation trap,\" where a high-performing teacher model generates outputs that are linguistically coherent but contain nonsensical or flawed reasoning, which misguides the student model during training. To study this, the paper proposes a novel method to actively engineer this failure mode, transforming standard LLMs into \"undistillable teachers.\" The authors demonstrate empirically across multiple model pairs and reasoning datasets that this method causes a catastrophic performance collapse in student models trained via state-of-the-art KD, thereby validating their hypothesis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  The primary strength is its \"constructive proof\" approach. Instead of merely observing KD failures and attempting to patch them, the authors actively engineer the failure mode. The framing of the \"distillation trap\" is clear, intuitive, and provides a valuable new lens through which to view the dynamics of knowledge transfer.\n\n2.  The proposed RFT-based method is both clever and well-motivated. The use of a smaller, frozen reference model to define the \"trap reward\"  is a particularly elegant solution. It operationalizes the abstract concept of \"confusing reasoning\" into a concrete, computable metric without the need for human annotation or more complex heuristics. This design choice is both theoretically sound and computationally practical."}, "weaknesses": {"value": "1.  The paper briefly mentions that the framework could be used to *enhance* distillability by using a negative weight for the trap reward (λ = -1) and provides a single qualitative example. This is a fascinating and potentially very impactful claim, but it is not supported by any quantitative evidence. Without experiments showing that a student distilled from such an \"enhanced\" teacher outperforms a student distilled from the original teacher, this remains a speculative assertion. \n\n2. The core assumption behind the trap reward is that \"correct and valuable reasoning should be self-concordant\" and thus not surprising to a smaller model. While this is a reasonable and effective heuristic, it may not be universally true. A particularly novel, complex, or \"out-of-the-box\" correct reasoning path might also register as high-entropy to a smaller reference model. The current method risks inadvertently penalizing such creative-yet-correct solutions.\n\n3. The paper also strongly assumes that the proposed distillation trap is the sole reason why LLM cannot be distilled, and then conducts experiments based on this \"belief.\" I find this logical relationship confusing. The current approach simply demonstrates that the teacher model can first speak a potentially irrelevant sentence, then \"adaptively\" ignore this irrelevant/illusory content and directly output the correct answer. It's obvious that this data construction method will prevent the student model from learning. There are many reasons for distillation failure, but the paper does not provide any other relevant discussion."}, "questions": {"value": "The authors attempt to demonstrate that this affects the effectiveness of knowledge distillation by constructing data that is \"unlearnable.\" So, if we remove all such data from the existing training set, will LLM be able to perform better distillation? I think this experiment would be more straightforward."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s6koKJUH4D", "forum": "QqNHozX7Ad", "replyto": "QqNHozX7Ad", "signatures": ["ICLR.cc/2026/Conference/Submission8795/Reviewer_FMXm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8795/Reviewer_FMXm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761492020092, "cdate": 1761492020092, "tmdate": 1762920567641, "mdate": 1762920567641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "LANh1GvMBE", "forum": "QqNHozX7Ad", "replyto": "QqNHozX7Ad", "signatures": ["ICLR.cc/2026/Conference/Submission8795/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8795/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762925212912, "cdate": 1762925212912, "tmdate": 1762925212912, "mdate": 1762925212912, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores when and why Knowledge Distillation (KD) fails for large language models (LLMs). The authors formalize a new failure mode called the “distillation trap,” in which a teacher model generates outputs that are superficially coherent yet semantically misleading, causing the student’s learning to diverge. They trace this to the optimization dynamics of KL divergence (forward vs reverse), showing how these divergences are vulnerable to deceptive modes. To validate their theory and to provide a tool, the authors propose a Reinforcement Fine-Tuning (RFT) method to make a teacher model undistillable. Experiments on multiple teacher-student pairs and datasets (gsm8k, CSQA, MMLU-Pro, superGPQA) show that these undistillable teachers preserve their original task performance (small deviations) while resulting in catastrophic performance drops on their corresponding distilled students."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel perspective on distillation: The paper takes an original and thought-provoking approach by shifting the focus from improving student performance to protecting the teacher model’s knowledge, introducing the concept of “distillation traps” as a new paradigm in model robustness and intellectual property protection.\n\n- Clarity and presentation: The paper is well written, clearly structured, and easy to follow, with solid motivation and intuition provided for each component of the proposed framework.\n    \n- Empirical effectiveness and generalization: The experimental results convincingly demonstrate that the proposed method successfully makes the teacher model undistillable while maintaining strong performance, and the approach generalizes well to out-of-distribution data."}, "weaknesses": {"value": "- The authors focus solely on making the teacher undistillable but ignore the inverse direction, improving the teacher through boosted distillation. Including experiments to evaluate this setup could provide a more complete understanding of the proposed framework.\n\n- While the experiments demonstrate the effectiveness of the approach, the results may be conditioned on the specific student model used during training. It would strengthen the paper to test whether the distillation trap generalizes to other student architectures or capacities.\n    \n - The undistillable version of the teacher exhibits hallucination behavior, which is undesirable for generative models. This raises concerns about potential trade-offs between making a model undistillable and maintaining its generation quality."}, "questions": {"value": "See weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EvdU7XEQbf", "forum": "QqNHozX7Ad", "replyto": "QqNHozX7Ad", "signatures": ["ICLR.cc/2026/Conference/Submission8795/Reviewer_mBKq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8795/Reviewer_mBKq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662319326, "cdate": 1761662319326, "tmdate": 1762920567228, "mdate": 1762920567228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why Knowledge Distillation (KD) sometimes fails for Large Language Models (LLMs). It identifies and formalizes the \"distillation trap,\" where a high-performing teacher model generates outputs that are linguistically coherent but nonsensical or hallucinatory, thus misguiding the student model.\n\nThe authors link this trap to the dynamics of the Kullback-Leibler (KL) divergence loss function. They then propose a Reinforcement Fine-tuning (RFT) method to intentionally amplify these traps, creating \"undistillable teachers.\" This serves as a novel method to protect a model's intellectual property (IP). Experiments demonstrate that while these modified teachers retain their original accuracy, students attempting to distill from them experience a catastrophic performance collapse (over 80% accuracy loss)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper provides a novel identification and formalization of a critical problem (the \"distillation trap\") that explains why LLM knowledge distillation fails.\n2. This paper then proposes an innovative and practical method (Reinforcement Fine-tuning) to control this phenomenon, applying it as a significant new technique for protecting model intellectual property (IP)."}, "weaknesses": {"value": "A primary weakness of this paper is the method's foundational trade-off: inducing \"distillation traps\" inherently introduces nonsensical or hallucinatory artifacts into the teacher model's reasoning process. Although empirical results suggest final answer accuracy is preserved, this degradation of the model's CoT significantly degrades the user experience in any application where reasoning is exposed. More importantly, it erodes trust and harms the model's fundamental interpretability.\n\nFurthermore, the paper's evaluation is narrowly focused on benchmarks characterized by short, definitive answers (e.g., mathematical or factual reasoning). This limitation raises significant doubts about the method's generalizability. For open-ended, generative tasks (such as creative writing or long-article summarization), the distinction between the \"reasoning path\" and the \"final answer\" effectively collapses. In such scenarios, it is highly uncertain whether the teacher's final output would remain coherent, or if the hallucinations introduced as \"traps\" would inevitably corrupt the final output itself."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xZMl6WkktZ", "forum": "QqNHozX7Ad", "replyto": "QqNHozX7Ad", "signatures": ["ICLR.cc/2026/Conference/Submission8795/Reviewer_aA1C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8795/Reviewer_aA1C"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900168710, "cdate": 1761900168710, "tmdate": 1762920566773, "mdate": 1762920566773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical and under-explored phenomenon of Knowledge Distillation (KD) failure in Large Language Models (LLMs). The central contribution is the introduction and rigorous definition of the\"Distillation Trap,\"which arises when a teacher LLM's high-quality final answer is decoupled from its low-quality, confusing, or misleading intermediate probability distribution. The authors propose a novel, controllable framework using Reinforcement Fine-Tuning with a custom Trap Reward to intentionally engineer \"undistillable\" teacher models. The RFT objective balances maintaining high task accuracy while maximizing confusion for a student model. Empirical results demonstrate that these engineered teachers can degrade student performance by over 90% across various LLM pairs and datasets. The work offers a profound insight into the mechanics of KD and presents a promising direction for intellectual property protection and model robustness analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper tackles an under-explored yet practically critical issue—the mechanism behind unexpected KD resistance. The concept of controllable undistill-ability is highly original, offering both a new tool for robustness testing and a potent defense for proprietary models.\n2.The analysis linking the asymmetry of Forward/Reverse KL Divergence to the distillation trap provides a clear conceptual foundation. This is effectively validated by the proposed RFT-based engineering approach, which successfully dissociates task performance from distill-ability.\n3.The experiments are extensive, covering diverse LLM architectures (Teacher-Student pairs) and datasets. The consistency and magnitude of the degradation (up to 90%+ performance drop) are highly convincing and underscore the efficacy of the proposed mechanism.\n4.The use of GRPO with LoRA adaptation within the RFT framework is an elegant and computationally feasible design. Algorithm 1 and the implementation details enhance the clarity and potential reproducibility of the work.\n5.The findings are directly relevant to next-generation LLM security, providing a fundamental mechanism for model IP protection against unauthorized imitation, thereby opening a vital new research frontier."}, "weaknesses": {"value": "1.Limited Scope of IP Defense and Attacker Model:The claimed IP protection is potentially overstated as the defense assumes a narrow attacker model that relies strictly on soft-label KD (e.g., GKD with JSD). The paper fails to quantify the defense's robustness against more practical attacker strategies, such as:\n-Supervised Fine-Tuning (SFT) on Hard Labels:Simply using the teacher's final, correct answer as the ground truth.\n-RL Imitation:Directly mimicking the teacher's action policy in an RL setting.\n-Top-K Logit Distillation:Filtering the confused logits.\n-Recommendation:A quantitative baseline comparing the proposed method against these bypass strategies is essential for a realistic assessment of the defense's efficacy.\n2.Insufficient Quantitative Interpretability of the Trap:While qualitative examples of \"hallucination-like\" outputs are provided, the paper lacks systematic quantitative metrics to characterize the nature of the confusion. Key metrics that should be analyzed include:\n-Consistency Measures:e.g., Perplexity gap between the trap output and a clean output.\n-Hallucination Rate:Token-level divergence from factual consistency.\n-Error Type Analysis:Classification of the confusion (e.g., repetition, semantic drift, logical fallacy).\n3.Generaliz ability Across KD Variants and Capacity:The reported results primarily focus on a single KD objective (GKD with JSD). The generality of the \"undistillability\" across key variants remains unclear:\n-KD Objectives:Evaluation must be extended to canonical Forward KL (FKL),Reverse KL (RKL), and Sequence-level KD (SeqKD).\n-Student Capacity:The relationship between student size (capacity) and the trap's effectiveness requires further ablation.\n4.Sensitivity and Stability Analysis Deficiencies:The critical dependency on hyperparameter selection and the reference model  is not adequately explored:\n Sensitivity:The impact of 's strength (size, pre-training quality, or architectural difference) on the resulting trap reward and strength needs to be rigorously analyzed.\n-Training Stability:Crucial factors like the trap weight  and potential reward hacking behaviors (where the teacher exploits the reward signal with nonsensical but high-scoring outputs) require dedicated sensitivity curves and qualitative discussion.\n5.Under-explored Ethical and Dual-Use Implications:The Broader Impact section, while present, needs expansion. Intentionally polluting knowledge streams to enforce IP protection introduces significant ethical concerns regarding the injection of unreliability, bias amplification, or deliberate obfuscation into the model's internal representations. This dual-use nature warrants a more comprehensive discussion on responsible deployment."}, "questions": {"value": "1.Please provide quantitative results assessing the effectiveness of the \"undistillable\" defense when the attacker employs Supervised Fine-Tuning (SFT)on hard labels or RL imitation instead of soft-label KD.\n2.Conduct a thorough sensitivity analysis of the trap reward by varying the strength, size, or architectural configuration of the reference model .\n3.Does the observed undistillability persist when employing canonical KD objectives such as Forward KL,Reverse KL, or Sequence-level KD?\n4.Provide quantitative metrics(e.g., perplexity, n-gram repetition, or a defined hallucination score) to systematically characterize the nature of the confusing outputs generated by the \"undistillable\" teacher.\n5.Detail the training dynamics of the RFT process: specifically, report sensitivity curves for the trap weight  and discuss any observed instances of reward-hacking behaviors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dXtAno0bRH", "forum": "QqNHozX7Ad", "replyto": "QqNHozX7Ad", "signatures": ["ICLR.cc/2026/Conference/Submission8795/Reviewer_X5dZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8795/Reviewer_X5dZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762095205792, "cdate": 1762095205792, "tmdate": 1762920566431, "mdate": 1762920566431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}