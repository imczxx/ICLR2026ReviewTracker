{"id": "ASzmchacx7", "number": 4918, "cdate": 1757805433222, "mdate": 1763762875663, "content": {"title": "Reward Learning through Ranking Mean Squared Error", "abstract": "Reward design remains a significant bottleneck in applying reinforcement learning (RL) to real-world problems. \nA popular alternative is reward learning, where reward functions are inferred from human feedback rather than manually specified.\nRecent work has proposed learning reward functions from human feedback in the form of ratings, rather than traditional binary preferences, enabling richer and potentially less cognitively demanding supervision. \nBuilding on this paradigm, we introduce a new rating-based RL method,  Ranked Return Regression for RL (R4).\nAt its core, R4 employs a novel ranking mean squared error (rMSE) loss, which treats teacher-provided ratings as ordinal targets.\nOur approach learns from a dataset of trajectory–rating pairs, where each trajectory is labeled with a discrete rating (e.g., \"bad,''  \"neutral,'' \"good'').\nAt each training step, we sample a set of trajectories, predict their returns, and rank them using a differentiable sorting operator (soft ranks). We then optimize a mean squared error loss between the resulting soft ranks and the teacher's ratings.\nUnlike prior rating-based approaches, R4 offers formal guarantees: its solution set is provably minimal and complete under mild assumptions. Empirically, using simulated human feedback, we demonstrate that R4 consistently matches or outperforms existing rating and preference-based RL methods on robotic locomotion benchmarks from OpenAI Gym and the DeepMind Control Suite, while requiring significantly less feedback.", "tldr": "", "keywords": ["Reinforcement Learning", "Human in the loop", "Preference Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d3129855614b5378603f8d7b5c11088d70cae7cf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Ranked Return Regression (R4), which trains a reward model from multi-class trajectory ratings using a ranking mean squared error (rMSE) loss built on soft ranks, and claims theoretical guarantees plus empirical gains on locomotion and DMC benchmarks. While this paper solves reward modeling by using the MSE loss of ordinal feedback, it is not a practical method because it requires human rating (classification) of each trajectory."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of using ranking information from ratings is clear and simple.\n\n2. The method is easy to implement within standard reward learning pipelines.\n\n3. The experiments show some improvement over the chosen baselines on locomotion tasks under simulated settings."}, "weaknesses": {"value": "1. The ordinal feedback is not a new idea for preference-based RL. The paper does not convincingly show that rMSE gives qualitatively different behavior than straightforward regression-to-targets, ordinal regression, or a calibrated cross-entropy approach.\n\n2. There is a lack of related work [1,2]. [1] is an important one that theoretically analyzes why ordinal feedback can work better than pair-wise ones. Meanwhile, some work has explored using tied preference to improve PbRL [2], which should be included in related work and compared with R4.\n\n[1] Liu, S., Pan, Y., Chen, G., & Li, X. Reward Modeling with Ordinal Feedback: Wisdom of the Crowd. 2025\n\n[2] Liu, J., Ge, D., and Zhu, R. Reward learning from preference with ties. 2024\n\n3. The most serious problem is that ordinal feedback is impractical in practice. This paper only improves the loss function without fundamentally solving the problem. Trajectory rating for real humans is a difficult and error-prone task, and solving this problem is the key to ordinal feedback.\n\n4. The experiments use a scripted teacher to provide rates for trajectories, which can demonstrate R4's improvement over RBRL. However, it can not fully convince that R4 is better than comparison-based methods. Although they use the same number of feedback (number of rating trajectories = number of comparisons), this is not a fair comparison, as ratings are much more difficult to compare for a real human. Therefore, it is extremely important to prove the robustness of this method. The paper must show sensitivity to label noise, inter-rater variance, and biased raters (human-in-the-loop experiments); otherwise, claims about “less human effort” or “better sample efficiency” are ungrounded.\n\n5. The experiment results are all curves but lack the IQM result report [3], which has the ability to accurately measure task performance across different tasks and seeds.\n\n[3] M., Castro, P. S., Courville, A. C., & Bellemare, M. Deep reinforcement learning at the edge of the statistical precipice. 2021\n\n6. The paper does not provide pseudocode, which makes it hard to understand."}, "questions": {"value": "1. How sensitive is the method to the soft ranking temperature and other hyperparameters?\n\n2. How does the method behave with strong class imbalance or missing classes?\n\n3. How robust is the method to noisy and inconsistent ratings across different raters?\n\n4. What is the relationship between learned reward sums and true returns? Is it more accurate than comparison-based methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i861Xf2cO4", "forum": "ASzmchacx7", "replyto": "ASzmchacx7", "signatures": ["ICLR.cc/2026/Conference/Submission4918/Reviewer_ypij"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4918/Reviewer_ypij"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761439004424, "cdate": 1761439004424, "tmdate": 1762917765479, "mdate": 1762917765479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response for questions that concerned all reviewers."}, "comment": {"value": "We thank the reviewers for their thoughtful comments and questions! In light of these, we have run and analyzed a small ethics-approved human subject study, which we discuss below. We also discuss the relative ease of providing ratings versus preferences.\n\n**Effectiveness of our proposed method with real humans**\n\nWe have now added a small human-subjects pilot study (Appendix B.1) with five participants (two authors and three non-authors). In this study, participants provided approximately 100-200 ratings for the OpenAI Gym Reacher task, a continuous state-and-action environment, demonstrating that humans can provide ratings for continuous-control trajectories. Participants could specify the number of bins they wanted for their ratings, ranging from 3 to 10. In Figures 5-10 in Appendix B.1, we observed that even with human-provided ratings, our algorithm R4 could (1) outperform the RbRL baseline and (2) perform comparably to R4 when using synthetic ratings. This suggests that our results obtained with synthetic ratings may generalize to settings with human raters. \n\nFuture work includes running more participants on more domains, comparing directly with collecting preferences from human subjects, and performing a qualitative analysis to understand where participants prefer one method over another. However, we hope this small study is sufficient to show the promise of R4 and its impact on data from humans (as well as the extensive experiments on synthetic data).\n\n\n**Imperfect / Non-uniform Data**\n\nIn Figures 6-10 in Appendix B.1, we found that participants often rated trajectories in a class-imbalanced manner, providing more labels for one class than another. Moreover, the distribution of labels per rater also varies. This further supports our claim that R4 is likely to be robust to real-world settings with class imbalance and inter-rater variability. These figures also contain the distribution of undiscounted environment returns associated with each label in the rated dataset. We found substantial overlap in true returns across labels. This means that trajectories with a similar ground truth return were labeled differently. This confirms that the human ratings are imperfect/noisy. These results further support our claim that R4 is robust to rater bias and variance.\n\nOur online experiments with simulated feedback also demonstrate that R4 performs well under non-uniform rating distributions. Rating class imbalance occurs naturally since trajectories are added as they are encountered by the agent, which rarely produces uniform coverage. In addition, R4 remains effective even when some rating bins are missing, such as when the agent fails to generate trajectories corresponding to the highest ratings.\n\n**Humans’ ability to provide ratings versus preferences**\n\nOur work builds on the RbRL paper, which provides a detailed analysis of the claim that ratings are more informative and cognitively efficient than preferences. In their human subject study, participants were asked to provide both ratings and binary preferences over trajectories from OpenAI Gym Mujoco environments (Hopper, Swimmer, and Half-Cheetah). The study also included a qualitative survey assessing factors such as frustration and mental demand. The survey responses suggested that participants found the task more frustrating and mentally demanding when providing preferences compared to ratings. Additionally, participants were able to provide more ratings than preferences within the same fixed time period, further supporting the claim that ratings are more cognitively efficient (see [1] for details). When we asked the authors of this paper if we could use their human subject data, we were unfortunately told that it was only used in real-time and was not recorded. However, validating the claims from this prior work is not the primary focus of our paper; we assume they are true and aim to build better rating-based algorithms.\n\n**References**\n\n[1] White, D., Wu, M., Novoseller, E., Lawhern, V. J., Waytowich, N., & Cao, Y. (2024). Rating-Based Reinforcement Learning. Proceedings of the AAAI Conference on Artificial Intelligence."}}, "id": "rvxp3TXMTo", "forum": "ASzmchacx7", "replyto": "ASzmchacx7", "signatures": ["ICLR.cc/2026/Conference/Submission4918/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4918/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4918/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763755417382, "cdate": 1763755417382, "tmdate": 1763755753566, "mdate": 1763755753566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a rating‑based reward‑learning method that treats human ratings as ordinal supervision. Given trajectory–rating pairs, the proposed method R4 predicts returns with a reward model, applies a differentiable ranking operator to obtain soft ranks, and minimizes a ranking mean squared error (rMSE) between these soft ranks and the (ordered) rating labels. The authors argue this avoids hand‑crafted decision boundaries used in Rating‑based RL (RbRL) and preserves within‑class variance. They provide theory claiming the rMSE objective is “minimal and complete” under assumptions (deterministic realizability, correct binning, and an exact differentiable ranking), and evaluate R4 with simulated raters in Gym and DMC locomotion tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Simple, practical objective. Treating ratings as ordered targets and aligning ranks, instead of absolute scores, gives an intuitive loss that removes the design burden of rating bin boundaries required by RbRL.\n- Leverages modern differentiable ranking. Building on fast, differentiable ranking/sorting (permutahedron projections) is sensible and computationally efficient compared to older proxies.\n- Broad empirical envs on standard control suites. The paper reproduces common baselines (PEBBLE, SURF, QPA) and reports consistent improvements in several tasks, with ablations on feedback scheduling and sampling."}, "weaknesses": {"value": "- The “minimality and completeness” results crucially assume (i) deterministic reward realizability, (ii) perfect ordinal binning by the teacher, and (iii) exact differentiable ranking that outputs true ranks. In practice, (iii) is not guaranteed—soft‑rank operators are precise for soft projections but do not generally equal hard ranks except in limiting or special cases. The paper cites [1] for “exact computation,” but that exactness refers to the soft operator and its gradients, not equality to hard ranks; the assumption that soft ranks equal true ranks is unrealistic for finite regularization and noisy scores.\n- All results derive ratings from the ground‑truth reward, which risks optimistic conclusions and sidesteps the very issues that make reward learning hard (ambiguity, inconsistency, bias). Prior work has highlighted reward misspecification and gaming; without human‑rated studies or at least real‑world noisy labels, it is hard to assess robustness.\n- Missing or under‑engaged related works: Rating/ordinal RM beyond RbRL: recent ordinal feedback reward modeling for LLMs extends beyond binary preferences and analyzes conditions for unbiasedness with graded labels—highly relevant to “ratings as ordinals.” [2]. VPIL models vague pairwise during the learning [3]. LiPO/PLPO optimize with ranked lists rather than pairs; while policy‑side, they show the broader move from pairwise to listwise/ordinal signals [4].\n\n## References\n[1] Fast differentiable sorting and ranking.\n\n[2] Reward Modeling with Ordinal Feedback: Wisdom of the Crowd.\n\n[3] Imitation Learning from Vague Feedback.\n\n[4] LiPO: Listwise Preference Optimization through Learning-to-Rank."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zj3KLG2zna", "forum": "ASzmchacx7", "replyto": "ASzmchacx7", "signatures": ["ICLR.cc/2026/Conference/Submission4918/Reviewer_RiUH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4918/Reviewer_RiUH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632443482, "cdate": 1761632443482, "tmdate": 1762917765098, "mdate": 1762917765098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Ranked Return Regression for RL (R4), a rating-based reward learning method that learns reward functions from ordinal human ratings instead of pairwise preferences or manually designed rewards. R4 introduces a ranking mean squared error (rMSE) loss that aligns predicted returns with teacher-provided ratings through differentiable ranking. The authors provide theoretical guarantees of completeness and minimality, and demonstrate through experiments on OpenAI Gym and DeepMind Control Suite that R4 outperforms existing rating- and preference-based methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed ranking mean squared error (rMSE) loss is conceptually simple yet technically elegant, effectively leveraging ordinal ratings through differentiable ranking.\n\n2. The theoretical analysis provides formal guarantees of completeness and minimality, strengthening the method’s conceptual foundation.\n\n3. The experimental results encompass both offline and online feedback settings across multiple continuous-control benchmarks.\n\n4. The ablation studies are carefully designed, demonstrating the stability of the core method and the incremental benefits of auxiliary design choices such as dynamic feedback scheduling and stratified sampling."}, "weaknesses": {"value": "1. The paper provides limited conceptual and empirical discussion on how rating-based feedback compares to preference-based feedback. While the authors claim that ratings are more informative and cognitively efficient, this assumption is not rigorously analyzed.\n\n2. The theoretical results rely on several strong assumptions, such as deterministic reward realizability, perfectly consistent rating bins, and nearly exact differentiable ranking, which may not hold in realistic human feedback scenarios. \nI acknowledge that Assumption 5 attempts to relax the requirement of exact ranking by introducing bounded ranking errors, but it only addresses the approximation error of the differentiable ranking operator rather than the inherent noise or inconsistency in human feedback.\n\n3. All experiments are conducted with simulated feedback derived from environment rewards. As a result, the evaluation does not account for real-world human variability, label noise, or subjective inconsistencies that often arise in practice. Moreover, the experimental environments (DMC and MuJoCo) and the rating-based setting are not well aligned. It would be extremely difficult for human annotators to provide consistent absolute ratings or rankings for these continuous-control trajectories, which differ subtly in motion quality rather than in clear success or failure outcomes. This mismatch raises questions about how well the current results translate to realistic human feedback scenarios.\n\n4. The proposed framework's generalization to true human feedback or more complex domains remains unverified, limiting the empirical validity of its broader claims."}, "questions": {"value": "1. How would the proposed rMSE formulation perform when ratings come from real human annotators, whose feedback may be noisy, inconsistent, or influenced by context?\nGiven that all experiments use simulated feedback, how might the results change in a genuine human-in-the-loop setting?\n\n2. Does the method remain theoretically sound or empirically stable if the assumption of perfectly ordered or deterministic ratings is relaxed?\n\n3. How sensitive is R4 to the number of rating levels or to non-uniform distributions of ratings (e.g., heavy bias toward mid-level ratings)?\n\n4. Conceptually, the current formulation seems to discretize what could otherwise be treated as continuous feedback. To what extent is R4 genuinely modeling human ordinal judgment, as opposed to learning from a discretized version of continuous return values?\n\nOverall, I find the proposed approach well-motivated and technically sound within the ranking-based setting. The method provides a reasonable and effective formulation for learning from ordinal ratings, and the rMSE loss represents a meaningful contribution in this direction. However, the paper would benefit from a deeper conceptual discussion of how rating-based feedback fundamentally differs from preference-based supervision, both in terms of information structure and practical trade-offs. Moreover, the theoretical and experimental analyses rely on strong and idealized assumptions. A broader reflection on these assumptions, or additional experiments that relax them, especially regarding their implications for real human feedback, would substantially strengthen the paper's overall impact and credibility. While I appreciate the noise-related experiments included in the appendix, these simulations do not fully capture the complexity and inconsistency of real human feedback, and thus leave open questions about the method’s robustness in practical settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VHnzw12AJ6", "forum": "ASzmchacx7", "replyto": "ASzmchacx7", "signatures": ["ICLR.cc/2026/Conference/Submission4918/Reviewer_T1Nu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4918/Reviewer_T1Nu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644653438, "cdate": 1761644653438, "tmdate": 1762917764512, "mdate": 1762917764512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}