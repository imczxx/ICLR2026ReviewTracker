{"id": "XL7ValpExh", "number": 12571, "cdate": 1758208690673, "mdate": 1759897501181, "content": {"title": "Maximizing Incremental Information Entropy for Contrastive Learning", "abstract": "Contrastive learning has achieved remarkable success in self-supervised representation learning, often guided by information-theoretic objectives such as mutual information maximization. Motivated by the limitations of static augmentations and rigid invariance constraints, we propose IE-CL (Incremental-Entropy Contrastive Learning), a framework that explicitly optimizes the entropy gain between augmented views while preserving semantic consistency. Our theoretical framework reframes the challenge by identifying the encoder as an information bottleneck and proposes a joint optimization of two components: a learnable transformation for entropy generation and an encoder regularizer for its preservation. Experiments on CIFAR-10/100, STL-10, and ImageNet demonstrate that IE-CL consistently improves performance under small-batch settings. Moreover, our core modules can be seamlessly integrated into existing frameworks. This work bridges theoretical principles and practice, offering a new perspective in contrastive learning.", "tldr": "", "keywords": ["Self-supervised Learning; Contrastive Learning;"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa3dd43757ef7c5f1ac48d3977029566ff665736.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work modifies the view generation process for contrastive learning by modfiying the query view generation process with a view-generating neural network which functionally acts as an augmentation generator. This view generator is trained with entropy maximization, making the negatives harder and therefore increasing the loss.  Overall, this results in stronger performance when combined with self-supervised methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* There is a mathematical intuition which, at a high level, is sensible is a novel motivatation for the method.\n* Empirical evaluations show small but consistent improvements.\n* It is not too costly, computationally."}, "weaknesses": {"value": "* There are limited transfer learning results, which is the main application of self-supervised pre-training. In particular, classification results are missing. \n\n* ViTs are not evaluated. Would the method work with a Vision Transformer backbone? Vision Transformers are ubiquitous. \n\n* There are missing strong self-supervised baselines, such as DINO[1] style training. \n\n[1] Oquab, Maxime, et al. \"Dinov2: Learning robust visual features without supervision.\" arXiv preprint arXiv:2304.07193 (2023)."}, "questions": {"value": "* How could this be extended to Vision Transformers, to modernize the method?\n\n* I'm curious if the gains could be attributed largely to spectral regularization? In the ablation table, what would be the result of ONLY encoder regularization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wh6NBWWA4L", "forum": "XL7ValpExh", "replyto": "XL7ValpExh", "signatures": ["ICLR.cc/2026/Conference/Submission12571/Reviewer_Trhc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12571/Reviewer_Trhc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761703169846, "cdate": 1761703169846, "tmdate": 1762923424793, "mdate": 1762923424793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces IE-CL, a novel information-theoretic framework for self-supervised learning. The motivation stems from two perceived limitations in existing Contrastive Learning methods: the inflexibility of static data augmentation and the representation compression caused by the deep encoder acting as an \"information bottleneck.\""}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The SAIB module is a genuinely clever algorithmic contribution. It replaces the inherent rigidity of manually designed data augmentations with a learnable, instance-specific mechanism.\n\n2. The paper is well-grounded in information theory. By explicitly framing the deep encoder as a bottleneck, the authors move beyond the standard alignment/uniformity analysis to propose a more granular, principled objective focused on incremental information gain.\n\n3. The empirical results show improvements upon previous SOTAs."}, "weaknesses": {"value": "1. The IE-CL loss function is highly complex, requiring the delicate balancing of at least four major hyperparameter terms. There needs to be more ablations on the sensitivity.\n\n2. While the method aims to make CL more accessible by excelling in small-batch scenarios, the introduction of the SAIB module, the complex loss terms, and the explicit regularization undoubtedly incur additional computational overhead. The paper critically omits a quantitative analysis of the increase in FLOPs, training time, or memory consumption relative to baselines. This lack of efficiency analysis weakens the overall practical value, as the potential computational cost might negate the performance gain, particularly in the resource-limited settings it targets.\n\n3. The transferability of the learned augmentation should be discussed. As the standard augmentations used are not combined with specific datasets."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LtVeCQDKU4", "forum": "XL7ValpExh", "replyto": "XL7ValpExh", "signatures": ["ICLR.cc/2026/Conference/Submission12571/Reviewer_DnaM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12571/Reviewer_DnaM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902595664, "cdate": 1761902595664, "tmdate": 1762923424310, "mdate": 1762923424310, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing self-supervised contrastive learning methods mainly rely on augmentation-based invariance constraints, which limit representation expressiveness. This paper introduces entropy as a measure to preserve semantic consistency and improve expressiveness. The proposed Sample Augmentation Incremental Block (SAIB) and Incremental Information Entropy (IncEntropy) objective capture entropy gain, reflecting the diversity of information lost after encoding. The main contribution is using the encoder’s information bottleneck (via SAIB) to extract semantic information while reducing noise."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The approach is interesting, particularly in introducing the concept of entropy into self-supervised learning through SAIB and IncEntropy. In particular, SAIB presents an appealing way to apply entropy, showing the largest performance gain in the ablation study (Table 3).\n\n- The method is also simple and can be easily integrated into existing SSL frameworks, as demonstrated in Table 5."}, "weaknesses": {"value": "1. Although the effectiveness of the proposed method is demonstrated throughout the paper, most experiments (except Table 1) are conducted on relatively small-scale settings such as ResNet-18 or ImageNet-100. Since Table 3 highlights the strong effect of entropy generation through SAIB, it would be valuable to evaluate the method on larger or standard-scale benchmarks. The same applies to Table 5.\n\n2. It would also be helpful to include an ablation study with semantic consistency only, to better isolate and verify the effectiveness of SAIB.\n\n3. It is well known that dense prediction tasks, such as semantic segmentation, differ significantly from image classification, and standard SSL methods often struggle to generalize well to these tasks. Therefore, to more convincingly demonstrate the effectiveness and general applicability of the proposed method, evaluation on additional (dense) prediction benchmarks beyond the Pascal dataset would be beneficial.\n\n4. As the authors also mentioned, SAIB relies on Spectral Normalization, which is designed for convolutional priors. Therefore, this approach cannot be directly applied to ViT-based backbones."}, "questions": {"value": "1. Could the authors provide insights or results on the performance of the proposed method in larger-scale settings?\n2. Have the authors considered whether there is any way to extend this approach to ViT-based backbones, despite the limitation that Spectral Normalization cannot be applied?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oo2yyVE1kZ", "forum": "XL7ValpExh", "replyto": "XL7ValpExh", "signatures": ["ICLR.cc/2026/Conference/Submission12571/Reviewer_gtzf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12571/Reviewer_gtzf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910028091, "cdate": 1761910028091, "tmdate": 1762923423823, "mdate": 1762923423823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces IE-CL, which is a framework designed to overcome the limitations of static augmentations and large batch sizes in CL. So the authors show that maximizing the \"incremental entropy\" (the entropy gain between augmented views) while preserving semantics is equivalent to minimizing the InfoNCE loss.\n\nTo achieve this, they propose the SAIB which is a lightweight, trainable module that plugs into the query branch. To my understanding, I think SAIB learns to expand the representation space (increasing entropy), while a KL divergence regularizer ensures the new view remains semantically consistent with the original. The authors have conducted several experiments show SAIB improves linear evaluation performance, works well in small-batch settings, can enhance other non-contrastive methods, and helps with downstream tasks like segmentation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I think the paper has some strengths:\n\nFirst, I think the theoretical foundation is a major plus. The authors provide a proof for their core claim, linking the minimization of contrastive loss to the maximization of incremental entropy. In my opinion, this reframing of contrastive learning as a trade-off between entropy expansion and semantic alignment is a novel information-theoretic perspective.\n\nAlso, I think the SAIB module itself is a contribution. It's not just an arbitrary layer but a lightweight block designed to execute the paper's goal (inducing positive entropy increments) while being regularized to preserve semantics.\n\nFinally, I think the experiments seem comprehensive. The authors validate their method across a wide range of datasets and tasks. I think it's particularly strong that they test not only linear probing but also show effectiveness in small-batch settings (a key weakness of many CL methods) and on downstream transfer tasks like segmentation and detection."}, "weaknesses": {"value": "I think the paper's primary weakness is its narrow experiment, which doesn't fully support the broad claims of a improved \"framework.\" My main issue is that all experiments are confined to ResNet architectures. The self-supervised learning field has largely migrated to Vision Transformers (ViTs), and their complete absence here is a glaring omission. In my opinion, I feel this makes the work somewhat dated and raises a critical question: is this incremental entropy principle a general SSL concept, or is it just a clever trick that happens to work well with the inductive biases of CNNs? The claim of generalizability is a little bit undermined when the method isn't tested on the field's dominant architecture.\n\nMy other concern is that the analysis of the method's new hyperparameters is too thin. The SAIB module adds a new layer of complexity, particularly the KL regularization weight, but the provided ablation study is basic. I think It's hard to tell how robust the method is or what the practical tuning cost would be for a new user. I think the paper misses the chance to investigate the interplay between the new module and existing crucial hyperparameters. For example, the temperature (τ) is important to tell how the model works. How does adding SAIB affect the model's sensitivity to temperature? Does the optimal τ change? I think demonstrating the analysis is needed for anyone trying to implement this method."}, "questions": {"value": "See the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pnbFsxrYr8", "forum": "XL7ValpExh", "replyto": "XL7ValpExh", "signatures": ["ICLR.cc/2026/Conference/Submission12571/Reviewer_bSVD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12571/Reviewer_bSVD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987482942, "cdate": 1761987482942, "tmdate": 1762923423439, "mdate": 1762923423439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes IE-CL (Incremental-Entropy Contrastive Learning) for self-supervised contrastive learning that explicitly models entropy generation and preservation in the learning process. The central idea is to inject entropy at the input level via a learnable augmentation block (SAIB), then constrain the encoder to preserve this expanded entropy through spectral normalization. This is motivated by an information-theoretic decomposition of contrastive learning objectives under the Data Processing Inequality (DPI), suggesting that maximizing representational informativeness requires both entropy creation (from augmentations) and controlled propagation (through the encoder). The model comprises three main components: 1) SAIB (Sample Augmentation Incremental Block): A learnable transformation enforcing \"volume-expanding\" Jacobian to expand local volume in the feature manifold, thereby increasing input-space entropy. 2) KL Regularizer: Maintains semantic consistency between entropy-expanded samples. 3) Encoder Preservation (Spectral Norm): Constrains the encoder’s Lipschitz constant to prevent information collapse.\n\nThe provided experimental results on CIFAR-10/100, STL-10, and ImageNet-100 demonstrate consistent performance improvements over SimCLR, BYOL, MoCo-v2, and SimSiam. The ablation studies show the SAIB block contributes the most, while the encoder regularizer has only a marginal effect."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed formulation leads to a new perspective on entropy control in SSL. Unlike prior works (e.g., InfoMax-SSL, VICReg, Matrix-IB) that maximize representation-level entropy, IE-CL proposes to inject entropy at the input level through a learnable augmentation mechanism. This shift from output-space to input-space entropy control is novel and conceptually meaningful.\n2. The proposed method is established based on sound theoretical motivation. The information-theoretic derivation connecting augmentation entropy, encoder Jacobian, and DPI is mathematically correct and clearly stated. The idea of treating entropy propagation as an “incremental” process is intuitive and reasonable.\n3. The empirical validation is comprehensive. The method is tested on multiple datasets and integrated into different contrastive baselines (e.g., SimCLR, BYOL, MoCo-v2, SimSiam). The experimental results show consistent gains, and training overhead is minimal.\n4. The presentation includes sufficient implementation and training details. The ablation studies reveal transparent module effects and interactions."}, "weaknesses": {"value": "1. The claim that encoder preservation is necessary appears overstated. The paper argues that spectral normalization of the encoder is required to prevent the loss of generated entropy (Section 3.3). However, Table 3 shows that removing this component leads to only a marginal change in performance (0.26 percent difference). This result indicates that the encoder likely already preserves entropy through existing normalization layers and the contrastive objective itself. Therefore, the Encoder Preservation module should be characterized as helpful rather than necessary, since the theoretical argument appears to overextend a sufficient condition into a claim of necessity.\n2. The plug-and-play experiment in Table 5 evaluates only the addition of the SAIB module to other baseline methods, without reporting results for the combined configuration of “SAIB plus Encoder Regularizer.” This omission further reinforces the impression that the SAIB module is the sole component contributing meaningfully to performance improvements, while the Encoder Regularizer has little demonstrable effect.\n3. The empirical gains in the experiments are limited. For example, ImageNet improvements are visible but not substantial (+1.3 \\% over Matrix-SSL at 800 epochs). Also, multi-seed or statistical analysis is not provided to confirm the significance of the performance gains.\n4. Although the principle of maximizing information entropy is well established, the novelty of IE-CL lies primarily in where the entropy is introduced, namely at the input level rather than the representation level. The paper would benefit from emphasizing this distinction more clearly and from explicitly differentiating its approach from prior InfoMax and variance-regularized methods such as VICReg, InfoMin, and EMP SSL."}, "questions": {"value": "The authors are suggested to respond to those raised in **Weaknesses.*\n\n**Additional Questions**\n\nThe proposed SAIB module is implemented as a small convolutional network. How can the IE-CL framework be extended to deal with Vision Transformer architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X48q0BPzJ8", "forum": "XL7ValpExh", "replyto": "XL7ValpExh", "signatures": ["ICLR.cc/2026/Conference/Submission12571/Reviewer_LX2z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12571/Reviewer_LX2z"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012226439, "cdate": 1762012226439, "tmdate": 1762923422958, "mdate": 1762923422958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}