{"id": "9iR2dprQNJ", "number": 10611, "cdate": 1758177312985, "mdate": 1759897640759, "content": {"title": "Mixture of Heterogeneous Grouped Experts for Language Modeling", "abstract": "Mixture-of-Experts (MoE) offers superior performance over dense models. However, current MoEs impose a critical limitation by enforcing uniform expert sizes, restricting the model's ability to dynamically match computational resources with token-specific requirements. Despite several attempts on heterogeneous experts have been made, they struggle either with limited performance and inefficient parameter utilization or unbalanced GPU utilization, there is still a lack of general heterogeneous MoE architecture.\nTo this end, we present Mixture of Heterogeneous Grouped Experts (MoHGE), an innovative MoE architecture that introduces a two-level routing mechanism and enables more nuanced and efficient expert selection tailored to each input token's characteristics. We also propose a Group-Wise Auxiliary Loss to enhance efficient parameter utilization without compromising model performance.\nTo address the resulted workload imbalance challenges, we develop: (1) an All-size Group-decoupling Allocation  strategy and (2) Intra-Group Experts Auxiliary Loss, collectively ensuring balanced GPU utilization.\nExtensive evaluations on multiple benchmarks demonstrate that MoHGE achieves comparable performance to state-of-the-art MoE architectures while reducing total parameter count by approximately 20\\% and maintaining balanced GPU utilization. Our work establishes a new paradigm for resource-aware MoE design, better aligning computational allocation with actual inference demands.", "tldr": "", "keywords": ["MoE", "LLM", "GPU Load Balance", "Heterogeneous Experts"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fd4da905774451692c91f9d0741728841c51b527.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Traditional Mixture-of-Experts LLM design assumes all experts to be of the same amount of parameters. Recent findings show that this could lead to a waste of computation, as different tokens have different difficulties, hence require different amounts of computation to process. Existing explorations in this direction either assume all experts should be treated equally during routing, or suffer from GPU utilization imbalance. The author proposes MoHGE, which introduces a two-layer routing schema, to allow better GPU utilization balance and more diverse routing. Empirical results show that the proposed method outperforms the baseline dense model and training model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The design is intuitive and well-motivated, making the core idea easy to grasp.\n\n- The method section (sec 3) is clearly written, which is helpful for the reader to understand the approach.\n\n- The paper provides appropriate background material, achieving a good balance between necessary context and conciseness without digressing into unrelated details."}, "weaknesses": {"value": "- Insufficient experiment and baseline selection. Only the dense model and traditional MoE model was selected for comparison. In my opinion, the author should **compare against the two baselines** mentioned on line 47, which are **MoDSE and HMoE**. However, there is not a direct comparison against these two baselines in the experimental section. The main result only shows that the proposed method is better than the traditional MoE or a dense model, yet the advantage of heterogeneous experts has already been described in aforementioned prior works. Without a clear comparison against these baselines, it's hard to convince the reader that it is the proposed change that improves the performance. Specifically:\n  - What if we utilize a global load balancing loss, like in MoDSE? Is there a row in table 3 that is equivalent to such a setting? If so, why is it not clearly mentioned?\n  - How bad would the load balance become if the author chooses to adopt the expert setting in HMoE? How would that affect the hardware utilization in your settings? What would be the difference in terms of the **batch size without OOM/GPU utilization/per iteration latency**?\n\n- Insufficient details regarding the hardware and other training/inference setup. The author describes the GPU used as \"NVIDIA GPUs\", and other details are also unclear, such as the batch size chosen for training or inference. Is any inference specific acceleration technique being used? What is the communication kernel used for MoE routing? **Code has not been provided.**\n\n- Minor writing issues:\n  - Font is way too small at many places.  This creates difficulty for the readers and please consider improving the readability. I found it very difficult to read after printing it out and I had to use a large screen to read this paper.\n    - All equations\n    - Figure 1 & 3 Caption\n  - Line 156: incorrect linewidth.\n  - Missing \",\" on line 297."}, "questions": {"value": "See weaknesses for a plethora of questions. Besides:\n- Why were two epochs being trained on the LLM? Doesn't the second epoch lead to overfitting?\n\n- What is the inference/training time for the dense model? Given that the dense models are typically faster than MoE when the amount of parameters are equal, I wonder whether there exists a significant advantage to choose MoE model under the described scenario.\n\n- One interesting design choice is that for each token, the final activated expert is selected top-$K_e$ experts globally, instead of always selecting a certain number of experts from each group, which theoretically has an even better hardware balance. Is there any empirical evidence for the design?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "58SGioxZo6", "forum": "9iR2dprQNJ", "replyto": "9iR2dprQNJ", "signatures": ["ICLR.cc/2026/Conference/Submission10611/Reviewer_6p47"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10611/Reviewer_6p47"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720389392, "cdate": 1761720389392, "tmdate": 1762921874417, "mdate": 1762921874417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To enable models to dynamically match computing resources based on token-specific needs, the development of Heterogeneous MoE is of great significance. Existing studies on Heterogeneous MoE either suffer from limited performance and low parameter utilization efficiency, or face the issue of unbalanced GPU utilization, with a universal heterogeneous MoE architecture still lacking. This paper proposes the Mixture of Heterogeneous Grouped Experts (MoHGE) — an innovative MoE architecture that introduces a two-level routing mechanism, enabling more refined and efficient expert selection based on the characteristics of each input token. This paper also proposes the \"Group-Wise Auxiliary Loss\", which improves parameter utilization efficiency without compromising model performance, including the All-size Group-decoupling Allocation strategy and Intra-Group Experts Auxiliary Loss."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper proposes the Mixture of Heterogeneous Grouped Experts (MoHGE) — an innovative MoE architecture that introduces a two-level routing mechanism, enabling more refined and efficient expert selection based on the characteristics of each input token. This paper also proposes the \"Group-Wise Auxiliary Loss\", which improves parameter utilization efficiency without compromising model performance, including the All-size Group-decoupling Allocation strategy and Intra-Group Experts Auxiliary Loss."}, "weaknesses": {"value": "The predefined grouping of experts in the authors' proposed Mixture of Heterogeneous Grouped Experts (MoHGE) leads to a significant reduction in the combinatorial diversity of experts during routing.\n\nAll comparative experiments conducted by the authors lack comparisons with existing research on Heterogeneous MoE (including HMoE), only comparing against MoE models and Dense models—among these, the comparison with Dense models is unnecessary. Furthermore, in experimental design, the authors should strive to ensure consistency in total parameters and activated parameters; currently, these two critical factors exhibit significant discrepancies, making the experiments insufficiently rigorous.\n\nWhen scaling the model, will the grouping mechanism of MoHGE affect the expert parallelism (ep) strategy?\n\nThere is a lack of rigorous ablation experiments on MoHGE.\n\nIt is anticipated that refining the experimental section would result in a higher rating."}, "questions": {"value": "Refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UYGibbnf5t", "forum": "9iR2dprQNJ", "replyto": "9iR2dprQNJ", "signatures": ["ICLR.cc/2026/Conference/Submission10611/Reviewer_3vJN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10611/Reviewer_3vJN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811599651, "cdate": 1761811599651, "tmdate": 1762921873968, "mdate": 1762921873968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Mixture of Heterogeneous Grouped Experts (MoHGE), a variant of the Mixture-of-Experts (MoE) framework that partitions experts into groups of varying sizes and introduces a two-level routing mechanism (group selection followed by intra-group expert routing). Two auxiliary objectives are added to encourage balanced expert usage: a Group-Wise Auxiliary Loss and an Intra-Group Experts Auxiliary Loss.\n Experiments on several language understanding and reasoning benchmarks (MMLU, GSM8K, SIQA, LAMBADA, PIQA, TriviaQA, MATH) across 1B, 3B, and 14B models show similar or slightly better accuracy compared to homogeneous MoE baselines, with roughly 20% fewer parameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses a practical bottleneck in large-scale MoE training.\n- Method is straightforward and compatible with existing frameworks.\n- Writing and figures are clear and professional.\n- Experiments span multiple scales and include ablations on loss terms."}, "weaknesses": {"value": "- Missing quantitative evidence of GPU utilization or efficiency.\n- Reported improvements are small and lack statistical validation.\n- No theoretical  analysis of routing dynamics or convergence.\n- Experimental reporting omits critical hyperparameters and setup details.\n- Contribution is incremental relative to prior heterogeneous MoE work.\n\n---\n1. Quantify “Efficiency” Claims (Page 3–4)\n- Provide actual GPU load statistics, throughput (tokens/sec), and communication cost.\n- Report training time per step and FLOPs vs. MoE baseline.\n- Showing real efficiency data would validate the motivation and significantly raise the Quality and Significance scores.\n2. Report Statistical Significance (Page 6–7, Table 1)\n- Repeat experiments with multiple random seeds.\n- Include standard deviation or confidence intervals.\n- Apply paired t-tests to confirm that improvements are not noise.\n3. Ablate Grouping vs. Routing (Page 5)\n- Isolate the effect of heterogeneous expert sizing and two-level routing separately.\n- Provide visualization of token-to-group assignment entropy.\n- Helps clarify where gains actually come from, strengthening Originality.\n4. Provide Theoretical or Analytical Justification (Page 4)\n- Offer a short analysis on expected load variance under two-level routing.\n- Discuss whether routing convergence differs from standard MoE gating.\n5. Expand Experimental Reporting \n- Include training setup: batch size, gradient accumulation, GPU count, communication strategy, optimizer, etc.\n- Report training hours and memory footprint.\n6. Broaden Evaluation Scope (Page 7–8)\n- Add large-scale pretraining or autoregressive datasets (e.g., C4, Pile).\n- Report cost per token to demonstrate real-world scalability.\n7. Tone Down Overstatements (Page 1 & 9)\n- Replace “establishes a new paradigm” with more measured phrasing like “offers a simple variant that modestly improves efficiency.”"}, "questions": {"value": "Although the goal—improving parameter efficiency and GPU load balance in MoE systems—is relevant, the evidence provided does not convincingly support that MoHGE achieves this goal in a meaningful way.\n1. The motivation is not empirically substantiated.\n The paper claims that homogeneous experts lead to “severe GPU imbalance” and “inefficient utilization,” yet no quantitative evidence (e.g., utilization variance, expert activation frequency, or throughput) is reported. The claim remains speculative.\n2. Efficiency claims lack measurements.\n The reported 20% reduction in parameters simply reflects reduced hidden dimensions for some expert groups (Page 6, Table 1). There is no analysis of actual computation cost, latency, or memory use. Without such data, it is unclear whether MoHGE is truly more efficient.\n3. Empirical improvements are small and inconsistent.\n Across benchmarks, improvements over MoE are ≤1 point—likely within training variance (Table 1, Page 6). On GSM8K, inference is slower. Without multiple runs or variance reporting, these gains are not statistically credible.\n4. Limited analysis of mechanism and behavior.\n The two-level routing and auxiliary losses are introduced heuristically, without analyzing how they affect routing stability or specialization. Figures 2–3 show routing distributions but offer no interpretation.\n5. Incremental novelty.\n Similar ideas appear in HMoE and MoDSE. MoHGE reorganizes them hierarchically but does not introduce a fundamentally new concept or analytical perspective."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dhsJFTtTlz", "forum": "9iR2dprQNJ", "replyto": "9iR2dprQNJ", "signatures": ["ICLR.cc/2026/Conference/Submission10611/Reviewer_aBiB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10611/Reviewer_aBiB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913198264, "cdate": 1761913198264, "tmdate": 1762921873571, "mdate": 1762921873571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new Mixture-of-Experts (MoE) architecture with different expert sizes. A two-level routing strategy is used to select experts based on task difficulty. Multiple techniques (group-wise auxiliary loss, all-size group-decoupling allocation strategy, and intra-group experts auxiliary loss) are utilized for addressing GPU load imbalance, routing imbalance issues. Extensive experiments are conducted to show the effectiveness of the proposed approaches."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of the two-level routing strategy is intuitive and interesting.\n2. Extensive experiments (1B, 3B, 14B models with 0.58T tokens) are conducted to show the effectiveness of the proposed methods."}, "weaknesses": {"value": "1. Multiple loss functions are introduced in the pretraining stage, which makes the hyperparameter selection costly.\n2. The design of group-wise auxiliary loss is not that clear to me. Why is routing to the larger experts a problem? The model performance improvement introduced by the group-wise auxiliary loss seems to be marginal. The authors claim that group-wise auxiliary loss reduces the number of activated parameters. Could the author provide more qualitative results?"}, "questions": {"value": "1. What is the intuitive reason for the design of equation 5?\n2. What is the final loss function? Please list it explicitly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NksEjyP08R", "forum": "9iR2dprQNJ", "replyto": "9iR2dprQNJ", "signatures": ["ICLR.cc/2026/Conference/Submission10611/Reviewer_BbM7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10611/Reviewer_BbM7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762771054045, "cdate": 1762771054045, "tmdate": 1762921873267, "mdate": 1762921873267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}