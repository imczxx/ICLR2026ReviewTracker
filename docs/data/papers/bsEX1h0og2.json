{"id": "bsEX1h0og2", "number": 2534, "cdate": 1757138675440, "mdate": 1763527483644, "content": {"title": "UniARM: Towards a Unified Autoregressive Reward Model for Multi-Objective Test-Time Alignment", "abstract": "Multi-objective alignment aims to align LLM responses with multiple human preference objectives. Among existing methods, guiding the generation of frozen LLMs through autoregressive reward models (ARMs) to accomplish multi-objective test-time alignment is a low-cost solution.\nHowever, these methods typically rely on independent parameters for each preference objective, either by training ARMs independently across preference dimensions, which neglects interactions among preference features, or by training a single ARM with separate feature extraction modules for each preference, which can cause feature entanglement. Both strategies can result in misalignment between generated outputs and user preferences.\nTo address this limitation, we propose Preference-Modulated \\& Shared Low-Rank Adaptation (MoSLoRA) for ARM training, which  first extracts shared features via a preference-agnostic module and then  applies affine transformations to shared features via a preference modulation module conditioned on mixed preference vectors.  This design mitigates feature entanglement and enables precise control over preference trade-offs during inference. Building on this, we introduce the Unified Autoregressive Reward Model (UniARM), a novel framework for multi-objective test-time alignment. UniARM jointly models all preference dimensions in a single parameter space, eliminating the need for independent parameters for each preference objective. Experimental results show that UniARM improves HV and MIP by 18.5\\% and 30.2\\% in the safety alignment task. It also enables weak-to-strong guidance, where a smaller UniARM guides a larger frozen LLM, yielding HV and MIP improvements of 9.1\\% and 6.8\\% in the safety alignment task, and 5.4\\% and 10.7\\% in the assistant task. Notably, these gains are achieved without introducing additional parameters or increasing inference latency.", "tldr": "", "keywords": ["Test-time Alignment", "Reward Model", "Multi-Objective Alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/925742336a6e8445aabe071fa626cd878739a83a.pdf", "supplementary_material": "/attachment/821951ed24a5fe87a7715f9f80a0d091c1721811.zip"}, "replies": [{"content": {"summary": {"value": "This paper targets multi-objective alignment, and proposes Preference-Modulated & Shared Low-Rank Adaptation (MoSLoRA) for autoregressive reward modelling. Specifically, MoSLoRA extracts features shared across preferences and then adopts affine transformation to shared features. Building upon MoSLoRA, this paper proposes the UniARM, a autoregressive reward modelling framework for multi-objective test-time alignment. Experiments demonstrate the effectiveness of UniARM in safety and helpfulness alignment tasks, outperforming existing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper addresses an interesting direction of multi-objective test-time alignment.\n- This paper proposes a new MoSLoRA architecture.\n- Experiments show the effectiveness of the proposed MoSLoRA and UniARM.\n- There is a clear structure in related work review.\n- The case study presentation is very intuitive."}, "weaknesses": {"value": "- It seems that MoSLoRA relies on predefined semantic representation $o$, whereas PBLoRA does not. Therefore, it is unclear whether the performance improvement of MoSLoRA over PBLoRA stems from its architecture that integrates more information.\n- It seems that this work is an incremental research study of previous PARM, and MoSLoRA is an improved version of PBLoRA. While many readers are familiar with standard LoRA, they may be less familiar with PARM and its proposed PBLoRA. I think that additional explanation or background from standard LoRA will enhance the readability of the paper.\n- The evaluation metrics used in this paper are mainly HV and MIP. However, WinRate is a more widely used metric in the LLM preference alignment studies, but it is not involved into this paper to assess the generation quality of the model.\n- It seems that the experiments are primarily conducted on Alpaca-7B, which is a relatively outdated model. Using more recent models, such as Qwen3 series, will be better convincing.\n- There is no anonymous code link to present the reproducibility of this work."}, "questions": {"value": "- Have you considered comparing UniARM with vanilla RLHF methods, such as PPO with a pretrained reward model, under test-time alignment scenario?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JdozFq9LeR", "forum": "bsEX1h0og2", "replyto": "bsEX1h0og2", "signatures": ["ICLR.cc/2026/Conference/Submission2534/Reviewer_97Hm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2534/Reviewer_97Hm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761315721031, "cdate": 1761315721031, "tmdate": 1762916271177, "mdate": 1762916271177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UniARM, a unified framework for multi-objective alignment of LLMs that balances multiple human preference goals at test time. It achieves this by introducing MoSLoRA, which learns shared features across preferences and modulates them through preference-conditioned transformations, reducing feature entanglement and enabling fine-grained trade-offs. The approach improves alignment quality and efficiency without slowing inference and without retraining the base LLMs over and over again."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper deals with a very relevant area of using test-time alignment method to achieve multi-objective goals and achieve good experimental result. \n\n2. Unlike the prior art PARM, this method does not require linearly combining different core tensors based on the preference vector during test time, which is more reasonable. \n\n3. The experiments are extensive, including helpfulness and harmlessness evaluation, as well as weak-to-strong extension. It is especially good to see the weak-to-strong experiments as it is very expensive to retrain larger LLMs for multi-objective goals using training-based method, and the proposed method seems like an efficient alternative.\n\n4. The paper is well-written and clearly explain the difference between the prior work."}, "weaknesses": {"value": "1. While the experiments setting follows prior work, the LLM used here seems not very up to date. It would be nice if the author can evaluate on more recent LLMs. For example, tulu-3 instead of tulu-2. \n\nOther minor issues\n1. Typo in equation (10)\n2. In Figure 2, the results of RS and MOD are set to zero. Although it is understandable that they are very expensive to run, I am not sure if it is a good idea to set them to be zero."}, "questions": {"value": "If the model has been trained for, for example, objective A and B, and then there is a different objective C, what is the quickest way to adapt to the new objective? It seems that in GenARM, the reward model of objective C can be immediately used. I wonder what we should do for your proposed UniARM."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GDfu9GRTwA", "forum": "bsEX1h0og2", "replyto": "bsEX1h0og2", "signatures": ["ICLR.cc/2026/Conference/Submission2534/Reviewer_rgLD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2534/Reviewer_rgLD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761686860271, "cdate": 1761686860271, "tmdate": 1762916270915, "mdate": 1762916270915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response"}, "comment": {"value": "We thank all reviewers for their valuable feedback.  The reviewers recognized our work for its strengths across multiple aspects: \n\n- **Method Strengths**:\n    - Recognized as **more reasonable** (rgLD)\n    - Achieves **superior performance** (SHwo, 9Eti, rgLD, 97Hm)\n    - Exhibits **higher parameter efficiency** (SHwo)\n- **Paper Quality**:\n    - **Well-written** (SHwo, rgLD)\n    - **Experiments are well-designed / extensive** (SHwo, rgLD)\n    - **Comprehensive related work** (9Eti, 97Hm)\n    - **Intuitive case study presentation** (97Hm)\n- **Implementation & Impact**:\n    - **Easy to implement** (SHwo)\n    - **Strong potential to significantly impact the community** (SHwo)\n    - **Efficient alternative to training large LLMs** (rgLD)\n    - Addresses an **interesting research direction** (97Hm)\n\nBelow, we address common concerns and misunderstandings and provide additional experimental results.\n\n**Concern 1:**  The reviewers notes that MoSLoRA appears to be a minor modification of PBLoRA, as used in PARM, and requests an intuitive explanation for why MoSLoRA yields a superior Pareto frontier.\n\n**Regarding differences between PBLoRA and MoSLoRA**  We respectfully clarify that MoSLoRA introduces a fundamental structural paradigm shift compared to PBLoRA [1], instead of a minor variant. **In PBLoRA, all modules are responsible for feature extraction, whereas in MoSLoRA, one module focuses on extracting shared features, and the other focuses on modulating these features based on preferences.**\n\n- PBLoRA constructs k preference-specific LoRA branches on top of a preference-agnostic module $B_1 W_1 A_1$, where each branch  $B_2 W_{2,i} A_2$ learns features tailored to a specific preference:\n\n$h_{\\text{PBLoRA}} = (W_{\\text{base}} + B_1 W_1 A_1 +   \\sum_{i=1}^{k}  \\alpha_i   B_2 W_{2,i} A_2 )h$\n\nBecause multiple branches operate on the same shared features and are ultimately aggregated at the output, no branch can independently specialize in its own preference-specific features, which in turn leads to feature entanglement across preferences.\n\n- MoSLoRA **removes all preference-specific branches and introduce preference-modulation module ( $B_2 W_\\gamma A_2$, $B_2 W_\\eta A_2$)**. The preference-agnostic module and the preference modulation module have distinct roles and operate in separate spaces:\n\n$h_{\\text{MoSLoRA}} = (\\gamma_{o'} + 1) \\odot (W_{\\text{base}} + B_1 W_1 A_1)h + \\eta_{o'}$\n\n$\\gamma_{o'} = B_2 W_\\gamma A_2 o'$\n\n$\\eta_{o'} = B_2 W_\\eta A_2 o'$\n\nThe preference-agnostic module ($W_{\\text{base}} + B_1 W_1 A_1$) learns shared features. The preference-modulation module  generates affine parameters ($\\gamma_{o'}$, $\\eta_{o'}$) conditioned on the mixed preference vector o’ to adjust the mean and scale of the shared features.\n\nThe preference-agnostic module is responsible for extracting shared features, while the modulation module does not learn features itself and only adjusts them at the output stage based on user preferences. This structural decoupling allows the learning of shared representations and the application of preference modulation to proceed independently, fundamentally preventing conflicts between the two modules.\n\n**Regarding why MoSLoRA yields a better Pareto Frontier？**\n\nPBLoRA relies on independent branches, which often produce discrete and fragmented solution regions. Features corresponding to different preferences frequently become entangled, resulting in a non-smooth Pareto frontier and reduced overall Pareto efficiency.\n\nIn contrast, MoSLoRA captures differences across preference combinations by applying affine transformations to the statistics (mean and scale) of the shared features, thereby constructing a continuous and adjustable objective manifold.\n\nWhile PBLoRA learns feature distributions for each individual preference and then combines them, MoSLoRA directly learns the feature distribution corresponding to a specific combination of preferences. As a result, solutions under different preferences no longer rely on separate parameter subsets but are realized as smooth transformations of the shared feature space. This structural re-parameterization reduces feature interference and naturally produces a smoother and superior Pareto frontier. Furthermore, our method is simple and effective."}}, "id": "i1x0HhLV3L", "forum": "bsEX1h0og2", "replyto": "bsEX1h0og2", "signatures": ["ICLR.cc/2026/Conference/Submission2534/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2534/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2534/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763528060621, "cdate": 1763528060621, "tmdate": 1763608297176, "mdate": 1763608297176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UniARM, a framework for multi-objective test-time alignment. The proposed method employs a parameter-efficient architecture called MoSLoRA, which consists of a preference-agnostic module and a preference-modulation module. Experimental results demonstrate the effectiveness of UniARM compared to existing test-time alignment approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The related work section is comprehensive.\n\n- The method eliminates the need to train multiple separate ARMs or preference-aware modules. Instead, it requires training only one preference-agnostic module and one preference-modulation module for alignment multiple objectives.\n\n- The experimental results show superior performance compared to previous test-time alignment methods."}, "weaknesses": {"value": "Overall, I find the methodological design of the paper reasonable, though the experimental section requires further strengthening. If the following concerns can be adequately addressed, I would consider raising my score.\n\n- I'm unfamiliar with test-time multi-objective alignment methods based on ARM. Therefore, I'm puzzled about the motivation behind this approach. Since we can fine-tune the ARM, why do we just fine-tune the LLM itself?\n\n- The backbone model used is relatively outdated. Employing a more recent backbone model (e.g., LLaMA-3, Qwen-3) and reward model (e.g., ARMO-RM, Skywork-LLaMA-V2) would strengthen the validity of the results.\n\n- Since the ARM also involves fine-tuning the base model, it is essential to compare UniARM with other training-based multi-objective methods, such as MODPO and other state-of-the-art baselines.\n\n- Incorporating LLM-as-a-judge evaluation on benchmarks like AlpacaEval (for helpfulness) would provide a more realistic and convincing assessment.\n\n- Table 2: It would be beneficial to include results without any ARM, using only Alpaca-65B. Additionally, the use of an older model raises concerns about whether similar performance can be achieved with newer backbone models.\n\n- An analysis of the sensitivity to hyperparameters such as λ and β is missing. Also, the impact of varying the text descriptions of objectives on the results should be investigated.\n\n- Details regarding training and inference computational costs are not provided.\n\nTypos:\n\n- Line 32: The meaning of \"PB\" should be explicitly explained to enhance clarity.\n\n- Eq. (3):The notation \"i\" is not defined, which may cause confusion.\n\n- Eq. (10): A closing bracket \")\" is missing."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5TC8M1Cgkf", "forum": "bsEX1h0og2", "replyto": "bsEX1h0og2", "signatures": ["ICLR.cc/2026/Conference/Submission2534/Reviewer_9Eti"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2534/Reviewer_9Eti"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807999435, "cdate": 1761807999435, "tmdate": 1762916270736, "mdate": 1762916270736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of multi-objective test-time alignment of LLMs. The authors propose UniARM based on the better parameterization method named MoSLoRA. The method is parameter-efficient because it uses the semantic embeddings of the model itself to encode the multiple preferences instead of additional weights. At the same time, the empirical results also demonstrate the superiority of the method in comparison to previous state-of-the-art methods. Although there is some weakness, I believe they can be improved with minor revisions after the rebuttal. Therefore, I recommend \"accept\"."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to understand.\n2. The proposed method performs better while being more parameter-efficient than baselines. I believe this gain outweighs the slightly weak originality of the proposed method (weak originality as it consists in a minor change to the low-rank adapter used in PARM plus a regularizer).\n3. I think the experiments are very well designed. Enough meaningful baselines are included. The ablation experiments provide clear clues as to how each component of the method affects the performance. The choice of the quantitative metric of HV and MIP makes the performance gap between the proposed method and baselines more convincing.\n4. The method itself is simple to reimplement, which implies a strong potential to be an impactful contribution to the community."}, "weaknesses": {"value": "1. Although the proposed method is empirically effective, there is a lack of intuitive or theoretical explanation of where the effectiveness (i.e., the better Pareto front) comes from. Can the authors explicitly provide such explanations? For example, why can a different parameterization of the token-level reward models alone (according to the ablation experiment when $\\lambda=0$) lead to a better Pareto front? \n2. (Partially related to the first weakness) The generality of the method is unknown. Is the better Pareto front only limited to the Alpaca family? Can the authors provide results on other families (Llama, Qwen, etc.) to make the generality of the method more convincing?\n3. I find the term \"Pareto-optimal\" too strong and vague to describe an ARM as there is still room for improvement. I suggest a softer term \"more/less Pareto-efficient\".\n\n---------------\nNote: I identified several typos (not a weakness but a nontrivial issue to fix): \n * (Less serious) Line#292-#293:  \"the reward of UniARM is computed as::\" --> \"the reward of UniARM is computed as:\"\n * (Less serious) Line#429: \"GenARM (Xu et al., 2025));\" --> \"GenARM (Xu et al., 2025);\"\n * (More serious) Equation (5): There is a missing right parenthesis for the difference of the two log probabilities.\n * (More serious) also Equation (5): If I am not mistaken, either $(-1)^{z_i}$ needs to be negated as $-(-1)^{z_i}$ or the meaning of $z_i$ needs be flipped."}, "questions": {"value": "see **Weakness** where I have included the questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FtoJEoP2KT", "forum": "bsEX1h0og2", "replyto": "bsEX1h0og2", "signatures": ["ICLR.cc/2026/Conference/Submission2534/Reviewer_SHwo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2534/Reviewer_SHwo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969712547, "cdate": 1761969712547, "tmdate": 1762916270611, "mdate": 1762916270611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}