{"id": "iedlZOdI0d", "number": 4840, "cdate": 1757775202320, "mdate": 1763615429520, "content": {"title": "Calibrated Information Bottleneck for Trusted Multi-modal Clustering", "abstract": "Information Bottleneck (IB) Theory is renowned for its ability to learn simple, compact, and effective data representations. In multi-modal clustering, IB theory effectively eliminates interfering redundancy and noise from multi-modal data, while maximally preserving the discriminative information. Existing IB-based multi-modal clustering methods suffer from low-quality pseudo-labels and over-reliance on \naccurate Mutual Information (MI) estimation, which is known to be challenging. Moreover, unreliable or noisy pseudo-labels may lead to an overconfident clustering outcome. To address these challenges, this paper proposes a novel CaLibrated Information Bottleneck (CLIB) framework designed to learn a clustering that is both accurate and trustworthy. We build a parallel multi-head network architecture—incorporating one primary cluster head and several modality-specific calibration heads—which achieves three key goals: namely, \ncalibrating for the distortions introduced by biased MI estimation thus improving the stability of IB, constructing reliable target variables for IB from multiple modalities and producing a trustworthy clustering result. Notably, we design a dynamic pseudo-label selection strategy based on information redundancy theory to extract high-quality pseudo-labels, thereby enhancing training stability. Experimental results demonstrate that our model not only achieves state-of-the-art clustering accuracy on multiple benchmark datasets but also exhibits excellent performance on the expected calibration error metric.", "tldr": "This paper proposes a novel CaLibrated Information Bottleneck (CLIB) framework for trusted multi-modal clustering.", "keywords": ["Multi-modal Clustering", "Information Bottleneck"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c84e2216db70b5945d299f397c562b6690d9ce0a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes CLIB, a calibrated information bottleneck framework for multi-modal clustering. It introduces a multi-head architecture with dedicated calibration heads to mitigate the impact of noisy pseudo-labels and biased mutual information estimation—key issues in existing IB-based methods. A dynamic label selection strategy further improves training stability. Experiments show CLIB achieves state-of-the-art clustering accuracy and superior calibration on multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-Well-motivated problem: The focus on improving trustworthiness and calibration in multi-modal clustering is timely and important.\n-Innovative architecture: The multi-head design with dedicated calibration heads is a clever way to decouple representation learning from label refinement."}, "weaknesses": {"value": "-Limited discussion on calibration in clustering: While ECE is adapted for clustering, the paper does not fully address the challenge of defining \"correctness\" without ground-truth labels during calibration evaluation.\n-Lack of computational analysis: No comparison of training time or model complexity is provided, making it hard to assess practical efficiency.\n-Hyperparameter sensitivity: The balance between IB objectives and calibration is controlled by hyperparameters; their robustness is not thoroughly analyzed."}, "questions": {"value": "The method relies on pseudo-labels for both clustering and calibration, yet these labels are inherently noisy and evolve during training. How does the proposed calibration mechanism avoid reinforcing or amplifying incorrect pseudo-labels in the early or unstable stages of training? A brief analysis or design justification on the robustness of calibration to label noise would significantly strengthen the paper's claim of producing \"trusted\" clustering."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LHjOZ9ikH4", "forum": "iedlZOdI0d", "replyto": "iedlZOdI0d", "signatures": ["ICLR.cc/2026/Conference/Submission4840/Reviewer_omRQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4840/Reviewer_omRQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761616983392, "cdate": 1761616983392, "tmdate": 1762917604802, "mdate": 1762917604802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Information bottleneck theory effectively removes redundancy or noise in multi-modal clustering while preserving discriminative information, but existing IB-based methods face challenges such as low-quality pseudo-labels, over-reliance on accurate mutual information estimation and unreliable clustering outcomes. This work proposes a calibrated information bottleneck framework featured a parallel multi-head network to reach three typical goals, calibrating biased MI estimation to enhance IB stability, building reliable IB targets from multi-modal data samples and getting trusted results. A dynamic pseudo-label selection strategy grounded in information redundancy theory improves training stability by filtering high-confidence labels. Experiments show the method achieves promising results across many benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The proposed CLIB framework is innovative. Its parallel multi-head architecture successfully decouples the calibration objective from the final clustering objective.\n\n2.Theorem 1 given in the paper is insightful. It theoretically connects the difficult problem of MI estimation bias with the pseudo-label screening mechanism via a clear logical chain.\n\n3.The experimental section is solid. On five widely-used benchmark datasets, CLIB achieves state-of-the-art performance on all three metrics. Particularly, the significant reduction in ECE robustly demonstrates CLIB's effectiveness in mitigating model overconfidence."}, "weaknesses": {"value": "1.One of the core motivations of the paper is that MI estimation is difficult and biased. However, in the actual implementation, different parts of the model use three different MI estimation strategies. The authors do not explicitly explain the rationale for choosing this.\n\n2.The implementation environments of the code are not given in the experiments, which may influence the reproductivity. A complete experimental details is usually needed in the experimental setup subsection."}, "questions": {"value": "1. In the adaptive fusion, could you provide the specific weights w_m learned during the experiments? This would help verify whether the model is truly adapting the importance of different modalities.\n\n2. The ablation study for L_con shows a potential trade-off between ACC and ECE.  In a practical application, if one must prioritize one over the other, how would you advise them to adjust the model?\n\n3. Is this the first work addressing the trusted multi-modal clustering problem with calibration? If yes, please explicitly describe it in the paper. If not, please deeply discuss the differences with the related works."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "G5Ue8P5Otn", "forum": "iedlZOdI0d", "replyto": "iedlZOdI0d", "signatures": ["ICLR.cc/2026/Conference/Submission4840/Reviewer_ofuD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4840/Reviewer_ofuD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846560766, "cdate": 1761846560766, "tmdate": 1762917604555, "mdate": 1762917604555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an information bottleneck-based multi-modal clustering method, which addresses the issue of overconfident clustering results caused by low-quality or unreliable pseudo-labels in multi-modal clustering. The parallel multi-head architecture is proposed to correct distortions in mutual information estimation. Experiments demonstrate that the proposed method achieves superior performance compared with existing baselines across multiple datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work addresses the reliance on reliable target variables and the overconfidence caused by noisy pseudo-labels, which is a commonly encountered problem in clustering.\n2. The proposed dynamic pseudo-label screening strategy based on information redundancy offers a promising alternative to existing probability-based thresholding.\n3. The proposed framework takes the inherent difficulty of precise MI estimation into consideration, alleviating the negative impacts of such estimation biases."}, "weaknesses": {"value": "1. The proposed method consists of M+1 heads, a two-stage training process, and multiple loss terms, which may increase computational overhead compared to baseline methods. The authors are encouraged to discuss the model’s computational complexity to better assess its practical applicability.\n2. In the current implementation, gradients are blocked from the calibration heads while allowing backpropagation from the cluster head to the IB to avoid contradictory objectives. It would be helpful if the authors clarify what specific contradictions were observed and whether the fusion-based cluster head’s objective is consistently better aligned with the IB optimization than the single-modality calibration heads.\n3. The pseudo-label screening excludes high-entropy, uncertain samples during training. It would be interesting to investigate the final clustering performance on these “difficult” samples after convergence. Would they yield much better performance than before?\n4. There are some other clustering methods (such as Twin Contrastive Learning for Online Clustering, IJCV 2022) that also use pseudo labels to boost clustering performance. The authors could clarify the differences between this work and previous ones."}, "questions": {"value": "I expect the authors to address my concerns in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wzkMwaodGo", "forum": "iedlZOdI0d", "replyto": "iedlZOdI0d", "signatures": ["ICLR.cc/2026/Conference/Submission4840/Reviewer_VCs7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4840/Reviewer_VCs7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900935126, "cdate": 1761900935126, "tmdate": 1762917603661, "mdate": 1762917603661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "By seeing the challenging problems faced by existing information bottleneck-based multi-modal clustering methods, in this work a novel calibrated information bottleneck is proposed for trusted multi-modal clustering to learn more accurate and trustworthy clustering outcome. It mainly presents a parallel multi-head network architecture containing clustering and calibration heads for outputting high-quality data assignments. Lots of different kinds of experiments have illustrated the superiority of the method on multiple benchmark datasets with metrics of clustering accuracy and calibration error."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper is well-written with clear logic. The flow from problem introduction and method description to experimental analysis is fluent. Figure 1 provides a clear and understandable illustration of the whole framework, helping readers quickly grasp the model's workflow.\n\n2) The control over the gradient flow is very fine-grained, it blocks the gradient from the calibration heads but allows it to back-propagate from the main cluster head. This design ensures that the feature learning of IB serves the final clustering task while avoiding potentially contradictory signals from the calibration objectives of different modalities.\n\n3) The parameter sensitivity analysis shows that the model maintains stable performance over a wide range of choices for hyper-parameters. This indicates that the method does not require excessive tuning and possesses good practical value."}, "weaknesses": {"value": "1)  Eq. 3 of the paper relies on the NT-Xent loss. The effectiveness of NT-Xent is highly dependent on the Batch Size (N), as its MI lower bound is log(N)-L_(NT-Xent). If N is too small, this bound becomes loose, leading to poor alignment. The paper does not mention the Batch Size used in the experiments or its impact.\n\n2) The paper does not detail the specific network architectures used for feature extraction and the various heads, where they are also important in reproducing the results for readers.\n\n3) The paper relies entirely on quantitative metrics. For a clustering task, providing t-SNE visualizations of the feature space would be highly persuasive. A comparison of the feature distributions after the warm-up and after calibration could visually demonstrate how the framework improves inter-class separation and intra-class compactness."}, "questions": {"value": "1) The method requires the number of clusters, C, to be specified in advance. How sensitive is the model to the choice of C? If C is set incorrectly, how much are the model's performance and calibration affected?\n\n2) Could you provide the specific network architectures for the Backbones and the CalHead/CluHead? For instance, what kind of encoders were used for the image and text modalities, respectively?\n\n3) What are the limitations that the proposed method still exist in calibrating the multi-modal clustering results? Could you provide some future insights in this area for readers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no"}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iuFg2FaMei", "forum": "iedlZOdI0d", "replyto": "iedlZOdI0d", "signatures": ["ICLR.cc/2026/Conference/Submission4840/Reviewer_6kV7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4840/Reviewer_6kV7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912020075, "cdate": 1761912020075, "tmdate": 1762917603190, "mdate": 1762917603190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We appreciate the constructive feedback from the reviewers. The revised manuscript has been uploaded, with all revisions marked in blue."}}, "id": "WCAzho8520", "forum": "iedlZOdI0d", "replyto": "iedlZOdI0d", "signatures": ["ICLR.cc/2026/Conference/Submission4840/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4840/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission4840/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763615786876, "cdate": 1763615786876, "tmdate": 1763615786876, "mdate": 1763615786876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}