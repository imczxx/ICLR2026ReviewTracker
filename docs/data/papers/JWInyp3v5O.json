{"id": "JWInyp3v5O", "number": 9179, "cdate": 1758114138586, "mdate": 1763736415969, "content": {"title": "Scalable and Generalizable Autonomous Driving Scene Synthesis", "abstract": "Generative modeling has shown remarkable success in vision and language, inspiring research on synthesizing driving scenes. \nExisting multi-view synthesis approaches typically operate in image latent spaces with cross-attention to enforce spatial consistency, but they are tightly bound to camera configurations, which limits model generalization.\nWe propose BEV-VAE, a variational autoencoder that learns a unified Bird’s-Eye-View (BEV) representation from multi-view images, enabling encoding from arbitrary camera layouts and decoding to any desired viewpoint. \nThrough multi-view image reconstruction and novel view synthesis, we show that BEV-VAE effectively fuses multi-view information and accurately models spatial structure. \nThis capability allows it to generalize across camera configurations and facilitates scalable training on diverse datasets.\nWithin the latent space of BEV-VAE, a Diffusion Transformer (DiT) generates BEV representations conditioned on 3D object layouts, enabling multi-view image synthesis with enhanced spatial consistency on nuScenes and achieving the first complete seven-view synthesis on AV2.\nCompared with training generative models in image latent spaces, BEV-VAE achieves superior computational efficiency.\nFinally, synthesized imagery significantly improves the perception performance of BEVFormer, highlighting the utility of generalizable scene synthesis for autonomous driving.", "tldr": "We introduce BEV-VAE, a variational autoencoder that unifies multi-view images into a BEV representation for scalable and generalizable autonomous driving scene synthesis.", "keywords": ["autonomous driving", "multi-view synthesis", "BEV representation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/91cf769f2966e349f0207043f11ac8391b085ea1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on multi-view generation in driving scenes. Previous works use image-level latent representations, relying on cross-view attention to maintain cross-view consistency. This work proposes encoding multi-view images into a unified and compact BEV-latent. This explicit latent representation directly guarantees cross-view consistency. The proposed method can be trained across datasets (with different camera layouts) and demonstrates strong cross-dataset generalization capability and high image quality."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The motivation and idea of this work are solid. The BEV latent representation not only explicitly ensures cross-view consistency, as the paper emphasizes, but I also guess it can largely mitigate the subjectivity issues of generative models (For example, the consistency and move/changes of the 3D content are reasonable only within the camera view). The authors could consider validating this point.\nThe designs of the BEV latent encoder, decoder, discriminator, and training pipeline are reasonable and well-founded. The writing is clear.\nExperiments are extensive and solid. The model's capability for cross-dataset training and its few-shot generalization ability are impressive. Visualization results show that the model achieves high accuracy in reconstructing views under the highly compressed BEV latent representation."}, "weaknesses": {"value": "Recommend defining F_stt in Section 3.1.2.\n\nThe title \"Scalable and Generalizable Autonomous Driving Scene Synthesis\" doesn't fully capture the paper's key feature (BEV latent representation / multi-view synthesis). The experiments primarily demonstrate the method's cross-dataset training capability (which is good) rather than its scalability. Consider adjusting the title to make it more distinctive？"}, "questions": {"value": "The current method doesn't seem to involve temporal modeling. Will explore it in future work?\n\nThe paper discusses the proposed method's relatively lower FID scores (which, given the difficulty of latent representation in BEV space compared to image-level representation, I find understandable). Could increasing the BEV spatial resolution / the number of CFV sampled rays improve the view resolution/realism?\n\nAre there any plans to open-source the code?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EEIL8K5o0R", "forum": "JWInyp3v5O", "replyto": "JWInyp3v5O", "signatures": ["ICLR.cc/2026/Conference/Submission9179/Reviewer_g4oi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9179/Reviewer_g4oi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760615883348, "cdate": 1760615883348, "tmdate": 1762920857194, "mdate": 1762920857194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overall Author Rebuttal"}, "comment": {"value": "We sincerely thank all reviewers for their valuable time and insightful feedback. We appreciate the reviewers’ positive recognition of the motivation, generalizability, and practical value of BEV-VAE. In this global response, we address the main concerns and summarize our key revisions and additional experiments.\n\n**(1) About the title.** Following suggestions from reviewers **K9ii** and **g4oi**, we revise the title to: **BEV-VAE: A Unified BEV Representation for Generalizable Driving Scene Synthesis** to emphasize the unified BEV representation and the generalizability across different camera viewpoints.\n\n**(2) About the resolution.** We adopt a 256×256 resolution to follow prior VAE designs (SD-VAE, ViT-VQGAN) with 8× downsampling (reviewer **jeYw**). We agree that resolution is especially important for autonomous driving (reviewer **K9ii**), as higher resolutions enable finer-grained spatial modeling. BEV-VAE follows the ViT-VQGAN design: the encoder maps a 256×256 image to a 32×32 patch grid using a single convolution (patch size 8), and the decoder upsamples back using a single transposed-convolution. When scaling to 512×512, the encoder patch size increases from 8 to 16, but a single 16× upsampling in the decoder is suboptimal. We therefore replace the decoder head with a lightweight U-Net–style multi-stage upsampling module (denoted BEV-VAE*). Following reviewer **g4oi**, we also evaluate higher-resolution BEV settings (160×160). Quantitative comparisons are shown below:\n|Model|Training|Image|BEV|PSNR $\\uparrow$|SSIM $\\uparrow$|MVSC $\\uparrow$|rFID $\\downarrow$|$\\mathcal{L}_{KL}$|$\\mathcal{L}_{2}$|$\\mathcal{L}_{\\text{perceptual}}$|$\\mathcal{L}_{A}$|$\\mathcal{L}_{D}$|\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|BEV-VAE|PAS|256x256|128x128|28.88|0.8028|0.9756|4.74|2.82e4|0.031|0.15|0.942|0.331|\n|BEV-VAE|nuScenes|256x256|128x128|26.13|0.7231|0.9250|6.66|2.4e4|0.057|0.216|0.361|0.879|\n|BEV-VAE*|nuScenes|512x512|128x128|25.71|0.6727|0.7729|20.54|1.95e4|0.063|0.33|3.713|0.017|\n|BEV-VAE*|nuScenes|512x512|160x160|25.73|0.6733|0.7823|20.99|1.93e4|0.063|0.33|4.168|0.03|\n\n- **512x512 vs. 256x256.** Increasing the resolution to 512×512 degrades performance: SNR drops slightly while SSIM declines more, indicating numerically close but structurally weaker reconstructions. MVSC decreases sharply, as higher resolution introduces richer view-specific details that make cross-view consistency harder. rFID rises significantly, suggesting that high-resolution spatial modeling is much more difficult than a naive 2× resolution scale-up.\n- **Loss analysis.** Following reviewer **jeYw**, we analyze validation losses. At 512×512, the discriminator loss drops markedly, indicating an imbalance toward the discriminator. Stronger generators (e.g., BEV 160×160) are required to restore training balance. Notably, the L2 loss remains nearly unchanged, consistent with the small PSNR drop, while the perceptual loss increases sharply, aligning with the degradation in SSIM and the rise in rFID.\n- **Higher BEV resolution.** As expected by reviewer **g4oi**, increasing the BEV resolution improves MVSC, indicating better multi-view fusion and spatial consistency. rFID improves more slowly due to increased model capacity and training complexity. \n\n**(3) About the memory consumption and inference latency.** We thank reviewers **K9ii** and **jeYw** for their insightful comments. We previously overlooked the efficiency advantages of compact BEV representations over image-space methods. We benchmark GPU memory usage of BEV-VAE w/ DiT and MagicDrive on an A800 across batch sizes: at batch size 4, MagicDrive nearly exhausts 80 GB, while BEV-VAE w/ DiT scales to batch size 32. We also evaluate inference on an RTX 3090 with 20-step DDIM and observe a 4× speedup over MagicDrive, even without BF16 or FlashAttention.\n\n|Model|1|2|4|8|16|32|\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|MagicDrive|26.4 GB |42.1 GB|73.5 GB|OOM|OOM|OOM|\n|BEV-VAE w/ DiT|9.5 GB|11.8 GB|16.2 GB|25.2 GB|43.1 GB|79.2 GB|\n\n|Model|BF16|FlashAttn|Latency (s)|FPS| \n|:-:|:-:|:-:|:-:|:-:|\n|MagicDrive|False|False|5.381|0.19|\n|BEV-VAE w/ DiT|False|False|1.16|0.86| \n|BEV-VAE w/ DiT|False|True|1.123|0.89|\n|BEV-VAE w/ DiT|True|False|0.93|1.08| \n|BEV-VAE w/ DiT|True|True|0.872|1.15| \n\n**(4)About the temporal modeling.** We fully agree that temporal modeling is critical for autonomous driving. BEV-VAE focuses on spatial modeling in static scenes, aiming to learn a unified BEV representation across camera configurations. This enables us to separate spatial and temporal factors, providing a clean foundation for building autonomous driving world models. While temporal consistency is important, our long-term goal is to model the behaviors, interactions, and causal dynamics of traffic participants. We view a dedicated temporal module built on top of the BEV-VAE as a key direction for our future work."}}, "id": "h6K2iN6DGx", "forum": "JWInyp3v5O", "replyto": "JWInyp3v5O", "signatures": ["ICLR.cc/2026/Conference/Submission9179/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9179/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9179/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763736778818, "cdate": 1763736778818, "tmdate": 1763737909041, "mdate": 1763737909041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on autonomous driving scene synthesis and presents BEV-VAE, a variational autoencoder that unifies multi-view driving images into a compact bird’s-eye-view (BEV) latent representation, allowing flexible encoding and decoding across arbitrary camera configurations. By incorporating a Diffusion Transformer conditioned on 3D object layouts, the method achieves spatially consistent and generalizable multi-view synthesis on nuScenes and AV2. The synthesized data further enhances BEVFormer’s perception performance, highlighting the value of scalable and generalizable scene synthesis from a training data perspective."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a clear motivation and is generally well written.\n2. In the autonomous driving domain, due to the inherent need for multi-view perception, a BEV-based VAE offers greater practical value than image-space VAEs.\n3. Using BEV representations makes it easier to transform different camera layouts, and simulate training data for different vehicle configurations."}, "weaknesses": {"value": "1. The paper does not clearly articulate the advantages of BEV-VAE over Image-VAE. In terms of generation quality, both rFID and gFID are inferior to those of Image-VAE. Moreover, recent image-based multi-view generation methods also achieve strong spatial consistency. The potential benefits of BEV-VAE, in my view, may lie in two aspects—information compression and better compatibility with 3D editing—but the paper does not appear to emphasize either of these points. The results in Table 6 also raise questions — if the primary application value lies in train data generation, the improvement in detection performance appears comparable to that achieved by BEVGen, making it difficult to identify a clear advantage of BEV-VAE in this aspect.\n2. The technical novelty of the paper is weak, as the BEV-VAE architecture largely follows that of BEVFormer. The use of BEV representations is also quite similar to BEVWorld, yet the paper lacks a detailed discussion of their differences. In addition, the rendering process to images resembles existing approaches such as self-occ. It would be beneficial for the authors to more clearly articulate the technical innovations, as the method section currently appears to primarily combine components from prior works."}, "questions": {"value": "1. From Table 3, doesn’t the comparison with SD-VAE suggest that BEV-VAE has weaker zero-shot generalization ability than image-based latent representations?\n2. Why does BEV-VAE use only 256×256 image resolution? Would scaling up the resolution introduce any potential issues or challenges?\n3. How much improvement in generation quality does using DiT with BEV-VAE provide compared to using BEV-VAE alone?\n4. During the model training process, which modules, if any, use pre-trained parameters, and which are trained entirely from scratch?\n5. GAN losses are usually sensitive to hyperparameter settings. Could the authors comment on potential issues regarding hyperparameter sensitivity and training stability in their setup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KWYREgVltG", "forum": "JWInyp3v5O", "replyto": "JWInyp3v5O", "signatures": ["ICLR.cc/2026/Conference/Submission9179/Reviewer_jeYw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9179/Reviewer_jeYw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761034759519, "cdate": 1761034759519, "tmdate": 1762920856751, "mdate": 1762920856751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BEV-VAE, a novel variational autoencoder framework designed for autonomous driving scene synthesis. The core contribution is a model that unifies multi-view images into a compact and camera-agnostic Bird's-Eye-View (BEV) latent representation. This approach decouples the scene's 3D structure and semantics from the specific camera configurations, enabling the model to be trained on diverse datasets with varying camera layouts and to generalize to arbitrary new viewpoints. For generative tasks, a Diffusion Transformer (DiT) is trained within the learned BEV latent space, conditioned on 3D object layouts represented as occupancy grids. The authors demonstrate the model's effectiveness through multi-view reconstruction, novel view synthesis, and cross-dataset generalization. While the proposed method achieves state-of-the-art multi-view spatial consistency, it shows a trade-off in per-image generative fidelity (gFID) compared to prior work. Finally, the practical utility of the synthesized data is validated by improving the performance of a downstream perception model, BEVFormer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Generalization:** The paper provides compelling evidence for the model's ability to generalize across different datasets (nuScenes, AV2, nuPlan) and camera setups. The experiments showing successful reconstruction of scenes from one dataset (e.g., nuPlan) using the camera intrinsics and extrinsics of another (e.g., AV2) are particularly impressive and strongly support the claims of generalizability. The demonstrated performance gains from training on a large, mixed dataset (PAS) validate the model's scalability.\n2. **Superior Multi-View Spatial Consistency (MVSC):** The model achieves a state-of-the-art MVSC score. This is a crucial metric for autonomous driving applications, where maintaining the correct 3D geometry and spatial relationships between objects across views is often more important than perfect photorealism. The architectural design, which generates all views from a single, coherent 3D representation, naturally leads to this strength.\n3. **Demonstrated Downstream Task Improvement:** The experiment in Section 4.8, showing that data augmentation using the proposed method improves the NDS score of BEVFormer, is a very strong point. It demonstrates that the synthesized data is not just visually plausible but also practically useful for training and improving perception models, closing the loop between generation and perception."}, "weaknesses": {"value": "1. **Lower Generative Fidelity (gFID):** The most apparent weakness, which the authors acknowledge, is the relatively high (worse) gFID score compared to state-of-the-art methods like MagicDrive and DriveWM. While the paper frames this as a trade-off for better spatial consistency, the gap is substantial (20.7 vs. ~13-16). This indicates that the generated images may lack the fine-grained texture and realism of other methods, which could limit their utility in certain applications.\n2. **Low Image Resolution:** All experiments are conducted at a 256x256 resolution, which is quite low for modern autonomous driving datasets and applications. While the authors suggest using super-resolution models as a post-processing step, this feels like an external fix rather than an integrated solution. The paper would be stronger if it discussed the challenges and potential architectural changes required to scale BEV-VAE to higher resolutions (e.g., 512x512 or higher).\n3. **Overstated \"Zero-Shot\" Capability:** The term \"zero-shot\" in Section 4.6 seems too strong given the quantitative results in Table 3. The zero-shot performance on WS101 is very poor (PSNR 16.6, rFID 56.7). The real strength demonstrated here is in *fast adaptation* or *efficient fine-tuning*, where the pre-trained model provides a strong prior that allows for rapid convergence on a new dataset. The terminology should be more precise to reflect this.\n4. **Static Scene Limitation:** The current framework operates on static scenes. The real world is dynamic, and the ability to model temporal evolution and generate coherent video sequences is a key direction in this field. While mentioned as future work, this is a significant limitation compared to the broader goals of full-world simulation.\n5. **Mismatched Framing of Contribution and Lack of Efficiency Analysis:** The title \"SCALABLE...SCENE SYNTHESIS\" may be slightly overstated, as the paper's core innovation lies not in the generative model itself—which is a standard Diffusion Transformer—but in the preceding VAE architecture for learning a unified BEV representation. A significant, yet underexplored, benefit of this design is its potential for computational efficiency; by compressing the multi-view scene into a compact latent space, the subsequent diffusion process should be substantially less demanding in terms of memory and latency. To truly validate the \"Scalable\" claim and better frame the work's practical contribution, the paper would be significantly strengthened by a quantitative comparison of GPU memory usage and inference times against other leading methods."}, "questions": {"value": "Regarding the FID/MVSC Trade-off: Could you elaborate on why you believe there is this trade-off? Is the lower FID an inherent consequence of the VAE's information bottleneck regularizing the latent space, potentially smoothing over high-frequency details? Have you experimented with alternative VAE formulations, such as a VQ-VAE, which might allow for sharper reconstructions while maintaining the unified BEV structure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qsbZWmgXje", "forum": "JWInyp3v5O", "replyto": "JWInyp3v5O", "signatures": ["ICLR.cc/2026/Conference/Submission9179/Reviewer_K9ii"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9179/Reviewer_K9ii"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818458517, "cdate": 1761818458517, "tmdate": 1762920856359, "mdate": 1762920856359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper address the problem of novel-view-synthesis (NVS) in driving scene, where the novel view are camera viewpoints around the cameras. \n\nThe author propose to address this problem through a 3D-aware Birds-eye-view VAE. \n\nThis VAE will encode multiple images together to 3D BEV latents, and then decode it back to images.  Afer training this VAE with MSE, perceptual loss and GAN loss over multiple datasets, the author showed that NVS can be down by using different camera extriniscs and intrinsics when decoding.  Which is a very neat idea. \n\n\n\nAlso the author showed that this 3D-aware VAE has relatively OK reconstruction PSNR compared with vanilla image space VAE used by stable diffusion. \n\n\n\nThe author also showed that the proposed method can be used to generate augmented data when trainingperception model: BEVFormer, increasing the performance, which is quite impressive."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea is so interesting.  3D aware VAE can be used for NVS by adjusting the camera parameters used in decoding process. Quite cool!\n2. The evaluation is quite comprehensive (even though used proxy metric for NVS), I understand that the PSNR is lower than those of image based VAE, e.g. SD-VAE. \n3. Using the proposed methods for data-augmentation is also very interesting!"}, "weaknesses": {"value": "I think the major weakness I have is about runtime efficiency. It seems that the deformable attention would be very slow compared with flash-attention style attention implementations.  Can the author provides more details about it?  I understand that it might be slow without a well-optimized flash-attention style kernel."}, "questions": {"value": "1. Is it possible to evaluate NVS without using proxy metrics as done in the paper (using reconstruction metric as proxy metric for NVS)\n2. Will dropping input images during training improves the NVS performance?  Seems that if you drop one image during the encoding stage, but still compute the reconstruction loss on that image, this process will resemble a NVS training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7rT9XC9gaz", "forum": "JWInyp3v5O", "replyto": "JWInyp3v5O", "signatures": ["ICLR.cc/2026/Conference/Submission9179/Reviewer_8x1r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9179/Reviewer_8x1r"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975970504, "cdate": 1761975970504, "tmdate": 1762920856000, "mdate": 1762920856000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}