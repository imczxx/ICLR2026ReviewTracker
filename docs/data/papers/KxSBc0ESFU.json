{"id": "KxSBc0ESFU", "number": 10652, "cdate": 1758178580269, "mdate": 1759897637675, "content": {"title": "UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following", "abstract": "Shaping the behavior of powerful Large Language Models (LLMs) to be both beneficial and safe is the central challenge of modern AI alignment. We posit that the post-training alignment process is fundamentally a unified challenge of Preference Learning, encompassing two distinct modalities: learning from demonstrated preferences (e.g., Supervised Fine-Tuning, SFT) and from comparative preferences (e.g., Reinforcement Learning, RL). The current industry-standard pipeline, which processes these preference types sequentially, is inherently flawed due to a critical distributional mismatch between the static expert data and the dynamic policy. This creates two interconnected problems: (1) Offline SFT trains on a fixed expert distribution, but as the policy's own generation distribution drifts, the learned knowledge becomes brittle and unreliable. (2) Subsequent online RL explores to improve generalization, but it operates without direct access to the rich, ground-truth knowledge within the expert demonstrations, making its exploration inefficient and ungrounded. This fundamental separation prevents the two data sources from synergistically regularizing each other. To resolve this, we first reframe alignment as a constrained optimization problem. We then propose Unified Adversarial Preference Learning (UniAPL), a novel framework that directly operationalizes this theory by dynamically bridging the gap between the policy's distribution and the expert's distribution. The ultimate expression of our framework is a simplified, single-stage unified training objective. This approach cohesively learns from mixed batches of SFT and preference feedback data, allowing the dense expert data to directly ground and regularize the online exploration process in every gradient update. This concurrent optimization inherently mitigates the distributional mismatch and maximizes data synergy.We empirically validate our approach on instruction-following tasks using Qwen3-235B-Instruct-2507 as the expert teacher. Our model demonstrates comparable or superior general capabilities in English, coding, mathematics, and Chinese, while significantly enhancing instruction-following ability; it surpasses the strong GRPO baseline by 5.77\\% on Qwen3-0.6B—matching a 32B model’s performance—and exceeds 3.75\\% on Qwen3-4B, even outperforming the teacher model. Furthermore, analysis of response length and log-probability (logp) distributions shows that models trained with UniAPL not only achieve stronger performance but also generate outputs closely resembling expert demonstrations.", "tldr": "", "keywords": ["Alignment", "Preference learning，Adversarial Learning", "Instruction Following"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3358ea0aa214f58ec2dc092d04c849f419c70406.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a single-stage unified adversarial preference learning framework (UniAPL) for LLM post-training. Unlike traditional multi-stage approaches, UniAPL integrates supervised fine-tuning (SFT) and reinforcement learning (RL) preference data within the same training batches. It further incorporates a distributional regularizer to ensure that each update simultaneously optimizes imitation learning, preference alignment, and consistency with the expert data distribution. The proposed approach demonstrates strong and consistent performance across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a precise, gradient-level formulation that clearly shows how SFT, RL, and distributional regularization interact at each update. This unified view is both conceptually clarifying and practically convenient.\n\nBy reusing POLAR, the method sidesteps the instability and computational cost of training a separate discriminator while still providing a strong distributional learning signal.\n\nSolid performance"}, "weaknesses": {"value": "The POLAR-based discriminator remains fixed, making the setup feel closer to a reference-regularized objective than true co-trained adversarial learning. Clarifying whether is ever updated, and analyzing how this affects robustness under domain shift.\n\nSince the teacher model is not guaranteed to be correct, UniAPL might inherit its biases or mistakes, particularly when adversarial and reward signals diverge. A stress test or controlled experiment could better demonstrate resilience in such cases.\n\nThe evaluation focuses mainly on instruction-following tasks. Including results on safety, harmlessness, or human preference settings would strengthen the broader alignment claims."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RvM2Jrauo9", "forum": "KxSBc0ESFU", "replyto": "KxSBc0ESFU", "signatures": ["ICLR.cc/2026/Conference/Submission10652/Reviewer_WXiZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10652/Reviewer_WXiZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760626798906, "cdate": 1760626798906, "tmdate": 1762921906587, "mdate": 1762921906587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a single-stage unified adversarial preference learning framework (UniAPL) for LLM post-training. Unlike traditional multi-stage approaches, UniAPL integrates supervised fine-tuning (SFT) and reinforcement learning (RL) preference data within the same training batches. It further incorporates a distributional regularizer to ensure that each update simultaneously optimizes imitation learning, preference alignment, and consistency with the expert data distribution. The proposed approach demonstrates strong and consistent performance across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a precise, gradient-level formulation that clearly shows how SFT, RL, and distributional regularization interact at each update. This unified view is both conceptually clarifying and practically convenient.\n\nBy reusing POLAR, the method sidesteps the instability and computational cost of training a separate discriminator while still providing a strong distributional learning signal.\n\nSolid performance"}, "weaknesses": {"value": "The POLAR-based discriminator remains fixed, making the setup feel closer to a reference-regularized objective than true co-trained adversarial learning. Clarifying whether is ever updated, and analyzing how this affects robustness under domain shift.\n\nSince the teacher model is not guaranteed to be correct, UniAPL might inherit its biases or mistakes, particularly when adversarial and reward signals diverge. A stress test or controlled experiment could better demonstrate resilience in such cases.\n\nThe evaluation focuses mainly on instruction-following tasks. Including results on safety, harmlessness, or human preference settings would strengthen the broader alignment claims."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RvM2Jrauo9", "forum": "KxSBc0ESFU", "replyto": "KxSBc0ESFU", "signatures": ["ICLR.cc/2026/Conference/Submission10652/Reviewer_WXiZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10652/Reviewer_WXiZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760626798906, "cdate": 1760626798906, "tmdate": 1763301946208, "mdate": 1763301946208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes UniAPL, a unified adversarial preference learning framework for LLM post-training. Instead of the standard two-stage SFT -> RL pipeline (which the authors argue suffers from a distributional mismatch between expert data and the evolving policy), UniAPL couples demonstrated preferences (SFT) and comparative preferences (RL) in a single-stage objective. Concretely, a discriminator produces an adversarial signal that pulls the student’s outputs toward the teacher/expert distribution during both SFT and RL steps. The unified loss mixes adversarial SFT and adversarial GRPO on the same training run. Empirically, using AutoIF/IFevallike/IFBench with verifiable rewards and Qwen3-235B-Instruct-2507 as the teacher, UniAPL improves instruction-following while maintaining or improving general capabilities"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Recasts alignment as a constrained optimization with an explicit expert-faithfulness constraint and identifies the distributional mismatch from sequential SFT -> RL as the core issue.\n2. The paper spells out the unified loss and decomposes the update into imitation, preference, and global distributional regularization terms.\n3. Strong empirical results on instruction-following with competitive general capability retention; includes staged and unified settings and ablations showing the adversarial term works best as a separate loss (not reward shaping)."}, "weaknesses": {"value": "1. The discriminator is adapted from POLAR and the teacher is Qwen3-235B; robustness to other teachers/discriminators is not evaluated. This matters because the “semantic manifold” you match could bias exploration if the teacher has idiosyncrasies.\n2. The paper reports staged pipelines (SFT -> GRPO, SFT -> A-GRPO, A-SFT -> A-GRPO) but never varies when the switch happens (e.g., early vs. late switch, partial SFT). It's unclear whether the staged pipelines are switched optimally. \n3. Results are presented as end-of-training tables and a few static analyses (e.g., response-length alignment), but there are no training curves for reward/pass-rate, KL, or the adversarial term over time.\n4. Implementation details list learning rates for SFT, GRPO, and mixed training but do not state the number of steps/epochs, early-stopping rules, eval cadence, or total tokens, so it’s unclear how SFT and RL were decided to stop (fixed budget vs. dev-set early stop)."}, "questions": {"value": "1. How do results change with a different teacher (e.g., a smaller or domain-specialized instructor)? Could UniAPL still outperform its teacher on IFEval under such swaps?\n2. Do you observe reduced exploration (e.g., narrower diversity in sampled candidates) when $\\lambda_{adv}$ is higher? Any metrics on entropy or solution diversity during RL?\n3. The unified run simplifies workflow, but what is the wall-clock/compute comparison vs. the best-tuned two-stage baseline at matched final quality? A cost-quality curve would be informative.\n4. What stopping criteria did you use for SFT, RL, and unified training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "txr90EnI4H", "forum": "KxSBc0ESFU", "replyto": "KxSBc0ESFU", "signatures": ["ICLR.cc/2026/Conference/Submission10652/Reviewer_gyGb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10652/Reviewer_gyGb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761180460239, "cdate": 1761180460239, "tmdate": 1762921906061, "mdate": 1762921906061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the fundamental limitations of the standard sequential alignment pipeline (SFT-then-RL). The authors argue that this approach creates a critical distributional mismatch: SFT leads to \"Imitative Brittleness\" by overfitting to static expert data, while subsequent RL suffers from \"Ungrounded Exploration,\" exploring without continuous access to the expert knowledge base."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a conceptualization of the alignment challenge. The identification of \"Imitative Brittleness\" and \"Ungrounded Exploration\" as consequences of the distributional mismatch in sequential training is insightful. The proposed solution—a unified framework based on constrained optimization (Lemma 1)—is theoretically elegant.\n\nThe core mechanism of UniAPL, formalized in the unified gradient, ensures that every parameter update simultaneously incorporates signals for imitation, preference-seeking, and distributional grounding. This concurrent optimization simplifies the alignment workflow and allows SFT data to actively regularize RL exploration."}, "weaknesses": {"value": "The core innovation of UniAPL is the adversarial regularization. However, the implementation deviates significantly from standard adversarial training (like GANs). The authors explicitly \"avoid introducing a discriminator that would be progressively updated\" (L359-360) and instead use a fixed, modified version of POLAR.9 This is a critical limitation. A fixed discriminator may be easily exploited by the evolving policy, or it may provide a static regularization signal that becomes ineffective as the policy distribution shifts. This undermines the claim of creating a \"dynamic bridge\" (L18) and raises questions about the robustness of the adversarial regularization.\n\nThe experimental validation is conducted exclusively on instruction-following tasks that utilize verifiable rewards (a binary reward based on a verification function; L364). This setting (RLVR) represents an idealized scenario with dense, perfect feedback. It is significantly different from general alignment (RLHF/RLAIF), where rewards are learned from noisy human preferences and ground truth is unavailable. The generalization of UniAPL to broader, subjective alignment challenges remains entirely unproven.\n\nThere is an inherent tension between the adversarial loss (enforcing mimicry of the teacher) and the RL objective (exploring to improve upon the base policy). Strong regularization might stifle exploration. The ablation studies (Figs. 5 and 6) confirm this sensitivity, showing significant performance degradation when the adversarial coefficient is set too high. This indicates potential instability and challenges in tuning the framework."}, "questions": {"value": "In addition to the weakness section, I have the below questions:\n\nCan you clearly distinguish UniAPL from existing Adversarial Imitation Learning frameworks like GAIL? What is the specific novelty beyond combining AIL with GRPO in a single stage?\n\nHow does UniAPL balance the constraint of the adversarial loss (mimicking the teacher) with the need for RL exploration? If the preference data suggests a solution that diverges significantly from the teacher's distribution, how does the unified objective handle this conflict?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XrQvEtTIPv", "forum": "KxSBc0ESFU", "replyto": "KxSBc0ESFU", "signatures": ["ICLR.cc/2026/Conference/Submission10652/Reviewer_XNu4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10652/Reviewer_XNu4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761451823462, "cdate": 1761451823462, "tmdate": 1762921905653, "mdate": 1762921905653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new training setup centered around adding an ‘adversarial gradient’ SFT and RL losses to guide the model to remain similar to the expert data distribution. They also propose a unified loss combining GRPO, SFT, and the adversarial gradient to unify post-training stages. Experiments on various evaluations show incorporating the adversarial gradient improves performance over just SFT or GRPO, and in unified settings that perform RL and SFT together."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Unifying SFT and RL is an interesting direction, and the approach of adding a gradient that encourages the student to produce similar response to the teacher is interesting.\n- The results seem to generally suggest that the additional gradient does indeed aid downstream performance without losing general abilities."}, "weaknesses": {"value": "- I found the paper quite unclear to read, in a number of places:\n  - How are the results in Figure 2 achieved? I couldn’t find any mention of it in the text. It would also be useful to see results across all the benchmarks used in the paper, beyond IFEval, as high IFEval could just be a result of overfitting to the benchmark (as shown by [3]).\n  - The authors argue that SFT is a special case of preference learning, but how does the adversarial gradient play a part in the unified view? Doesn’t the SFT loss already encourage the model to match the expert distribution? Lines 243-246 say it is doing some form of ‘global distributional regularisation’, but it would be useful to explain this further. As it is, it feels like an additional term without much extra motivation.\n  - How does the unified view link to the later experiments? As far as I can tell, the main experiment where SFT and DPO are unified instead uses CHORD, prior work on unifying SFT and RL, instead of the proposed loss.\n  - The bolding in Table 2 is sometimes incorrect: the highest average score for 0.6B size models is SFT -> A-GRPO, not A-SFT->A-GRPO.\n  - The proposed method is not always better than baselines: CHORD outperforms A-CHORD on average in table 3, SFT outperforms A-SFT in Table 2 for the 4B model, and A-SFT -> A-GRPO consistently underperforms SFT -> A-GRPO. Additionally, the difference in performance between methods is often under a point on average, and as such it would be useful to have some idea of the statistical significance of these results.\n- Incorporating SFT losses into RL losses itself is not particularly novel, and was proposed by InstructGPT [1]. Additionally, it would be good to compare to other baselines that combine SFT and RL such as LUFFY [2].\n- The experimental setup feels a little odd: the authors only train on instruction following data, but then evaluate on broader evaluations like math and MMLU. Does this not mean that the results are also reflecting how well each method maintains the original model performance, while fitting to the training data? More justification for why a more general RL / SFT dataset is not used would be useful.\n- Very minor, but “adversarial” is consistently misspelled as “adversaril” in the paper.\n\nOverall, I think this paper needs major work to be accepted. Currently, it is written in an unclear manner, and its results do not seem to show that it performs significantly better than baselines.\n\n[1] Ouyang, Long et al. “Training language models to follow instructions with human feedback.” ArXiv abs/2203.02155 (2022).\n\n[2] Yan, Jianhao et al. “Learning to Reason under Off-Policy Guidance.” ArXiv abs/2504.14945 (2025).\n\n[3] Pyatkin, Valentina et al. “Generalizing Verifiable Instruction Following.” ArXiv abs/2507.02833 (2025)."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OYEdEp7Ytf", "forum": "KxSBc0ESFU", "replyto": "KxSBc0ESFU", "signatures": ["ICLR.cc/2026/Conference/Submission10652/Reviewer_TS3p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10652/Reviewer_TS3p"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761588408634, "cdate": 1761588408634, "tmdate": 1762921904582, "mdate": 1762921904582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}