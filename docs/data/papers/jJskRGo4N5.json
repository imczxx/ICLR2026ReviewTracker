{"id": "jJskRGo4N5", "number": 4128, "cdate": 1757607374005, "mdate": 1763726129540, "content": {"title": "Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for LLMs", "abstract": "Large Language Models (LLMs) have achieved success across various domains. However, their applicability tends to degrade when confronted with different types of data inputs, especially for LLMs that have been fine-tuned for specific tasks. Despite its importance, the study of knowledge sharing among domain-specific LLMs—such as those trained for mathematics or code—remains largely underexplored. To address the fragmentation of knowledge across domain-specialized LLMs, we propose a unified parameter integration framework that enables modular composition of expert capabilities. Our method is grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy, which leverages both local functional attribution and global information-theoretic signals to guide selective parameter fusion. By extending this mechanism to the low-rank adaptation layer granularity, we ensure efficient integration with minimal inference overhead. Furthermore, we introduce a domain compatibility scoring mechanism that quantifies inter-expert alignment at the activation level and correlates with downstream task utility. This principled fusion protocol allows the final model to synergize heterogeneous expertise while preserving structural modularity. Extensive evaluations across diverse language and reasoning benchmarks validate the effectiveness of our framework, offering a scalable path toward compositional, domain-adaptive LLMs. Our project is available at https://anonymous.4open.science/r/Graft-8213.", "tldr": "", "keywords": ["Model Fusion"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dd32fc650efe0f863e468c4cdf9c4ecfea3a5c05.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Graft, a dual‑gate parameter fusion method that mixes weights from domain experts using a local channel‑wise gate and a global entropy‑based gate. The authors propose a compatibility score derived from activations helps select expert pairs, and a semantic subsampling pipeline builds representative datasets for fusion. Experiments on Qwen2/phi‑3 LLMs and Qwen2‑VL MLLMs show the effectiveness of Graft."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The dual-gate design is considered and is applicable to full fine-tuning and LoRA by fusing both attention and MLP layers.\n2. This paper porvides experimental results on both LLMs and MLLMs, demonstrating the effectiveness of the proposed method.\n3. The introduced activation‑based compatibility metric is practical and it shows how this metric correlates with improvements across domains."}, "weaknesses": {"value": "1. The use of weight‑entropy is mostly heuristic and lacks theoretical support, and alternative signals such as spectral norms are not compared. This makes it hard to attribute gains to the specific signal design.\n2. The proposed method relies on small datasets for gate learning and compatibility estimation, so calling it \"data‑free\"  or \"training free\" is not accurate.\n3. The experiments are mainly conducted on small LLMs/MLLMs, lacking results on 7B level models. It is unclear if the method scales smoothly to larger models. Also, it is suggested to add missing results of baseline methods in Table 2 by running baselines in the same settings to ensure fair comparison instead of simply refering.\n4. When implement the method on larger models, it would be good to see results and analysis on merging efficiency.\n5. I'm also curious about if it is possible to compute the compatibility score at inference time to enable dynamic gating rather than a static pre‑merge decision."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "StnNMVNVGp", "forum": "jJskRGo4N5", "replyto": "jJskRGo4N5", "signatures": ["ICLR.cc/2026/Conference/Submission4128/Reviewer_eigy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4128/Reviewer_eigy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761530237832, "cdate": 1761530237832, "tmdate": 1762917189434, "mdate": 1762917189434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "I'm not confident enough to provide technical assessment to this paper."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "n/a"}, "weaknesses": {"value": "n/a"}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "2DO2vTI5W5", "forum": "jJskRGo4N5", "replyto": "jJskRGo4N5", "signatures": ["ICLR.cc/2026/Conference/Submission4128/Reviewer_ee3g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4128/Reviewer_ee3g"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878131632, "cdate": 1761878131632, "tmdate": 1762917189229, "mdate": 1762917189229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed the method Graft, aiming to integrate the parameters from multiple fine-tuned models efficiently so that the base model is competitive in the corresponding tasks. \n\nThe framework is a combination of the model fusion and data exploitation methods. In terms of the model fusion, Graft calculates the local and global weights according to the difference between the base model and the graft (target) model at the channel and global levels, and then combines the parameters of the two models with the weighted (as a function of the global and local weights) average. However, not all models are good fits for fusion. Thus, the method proposes to do a dataset compatibility analysis according to the activation pattern of the model w.r.t the dataset. If the model is compatible with the target dataset, it would be eligible for the fusion.\n\nIn terms of data exploitation, the method does a representative subsampling for the samples close to the cluster means."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper explores model fusion across domains, particularly integrating models specialized in mathematics and code—an area that remains largely underexplored.\n\n2. The proposed Graft method demonstrates strong and consistent performance across multiple datasets, often outperforming or matching domain-specific models.\n\n3. By combining local and global adjustments, the method achieves fine-grained control over the fusion process, leading to improved overall performance.\n\n4. Furthermore, Graft incorporates a compatibility analysis mechanism to assess the alignment between the source model and the target datasets, ensuring successful and meaningful model fusion."}, "weaknesses": {"value": "1. Lack of design intuition and theoretical grounding:\nThe proposed methods appear somewhat rough, or at least not well-explained. The paper does not provide sufficient intuition behind the design of the fusion strategy, particularly regarding Eqs. (5), (6), and (12–14). Moreover, the connections to existing model fusion techniques (mathematical formulations) are not established, making these formulations seem unsupported. A more comprehensive discussion of related work and the rationale behind the design choices would greatly improve clarity and credibility.\n\n2. Missing computational complexity analysis:\nAlthough the authors claim that the Graft method is computationally efficient, no comparison or quantitative analysis of its complexity is provided. Including such results would help substantiate the efficiency claims.\n\n3. Absence of ablation and component analysis:\nGiven that the framework involves a multi-stage pipeline with several intermediate components, an ablation study is essential to demonstrate the contribution of each step. For instance, evaluating the effect of the representative data subsampling method would offer valuable insight into the method’s internal dynamics.\n\n4. Unclear parameter selection and lack of sensitivity analysis:\nThe procedure for determining key parameters—such as the value of $k$ in representative subsampling or the threshold used in compatibility analysis—is not explained. Moreover, a sensitivity analysis is missing, leaving readers uncertain about how robust the results are to these hyperparameter choices."}, "questions": {"value": "Please refer to the weaknesses section.\n\nBesides, in the ablation study on gating components (Table 5), the MME with only local is the best, but the authors claim the combination of local and global is leading. Is there a typo in the table?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xSwmsgNUrK", "forum": "jJskRGo4N5", "replyto": "jJskRGo4N5", "signatures": ["ICLR.cc/2026/Conference/Submission4128/Reviewer_akdK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4128/Reviewer_akdK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943685359, "cdate": 1761943685359, "tmdate": 1762917188908, "mdate": 1762917188908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a dual-gated parameter fusion framework named Graft for integrating domain-specialized models. At the local scale, a learnable gate assigns channel-wise fusion weights based on parameter differences; at the global scale, an entropy-based score modulates a single fusion weight to mitigate cross-domain conflicts. The paper also introduces an activation-driven compatibility score to predict whether two experts will fuse well, and a representative data subsampling pipeline to keep costs manageable. Experiments on LLMs/MLLMs show improvements over Task Arithmetic, TIES and DARE across Math, Code, and several multimodal benchmarks, with ablations indicating the dual-gate design outperforms single gates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Addressing domain fragmentation in practice (especially with LoRA experts) is timely and relevant; evidence suggests the framework scales beyond pairwise fusion without severe catastrophic forgetting in the tested settings.\n2. The dual-gate idea—combining channel-wise (local) gating with an entropy-based (global) gate—offers a principled way to balance complementarity vs. interference, going beyond element-wise or sign-based heuristics used in prior merging methods. \n3. The activation-driven compatibility metric is a practical contribution for select-then-fuse, reducing trial-and-error when pairing experts."}, "weaknesses": {"value": "1. While the paper includes multimodal evaluations (MathVista, MMMU, MME) and some multi-domain fusions (adding Finance/Medical adapters), the core LLM story remains Math+Code-centric. Evaluations on more domains are encouraged.\n2. The approach relies on representative data subsampling (embeddings→K-Means→centroids), which somewhat INTERVENES the training stage. IMO, a good merging algorithm shall outperform baselines on any model groups (trained with or without data subsampling). Could the author provide the comparison results without the data subsampling?"}, "questions": {"value": "Pls see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "eajxGLuvK7", "forum": "jJskRGo4N5", "replyto": "jJskRGo4N5", "signatures": ["ICLR.cc/2026/Conference/Submission4128/Reviewer_ShKj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4128/Reviewer_ShKj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979750091, "cdate": 1761979750091, "tmdate": 1762917188646, "mdate": 1762917188646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Graft, a novel parameter fusion framework for integrating multiple domain-specialized Large Language Models (LLMs) or LoRA-adapted models into a unified model without retraining. The method introduces a dual-gate fusion mechanism that combines: 1. **Local weight adjustment**: a channel-wise gating network that quantifies parameter differences to emphasize locally important features; 2. **Global weight adjustment**: an entropy-based signal capturing distributional information content for global parameter alignment. \n\nTo ensure reliable fusion, the authors propose a dataset compatibility analysis based on activation statistics (magnitude, sparsity, variance) and a representative data subsampling approach using K-Means clustering for semantic diversity.  \n\nEmpirically, Graft outperforms several baselines (Task Arithmetic, TIES-Merging, and DARE) across diverse LLMs and multimodal models (e.g., Qwen2, Phi-3, Qwen2-VL), achieving superior results on both domain-specific (Math, Code) and general benchmarks (MMLU, TruthfulQA). The framework scales effectively to multi-domain fusion while maintaining performance stability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Originality**:\n1. The dual-gate mechanism elegantly combines channel-level and entropy-based fusion for adaptive parameter integration.\n2. The activation-based compatibility metric provides a principled criterion for selecting which domain experts to fuse.\n3.  The semantic-aware data subsampling step is an innovative procedural contribution for efficiency and data balance.\n\n**Clarity**:\n1. The paper is clearly written and visually well-organized.\n2. Figures effectively illustrate the pipeline and mechanism (e.g., Fig.1–2), and Algorithm 1 concisely summarizes the method.\n\n**Significance**:\n\n1. Addresses a timely and practical challenge in efficient model merging and domain adaptation for LLMs.\n2. Empirically strong, achieving notable gains (up to $+8$–$10$ points) on specialized benchmarks without degrading general performance.\n3.  Offers a scalable, modular paradigm for compositional LLM construction."}, "weaknesses": {"value": "**Theoretical justification**: The link between entropy and representational richness remains heuristic; additional theoretical or empirical validation would strengthen the argument.\n\n**Efficiency analysis**: The computational cost of training or applying the gating network is not reported; an explicit runtime comparison would improve transparency.\n\n**Baselines**: Some baselines (e.g., DARE, TIES-Merging) may not have been fully optimized for large-scale settings, potentially affecting fairness.\n\n**Interpretability**: The work lacks qualitative visualization of how the dual gates behave across domains or layers.\n\n**Ablation completeness**: While the compatibility metric is correlated with performance, a random or naive pairing control would clarify its contribution."}, "questions": {"value": "1. How sensitive is the global weighting performance to the constants $a$ and $c$ in Eq.~(4)? Could the authors provide an intuition or sensitivity analysis?\n2.  What is the computational overhead of training the gating network $\\phi(\\cdot)$ relative to the base model size?\n3.  In multi-domain fusion (Table~4), performance gains plateau. How does Graft handle conflicts when merging more than four experts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OPovWue46X", "forum": "jJskRGo4N5", "replyto": "jJskRGo4N5", "signatures": ["ICLR.cc/2026/Conference/Submission4128/Reviewer_hcED"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4128/Reviewer_hcED"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762134581980, "cdate": 1762134581980, "tmdate": 1762917188412, "mdate": 1762917188412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Graft, a novel parameter fusion framework for integrating multiple domain-specialized Large Language Models (LLMs) or LoRA-adapted models into a unified model without retraining. The method introduces a dual-gate fusion mechanism that combines: 1. Local weight adjustment: a channel-wise gating network that quantifies parameter differences to emphasize locally important features; 2. Global weight adjustment: an entropy-based signal capturing distributional information content for global parameter alignment. \n\nTo ensure reliable fusion, the authors propose a dataset compatibility analysis based on activation statistics (magnitude, sparsity, variance) and a representative data subsampling approach using K-Means clustering for semantic diversity.  \n\nEmpirically, Graft outperforms several baselines (Task Arithmetic, TIES-Merging, and DARE) across diverse LLMs and multimodal models (e.g., Qwen2, Phi-3, Qwen2-VL), achieving superior results on both domain-specific (Math, Code) and general benchmarks (MMLU, TruthfulQA). The framework scales effectively to multi-domain fusion while maintaining performance stability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality:\n1. The dual-gate mechanism elegantly combines channel-level and entropy-based fusion for adaptive parameter integration.\n2. The activation-based compatibility metric provides a principled criterion for selecting which domain experts to fuse.\n3.  The semantic-aware data subsampling step is an innovative procedural contribution for efficiency and data balance.\n\nClarity:\n1. The paper is clearly written and visually well-organized.\n2. Figures effectively illustrate the pipeline and mechanism (e.g., Fig.1–2), and Algorithm 1 concisely summarizes the method.\n\nSignificance:\n\n1. Addresses a timely and practical challenge in efficient model merging and domain adaptation for LLMs.\n2. Empirically strong, achieving notable gains (up to $+8$–$10$ points) on specialized benchmarks without degrading general performance.\n3.  Offers a scalable, modular paradigm for compositional LLM construction."}, "weaknesses": {"value": "Theoretical justification: The link between entropy and representational richness remains heuristic; additional theoretical or empirical validation would strengthen the argument.\n\nEfficiency analysis: The computational cost of training or applying the gating network is not reported; an explicit runtime comparison would improve transparency.\n\nBaselines: Some baselines (e.g., DARE, TIES-Merging) may not have been fully optimized for large-scale settings, potentially affecting fairness.\n\nInterpretability: The work lacks qualitative visualization of how the dual gates behave across domains or layers.\n\nAblation completeness: While the compatibility metric is correlated with performance, a random or naive pairing control would clarify its contribution."}, "questions": {"value": "1. How sensitive is the global weighting performance to the constants $a$ and $c$ in Eq.~(4)? Could the authors provide an intuition or sensitivity analysis?\n2.  What is the computational overhead of training the gating network $\\phi(\\cdot)$ relative to the base model size?\n3.  In multi-domain fusion (Table~4), performance gains plateau. How does Graft handle conflicts when merging more than four experts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OPovWue46X", "forum": "jJskRGo4N5", "replyto": "jJskRGo4N5", "signatures": ["ICLR.cc/2026/Conference/Submission4128/Reviewer_hcED"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4128/Reviewer_hcED"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762134581980, "cdate": 1762134581980, "tmdate": 1763654036999, "mdate": 1763654036999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear Reviewers, Area Chairs and Program Chairs,\n\nWe sincerely thank you for your time, effort, and the constructive feedback provided during the review process. We are greatly encouraged by the positive reception of our work and the recognition of Graft as a timely solution for efficient model merging.\n\nWe specifically appreciate that the reviewers highlighted the following strengths in our work:\n\n* Novelty and Design: Our dual-gate mechanism was recognized as \"novel\" and \"elegant\" (Reviewer hcED), a \"principled way to balance complementarity vs. interference\" (Reviewer ShKj), and a design that achieves \"fine-grained control\" over the fusion process (Reviewer akdK).\n\n* Practical Contribution: The activation-based Compatibility Analysis was highlighted as a \"principled criterion\" (Reviewer hcED) and a \"practical contribution\" that reduces trial-and-error in expert selection (Reviewer ShKj, Reviewer akdK, Reviewer eigy).\n\n* Empirical Strength: Reviewers acknowledged our \"strong empirical evidence\" (Reviewer hcED), \"consistent performance\" across multiple datasets (Reviewer akdK), and the effectiveness of the method on both LLMs and Multimodal LLMs (Reviewer eigy).\n\nWe have carefully considered the concerns raised regarding theoretical justification, scalability, and efficiency. We have provided detailed point-by-point responses to each reviewer and have updated the manuscript accordingly (revisions are marked in blue).\n\nWe truly appreciate the opportunity to improve our paper based on your insightful comments. We look forward to any further discussion.\n\nBest regards,\n\nSubmission4128 Authors"}}, "id": "NnB0Jo3WeA", "forum": "jJskRGo4N5", "replyto": "jJskRGo4N5", "signatures": ["ICLR.cc/2026/Conference/Submission4128/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4128/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission4128/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763733647395, "cdate": 1763733647395, "tmdate": 1763733647395, "mdate": 1763733647395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}