{"id": "5upNiIb6UQ", "number": 7548, "cdate": 1758027074862, "mdate": 1759897847041, "content": {"title": "Relation-Augmented Diffusion for Layout-to-Image Generation", "abstract": "Existing layout-to-image generation methods often struggle in complex scenes with multiple objects, frequently exhibiting issues such as missing objects, positional errors, and semantic inconsistencies. These shortcomings largely stem from a fundamental inability to model inter-object relationships, which limits their capacity to capture spatial and relational cues effectively. To address these challenges, we propose \\textit{Relation-Augmented Diffusion}, a novel framework for layout-to-image generation that explicitly models inter-object relations and implicitly coordinates background-object interactions. We introduce a relation bounding box computation module to spatially encode object interactions, transforming abstract relations into concrete visual representations. These are further embedded into a topological scene graph via a graph convolutional network, enabling bidirectional reasoning between objects and their relations. Additionally, we employ a layout fusion module to harmonize implicit background-object spatial dependencies, which integrates global layout structures with background features to enhance overall scene coherence. Extensive experiments on HICO-DET, COCO-Position, and T2I-CompBench demonstrate that our framework significantly outperforms state-of-the-art methods in generating spatially and semantically consistent images.", "tldr": "", "keywords": ["Layout-to-image", "Image Generation", "Relation-Augmented", "Diffusion"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5713da8db6f4958b69ef40081d659805f4549760.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Relation-Augmented Diffusion for layout-to-image generation. Specifically, they start with a relation box computation module to produce the capture explicit interactions between objects. Then, they adopt a GCN to fuse the information between objects and relations. Then, they deploy a layout fusion module to handle background-object dependencies. Experiments on HICO-DET, COCO-Position, and T2I-CompBench report gains in spatial and semantic consistency over prior L2I methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method improves the controllability and faithfulness issue in text-to-image generation.\n2. The authors commit to releasing the code, which would facilitate reproducibility and benefit future research in relation-aware text-to-image generation."}, "weaknesses": {"value": "1. The main quantitative results report mAP on detection benchmarks, but the proposed model is trained on HICO-DET, whereas most baselines are not. Using a detection model fine-tuned on HICO-DET could bias results toward generations with similar distributions, making comparisons less fair. Moreover, several baselines achieve better FID on COCO-Position, suggesting that overall image quality may not improve. The omission of FID results on HICO-DET further limits the evaluation’s completeness.\n2. Ablation studies also use detection-based metric, even for modules (e.g., Layout Fusion or Background) that primarily affect visual qualities. Evaluating these modules with image-quality metrics (FID, CLIP score, human preference) would be more appropriate to justify their contributions.\n3. The experiments focus on relation detection but do not show whether modeling relations explicitly improves overall text-to-image generation quality. Reporting on broader datasets (e.g., COCO) would help assess general benefits when adding relation-aware mechanism to text-to-image models.\n4. The idea of modeling relationship explicitly has been explored in previous layout-to-image works like LayoutTransformer [1]. This study also suggests that transformers can capture scene-graph relationships more effectively than GCNs, which questions the necessity of the proposed GCN-based design.\n5. How are the relation texts extracted from the general text descriptions? Does the method assume access to explicit relation annotations or texts? It is unclear how relations are extracted from general captions or how the model would function on large-scale, unannotated text-image corpora. This dependence on structured relational data may severely limit scalability and practical applicability.\n\n[1] LayoutTransformer: Scene Layout Generation with Conceptual and Spatial Diversity, CVPR 2021."}, "questions": {"value": "Please see my comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uXZ84mT14j", "forum": "5upNiIb6UQ", "replyto": "5upNiIb6UQ", "signatures": ["ICLR.cc/2026/Conference/Submission7548/Reviewer_tDgN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7548/Reviewer_tDgN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803773927, "cdate": 1761803773927, "tmdate": 1762919644163, "mdate": 1762919644163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Relation-Augmented Diffusion for layout-to-image generation. The key idea is to convert abstract inter-object relations into explicit spatial supervision via relation bounding boxes computed from object pairs. Relations are categorized into six spatial types (e.g., containment, axial overlap) using axis-wise overlap/containment rules; relation regions are then grounded via masks and fused with subject/object semantics using a trainable GCN. A Layout Fusion module injects layout-aware global background guidance to harmonize foreground–background coherence. The method is implemented as an add-on to Stable Diffusion with frozen base weights and trained modules for object cross-attention, relation cross-attention, GCN, Fourier embedders, and layout fusion. Experiments on HICO-DET (HOI), COCO-Position (spatial control), and T2I-CompBench (compositionality) show consistent improvements over recent layout-aware baselines. The renewed version improves rigor by: (1) formalizing the six-type relation classification via axis-wise overlap/containment; (2) adding per-relation analyses and ablations isolating the contributions of Relation Bounding Boxes (RBC), GCN, Layout Fusion (LF), and Fourier embedding; and (3) reporting broader comparisons. However, replication-critical details of the GCN and Layout Fusion internals remain sparse, statistical significance is not reported, and robustness to noisy/incomplete layouts is untested."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear and useful reformulation: relation bounding boxes turn abstract interactions into explicit, trainable spatial supervision; masks reduce attention spillover and ambiguity.\n\n2. Solid empirical gains across three benchmarks, including relation-centric HOI metrics, without degrading image quality.\n\n3. Improved methodological rigor over the previous version: principled six-type spatial classification; per-relation results; module-level ablations.\nBackground–foreground harmonization via layout fusion addresses a common blind spot in L2I methods that treat background as “negative space.”"}, "weaknesses": {"value": "1. Novelty is incremental: core building blocks (GCN, layout fusion, CLIP/Fourier-guided cross-attn) are standard; the main novelty remains the RBC design and integration. Similar ideas exist in scene-graph-guided generation and region/box-conditioned diffusion. The incremental novelty is primarily the concrete, taxonomy-driven RBC mechanism and its tight integration with cross-attention and GCN; diffusion-side innovations are limited.\n\n2. The GCN architecture (layers, hidden dims, aggregator, normalization) and the Layout Fusion module (exact network, feature dimensions, training losses, interfaces) lack sufficient detail for reproduction without code.\n\n3. Statistical rigor: no error bars, CIs, or significance tests; some gains over strong baselines are modest, leaving uncertainty about reliability.\n\n4. Some commercial or recent open models already offer strong spatial/relational control via ControlNet variants or attention guidance where increasing the capability of base model potentially yield better performance under scaling law; the paper’s edge is the explicit relational region grounding and background harmonization, which is valuable but not a paradigm shift.\n\n5.T2I-CompBench spatial-relations are only indirectly reflected via the 3-in-1 metric; standalone spatial relation scores are not reported.\n\n6. Ambiguity resolution for crowded scenes is not fully formalized (tie-breaking when multiple relation types apply, multi-pair overlaps, conflicting masks)."}, "questions": {"value": "Please provide the exact architecture (number of layers, hidden sizes, activation, normalization, edge formulation, aggregator, dropout) and training details (losses, learning rates specific to GCN parameters).\n\nSix-type computation: Beyond diagonal overlap, please provide closed-form formulas (or unambiguous algorithms) for all five remaining types, including edge cases (zero-area intersections, floating-point tolerances).\n\nHow do you select a relation type when multiple classifications are possible (e.g., nearly edge-touching but slight overlap)? What is the policy when one pair participates in multiple distinct relations across time-steps?\n\nHow does performance degrade with synthetic noise in bounding boxes (translation/scale/rotation), occlusions, or missing instances? Can RBC and masks degrade gracefully? Please include quantitative curves.\n\nReport error bars or confidence intervals for key tables; run multiple seeds to establish reliability of improvements.\n\nT2I-CompBench spatial: Provide standalone spatial-relation scores (e.g., UniDet-related metrics) rather than only the 3-in-1 aggregate.\nCompute and efficiency: What is the added latency/memory footprint from relation boxes and GCN under dense layouts? Any scaling issues for O(N^2) relations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5ClmHTxqem", "forum": "5upNiIb6UQ", "replyto": "5upNiIb6UQ", "signatures": ["ICLR.cc/2026/Conference/Submission7548/Reviewer_kw8i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7548/Reviewer_kw8i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899347160, "cdate": 1761899347160, "tmdate": 1762919643813, "mdate": 1762919643813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a relation-augmentation diffusion framework that effectively alleviates common problems such as missing objects and positional errors in complex multi-object generation scenarios by explicitly modeling inter-object relationships and implicitly coordinating the background-object interactions. The paper designs a relation bounding box computation module to transform abstract object relationships into concrete visual representations; utilizes a GCN to construct a topological scene graph, enabling bidirectional reasoning between objects and relationships; and introduces a layout fusion module to integrate global layout structure and background features, enhancing scene coherence. Furthermore, experiments on a large dataset validate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work proposes a fine-grained layout modeling method;\n2. The GCN enables bidirectional reasoning and fosters a tighter integration between layout and semantics;\n3. The work facilitates a more effective collaboration between the layout and the background;\n4. Extensive experimental results demonstrate the effectiveness of this framework."}, "weaknesses": {"value": "1. Whether the complexity of layout relationship modeling increases dramatically with the number of objects. The work does not explicitly handle the scenario of multi-object complex interactions (e.g., more than 5). When the number of objects increases, the number of relationships bounding boxes increases quadratically, which will cause a surge in GCN computation, a decrease in inference speed, and may lead to relationship conflicts;\n2. I'm also curious about the framework's generalizability, such as the types of objects and spatial relationships it supports."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VLVsWsUezc", "forum": "5upNiIb6UQ", "replyto": "5upNiIb6UQ", "signatures": ["ICLR.cc/2026/Conference/Submission7548/Reviewer_seL2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7548/Reviewer_seL2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900307552, "cdate": 1761900307552, "tmdate": 1762919643279, "mdate": 1762919643279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Relation-Augmented Diffusion framework, a new approach designed to significantly improve semantic and spatial consistency in generated images. The framework's key contribution is to treat inter-object relations as independent elements for processing. It addresses both: Explicit relations (between objects) using a novel relation bounding box module. Implicit relations (between objects and the background) using a layout fusion module to ensure the foreground and context are harmonized."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation behind the work is interesting and well-presented.\n\n- The paper is well-written and easy to follow.\n\n- The proposed Relationship Box Computation is technically sound."}, "weaknesses": {"value": "- Missing some important details for the experiments: Was the proposed model and were previous works fine-tuned on HICO-DET? The HICO-DET is an Human-Object Interaction benchmark established at 2017, and it is not often considered in the latest layout-to-image generation works. What's the motivation of using this benchmark? The reviewer is concerned that existing layout-to-image models may not have been fine-tuned on it; if the proposed model is fine-tuned, then the comparison is unfair. More details on how to fairly compare previous works on this benchmark should be clarified.\n\n- The performance on the classical COCO-Position is mixed. The proposed model, finetuned from SD1.5, seems to degrade the generated image quality and the cross-modal semantic matching (CLIP score). The reason behind this should be investigated and carefully discussed. Considering the mixed performance improvement when compared to previous works, the effectiveness of the proposed work cannot be convincing for me.\n\n- Missing performance discussion on COCO-MIG. COCO-MIG features multi-instance generation and is considered in the previous works of layout-to-image generation; it should be an ideal benchmark for the proposed work. Do the authors have performance results on this benchmark?"}, "questions": {"value": "What scientific findings or observations exist that go beyond the self-evident principle that \"adding explicit constraints improves controllability\"? Discussion or analysis along this line, perhaps exploring the optimal granularity of diffusion models, or trade-offs introduced by increasingly explicit label guidance, would bring significantly more depth to this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "afT1pf0Sv9", "forum": "5upNiIb6UQ", "replyto": "5upNiIb6UQ", "signatures": ["ICLR.cc/2026/Conference/Submission7548/Reviewer_kuFx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7548/Reviewer_kuFx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044828962, "cdate": 1762044828962, "tmdate": 1762919642730, "mdate": 1762919642730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}