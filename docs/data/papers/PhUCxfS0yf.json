{"id": "PhUCxfS0yf", "number": 6062, "cdate": 1757951877779, "mdate": 1759897937333, "content": {"title": "When Silence Is Golden: Can LLMs Learn to Abstain in Temporal QA and Beyond?", "abstract": "Large language models (LLMs) rarely admit uncertainty, often producing fluent but misleading answers. This weakness is even evident in temporal question answering (QA), where models frequently ignore time-sensitive evidence and conflate facts across different periods. In this paper, we present the first empirical study of reinforcement learning (RL) at the intersection of abstention and reasoning ability in LLMs for temporal QA. Existing approaches such as calibration might be unreliable in capturing uncertainty in complex reasoning. We instead frame abstention as a teachable skill and introduce a pipeline that couples Chain-of-Thought supervision with RL guided by abstention-aware rewards. Our novel method yields strong empirical gains on reasoning: a model initialized by Qwen2.5-1.5B-Instruct surpasses GPT-4o by 3.46\\% and 5.80\\% in Exact Match on TimeQA-Easy and Hard, respectively. Moreover, it improves the True Positive rate on unanswerable questions by 20\\% over a pure supervised fine-tuned (SFT) variant. In addition, further analysis shows that SFT induces overconfidence and harms reliability, while RL improves overall accuracy but carries similar risks.  Our study provides new insights into how abstention and reasoning can be jointly optimized, offering a foundation for building more dependable language models.", "tldr": "We introduce a new model based on Qwen2.5-1.5B-Instruct that outperforms GPT-4o on selective abstention and reasoning on temporal questions.", "keywords": ["LLM Abstention", "Temporal and non-temporal reasoning", "Question answering"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e784655280e0b923a93e62f8f58bd5ec1395fee.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies abstention-aware temporal question answering with LLMs. It proposes a two-stage pipeline: (i) distill CoT rationales with GPT‑o1 and SFT the policy; (ii) continue training with GRPO using a simple, rule-based abstention-aware reward. The work also explores implicit (time-filtered sub-context, KG snippets) and explicit CoT reasoning signals. On TimeQA, the best model exceeds GPT‑4o with the original context by +3.46 and +5.80 EM, respectively. The paper further analyzes prompt designs, number of KGs, reward variants, and out‑of‑distribution generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Timely focus on abstention + temporal reasoning.\n2. Comparison across input settings (question only, full context, time-filtered sub-context, KGs), model scales, SFT vs RL, and prompt variants sheds insights into the domain.\n3. The experimental setup is detailed nicely for reproduction.\n4. Some interesting analysis is performed, including:\n    4.1. SFT increases overconfidence\n    4.2. Increasing unanswerable questions in training can collapse the model\n    4.3. Impact of KG on abstention"}, "weaknesses": {"value": "1. **Lack of Benchmarks** - Results are confined to TimeQA. Other temporal reasoning sets (e.g., [1,2]) would better validate generality. The OOD experiments (Table 4, p. 9) focus on non‑temporal datasets and show very poor transfer after RL (e.g., TP -> 0 on RL+c), which underscores brittleness.\n\n2. **Heavy reliance on GPT-o1 for CoT** - CoT collection use GPT‑o1. This raises questions about measuring knowledge distillation from larger models, rather than assessing the impact of suggested training and potential subtle leakage or bias from those systems.\n\n3. **Training Methodology isn't novel** - The training stack (CoT‑SFT + GRPO) and rule‑based reward (Eq. 2) are standard training paradigms; the main novelty lies in the selected domain and results rather than new learning algorithms.\n\n4. **Lack of LRMs** - Recent LRMs (eg, o4-mini, Gemini-2.5-Pro, or open-source LRMs -- DeepSeek-R1, Qwen-Thinking) have shown substantial reasoning improvements in temporal domains. Using them will shed more insights into state-of-the-art models and performance.\n\n[1] Uddin, Md Nayem, et al. \"UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs' Memorization.\" arXiv preprint arXiv:2407.03525 (2024).\n[2] Fatemi, Bahare, et al. \"Test of time: A benchmark for evaluating llms on temporal reasoning.\" arXiv preprint arXiv:2406.09170 (2024)."}, "questions": {"value": "See weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XWYC7WiyDE", "forum": "PhUCxfS0yf", "replyto": "PhUCxfS0yf", "signatures": ["ICLR.cc/2026/Conference/Submission6062/Reviewer_aoEZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6062/Reviewer_aoEZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761182472365, "cdate": 1761182472365, "tmdate": 1762918439359, "mdate": 1762918439359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the important question of how to teach large language models (LLMs) the skill of abstention: not answering a question. This work focuses on questions involving a temporal dimension. The authors explore various forms of SFT and RL (using GRPO) to induce abstention. The authors show training a model with chain-of-thought supervised finetuning followed by reinforcement learning can boost abstention. The authors highlight that generalizing outside of the TimeQA benchmark to out of domain benchmarks such as MMLU is still challenging."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors tackle the important open problem of the best approach to teach models the skill of abstention for temporal questions. The authors cover a reasonable set of closed and open models as well as explore various setups for inducing abstention, including various approaches to including context. The authors make reasonable choices in terms of post-training methods (GRPO, SFT) and adapt them for abstention. I commend the authors on the perspective that abstention is a learnable skill and the thorough exploration in post-training approaches to induce it.\n\nI appreciate the authors were careful about evaluation by including both correct abstention and over-abstention in the experiments. I also appreciate the evaluation of both in-domain TimeQA as well as other out-of-domain benchmarks to assess generalization. I also appreciate the authors' proactive inclusion of limitations such as model size.\n\nThe findings are quite interesting and offer a practical recipe for inducing abstention for temporal questions. The finding regarding the importance of data mix (answerable versus unaswerable) is also quite neat! The authors also highlight the important open problem of teaching LLMs the skill of abstention more generally, as well as the limitations of only teaching abstention using SFT."}, "weaknesses": {"value": "The authors offer some nice findings comparing post-training approaches for abstention, including the lack of success of some approaches (SFT). One aspect that could be improved here is some more intuition regarding why some setups work better than others. There is a growing body of literature along the lines of https://arxiv.org/abs/2501.17161 which explains memorization and generalization learning dynamics of post-training approaches that can be used to better contextualize this works' findings.\n\nThe hyperparameters and exact setups used can play a large factor here. The authors' present claims in quite a general manner, not sufficiently accounting for the limited setup used to justify them. For example, broad claims about SFT versus RL are supported only with a single method (GRPO) or limited hyperparameter selection choices for SFT. I'd also be curious to see whether the result in line 397 holds with LoRA, which as been shown to reduce overfitting.\n\nThe experiments regarding generalization are quite interesting. I imagine most common post-training setups will include other data for alignment. How would this skill of abstention interact with the standard post-training pipeline aimed at aligning models? \n\nWhile I believe temporal questions are certainly important, I believe the authors could do a better job setting up why it's worth focusing only on temporal questions. Adding more context about why temporal questions are particularly important or worth focusing on solely would help to better frame the contribution."}, "questions": {"value": "- While temporal questions are certainly important, have the authors considered whether this approach would generalize to other types of unanswerable questions? It's not necessary to run these additional experiments for the scope of this paper, but it would be useful to discuss in the context of future work. \n- How are the hyperparameters in lines 244-246 selected? Is there precedent or justification in prior work or was a sweep conducted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Yml5X3XCb4", "forum": "PhUCxfS0yf", "replyto": "PhUCxfS0yf", "signatures": ["ICLR.cc/2026/Conference/Submission6062/Reviewer_Ght9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6062/Reviewer_Ght9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761619405537, "cdate": 1761619405537, "tmdate": 1762918438739, "mdate": 1762918438739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how RL can teach language models to abstain from answering when uncertain, particularly in temporal question answering. The authors introduce an RL framework with abstention-aware rewards where models are rewarded for saying \"no answer\" on unanswerable questions. Their method outperforms both GPT-4o and SFT baselines, showing higher accuracy and better handling of unanswerable questions. Results are supported by numerous ablations over prompt configurations, hyper-parameter tuning and generalization."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- **Relevance and Importance**: The paper addresses an important and timely problem. Current models struggle to abstain and the proposed RL technique is simple and effective. \n\n- **Strong Results**: The results, although surprising, are strong. RL significantly beats SFT, even for SFT models of larger sizes."}, "weaknesses": {"value": "- **Poor Structure and Presentation**: The paper’s organization lacks coherence. Sections jump between unrelated topics (e.g., implicit reasoning, KG extraction, RL training) without clear motivation or integration into the main story. Figures and experiments are presented out of logical order, reducing readability.\n\n- **Weak Experimental Design**: Some experiments feel arbitrary or poorly motivated. Dataset choices, baselines, and prompt configurations are insufficiently justified, and several results are unintuitive.\n\n- **Limited Applicability**: This approach assumes access to datasets with unanswerable questions, which is generally not applicable. The proposed approach also seems very targeted to TemporalQA, which further limits applicability."}, "questions": {"value": "- **Classifier Baseline:** Can the authors add a classifier baseline that predicts if a question is answerable? This could be combined with any model that always generates an answer, and is a simple post-hoc baseline that can compliment models that struggle to abstain. \n\n- **Existing Work:** There is some prior work [1] training models to abstain using RL on unanswerable questions. Can the authors discuss novelty compared to this (and possibly more) existing works. It is okay if they were concurrent works. \n\n- **SFT Data:** Does the SFT data also include unanswerable questions? If so, how are these distributed? They should ideally have the same proportion of unanswerable questions as the RL training (where data ratio was so critical). \n\n- **SFT performance:** Why is SFT performance so poor relative to RL? The SFT dataset appears to have only 1K examples compared to 20K for RL. Could this explain the gap? Note that I am not surprised by the fact that RL beats SFT, but rather by the margin of defeat. A 1.5B model beating a 8B model this significantly suggests that the SFT pipeline has issues.\n\n- **Frontier Model evaluations:** For frontier model evaluations, what prompts were used? Were models explicitly told they could abstain? I think the best prompt to use for these evaluations is exactly the training prompt in Table 8 (except the Qwen/Alibaba text). \n\n- **Implicit Reasoning:** What is the importance of implicit reasoning, such as temporal reasoning or KG extraction, in the story of this paper? In particular, the best models seem to perform well even without these augmentations—what value do these methods add? If the focus of this paper is RL, then the implicit reasoning methods should be presented as baselines. \n\n- **Generalization**: The OOD generalization tasks differ substantially. In TimeQA, abstention is due to lack of information; in MCQ datasets, abstention reflects model inability. Are these two forms of abstention comparable? Why not evaluate on AbstentionBench, which contains ambiguous/unanswerable tasks and seems more aligned with the paper’s goals?\n\n- **Focus on TemporalQA**: The emphasis on temporal QA is unclear. Why is this chosen as the focus, when the RL framework could generalize to other question-answering tasks which require abstention as well (multi-hop reasoning for example) ? \n\n- **Qwen 2.5-7B Results**: The results of this model on TimeQA-Hard are surprising. EM Accuracy is highest when only given the question. Context seems to reduce performance. Why is this happening?  \n\n- Section 3.1 should clearly specify that some questions are explicitly unanswerable.\n\n- The figures are misordered and disrupt reading flow. For instance, results for Figure 4 precede those for Figure 3, and Figure 5 appears beside unrelated text (Experiment 6.2).\n\n[1]: Song, L., Shi, T., & Zhao, J. (2025). The hallucination tax of reinforcement finetuning. arXiv preprint arXiv:2505.13988."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8UDEq02AKl", "forum": "PhUCxfS0yf", "replyto": "PhUCxfS0yf", "signatures": ["ICLR.cc/2026/Conference/Submission6062/Reviewer_eqVi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6062/Reviewer_eqVi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876655302, "cdate": 1761876655302, "tmdate": 1762918438168, "mdate": 1762918438168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}