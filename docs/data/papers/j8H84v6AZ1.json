{"id": "j8H84v6AZ1", "number": 21523, "cdate": 1758318484630, "mdate": 1759896917704, "content": {"title": "The Limits of Inference Scaling Through Resampling", "abstract": "Recent research has generated hope that inference scaling, such as resampling solutions until they pass verifiers like unit tests, could allow weaker models to match stronger ones. Beyond inference, this approach also enables training reasoning models, where data is curated using rejection sampling against a verifier. However, we show that this approach is fundamentally limited when verifiers are imperfect and have a non-zero probability of producing false positives. Resampling cannot decrease this probability, so it imposes an upper bound to the accuracy of resampling-based inference scaling, regardless of compute budget. Our analysis shows that there is a strong correlation between the model’s single-sample accuracy and its false positive rate on HumanEval and MBPP, whose unit tests have limited coverage. Therefore, no amount of inference scaling of weaker models can enable them to match the single-sample accuracy of a sufficiently strong model. Empirical results show that optimal sampling attempts are often fewer than 10, as the negative utility of false positives outweighs benefits, bending inference scaling curves downward. Finally, false positives may have other undesirable qualities, like poor adherence to coding style conventions.", "tldr": "Resampling with imperfect verifiers has fundamental limits, making it impossible for weaker models to match stronger ones, regardless of compute budget.", "keywords": ["Inference scaling", "Resampling", "Reasoning", "LLMs", "NLP"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8a3f092b08181ee5260ae55d96621b2db22267c5.pdf", "supplementary_material": "/attachment/c42335fd1d2f810336af53727225a0fa9eaaaf7c.zip"}, "replies": [{"content": {"summary": {"value": "The paper examines inference-time “resampling with verifiers” and shows that when verifiers are imperfect (i.e., admit false positives), weaker models cannot sample their way up to a strong model’s single-sample accuracy. On HumanEval+ and MBPP+, many solutions from weaker models that pass standard unit tests fail extended tests, so even with unlimited samples, they remain below a strong model's Pass@1. Incorporating the cost of false positives further implies that the optimal sample count is small. The authors also show that false-positive solutions are lower-quality code beyond correctness (naming, comments, length)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The central message is clearly motivated and explained: imperfect verifiers induce an upper bound on performance gains from resampling, and weaker models cannot match stronger models’ Pass@1 via increased sampling.\n\n2. The extensive experiment demonstrates a few interesting observations: 1) the ceiling effect persists across model families and sampling budgets, 2) the optimal sample count is typically small when false positives incur penalties, and 3) false positive solutions exhibit systematically poorer code properties (naming, comments, verbosity)."}, "weaknesses": {"value": "1. The scope of the paper is coding-centric. The arguments are persuasive for unit-test verifiers, but other imperfect verifiers, such as LM-judges for QA/reasoning, could also be studied. \n\n2. Resampling changes the distribution of candidate solutions; the verifier’s FPR may not be stationary with K (e.g., later samples are weirder/hackier). Do you observe drift in the verifier errors as K grows?\n\n3. A single verifier (tested in the paper) would create a brittle FPR. What is the ceiling under, for instance, majority ensembles of various verifiers? In that case, how does diversity (e.g., correlation of errors) mathematically translate into ceiling relief?"}, "questions": {"value": "1. If the verifier can abstain at a tunable threshold to bound FPR$\\leq \\varepsilon$, how does the ceiling shift? Can you train a “selective verifier” with coverage guarantees and show a different optimal K frontier?\n\n2. The cost of a false positive is treated uniformly in the paper. In practice, costs vary by task and downstream impact. Is it possible to learn task-conditioned costs (e.g., via proxy severity scores) and optimize K per instance?\n\n3. How does sampling configuration (e.g., top-p/temperature) affect the conclusions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5jXLkPnx2t", "forum": "j8H84v6AZ1", "replyto": "j8H84v6AZ1", "signatures": ["ICLR.cc/2026/Conference/Submission21523/Reviewer_Tt4Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21523/Reviewer_Tt4Y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943695152, "cdate": 1761943695152, "tmdate": 1762941817608, "mdate": 1762941817608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that inference-time scaling via repeated sampling + imperfect verifiers (like unit tests) has a hard ceiling. Even if you spend infinite inference compute, weaker models cannot necessarily catch up to stronger models, because false positives (incorrect-but-accepted answers) become the bottleneck. The paper backs this up with theory and experiments on HumanEval+/MBPP+, and also studies utility tradeoffs when false positives are costly, plus code quality effects."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper challenges a widely held assumption (“a weak model can just sample more and pass tests to match a stronger model”) and shows that imperfect verifiers are actually the fundamental bottleneck. This is framed cleanly and feels very relevant right now.\n\n2. Using benchmarks like HumanEval+ and MBPP+, the paper quantifies how weaker models produce many false positives (wrong answers that still pass tests) and shows that even with unlimited resampling they hit a ceiling and still can’t match stronger models.\n\n3. The paper doesn’t stop at benchmark accuracy — it talks about real-world cost of shipping bad code, the risk of contaminating training data with verifier-approved mistakes, and why “just sample more” is not a reliable path to replacing stronger models in production.\n\n4. The figures and tables (e.g. showing the accuracy ceiling line, and how the optimal number of attempts K is actually low when false positives are costly) make the main ideas easy to understand and memorable\n\n5. The experimental ideation in Section 4 is novel. Although the assumptions regarding the cost are simplified, this is inevitable. The experiment provides a practical and insightful analysis of the cost associated with infinite or large-scale sampling."}, "weaknesses": {"value": "Overall, I appreciate the quality of the paper, though I have several points of concern outlined below.\n\n1. Because the verifier’s effectiveness depends on the specific test suite, its accuracy and false-positive behavior may vary across different sets of test cases. The paper would benefit from an analysis of how the results change when the verifier’s quality or coverage is systematically varied.\n\n2. A considerable amount of the introductory section (up to approximately page 4) is spent discussing scaling methods beyond the imperfect verifier. It is somewhat questionable whether this level of detail is essential, given that the paper’s primary scope focuses on tasks involving imperfect verifiers in the coding domain. Such elaborations might be better suited for the appendix."}, "questions": {"value": "Question and suggestion are listed in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d31dc8zDJo", "forum": "j8H84v6AZ1", "replyto": "j8H84v6AZ1", "signatures": ["ICLR.cc/2026/Conference/Submission21523/Reviewer_JXaJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21523/Reviewer_JXaJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992405405, "cdate": 1761992405405, "tmdate": 1762941817222, "mdate": 1762941817222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the limitations of scaling resampling in two coding benchmarks and shows that weaker models are more prone to generating false positives—solutions that pass some unit tests but fail when subjected to more comprehensive testing. The study reveals that, in the coding domain, the effectiveness of resampling is fundamentally constrained by the insufficiency of unit tests, a challenge frequently encountered in real-world applications"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The main finding is compelling: Limited test cases constrain the scalability of resampling, and weaker models experience a larger generalization gap than stronger ones. Notably, most weak models cannot outperform the single-sample accuracy of stronger models even when resampling is scaled up. These results highlight the inherent limitations of resampling in real-world coding scenarios, where test cases are typically limited.\n\nThe analyses are insightful: By introducing the concept of the cost of false positives and evaluating the quality of generated code, the authors illuminate the potential risks associated with applying resampling techniques in practical coding applications.\n\nThe experiments are comprehensive, spanning various model families and two benchmarks."}, "weaknesses": {"value": "- The use of a large table (Table 1) to list numerous inference scaling techniques feels excessive, particularly since this paper only analyzes the limitations of one specific technique. Including such an extensive table for all techniques seems unnecessary and distracts from the actual focus of the study. A similar concern applies to Table 2.\n\n- The authors attempt to extend their findings to broader domains; however, the paper’s analysis is limited to the coding domain. While test cases serve as verifiers in coding, other domains (e.g., mathematics) might utilize reward models, which differ fundamentally. Although both are imperfect verifiers, the distinctions between test cases and reward models are substantial, making broad generalization of the findings from \ncoding to other domains unconvincing\n\nInsufficient justification for main finding:  \n\n- (a) The central claim that “the weaker model cannot match the performance of a single invocation of the stronger model” is presented too absolutely. For instance, Figure 3 shows cases where Llama 3.1 70B outperforms a single invocation of GPT-4o, and Llama 3.1 8B outperforms a single invocation of Llama 3.1 70B, contradicting the claim.\n- (b) In line 246, the authors state they generate at least 50 samples for each model and benchmark. The rationale for the “at least” threshold is unclear. Why is it not standardized? How are the actual sampling counts determined?\n- (c) In lines 261–263, the statement that the finding holds “no matter how big the compute budget for the weaker model” is insufficiently supported. Only experiments with “at least 50 samples” are reported, leaving the maximum compute budget used ambiguous and raising the question of whether a larger compute budget could yield different results."}, "questions": {"value": "See the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8apuvz9NHj", "forum": "j8H84v6AZ1", "replyto": "j8H84v6AZ1", "signatures": ["ICLR.cc/2026/Conference/Submission21523/Reviewer_9qjn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21523/Reviewer_9qjn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762189865995, "cdate": 1762189865995, "tmdate": 1762941817010, "mdate": 1762941817010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a critical analysis of inference-time scaling, specifically the \"resampling\" approach where solutions are generated until one passes a verifier. The authors argue that this technique is **fundamentally limited** when the verifier is imperfect (i.e., has a non-zero false positive rate).\n\nThe core contributions are:\n1.  An empirical demonstration on coding benchmarks (HumanEval+ and MBPP+) that a model's single-sample accuracy (Pass@1) is **strongly correlated with its false positive rate**. Weaker models produce *more* false positives (solutions that pass limited unit tests but fail comprehensive ones) than stronger models.\n2.  This correlation establishes a **hard upper bound** on the performance of resampling. A weaker model, even with an infinite compute budget, cannot match the single-sample accuracy of a stronger model if the stronger model's accuracy is already higher than the weaker model's *conditional* accuracy (its accuracy on solutions that pass the imperfect verifier).\n3.  By introducing a \"cost\" for false positives, the paper shows that the optimal number of sampling attempts (K) is often **finite and very low** (e.g., fewer than 10), as the expected reward curve bends downward when the risk of a false positive outweighs the benefit of finding a true positive.\n4.  A secondary finding that these false positive solutions are not just functionally incorrect but are also of **lower code quality** (e.g., poor adherence to style conventions)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Novel, Important, and Non-Obvious Finding:** The paper's main strength is the empirical discovery of the strong linear correlation between a model's Pass@1 accuracy and its false positive rate. This is not an obvious or trivial result; one might have assumed the \"imperfect verifier\" was an independent challenge. This finding is the engine for all the paper's other conclusions.\n* **Elegant and Sound Methodology:** As mentioned in \"Soundness,\" the use of the `Benchmark` vs. `Benchmark+` test suites is a perfect experimental setup to test the hypothesis.\n* **Strong, Clear-Cut Results:** The data presented is not ambiguous. The linear relationship is strong ($R^2 \\approx 0.89$), and the performance ceiling is clearly visible. The downward-bending reward curves are also unambiguous.\n* **Practicality of the Critique:** The paper addresses a real-world, practical strategy (resampling) and exposes its fundamental limitations in a way that is immediately useful for both researchers and practitioners.\n* **Secondary Quality Analysis:** The analysis in Section 5, showing that false positives are *also* lower-quality code (e.g., worse style), is a strong supporting argument that adds another dimension to the critique."}, "weaknesses": {"value": "* **Static, Domain-Specific Verifier:** The paper's *entire* analysis hinges on a **static, human-written verifier** (the original HumanEval/MBPP unit tests) in the single domain of **coding**. The central claim that resampling is \"fundamentally limited\" is very strong, but the evidence is scoped to this specific setup. The paper's claim of being \"domain-agnostic\" is not well-supported, especially since Table 2 explicitly lists \"Math\" as a domain where \"Oracle\" verifiers (like proof checkers) *do* exist, which would make resampling a perfectly valid strategy in that domain.\n* **Doesn't Account for Scaling Verifiers:** The analysis assumes the *generator* scales (from Llama 7B to GPT-4o) but the *verifier* is fixed. In many modern systems, the verifier is *also* an LLM (e.g., \"LM-as-judge\") or is *generated* by the model itself (e.g., model-generated unit tests). A stronger model is likely also a stronger verifier. The paper's conclusions might not hold in a \"co-scaling\" scenario where the verifier's capability (and thus its false positive rate) improves along with the generator's. The paper mentions this but does not investigate it, which is the most significant limitation of this work.\n* **Ambiguity of \"Cost\":** The analysis of the optimal K in Section 4 is highly dependent on the \"cost-benefit ratio,\" a variable for which the paper provides no empirical grounding. While it's intuitive that a bug (false positive) has a high cost, showing a range from 0 to 8 makes the finding abstract. Without a \"realistic\" C/B ratio, it's difficult to conclude whether the optimal K for a real-world application is 3 or 300.\n* **Significant Task Exclusion:** The appendix reveals that a large number of tasks were excluded from the analysis (e.g., 78 from MBPP+ and 14 from HumanEval+) due to issues with the test harness. The paper asserts this \"did not significantly impact our final results\", but this requires the reviewer to trust this claim. It's plausible that these \"broken\" or ambiguous tasks are precisely the ones that would most effectively probe the limitations of weaker models."}, "questions": {"value": "1.  **Scaling Verifiers:** Your analysis relies on a static, human-written verifier. How do you believe your findings would change in a setting where the verifier also scales? For example, what if you used GPT-4o as an \"LM-as-judge\" to verify the solutions from all models, or used each model to generate its *own* unit tests?\n2.  **Domain-Agnostic Claim:** You claim the results are \"domain-agnostic\", yet your own Table 2 shows that \"oracle\" verifiers are common in mathematics. Can you clarify your claim? Does the \"limit\" you've identified primarily apply to domains *lacking* formal, oracle verifiers (like coding with incomplete tests, or creative writing)?\n3.  **Cost-Benefit Ratio:** Your \"optimal K\" analysis is very sensitive to the C/B ratio. Do you have any suggestions for how one might empirically estimate a \"realistic\" C/B ratio for a typical software engineering task, to move this finding from a theoretical observation to a practical prescription?\n4.  **Task Exclusion:** You excluded 78 tasks from MBPP+ due to test harness issues. Can you provide more detail on why you are confident this did not bias the results? For instance, did you check if the Pass@1 or conditional accuracy *on this subset of tasks* showed a different trend?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2ZxuTHxgzK", "forum": "j8H84v6AZ1", "replyto": "j8H84v6AZ1", "signatures": ["ICLR.cc/2026/Conference/Submission21523/Reviewer_Tash"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21523/Reviewer_Tash"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762449832955, "cdate": 1762449832955, "tmdate": 1762941816739, "mdate": 1762941816739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}