{"id": "WAF8eIYsae", "number": 11861, "cdate": 1758204312409, "mdate": 1759897550203, "content": {"title": "Efficient Thinking via Meta Chain-of-Thought Evaluation", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in handling complex tasks. Recent breakthroughs in Large Reasoning Models (LRMs)—such as OpenAI’s o1 and DeepSeek-R1—have pushed performance even further in System-2 reasoning domains like mathematics and programming. By leveraging supervised fine-tuning (SFT) and reinforcement learning (RL), these models enhance Chain-of-Thought (CoT) reasoning. However, while longer CoT sequences boost accuracy, they also lead to increased computational costs due to verbose and redundant outputs, a challenge termed the \"overthinking phenomenon\". In this paper, we design a novel and efficient framework called Dynamic Verify Stopping in Long Reasoning (DVS-LR) to resolve the issue of overthinking. An early-stop verifier is trained to evaluate the Meta-CoT from multiple dimensions, like completeness, correctness, and self-validation. DVS-LR receives the generating CoT stream and activates the verifier at adaptive checkpoints. If the score received from the verifier reaches a threshold, the current CoT generation is terminated, and the LRMs directly output the final answer without further thinking. Experiments on various math tasks benchmark show that our proposed method achieves 30\\% cut ratio while maintaining original accuracy. Based on the observation of the average length of after-cut length, we propose the ``Budget Forcing Early Stop Majority Voting'' method when the token budget is fixed. Experiments show that this method can improve the accuracy on various benchmarks compared with the original one-chain generation.", "tldr": "", "keywords": ["LLM reasoning", "meta chain-of-thought"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3e87bfb27397d2e2e6ee828460cb4512fcfd1f97.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an efficient thinking mechanism DVS-LR, a framework to improve reasoning efficiency in Large Reasoning Models by introducing an early-stop verifier that stops generation once it assesses the reasoning trace is already sufficient. The verifier is trained using reasoning trees built from full CoT traces, where each partial CoT (separated in steps) is automatically scored by another LLM on six reasoning-quality dimensions. During inference, the verifier monitors the CoT stream and stops generation when a quality threshold is met. A parallel budget-forcing and majority-voting scheme was proposed to further enhance the accuracy. Experiments show comparable accuracy to full CoT reasoning with shorter outputs during generation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is trying to solve a real-world problem with a long generation in reasoning model.\n\n\nThe empirical results show some improvements that under shorter length, it could reach similar accuracy."}, "weaknesses": {"value": "First, the paper is not well-written. There are a bunch of typos or space related issue starting from introduction section (DeepSeek-R1have, (LRMs)such, LLMsoften, (System 1) vs. System2, Thus„ ,linearit,  stepindicating). Please double check the manuscript to make it organized.\n\n\nThe paper proposes to use a verifier (another LLM) for deciding whether to early stop or not. It needs to evaluate on the checkpoints for each generation segments. Do you have any end-to-end efficiency evaluation regarding including the overhead of the verifier?\n\n\nRegarding “Extracting Tree Structure algorithm”. The method for constructing verifier training data is not entirely clear to me. Why not simply evaluate the generated CoT step by step, which would be more closely reflecting the inference time flow of reasoning? Instead, the paper restructures full CoTs into reasoning trees and concatenates/reorders nodes for training. Why is this necessary?\n\n\nThe early stop + parallel inference:  The paper says “Based on our observation of those early stop chain, the average token length of CoT after-cut is about 8K, which means majority problems can be solved within 8K token budget.”. Does the generation really reach the maximal of 32k every time with a normal prompt? If generations do not actually reach the 32K token limit, it is unclear how this approach leads to measurable efficiency gains. Moreover, since the length of each reasoning trace is unpredictable beforehand, it is not obvious how one should determine the number of parallel runs to launch in advance.\n\n\nIn the meantime, it is known that this scaling inference[1] like technique will increase the accuracy, with more overhead that it runs several times, the manuscript is currently lacking such results on how much more overhead is induced.\n\n\n[1] https://huggingface.co/blog/Kseniase/testtimecompute"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4VNCnlI7AF", "forum": "WAF8eIYsae", "replyto": "WAF8eIYsae", "signatures": ["ICLR.cc/2026/Conference/Submission11861/Reviewer_weH1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11861/Reviewer_weH1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868225078, "cdate": 1761868225078, "tmdate": 1762922881392, "mdate": 1762922881392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose Dynamic Verify Stopping in Long Reasoning (DVS-LR), a method to stop reasoning model overthinking. DVS-LR trains a verifier to evaluate the CoT trajectory, and assign score based on its completeness, correctness (soundness), and self-validation to decide if it should early terminate the reasoning trajectory. Experiment shows a promising token saving in mathematical reasoning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Writing. The writing is relatively clear and easy to follow. The illustration helps understand the paper.\n2. Methodology. The methodology is relatively sound and practical."}, "weaknesses": {"value": "1. Novelty. Entropy-based early-exit methods (especially training-free methods) has been quite extensively studied in prior works such as the following. The paper needs to make a clearer distinction between the method and the prior works (in the related work). Just listing a few as addition to the current related work section:\n\n- https://arxiv.org/abs/2502.12067\n- https://arxiv.org/abs/2412.21187\n- https://arxiv.org/abs/2504.01296\n- https://arxiv.org/abs/2508.15260\n- https://arxiv.org/abs/2412.20993\n- https://arxiv.org/abs/2412.18547\n- https://arxiv.org/abs/2207.05221\n\n2. Baseline. The paper need to compare against stronger baselines (perhaps one of the above) to demonstrate the trained method indeed has a better token saving compared to the state of the art, while preserving the accuracy.\n\n3. Interpretability and case study. It is unclear that having interpretability as the reward of reasoning trajectory (as proposed in the paper) will have a benefit in boosting the reasoning quality. The paper will benefit by having case study on specific problem (or listing the the graphical representation of the trajectory) to give reader a better understanding."}, "questions": {"value": "1. Novelty and related work as describe in weakness section.\n2. (The lack of state of the art) Baselines and experiment as shown in weakness section.\n3. Interpretability and case study as describe in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dnfq7Aipu9", "forum": "WAF8eIYsae", "replyto": "WAF8eIYsae", "signatures": ["ICLR.cc/2026/Conference/Submission11861/Reviewer_WYEd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11861/Reviewer_WYEd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931373583, "cdate": 1761931373583, "tmdate": 1762922880471, "mdate": 1762922880471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the overthinking and early exit of large reasoning models (LRMs). The authors propose Dynamic Verify Stopping in Long Reasoning, which applies an early stop verifier trained on a Meta-CoT dataset. During answer generation, the verifier justify the completeness, correctness and existence of verification to decide of the thinking should early exit. DVS-LR further applies a total budget based parallel decoding and majority voting. Experiments show that DVS-LR reduces the output length by 30% without sacrificing accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The Meta-CoT dataset applies LLM-as-a-Judge to improve its scalability.\n- The idea of a global budget for the whole CoT with majority voting is novel."}, "weaknesses": {"value": "- This paper lacks comparison with existing works on early exiting language model reasoning, such as [1][2][3]. It is unclear if using Meta-CoT is a more efficient way.\n- What is the early stop verifier model architecture, as well as the cost and frequency running the verifier model? If running the verifier model itself has a non-negligible cost, does it reduce the gain of early exiting?\n\n[1] Chen, Xingyu, et al. \"Do not think that much for 2+ 3=? on the overthinking of o1-like llms.\"\n[2] Fu, Yichao, et al. \"Reasoning without self-doubt: More efficient chain-of-thought through certainty probing.\"\n[3] Zhang, Anqi, et al. \"Reasoning Models Know When They're Right: Probing Hidden States for Self-Verification.\""}, "questions": {"value": "nits:\n- L042 \"over road\" is supposed to be \"overhead\"?\n- L053 \"those transition step\" should be \"steps\"?\n- L054 \"lead to he 'overthinking'\" missing \"t\"\n- L056 \"when LRMs make sure its solution is correct\". \"its\" should be \"their\". The same happens in several places (e.g. L065 \"the LRMs generates ...\")\n- L137, Missing hyphens (\"System 1\" and \"System2\")\n- L372, \"the DVS-LR method performs more conservative\", should be \"conservatively\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vxqiro0XCW", "forum": "WAF8eIYsae", "replyto": "WAF8eIYsae", "signatures": ["ICLR.cc/2026/Conference/Submission11861/Reviewer_ZVcD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11861/Reviewer_ZVcD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942285480, "cdate": 1761942285480, "tmdate": 1762922879241, "mdate": 1762922879241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposed an early stopping-based method for efficient reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- LRM efficiency is important, and early stopping is a straightforward idea that worth exploring."}, "weaknesses": {"value": "This work is likely too incomprehensive in the evaluation department.\n\n- Lack of model coverage. Seems to only evaluated on one 32B model.\n- Lack of dataset coverage. Only evaluated on math reasoning tasks while trained on s1k (also a math dataset).\n- No comparison to existing baseline methods.\n- No description of basic experiment setting like temperature, number of run, etc.\n\nThere should also be better discussion and recognition of tightly connected work. Doing early stopping on CoT/LRM is a well-explored area with literature like HALT-COT, Answer Consistency, FlashThink, DEER... These work are not discussed nor compared. Also, the idea of using \"wait\" like tokens as separator are also utilized in many reasoning-related work and deserve crediting."}, "questions": {"value": "- Since the proposed method generate 4 chains per question and do majority voting, does the reported length include discarded chains?\n- Which model is used as the verifier?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ynz2rfdLqi", "forum": "WAF8eIYsae", "replyto": "WAF8eIYsae", "signatures": ["ICLR.cc/2026/Conference/Submission11861/Reviewer_4hE2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11861/Reviewer_4hE2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007087640, "cdate": 1762007087640, "tmdate": 1762922878845, "mdate": 1762922878845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}