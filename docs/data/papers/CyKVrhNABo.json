{"id": "CyKVrhNABo", "number": 1784, "cdate": 1756923109904, "mdate": 1759898187094, "content": {"title": "Paper Copilot: Tracking the Evolution of Peer Review in AI Conferences", "abstract": "Submissions are rising fast, and venues use different rules, data formats, and update times. As a result, signals of progress get split across places, and key moments (rebuttal, discussion, final decision) are easy to miss, making analysis hard. We present Paper Copilot, a system and scalable peer-review archive that pulls data from official sites, OpenReview, and opt-in forms into a single, standardized, versioned record with timestamps. This lets us track trends over time and compare venues, institutions, and countries in a consistent way. Using the archive for ICLR 2024/2025, we see larger score changes after rebuttal for higher-tier papers, reviewer agreement that dips during active discussion and tightens by the end, and in 2025 a sharper, mean-score–driven assignment of tiers with lower decision uncertainty than expected at that scale. We also state simple rules for ethics—clear sourcing and consent, privacy protection, and limits on use for closed venues. Together, we provide a clear, reusable base for tracking AI/ML progress, and, with this data, enable validation, benchmarking, and otherwise hard-to-run studies.", "tldr": "", "keywords": ["peer review", "review dynamic", "aiml", "community"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed3d5117ebf4050b20eb78a4fc29656e6edb1cc1.pdf", "supplementary_material": "/attachment/f419bb85a46282ad3a3c8ddbabb9e1c2e3c78355.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces Paper Copilot, an open-source framework for collecting, visualizing, and open-sourcing peer review data from major AI conferences. The paper addresses the lack of transparency in current peer review practices, arguing for a collective and standardized framework to track peer review processes and make them easily accessible to the general public and researchers. Currently, peer review information remains largely closed (except at certain conferences like ICLR) or difficult to access due to variations across fields and the structure of review systems at different conferences. Paper Copilot seeks to consolidate these heterogeneous sources into a single centralized platform. The paper makes this infrastructure public, releases a dataset for research, and presents interesting quantitative results that explain shifts in peer review practices."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- The reviewer finds this paper timely and necessary for the academic community, introducing a valuable open-source asset to researchers working on peer review practices. \n- The reviewer has experienced collecting review data across different venues, and understands the difficulty of collecting a unified review dataset. The difficulty stems from different fields (categories, review criteria) used across venues, and the proposed framework seems to have an independent bot to process these fields for each venue. Utilizing this framework would be of real help to the community if it is well maintained.\n\n- The results and visualizations in Figures 2 and 3 are excellent and easy to understand. Some findings, especially the results for ICLR25, are surprising.\n- Most of the questions that I had at the beginning of reading this paper were well addressed throughout the paper. The paper is easy to follow and limitations were well discussed."}, "weaknesses": {"value": "- My primary concern with this work is the limited depth of discussion on re-identification risks. While the authors acknowledge the possibility of re-identification, several questions remain. For instance, to what extent will the authors employ differential privacy? Additionally, given recent advancements in LLMs, there is a significant possibility that tracking reviews across multiple venues could enable re-identification of reviewers based on their writing styles and fields of interest. I believe these issues and potential mitigation strategies warrant further discussion.\n\n- Section 6.3, \"Potential for Misuse and Dual Use Risks,\" outlines possible measures the authors can take to prevent platform misuse. While I understand these represent the best efforts the authors can make, this remains a weakness that cannot be easily addressed.\n\n- Most of the results are derived from ICLR due to its open-ness, which I do think is a weakness despite being the only option."}, "questions": {"value": "Minors.\n- Check L269. A comma is missing.\n\nQuestions.\n- Is it possible for malicious users to manipulate the survey data? Any user can submit erroneous results to the survey.\n\n- Is Figure 2 reproducible for other venues such as NeurIPS?\n\n- Can you provide a snapshot of how the review dataset looks like? \n\n- Visualization of Fig1 need to be further improved. Better caption illustrating the flow of the system is needed. Also, the text inside the figures are too small."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "This paper relies heavily on web-crawled data, which could raise potential copyright issues. While the authors have thoroughly addressed this concern, I am still flagging potential issues that the reviewer may not have identified."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nyfB6cOj4m", "forum": "CyKVrhNABo", "replyto": "CyKVrhNABo", "signatures": ["ICLR.cc/2026/Conference/Submission1784/Reviewer_Pkuc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1784/Reviewer_Pkuc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760502725826, "cdate": 1760502725826, "tmdate": 1762915889616, "mdate": 1762915889616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Paper Copilot, a pipeline-based system for collecting and analyzing peer-review data across AI/ML conferences. It aggregates multi-source information to enable large-scale statistical analysis of review dynamics, yielding insights such as the 2025 shift toward more score-driven decisions. It is also worth noting that Paper Copilot has already gained significant visibility in the community. Many researchers, myself included, have used or browsed the platform and found its analytics informative and impactful. In essence, the authors have built a system that performs large-scale statistical and visualization tasks similar to what platforms like OpenReview could provide, but with a stronger focus on analysis and accessibility."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "• As mentioned in the summary, this paper corresponds to a complete solution and a mature product. This is something that many AI papers lack. It is not just a paper but a real, influential system that has already helped many researchers.\n\n• Since my research area is different, I am not very familiar with the LAMP stack, but considering that a mature web service is already running, these technologies should be reliable.\n\n• The authors present several interesting findings, such as the observation that in ICLR 2025, under the large submission volume, acceptance decisions were more influenced by mean scores, suggesting limited reviewing capacity. These statistics and data may become valuable references for future work."}, "weaknesses": {"value": "• Although I fully acknowledge the influence and practical value of Paper Copilot, I must point out several weaknesses when considering it as an ICLR submission. The most significant issue is that the work is highly engineering-oriented. Beyond the statistical analyses, there is little conceptual or methodological novelty. The paper essentially combines software engineering (LAMP stack, web scraping), basic statistics, and visualization. While the results are interesting, the overall framing feels somewhat utilitarian. It focuses mainly on acceptance outcomes rather than on the fundamental purpose of peer review itself, which is to help manuscripts improve and to strengthen trust in published research. This is the main reason I would rate the paper a 6 rather than an 8.\n\n• The authors introduce an entropy-based metric and visualize its trend over time. When observing the sharp entropy drop in 2025, they attribute it to limited reviewing capacity. However, this conclusion seems overly speculative. The reasoning appears circular since the authors assume that reviewing pressure led to algorithmic decision-making, design an analysis around that assumption, and then use the results to confirm it. In fact, the entropy decline could also result from improved reviewer quality or enhanced matching processes, possibly aided by AI tools that reduce variance among reviewers and area chairs, leading to more consistent decisions. Building on that, the authors’ interpretation that low entropy represents an undesirable state may not be universally valid. As Schrödinger once said, “life feeds on negative entropy,” and in physics, low entropy generally denotes higher order and structure. In peer review, high entropy indeed reflects confusion and stress, but low entropy can have two possible explanations: (1) decision-making rigidity due to over-reliance on reviewer scores under pressure, or (2) improved efficiency from better reviewer–paper matching and enhanced consistency across evaluations. In the latter case, lower entropy could actually indicate a healthier and more coordinated review process.\n\n• There are also some writing issues. First, in the abstract, the sentence beginning with “We present PAPER COPILOT, …” contains too many clauses, which makes it hard to understand and potentially ambiguous. In addition, the introduction part is not very clear and is quite difficult to follow. If I did not already know what Paper Copilot is, I might not be able to figure out the background, purpose, and motivation of the system within limited reading time. There are also a few typos, for example, in line 199 “ArXiv” should be “arXiv.”."}, "questions": {"value": "I do not have significant concerns about this paper, but I would like to offer a few suggestions. \n\n• Why does Figure 5 in the Supplementary Material appear to be identical to Figure 3 in the main manuscript?\n\n• Paper Copilot collects some review scores voluntarily uploaded by authors (I have contributed such data myself). However, the reliability of this information largely depends on self-reporting, and anyone could, in principle, submit arbitrary data. The authors are encouraged to estimate the extent of this potential bias. For example, they could compare the statistics collected on the platform with the actual post-acceptance distributions in later stages to assess consistency.\n\n• The name Paper Copilot does not seem closely aligned with the system’s functionality. Without prior knowledge of the project, one might assume it is a writing-assistance tool similar to Cursor or other AI copilots."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BNGOJ8cDgQ", "forum": "CyKVrhNABo", "replyto": "CyKVrhNABo", "signatures": ["ICLR.cc/2026/Conference/Submission1784/Reviewer_reMW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1784/Reviewer_reMW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760981248644, "cdate": 1760981248644, "tmdate": 1762915889374, "mdate": 1762915889374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a system and dataset, Paper Copilot, aimed at tracking and analyzing the evolution of peer review in AI/ML conferences. The authors note that with the surge in submissions, the current peer review system is under immense pressure, facing issues of low transparency, inconsistent standards, and heavy reviewer load. To address this, the authors have built a hybrid data collection pipeline (combining the OpenReview API, web scraping, and community contributions) to create a longitudinal dataset. A unique contribution of this dataset is its capture of temporal dynamics, such as pre/post-rebuttal score changes.\n\nThe paper's core empirical contribution is a large-scale analysis of ICLR (2017-2025). The analysis reveals several key findings, including a decision entropy anomaly in 2025 (suggesting AC decisions are becoming more reliant on mean scores), the clear role of the rebuttal phase in boosting scores for high-rated papers, and an asymmetry observed for borderline papers. Finally, the authors discuss the ethical considerations related to the system and dataset, particularly regarding data sourcing, privacy, and potential self-selection bias."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a problem of critical importance and significant interest to the machine learning community. The challenges of maintaining a fair, transparent, and effective peer review system at scale are widely recognized, and this work provides a valuable quantitative lens through which to examine these issues. \n\n2. A major strength lies in the creation of a unique dataset that captures the temporal dynamics of the review process. By archiving review snapshots that are often overwritten, the authors provide a durable and highly valuable resource for future metascience research. This contribution has the potential to enable a new class of studies on reviewer and author behavior.\n\n3. The analysis of ICLR data is thorough and leads to several non-trivial conclusions. The finding on \"decision entropy\" (Section 5.1) is particularly compelling, offering strong quantitative evidence for a hypothesis that many in the community have anecdotally suspected: the review system may be simplifying its decision-making process under load. Similarly, the analysis of rebuttal dynamics and the \"asymmetry of borderline papers\" provides practical insights for authors, reviewers, and ACs.\n\n4. The authors' transparent and thoughtful discussion of the ethical dimensions of their work (Section 6) is highly commendable. Acknowledging issues related to data sourcing, privacy risks, potential for misuse, and self-selection bias demonstrates a mature and responsible approach to research."}, "weaknesses": {"value": "1. This paper's main contribution seems to be positioned more as a resource/tool (the Paper Copilot website and dataset) rather than a novel methodology or benchmark. While the analysis is insightful, it primarily applies (relatively standard) statistical methods to a novel dataset. This makes the paper's positioning somewhat ambiguous; it feels more akin to a \"Demo Paper\" than a typical ICLR main conference paper.\n\n2. The paper references a Position Paper (Yang, 2025) whose title (\"...community should adopt a more transparent and regulated peer review process\") suggests that the core motivation and possibly even the high-level framework for the Paper Copilot system itself may have been outlined previously. The fundamental issue is that if the Paper Copilot system itself (the core concept and initial framework) was the primary contribution of that prior Position Paper, the novelty of the system component in the current submission is substantially diminished. I would hope the authors could clarify in the paper or in the rebuttal the specific, concrete and incremental technical contribution (e.g., the complex implementation details, the novel longitudinal data collection pipeline, and the empirical analysis) presented here, clearly distinguishing it from the system's initial concept outlined in the Position Paper. \n\n3. The paper claims in its abstract and introduction that the Paper Copilot system supports \"tracking talent trajectories\" as one of its core motivations (e.g., analyzing institutional or national influence). However, the core empirical analysis (Section 5) seems to lack analysis on this topic; all analysis focuses on the review process itself (entropy, score dynamics). I hope the authors might consider adjusting the paper's framing and claims to be more consistent with the presented contributions, or perhaps supplement this analysis in the rebuttal, as there currently appears to be a disconnect between the two.\n\n4. For closed-review conferences, the system relies on voluntary community contributions. This is a potentially significant limitation, as the authors acknowledge in the ethics section. While the authors conducted a user study on \"consent rate\" (59.9%), this does not guarantee that the distribution of actually submitted data is representative (e.g., submitters might be skewed toward high-scoring papers or specific institutions). This impacts the generalizability of any conclusions that might be drawn from this data in the future."}, "questions": {"value": "1. Your analysis identified a shift toward \"score determinism\" in ICLR 2025. During your research, did you find any concrete reasons for this shift (e.g., specific instructions from conference organizers to ACs, or the introduction of new reviewing policies)? Furthermore, beyond diagnosing problems (like opacity or score dependency), can your analysis offer any specific, actionable solutions or intervention recommendations for conference organizers?\n\n2. Could you please clarify in detail the relationship with the Yang (2025) position paper? Was that paper merely a \"proposal\" or \"call to action,\" or did it already include the design, data, or preliminary analysis of the Paper Copilot system? For the reviewers, clarifying the precise incremental contribution of this ICLR submission is quite important.\n\n3. The \"talent trajectory\" analysis was presented as a core motivation but seems to be missing from the empirical results. Could you comment on the status of this analysis? Do you have preliminary findings you can share? Or is this purely future work? If the latter, perhaps you might consider revising the paper's framing and claims to align them more closely with the presented contributions.\n\n4. The authors mention making the dataset available on GitHub. To better evaluate this contribution, a clear data and code availability statement would greatly help reviewers assess the reproducibility and contribution to the community.\n\n5. Regarding the self-selection bias of community-contributed data, you mention a plan to compare against (future) official NeurIPS 2025 data. This is a good plan, but have you conducted any preliminary analysis to check the representativeness on the currently collected data? For example, comparing the institutional, national, or track distribution in your collected data (for CVPR/ICCV) against known public statistics (e.g., from accepted paper lists) from those conferences?\n\n**I highly look forward to the authors' discussion. If my concerns can be satisfactorily addressed, I am open to increasing my score.**"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0eKvY9BKUs", "forum": "CyKVrhNABo", "replyto": "CyKVrhNABo", "signatures": ["ICLR.cc/2026/Conference/Submission1784/Reviewer_c5VV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1784/Reviewer_c5VV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750937937, "cdate": 1761750937937, "tmdate": 1762915889095, "mdate": 1762915889095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Paper Copilot , a system designed to track and analyze the peer review process at AI conferences. It collects data from public APIs, website scraping, and voluntary community submissions to create an open dataset. The authors use this dataset to conduct an empirical analysis of ICLR review dynamics, aiming to bring more transparency to the field."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a system architecture for aggregating peer review data from diverse sources, including public APIs like OpenReview and opt-in community contributions for closed-review venues.\n- It provides a dataset that captures temporal review dynamics, such as tracking score and confidence changes throughout the discussion and rebuttal phases, which is not always preserved by official platforms."}, "weaknesses": {"value": "- The reliance on voluntary, \"community-contributed\" data for all closed-review venues is the paper's biggest weakness. The authors acknowledge this self-selection bias, but it's a fundamental limitation. It's hard to trust that this data (e.g., 1,860 responses for four major conferences ) is representative, which limits the reliability of any insights drawn about those venues.\n- I knew this (similar) project before and I have used the corresponding website. At that time, my biggest concern is the potential bias, which (I think) is inevitable but sadly could falsely affect the judgement of some researchers (especially those junior ones). So, I personally think that such project have clear advantages and disadvantages at the same time that should be critically balanced.\n- Data integrity and accuracy are potential issues, stemming from known error rates in LLM-based affiliation extraction and a reliance on scraping methods that may conflict with platform Terms of Service.\n- I am not sure whether ICLR would accept such form of paper as this paper looks more like a position paper rather than a typical ICLR technical paper."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Potentially harmful insights, methodologies and applications"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DimkKmxeE3", "forum": "CyKVrhNABo", "replyto": "CyKVrhNABo", "signatures": ["ICLR.cc/2026/Conference/Submission1784/Reviewer_hF5U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1784/Reviewer_hF5U"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918781716, "cdate": 1761918781716, "tmdate": 1762915888907, "mdate": 1762915888907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}