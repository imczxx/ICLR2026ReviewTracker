{"id": "Pi1Y2huHLg", "number": 4436, "cdate": 1757680656738, "mdate": 1763413131653, "content": {"title": "ChartRef: Benchmarking Fine-Grained Visual Element Localization in Charts", "abstract": "Humans interpret charts by first localizing visual elements—such as bars, markers, and segments—before reasoning over the data. In contrast, current multimodal models primarily rely on text reasoning, limiting their ability to leverage fine-grained visual information. To address this, we introduce ChartRef, a dataset of 38,846 questions, answers, referential expressions, and bounding boxes across 1,141 figures and 11 chart types. Our key insight is that the chart-rendering code makes it possible to generate visual element localizations that are aligned with question–answer pairs. Given only the Python script, a large language model infers the semantics of plotted data, maps data series to visual encodings, and programmatically extracts bounding boxes, yielding visual annotations for charts at scale. Using ChartRef, we benchmark multimodal LLMs and find 3–7\\% accuracy improvements on chart question answering when models are provided with ground-truth bounding boxes. We further evaluate vision and multimodal models on chart object detection and visual grounding. While object detection exceeds 80 AP@50, phrase grounding accuracy is only 2.8, revealing a significant gap: current models can recognize chart elements perceptually but struggle to integrate context cues from axes, legends, labels, and data to ground fine-grained textual references. We hope to inspire future work in developing multimodal models capable of human-like chart visual grounding.", "tldr": "We propose a scalable data generation pipeline to collect ChartRef, a benchmark with paired questions, answers, and bounding boxes of chart visual elements, and evaluate the capabilities of foundation models on fine-grained chart visual grounding.", "keywords": ["chart perception", "chart object detection", "chart visual grounding", "synthetic data generation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/4889c9797b21660a04a2e956b1bc47914d7a7e4c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces CHARTREF, a new benchmark targeting fine-grained visual element localization in charts. The core idea is to leverage the chart-rendering code (primarily matplotlib Python scripts) as a privileged signal to automatically produce aligned tuples of (question, answer, referential expression, bounding box) at scale. The pipeline prompts an LLM to: (i) mask out plotted data while preserving semantics (labels, legends); (ii) infer which plotted series correspond to which visual encodings; (iii) programmatically query the figure object to extract data values and transform them to pixel coordinates; and (iv) synthesize referential expressions and bounding boxes for the relevant marks. The resulting resource comprises 38,846 visual-grounding examples filtered across 1,141 figures and 11 chart types (with 44,345 total QA pairs and 40,336 detection annotations reported before filtering). Using CHARTREF, the authors show that supplying ground-truth bounding boxes improves chart QA accuracy by 3–7% over standard prompting; they also benchmark object detection (up to ~80.6 AP@50) and visual grounding (best 2.8 Acc@1 after finetuning), revealing a persistent gap in chart phrase grounding despite strong detection. The work argues for tighter vision–language alignment to exploit chart context cues (axes, legends, subplots) and provides prompts and stats to support reproducibility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduces a code-centric supervision pipeline that (i) scales, (ii) preserves semantics (legend/axes roles), and (iii) yields aligned QA and localization—addressing limitations of image-only annotations.\n\n2. Comprehensive benchmarking across closed and open VLMs and vision detectors; consistent improvements from providing bounding boxes for QA (3–7%); solid AP@50 after finetuning; careful qualitative analyses of error modes (saliency over-reliance, cluster confusion).\n\n3. Pipeline diagrams (Figures 1–2) convey the three-stage process; tasks are cleanly split (QA vs detection vs grounding) with appropriate metrics.\n\n4. Surfaces a clear research target: chart visual grounding remains hard (Acc@1 ≈ 2–3% after finetuning) despite strong detection, motivating better text–structure alignment (axes/legends/subplots)."}, "weaknesses": {"value": "1. Training and evaluation are centered on matplotlib renderings; generalization to in-the-wild charts (PDF scans, raster artifacts, non-Python toolchains) is not empirically established. Consider a held-out real-world test set (e.g., CharXiv/ChartMuseum excerpts) annotated with boxes to quantify transfer.\n\n2. Heavy reliance on a single proprietary LLM (for templating, extraction code, and ambiguity filtering) risks style bias and silent failure modes. Provide manual audit statistics (e.g., 500-sample verification of bbox correctness; inter-annotator agreement on ambiguity labels).\n\n3. Because ChartMimic scripts inform both generation and evaluation, ensure strict figure-level splits and report any template/code overlap between train/val/test, particularly when finetuning detectors.\n\n4.Acc@K is coarse for phrases that may match multiple near-duplicate marks (e.g., overlapping points). Consider phrase-aware soft matching, normalized localization error (distance to true mark normalized by axis scale), or IoU@τ, τ∈{0.25,0.5,0.75} to tease apart near misses vs gross failures.\n\n5. Reported totals vary across sections (e.g., 38,846 vs 44,345 examples; 1,141 vs 1,259 figures). Clarify which subsets underpin each table to avoid confusion and to support reproducibility."}, "questions": {"value": "1. Did you perform a human audit of (i) bbox correctness and (ii) ambiguity filtering? If so, please report sample sizes and error rates; if not, can you add a small audit in the camera-ready?\n\n2. Have you evaluated zero-shot or finetuned models on non-matplotlib charts (e.g., D3, ggplot2, Excel screenshots, arXiv PDFs)? A small transfer study, even with limited annotations—would strengthen external validity.\n\n3. How exactly are SoM overlays rendered (box color/thickness/labeling), and do style changes affect the reported 3–7% QA gains? A robustness sweep here would help practitioners adopt the method.\n\n4. For phrases that may refer to a set of marks (e.g., clusters), how do you define a single ground-truth box? Would a set-prediction or mask-based variant better reflect the task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qfzHycf4pa", "forum": "Pi1Y2huHLg", "replyto": "Pi1Y2huHLg", "signatures": ["ICLR.cc/2026/Conference/Submission4436/Reviewer_Ck79"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4436/Reviewer_Ck79"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704190587, "cdate": 1761704190587, "tmdate": 1762917364363, "mdate": 1762917364363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We thank the reviewers for their constructive feedback on our work."}}, "id": "Z0UJtmlobh", "forum": "Pi1Y2huHLg", "replyto": "Pi1Y2huHLg", "signatures": ["ICLR.cc/2026/Conference/Submission4436/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4436/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763413130929, "cdate": 1763413130929, "tmdate": 1763413130929, "mdate": 1763413130929, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ChartRef, a large-scale dataset designed to benchmark fine-grained visual element localization in charts. The authors leverage the Python chart-rendering code from ChartMimic to programmatically generate aligned (question, answer, referential expression, bounding box) pairs across 11 chart types, totaling over 38k examples. The authors further evaluate a diverse range of models on the proposed benchmark, including both vision-language models and pure vision models, to assess their ability to localize fine-grained visual elements in charts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper makes use of chart-rendering code to generate aligned bounding box annotations without manual labeling, representing an elegant and efficient procedural data synthesis strategy.\n2. It presents a dataset covering 11 chart types with 38,000 examples, enabling cross-task evaluation across chart question answering, object detection, and visual grounding."}, "weaknesses": {"value": "1. Using LLMs to generate code for chart parsing is risky: hallucinations may lead to inaccurate data extraction. Human verification and evaluation are necessary, and the overall dataset quality remains uncertain.\n2. Based on the examples in Appendix E, the proposed benchmark appears overly templated and simple, focusing largely on questions about specific numeric values, which has fully explored (e.g., PlotQA). Moreover, the benchmark is built incrementally on ChartMimic, which raises questions about its significance and novelty.\n3. The experiments provide limited insight. The results in Sec.4.1 are largely predictable, and the object-detection setting in Sec.4.2 (as well as Sec.4.3) seems to have limited practical value for the chart domain. The authors should reconsider the experimental design to better highlight the unique value of the proposed benchmark."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IgO0N3NoIP", "forum": "Pi1Y2huHLg", "replyto": "Pi1Y2huHLg", "signatures": ["ICLR.cc/2026/Conference/Submission4436/Reviewer_pZt5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4436/Reviewer_pZt5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817905080, "cdate": 1761817905080, "tmdate": 1762917364105, "mdate": 1762917364105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CHARTREF, a large-scale benchmark for fine-grained visual element localization and grounding in charts. It programmatically pairs questions, answers, and referential expressions with precise bounding boxes across 11 chart types (38,846 grounded examples), generated from Python chart-rendering scripts. Evaluations cover Chart Question Answering (CQA) with and without localization signals, object detection, and visual grounding. Providing ground-truth boxes yields 3-7% CQA gains, but visual grounding remains very challenging (best finetuned Acc@1 ≈ 2.8), revealing a core limitation in current multimodal models’ alignment of language with chart structure (axes, legends, labels)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of extracting bounding boxes from Python rendering code is innovative and scalable. Instead of manually annotating images or relying on computer vision models, the paper extracts coordinates directly from matplotlib figure objects. \n\nThe paper makes a case that visual grounding is fundamental to chart understanding. The observation that humans localize visual elements before reasoning is intuitive, and the finding that providing ground-truth bounding boxes improves question answering by 3-7% validates this approach.\n\nTesting across three distinct tasks (question answering, object detection, and visual grounding) provides a thorough assessment. The contrast between high object detection performance (80+ AP@50) and low visual grounding accuracy (2.8) is particularly revealing about where current models fail.."}, "weaknesses": {"value": "While 11 chart types sounds good, the paper relies entirely on ChartMimic as source material. This means the charts are all programmatically generated with clean, consistent styling. Real-world charts from papers, reports, or presentations often have messy formatting, overlapping elements, custom styling, and imperfections that won't appear here. \n\nThe paper admits that expressions \"only require extracting individual data from the chart\" and don't cover comparisons or relationships between multiple data points. This is a significant limitation - phrases like \"the bar for Model A\" are much simpler than natural language people actually use, such as \"the product with the highest sales in Q3\" or \"the trend showing declining performance.\" The benchmark may not reflect real visual grounding difficulty.\n\nWhile the paper shows some qualitative examples in Figures 5 and 6, there's no systematic breakdown of error types. \nWhy does visual grounding fail?  Is it spatial precision?  Context integration? Ambiguity handling? \n\nThe paper doesn't compare against human performance on the localization tasks. We know humans achieve high accuracy on chart question answering, but how well do humans perform on the exact visual grounding task with the exact referential expressions used here? Without this baseline, it's hard to assess whether 2.8 accuracy represents a fundamental model limitation.\n\nFigure 4 shows the data is \"balanced\" across chart types, but the middle panel reveals huge variation in questions per type. Line plots have far more examples than other types. \n\nSection 2.2 mentions using a multimodal LLM to classify questions as ambiguous, defective, or valid, but doesn't report how many questions were filtered at each stage or provide inter-annotator agreement metrics.  What percentage of initially generated questions were rejected? \n\nAll training and testing happens on ChartMimic-derived data. There's no evaluation on held-out chart sources to assess whether models generalize beyond the specific rendering style and structure of matplotlib-generated figures."}, "questions": {"value": "Why does chain-of-thought prompting sometimes hurt performance (Table 2) while set-of-marks helps? What does this tell us about how models process visual information?\n\nWhat is human accuracy on the visual grounding task using your exact referential expressions? Can you provide a quantitative grounding error taxonomy that separates reference resolution errors from bounding-box misalignment (e.g., thin lines, no markers) ?​\n\nNeeds a quantitative analysis of failure modes for visual grounding? For example, what percentage of errors are due to: wrong chart element entirely, correct element but imprecise localization, confusing similar elements, failing to parse legends/axes?\n\nFor charts without explicit markers where 10×10 boxes are used, how often do these boxes truly cover the intended visual mark, and what is the measured impact on evaluation noise (Tables 6–7) ?​ How does box size affect detection and grounding performance? Are errors mainly about choosing the wrong element or about imprecise localization?\n\nWhat fraction of questions are direct readings versus interpolated/estimated numbers under your postprocessing definitions ?​\n\nExpand grounding to relational queries (comparisons, trends, multi-mark references)? Provide chart-structure annotations (axis/legend/linking tables) or intermediate supervision to encourage language-aware alignment? \n\nRelease evaluation tools + SoM prompts to standardize testing and promote adoption. Add a real-world subset (e.g., from papers/reports) with human-verified boxes to measure domain transfer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YQ9GBepDpo", "forum": "Pi1Y2huHLg", "replyto": "Pi1Y2huHLg", "signatures": ["ICLR.cc/2026/Conference/Submission4436/Reviewer_XwLu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4436/Reviewer_XwLu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940530898, "cdate": 1761940530898, "tmdate": 1762917363822, "mdate": 1762917363822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ChartRef, a new benchmark dataset for evaluating the fine-grained visual localization capabilities of models on charts. The key contribution is a scalable data generation pipeline that uses LLMs to parse Python chart-rendering scripts. This process programmatically extracts bounding boxes for visual elements (like bars and markers) and aligns them with generated question-answer pairs and referential expressions. Using this dataset, the authors benchmark VLMs and LVLMs on three tasks: Chart Question Answering (CQA), Object Detection, Visual Grounding."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of the benchmark is interesting and important to assess at the fundamental capabilities of LVLMs on structured data. \n- The data generation pipeline is technically sound."}, "weaknesses": {"value": "- The main problem of this paper is its poor organization and clarity. While the data generation pipeline seems technically sound, the motivation for the experimental sections is not clearly stated and suffers from unclear notation. For example, the motivation for Sec 4.1 is unclear. The finding that providing ground-truth bounding boxes as visual cues can improve chart QA performance is obvious and not novel. The paper itself notes this is done using Set-of-Marks (SoM) prompting, and the reviewer cannot understand the main message the authors want to deliver.\n\n- Also, the paper suffers from multiple missing definitions and unclear notation. It is missing crucial definitions for its experimental setup, which makes the results difficult to interpret. For example, evaluation pipeline and metrics are not well-defined. The paper never explains what \"AR\" (Table 3) or \"Acc@N\" (Table 5) represent, and model abbreviations are not explained. Table 3 uses (T), (B), and (L) without defining them. The presentation of this paper is below the acceptance threshold, and reading a paper with multiple typos and unclear notation is frustrating.\n\n- The findings lack depth. It's obvious that the zero-shot visual grounding performance of both VLMs and LVLMs on charts is pretty low. The distribution of chart images is far from their generic training data distribution. What are the findings other than just a performance drop? Is the degradation due to the vision encoder backbone not encoding the information needed, or is it due to cross-modal misalignment between the referential expression and the visual features?\n\n- Missing analysis of the proposed dataset. The dataset mostly relies on LLM generation. However, LLMs can make mistakes, especially in such a complex task. A human evaluation should be considered to assess the quality of the dataset (for example, its correctness and diversity)."}, "questions": {"value": "- What's the visual grounding performance of Qwen-2.5-VL after fine-tuning on the proposed dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Yg4cnxrMr2", "forum": "Pi1Y2huHLg", "replyto": "Pi1Y2huHLg", "signatures": ["ICLR.cc/2026/Conference/Submission4436/Reviewer_r9Nn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4436/Reviewer_r9Nn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957546265, "cdate": 1761957546265, "tmdate": 1762917363489, "mdate": 1762917363489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}