{"id": "Efekr8cBMG", "number": 18408, "cdate": 1758287376249, "mdate": 1759897105210, "content": {"title": "Controllable Test-Time Scaling via Sparse Autoencoder‑Based Reasoning Steering", "abstract": "A common Test-Time Scaling (TTS) strategy for Large Language Models (LLMs) reasoning is allocating additional computation during inference to generate longer Chains-of-Thoughts (CoTs). \nHowever, simply scaling CoT length often introduces redundancy and aimless exploration, which can paradoxically degrade performance.\nWe propose that effective TTS requires a shift from merely lengthening reasoning to actively steering reasoning trajectory, thereby directing additional computation toward productive reasoning.\nTo this end, we propose SAE-Scaling, a framework for fine-grained control over an LLM's reasoning trajectory. \nSAE-Scaling first employs Sparse Autoencoders\nto identify and disentangle interpretable features associated with five key reasoning strategies: *Problem Understanding*, *Procedural Planning*, *Backtracking*, *Multi-perspective Verification*, and *Hypothesis Reasoning*. \nNext, we train a lightweight strategy router that dynamically chooses a reasoning strategy at each step of the reasoning trajectory.\nBy actively manipulating the strategy-specific feature during generation, SAE-Scaling steers the CoT to follow a target reasoning strategy, thereby channeling the additional computation\nto more productive reasoning.\nExperiments on three LLMs across three challenging reasoning benchmarks show a 68\\% average success rate in controlling reasoning strategies alongside an average absolute accuracy gain of 3.6\\% over the vanilla baseline, highlighting the effectiveness of SAE-Scaling.", "tldr": "", "keywords": ["Test Time Scaling", "Reasoning Steering", "Vector Steering", "Sparse Autoencoders", "LLM Reasoning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b5ca87c35787d03dd67eea80beccfa7aefb8a268.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper identifies a key limitation in current Test-Time Scaling (TTS) strategies: simply allocating more computation to generate longer Chains-of-Thought (CoTs) often introduces redundancy and can degrade performance. The authors propose a new task, \"reasoning steering,\" which aims to actively guide the LLM's reasoning trajectory toward productive strategies rather than just extending its length. Their proposed method, SAE-Scaling, uses Sparse Autoencoders (SAEs) to decompose an LLM's hidden states into disentangled, interpretable features. The core of the method is a multi-stage pipeline designed to identify features corresponding to five key reasoning strategies (e.g., Backtracking, Multi-perspective Verification). This pipeline first uses keywords to find candidate features , then validates their steering capability using causal analysis , and finally ranks them by their ability to correct errors on a training set . A lightweight \"strategy router\" is then trained to dynamically select the most effective, pre-vetted feature at inference time. \n\nExperiments show that SAE-Scaling can control reasoning strategies with an average 68% success rate and achieves a 3.6% average absolute accuracy gain over a baseline, outperforming a \"Budget Forcing\" (wait-token) method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1). The paper argues convincingly that TTS should steer reasoning rather than indiscriminately elongate it, and formalizes “reasoning steering” as residual-stream interventions via control vectors. The method presentation is concrete, including injection site, strength, and horizon.\n\n2). The three-stage pipeline (logit-based recall → ATE ranking → accuracy ranking) is systematic and computationally frugal relative to brute-force search over SAE features. The router architecture and InfoNCE training are straightforward.\n\n3). Results span three Qwen3 sizes and three math benchmarks, with ablations on router utility, top-k features, intervention length, and turns. The observation that very long extensions hurt accuracy is useful to practitioners."}, "weaknesses": {"value": "1). The paper's entire methodology is circular. It claims to discover features that lead to \"productive reasoning\", but it explicitly defines \"productive\" as \"leading to a correct answer on the training set.\" The stage-3 explicitly filters the \"steering-capable features\" by testing them on problems the LLM initially answered incorrectly. Features are then ranked and selected based on their \"correction rate\"; The Strategy Router is not trained to identify a \"strategy\" in the abstract. It is trained via contrastive learning where a positive pair $(Y_{<t}, f_s^+)$ is defined as one where steering with feature $f_s^+$ leads to a correct final answer. These reveals that the system is not learning a generalizable policy about why a given reasoning strategy is appropriate. Instead, it is a complex mechanism for memorizing and applying specific feature-interventions that were empirically proven to work on the training data. This is a form of supervised overfitting to the answer key, and it is highly unlikely to generalize to new problems where the \"correct\" intervention is unknown.\n\n\n2). The paper claims to find \"disentangled features\"  for high-level concepts like \"Backtracking\" or \"Hypothesis Reasoning.\" However, the identification process is anchored on a superficial proxy: keywords. In stage0/1, features are first identified by checking if they increase the logits of manually selected keywords (e.g., \"earlier,\" \"previous\" for Backtracking; \"another,\" \"approach\" for Multi-Perspective Verification); In stage-2, the \"causal effect\" is validated using an LLM-as-judge. However, the judge's prompt (Figure 8) is itself just a keyword-and-pattern-matching template (e.g., it defines Backtracking as text that says \"The initial calculation is wrong...\" or \"Let me go back to...\"). This process does not prove the system has found a feature for the concept of backtracking. It proves it has found a feature that reliably makes the LLM output the word \"earlier\" or say \"I made an error.\" The claim of controlling a \"reasoning strategy\" is a significant overstatement; the method primarily controls the generation of specific, strategy-related lexical tokens.\n\n\n3). The main performance gains (Table 2) are shown against \"Budget Forcing,\" a method that extends CoTs by appending \"wait\" tokens. This is a weak, strawman baseline that the paper itself criticizes as leading to \"aimless exploration\". More damningly, the paper's own ablation study (\"w/o router\") —which randomly selects from the accuracy-filtered features—still outperforms the Budget Forcing baseline. This strongly suggests that the 3.6% performance gain has very little to do with the sophisticated SAEs or the \"dynamic strategy router.\" The gain comes almost entirely from the feature-filtering in Stage 3. The experiment is not comparing \"intelligent steering\" vs. \"dumb lengthening.\" It is comparing \"steering with vectors pre-selected for high accuracy on a training set\" vs. \"dumb lengthening.\" The victory is pre-ordained by the experimental design. The paper fails to compare against stronger, more relevant baselines, such as the keyword-boosting method from its own Table 1 or other published vector-steering techniques.\n\n\n4). The paper's own analysis reveals the brittleness of the proposed intervention. Figure 4 shows that while accuracy initially increases, it becomes unstable and declines sharply after 7 extension turns. The authors identify the cause: \"regressions\" (correct answers flipping to incorrect) \"begin to surge,\" offsetting any gains . This indicates that the steering intervention is not a robust enhancement but a high-risk manipulation that quickly destabilizes the model, causing it to \"lose conviction in its correct answers\". This contradicts the central goal of \"productive reasoning.\"\n\n\n5). Results are primarily aligned by turns, while cumulative tokens per method differ, and accuracy is reported as an average over eight samples per problem (not standard pass@1 or pass@k). Although the paper plots tokens vs. turns, it does not present budget-matched accuracy (e.g., at equal cumulative tokens) as the primary figure of merit. This makes it difficult to assess whether SAE-Scaling is genuinely more compute-efficient than Budget Forcing under equal token budgets. Statistical significance is also not reported."}, "questions": {"value": "1) How do you ensure the GPT-4o judge is not picking up keyword-driven cues introduced in Stage 1? Can you provide human evaluation at scale with a strategy-specific rubric, and include inter-annotator agreement and confidence intervals?\n\n\n2) Please report accuracy at equal cumulative tokens across methods (and pass@1), with statistical tests across seeds/problems. The current “turn-matched” view can hide efficiency differences.\n\n\n3) How sensitive are the Stage-2 rankings to the choice of N=10, temperature 0, and thresholds? Provide ablations with larger N and uncertainty estimates; ideally, release per-feature ATE distributions. \n\n\n4) Can you compare to other control or steering approaches beyond Budget Forcing, including prompt-level strategy steering and learned vector steering without SAEs? Table 2 alone is not sufficient to support the central claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fuv9Skeoxv", "forum": "Efekr8cBMG", "replyto": "Efekr8cBMG", "signatures": ["ICLR.cc/2026/Conference/Submission18408/Reviewer_YjiZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18408/Reviewer_YjiZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879511892, "cdate": 1761879511892, "tmdate": 1762928112517, "mdate": 1762928112517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an inference time scaling technique that uses SAE steering to control and add diversity to inference paths, with the goal of reducing redundancy and improving performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The method is interesting from an interpretability point of view and shows that it is indeed possible to somewhat control inference for diversity through SAEs"}, "weaknesses": {"value": "- The method is only tested on math, and only on AIME and HMMT. This is very limited.\n\n- Improvements even in these datasets are limited.\n\n- Only a limited number of baselines is considered. There could be other prompting or token interjection approaches that perhaps provide similar benefits. E.g. explicitly asking the model to use different approaches or personas (see \"Diversity of thought improves reasoning abilities of large language models\") or saying \"Let's try something else\" etc.\n\n- The results presented are not equally visualized for all datasets. For example Fig 4 and 6 are shown for AIME 24 but not for the other datasets?"}, "questions": {"value": "Suggestions\n- Would recommend not leaving related work for the appendix. This is not considered good practice and delays the discussion on how the work is positioned in the literature wrt novelty.\n- The idea of steering disentangled representations has also been discussed in related work for image generation. The authors could consider making that connection too in the discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Za6EyRFXrU", "forum": "Efekr8cBMG", "replyto": "Efekr8cBMG", "signatures": ["ICLR.cc/2026/Conference/Submission18408/Reviewer_TjbW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18408/Reviewer_TjbW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931610324, "cdate": 1761931610324, "tmdate": 1762928112159, "mdate": 1762928112159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SAE-Scaling to improve an LLM's reasoning ability during inference. The authors state that simply forcing a model to generate a longer CoT is inefficient and often leads to aimless exploration. The authors first use Sparse Autoencoders to identify and disentangle specific, interpretable reasoning strategies within the model's hidden states. They then train a lightweight \"strategy router\" to dynamically select the most effective strategy for the current context. By actively steering the model's generation to follow this chosen strategy, SAE-Scaling channels the extra computation into more productive reasoning, achieving a 3.6% average accuracy gain on complex benchmarks while being more efficient than methods that just blindly extend the CoT length."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper includes a comprehensive ablation study. The experiment effectively isolates the router's contribution, proving that the system's performance gain comes from smart, dynamic strategy selection and not just from the steering mechanism itself.\n\n2. The paper is well-written, using clear language and a logical structure to explain its complex, multi-stage methodology. By immediately grounding the problem with an intuitive example in Figure 1 and organizing the experiments around specific research questions, the authors make their novel ideas highly accessible to the reader."}, "weaknesses": {"value": "1. Baseline is not fair. There are many other diverse prompting methods, such as \"Self-discover: Large language models self-compose\nreasoning structures.\". These advanced prompting techniques are also training-free, much easier to implement, and might achieve similar or better results, making the paper's claimed performance gains less convincing.\n\n2. Narrow Domain:\nThe method was only tested on math reasoning problems. The authors needed to show it works on a wider variety of tasks like coding challenges or logical puzzles (e.g., Big-Bench Hard), the paper can't convince the reader that this does not just work for math.\n\n3. The entire framework is built around identifying and using five pre-defined reasoning strategies. Please refer to \"Self-discover\" paper to get more strategies. Other research shows that LLMs can use many different strategies, and this method provides no way to discover new or better ones. Now it is permanently locked into its pre-defined list.\n\n4. The proposed pipeline is not a simple \"plug-and-play\" solution. It involves multiple complex and costly steps to set up, which could be a major barrier to adoption."}, "questions": {"value": "1. How to use it easily in any new model? need to train a new Sparse Autoencoder?\n\n2. Any latency added?\n\n3. Why did you choose \"Budget Forcing\" instead of  more advanced, training-free prompting techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K5N7ZWQ4W1", "forum": "Efekr8cBMG", "replyto": "Efekr8cBMG", "signatures": ["ICLR.cc/2026/Conference/Submission18408/Reviewer_fDMm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18408/Reviewer_fDMm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954894224, "cdate": 1761954894224, "tmdate": 1762928111786, "mdate": 1762928111786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed method learns the embedding of a set of predefined reasoning strategies and learns a router to decide which strategy to use for each problem. The feature embedding of each reasoning strategy was extracted using a sparse autoencoder. The results show that the proposed method is able to steer the model generation successfully."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of steering reasoning using model embeddings is a new idea to me."}, "weaknesses": {"value": "- Writing is hard to follow. I highly recommend that the author use pseudocode to communicate the algorithm since there are too many steps. See my questions.\n- The significance of the performance gain is not significant in Table 2. I'm not convinced that the added complexity in this proposed method is worthwhile. I suggest the author consider other benefits beyond just accuracy."}, "questions": {"value": "- Line 219: What is logit lens? How it was implemented needs to be explained.\n- Line 222: How do you get the entangled feature f_i? \n- Line 236: What is the intuition or brief explanation of ATE? It's hard to follow if you only put a reference here. \n- Line 239: What is the treatment condition?\n- Line 302: Why do you need GPT-4o as a judge to evaluate steering success rate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HzDbcDaLpi", "forum": "Efekr8cBMG", "replyto": "Efekr8cBMG", "signatures": ["ICLR.cc/2026/Conference/Submission18408/Reviewer_Cv6u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18408/Reviewer_Cv6u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762130383649, "cdate": 1762130383649, "tmdate": 1762928111298, "mdate": 1762928111298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}