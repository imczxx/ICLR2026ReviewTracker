{"id": "YQ1muQBDV4", "number": 24619, "cdate": 1758358658266, "mdate": 1759896758105, "content": {"title": "Potentially Optimal Joint Actions Recognition for Cooperative Multi-Agent Reinforcement Learning", "abstract": "Value function factorization is widely used in cooperative multi-agent reinforcement learning. Existing approaches often impose monotonicity constraints between the joint action value and individual action values to enable decentralized execution.\nHowever, such constraints limit the expressiveness of value factorization, restricting the range of joint action values that can be represented and hindering the learning of optimal policies. To address this, we propose Potentially Optimal Joint Actions Weighting (POW), a method that ensures optimal policy recovery where existing approximate weighting strategies may fail. POW iteratively identifies potentially optimal joint actions and assigns them higher training weights. Our approach can be seamlessly incorporated into a variety of value function factorization algorithms, and we provide a theoretical proof that this iterative weighted training guarantees recovery of the optimal policy. Experiments on matrix games, difficulty-enhanced predator-prey tasks, SMAC, SMACv2, and a highway-env intersection scenario demonstrate that our method consistently improves performance and surpasses state-of-the-art value-based multi-agent reinforcement learning methods.", "tldr": "Recovers the optimal joint policy by iteratively recognizing potentially optimal joint actions and assigning higher weights to them.", "keywords": ["Reinforcement Learning", "Value function factorization", "Multi-Agent"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fdac04b91832b1b2d7f62ce9508ee896dd5f44fc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new way to identify the optimal joint actions to weight the learning of value-decomposition methods. An additional recognition $Q$-function is learned, which induces a set of actions that may potentially be optimal. While its learning proceeds toward approximating the optimal values, this set gets further refined, until only the optimal actions are present. Experimental results on a wide set of different problems show the better performances achieved by this method, as well as how it can be combined with general value-decomposition algorithms and improve over their vanilla versions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The issue of overcoming the representation limitations of existing value-decomposition methods, for example through mean of weighting their learning updates as done by this paper, is a key direction to deliver better MARL algorithms. The idea of doing so by identifying the optimal joint actions in an effective way is a viable path for this, and already proved capable of achieving improvements in driving existing methods, as with WQMIX. The proposed set of experiments is very wide and varied, and the reported empirical results are strong. The paper is in general quite clear and easy to follow."}, "weaknesses": {"value": "I struggle to understand some of the implementation choices made here: for example, the use of a separate $Q_r$ over the already existing $Q^\\*$ is not entirely justified to me. Also, it is not completely clear how the proposed method should overcome the limitations identified in the existing CW-QMIX and OW-QMIX algorithms. Finally, the cost (in terms of computational time) of the proposed method seems a bit hidden and not sufficiently and clearly highlighted, making it difficult to actually assess the trade-off one has to make when choosing the proposed method over other ones. Please see the Questions below for a more in-details explanation."}, "questions": {"value": "- You claim that, being trained against the optimal $Q^\\*$, the maximizing actions of $Q_r$ are going to be the same as those of $Q^\\*$ itself, and thus the set $A_r$ will include them. But the training of both $Q_r$ and $Q^\\*$ is done simultaneously, and thus we are not guaranteed that their interplay is accurate before convergence occurred. This would probably lead to similar problems to those you highlighted for CW-QMIX and OW-QMIX (i.e., inaccuracies due to learning) no? Am I missing something here?\n\n- I struggle to understand how the additional $Q_r$ is bringing any benefit over the use of $Q^\\*$ directly to identify the optimal joint actions: at convergence, these will both represent the same action-value function, no? And $Q_r$ is indeed trained to chase the optimal unrestricted $Q^\\*$. So why adding an additional structure rather than simply restructuring $Q^\\*$ itself to be formulated as Equation (3)?\n\n- When computing $w(s,\\mathbf{a})$, how do we check if $\\mathbf{a}\\in A_r$? If we need to explicitly construct the set of recognized optimal actions $A_r$, then this may be a quite expensive operation. Such an aspect should be eventually stated clearly.\n\n- It would be good to explicitly state the loss function for training $Q^\\*$ in the paper, as currently it can only be found in Figure 1.\n\n- In Figure 2, why are the values for $Q_1$ and $Q_2$ of ResQ different between sub-figure (h) and (i)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XcZhCsl9Ym", "forum": "YQ1muQBDV4", "replyto": "YQ1muQBDV4", "signatures": ["ICLR.cc/2026/Conference/Submission24619/Reviewer_LRKB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24619/Reviewer_LRKB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24619/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761221982752, "cdate": 1761221982752, "tmdate": 1762943138944, "mdate": 1762943138944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes POW (Potentially Optimal Joint Actions Weighting), a value-decomposition method for cooperative MARL. It introduces a recognition module $Q_r$ that identifies potentially optimal joint actions and up-weights them during training. The authors prove convergence guarantees and evaluate POW on matrix games, predatorâ€“prey, SMAC, and other benchmarks, demonstrating improved performance over QMIX, WQMIX variants, and other MARL algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses the gap between WQMIX theory and its heuristic approximations.\n2. The authors provide proof that the recognized action set converges to optimal actions.\n3. The method was tested on extensive benchmarks, showing performance gain across both monotonic and non-monotonic tasks."}, "weaknesses": {"value": "See the questions section."}, "questions": {"value": "Questions about the current manuscript:\n1. Will $Q_r$ and iterative loops increase implementation difficulty compared to QMIX or QPLEX? If so, how can this be mitigated?\n2. May not be strictly required, but I wonder if any possible comparisons with CTDE actor-critic MARL methods like MAPPO?\n3. Can we include the algorithm pseudo-code for clarity?\n4. [Minor]: Line 423: the cited work REMIX seems not to match the given reference. Please check."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M1dpfYNGt5", "forum": "YQ1muQBDV4", "replyto": "YQ1muQBDV4", "signatures": ["ICLR.cc/2026/Conference/Submission24619/Reviewer_Nw5q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24619/Reviewer_Nw5q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24619/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847757854, "cdate": 1761847757854, "tmdate": 1762943138731, "mdate": 1762943138731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new method named Potentially Optimal Joint Actions Weighting (POW) to address the problem of representativeness of a wide range of value factorization functions. This is achieved by assigning higher weights to actions that are found with potential to be optimal."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This work good theoretical groundings and a comprehensive range of experiments across different MARL environments. The matrix games provided are also useful to demonstrate the representational properties of the method and support the claims made. Theorem 1 and definition 1 are sound."}, "weaknesses": {"value": "A number of different methods for value function factorization has been explored recently, and some of them can theoretically guarantee the factorization of any family of the environments (such as QTRAN). I feel the motivations for the proposed method are hence not sufficient, simply by saying that their object is to try to improve the expressiveness of the range of factorisation functions that can be represented.\n\nIn section 4.5.2, WQMIX seems to be better than POW in Figure 7(a) and 7(b); it is unclear to me how these results show that the performance of POW \"stems from its recognition-weighting design, not from parameter count\" (line 400); what it shows is that other methods such as WQMIX improve when the network size increases and QPLEX stays the same; it is unclear how it says something about POW's parameter count and in fact it shows that WQMIX performs better in 7(a) and 7(b) when the number of parameters is the same.\n\nSome notations could also be made more clear; for instance, in figure 1, it is unclear to me the meaning of $A_{tot}$.\n\nPlease find below some more questions that reflect other concerns."}, "questions": {"value": "1. in lines (168-169): \" In all our experiments, we set $\\alpha=0$, so only actions in $A_r$ contribute to updates, aligning theory with practice.\" - so what is the pointof proposing this $\\alpha$ weight?\n2. i wonder what happens if $\\alpha$ is not 0? has that been tested?\n3. could the authors elaborate on what is the loss \"$L_{Q^*}$\" in figure 1?\n4. in eq 4, is the optimal value function $Q_*$ also learned? or is it known a priori? from the notation it seems almost like an on policy approach being described, which makes the link between notation and practice a bit unclear\n5. in figure 3, the performances across the 3 different levels of penalties seem a bit inconsistent; for example, could the authors elaborate on why the proposed method performs better in p=-4 and p=-5 but not in p=-3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2DQ2cR88I1", "forum": "YQ1muQBDV4", "replyto": "YQ1muQBDV4", "signatures": ["ICLR.cc/2026/Conference/Submission24619/Reviewer_P6qW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24619/Reviewer_P6qW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24619/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911908484, "cdate": 1761911908484, "tmdate": 1762943138335, "mdate": 1762943138335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes POW, a value decomposition based MARL method under CTDE for optimal joint policy recovery using recognition weighting design. Empirically, the method demonstrates improved performance over baselines on multiple benchmarks"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is generally well written and easy to follow; With a constrained Q_r to over include promising joint actions and focus learning signal on them and bridges the gap of WQMIX in ideal weighting and practical approximation.\n\n- There are multiple benchmarks demonstrating the effectiveness empirically."}, "weaknesses": {"value": "- Some of the claims can be too strong. For theorem 1, it assumes the condition of Q_r can converge to  ${Q^\\star}$, however  ${Q^\\star}$ is also unknown and to be learned. E.g in eq.4 it is not using \\hat{Q^\\star} but ${Q^\\star}$, yet also claims Q_r is optimizized to $\\hat{Q^\\star}$. The gurantee in theorem 2 also requires A_r to converge to only optimal a; These are the actual challenging points in practice and cannot be just assumed; This setting is not beneficial to the actual experiment or pracitcal settings.\n\n- Also as above, Q_r and Q_tot rely on $ \\hat{Q^\\star}$ for bootstrapping, however how the bias in  $\\hat{Q^*}$ will effect A_r and Q_r is not discussed. Under extrem case they will not converge at all.\n\n- Some of the SOTA works are missing. This paper only compares with some of the classic Cooperative MARL works."}, "questions": {"value": "- I believe in multiple equations $ \\hat{Q^\\star}$ and  ${Q^\\star}$ and mixed and used against the verbal description in paper, making it hard to understand if it's typo, approximation or error.\n\n- Q_r(s,a) and Q_r (\\tau, a) are mixed and used; According to verbal description it should be Q_r(s,a)?\n\n- I wonder the C in eq5 can be discussed in terms of its sensitivity\n\n- Is Q^* and Q_tot network the same as in WQMIX? Also I wonder the \\alpha setting used for WQMIX in the experimental setting"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sgTE9r979E", "forum": "YQ1muQBDV4", "replyto": "YQ1muQBDV4", "signatures": ["ICLR.cc/2026/Conference/Submission24619/Reviewer_ZhCF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24619/Reviewer_ZhCF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24619/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984847274, "cdate": 1761984847274, "tmdate": 1762943138067, "mdate": 1762943138067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}