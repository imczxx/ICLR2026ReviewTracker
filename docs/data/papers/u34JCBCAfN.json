{"id": "u34JCBCAfN", "number": 16275, "cdate": 1758262572616, "mdate": 1763101686229, "content": {"title": "Cost Volume Meets Prompt: Enhancing MVS with Prompts for Autonomous Driving", "abstract": "Metric depth is foundational for perception, prediction, and planning in autonomous driving.\nRecent zero-shot metric depth foundation models still exhibit substantial distortions under large-scale ranges and diverse illumination.\nWhile multi-view stereo (MVS) offers geometric consistency, it fails in regions with weak parallax or textureless areas. \nOn the other hand, directly using sparse LiDAR points as per-view prompts introduces noise and gaps due to occlusion, sparsity, and projection misalignment.\nTo address these challenges, we introduce \\textbf{Prompt-MVS}, a cross-view prompt-enhanced framework for metric depth estimation.\nOur key insight is to inject LiDAR-derived prompts into the cost volume construction process through a differentiable, matching-aware fusion module, enabling the model to leverage accurate metric cues while preserving dense geometric consistency provided by the MVS process.\nFurthermore, we propose depth-spatial alternating attention (DSAA), which combines spatial information with depth context, significantly improving multi-view geometric consistency.\nExperiments on KITTI, DDAD, and NYUv2 demonstrate the effectiveness of Prompt-MVS, which outperforms state-of-the-art methods by up to 34.6\\% in scale consistency.\nNotably, our method remains effective even with missing or highly sparse prompts and produces stable metric depth under severe occlusion, weak texture, and long-range scenes, demonstrating strong robustness and generalization.\nOur code will be publicly available.", "tldr": "", "keywords": ["multi-view stereo", "depth estimation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/c2ac08fded08561901ded903d38bc5db37c2bd53.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Prompt-MVS aims to address the robustness issue of binocular depth estimation in autonomous driving scenarios. Its core innovation lies in integrating sparse geometric prompts generated by LiDAR with the dense geometric consistency of MVS. This integration compensates for two shortcomings: the insufficient spatial integrity of pure prompt methods and the failure of pure MVS in motion degradation scenarios. Experiments on datasets such as KITTI and DDAD show that Prompt-MVS maintains robustness in complex scenarios like prompt absence, sparsity, and severe occlusion. However, the manuscript has issues in method description and robustness verification that need to be resolved."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The manuscript describes the core research problem clearly enough.\n\n2. Targeting the inherent flaws of pure prompt methods and pure MVS methods in depth estimation, the manuscript clearly proposes a collaborative integration approach of \"absolute scale anchors + relative multi-view cues\"."}, "weaknesses": {"value": "1. The experiments and descriptions regarding prompt absence are not sufficient.\n\n2. The comparison with recent generative depth estimation methods is not adequate."}, "questions": {"value": "1. Only qualitative results are provided for missing prompts. Why are quantitative experiments not included?\n\n2. In Section 4.3, it is stated that \"ablation studies on key components\" are in Table 3. Should they not be in Table 1 instead?\n\n3. Additionally, the authors claim that the operations in Section 3.1 are effective. However, how to prove the rationality of the settings in Section 3.1? Further analysis is required.\n\n4. Can the results in Table 1 fully represent the latest progress in depth estimation? Given the limited comparisons, the authors should further explain the significance of the compared models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q6q6XcPGWo", "forum": "u34JCBCAfN", "replyto": "u34JCBCAfN", "signatures": ["ICLR.cc/2026/Conference/Submission16275/Reviewer_C2TX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16275/Reviewer_C2TX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761568200184, "cdate": 1761568200184, "tmdate": 1762926422517, "mdate": 1762926422517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "IfuSnjaWeP", "forum": "u34JCBCAfN", "replyto": "u34JCBCAfN", "signatures": ["ICLR.cc/2026/Conference/Submission16275/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16275/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763101684883, "cdate": 1763101684883, "tmdate": 1763101684883, "mdate": 1763101684883, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a unified framework that integrates sparse LiDAR prompts with multi-view stereo learning, enabling accurate depth estimation results. Specifically, the authors introduce a confidence-aware multi-view aggregation strategy and a Depth–Spatial Alternating Attention module to maintain structural consistency. Experiments show that this method achieves state-of-the-art performance on benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors propose a confidence map to separate original lidar points from depth pixels generated via KNN interpolation during semi-densification. More importantly, this confidence map quantifies the reliability of each densified depth pixel and can be combined with densified depth to form weighted depth prompts.\n2. This method achieves better results compared with existing methods, with improvements not only in quantitative analysis, but also in qualitative visual results (Fig 2)."}, "weaknesses": {"value": "1. If the authors had included more evaluation metrics (such as AbsRel) in Table 1, the comparative analysis of depth estimation performance would have been more comprehensive.\n2. This method relies on multi-view stereo technology and does not explicitly address temporal coherence. In real-world autonomous driving scenarios, dynamic objects (e.g., moving vehicles or pedestrians) may exacerbate the lack of temporal consistency, leading to potential inconsistencies in depth estimation results."}, "questions": {"value": "I am concerned about whether Prompt-MVS needs to be trained on the KITTI or DDAD dataset respectively. It would be even better if its cross-dataset generalization ability could be demonstrated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uYyYrITPvl", "forum": "u34JCBCAfN", "replyto": "u34JCBCAfN", "signatures": ["ICLR.cc/2026/Conference/Submission16275/Reviewer_SdBo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16275/Reviewer_SdBo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903219352, "cdate": 1761903219352, "tmdate": 1762926421880, "mdate": 1762926421880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Prompt-MVS, a unified framework that integrates sparse LiDAR-derived metric prompts into the multi-view stereo (MVS) cost volume construction process to achieve robust metric depth estimation for autonomous driving.The method addresses the complementary limitations of existing paradigms:\n1. LiDAR prompts provide accurate metric scale but are spatially sparse and noisy.\n\n2. MVS enforces dense geometric consistency but collapses under degenerate motion or weak parallax.\n\nPrompt-MVS introduces three main innovations:\n\n1. Confidence-aware multi-view aggregation — modulates cost volume matching with prompt-conditioned soft priors.\n\n2. Semi-densification with Bayesian confidence modeling — interpolates sparse LiDAR prompts and assigns uncertainty-aware weights.\n\n3. Depth–Spatial Alternating Attention (DSAA) — disentangles depth-level and spatial dependencies via alternating self-attention based on DINOv2.\n\nThe method achieves state-of-the-art results on KITTI and DDAD datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1.Novel cross-modal fusion strategy. The paper elegantly bridges geometric priors from LiDAR with multi-view cost volumes through differentiable prompt-conditioned fusion — a clear conceptual advance over purely image-based or LiDAR-guided approaches.\n\n2. Well-designed uncertainty modeling. The Bayesian formulation for confidence estimation is principled and interpretable, improving robustness to LiDAR noise and sparsity.\n\n3. Effective architecture design (DSAA). The alternating depth–spatial attention balances computational cost and representational power, showing careful design grounded in MVS geometry.\n\n4. Strong empirical validation. Extensive experiments and ablation studies on KITTI and DDAD validate each component. The model maintains stable performance even with missing prompts — a critical advantage for real-world deployment.\n\n5. Clear motivation and storytelling. The paper articulates a well-motivated problem — bridging MVS degeneracy and LiDAR sparsity — with a coherent methodological narrative and clear technical contributions."}, "weaknesses": {"value": "1. Limited scalability to uncalibrated or dynamic settings. The method assumes accurate calibration and ego-motion, which is unrealistic in many autonomous driving scenarios. Fails to address dynamic objects or moving cameras without reliable poses.\n\n2. Although the paper mentions higher computational cost, it does not present a clear trade-off curve (accuracy vs. latency vs. memory). Without such analysis, it is difficult to judge whether the additional performance gain justifies the increased complexity.\n\n3. Dependence on pretrained MVS backbone (MVSAnywhere). The improvements might partially inherit from pretrained backbones rather than the prompt mechanism itself; cross-backbone comparison would strengthen claims.\n\n4. The paper lacks a systematic study on how prompt quality (e.g., noise level, spatial sparsity, or misprojection) affects final performance. While dropout and additive noise are applied during training, there is no quantitative evaluation of robustness under degraded or corrupted LiDAR prompts.\n\n5. Ablation visualization could be richer. While quantitative results are solid, qualitative examples (e.g., failure cases, prompt-confidence maps) are limited and could further illustrate interpretability."}, "questions": {"value": "1. With the advent of powerful end-to-end 3D foundation models (e.g., Dust3R, VGGT, MASt3R) that already unify geometry, appearance, and correspondence reasoning at scale, the rationale for further advancing traditional MVS pipelines becomes questionable. The paper does not convincingly explain why reconstructing cost volumes or fusing LiDAR prompts within an MVS paradigm remains essential, instead of leveraging or extending foundation models that already encapsulate multi-view consistency and metric understanding.\n\n2. Prompt alignment and projection: How is prompt–image alignment handled under LiDAR–camera miscalibration or occlusion? Are depth priors reprojected dynamically per view?\n\n3. Generalization to non-driving scenes: Can the proposed framework generalize to indoor or aerial datasets (e.g., ScanNet, DTU), where LiDAR priors are unavailable or extremely sparse?\n\n4. Computational efficiency: What is the runtime and memory footprint compared to MVSAnywhere or PromptDepthAnything? Could DSAA be applied in a lightweight manner?\n\n5. Prompt dropout robustness: While prompt dropout is applied during training, how does the model quantitatively behave when 100% prompts are missing — does it degrade to MVSAnywhere?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wP9Xxv8crv", "forum": "u34JCBCAfN", "replyto": "u34JCBCAfN", "signatures": ["ICLR.cc/2026/Conference/Submission16275/Reviewer_WZUH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16275/Reviewer_WZUH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989112014, "cdate": 1761989112014, "tmdate": 1762926421442, "mdate": 1762926421442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Prompt-MVS proposes a prompt-enhanced multi-view stereo (MVS) framework for metric depth estimation in autonomous driving, aiming to address the limitations of sparse LiDAR prompts (noise, gaps) and MVS (failure in weak parallax/textureless regions). It integrates LiDAR-derived sparse prompts into cost volume construction via confidence-aware fusion, after semi-densification through KNN interpolation and Bayesian fusion. The framework also introduces a Depth-Spatial Alternating Attention (DSAA) module to fuse depth and spatial context. Experiments on KITTI, DDAD, and NYUv2 show state-of-the-art performance, with up to 34.6% improvement in scale consistency and robustness to sparse/missing prompts. However, the core prompt design lacks substantial innovation, relying on standard interpolation rather than MVS-specific geometric or semantic enhancements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Evaluated on multiple datasets (KITTI/DDAD/NYUv2) with diverse metrics (MAE/RMSE/AbsRel/τ), including ablation studies on prompt density/absence and component effectiveness."}, "weaknesses": {"value": "1. The core \"prompt\" component relies on KNN interpolation + Bayesian fusion for semi-densification, which is a standard sparse-to-dense strategy (e.g., OMNI-DC, Marigold-DC) rather than an MVS-tailored innovation. It does not leverage MVS’s inherent geometric properties (e.g., epipolar constraints, parallax consistency) to refine prompts—instead, it treats prompts as independent dense depth priors and injects them into the cost volume. No semantic information (e.g., object categories, scene structure) is integrated to filter noisy prompts or enhance prompt relevance for MVS matching, leading to a lack of originality in how prompts interact with the MVS pipeline.\n2. While the paper claims prompt fusion improves scale consistency, it does not quantify how prompts enhance MVS’s core geometric constraints e.g., 3D structure precision."}, "questions": {"value": "1. How does this prompt design differ from existing methods (e.g., OMNI-DC, Marigold-DC) in terms of integration with MVS’s geometric pipeline?\n2. The advantages of the proposed method over depth completion methods are not clearly observable in Table 2, and what is the motivation for adopting the mentioned MVS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dkyGhooowS", "forum": "u34JCBCAfN", "replyto": "u34JCBCAfN", "signatures": ["ICLR.cc/2026/Conference/Submission16275/Reviewer_PRcF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16275/Reviewer_PRcF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762178808544, "cdate": 1762178808544, "tmdate": 1762926421018, "mdate": 1762926421018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}