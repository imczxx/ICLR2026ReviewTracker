{"id": "K7DCGOzSVW", "number": 20983, "cdate": 1758312352659, "mdate": 1763389160848, "content": {"title": "Universal Likelihood Rewards for LLM Reasoning", "abstract": "Fine-tuning large language models (LLMs) for reasoning is usually done by reinforcement learning on reasoning benchmarks, with a specific reward function, often binary, for each benchmark.  Here, we systematically investigate chain-of-thought (CoT) training in LLMs using rewards derived from the probability or log-probability of emitting the reference answer (or any other prompt continuation present in the data). Several recent works have advocated for the use of similar rewards (e.g., VeriFree, JEPO, RLPR, NOVER), which have the advantage of not relying on specific verifiers and being available at scale.\nWe systematically compare such probability-based variants with standard baselines, testing performance both on standard mathematical reasoning benchmarks, and on long-form answers where no external verifier is available. We find that using the \\emph{log-probability} of the reference answer as the reward for CoT learning is the only option that performs well in all setups.  This is also consistent with the next-token log-likelihood loss used during pre-training.\nIn verifiable settings, log-probability rewards bring comparable or better success rates than reinforcing with standard binary rewards, and\nyield much better perplexity. In non-verifiable settings, they perform on par with SFT. On the other hand, methods based on probability, such as VeriFree, flatline on non-verifiable settings due to vanishing probabilities of getting the correct answer.\nOverall, this establishes log-probability rewards as a viable method for CoT fine-tuning, bridging the short-answer, verifiable and long-answer, non-verifiable settings.", "tldr": "We investigate likelihood-based rewards for LLM reasoning, applying them to both verifiable (mathematical) questions, as well as long-form nonverifiable domains.", "keywords": ["reinforcement learning", "reasoning", "large language models", "verifiable rewards", "nonverifiable rewards"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/0e352c1804fae52e26ab9618aae9af36e7f9e1ec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores **reward design beyond binary 0/1 signals** in LLM reinforcement learning, studying how denser reward schemes can affect training. It provides an empirical comparison of multiple reward formulations and shows that richer reward signals can lead to better performance and faster convergence."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "It’s definitely worth studying **reward design for LLM RL** beyond binary feedback, since denser rewards can provide stronger learning signals and potentially improve both performance and convergence speed. This paper delivers a solid **empirical study** comparing different reward formulations in this space."}, "weaknesses": {"value": "+ The paper mainly reports empirical findings but provides **limited deeper insights**. It would be more engaging to see either a theoretical explanation or a more detailed empirical breakdown of *why* certain reward designs work better in specific ways.  \n+ Some **technical details are important** and deserve discussion. In short, some commonly-adopted techniques can mitigate the disadvantage of 0/1 reward, and as people almost always use them, it would be nice to do experiments with them too:\n  - Do you use **dynamic filtering for GRPO** (i.e., filtering out prompts with zero-variance rewards)? Without it, the 0/1 reward baseline is at a disadvantage since all-0 or all-1 prompts contribute nothing to the gradient, effectively reducing batch size. This issue is naturally less severe for your dense reward, but since dynamic filtering is now common in LLM RL, it’d be nice to see results with it enabled.  \n  - Similarly, using **D = 4** for GRPO may be too small. As shown in [https://arxiv.org/abs/2510.01180](https://arxiv.org/abs/2510.01180), increasing D raises the chance that a prompt contributes meaningfully to the gradient. Recent setups often use D ≥ 16, which would make the comparison fairer."}, "questions": {"value": "It would be nice to also **cite [https://arxiv.org/abs/2505.11080](https://arxiv.org/abs/2505.11080)** and discuss how your approach connects or differs from it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rl66vOD4sc", "forum": "K7DCGOzSVW", "replyto": "K7DCGOzSVW", "signatures": ["ICLR.cc/2026/Conference/Submission20983/Reviewer_qY12"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20983/Reviewer_qY12"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760662935960, "cdate": 1760662935960, "tmdate": 1762999998833, "mdate": 1762999998833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We thank the reviewers for their insightful reviews. We are withdrawing this paper and will improve upon it, incorporating your remarks."}}, "id": "ZNhmrha5Cd", "forum": "K7DCGOzSVW", "replyto": "K7DCGOzSVW", "signatures": ["ICLR.cc/2026/Conference/Submission20983/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20983/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763389160009, "cdate": 1763389160009, "tmdate": 1763389160009, "mdate": 1763389160009, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies probability-based reinforcement learning (RL) rewards for chain-of-thought (CoT) training for both verifiable and non-verifiable tasks. It compares probabilities-based rewards against standard binary rewards and supervised fine-tuning. The main findings include log-probability rewards overall generalize across settings and tasks, by achieving best performance and stable CoT behavior."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a comprehensive and systematic study of RL training for chain-of-thought reasoning, examining various reward formulations derived from the (log-)probabilities of reference answers.\n\n- It offers several insightful empirical findings. Notably, using log-probability as the reward signal proves consistently more effective across tasks. In addition, the work highlights a interesting phenomenon where RL training tends to shorten or even eliminate CoTs in non-verifiable domains, while CoTs recover naturally in verifiable settings."}, "weaknesses": {"value": "- The paper is primarily empirical and does not introduce a novel training algorithm or method beyond analyzing different probability-based reward formulations.\n\n- The proposed approaches depend on the availability of reference or ground-truth answers, which restricts their applicability to tasks where such data exist. This dependency limits the diversity and scalability of training data, especially for non-verifiable tasks with multiple valid answers.\n\n- The evaluation can be strengthened by including additional task performance metrics such as accuracy or pass@k, beyond the current set of reported measures.\n\n- The presentation of results needs improvement. Tables 1 and 2 are difficult to interpret, with unclear labeling, multiple boldfaced entries per row and column, and some missing values (e.g., in Table 2). Clearer explanations are needed for the reported metrics and the missing results."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oSz6EQ3AiM", "forum": "K7DCGOzSVW", "replyto": "K7DCGOzSVW", "signatures": ["ICLR.cc/2026/Conference/Submission20983/Reviewer_hVRZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20983/Reviewer_hVRZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880205654, "cdate": 1761880205654, "tmdate": 1762999999245, "mdate": 1762999999245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript investigates the use of log-probability as a universal reward for RL post-training of LLMs. Specifically, the authors evaluate the effectiveness of log-probability–based rewards across multiple LLMs on both verifiable and non-verifiable tasks. Experimental results demonstrate that log-probability consistently outperforms alternative reward formulations, such as raw probability, highlighting its potential as a more stable and generalizable reward metric for LLM post-training."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "This manuscript conducts extensive experiments comparing probability and log-probability as reward signals in LLM RL post-training. The results, benchmarked against SFT and Base RL methods, demonstrate the superiority of using log-probability as the reward formulation."}, "weaknesses": {"value": "1. My primary concern lies in the writing and organization of the manuscript. The presentation lacks clarity, and several tables are difficult to interpret. For instance, in Tables 1 and 2, the key metric of accuracy is overshadowed by many less relevant metrics such as perplexity, which distract from the main results. In my opinion, the current version is not yet ready for publication at ICLR.\n\n\n2. The conclusion is also insufficiently supported. The paper claims that log-probability can serve as a universal reward signal for LLM RL post-training; however, the experimental results show only marginal improvements over the base RL baseline. Moreover, the evaluation is limited to 3B-scale models and a small set of datasets, which weakens the overall persuasiveness of the conclusion."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kbCdJ05vAO", "forum": "K7DCGOzSVW", "replyto": "K7DCGOzSVW", "signatures": ["ICLR.cc/2026/Conference/Submission20983/Reviewer_F7WB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20983/Reviewer_F7WB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896427425, "cdate": 1761896427425, "tmdate": 1762999998942, "mdate": 1762999998942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically evaluates probability-based rewards for reinforcement learning in LLMs, focusing on using the likelihood of the reference answer conditioned on the generated CoT. It compares log-probability, average log-probability, probability (VeriFree), average probability (RLPR), and JEPO against base RL (RLOO) and SFT across verifiable math tasks (MATH, DeepScaleR) and non-verifiable long-form datasets (Alpaca, a proof subset of NuminaMath), using Llama-3.2-3B and Qwen-2.5-3B. Metrics include success rates, log-likelihood/perplexity, and CoT length. The authors finally claimed that log-probability rewards are the only variant that works well across both regimes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear and well-scoped research topic.\n\n2. Thorough empirical comparison. Multiple reward designs (log-prob, avg-log-prob, prob, avg-prob, JEPO) against both RL and SFT baselines. Multiple metrics, including perplexity/log-prob estimates with MC approximations.\n\n3. Insightful analysis of training dynamics."}, "weaknesses": {"value": "1. Limited novelty in the core idea. Prior works (VeriFree, RLPR, JEPO) have explored likelihood-based signals. The main novelty here is a careful, unified comparison and the empirical conclusion that log-prob rewards are the most robust across regimes. This is valuable but incremental.\n\n2. The evaluation metric of perplexity on non-verifiable tasks is not convincing. Suggest adding LLM-judge or human evaluation (or automatic structural/proof verifiers where possible) for long-form tasks to show actual quality gains or parity.\n\n3. Only 3B instruction models are considered; results may not transfer to larger/production-grade models. Suggest adding at least one larger model (e.g., 7B/8B) to demonstrate scalability.\n\n4. The failure of pure probability rewards on long answers is somewhat expected (vanishing probabilities). It would be useful to test simple fixes: 1) Log transform (e.g., log1p), likelihood ratio against a frozen reference model, or per-sample normalization to stabilize scales. 2) Entropy or variance-aware control variates beyond RLOO.\n\n5. The gradient splits into a REINFORCE term plus a direct SFT-like term. An ablation that removes the direct SFT term would clarify whether the benefits stem from the policy gradient on CoT vs pure SFT on answers conditioned on CoTs."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eKQ2aWULin", "forum": "K7DCGOzSVW", "replyto": "K7DCGOzSVW", "signatures": ["ICLR.cc/2026/Conference/Submission20983/Reviewer_fchY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20983/Reviewer_fchY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762096988751, "cdate": 1762096988751, "tmdate": 1762939449977, "mdate": 1762939449977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}