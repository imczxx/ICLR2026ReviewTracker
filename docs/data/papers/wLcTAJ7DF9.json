{"id": "wLcTAJ7DF9", "number": 5457, "cdate": 1757911695774, "mdate": 1759897973511, "content": {"title": "Multi-ReduNet: Interpretable Class-Wise Decomposition of ReduNet", "abstract": "ReduNet has emerged as a promising white-box neural architecture grounded in the principle of maximal coding rate reduction, offering interpretability in deep feature learning. However, its practical applicability is hindered by computational complexity and limited ability to exploit class-specific structures, especially in undersampled regimes. In this work, we propose Multi-ReduNet and its variant Multi-ReduNet-LastNorm, which decompose the global learning objective into class-wise subproblems. These extensions preserve the theoretical foundation of ReduNet while improving training efficiency by reducing matrix inversion costs and enhancing feature separability. We provide a concise theoretical justification for the class-wise decomposition and show through experiments on diverse datasets that our models retain interpretability while achieving superior efficiency and discriminative power under limited supervision. Our findings suggest that class-wise extensions of ReduNet broaden its applicability, bridging the gap between interpretability and practical scalability in deep learning.", "tldr": "We propose Multi-ReduNet, an interpretable white-box model that achieves efficient, class-wise feature learning with improved separability.", "keywords": ["interpretable machine learning", "white-box neural networks", "ReduNet", "Multi-ReduNet"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6fa069629f0f4654a351efb15e6e6d3dfea982ac.pdf", "supplementary_material": "/attachment/29bb388970b1675afaf600af38553fef413335f4.zip"}, "replies": [{"content": {"summary": {"value": "This paper improves the objective of ReduNet under the assumption of “undersampled regimes” and proposes two extensions, Multi-ReduNet and Multi-ReduNetLastNorm, for computational efficiency and representation separability"}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well-structured.\n2. The mathematics is generally written in a professional way."}, "weaknesses": {"value": "Major:\n\n1. The motivation is quite weak. In the introduction, the authors claim representation learning in the “undersampled regimes” is challenging, but without citing any literature or explaining whether this is a widely-accepted concern.\n\n2. Similarly, the questions to be tackled are also unclear. The authors do not define and thoroughly explain “class-specific structures” which they highlight as missing components in previous works.\n\n3. The contributions seem to be incremental and trivial. This paper only performs slight improvements on ReduNet under niche and small-scale settings. It does not bring in novel tools or appealing theoretical insights other than decomposing ReduNet’s objectives, either. In fact, the authors said in line 255:  “Although the class-orthogonality property of MCR2 optima has been established in prior work (Chan et al., 2021), our proof leverages a simpler and more streamlined argument.”, meaning the findings have already been established. These all make me question its significance.  \n\n4. The technical soundness is also questionable. The gradients derived in lines 274 and 277 are basically the same, subject to different scaling. I hardly believe they have a significant functional difference, which makes the decomposition in line 269 less compelling.\n\nMinor:\n\n1. No experimental details and discussion on limitations.\n\n2. Figures are quite hard to interpret immediately due to font size and colorization."}, "questions": {"value": "1. I’m confused about the last two sentences from lines 32 to 35. Why scenarios where the number of features exceeds the number of samples will lead to overfitting and unstable generalization. Is there literature supporting this claim? The explanations of the background are missing.\n\n2. In Theorem 1, why does $Z^i( Z^j )^T= 0$ mean class-orthogonality? This seems to be misaligned with line 172. Should it be $( Z^i )^T Z^j= 0$?\n\n3. How does the last equality hold in Eq.(2) under the assumption $(Z^{*j_1})^TZ^{*j_2} \\neq 0$?\n\n4. The whole analysis in the paper assumes $m \\ll d$. Is this assumption even practical for most tasks? I won’t buy it if it’s just an idealistic setting, and rare in practical scenarios. \n\n5. Why compare to variants with a random forest classifier?\n\n6. How much does training time improve numerically? From figure 1, the proposed model is almost the same with ReduNet."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h6hNbr9PDQ", "forum": "wLcTAJ7DF9", "replyto": "wLcTAJ7DF9", "signatures": ["ICLR.cc/2026/Conference/Submission5457/Reviewer_bnBb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5457/Reviewer_bnBb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761283826532, "cdate": 1761283826532, "tmdate": 1762918074284, "mdate": 1762918074284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends ReduNet — a theoretically grounded, interpretable architecture based on the principle of maximal coding rate reduction — to address its limitations in scalability and class-specific representation. The authors propose Multi-ReduNet and Multi-ReduNet-LastNorm, which decompose the global learning objective into class-wise subproblems. This decomposition maintains ReduNet’s theoretical interpretability while substantially improving computational efficiency by lowering matrix inversion costs. Moreover, it enhances feature separability, making the model more effective in undersampled  regimes. The paper provides theoretical justification for this class-wise formulation and demonstrates empirically, across multiple datasets, that the proposed models preserve interpretability and achieve better efficiency and discriminative power."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe empirical results align well with the theoretical analysis, and Multi-ReduNet demonstrates significant performance improvements in the undersampled regime.\n2.\tThe writing is clear, well-structured, and easy to follow.\n3.\tThe theoretical analysis seems rigorous, and I did not find errors in the proofs."}, "weaknesses": {"value": "1.\tThe experiments in this paper are primarily conducted on toy datasets such as MNIST. I believe it is necessary to include experiments on more realistic datasets, such as CIFAR. In addition, I am curious about the performance of Multi-ReduNet in the oversampled regime — is it still competitive under such conditions?\n2.\tThe motivation behind Multi-ReduNet is clear and intuitive. However, the rationale for introducing Multi-ReduNet-LastNorm is somewhat unclear, as there is no theoretical comparison between the two. It would strengthen the paper to include a clearer discussion on the logical progression from Multi-ReduNet to Multi-ReduNet-LastNorm.\n3.\tI noticed that the main proofs are presented in the main text. This makes the paper rather math-heavy and potentially difficult to follow. It might be preferable to include simplified versions of the proofs in the main paper and move the detailed derivations to the appendix.\n4.\tIn Figure 2, the representations learned by ReduNet appear well-clustered. However, Table 2 reports relatively low classification accuracy for the same model. Did I miss something here? It would be helpful to clarify the reason behind this apparent discrepancy between the results.\n5.\tThe authors state that “Although the class-orthogonality property of MCR² optima has been established in prior work (Chan et al., 2021), our proof leverages a simpler and more streamlined argument.” I view this as one of the main theoretical contributions of the paper. However, it remains unclear in what sense the proof is simpler. A more detailed explanation or discussion would strengthen this claim.\n6.\tThe presentation quality of the paper could be improved. For example, the title in Figure 2 is too small and difficult to read. Moreover, if the detailed proofs are moved to the appendix, it would be beneficial to include more discussion or intuitive explanations in the main text to improve readability and accessibility.\n7.\tIn summary, this paper presents a theoretically grounded and interpretable extension of ReduNet with promising results. However, the work could be significantly strengthened through better presentation, clearer motivation for the proposed variants, and more comprehensive experiments on realistic datasets."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TLkaK61WoB", "forum": "wLcTAJ7DF9", "replyto": "wLcTAJ7DF9", "signatures": ["ICLR.cc/2026/Conference/Submission5457/Reviewer_KGJT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5457/Reviewer_KGJT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761628346735, "cdate": 1761628346735, "tmdate": 1762918073904, "mdate": 1762918073904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes two class-wise extensions to ReduNet, namely, Multi-ReduNet and Multi-ReduNet-LastNor, to improve the performance under undersampled, high-dimensional conditions. The authors theoretically justify that the global MCR² objective can be decomposed into class-specific subproblems and leverage this result to design more efficient and class-discriminative models. Experiments on multiple datasets show consistent improvements in classification accuracy and training efficiency, while maintaining interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper gives a theoretical analysis showing that the MCR² objective under certain conditions can be equivalently decomposed into class-wise subproblems. \n2. The proposed models maintain the white-box property of ReduNet and preserve its forward-only optimization strategy. This makes the approach more transparent and easier to analyze compared to conventional backpropagation-based deep networks.\n3. The class-wise decomposition reduces the cost of matrix inversion from high-dimensional global matrices to smaller class-specific ones, which is computationally advantageous in settings with limited data and large feature dimensionality."}, "weaknesses": {"value": "1. The method assumes that the class-wise decomposition is meaningful under undersampled regimes. However, the paper does not investigate how the approach behaves when this assumption is less valid, e.g., when the sample size is moderately large.\n2. The evaluation focuses on relatively simple or small-scale datasets (e.g., MNIST, Fashion-MNIST), which may limit the conclusions.\n3. The experimental comparisons are restricted to ReduNet and its variants. The paper could be strengthened by comparing against a broader set of interpretable or class-structured learning methods, to better situate the approach within the existing literature.\n4. Although Multi-ReduNet-LastNorm performs slightly better in most cases, the role of the final-layer-only normalization is not fully analyzed. It would be helpful to understand under what circumstances this variant is preferable."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZtgbYlXI1q", "forum": "wLcTAJ7DF9", "replyto": "wLcTAJ7DF9", "signatures": ["ICLR.cc/2026/Conference/Submission5457/Reviewer_tTgf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5457/Reviewer_tTgf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652330065, "cdate": 1761652330065, "tmdate": 1762918073653, "mdate": 1762918073653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors improve the scalability of the classifier ReduNet, which is specialized for solving tasks where there are a lot more features $d$ than samples $m$. Given $K$ classes and $m_j$ samples for class $j$, new algorithms scales as $\\mathcal{O}(\\sum_{j=1}^Km_j^3)$ compared to the $\\mathcal{O}(Kd^3)$ of the original algorithm. The scalability and performance is demonstrated on 4 real datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper does a great job at explaining the ReduNet method to readers without prior knowledge on the field.\n\nThe theoretical contribution of the paper is significant since the new algorithm scales as $\\mathcal{O}(\\sum_{j=1}^Km_j^3)$ compared to the original $\\mathcal{O}(Kd^3)$.\n\nThe improvements in accuracy are also significant compared to prior work."}, "weaknesses": {"value": "## Orthogonality Constraints\n\nAt line 270 it is stated that the optimization problem outlined at line 269 is solved \"subject to class-wise orthogonality and norm constraints\".\nI don't think the manuscript clarifies how these constraints are applied in practice. The proposed algorithm iteratively solves the equation at line 269 in order to infer the representation $Z_j$ of each class. The $Z_j$ are updated independently by maximizing a separate objective so it is not clear how orthogonality is enforced.\n\n## A smoother-introduction to Multi-ReduNet\n\nThe paper actually has two contributions : Imp-ReduNet which changes the inversion of a $d\\times d$ matrix to a $m\\times m$, and\nMulti-Redunet which furthers improves this to $K$ inversions of $m_j\\times m_j$ matrices. But the order in which these contributions are presented in confusing : multi-redunet is presented first and then imp-redunet. I think that describing imp-redunet first would help motivate the need to separate the objective into $K$ objectives.\n\nBy introducing imp-redunet first (via lemma 1) it is clear to the reader that the computational bottleneck is inversion of a $m\\times m$ matrix to compute the gradient of $\\text{log det}(I+\\alpha Z Z^T)$. If we were able to replace this with $K$ terms $\\text{log det}(I+\\alpha Z_j Z_j^T)$, we could inverse $K$ matrices of shape $m_j\\times m_j$ instead, which is a lot better. After this initial high-level motivation, and hinting that $Z Z^T=\\sum_{j=1}^K Z_j Z_j^T)$, the main theorems can be presented.\n\nThis is a subtle change, but I think it will improve the flow of the paper significantly.\n\n## Technical Overload\n\nOn a similar topic, the paper would benefit from moving some technical content to the appendix and focusing more on high-level ideas in the main manuscript. Notably, the proof of the Theorem could be moved in the appendix, and replaced with a high-level proof description.\nThis description would only need to accentuate the most crucial parts of the proof e.g. that $\\text{log det} (I+\\alpha Z Z^T)= \\text{log det} (I+\\alpha \\sum_{j=1}^K Z_j Z_j^T) =  \\sum_{j=1}^K \\text{log det} (I+\\alpha Z_j Z_j^T)$ whenever class representations are orthogonal.\nThis clarifies to the reader that class-orthogonality is the key assumption to separate the objective into $K$ sub-objectives.\n\nThe freed space in the main manuscript could be used to introduce intuition for ReduNet e.g. extended content from the Appendix B.\n\n## Figures 1\n\nThis is a very minor point, but I think that Figure 1 is hard to read. It is hard to see the color of the markers because it is very dark. Also, the marker color does not match with the line, so I constantly have to read the legend to be sure. Moreover, methods based on RF perform so bad that they hide the differences between ReduNet and Multi-Redunet (the main contributions of the paper). I would suggest removing the RF methods from the plot and simply indicate in the text that they take orders of magnitude more time."}, "questions": {"value": "What is the $d_j$ in theorem 1? It is not introduced before being used.\n\nTheorem 1 defines class-orthogonality as $Z_j Z_i^T=0$. Shouldn't is be $Z_j^T Z_i=0$ instead following Corollary 1? \n\nWhy is the Frobenius norm $||Z_j||^2_F\\leq m_j$ bounded in theorem 1 but the optimization algorithm projects to the unit sphere? Projecting $m_j$ points on the unit sphere guarantees that $||Z_j||^2\\leq m_j$ , but the converse is not true. Perhaps a more in-depth introduction to ReduNet in the main manuscript would help.\n\nFrom my understanding, multi-redunet an improvement in terms of scalability : it solves the same problem as ReduNet but more efficiently. Then how can multi-redunet perform better than redunet in terms of accuracy? Is ReduNet reaching a different optimum?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "faPC0OjmFf", "forum": "wLcTAJ7DF9", "replyto": "wLcTAJ7DF9", "signatures": ["ICLR.cc/2026/Conference/Submission5457/Reviewer_d7rQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5457/Reviewer_d7rQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761769185046, "cdate": 1761769185046, "tmdate": 1762918073420, "mdate": 1762918073420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}