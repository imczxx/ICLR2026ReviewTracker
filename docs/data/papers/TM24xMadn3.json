{"id": "TM24xMadn3", "number": 22890, "cdate": 1758336756587, "mdate": 1759896841191, "content": {"title": "MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation", "abstract": "Vision-Language-Action (VLA) models trained on large robot datasets promise general-purpose, robust control across diverse domains and embodiments. However, existing approaches often fail out-of-the-box when deployed in novel environments, embodiments, or tasks. We introduce Mixture of Skills VLA (MoS-VLA), a framework that represents robot manipulation policies as linear combinations of a finite set of learned basis functions. During pretraining, MoS-VLA jointly learns these basis functions across datasets from the Open X-Embodiment project, producing a structured skill space. At test time, adapting to a new task requires a single expert demonstration: the corresponding skill representation is inferred via a lightweight convex optimization problem that minimizes action L1 error, without any gradient updates. This gradient-free adaptation incurs minimal overhead while enabling rapid instantiation of new skills. Empirically, MoS-VLA achieves lower action-prediction error on five out of five unseen datasets and succeeds in both simulation and real-robot tasks where a pretrained VLA model fails outright.", "tldr": "Learning a linearly composable skill space during VLA pretraining enables 1-shot domain adaptation.", "keywords": ["Vision-Language-Action Models", "Skill Learning", "Gradient-Free Adaptation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/301de6e85a778c2f2235f6692190d7f2ed587b0d.pdf", "supplementary_material": "/attachment/f669604869c14ce9f78371648aed0e0a681f1e23.zip"}, "replies": [{"content": {"summary": {"value": "The paper MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation introduces a method that helps robots quickly adjust to new environments and tasks using only one demonstration. Instead of retraining large models through expensive fine-tuning, MoS-VLA represents robot control policies as combinations of smaller learned skills, called basis functions. During training, these basis functions are learned from diverse robot datasets to form a flexible skill space. When facing a new task, the model calculates a simple optimization to find the right mix of skills, without any gradient updates. This allows fast and efficient adaptation, even on regular hardware. Experiments show that MoS-VLA achieves lower prediction errors on multiple unseen datasets and succeeds in both simulation and real-world robot tasks where standard models fail. The work demonstrates that one-shot adaptation can effectively transfer learned abilities to new conditions and helps bridge the gap between large-scale training and practical deployment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes *MoS-VLA*, a vision-language-action (VLA) framework that achieves one-shot skill adaptation through a gradient-free convex optimization procedure. The motivation is strong—reducing the high computational overhead of fine-tuning large VLA models for new environments is a relevant and timely challenge in embodied robotics. The method is clearly described, mathematically grounded in the function encoder formalism, and demonstrates an elegant way to represent policies as combinations of learned basis functions. The architecture and training pipeline are systematically presented, and both simulated and real-robot experiments confirm the feasibility of lightweight adaptation. The paper is well written, conceptually coherent, and provides promising insights into how structured skill spaces can support efficient policy transfer in large embodied models, even though its empirical validation remains limited in scale and complexity."}, "weaknesses": {"value": "1.The real world tests (block lifting, goal reaching, pen insertion) are very short-horizon and low-complexity. These setups mainly assess spatial calibration, not compositional reasoning or long-horizon manipulation, such simple tasks can also be solved by small sclae reinforcement learning policy, making it unclear whether the proposed one-shot adaptation method scales to realistic long-horizon or contact-rich tasks, in which the VLA outperform recent tranditional policy learning framework.\n\n2.Only a single robot platform (Franka Panda) and three real-world tasks are reported, each adapted from demonstration by one expert trajectory. There is no systematic analysis of robustness to in-the-wild scenarios such as noisy demonstrations, viewpoint shifts, or sensor noise, which limits confidence in the generality of the claimed one-shot transfer.\n\n3.The paper compares only to OpenVLA and not to other lightweight adaptation strategies (e.g., LoRA-based finetuning, prompt-conditioning, diffusion-based adapters, or in-context imitation frameworks). Without these, the improvement magnitude is difficult to interpret.\n\n4.Key hyperparameters such as the number of basis functions, choice of L1 vs. L2 objective, and calibration buffer size are fixed without detailed justification. No experiments isolate how these factors affect adaptation speed or accuracy.\n\n5.The central algorithm directly builds on function encoder algorithm Ingebrand et al. (2025) with modest architectural adjustments (using transformer basis heads). The contribution therefore feels incremental rather than theoretically new.\n\nOverall In summary, while the proposed approach is seemingly interesting, the limited and small-scale experiments fail to convincingly demonstrate its effectiveness compared with the rapidly advancing state-of-the-art in VLA."}, "questions": {"value": "1.The key contribution for this paper is inherently introducing a one-shot adaptation method in the VLA domain that require only one expert demonstration for new task adaptation. As I understand, this method should be framework agnostic, did author attempt using other open-sourced VLA models for testing (e.g., pi0)? Or this method only valid for OpenVLA?\n\n2.Can the authors provide larger-scale real-robot evaluations or report robustness tests (e.g., noisy demonstrations, camera shifts, or unseen lighting conditions) to validate one-shot adaptation under real-world unstructured environments? It is best to provide the complete real-world demonstration videos via an anonymous link.\n\n3.Why are recent lightweight adaptation baselines, such as LoRA-based fine-tuning, prompt-conditioning, or in-context imitation approaches, not included for comparison? How would MoS-VLA perform relative to them? Comparison with recent state-of-the-art open-sourced VLA models is necessary, including at least diffusion policy, OpenVLA-oft, pi0, pi0.5.\n\n4.How sensitive is MoS-VLA to the number of basis functions, the choice of L1 vs. L2 loss, and the calibration buffer size? Could the authors provide ablation or scaling studies to support these design choices?\n\nI reserve the possibility of raising my score if the authors can adequately address all the concerns raised during the rebuttal phase."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IozTliAv5N", "forum": "TM24xMadn3", "replyto": "TM24xMadn3", "signatures": ["ICLR.cc/2026/Conference/Submission22890/Reviewer_qder"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22890/Reviewer_qder"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705214273, "cdate": 1761705214273, "tmdate": 1762942426776, "mdate": 1762942426776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a one-shot, gradient-free adaptation method for vision–language–action (VLA) models that substantially reduces computational overhead during online adaptation. The core idea is a function-encoder approach: multiple basis functions are pretrained to span the policy space, and at adaptation time, only the basis coefficients are fine-tuned by solving a linear program. Experiments indicate that the method significantly improves OpenVLA performance on out-of-distribution VLA tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "First, the paper is clearly written and easy to follow. The paper adopts a function-encoder algorithm with intuitive appeal and theoretical grounding, and scales it to a modern VLA architecture capable of handling high-dimensional vision–language inputs. Finally, the paper provides extensive experimental evaluations, including the real robot experiments."}, "weaknesses": {"value": "- While the paper emphasizes computational efficiency, a quantitative analysis is missing. Comparison against fine-tuning baselines (e.g., LoRA tuning) in both accuracy and efficiency (e.g., wall-clock time, FLOPs, memory/VRAM usage) should be included to support the main claim.\n\n- The paper states that “transformer basis functions” are used, yet the bases appear to differ only in their output heads. According to Appendix C, these are effectively two-layer MLP heads on top of a pretrained VLA transformer’s feature space. Are such two-layer MLP bases sufficient to span the space of robot tasks more broadly? Under what conditions does the approach fail to generalize?\n\n- Although computational overhead scales linearly with the number of bases, achieving adequate expressivity may require many bases. How does performance change as the number of bases increases?\n\n- Evaluating with an L1 loss may be misleading because the action vector comprises distinct components (position, angle, and gripper state). It may also advantage the proposed method, which directly targets L1 minimization. Task-level metrics such as success rate would provide a more comprehensive and fair evaluation.\n\n- Because the proposed approach introduces additional parameters and computation relative to vanilla OpenVLA (e.g., extra LoRA parameters and basis heads), a fair comparison should include a fine-tuned OpenVLA variant with matched parameter counts (same LoRA configuration and head size) and matched optimization budget (e.g., identical numbers of gradient steps)."}, "questions": {"value": "Please refer to the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1Maa3rvXeY", "forum": "TM24xMadn3", "replyto": "TM24xMadn3", "signatures": ["ICLR.cc/2026/Conference/Submission22890/Reviewer_Y2p9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22890/Reviewer_Y2p9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890917917, "cdate": 1761890917917, "tmdate": 1762942426502, "mdate": 1762942426502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MoS-VLA, a novel way of adapting vision-language-action (VLA) models via basis functions. The idea of this paper stems from the mixture of experts approach in language models, in which instead of using one action head from OpenVLA, you use multiple action heads and take a linear combination of them to reproduce the expert actions. This has smaller overhead (thanks to parallelization) and can learn a more expressive set of actions by learning both the basis actions (by ensuring orthogonality between each basis) and the action weights. When deployed on a set of simulated and real world manipulation tasks, MoS-VLA is able to distill a useful set of action representations to adapt to a new task with only one demonstration. As a result, MoS-VLA is able to exhibit nonzero success rate in both simulated and real world tasks while the barebone policy cannot complete the task at all."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. I liked the analysis that you have performed across the OXE dataset, which demonstrated the need of many basis functions and the role coefficient space plays.\n2. The paper itself flows very naturally. I am able to know most about the method in the first pass, which I haven’t been able to do for many other papers.\n3. Both simulated and real world settings are evaluated, which gives reproducible insights into the performance of MoS-VLA."}, "weaknesses": {"value": "1. I believe there are better ways to address some of the design choices and reasons for mentioning terminology. For example, the paper discussed Banach space structure, although I have not seen relevant work that is being cited. I also believe that there are better empirical works that justify why using L1 regression is better, as shown in works such as OpenVLA-OFT [1].\n2. Additionally, some of the terms such as action are being ill-defined here, as OpenVLA used discrete action outputs, and from what I can parse here, you have used continuous action outputs. \n3. Building upon (2), this might be one of the reasons why you are able to achieve a lower L1 validation error when compared to OpenVLA: if you were to simply decode your actions from output tokens, then it is inherently less expressive than a set of MLPs (which are going to be more parameterized than that of the single action head + detokenizer with vanilla OpenVLA). \n\nPlease correct me if there any conceptual misunderstandings from my part.\n\nReferences:\n\n[1] Kim, M. et al,. 2025. “Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success”. RSS."}, "questions": {"value": "1. Are there any concerns about overfitting in your method’s setup? Additionally, is it possible to demonstrate that if you have more demonstrations, MoS-VLA’s performance does not degrade?\n2. Is there any way to show what each action head is attending to when rolling out a policy? I believe this can be interesting to see the interpretability aspect of the method.\n3. How does this method compare against other adaptation methods that are available?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5MsPBx6R3C", "forum": "TM24xMadn3", "replyto": "TM24xMadn3", "signatures": ["ICLR.cc/2026/Conference/Submission22890/Reviewer_awN2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22890/Reviewer_awN2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902168241, "cdate": 1761902168241, "tmdate": 1762942426231, "mdate": 1762942426231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose \"Mixture of Skills VLA\" (MoS-VLA), a framework that models a robot's policy as a linear combination of a few learned basis functions (or \"skills\"). These basis functions in practice resemble many different action heads after a base VLM (here, the llama 2 model within OpenVLA). At test time, a expert demonstration from a new task is used to infer the the linear coefficients by solving a L1-norm convex optimization problem (which doesnt require gradient updates since it uses cvxpy). MoS-VLA achieves high success rates on 5 new sim-and-real robot tasks where the 0-shot OpenVLA baseline fails."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea to use learned basis functions and some simple convex optimization at test time for adaptation is wonderful. It provides a novel approach to adaptation, perhaps most reminiscent of older control theory papers.\n\nThe results clearly demonstrate, on some simple tasks, that OpenVLA's struggle to zero shot adapt, can be solved with a little bit of convex optimization and a more suitable architecture. \n\nThe t-sne plot visualization of the coefficients is very interesting, and perhaps could be used as a method to measure the quality and diversity of data."}, "weaknesses": {"value": "Experimental evaluation is limited: My first main concern is that, after using many GH200 GPUs for a few hours, the actual evaluation is overly simplified in a couple of ways: (1) very simple block (and one pen) pick and place tasks in the real world, (2) disabling\nforward/backward motions. If I remember correctly, openvla can take as input both robot proprioception and multiple camera views but the authors have chosen to only provide one view and no joint angles as input. Both of these can be easily added to the pipeline, especially given the model itself is trained on the hugely diverse RT-X dataset. \n\nFurther, it might be helpful to compare with and use recent VLAs like pi0 that should be able to complete simpler tasks zero shot. Perhaps the authors can use test on the tasks like those in [1], where pi0 fails, for a more comprehensive evaluation?\n\nMissing related work with similar (or atleast relevant) high-level ideas: A few highly relevant papers on few-shot adaptation are missing from the related work, and possibly from the baselines. These papers also aim at generalization to new tasks and objects with a few-examples, and gradient-free (in-fact, completely optimization-free) updates. RICL [1] would be the closest work of the following [1,2,3,4] to MoS-VLA.\n\n[1] RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models, CoRL 2025\n\n[2] Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation, CoRL 2025\n\n[3] REGENT: A Retrieval-Augmented Generalist Agent That Can Act In-Context In New Environments, ICLR 2025\n\n[4] Generalization to New Sequential Decision Making Tasks with In-Context Learning, ICML 2024"}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4w3McZaMgK", "forum": "TM24xMadn3", "replyto": "TM24xMadn3", "signatures": ["ICLR.cc/2026/Conference/Submission22890/Reviewer_1LCB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22890/Reviewer_1LCB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762179627734, "cdate": 1762179627734, "tmdate": 1762942425863, "mdate": 1762942425863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}