{"id": "ahyVufh4l4", "number": 11222, "cdate": 1758193811071, "mdate": 1759897600172, "content": {"title": "Define latent spaces by example: optimising over the outputs of generative models", "abstract": "Modern generative AI models such as diffusion and flow matching can sample from rich data distributions, but many downstream tasks — such as experimental design or creative content generation — require a higher level of control than unconstrained sampling. The challenge is to efficiently identify outputs that are both probable under the model and satisfy task-specific constraints. We address this by introducing surrogate latent spaces: non-parametric, low-dimensional Euclidean embeddings that can be extracted from any generative model without additional training. The axes in the Euclidean space can be defined via examples, providing a simple and interpretable approach to define custom latent spaces that both express intended features and are convenient to use in downstream tasks. The representation is Euclidean and has controllable dimensionality, permitting direct application of standard optimisation algorithms to traverse the outputs of generative models. Our approach is architecture-agnostic, incurs almost no additional computational cost, and generalises across modalities, including images, audio, videos, and structured objects like proteins.", "tldr": "We introduce surrogate latent spaces and show how they enable effective optimisation over the outputs of generative models", "keywords": ["generative models", "latent space", "surrogate latent space", "latent space optimisation", "black-box optimisation", "experimental design", "latent optimal linear combinations"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fe3777ef321bec273aef146455218a2e56be057f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper deals with sampling of generative models, aiming at identifying outputs that are probable given the estimated density but also that satisfy task-specific constraints. The authors propose to build a simple \"surrogate latent space\" that allows to conduct directly an optimization with regards to various tasks. The approach is evaluated in the context of text-to-image generation and protein design, exhibiting interesting results when one considers the low computation cost of the proposed approach."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* the authors adopt a constructivist approach to define their surrogate latent space, stating by establishing key principles it should support then explaining, step by step, how the space is defined and which properties they assume on the bijective functions that allow the mapping to the latent space of the model. The make a real effort of clarity, referring to the appendices for mathematical details and variations in their approach. Each time, the assumptions and their implementation are well motivated.\n  - they also clearly credit previous works, in particular (Bodin et al., 2024) \n\n* the related work is short but efficiently highlights the novelty of the proposed approach with regards to three families of approaches.\n\n* the proposed approach is agnostic to the generative model considered (as long as it has a latent space with -- quite common -- properties) and thus the type of data handled.\n  - The approach is evaluated in two very different tasks, namely two experiment in text-to-image generation and one for protein design. Each time, the approach is compared to a recent and relevant baseline.\n  - The quantitative results (Table 1) are conducted over 30 repetitions and the results report the median and 90% confidence interval."}, "weaknesses": {"value": "* the experiment on images (section 5.2) is conducted for three prompts only. However, a previous work such as (Denker et al, 2025) report their results on one prompt only. They nevertheless conduct a more ambitious experimetn on the 10 classes of MNIST (although this dataset may not be the most relevant). One must also note that the experimemnt in section 5.3 also deals with text-to-image generation and is conducted at a much larger scale. Hence, the experiment of section 5.2 can be considered as an illustrative example and is thus, in that sense, well conducted. \n  - for future works (*not* rebuttal) one can suggest to conduct a similar experiment on a larger number of prompts and reporting aggregated performance. Reporting the 90% confidence interval for individual prompts is nevertheless more intersting than reporting an \"average median performance and its standard deviation\".\n\n* In Table 1, the quantitative results are better for the proposed approach for two over three prompt, as highlighted, but one must consider various variant of the approach to get this result. However, the proposed approach is much less costly to compute and, in that sense, the results can be considered as intersting in comparison to the threee baselines.\n\n* The results for protein design (Table 2) are reported in the appendix only. One can understand that there is a limited place but the main values should at least be reported in the main text, on lines 467-472.\n\n* minor:\n  - equations line 240 and 323 are not numbered\n  - Snyder (1987) --> (Snyder, 1987) -- or ... problem *of* Snyder (1987)\n  - although Bayesian Optimisation (BO) is defined on line 064, the full term could be used in the related works, line 341 (there is enough place)\n  - references on lines 431 shoul use `\\cite` instead of `\\citet`\n  - reference (Denker et al, 2025) line 510 reports the arxiv preprint while it has been published at Frontiers of Probabilistic Inference workshop at ICLR 2025.\n  - the ICLR 2026 guideline asked for a dedicated section in the appendix explaining possible LLM usage. See \"The Use of Large Language Models (LLMs)\" on [this page](https://iclr.cc/Conferences/2026/AuthorGuide)"}, "questions": {"value": "Overall, the weaknesses identified are fairly minor and could be corrected by slightly reorganizing/rewriting certain sentences."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Ethic concerns are not addressed in the paper. However, there is also no obvious problem with this particular article, beyond what can be said about any article published in ICLR."}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tqoC3nevo9", "forum": "ahyVufh4l4", "replyto": "ahyVufh4l4", "signatures": ["ICLR.cc/2026/Conference/Submission11222/Reviewer_tiXi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11222/Reviewer_tiXi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761036828493, "cdate": 1761036828493, "tmdate": 1762922385457, "mdate": 1762922385457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a method for optimization of deterministic generative models over arbitrary value functions. It does so by constructing a surrogate latent spacing using seed samples, which in combination with blackbox optimization procedures (like BO), recovers higher scoring samples than the seeds themselves, and in comparison to random search in latent space. The method is demonstrated on a number of modalities, including text-conditioned image generation, audio, video, and protein sequences."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an important problem in optimizing arbitrary functions for deterministic-trajectory generative models with high dimensional latent space (e.g., DDIM, flow matching, etc.). The problem is clearly stated in the paper.\n- The proposed solution is elegant and intuitive, appears to perform well on different modalities and without training, and the qualitative results are convincing.\n- The paper is overall clearly written, though some critical sections on methodological details were somewhat dense."}, "weaknesses": {"value": "- In general the paper may be suffering from having too many results with too little space to describe all of them in full, but critical parts are then not sufficiently described.\n- This may be a lack of familiarity on my part, but section 3.1 and 3.2 could perhaps use some detail, as I was confused about the dimensionalities of the intermediate spaces and how a bijective mapping back onto the higher-dimensional latent space (z) is achieved with uniqueness guarantees. Perhaps some graphic intuition would help.\n- The quantitative evaluations could be more systematic, or possibly just more clearly presented. The main result on the quality of the method is Figure 4, which, if I understand correctly, are evaluated over 1 million prompts of the same variety with the 3 different features involving vehicles in different terrains. The method clearly does better than random search, though the scoring seems arbitrary / badly scaled since a black image scores 20. More generally, it does not compare against other, more competitive methods as presented in Table 1, which seems to be only shown for the 3 prompt value functions in the different columns. Similarly, for the other modalities, while examples of the surrogate latent space and samples are given (e.g., for audio and images), I missed their quantitative evaluations, and in relation to other methods. Overall, it’s clear from the results that the proposed method works, but I’m unsure to what extent (how well relative to others) and in what settings. To be clear, I’m not saying it has to beat SOTA on every evaluation, just that it’s somewhat unclear what and how systematic the experimental setups are.\n- Main result from Table 1 is essentially interpreted in one line (line 433), but there seems to be more insights to be gained by comparing across the different setups, and the table lacks highlighting or just some kind of visual guidance to help the readers make their conclusions."}, "questions": {"value": "- What is the dimensionality of epsilon? Is the dimensionality of w = K? If so, how is the transformation to/from the lower-dimensional unit sphere orthant (w) mapped to the original higher-dimensional z- and x-space with bijective mappings (line 210-222)?\n- intuitively, do the K seeds need to satisfy some kind of diversity criterion to construct a  latent space with a meaningful span? If so, how to evaluate this? Table 1 reports results on the quality and diversity of downstream generation with oversampling (e.g., best 1-of-6), but not relative to the quality / diversity of these seeds.\n- are there systematic quantitative evaluations beyond the 3 example prompts in Table 1?\n- Just to clarify, Fig 4 median and CI are computed over 1 million targets from the combination of the 3 attributes?\n- typos on line 395 and 397? 100/500 should be 200 and 600?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HXnDVpvvOE", "forum": "ahyVufh4l4", "replyto": "ahyVufh4l4", "signatures": ["ICLR.cc/2026/Conference/Submission11222/Reviewer_Qm2a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11222/Reviewer_Qm2a"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932350289, "cdate": 1761932350289, "tmdate": 1762922384397, "mdate": 1762922384397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to map high-dimensional representation spaces of deep generative models to low-dimensional latent spaces using example-based transformations. The approach is training-free and can be applied across various generative models for different modalities. It is qualitatively evaluated on image and protein datasets and quantitatively compared with baseline methods that require additional training, achieving comparable performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an interesting problem of enhancing controllability in modern generative AI models when generating new samples, which could broaden their range of applications. Considering that real-world images are often complex, defining latent spaces based on given examples is a reasonable and intuitive idea. \n2. The latent spaces constructed by the proposed method satisfy desirable properties such as validity, uniqueness, and approximate stationarity. \n3. The proposed method is evaluated on two distinct data modalities, and the results are promising."}, "weaknesses": {"value": "1. Since the latent space is determined by the provided examples, it would be helpful to clarify the assumptions or requirements for selecting these examples. For instance, in Figure 4, should the examples collectively cover all key elements (e.g., matte black, dull hovercraft vehicle, vineyards) in the target image? \n2. Table 1 compares several sampling strategies for the proposed method, but it remains unclear which strategy performs best or how to choose among them. Providing selection criteria or guidance would strengthen the paper. \n3. According to Table 1, the proposed method does not significantly outperform baseline methods on quantitative metrics such as Reward and Diversity. While its training-free nature is a clear advantage, the reported training cost of some baselines (e.g., 4 GPU hours for Adjoint Matching) is not excessively high. Furthermore, including the inference or generation time (e.g., the average time to generate a finalized target example) would make the performance comparison more comprehensive."}, "questions": {"value": "See \"Weaknesses\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VNVHQ0Zmq2", "forum": "ahyVufh4l4", "replyto": "ahyVufh4l4", "signatures": ["ICLR.cc/2026/Conference/Submission11222/Reviewer_HMMU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11222/Reviewer_HMMU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994044585, "cdate": 1761994044585, "tmdate": 1762922383721, "mdate": 1762922383721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the task of Latent Space Optimization (LSO) in diffusion/flow-based generative models. Authors argue that naive LSO in such models suffers from (i) Inefficient optimization due to the high dimensionality of the latent space, and (ii) difficulty to  stay on the valid support of the generative model. To address these challenges, authors propose a method for constructing lower-dimensional \"surrogate latent spaces\" for optimization with 3 main properties:\n- **Validity** of all the latent within the space\n- **Uniqueness** of the mapping between each latent in the surrogate space to the generated outputs.\n- **Approximate** stationarity• of the surrogate space, where similarity between the generated objects approximately depends on the euclidean distance between their latents in the proposed space.\n\nThe proposed method constructs a small surrogate search space $U$ around a set of seed examples. To ensure uniqueness, it maps each $u\\in U$ through a Weight Chart (Angular or Knothe–Rosenblatt) to a unit-norm, non-negative weight vector and uses a deterministic inversion/decoding path. To ensure validity, seed latents are combined in an inner latent via Latent Optimal Linear transport and then mapped back to the model latent. The authors argue that $U$ is approximately stationary because (i) the Weight Chart ties similarity largely to the dot product of the unit weights—hence to Euclidean distance in $U$—and (ii) in the isotropic inner latent, cosine similarity concentrates and is well-approximated by that weight dot product.\n\nThe authors evaluate their approach across multiple modalities (images, audio, video, proteins) against baselines operating in the original latent space. The authors also provide ablations and visualizations in support of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method is training-free and is demonstrated across multiple modalities (images, audio, video, proteins).\n\n- The proposed method keeps optimization on the model’s support (on-manifold), addressing a core challenge in latent-space optimization.\n\n- Defining the search space from seed examples yields a more interpretable and steerable optimization process.\n\n- Results\n\t- The method matches or exceeds fine-tuning–based baselines on the evaluated image benchmark without additional training.\n\t- The experiments show standard black-box optimizers (CMA-ES, BO) succeed inside the surrogate space, whereas the same optimizers often fail in the original latent (e.g., CMA-ES collapses to black images).\n\t- In the protein design experiments, it improves recovery metrics and the number/diversity of successful designs compared to standard sampling."}, "weaknesses": {"value": "- Seed dependency\n    - The attainable solution set is limited by the span, diversity, and quality of the chosen seed examples.\n    - Each new optimization target (e.g., prompt or objective) typically requires its own seed set to construct the surrogate space.\n\n- Approximate stationarity lacks a formal guarantee and may degrade as the surrogate dimension grows.\n \n- Minor: The method assumes (quasi-)deterministic inversion/decoding; using stochastic samplers or strong guidance can weaken the uniqueness and approximate stationarity properties."}, "questions": {"value": "See the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "reeL7tfwDv", "forum": "ahyVufh4l4", "replyto": "ahyVufh4l4", "signatures": ["ICLR.cc/2026/Conference/Submission11222/Reviewer_DyF4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11222/Reviewer_DyF4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762699070127, "cdate": 1762699070127, "tmdate": 1762922382737, "mdate": 1762922382737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}