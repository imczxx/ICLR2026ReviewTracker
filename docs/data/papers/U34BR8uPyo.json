{"id": "U34BR8uPyo", "number": 9899, "cdate": 1758147673797, "mdate": 1759897688292, "content": {"title": "STRIDE: Structure and Embedding Distillation with Attention for Graph Neural Networks", "abstract": "Recent advancements in Graph Neural Networks (GNNs) have led to increased model sizes to enhance their capacity and accuracy. Such large models incur high memory usage, latency, and computational costs, thereby restricting their inference deployment. GNN compression techniques compress large  GNNs into smaller ones with negligible accuracy loss. One of the most promising compression techniques is Knowledge Distillation (KD). However, most KD approaches for GNNs only consider the outputs of the last layers and do not consider the outputs of the intermediate layers of the GNNs. The intermediate layers may contain important inductive biases indicated by the graph structure and embeddings. Ignoring these layers may lead to a high accuracy drop, especially when the compression ratio is high. To address these shortcomings, we propose a novel KD approach for GNN compression that we call Structure and Embedding Distillation with Attention (STRIDE). STRIDE utilizes attention to identify important intermediate teacher-student layer pairs and focuses on using those pairs to align graph structure and node embeddings. We evaluate STRIDE on several datasets, such as OGBN-Mag and OGBN-Arxiv, using different model architectures, including GCNIIs, RGCNs, and GraphSAGE. On average, STRIDE achieves a 2.13% increase in accuracy with a 32.3X compression ratio on OGBN-Mag, a large graph dataset, compared to state-of-the-art approaches. On smaller datasets (e.g., Pubmed), STRIDE achieves up to a 141X compression ratio with higher accuracy compared to state-of-the-art approaches. These results highlight the effectiveness of focusing on intermediate-layer knowledge to obtain compact, accurate, and practical GNN models. During the discussion phase, we will privately share the anonymized repo with reviewers and area chairs, and we will release it publicly upon acceptance.", "tldr": "We propose  STRIDE, a new knowledge distillation method that uses attention to align both structure and embeddings of important intermediate layers of GNNs, enabling a high compression ratio (up to 141X)  with the same accuracy as SOTA approaches.", "keywords": ["Graph Neural Networks", "Knowledge  Distillation", "Compression"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/633b4169fe011beebcd5cdc484032aee1ec00c42.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "STRIDE introduces an attention-based knowledge distillation framework for compressing Graph Neural Networks. The core contribution is a method to align both structural and embedding knowledge across all intermediate layers of teacher and student networks without requiring one-to-one layer correspondence. STRIDE uses a trainable attention mechanism to identify important teacher-student layer pairs and projects intermediate representations into a shared latent space. The final loss combines attention-weighted embedding and structural dissimilarity scores. Evaluation on OGBN-Mag, OGBN-Arxiv, and smaller citation networks (Cora, Citeseer, Pubmed) demonstrates gains in accuracy at various compression ratios (2.13% improvement at 32.3× compression on OGBN-Mag; 141× compression matching teacher accuracy on Pubmed)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Handles architectural mismatch elegantly**: Unlike GeometricKD (which requires T_l = S_l), STRIDE accommodates different depths via learned attention. This is practically valuable and well-executed.\n\n2. **Strong empirical results across diverse settings**:\n   - Large graphs (OGBN-Mag, OGBN-Arxiv) with out-of-distribution evaluation\n   - Deep networks (GCNII with 64 layers → 4 layers at 141× compression)\n   - Multiple architectures (RGCN, GAT, GraphSAGE, GCNII)\n   - Consistent improvements with reasonable standard deviations\n\n3. **Thorough ablation studies**: Tables 4-7 and Appendix sections validate each component (structure vs. embeddings, intermediate layers, subspace projection, per-layer projections). This strengthens confidence in design choices.\n\n4. **Visualization of learned attention** (Figure 3): Before/after heatmaps provide intuitive evidence that the attention mechanism learns meaningful layer correspondences.\n\n5. **Multiple evaluation protocols**: Standard ranking metrics (Recall, nDCG, MRR) + practical compression ratios provide comprehensive assessment.\n\n6. **Practical weight initialization technique** (Section 3.2.3): Clever use of attention maps to initialize extremely compressed networks (1-layer). Adds practical value beyond the core distillation method."}, "weaknesses": {"value": "1. **No latency validation for primary motivation**: \n   - Paper opens with urgency of deployment latency (lines 54-73)\n   - Only shows parameter counts and compression ratios\n   - Figure 5 (Appendix) shows parameter-latency correlation for one baseline GCN on one dataset\n   - **Missing**: Actual inference time comparison between STRIDE-compressed student and baselines on representative hardware (GPU, CPU, mobile)\n   - This is a critical gap—compression ratio ≠ latency reduction (depends on hardware, sparsity support, etc.)\n\n2. **Missing comparison with recent strong baselines**:\n   - Cites SA-MLP (Chen et al. 2024a) and KDGCL (Wang & Yang 2024) but doesn't compare against them\n   - Comparisons are limited to LSP, GSP, G-CRD (2020-2022) and CNN methods (Fitnets, AT from 2015-2017)\n   - Recent work may have closed the gap\n\n3. **Computational cost of STRIDE training not analyzed**:\n   - Computing attention (Eq. 3): O(T_l × S_l × d_a)\n   - Computing embedding dissimilarity (Eq. 4): O(T_l × S_l × n × d_a) [expensive for large graphs]\n   - Computing structural dissimilarity via G-CRD: O(T_l × S_l × n^2) or approximate\n   - No wall-clock training time comparison with baselines\n   - For OGBN-Mag (n=1.9M), this could be prohibitive\n\n4. **Attention mechanism design under-justified**:\n   - Why mean-pooling for attention vs. other aggregations (e.g., max, CLS token)?\n   - Why row-wise softmax? (Could apply column-wise or global softmax)\n   - Why separate projection matrices for each layer? (Table 7 shows ablation, but not theoretical justification)\n   - No sensitivity analysis on these choices\n\n5. **Weak Modified STRIDE baseline** (Table 5):\n   - Compares learned attention vs. last-layer-only alignment\n   - Doesn't isolate: **contribution of attention weighting** vs. **contribution of intermediate-layer alignment**\n   - Better ablation: uniform attention on all intermediate layers\n\n6. **Subspace projection P is ad-hoc**:\n   - Described as \"alleviating issues from high-dimensional spaces\" but not rigorously justified\n   - No explicit low-rank constraint; relies on implicit regularization\n   - Modest improvement (Table 6: +0.5-0.9% accuracy)\n   - Adds parameters and computation\n\n7. **Limited analysis of learned correspondences**:\n   - Figure 3 shows attention maps are learned, but no analysis of what they reveal\n   - Do they correspond to conceptual layers in the teacher? (e.g., early layers → early layers?)\n   - Do they differ by dataset or architecture?\n   - This would provide insight into why the method works\n\n8. **Theorem 1 is somewhat vacuous**:\n   - Proves gradients of all student layers depend on all teacher layers\n   - But this is expected for any method that jointly optimizes all layers (e.g., standard training)\n   - Doesn't explain **why** this particular gradient dependency is beneficial for knowledge transfer\n   - The main-paper framing (\"richer, more comprehensive knowledge transfer\") is not formally justified\n\n9. **Generalization beyond evaluated settings unclear**:\n   - All experiments use node classification on citation/large-scale graphs\n   - What about graph-level tasks (graph classification, regression)?\n   - What about heterogeneous GNNs beyond RGCN?\n   - What about GNN variants like GANs, Transformers on graphs?\n\n10. **Improved weight initialization (Section 3.2.3) is somewhat orthogonal**:\n   - Clever practical contribution but loosely connected to main STRIDE mechanism\n   - Could be applied to any distillation method, not specific to STRIDE\n   - Results (Table 3) don't compare to training a 1-layer student from scratch with distillation"}, "questions": {"value": "1. **Latency validation**: Can you provide actual inference latency (ms per node/graph) on GPU and CPU for STRIDE-compressed models vs. teacher and baseline methods? This is essential to validate the deployment motivation.\n\n2. **Training cost**: What is the wall-clock training time for STRIDE vs. baselines (LSP, G-CRD) on OGBN-Mag? How does it scale with T_l and S_l?\n\n3. **Attention design choices**: \n   - Why not use alternative attention mechanisms (e.g., multi-head, learnable temperature)?\n   - How sensitive are results to row-wise vs. global softmax?\n   - Why mean-pooling for attention vs. alternatives?\n\n4. **Uniform attention baseline**: Can you provide results for a variant where α_ij = 1/(T_l × S_l) (uniform weighting) while keeping intermediate-layer alignment? This isolates the attention weighting contribution.\n\n5. **Learned correspondences**: Do the attention maps (Figure 3) follow a structured pattern (e.g., diagonal, block structure)? What does this reveal about teacher-student layer relationships?\n\n6. **Comparison with recent work**: Can you include comparisons with SA-MLP and KDGCL (cited but not compared)?\n\n7. **Computational complexity**: What is the per-iteration cost of STRIDE vs. G-CRD? For n=1.9M (OGBN-Mag), computing D_str via G-CRD seems expensive—how is this handled?\n\n8. **Graph-level tasks**: Does STRIDE work for graph classification or only node classification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aRb79AOw9u", "forum": "U34BR8uPyo", "replyto": "U34BR8uPyo", "signatures": ["ICLR.cc/2026/Conference/Submission9899/Reviewer_gnKp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9899/Reviewer_gnKp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761189125438, "cdate": 1761189125438, "tmdate": 1762921359424, "mdate": 1762921359424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces STRIDE, a new knowledge distillation framework for compressing GNNs. The key idea is to use a learnable attention mechanism to dynamically match intermediate layers between the teacher and student, instead of relying only on the final layer outputs as in most prior work. STRIDE distills both structural and node embedding information and includes subspace projections so that teacher and student layers with different dimensions can be aligned. Experiments on large benchmarks such as OGBN-Mag and OGBN-Arxiv, as well as smaller citation datasets, show consistent improvements over state-of-the-art GNN KD baselines, especially under high compression ratios. The paper also provides ablations and a theoretical discussion supporting the learning dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a trainable attention mechanism to dynamically distill knowledge from all intermediate layers. This resolves the key challenge of aligning teacher and student models with different architectures (e.g., varying depths) without requiring fixed layer correspondence.\n2. The method is the first attention-based GNN KD framework to simultaneously align both graph structure and node embeddings. Ablation studies confirm that aligning both is superior to aligning only one.\n3. The approach demonstrates superior accuracy on large-scale, out-of-distribution benchmarks like OGBN-Mag and achieves extreme compression ratios on smaller datasets like Pubmed while maintaining high accuracy."}, "weaknesses": {"value": "1. Distillation cost and memory overhead from layer-wise projections and attention are not analyzed; may limit applicability to very large GNNs.\n2. The method requires separate, trainable linear projection layers for every teacher and student layer (for attention, structure, and embedding alignment). This adds significant parameter overhead and computational complexity during the training phase.\n3. Empirical scope is limited to node classification; no results for link prediction, inductive graphs, dynamic graphs, or molecular domains."}, "questions": {"value": "1.\tWhat is the training overhead (in terms of wall-clock time, GPU memory, and convergence speed) of STRIDE compared to baselines like G-CRD and LSP, given the added complexity of computing the $T_l \\times S_l$ matrices and the multiple trainable projection layers?\n2.\tThe method relies on \"average node features\" to compute inter-layer attention. Is this simple aggregation mechanism sufficient to capture complex layer-wise semantics for accurate alignment, especially on large heterogeneous graphs like OGBN-Mag? Were more expressive graph pooling methods explored?\n3.\tThe paper claims G-CRD provides the best results for the structural dissimilarity component over LSP or GSP, but the supporting ablation study appears to be missing. Could the authors provide the experimental comparison of STRIDE variants using LSP and GSP for this component?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7pagtEBL43", "forum": "U34BR8uPyo", "replyto": "U34BR8uPyo", "signatures": ["ICLR.cc/2026/Conference/Submission9899/Reviewer_xnvr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9899/Reviewer_xnvr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869141677, "cdate": 1761869141677, "tmdate": 1762921359100, "mdate": 1762921359100, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is motivated by the observation that most GNN knowledge distillation (KD) methods only align the final layer. The authors argue that this ignores the rich inductive biases about graph structure and node embeddings found in GNN intermediate layers, which can lead to insufficient learning for the student model. This paper aims to align the intermediate layers between a large teacher GNN and a compact student GNN.\nTo this end, the paper proposes STRIDE, a framework that uses a trainable attention mechanism to dynamically identify and align salient teacher-student layer pairs, matching both structural and embedding information. Ablation studies support the method's design, demonstrating that simultaneously aligning both structure and embeddings (full STRIDE) is significantly more effective than aligning only structure (S-STRIDE) or only embeddings (E-STRIDE)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses a limitation in GNN compression by moving beyond final-layer KD. The core idea of using a new attention mechanism to dynamically match layers. Ablation studies provide evidence for the method's key design choices."}, "weaknesses": {"value": "While the core objective is to compress GNNs to reduce inference costs, the STRIDE method itself introduces overhead during the training phase.\nThe method requires equipping each teacher and student layer with multiple sets of trainable linear projection layers (for attention, embedding dissimilarity, and structural dissimilarity), as well as an additional subspace projection matrix $P$.\nIn essence, STRIDE represents a strategy of \"using high training costs to achieve excellent inference gains,\" but the extent of this extra training cost is not fully quantified."}, "questions": {"value": "1.  Theorem 1 seems too simple to be a \"Theorem.\" Its proof is primarily a straightforward application of the chain rule. It might be better to present this as a \"Proposition.\"\n\n2.  STRIDE is indeed a form of intermediate-layer distillation, specifically an \"all-to-all\" dynamic matching. It computes the knowledge gap between all teacher layers and all student layers, then uses an attention mechanism $\\alpha_{ij}$ to dynamically decide which pairs are most important to align. The authors have effectively shown that STRIDE is superior to baseline methods that only align the last layer. However, to more clearly demonstrate the value of the *dynamic attention* component itself, the authors may compare it against simpler, less computationally expensive, *non-dynamic* intermediate-layer alignment strategies? For example:\n\n    * Strategy A (Many-to-One): Distill knowledge from all teacher intermediate layers into a single student layer (e.g., the final layer).\n    * Strategy B (One-to-Many): Distill knowledge from a single teacher layer (e.g., the final layer) to all student intermediate layers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "AHHLzYaula", "forum": "U34BR8uPyo", "replyto": "U34BR8uPyo", "signatures": ["ICLR.cc/2026/Conference/Submission9899/Reviewer_GwmG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9899/Reviewer_GwmG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762232280656, "cdate": 1762232280656, "tmdate": 1762921358546, "mdate": 1762921358546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}