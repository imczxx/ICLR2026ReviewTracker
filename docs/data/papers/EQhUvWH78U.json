{"id": "EQhUvWH78U", "number": 9652, "cdate": 1758132651175, "mdate": 1763588747039, "content": {"title": "Shoot First, Ask Questions Later? Building Rational Agents that Explore and Act Like People", "abstract": "Many high-stakes applications of AI require forming data-driven hypotheses and making targeted guesses; e.g., in scientific and diagnostic settings. Given limited resources, to what extent do agents based on language models (LMs) act rationally? We develop methods to benchmark and enhance agentic information-seeking, drawing on insights from human behavior. First, we introduce a strategic decision-oriented dialogue task called *Collaborative Battleship*, in which a partially-informed *Captain* must balance exploration (asking questions) and action (taking shots), while a fully-informed *Spotter* must provide accurate answers under an information bottleneck. Compared to human players (N=42), we find that LM agents struggle to ground answers in context, generate informative questions, and select high-value actions. Next, to address these gaps, we develop novel Monte Carlo inference strategies for LMs based on principles from Bayesian Experimental Design (BED). For Spotter agents, our approach boosts accuracy by up to 14.7% absolute over LM-only baselines; for Captain agents, it raises expected information gain (EIG) by up to 0.227 bits (94.2% of the achievable noise ceiling). Combined, these components yield sharper targeting (+0.303–0.374 F1), and enable weaker LMs, such as Llama-4-Scout, to outperform both humans (8% → 82% win rate) and frontier models (0% → 67% win rate vs. GPT-5) at ≈1% of GPT-5's cost. We replicate these findings on *Guess Who?* where our methods significantly boost accuracy (+28.3–42.4 p.p.), demonstrating their general applicability for building rational information-seeking agents.", "tldr": "We introduce a collaborative Battleship task to evaluate information-seeking in humans and agents; insights from Bayesian Experimental Design (BED) yield inference-time strategies for building resource-rational agents in discovery settings.", "keywords": ["Bayesian experimental design", "information-seeking", "question asking", "Collaborative Battleship", "expected information gain (EIG)", "explore-exploit tradeoffs", "resource rationality", "probabilistic inference", "Monte Carlo sampling", "symbolic grounding", "code generation", "reasoning", "decision-oriented dialogue", "cognitive modeling", "human behavior", "language model agents", "scientific discovery"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f6849a88d3e0b3b7278df91ce175b359164ba11a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The present paper evaluates and improves the ability of LLMs to ask goal-directed questions and take actions in a dynamic environment. For this, the authors design a novel task called Collaborative Battleship. They run a human study on this task and compare human performance to that of various LLMs. This revealed a performance gap that the authors then subsequently address by developing Bayesian-inspired inference-time strategies for LLMs, leading to significant improvement in LLM performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "I enjoyed this paper a lot. It has all the ingredients for a great paper: a new task, human evaluation, a decent set of different models that are evaluated, a novel technique for improving models, and demonstration that the findings generalize to another domain. The paper was exceptionally well written and easy to follow. The method is clean and simple, yet effective. CaptainQA is an interesting agentic test bed for LLMs and the whole methodology fits thematically well into ICLR."}, "weaknesses": {"value": "I found the usage of the term Bayes-rational weird. Best to my knowledge, this is not an accepted term in the literature. It implies that the strategies developed by the authors are Bayes-optimal, which is not the case (as also noted by the authors). To avoid this confusion, I would suggest using a different term instead.\n\nThere is not so much negative to say about the paper. Perhaps the only downside is that, while the results and methods are interesting, they are not groundbreaking. For me, that is the only reason for giving this paper a score of 8 (instead of the full 10).\n\nMinor:\n* RSA not defined (p9)."}, "questions": {"value": "The indicator function in Equation 1 seems to be redundant (unless I am missing something).\n\nThe authors find that GPT-5 does not significantly benefit from Bayesian question or move selection. This raises the question of whether the proposed is useful for future models. Do you think that there is a risk of this being the case?\n\nI was a bit confused by the description of Equation 7. Why is \\pi_{t+1}^a introduced but then never used? Why p_{t+1}^{hit} defined as a distribution over questions? That seems strange. Why is u_t^* mentioned twice under step 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "udnA0KafJk", "forum": "EQhUvWH78U", "replyto": "EQhUvWH78U", "signatures": ["ICLR.cc/2026/Conference/Submission9652/Reviewer_QbJw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9652/Reviewer_QbJw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761234062803, "cdate": 1761234062803, "tmdate": 1762921178832, "mdate": 1762921178832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Collaborative Battleship, a novel two-player task designed to evaluate the trade-offs between information-seeking (asking questions) and exploitation (taking actions) in language model (LM) agents under communication constraints (yes/no answers). The authors collected a human dataset, BATTLESHIPQA (N=42 participants, 126 games), establishing human baselines for question quality and strategic play. They compare human performance to various LMs, finding that while weaker LMs struggle, frontier reasoning models approach human levels. To improve agent rationality, the paper proposes inference-time strategies based on Bayesian Experimental Design (BED), using Sequential Monte Carlo (SMC) to approximate belief states and guide question selection (maximizing EIG), action selection (maximizing hit probability), and the explore/exploit decision (one-step lookahead). These strategies significantly boost performance, enabling weaker LMs (Llama-4-Scout) augmented with Bayesian methods to outperform average humans and stronger LMs like GPT-5 in win rate, at substantially lower computational cost. Findings were replicated on the Guess Who? game. The authors provide IRB approval details and plan to release the dataset and code."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Novel Task and Dataset:** Collaborative Battleship provides a clean, interpretable environment for studying the explore/exploit dilemma and grounded communication. The BATTLESHIPQA dataset, collected from human interactions, is a valuable contribution for benchmarking and analysis.\n\n**Principled Bayesian Framework:** The application of BED principles (EIG maximization, belief updating via SMC, MAP action selection) provides a strong theoretical grounding for the proposed inference-time strategies.\n\n**Strong Empirical Results:** The Bayesian strategies demonstrate significant improvements in LM agent performance across multiple metrics (accuracy, EIG, F1, win rate). The finding that augmenting weaker LMs can lead to super-human and SOTA-LM-beating performance at lower cost is particularly compelling.\n\n**Generalization:** Successful replication of performance gains on the Guess Who? task suggests the framework's potential applicability to other information-seeking domains.\n\n**Transparency and Reproducibility:** The paper includes ethical considerations for human subjects, clear plans for releasing code and data, and explicit disclosure of AI assistance."}, "weaknesses": {"value": "**Limited Scope:** While the paper introduces an interesting method, its evaluation is confined to a specific domain defined by the authors, with generalization demonstrated only on one additional ad-hoc task. This limited scope makes it difficult to assess the method's potential for broader applicability and overall impact.\n\n**\"Surpass Humans\" Claim Qualification:** The claim that augmented LMs surpass human performance needs stronger qualification. Details on human participants' prior experience, the number of trials per condition, and how performance compares when normalizing for computational resources (cost, latency, tokens) are necessary for a fair comparison. The large cost disparity is noted but not integrated into the primary win-rate comparisons.\n\n**Rationality Framing:** The paper frames the goal as building \"rational\" agents using Bayes-optimal principles (BED). However, human behavior often follows boundedly rational heuristics. The evaluation primarily uses game performance (win rate, F1) as a proxy for rationality, potentially conflating task success with optimal information processing under constraints.\n\n**Potential Scaffolding Effects:** The interaction between different components (question generation, answering, strategy selection, potential code generation for grounding) needs further ablation. It's unclear if performance gains could stem from implicit prompt leakage or interactions between modules rather than purely the Bayesian logic."}, "questions": {"value": "1. Could you provide more details on the human study participants? What was their prior experience with Battleship? How were the number of human trials balanced against LM evaluations in terms of total interaction opportunities or budget? \n\n2. Could you please provide performance results (e.g., win rate, F1 score) normalized by computational cost (tokens, latency, API cost)?  Cost-controlled comparisons (e.g., win rate vs. budget curves) would strengthen the claims about efficiency.\n\n3. Can you provide ablations that isolate the capabilities of the Captain (questioner/actor) and Spotter (answerer) roles? For instance, how does a strong Captain perform with a weak Spotter, and vice-versa? How much does the specific code generation strategy contribute to the Spotter's grounding? \n\n4. How sensitive are the results of the Bayesian strategies to hyperparameters like the SMC particle count, the number of candidate questions sampled (k for $Q_{Bayes}$), the decision discount factor ($\\gamma$ for $D_{Bayes}$), and decoding parameters (e.g., temperature)?\n\n5. What was the inter-rater reliability for the manual annotation of human questions and gold answers in BATTLESHIPQA?\n\n6. Consider citing and comparing with SPIN-Bench (Yao et al., 2025) which evaluates LLMs in multi-agent cooperative and strategic settings (like Hanabi)  and documents coordination failures, making it relevant context for evaluating interactive agent strategies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x9PCP5suCL", "forum": "EQhUvWH78U", "replyto": "EQhUvWH78U", "signatures": ["ICLR.cc/2026/Conference/Submission9652/Reviewer_g4p6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9652/Reviewer_g4p6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719014686, "cdate": 1761719014686, "tmdate": 1762921178510, "mdate": 1762921178510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Collaborative Battleship, a novel two-player task designed to evaluate the trade-offs between information-seeking (asking questions) and exploitation (taking actions) in language model (LM) agents under communication constraints (yes/no answers). The authors collected a human dataset, BATTLESHIPQA (N=42 participants, 126 games), establishing human baselines for question quality and strategic play. They compare human performance to various LMs, finding that while weaker LMs struggle, frontier reasoning models approach human levels. To improve agent rationality, the paper proposes inference-time strategies based on Bayesian Experimental Design (BED), using Sequential Monte Carlo (SMC) to approximate belief states and guide question selection (maximizing EIG), action selection (maximizing hit probability), and the explore/exploit decision (one-step lookahead). These strategies significantly boost performance, enabling weaker LMs (Llama-4-Scout) augmented with Bayesian methods to outperform average humans and stronger LMs like GPT-5 in win rate, at substantially lower computational cost. Findings were replicated on the Guess Who? game. The authors provide IRB approval details and plan to release the dataset and code."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "**Novel Task and Dataset:** Collaborative Battleship provides a clean, interpretable environment for studying the explore/exploit dilemma and grounded communication. The BATTLESHIPQA dataset, collected from human interactions, is a valuable contribution for benchmarking and analysis.\n\n**Principled Bayesian Framework:** The application of BED principles (EIG maximization, belief updating via SMC, MAP action selection) provides a strong theoretical grounding for the proposed inference-time strategies.\n\n**Strong Empirical Results:** The Bayesian strategies demonstrate significant improvements in LM agent performance across multiple metrics (accuracy, EIG, F1, win rate). The finding that augmenting weaker LMs can lead to super-human and SOTA-LM-beating performance at lower cost is particularly compelling.\n\n**Generalization:** Successful replication of performance gains on the Guess Who? task suggests the framework's potential applicability to other information-seeking domains.\n\n**Transparency and Reproducibility:** The paper includes ethical considerations for human subjects, clear plans for releasing code and data, and explicit disclosure of AI assistance."}, "weaknesses": {"value": "**Limited Scope:** While the paper introduces an interesting method, its evaluation is confined to a specific domain defined by the authors, with generalization demonstrated only on one additional ad-hoc task. This limited scope makes it difficult to assess the method's potential for broader applicability and overall impact.\n\n**\"Surpass Humans\" Claim Qualification:** The claim that augmented LMs surpass human performance needs stronger qualification. Details on human participants' prior experience, the number of trials per condition, and how performance compares when normalizing for computational resources (cost, latency, tokens) are necessary for a fair comparison. The large cost disparity is noted but not integrated into the primary win-rate comparisons.\n\n**Rationality Framing:** The paper frames the goal as building \"rational\" agents using Bayes-optimal principles (BED). However, human behavior often follows boundedly rational heuristics. The evaluation primarily uses game performance (win rate, F1) as a proxy for rationality, potentially conflating task success with optimal information processing under constraints.\n\n**Potential Scaffolding Effects:** The interaction between different components (question generation, answering, strategy selection, potential code generation for grounding) needs further ablation. It's unclear if performance gains could stem from implicit prompt leakage or interactions between modules rather than purely the Bayesian logic."}, "questions": {"value": "1. Could you provide more details on the human study participants? What was their prior experience with Battleship? How were the number of human trials balanced against LM evaluations in terms of total interaction opportunities or budget? \n\n2. Could you please provide performance results (e.g., win rate, F1 score) normalized by computational cost (tokens, latency, API cost)?  Cost-controlled comparisons (e.g., win rate vs. budget curves) would strengthen the claims about efficiency.\n\n3. Can you provide ablations that isolate the capabilities of the Captain (questioner/actor) and Spotter (answerer) roles? For instance, how does a strong Captain perform with a weak Spotter, and vice-versa? How much does the specific code generation strategy contribute to the Spotter's grounding? \n\n4. How sensitive are the results of the Bayesian strategies to hyperparameters like the SMC particle count, the number of candidate questions sampled (k for $Q_{Bayes}$), the decision discount factor ($\\gamma$ for $D_{Bayes}$), and decoding parameters (e.g., temperature)?\n\n5. What was the inter-rater reliability for the manual annotation of human questions and gold answers in BATTLESHIPQA?\n\n6. Consider citing and comparing with SPIN-Bench (Yao et al., 2025) which evaluates LLMs in multi-agent cooperative and strategic settings (like Hanabi)  and documents coordination failures, making it relevant context for evaluating interactive agent strategies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x9PCP5suCL", "forum": "EQhUvWH78U", "replyto": "EQhUvWH78U", "signatures": ["ICLR.cc/2026/Conference/Submission9652/Reviewer_g4p6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9652/Reviewer_g4p6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719014686, "cdate": 1761719014686, "tmdate": 1763607224393, "mdate": 1763607224393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper introduces the collaborative battleship game, where a “captain” and a “spotter” collaborate in the game of battleship. The captain must decide between asking the spotter a question, or taking a hit at a board position, thereby having to balance information seeking and reward seeking.\n\nThe authors collected the BATTLESHIPQA human dataset where pairs of humans collaborated to play the game (42 participants, 21 pairs). This establishes human performance, the kind of questions humans ask, and how accurately the human (spotter) responds. \n\nThey then evaluated 15 LMs in the role of the spotter (i.e. Q&A), finding that the best frontier models can do well, but they show greater degradation in hard (i.e. context dependent) questions. Then, they evaluate 3 LMs in the role of the captain, with various “bayes rational” strategies.  The “bayes rational” strategies improve the performance of weak LMs to the level of / slightly beyond GPT-5.\n\nFinally, they extended the bayesian inference framework to other information seeking games and see some performance gains."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "There are numerous strengths with this paper. To start, the collaborative battleship game sets up an interesting scenario for the captain agent, where they not only have to balance information seeking vs. reward seeking behaviour, but also maintain uncertainty about the correctness of its collaborator (noisy spotter agent). Information seeking in the space of natural language is an interesting setting to study.\n\nThe human dataset provides valuable information to ground “human level” performance, the type of questions asked, and human-like behaviour. The LM evaluations are comprehensive, with interesting findings including code generation boosting spotter accuracy. The “bayes rational” captain strategies improving worse models to perform at GPT-5 level has useful application implications. \n\nOverall, the paper is very well written and presented."}, "weaknesses": {"value": "A number of simplifying assumptions were made in the “bayes rational” modelling choices. For instance, modelling with fixed $\\epsilon$ (as the authors already point out) and $\\gamma$ (not sure how this is set), and only modelling single-step look-ahead are simplifying assumptions. I do not think this detracts from the main point of the paper, as to my understanding this paper is about improving empirical performance of weak LMs using some cognitively inspired “bayes rational\" strategies. Nevertheless, it is worth pointing this out for scientific rigour. \n\nHowever, there are a few simplifications that I am quite confused about, and likely warrant deeper discussions. I ask them in the question section below."}, "questions": {"value": "### On assumptions\n\n**(1)**\nis maximizing EIG (Eq 5, L214) in this game the “right” thing to do to maximize performance of hitting a ship? My understanding is that in some games from previous works (e.g. ActiveACRE, blickets, feature world) [1,2,3], the explicit goal is to find out how the environment works. In these cases, directly optimizing for information gain is the “right” thing to do. \n\nIn battleships, seeking information is only ever in service of the goal of hitting ships. Therefore, rather than computing the post-question hit probability (Eq 7) with the max-EIG question (Eq 5), i.e. $p_{t+1}^{hit} (q_{t}^{*} | x, H)$, shouldn’t we directly find $\\arg\\max_q p_{t+1}^{hit}(q | x, H)$ instead, and use that in the decision rule in L217? Is there no scenario in which this will change which question is selected? This is discussed empirically in L405-412 but it would be good to discuss a bit theoretically as well. \n\n**(2)**\nI do not think the information gained from the “shoot” action is appropriately considered. The act of shooting perfectly reveals information about whether or not a tile contains a ship, akin to asking a noise-less spotter “is there a ship on this tile”. Indeed, for a 8x8 board and a budget of 40 shots, 40/64 = 62.5% of tiles can be revealed this way. Thus, shouldn’t the captain’s decision really be between (i) asking a question based on post-hit probability (per point (1) above), and (ii) choosing a shot that will both hit a current target and/or increase post-hit probability of the next shot? \n\n\n### Minor clarifications\n\n1. What does Figure 3 b and c error bars denote? Please label / state in figure caption\n2. How is the discount factor $\\gamma$ in L216 selected?\n\n---\n\n[1] Piriyakulkij, Top, et al. \"Doing experiments and revising rules with natural language and probabilistic reasoning.\" Advances in Neural Information Processing Systems 37 (2024): 53102-53137.\n\n[2] GX-Chen, Anthony, et al. \"Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?.\" arXiv preprint arXiv:2505.09614 (2025).\n\n[3] Sawyer, Danny P., et al. \"Can foundation models actively gather information in interactive environments to test hypotheses?.\" arXiv preprint arXiv:2412.06438 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X8IIAqKImt", "forum": "EQhUvWH78U", "replyto": "EQhUvWH78U", "signatures": ["ICLR.cc/2026/Conference/Submission9652/Reviewer_VQ6M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9652/Reviewer_VQ6M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992606630, "cdate": 1761992606630, "tmdate": 1762921178165, "mdate": 1762921178165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}