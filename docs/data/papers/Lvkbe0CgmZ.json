{"id": "Lvkbe0CgmZ", "number": 15465, "cdate": 1758251665056, "mdate": 1759897305210, "content": {"title": "Mirror Mean-Field Langevin Dynamics", "abstract": "The mean-field Langevin dynamics (MFLD) minimizes an entropy-regularized nonlinear convex functional on the Wasserstein space over $\\mathbb{R}^d$, and has gained attention recently as a model for the gradient descent dynamics of interacting particle systems such as infinite-width two-layer neural networks. However, many problems of interest have constrained domains, which are not solved by existing mean-field algorithms due to the global diffusion term. We study the optimization of probability measures constrained to a convex subset of $\\mathbb{R}^d$ by proposing the mirror mean-field Langevin dynamics (MMFLD), an extension of MFLD to the mirror Langevin framework. We obtain linear convergence guarantees for the continuous MMFLD via a uniform log-Sobolev inequality, and uniform-in-time propagation of chaos results for its time- and particle-discretized counterpart.", "tldr": "", "keywords": ["mean-field Langevin dyanamics", "propagation of chaos"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/29551b3bd05fb025b4639691f6ebaadb55ff5ef3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper extends mean-field Langevin dynamics (MFLD) to tackle mean-field optimization problems constrained within a convex subset of $\\mathbb{R}^d$. To this end, the authors introduce the mirror mean-field Langevin dynamics (MMFLD), which integrates MFLD into the mirror Langevin framework. They establish linear convergence of the continuous-time MMFLD under a uniform logarithmic Sobolev inequality (LSI) and further prove uniform-in-time propagation-of-chaos results for both its time-discretized and particle-discretized counterparts."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The constrained mean-field optimization problem is important to the machine learning community due to its broad and interesting applications, and this paper presents an effective algorithmic approach to address it.\n\n* The paper provides a comprehensive review of relevant prior work.\n\n* The MMFLD formulation, along with its convergence and discretization analyses, is largely comparable to those in the unconstrained setting and constitutes a relatively straightforward extension."}, "weaknesses": {"value": "* The practical applicability of the proposed method remains unclear due to the strong assumptions and the limited, toy-level empirical results.\n\n* The proposed scheme does not constitute a genuine discretization, as it assumes exact simulation of the Brownian motion. Moreover, the analysis largely builds upon existing results from prior works, such as [Ahn & Chewi, 2021] and [Vempala & Wibisono, 2019], and offers limited novelty."}, "questions": {"value": "* The (mirrored) smoothness assumptions are invoked in several theorems, but the corresponding constants are not explicitly reflected in the statements. For instance, is Assumption 2 required for Theorem 2.3? If so, why do the constants $M_1$ and $M_2$ not appear in the theorem? Similarly, is Assumption 5 necessary for Theorem 3.2? And is it also used in Theorem 4.1?\n\n* I understand that the main contribution of this paper is theoretical. However, prior works on mean-field optimization typically include experiments on training two-layer neural networks. It would be valuable if the authors could provide a similar experiment with constrained parameters, which would greatly enhance the practical relevance and applicability of the paper.\n\n* Regarding the experiment, is $\\eta = 3 \\times 10^{-3}$ chosen as the optimal stepsize for both methods? Different algorithms may exhibit different sensitivities to the stepsize, so using the same value without tuning may raise concerns about fairness in comparison. In addition, since this is an $N$-particle algorithm, it would be helpful to evaluate its sensitivity to the number of particlesâ€”for example, by testing $N \\in \\{256, 512, 1024, 2048, ...\\}.\n\n**I will be happy to raise my score if the authors can address my concerns on the assumptions and experiments.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G4P5VZdCqt", "forum": "Lvkbe0CgmZ", "replyto": "Lvkbe0CgmZ", "signatures": ["ICLR.cc/2026/Conference/Submission15465/Reviewer_rL3e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15465/Reviewer_rL3e"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877909366, "cdate": 1761877909366, "tmdate": 1762925753518, "mdate": 1762925753518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies an extension of mean-field Langevin dynamics to constrained settings using ideas from mirror Langevin dynamics. They obtain linear convergence of continuous time dynamics under a uniform log-Sobolev inequality, and also give a propagation of chaos result to give a result that is discrete in space and time. A numerical experiment on the simplex is given that shows some slight advantage to the mirror mean-field Langevin dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is generally well written and easy to understand. The theorem statements are generally clear to me and the proofs are readable and easy to follow.\n- This paper represents a natural extension of mean-field Langevin dynamics using a mirror map. As was done in the non-mean-field case, this extension has desirable properties of relying on relative Lipschitz types of assumptions, and also naturally maintains the constraints of the problem.\n- The results utilize the full range of available tools to prove a convergence bound for a particle distribution in discrete time."}, "weaknesses": {"value": "- The theorems given seem to be standard extensions of existing results in the literature. The heavy lifting seems to have been done in past works like Nitanda et al '22, Nitanda '24, and Nitanda et al '25. Because of this, I am worried that this work is more of a synthesis work than giving some novel and new ideas that would be sufficient for publication in ICLR.\n- Coupled with the above limited theoretical novelty, there is a lack of experimental evidence. The authors only give one low-dimensional experiment that barely shows an advantage for MMFLD.\n- There is little attention paid to a motivating example. Why should the reader care about constrained sampling? Grounding this problem in real problems of interest to the machine learning community would greatly strengthen the position of this paper."}, "questions": {"value": "- For the continuous setting, in Assumption 4 the authors assume a uniform LSI. It would be useful if the authors could provide a discussion of the cases when this holds rather than offloading this to references. The same comment goes for Assumptions 6 and 8.\n- The modified Wasserstein distance is not symmetric in $\\mu, \\mu'$? Does this have any connnection to a Bregman Wasserstein divergence, or some other \"distance\" of interest?\n- The constant in Theorem 4.2 is exponential in $D$, and for common barrier mirror maps, $\\nabla \\phi$ is surjective. Doesn't this mean that $D$ is unbounded? The authors should comment on this.\n- Where do I see the self concordance parameter $\\gamma_1$ in the statement of Theorem 4.2?\n- Can the authors comment on their results in comparison with the analysis of projected Langevin (Bubeck et al '15), projected SGLD (Lamperski '21), and other recent methods for sampling from convex bodies (like Gu et al '24)? Having a more in depth theoretical comparison of the advantages of a mirror approach in this setting would be useful, and could also help to guide experimental settings to show where this method has a true advantage.\n\nReferences:\nBubeck, Sebastien, Ronen Eldan, and Joseph Lehec. \"Finite-time analysis of projected Langevin Monte Carlo.\" Advances in Neural Information Processing Systems 28 (2015).\nLamperski, Andrew. \"Projected stochastic gradient langevin algorithms for constrained sampling and non-convex learning.\" Conference on Learning Theory. PMLR, 2021.\nGu, Yuzhou, et al. \"Log-concave sampling from a convex body with a barrier: a robust and unified dikin walk.\" Advances in Neural Information Processing Systems 37 (2024): 69230-69298."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4635CLKpT0", "forum": "Lvkbe0CgmZ", "replyto": "Lvkbe0CgmZ", "signatures": ["ICLR.cc/2026/Conference/Submission15465/Reviewer_qe14"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15465/Reviewer_qe14"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917378991, "cdate": 1761917378991, "tmdate": 1762925753131, "mdate": 1762925753131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors consider a combination of mirror Langevin dynamics and mean-field Langevin dynamics, analysing the setting of entropy-regularised functionals on constrained convex domains. They provide convergence guarantees for the continuous-time flow under logarithmic Sobolev inequalities and develop guarantees for a time- and particle-discretized scheme. They also provide experiments comparing the scheme to projected mean-field Langevin dynamics, showing that their scheme attains a lower final loss."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* The authors target a fairly important and surprisingly open problem, since mean-field dynamics are used to understand two-layer neural networks and mirror descent is frequently used in constrained optimisation.\n* The paper is very well-written.\n* The guarantees are strong and are under relatively standard conditions in this area (e.g., uniform LSI)."}, "weaknesses": {"value": "* The majority of the proof techniques appear to be borrowed or adapted from other papers (e.g., Nitanda et al., 2022; Jiang, 2021 and Nitanda, 2024), so the work may have limited technical novelty at the proof level.\n* The analysis of the discretized algorithm assumes that the pure diffusion step (Algorithm 1, step 5) can be **simulated exactly**. The authors note this is for \"simplicity of exposition\", but this is rarely possible in practice and creates a gap between the theory and the implementation (which used a one-step discretization).\n* The discrete-time convergence analysis (Section 4.3) is presented for the specific setting of the mean-field neural network risk minimization problem, which may limit the perceived generality of the result."}, "questions": {"value": "* What are the primary technical novelties of this work at the level of the proof, beyond the synthesis of existing analytical frameworks?\n* Regarding Weakness 2: Can the authors comment on the error introduced by *not* simulating the diffusion step exactly? Would a practical, one-step discretization of this diffusion term impact the final convergence guarantee in Theorem 4.2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lh82BxwZUo", "forum": "Lvkbe0CgmZ", "replyto": "Lvkbe0CgmZ", "signatures": ["ICLR.cc/2026/Conference/Submission15465/Reviewer_ZizB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15465/Reviewer_ZizB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762131530812, "cdate": 1762131530812, "tmdate": 1762925752780, "mdate": 1762925752780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces and studies mirror mean-field Langevin dynamics, which can be used to solve constrained distributional optimization problems. The authors establish the exponential convergence of this dynamics under a (mirror) log-Sobolev inequality akin to the Euclidean counterpart, and also carry out a complete discretization analysis on both time and the number of particles."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper provides a clean and novel analysis of mirror mean-field Langevin dynamics as a generalization of mean-field Langevin, and the numerical illustration demonstrates that this can be a better idea to solve distributional optimization problems compared to projected mean-field Langevin. This is interesting in particular since currently there are not many well-studied algorithms for distributional optimization that work well in high dimensions beyond the mean-field Langevin dynamics."}, "weaknesses": {"value": "* I think the authors can better motivate the study of mirror mean-field Langevin by showing what new settings can be unlocked by their analysis, e.g. for training weight-constrained two-layer neural networks or for generative modeling.\n\n* Since the discretization cost of Step 5 is not analyzed, it could potentially be helpful to have, perhaps an informal, discussion of why simulating this step is easier than simulating a Brownian motion on $\\mathcal{X}$ (and consequently performing MFLD on $\\mathcal{X}$)."}, "questions": {"value": "A typical framework for the theoretical study of two-layer nets is to constrain the first layer weights to live on the unit sphere, and allow the second layer weights to be unbounded. A challenge here is that one can not show uniform LSI due to the unbounded weights of the second layer. One way to remedy the issue is to perform bilevel optimization as in [1], which reduces the problem to MFLD on the unit sphere with a bounded uniform-LSI constant. However, [1] still requires performing MFLD on the unit sphere, without discretization guarantees. A concrete application of the results of this paper can be to instead use mirror MFLD after the bilevel reduction, to provide an end-to-end guarantee for training two-layer networks in the mean-field regime.\n\n[1] G. Wang et al, \"Mean-Field Langevin Dynamics for Signed Measures via a Bilevel Approach.\" NeurIPS 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0KRb1MTVor", "forum": "Lvkbe0CgmZ", "replyto": "Lvkbe0CgmZ", "signatures": ["ICLR.cc/2026/Conference/Submission15465/Reviewer_8nHW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15465/Reviewer_8nHW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153727402, "cdate": 1762153727402, "tmdate": 1762925752234, "mdate": 1762925752234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}