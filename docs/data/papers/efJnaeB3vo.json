{"id": "efJnaeB3vo", "number": 12514, "cdate": 1758208332267, "mdate": 1763274722827, "content": {"title": "AutoDavis: Automatic and Dynamic Evaluation Protocol of Large Vision-Language Models on Visual Question-Answering", "abstract": "Large Vision-Language Models (LVLMs) have become essential for advancing the integration of visual and linguistic information. While existing benchmarks have laid a solid foundation for evaluation, they are often static, resource-intensive to build, and limited in adaptability. In comparison, automatic evaluation has shown promise in the textual domain, but the visual modality remains far less explored. To advance this frontier, in this work, we introduce AutoDavis, a first-of-its-kind automatic and dynamic evaluation protocol that enables on-demand benchmarking of LVLMs across specific capability dimensions. AutoDavis leverages text-to-image models to generate relevant image samples and then utilizes LVLMs to orchestrate visual question-answering (VQA) tasks, completing the evaluation process efficiently and flexibly. To ensure data diversity, our framework employs a hierarchical aspect-driven generation process enhanced with semantic graph-based constraints. To safeguard reliability, the framework incorporates a self-validation mechanism to detect and correct errors, along with an error-driven adjustment module to mitigate potential bias. Through an extensive evaluation of 11 popular LVLMs across five demanded user inputs (i.e., evaluation capabilities), the framework shows effectiveness and reliability, offering a new paradigm for dynamic benchmarking of multimodal intelligence.", "tldr": "An automatic and dynamic evaluation protocol for LVLMs.", "keywords": ["LVLM", "dynamic evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/744263e2d9329187523603a8d11361ab70a04c1e.pdf", "supplementary_material": "/attachment/d1aedef6a27c7c7316cf68c2cbab3c9324068643.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces AutoDavis, an automatic and dynamically regenerable evaluation protocol for assessing LVLMs on VQA. The framework allows users to specify capability dimensions and difficulty levels. AutoDavis then employs a LVLM-based examiner coupled with a T2I generator to automatically produce question–image pairs, followed by self-consistency validation and error-driven distractor generation. The system integrates image-free controls and answer position balancing to mitigate textual shortcuts and positional bias.\nExperiments cover 5 capability categories × 3 difficulty levels × 11 LVLMs, showing clear performance degradation with increasing difficulty. The paper further demonstrates that multi-examiner ensembles yield more stable rankings, and dynamic regeneration mitigates data leakage from repeated exposure. The paper also provides a theoretical sample–validation bound for controlling evaluation error and explores data re-use for training, showing minor improvements on external benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tSystematic and operational definition of “dynamic evaluation”\nAutoDavis formalizes three key criteria—flexibility, anti-leakage, and visual grounding—and instantiates them with concrete mechanisms. The framework transforms regenerable evaluation from a conceptual goal into a practical, verifiable protocol.\n2.\tHierarchical task decomposition with controlled diversity\nThe authors decompose abilities hierarchically, ability → sub-skill → constrained diversity, and use semantic-graph–constrained description generation to ensure topical coverage and reduce redundancy. Quantitative diversity analysis supports the method’s effectiveness.\n3.\tComprehensive experiments with clear separation effects\nThe benchmark includes 11 LVLMs and reports fine-grained breakdowns by ability, difficulty, and visual evidence. The results convincingly show increasing difficulty correlates with performance degradation, and image-free variants sharply reduce accuracy—confirming reliance on visual reasoning."}, "weaknesses": {"value": "1.\tEvaluator family dependence and circularity risk\nThe entire loop—question generation, VQA validation, distractor rewriting, and scoring—relies on strong LVLMs from the same or related model families. Although the authors introduce “multi-examiner” diversity, they do not quantify cross-family variance or bias. Without cross-judge robustness, AutoDavis risks overfitting to shared linguistic priors or visual biases.\n2.\tReliability of T2I and self-verification\nThe faithfulness of generated images is only indirectly ensured by the self-check VQA threshold (ζ). Yet VQA self-checkers may share textual priors with the examiner, producing false positives for visually inconsistent images. The paper lacks manual inspection results or sensitivity studies under varied T2I or checker models.\n3.\tLeakage and regeneration analysis remains limited\nThe anti-leakage experiment is small-scale and defines “leakage” narrowly. Broader experiments would provide stronger evidence for robustness against contamination.\n4.\tExternal validity limited to MMMU correlation\nWhile a Spearman ρ = 0.817 with MMMU is encouraging, validation against additional human-annotated benchmarks (e.g., MMBench, SEED-Bench 2, MMMU-Pro) and fine-grained per-ability consistency would better support generalizability.\n5.\tPotential ambiguity in error-driven distractor generation\nForcing “plausible wrong” alternatives by relabeling the correct answer can introduce ambiguous or semi-correct options. The paper lacks quantitative measures of question unambiguity or human adjudication consistency after augmentation."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QJa8oq0hPC", "forum": "efJnaeB3vo", "replyto": "efJnaeB3vo", "signatures": ["ICLR.cc/2026/Conference/Submission12514/Reviewer_wVDn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12514/Reviewer_wVDn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761633158479, "cdate": 1761633158479, "tmdate": 1762923382761, "mdate": 1762923382761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AUTODAVIS, a dynamic and automated evaluation protocol for Large Vision-Language Models (LVLMs). It addresses limitations of static benchmarks by enabling on-demand test generation, preventing data leakage through dynamic regeneration, and ensuring visual grounding via error-driven option adjustment. Key contributions include: (1) the AUTODAVIS framework with modules for user-oriented aspect generation, guided description creation, self-validated image generation, and test case evaluation; (2) comprehensive experiments demonstrating its effectiveness in generating diverse, reliable assessments across dimensions like basic, spatial, and reasoning understanding, with high human alignment and correlation to existing benchmarks; (3) insights into model capabilities, revealing performance drops with increasing difficulty and systemic weaknesses in fine-grained visual reasoning; and (4) evidence that AUTODAVIS-generated data can enhance model training and generalization. The protocol offers a scalable, cost-effective supplement to static benchmarks, promoting trustworthy LVLM evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: This work moves beyond creating another static benchmark by proposing a dynamic, on-demand generation system. This effectively addresses known limitations of static benchmarks, such as data leakage and rapid obsolescence\n\nSignificance: The work's importance is significant for the sustainable and trustworthy evaluation of LVLMs. It provides a practical solution to critical issues of benchmark staleness and data contamination that plague static datasets.\n\nClarity: The exposition is clear and well-structured."}, "weaknesses": {"value": "1. The authors designate GPT-4o, Gemini-1.5-Pro, and Claude-3.5-Sonnet as examiner models; however, these models are simultaneously included among the representative models being evaluated. Could this dual role introduce examiner bias and potentially compromise the fairness of the evaluation?\n\n2. Could multiple trials be conducted with humans serving as examiners for the same questions, to observe whether human scoring is consistent or inconsistent with automated scoring?"}, "questions": {"value": "1. Figure 3 on page 7 is missing an overall caption. Figure 5 is not cited or mentioned in the body of the text. In Figure 3(b), the term 'relative change' is not clearly defined.\n\n2. Please provide statistical information about the dynamic benchmark, for example, the average length of the generated questions and the average length of the answer options. Multiple experiments may be conducted to determine either a range or an average.\"\n\n3. Within the AUTODAVIS pipeline, does generating images at different resolutions have an impact on the evaluation performance?\n\n4. What is the actual visual complexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Nothing"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "far8e7pdQW", "forum": "efJnaeB3vo", "replyto": "efJnaeB3vo", "signatures": ["ICLR.cc/2026/Conference/Submission12514/Reviewer_uSUX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12514/Reviewer_uSUX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754306591, "cdate": 1761754306591, "tmdate": 1762923382234, "mdate": 1762923382234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets the limits of static LVLM benchmarks and proposes AUTODAVIS, a system that builds capability-focused tests on demand. It synthesizes images with a text-to-image model and then probes models via VQA prompts, using simple structural rules to keep variety. A self-check and error-driven update step aim to catch mistakes and curb bias. The authors report trials on 11 LVLMs across five user-specified skill areas, showing the setup is practical and fairly stable. The idea is appealing for scale and flexibility, but it leans heavily on generative components, so bias and domain transfer remain concerns; clearer cost and robustness analyses would strengthen the case."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Proposes an interesting evaluation idea that treats LVLMs as both examiners and test-set generators, yielding a largely model-driven and potentially more scalable pipeline.\n\n- The framework appears solid, producing diverse and accurate images conditioned on user queries, as also the question desination.\n\n- Ablation studies are informative, especially the reported performance gains after integrating the framework."}, "weaknesses": {"value": "- The motivation is not clear enough. This paper point out that static benchmarks are insufficient at first, it sounds important, but, why must use LVLM itself to solve this question. In other words, what the necessary to use this pipeline to solve this problem? As the description is the introduction, the Dynamic-ME [1] also can solve it.\n\n- The evaluation set of models is dated. Recently released open-source LVLMs available before the ICLR deadline, such as InternVL-3 and Qwen2.5-VL, should be included, but your works only test InternVL-2.5 and Qwen2-VL.\n\n- The analysis is shallow, more explanation should be obtained, such as the deep reason on the bad performance on spatial reasoning.\n\n- While the ablation study indicates that the proposed design influences model outcomes, the paper does not further investigate the underlying causes or compare the effects of different improvement directions. More comprehensive analysis is needed.\n\n[1] Yang, Yue, et al. \"Dynamic multimodal evaluation with flexible complexity by vision-language bootstrapping.\" arXiv preprint arXiv:2410.08695 (2024)."}, "questions": {"value": "- This paper points that these static benchmarks are inefficient, but you conduct the experiment and obtain that high correlation with existing benchmark, this make me confuse. If your works obtain a similar conclusion, what the necessary of your work?\n\n- This work is proposed based that existing static benchmarks are inefficient, so I want to know the difference of performance with the traditional benchmarks and yours, like, if there is one model which is good at others but bad at yours, or contrast.\n\n- Please report results for current LVLMs available (e.g., Intern3-VL, Qwen3-VL).\n\n- This pipeline point that ‘leakage and self-enhancement bias when the same family of models writes and takes the exam’, so I want to know how about the performance with the same family such as Qwen3-series."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hQslgKjnyG", "forum": "efJnaeB3vo", "replyto": "efJnaeB3vo", "signatures": ["ICLR.cc/2026/Conference/Submission12514/Reviewer_DcV9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12514/Reviewer_DcV9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818541525, "cdate": 1761818541525, "tmdate": 1762923381891, "mdate": 1762923381891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AutoDavis, an automatic and dynamic evaluation protocol for large vision-language models (LVLMs). Unlike traditional static benchmarks, AutoDavis can dynamically generate evaluation datasets on-demand using text-to-image synthesis and LVLM-based question–answer generation. The pipeline includes hierarchical aspect generation, semantic graph–guided prompt diversification, self-validation for image–text alignment, and multi-examiner evaluation to reduce bias. It supports five core evaluation aspects—basic, spatial, semantic, reasoning, and atmospheric understanding—and shows strong correlation (Spearman ρ = 0.817) with human-curated benchmarks such as MMMU, suggesting reliability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "-  Benchmarking LVLMs dynamically is an important and emerging challenge as static datasets become saturated and prone to leakage.\n\n- Systematic design: The modular pipeline (aspect generation, semantic graph, self-validation, option adjustment) is thoughtfully structured, and the authors provide theoretical guarantees for diversity and alignment.\n\n- Strong empirical study: Evaluates 11 LVLMs with extensive analysis across difficulty levels, examiner configurations, and bias controls.\n\n- Correlations and validations: Human studies and comparison with MMMU substantiate reliability.\n\n- Practical implications: Demonstrates that AutoDavis can both evaluate and generate synthetic data useful for fine-tuning LVLMs."}, "weaknesses": {"value": "- Lack of discussion why such new protocol will be adopted by community.\n\n- Weak novelty in methodology: Many core mechanisms (semantic graph diversity, self-validation, error-driven adjustment) are adapted from prior works like TIFA and AutoBencher, with limited algorithmic innovation.\n\n- Lack of quantitative clarity: While AutoDavis claims dynamic flexibility, there is no quantitative analysis comparing generation diversity or cost-efficiency with prior dynamic benchmarks (e.g., MME-Unify, LENS).\n\n- Evaluation reliability: Since LVLMs serve as both examiners and subjects, it remains unclear how much bias persists even with the multi-examiner setup.\n\n- Limited impact of results: The performance gap among models largely mirrors existing benchmarks, suggesting AutoDavis may not yet reveal qualitatively new insights about LVLM capabilities.\n\n- Presentation issue: The paper is lengthy and reads more like a system report; tighter focus on unique contributions would help."}, "questions": {"value": "How do you ensure examiner–evaluatee independence when many models share similar training corpora?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QhhZve4aAG", "forum": "efJnaeB3vo", "replyto": "efJnaeB3vo", "signatures": ["ICLR.cc/2026/Conference/Submission12514/Reviewer_jUFi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12514/Reviewer_jUFi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762334195733, "cdate": 1762334195733, "tmdate": 1762923381468, "mdate": 1762923381468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}