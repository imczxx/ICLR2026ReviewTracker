{"id": "JLUxTl1Um6", "number": 21955, "cdate": 1758324052718, "mdate": 1759896894239, "content": {"title": "AIRE-Prune: Asymptotic Impulse-Response Energy for State Pruning in State Space Models", "abstract": "State space models (SSMs) often sacrifice capacity, search space, or stability to offset the memory and compute costs of large state dimensions. We introduce a structured post-training pruning method for SSMs — AIRE-Prune (Asymptotic Impulse- Response Energy for State PRUN(E)ing ) — that reduces each layer’s state dimension by directly minimizing long-run output-energy distortion. AIRE-Prune assigns every state a closed-form asymptotic impulse-response energy based score, i.e., the total impulse-response energy it contributes over an infinite horizon (time), and normalizes these scores layer-wise to enable global cross-layer comparison and selection. This extends modal truncation from single systems to deep stacks and aligns pruning with asymptotic response energy rather than worst-case gain. Across diverse sequence benchmarks, AIRE-Prune reveals substantial redundancy in SISO and MIMO SSMs with average pruning of 60.8%, with average accuracy drop of 0.29% without retraining while significantly lowering compute.", "tldr": "State Pruning of Deep State Space Model (S5) using a closed form scoring method inspired by Asymtotic Impulse Response Energy.", "keywords": ["State Space Models", "Pruning", "Impulse Response", "S5", "State Pruning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/50e0b7bdea999535d06ed294746eb0cbf7eb6059.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes AIRE-Prune, a post-training compression method to reduce the dimension of latent state space of a trained state-space model. The method is based on the idea of maximally preserving the energy of LTI systems while truncation. Results show that on Long-Range Arena, roughly $60\\\\%$ of states can be pruned without defecting the model's performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The results of compressing S5 on LRA look compelling. The proposed method seems to perform significantly better than existing methods.\n* The energy score of every state can be easily computed without resorting to large matrix operations or simulations, making the approach efficient for large models."}, "weaknesses": {"value": "* The discussion of energy, while intuitively, is not rigorous. More theory of why the proposed method tends to work better than the existing ($\\mathcal{H}\\_\\infty$-based) ones would strengthen the paper.\n* The experiment section could involve more diagnostic experiments. For example, it would be interesting to see which states are pruned by the proposed method and more diagnosis around the kink would be useful.\n* While I believe the paper contains nice ideas, its presentation has significant issues:\n   * \\\\citep and \\\\citet are completely misused.\n   * Similarly, en dashes (- in LaTeX) and em dashes (-- in LaTeX) are also misused all over the place.\n   * ICLR papers use boldface letters for matrices and vectors.\n   * I would avoid the subsection-style, which could disrupt the overall narratives of the paper.\n   * The paper is not proofread as it contains many typos. For instance, on line 151, the equation is not properly cross-referenced; on line 150, you wrote \"zero-order-hold\" but you use \"zero-order hold\" elsewhere, which is more standard; the caption of Figure 2 misses the right parenthesis, etc.\n\nThe paper, in its current form, is not acceptable. I would raise my score to 4 if the author(s) could fix the presentation issues outlined above. I will be happy to further increase the score if my questions below are properly addressed."}, "questions": {"value": "1. I am confused about eq. (9) and the statement above that \"the total layer energy is additive across modes.\" According to the definition in  eq. (5), the energy is defined as the $L^2$ norm of the transfer function. For a diagonal LTI system, the transfer function is the sum of partial fractions that correspond to all states. However, there is no guarantee in general that these partial fractions are mutually orthogonal in $L^2$. If so, how can it be that the total layer energy is additive across modes?\n2. Can you show the energies of the states being pruned in your experiments also as a function of \"pruning ratio\"? This could potentially explain the \"elbow\" that you observe in Image, Pathfinder, and PathX.\n3. Have you tried the same method on S4; if so, how does the result compare?\n4. How does the model benefit from post-training compression in terms of time and space usage?\n5. While I understand intuitively why the $\\mathcal{H}_2$-based method works, as explained in the paper, I don't have intuition in why it performs better than an $\\mathcal{H}\\_\\infty$-based one. Can the author(s) provide more justification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rcVBEqCnMt", "forum": "JLUxTl1Um6", "replyto": "JLUxTl1Um6", "signatures": ["ICLR.cc/2026/Conference/Submission21955/Reviewer_EJJM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21955/Reviewer_EJJM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761009126680, "cdate": 1761009126680, "tmdate": 1762941996624, "mdate": 1762941996624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AIRE-Prune, a structured post-training pruning method for state space models (SSMs) that uses asymptotic impulse-response energy to identify and remove redundant states. The method assigns each state a closed-form energy score representing its total output contribution over infinite time, then uses layer-wise prefix normalization to enable global cross-layer pruning decisions. Evaluated on S5 MIMO models across Long Range Arena benchmarks, AIRE-Prune achieves 60.8% average state pruning with only 0.29% accuracy degradation without retraining, substantially outperforming the prior LAST method (33% pruning at 0.52% loss). The approach extends classical modal truncation from single systems to deep SSM stacks and provides both typical-case (H₂ energy) and worst-case (H∞) theoretical justifications."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strong Theoretical Foundation: The paper elegantly bridges classical control theory (modal truncation) with modern deep learning (layer-adaptive pruning). The energy-based criterion has clear physical interpretation: states with low Eᵢ = ‖C:,i‖²₂‖Bi,:‖²₂/(1-|λᵢ|²) contribute minimal long-run output energy.\n\nPractical Algorithm with Closed-Form Solutions: Unlike iterative methods, AIRE-Prune requires only: (1) compute Eᵢ per mode, (2) sort and compute prefix sums, (3) apply global threshold. No matrix inversions or optimization loops needed.\n\nImpressive Empirical Results: Achieving 60.8% pruning at 0.29% accuracy loss substantially improves over LAST (33% at 0.52% loss). The step-function accuracy profiles (Figure 4) suggest the method effectively separates critical from redundant states.\n\nComprehensive Mathematical Analysis: Appendix B derives H∞ certificates showing ε ≤ κ(ρ)min{∑√Eᵢ, √|T|∑Eᵢ}, proving the energy-based method also bounds worst-case distortion. This dual justification is valuable.\n\nActionable Architectural Insights: Layer-wise profiles show task-dependent patterns—some layers can be entirely removed (enabling block-structured speedups), which is more deployment-friendly than fine-grained sparsity."}, "weaknesses": {"value": "Severely Limited Experimental Scope:\n- Only S5 models evaluated: No experiments on Mamba, Mamba2, or hybrid architectures which dominate current practice\n- Only LRA benchmark: Missing speech (Speech Commands), language modeling (WikiText), or modern long-context tasks\n- Input-selective SSMs (Mamba) have input-dependent B, C—the energy formulation assumes these are fixed. How does AIRE extend to this case?\n\n\nIncomplete Comparison with LAST:\n\n- Table 1 shows different pruning ratios per task, making direct comparison difficult. Need controlled experiments at {30%, 40%, 50%, 60%, 70%} pruning with both methods. Computational cost comparison missing: Is AIRE faster than LAST to compute scores?\n\n\nMissing Key Experiments:\n-No retraining: Even 1-2 epochs could show full potential. Other SSM pruning work achieves near-zero loss with minimal fine-tuning\n- No ablations: What if using median normalization instead of prefix-sum? How sensitive to ε?\n- No failure analysis: Why does ListOps only tolerate 20% pruning? Is this fundamental or fixable?\n\n\nLimited Technical Analysis:\n\n- Equation 8 assumes |λᵢ|²ᵀ → 0, but convergence rate matters for numerical stability near the unit circle. Claim that entire layers can be removed needs gradient flow analysis—does this harm subsequent fine-tuning?\n-No discussion of how to extend to non-diagonal parameterizations (e.g., low-rank structured matrices)"}, "questions": {"value": "- Mamba Extension (Critical): Can you extend AIRE to input-dependent SSMs? For Mamba, where Bₜ = sB(xₜ), Cₜ = sC(xₜ), how would you estimate Eᵢ? Monte Carlo over calibration data? Expected energy under input distribution?\n\n- Direct LAST Comparison: Can you provide results at matched pruning ratios (e.g., both methods at 50%, 60%) to help me get an equivalent comparison? What is the compute cost ratio for scoring?\nRetraining: What accuracy can you achieve with 1-5 epochs of fine-tuning after AIRE pruning? This would strengthen claims about practical deployment.\n-Layer Collapse: When entire layers are removed, does this create training instabilities if later fine-tuning? Have you tested this?\n\n-Failure Modes: ListOps only tolerates 20% pruning. Is this because: (a) the task truly needs full capacity, (b) S5 architecture is suboptimal for this task, or (c) AIRE scoring doesn't capture syntactic structure dependencies? Can you investigate?\n\n-Computational Complexity: What is the wall-clock time for computing AIRE scores vs. LAST scores for a typical S5 model? Is O(n) per state vs. O(n) amortized?\n\n\nI appreciate the authors for their wonderful work. I learned a lot while reading the manuscript.  I do understand that I asked a lot of questions. However, as long as the critical and decent number of other questions/concerns are answered, I am happy to increase my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dXP11bFK8Z", "forum": "JLUxTl1Um6", "replyto": "JLUxTl1Um6", "signatures": ["ICLR.cc/2026/Conference/Submission21955/Reviewer_HyDj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21955/Reviewer_HyDj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603328629, "cdate": 1761603328629, "tmdate": 1762941996339, "mdate": 1762941996339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel state pruning technique for SSMs. The core idea is to use the total impulse-response energy as a criterion to identify and remove less important states. The authors demonstrate that this method enables more aggressive pruning ratios compared to existing baselines, achieving new state-of-the-art results on the benchmark task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  Novel Pruning Criterion: The paper proposes a new and theoretically motivated pruning technique for SSMs. \n2.  Strong Empirical Results: The method achieves state-of-the-art performance on the tested benchmark (S5 model), effectively demonstrating its capability to outperform prior pruning approaches in that specific setting."}, "weaknesses": {"value": "1.  **Clarity of Methodology (Sections 3.2 & 4):** The paper's core methodology is difficult to understand as written. Section 3.2 lacks clarity and citations to fully grasp. Furthermore, the theoretical part (Section 4) is hard to understand\n\n2.  **Limited Evaluation and Generalizability:** The empirical validation is a significant weakness. The approach is only tested on a single model (S5) and a single benchmark. This narrow scope makes it impossible to assess the generalizability of the technique. It remains unclear whether these performance gains would translate to other important SSM architectures (e.g., Mamba) or to different tasks and datasets. The authors should expand their evaluation to include more models and benchmarks.\n\n3.  **Unclear Motivation and Practical Impact:** The paper's motivation for state pruning is not well-articulated, and its practical impact is obscure. The authors claim, for example, to prune \"60% of states,\" but it is unclear what this means in practice. In many modern SSMs (like Mamba), the state itself represents a small portion of the total parameters. The paper fails to connect state pruning to crucial downstream metrics. The authors must clarify:\n    * What is the impact of state pruning on the total parameter count?\n    * Does this pruning translate to tangible latency improvements (e.g., in inference or training) or memory reduction?\n\n    Without this context, the practical benefits of the proposed method are unclear.\n\nMinor:\n\n* Missing Citations: Several claims and components lack proper attribution.\n    * Section 3.1 makes assertions that require supporting citations.\n    * The S5 model is mentioned in Section 3.2 but is not cited.\n    * There appears to be a missing reference at L151.\n* Undefined Acronyms: The terms \"CT\" (Continuous-Time) and \"DT\" (Discrete-Time) are used before they are formally defined, which could confuse readers."}, "questions": {"value": "1.  **Motivation and Practical Impact:**\n    * Could the authors provide a clearer motivation for state pruning, especially in the context of models like Mamba where the state itself is a small fraction of the total parameters?\n    * When the paper claims to prune \"60% of states,\" what is the corresponding impact on the total parameter count of the model?\n    * Does this state pruning translate into tangible performance gains, such as reduced latency (in inference or training) or lower memory usage? We request that the authors provide empirical measurements for these metrics.\n\n* **Generalizability:**\n    * How well does the proposed impulse-response energy criterion generalize beyond the S5 model? Have the authors tested this approach on other significant SSM architectures, such as Mamba or S4?\n    * To what extent are these results dependent on the specific benchmark used? How does the method perform on other tasks or datasets?\n\n* Could the authors revise Sections 3.2 and 4 to make them more readable"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "mR0kJK5NHF", "forum": "JLUxTl1Um6", "replyto": "JLUxTl1Um6", "signatures": ["ICLR.cc/2026/Conference/Submission21955/Reviewer_p2pk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21955/Reviewer_p2pk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891989902, "cdate": 1761891989902, "tmdate": 1762941995932, "mdate": 1762941995932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a post-training pruning method for state space models (SSMs) that reduces state dimensions by minimizing long-run output energy distortion. Unlike previous worst-case gain approaches, their method assigns each state a closed-form asymptotic impulse-response energy score. These scores are normalized layer-wise for global comparison. The method achieves good results on Long Range Arena benchmarks, pruning 60.8% of states on average with only 0.29% accuracy drop without retraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Good empirical results: 60.8% average pruning with only 0.29% accuracy drop without retraining on LRA\n- Closed-form solution for importance scores seems efficient\n- Energy-based metric is well-motivated from control theory perspective and has clear mathematical grounding\n- Works for both SISO and MIMO SSMs"}, "weaknesses": {"value": "- Minor: missing reference details in line 117\n- Limited to diagonal/diagonalizable SSMs and doesn't extend to input-selective models like Mamba\n- Only evaluated on Long Range Arena without speech and language benchmarks\n- No retraining experiments to show potential further improvements\n- Comparison mainly against only one recent baseline (LAST)\n- No discussion of computational overhead of computing energy scores\n- Limited analysis of why certain tasks (ListOps) are more sensitive to pruning"}, "questions": {"value": "Are entire layers ever actually pruned? The paper states: \"our method can help achieve low latency SSM model as we are able to prune layers\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "49wZbqQ2Wg", "forum": "JLUxTl1Um6", "replyto": "JLUxTl1Um6", "signatures": ["ICLR.cc/2026/Conference/Submission21955/Reviewer_4JBM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21955/Reviewer_4JBM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946938849, "cdate": 1761946938849, "tmdate": 1762941995318, "mdate": 1762941995318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}