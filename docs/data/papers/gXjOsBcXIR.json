{"id": "gXjOsBcXIR", "number": 6859, "cdate": 1757998596001, "mdate": 1759897887746, "content": {"title": "VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL", "abstract": "With the rapid advancement of AI-generated videos, there is an urgent need for effective detection tools to mitigate societal risks such as misinformation and reputational harm. In addition to accurate classification, it is essential that detection models provide interpretable explanations to ensure transparency for regulators and end users. To address these challenges, we introduce VidGuard-R1, the first video authenticity detector that fine-tunes a multi-modal large language model (MLLM) using group relative policy optimization (GRPO). Our model delivers both highly accurate judgments and insightful reasoning. We curate a challenging dataset of 140k real and AI-generated videos produced by state-of-the-art generation models, carefully designing the generation process to maximize discrimination difficulty. We then fine-tune Qwen-VL using GRPO with two specialized reward models that target temporal artifacts and generation complexity. Extensive experiments demonstrate that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing benchmarks, with additional training pushing accuracy above 95%. Case studies further show that VidGuard-R1 produces precise and interpretable rationales behind its predictions.", "tldr": "We present VidGuard-R1, the first multimodal LLM fine-tuned with reinforcement learning to detect and explain AI-generated videos, achieving state-of-the-art accuracy with interpretable reasoning.", "keywords": ["Discriminator", "MLLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e6abf72dadefa745c17cba179fd8a0d65083fe8b.pdf", "supplementary_material": "/attachment/120d5bcae8ec94007c5960fab04e5a326aa21dc1.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces VidGuard-R1, a multimodal large language model (MLLM)-based system designed for detecting AI-generated videos and providing interpretable explanations for its decisions. The model utilizes reinforcement learning (RL) and group relative policy optimization (GRPO) to fine-tune a Qwen-VL model for better accuracy and reasoning capability. VidGuard-R1 targets temporal artifacts and generation complexity to improve detection performance. It achieves high accuracy on several benchmarks and presents detailed case studies showing its ability to generate interpretable rationales behind predictions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Dataset Scale: The paper introduces a dataset of 140k real and AI-generated videos designed to challenge current detection methods.\n2. Introduce Explanation in AIGV Detection: the ability to explain why a video is classified as real or fake.\n3. Advanced Fine-Tuning Techniques (GRPO and RL): The use of GRPO (Group Relative Policy Optimization) and reinforcement learning for fine-tuning is a notable strength. The GRPO-TA and GRPO-Q variants further enhance performance by targeting temporal artifacts and generation complexity, offering additional levels of refinement that set VidGuard-R1 apart from previous methods.\n4. Paper Clarity: The paper presents a clear and structured methodology for VidGuard-R1, and the results are presented clearly in tables and figures that allow for easy comparison of VidGuard-R1’s performance against other methods."}, "weaknesses": {"value": "1. Dataset Generalization: While the paper introduces a well-curated dataset, its generalizability could be questioned, as it only includes videos generated by HunyuanVideo and CogVideoX. The paper does not explore the performance of models when trained on the proposed dataset and evaluated on others. \n2. Model Generalization: the lack of cross-dataset validation raises concerns about the model’s ability to generalize to unseen data or models outside of these two specific generative systems. The paper does not explore the performance of VidGuard-R1 when trained on one dataset and evaluated on others, or trained on some AIGV model generations and tested on other unseen AIGV model generations.\n3. Ground-Truth Labeling Reliability: The method of conditioning CoT rationales on ground-truth labels from a more powerful model may compromise the genuineness of the model's discrimination ability. Since the annotations are directly influenced by the 72B model’s reasoning, the 7B model might struggle to perform true reasoning on its own, leading to potential overfitting to the provided cues rather than developing independent judgment.\n4. Innovation in the framework: Existing works such as Q-insight and VQ-insight have already employed similar reinforcement learning-based approaches GRPO for optimizing Qwen2.5-VL 7B models and improving their reasoning abilities. The main difference in this paper seems to be the use of a new reward method or a new dataset rather than introducing a fundamentally new approach or technique.\n5. Limited Explanation Comparison and Model Comparison: The explanation comparison fails to include a more powerful model like Qwen2.5-VL 72B, GPT5, Grok, etc., which is crucial for a more comprehensive evaluation of the explanation quality."}, "questions": {"value": "1. Have you tested the model on unseen AI-generated video models? How does the performance of VidGuard-R1 compare when it is trained on AIGV model-generated videos from one source and tested on videos generated from other unseen AI models?\n2. Could you provide more details on how VidGuard-R1 performs when trained on your proposed dataset but evaluated on other datasets or with video content generated by different AI models?\n3. Given that the CoT rationales are conditioned on ground-truth labels from a more powerful model (Qwen2.5-VL 72B), how do you ensure that VidGuard-R1 can develop independent reasoning without overfitting to the potentially biased or predefined cues provided by the larger model?\n4. Given that similar approaches, such as Q-insight and VQ-insight, have used GRPO for optimizing multi-modal models, can you clarify the novel contributions of your work, especially in relation to GRPO? How does the reward method or the new dataset significantly advance the existing literature?\n5. Could you provide a more comprehensive comparison of VidGuard-R1’s explanation quality against state-of-the-art models (such as GPT-5 or Qwen2.5-VL 72B) to give a clearer understanding of how VidGuard-R1’s interpretability holds up against larger, more capable models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JXrM0jguGY", "forum": "gXjOsBcXIR", "replyto": "gXjOsBcXIR", "signatures": ["ICLR.cc/2026/Conference/Submission6859/Reviewer_3kcH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6859/Reviewer_3kcH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760957449448, "cdate": 1760957449448, "tmdate": 1762919114362, "mdate": 1762919114362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VidGuard-R1, a multi-modal large language model (MLLM)–based system for AI-generated video detection and explanation. The work addresses growing societal risks from realistic video generation by developing a reasoning-capable authenticity detector that produces both accurate judgments and interpretable explanations. The model fine-tunes Qwen-VL using a two-stage framework: supervised chain-of-thought (CoT) initialization followed by reinforcement learning with Group Relative Policy Optimization (GRPO) and two specialized reward models—one emphasizing temporal artifacts (GRPO-TA) and another focusing on generation complexity (GRPO-Q). The authors construct a large dataset of real and synthetic videos generated by recent diffusion-based models and demonstrate that VidGuard-R1 achieves strong zero-shot and fine-tuned performance on several benchmarks, accompanied by qualitative examples illustrating its reasoning process."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an important societal problem by adapting reasoning MLLMs for deepfake and synthetic video detection.\n- The combination of supervised CoT initialization and RL-based GRPO fine-tuning is conceptually sound and clearly explained, with reward models thoughtfully designed to encourage temporal and quality-aware reasoning.\n- The curated dataset is large, diverse, and standardized to minimize shortcut cues such as duration or resolution differences, making it well suited for robust learning. The dataset can be of good value to the community as well.\n- Experimental results show consistent and often state-of-the-art accuracy across multiple benchmarks, including GenVideo and GenVidBench, with competitive zero-shot generalization.\n- The inclusion of reasoning traces and qualitative explanations enhances transparency and potential user trust, distinguishing this work from prior black-box detectors.\n\n- The paper is well written and easy to follow, with clear explanations of both the methodology and its motivation. Figures and examples  effectively convey the reasoning process and highlight the interpretability of the model’s predictions."}, "weaknesses": {"value": "- Although interpretability is a stated goal, the paper does not evaluate the quality of explanations beyond qualitative examples. It remains unclear whether the rationales are causally consistent with correct predictions or merely plausible-sounding. Established metrics (e.g., LLM-as-judge, human evaluation, or coherence scores) should be adopted to substantiate this claim.\n\n- Several key ideas have been explored in prior work on image and video authenticity detection (e.g., SafeWatch, DeMamba-XCLIP, or recent surveys on AI-generated media detection). The paper could more clearly articulate what is fundamentally new in its contribution beyond integrating existing elements.\n\n- The related work section omits relevant prior literature on video safety and explainable detection, such as SafeWatch (ICLR 2025) and other text-to-video safety models, which share similar objectives and techniques. A more explicit comparison—methodologically and empirically—would strengthen positioning.\n\n- The training dataset, while large, is largely synthesized from one or two generative sources (e.g., HunyuanVideo-I2V, CogVideoX). This raises concerns about generalization to unseen generation models. Evaluating across broader generative distributions or conducting cross-model tests would better support claims of robustness."}, "questions": {"value": "- The statement that “directly assigning a reward of 1 to real videos and 0 to fake ones presents challenges” is not justified theoretically or empirically; a short ablation or analysis on this design choice would clarify its necessity.\n\n- Will the dataset and code be publicly released for community use?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ib9TQCAV3J", "forum": "gXjOsBcXIR", "replyto": "gXjOsBcXIR", "signatures": ["ICLR.cc/2026/Conference/Submission6859/Reviewer_1fnv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6859/Reviewer_1fnv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753870094, "cdate": 1761753870094, "tmdate": 1762919113945, "mdate": 1762919113945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a detector finetuning MLLM using GRPO producing 2 reward models, one based on temporal artifacts and the other focused on image generation quality. They also produced an additional dataset with standardised formatting of the videos so as to prevent models from taking shortcuts to easily identify real and fake videos. The authors also produce CoT data that supports the production of text that can be used as a possible explanation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: The way the finetuning procedure is performed is non-trivial and proposes interesting ideas. The methodology is simple but effective. \n\nS2: The way the dataset is constructed, and the special attention in avoiding models taking shortcuts is appreciated.\n\nS3: The availability of rich CoT data could be really impactful and useful for many applications, but ..."}, "weaknesses": {"value": "W1: The way the CoT data is generated is likely to produce hallucinations. The models may easily base their explanations based on prior knowledge about video artifacts, and the model could be correct at random. \n\nW2: Claims about interpretability should be made cautiously: the generated text is simply a sequence of tokens, not really an explanation of the model's decision."}, "questions": {"value": "Q1: Could the authors discuss or introduce quality checks to ensure the CoT data is high quality?\n\nQ2: The idea of using the number of diffusion steps to control the quality is interesting, have you considered also manipulating other hyperparameters (e.g. changing the number of frames?)\n\nQ3: Is there a specific rationale for the hcoice of those 2 specific temporal artifacts? Indeed they're quite different from naturally occurring ones. Is there a chance that applying more realistic ones could result in even better performance?\n\nIn relation to W1,W2,Q1: It would be interesting if the authors could show 1) human studies to assess the quality of the explanations, 2) produce a study on how many times the explanations are in disagreement with humans and whether the disagreement is caused by an hallucination or by the fact humans leverage different cues. These would also guarantee the produced CoT data is high quality and can be more broadly useful for the community. \n\nI am very favourable to increasing my score if these concerns are addressed, as I think it will greatly increase the impact of the author's work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8UxT3SsTkM", "forum": "gXjOsBcXIR", "replyto": "gXjOsBcXIR", "signatures": ["ICLR.cc/2026/Conference/Submission6859/Reviewer_AX7C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6859/Reviewer_AX7C"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772996303, "cdate": 1761772996303, "tmdate": 1762919113520, "mdate": 1762919113520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VIDGuard-R1, a video authenticity detector leveraging multi-modal large language models (MLLMs) and reinforcement learning with group relative policy optimization (GRPO). The approach involves fine-tuning Qwen-VL via supervised and reinforcement learning with two reward models: one designed to incentivize temporal artifact detection and another to guide the model towards assessing generation quality using diffusion step progression. The authors also curate a challenging dataset of 140k real and AI-generated videos. Extensive experiments show VIDGuard-R1 outperforms prior methods and baselines on multiple benchmarks and provides interpretable, step-by-step rationales for its decisions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper advances the state-of-the-art by developing the first MLLM-based, RL-enhanced video authenticity detector that not only improves accuracy but also generates detailed, interpretable reasoning, as supported by the architecture overview in Figure 1 and illustrated in Figures 3 and 4 showing stepwise explanations.\n\n2.  Introduction of specialized reward models one that encourages careful temporal reasoning (GRPO-TA) and another that uses diffusion step quality signals (GRPO-Q) is a thoughtful step toward aligning model incentives with the challenges of real/fake video discrimination. The mathematical formulations are clearly presented, especially the explicit conditions and reward assignment logic.\n\n3. The authors curate a large, carefully controlled real/fake video dataset (140k paired samples) where low-level cues and trivial metadata are equalized, thus demonstrating scientific rigor in experimental setup. The commitment to removing “shortcut” biases strengthens the validity of their results."}, "weaknesses": {"value": "1. The article proposes a method for identifying fake videos using an enhanced reasoning model and incorporates additional video-related designs in the reward scheme. Although it cannot be ignored the author's contribution of applying enhanced reasoning training to a specific field, the overall approach still follows common training techniques, limiting its overall innovativeness.\n\n2. While the curated dataset is a step forward, its scope is limited by only including videos generated by HunyuanVideo and CogVideoX. As acknowledged in Section 5.1, this restricts the external validity and robustness of the detector. The lack of inclusion of generative outputs from other recent or diverse models (such as those evaluated in the GenVideo or GenVidBench test sets) in the training phase could lead to overfitting to specific generation artifacts. The importance of this limitation is amplified by the single-source pairing and should be further addressed experimentally.\n\n3. The use of reward thresholds and hyperparameters, for example $\\alpha_1$, $\\alpha_2$, and $\\mu$ in Equation (GRPO-TA, Section 3.3.2) or step mapping function $g(\\cdot,\\cdot)$ in GRPO-Q (Section 3.3.3), is not sufficiently justified theoretically nor by ablation for practical robustness. While Table 5 presents some grid search results for $\\alpha_1$ and $\\alpha_2$, the rationale for selecting these ranges and assessing sensitivity is shallow—potentially undermining reproducibility if the optimal region is narrow."}, "questions": {"value": "1. How is the detection capability for ood videos?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GHLTBRk4TG", "forum": "gXjOsBcXIR", "replyto": "gXjOsBcXIR", "signatures": ["ICLR.cc/2026/Conference/Submission6859/Reviewer_acmo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6859/Reviewer_acmo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991942931, "cdate": 1761991942931, "tmdate": 1762919113088, "mdate": 1762919113088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}