{"id": "CGLlZsWWhj", "number": 1312, "cdate": 1756869924085, "mdate": 1762933667931, "content": {"title": "Ariadne: Advancing Real-world Path-finding Capabilities of VLMs via Difficulty-aware Reinforcement Learning", "abstract": "Recent advances have seen Vision-Language Models (VLMs) achieve impressive reasoning capabilities, largely demonstrated on tasks like mathematical problem solving via reinforcement learning. However, whether such methods can extend the fundamental reasoning bounds of VLMs to out-of-distribution complexities remains an underexplored question, as the cumulative and interconnected nature of knowledge in domains like mathematics makes it difficult to create truly isolated training and testing splits. To address this, we investigate multistep spatial reasoning, a domain where task difficulty can be systematically controlled. We introduce Ariadne, a training and evaluation framework centered on pathfinding puzzles where complexity is precisely defined by path length and turn count. This allows us to train on a curriculum of simpler puzzles and evaluate generalization on quantifiably harder, unseen tasks (e.g., training on paths with $\\le$3 steps and testing on paths with $\\ge$5 steps). Our experiments reveal that while a strong base model like Qwen-VL-7B-Instruct fails on paths longer than two steps, our model, trained with RLVR, successfully generalizes to solving five-step puzzles unseen during training. This result demonstrates that reinforcement learning can genuinely extend the intrinsic reasoning capabilities of VLMs. Surprisingly, although trained exclusively on synthetic mazes, Ariadne demonstrates performance gains on real-world benchmarks like MapBench and ReasonMap, showcasing that core spatial reasoning skills transfer effectively even when the visual inputs, from simple mazes to complex real-world maps, are entirely distinct.", "tldr": "", "keywords": ["Reinforcement Learning", "Vision–Language Model", "GRPO"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/4f503013550f1a1329f4948451ec7d9d32d8ed8c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Ariadne, a reinforcement learning framework for training and evaluating multimodal large language models (MLLMs) on pathfinding puzzles. Ariadne is deliberately designed for controllability, allowing precise examination of the dynamics of RL-based MLLM training. Experimental results show that while current state-of-the-art vision-language models struggle with even simple maze-solving tasks, reinforcement learning enables them to generalize effectively to unseen and more challenging environments. Moreover, Ariadne achieves notable improvements on real-world benchmarks such as MapBench and ReasonMap, despite being trained exclusively on synthetic maze data."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The problem statement is clear: investigating multi-step spatial reasoning provides an effective lens for assessing MLLMs’ visual reasoning abilities that extend beyond conventional mathematical reasoning. \n2. The out-of-domain evaluation on real-world map reasoning serves as a useful exploratory experiment, illustrating that RLVR trained purely on synthetic data can potentially transfer its learned spatial reasoning skills to complex, real-world settings."}, "weaknesses": {"value": "1. Contribution is limited. The paper adopts an existing training algorithm (GRPO) and dataset (Alphamaze) without any modification or new insight, which is already done in the original Alphamaze paper. The only novelties are the new training dataset difficulty distribution and a difficulty-aware reward function. However, no ablation study is performed to demonstrate the effectiveness of the dataset distribution and reward function design.\n2. Lack of thorough description of the research content. Before section4, there is no introduction or description of the training dataset used in the main experiment, making the design of the dataset distribution and the reward function confusing in section3. \n3. The claimed performance gains on real-world dataset is not convincing. In Table2, Ariadne underperforms raw Qwen2-VL and Qwen2.5VL in two metrics respectively, still lagging significantly behind the strong proprietary models like OpenAI-o3.\n\nOverall, this paper fails to deliver novel insights or contributions to the research community. The proposed methods and design choices are inadequately motivated and lack rigorous justification. Furthermore, the experimental results do not convincingly support the claimed contributions. In its current form, the paper does not meet the standards of originality, methodological rigor, or empirical validity required for publication. I therefore recommend rejection."}, "questions": {"value": "1. It is important to elaborate the difference of this work with the training in AlphaMaze\n2. What is the motivation of the Gaussian-like distribution of the training dataset? Have the author try to compare it with normal distribution?\n3. It is also important to include more details about how the trained models solve the out of domain map dataset, is there any behavior shift?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YKOHOFEaIT", "forum": "CGLlZsWWhj", "replyto": "CGLlZsWWhj", "signatures": ["ICLR.cc/2026/Conference/Submission1312/Reviewer_txtD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1312/Reviewer_txtD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761336527558, "cdate": 1761336527558, "tmdate": 1762915732030, "mdate": 1762915732030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "PBDbB4Pia7", "forum": "CGLlZsWWhj", "replyto": "CGLlZsWWhj", "signatures": ["ICLR.cc/2026/Conference/Submission1312/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1312/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762933325106, "cdate": 1762933325106, "tmdate": 1762933325106, "mdate": 1762933325106, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Ariadne, a training and evaluation framework that uses reinforcement learning (specifically GRPO - Group Relative Policy Optimization) to enhance the spatial reasoning and path-finding capabilities of Vision-Language Models (VLMs). The key innovation is systematic difficulty control through path length and turn count, enabling rigorous evaluation of whether RL can genuinely extend VLM reasoning capabilities to out-of-distribution complexities."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper is a nice practice of applying GRPO on an existing dataset (AlphaMaze) and testing on existing benchmarks (MapBench and ReasonBench)."}, "weaknesses": {"value": "**1. Limited Technical Contribution**\n\ni. The paper's core technical contribution is limited, primarily consisting of a reward function design (Algorithm 1) and a difficulty-controlled sampling strategy for an existing dataset. The training methodology itself applies standard GRPO without modifications, and the evaluation uses existing benchmarks (MapBench, ReasonMap).\n\nii. More critically, the paper lacks ablation studies to validate its design choices. Specifically: (1) no ablation on the reward function components (why 0.2 vs 0.1 scaling factors? why scale by turn count?), (2) no comparison with simpler reward designs (e.g., binary correct/incorrect rewards), (3) no analysis of whether the difficulty-controlled sampling distribution is optimal compared to alternatives (e.g., uniform sampling, different Gaussian parameters), and (4) no ablation on prompt template design. Without these ablations, it is unclear which components are essential versus incidental to the reported improvements.\n\n**2. Limited Dataset Transparency and Methodology**\n\ni. The paper suffers from insufficient citation and unclear attribution of the AlphaMaze[1] dataset, which forms the foundation of the entire training framework. While AlphaMaze is briefly mentioned once in line 043, this single reference is inadequate given the dataset's central role in the work. Throughout the rest of the manuscript, including the methodology (Section 4.1.2) and results sections (Section 4.3.1), the authors refer to \"AlphaMaze training set\" and \"AlphaMaze test set\" without proper citations, creating ambiguity about whether this is an existing benchmark or a novel contribution.\n\nii. The paper omits essential information about the original AlphaMaze dataset, including its total size, construction methodology, and statistical properties. Readers cannot determine: (1) whether the ~2,000 selected samples represent the full dataset adequately, (2) how the authors' controlled difficulty distribution (Figure 1) compares to AlphaMaze's original distribution, or (3) what sampling biases may exist. This lack of transparency undermines both proper attribution to prior work and the reproducibility of the experimental design.\n\n**3. Limited Comparisons**\n\ni. Unfair baseline setup. Baselines (Qwen2.5-VL-7B-Instruct, LLaMA-3.2-11B, InternVL3-8B) are evaluated zero-shot, while Ariadne is trained on 2,000 AlphaMaze samples. This makes the comparison invalid and does not show GRPO is necessary. The below baselines are simpler but important to verify the proposed methods and at least some of them should be evaluated:\n\n- SFT on the same 2,000 samples\n- Few-shot prompting\n- Distill on closed-source models' trajectories\n\n\nii. No evaluation of closed-source models. The paper omits comparisons with state-of-the-art proprietary models (i.e. GPT-4o) on both AlphaMaze and real-world benchmarks. It can be at least reference signals for your results.\n\n\n**4. Misaligned results**\n\n- Performance of Qwen2-VL-7B-Instruct and LLaMA-3.2-11B-Vision-Instruct  does not align with MapBench's[2] original paper. Can the authors answer why?\n\n\n**5. Wrong citation format**\n\nThe author should use \\citep for most of the citations in the paper.\n\n[1] Dao, Alan, and Dinh Bach Vu. \"AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO.\" arXiv preprint arXiv:2502.14669 (2025).\n\n[2] Xing, Shuo, Zezhou Sun, Shuangyu Xie, Kaiyuan Chen, Yanjia Huang, Yuping Wang, Jiachen Li, Dezhen Song, and Zhengzhong Tu. \"Can Large Vision Language Models Read Maps Like a Human?.\" arXiv preprint arXiv:2503.14607 (2025)."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bzl0yuhR49", "forum": "CGLlZsWWhj", "replyto": "CGLlZsWWhj", "signatures": ["ICLR.cc/2026/Conference/Submission1312/Reviewer_8H4T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1312/Reviewer_8H4T"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758946725, "cdate": 1761758946725, "tmdate": 1762915731912, "mdate": 1762915731912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Using Group Relative Policy Optimization (GRPO) ARIADNE is trained on synthetic maze-based path-finding puzzles and evaluated on real-world map-navigation benchmarks such as MapBench and ReasonMap. The authors claim that Ariadne enables generalization from simple synthetic mazes to complex, real-world navigation tasks, demonstrating improved out-of-distribution reasoning capabilities and substantially strengthened spatial-visual comprehension."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "ARIADNE shows improved performance on real-world out-of-distribution data after training on significantly different synthetic training data."}, "weaknesses": {"value": "The experimental framework is interesting and technically plausible, but key elements (e.g., the incorrect GRPO formula, lack of ablations, and absence of multiple runs or confidence intervals) weaken the empirical support for the central claims. The current validation does not convincingly demonstrate that the observed gains arise from deeper reasoning improvements rather than pipeline or prompt robustness.\n\n# Presentation\nThe GRPO formula (2), which is the main formula, is wrong (A_i missing). This pattern of lack of attention to detail and confusing presentation remains throughout the paper.\n\nThe readability of the figures is poor and they are hard to read in both formatting and content-wise. Examples include:\n\nFigure 3:\n- text size too small\n- caption is hard to understand; difference between B & D is not directly apparent, same for C & E\n- the range of the y-axis of B & D is comparable and it would be way easier to compare the two if the same limit was used, same for C & E\n- maybe consider merging the plots so that the approaches can actually be compared and at the same time, this allows for bigger text size\n\nFigure 6:\n- “Accuracy (Left)” is wrong, the accuracy plot is on the right.\n\n\n\nMinor:\n- 352: “In addition, the qualitative cases in Figure 4 provide additional insight …” text fluency could be improved\n\n# Method\nSeperate runs, confidence intervals and an ablation study would strengthen the claims. As of now, the validation study of the generalization to real maps does not provide sufficient support for the claims made.\n\nFigure 6 (right) has 5 entries at 50%, while this is possible, it is curious and should be addressed. It seems like it was evaluated on little data and with a single run. Furthermore on the left side the biggest difference is seen in the score for the easy-easy tasks, but this is not reflected in the accuracy for the easy-easy tasks on the right. Similar discrepancies are seen for the easy-hard tasks where the accuracy for the long versions goes down significantly. A more thorough and detailed validation analysis is required. The data presented in Figure 6 also raises questions about the effectiveness of the method, which needs to be addressed more strongly. “The results show that Ariadne delivers consistent gains in long-reasoning regimes, particularly in higher-complexity categories such as easy–middle and easy–hard, which correspond to question complexity and map complexity, respectively.” This is a strong statement considering that the scores do not reflect this. The scores that indicate the opposite should be more clearly addressed.\n\nI remain sceptical on the origin of the gains in path planning skills. How much is purely due to improved robustness of the pipeline and understanding of the task+prompt vs. the claimed improvement in fundamental reasoning capabilities and substantial strengthening of spatial-visual comprehension: “This result demonstrates that reinforcement learning can genuinely extend the intrinsic reasoning capabilities of VLMs.” and “These findings collectively confirm that GRPO training on the synthetic maze dataset substantially strengthens spatial-visual comprehension, yielding a model that stands at the forefront of its scale class for advanced spatial reasoning and real-world navigation tasks.” While the authors provide a list of failure reasons for the baseline: “The baseline Qwen2.5-VL-7B-Instruct frequently suffers from systematic localization and planning errors, including misidentifying target positions, selecting inefficient detours, and ignoring environmental constraints such as rivers, enclosed building walls, or other impassable barriers.” There is no evidence supporting this or further explanations. There is a lack of information and validation to allow for a systematic attribution of capability improvements. Even then, the improvement on real-world benchmarks is interesting but narrow (map-based reasoning). It’s unclear how transferable these benefits are to broader VQA or embodied reasoning tasks.\nContribution\nThe contribution is overstated as mentioned above as well: “In this work, we systematically investigated the spatial reasoning capabilities of GRPO-fine-tuned vision-language models within our controlled complexity framework, Ariadne.”\n\nThe paper extends GRPO-based reasoning work to a path-finding domain. While the experimental setup is appealing and the results on real-world map data are promising, the conceptual novelty is limited. The contribution would be stronger if framed as a domain-specific study of GRPO generalization rather than as evidence of fundamentally new reasoning capabilities."}, "questions": {"value": "Addressing the weaknesses and a more thorough and careful analysis of the generalization to the real maps would make this a valuable contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UkIKK3AYcV", "forum": "CGLlZsWWhj", "replyto": "CGLlZsWWhj", "signatures": ["ICLR.cc/2026/Conference/Submission1312/Reviewer_AW46"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1312/Reviewer_AW46"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937994907, "cdate": 1761937994907, "tmdate": 1762915731756, "mdate": 1762915731756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}