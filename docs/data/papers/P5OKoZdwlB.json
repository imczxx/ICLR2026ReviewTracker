{"id": "P5OKoZdwlB", "number": 6469, "cdate": 1757986132258, "mdate": 1759897912471, "content": {"title": "MicroMix: Efficient Mixed-Precision Quantization with Microscaling Formats for Large Language Models", "abstract": "Quantization significantly accelerates inference in large language models (LLMs) by replacing original high-precision matrices with low-precision counterparts. Recent advances in weight-activation quantization have primarily focused on mapping both weights and activations to the INT4 format. Although the new FP4 Tensor Cores in NVIDIA’s Blackwell architecture offer up to 4$\\times$ speedup over FP16, existing INT4-based kernels fail to fully exploit this capability due to mismatched data formats. To bridge this gap, we propose MicroMix, a co-designed mixed-precision quantization algorithm and GEMM kernel based on Microscaling (MX) data formats. Tailored for the Blackwell architecture, the MicroMix kernel supports arbitrary combinations of MXFP4, MXFP6, and MXFP8 channels, and produces BFloat16 outputs. To achieve a favorable trade-off between accuracy and efficiency for each linear layer, we introduce quantization thresholds that identify activation elements where lower-precision formats (MXFP4 or MXFP6) incur excessive quantization error. Our algorithm selectively allocates higher-precision channels to preserve accuracy while maintaining compute efficiency. On the Llama and Qwen model families, MicroMix achieves near-FP16 performance across diverse downstream tasks with an average precision of 5 bits. In particular, Qwen2.5-32B-Base and Coder exhibit lossless accuracy on zero-shot, code generation, and mathematical reasoning benchmarks. In addition, on consumer-grade (RTX 5070Ti laptop) and server-grade (RTX 5090) GPUs, our kernel achieves 2.29-3.38$\\times$ acceleration compared to TensorRT-FP16.", "tldr": "A mixed-precision quantization method using MXFP8, MXFP6 and MXFP4.", "keywords": ["Mixed-precision Quantization", "Microscaling Formats", "Post-training Quantization", "Large Language Models"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f1217f8bd43a1190d1fc17ce2355ee84f4bef96a.pdf", "supplementary_material": "/attachment/a2be4e3b1c1b7ceae3fb053aa8435fc1f1f484da.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes MicroMix, a novel mixed-precision quantization approach that leverages the MXFP4, MXFP6, and MXFP8 formats supported by recent NVIDIA architectures. The method performs channel-wise mixed-precision quantization for both weights and activations, assigning different bit-widths based on value distribution thresholds. In addition to the quantization algorithm, the paper presents a system-level co-design, including fused kernels for quantization, permutation, and GEMM operations to minimize latency and memory overhead. Extensive experiments on large language models (Llama3.1-8B and so on) demonstrate that MicroMix achieves state-of-the-art performance without retraining. The work highlights both algorithmic insights and system-level engineering optimizations for efficient mixed-precision inference on modern GPUs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method adopts a channel-wise mixed-precision strategy, which is more flexible and effective than conventional layer-wise approaches. \n\nThe insight for permutation and assign different bit-widths through threshold is conceptually clean yet empirically powerful.\n\nThe work tightly couples algorithmic design with low-level kernel fusion\n\nExtensive experiments have been conducted to prove the effectiveness of MicroMix for both accuracy and system efficiency."}, "weaknesses": {"value": "The work is more engineering-oriented than fundamentally novel from a research standpoint."}, "questions": {"value": "I doubt the permutation and mixed-precision channels multiplication may cause extra latency. \n\nIn Figure 7, what exactly does “end-to-end throughput” measure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LaVWGNr17g", "forum": "P5OKoZdwlB", "replyto": "P5OKoZdwlB", "signatures": ["ICLR.cc/2026/Conference/Submission6469/Reviewer_BbmQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6469/Reviewer_BbmQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761548715482, "cdate": 1761548715482, "tmdate": 1762918857575, "mdate": 1762918857575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MicroMix, a mixed-precision quantization framework designed for Microscaling (MX) floating-point formats (MXFP4, MXFP6, MXFP8), targeting NVIDIA’s Blackwell FP4 Tensor Cores.\nIt introduces:\n\nA quantization threshold–based precision assignment, determining which channels use 4-, 6-, or 8-bit MX formats.\n\nAn offline calibration to precompute per-layer precision ratios.\n\nA fused reorder-and-quantize kernel optimized for MX formats.\n\nMicroMix reportedly achieves near-FP16 accuracy at ~5 bits average precision and up to 3.3× kernel-level speedup on RTX 5090, outperforming FP16 and INT8 baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper targets a relevant and timely topic, leveraging upcoming FP4/6/8 Tensor Cores for efficient LLM inference.\n\nThe system–algorithm co-design is well-executed, integrating quantization thresholds and fused CUDA kernels."}, "weaknesses": {"value": "The main algorithmic ideas—mixed-precision assignment, threshold-based outlier handling, and fused reorder kernels—are incremental extensions of prior works. The combination feels straightforward rather than conceptually new.\n\nThe method mostly refines known strategies (precision partitioning, block scaling) for a specific hardware (Blackwell FP4), rather than introducing a new quantization principle."}, "questions": {"value": "The paper is competently executed and well-written, with solid experiments and system results.\nHowever, the conceptual novelty is limited—it mostly integrates existing techniques (mixed precision + threshold + MX formats + kernel fusion) rather than proposing a fundamentally new approach. Could you clearly articulate what aspect of MicroMix is genuinely novel compared to prior mixed-precision or microscaling quantization methods?\n\nHow sensitive is the threshold-based precision assignment to the choice of calibration data and ratio parameters?\n\nCould the proposed mixed-precision calibration generalize to other quantization formats (e.g., MXINT4/8,NVFP4)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vcc1jOIvr9", "forum": "P5OKoZdwlB", "replyto": "P5OKoZdwlB", "signatures": ["ICLR.cc/2026/Conference/Submission6469/Reviewer_6987"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6469/Reviewer_6987"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901403913, "cdate": 1761901403913, "tmdate": 1762918857005, "mdate": 1762918857005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MicroMix, a mixed MXFP8/FP6/FP4 quantization method that mainly targets NVIDIA's Blackwell generation GPU and benefits from the new FP4 tensor cores. A quantization algorithm is proposed to allocate different bitwidths to different channels, and a fused quantization kernel is implemented to support the dynamic reordering and mixed precision quantization. The performance is evaluated on consumer-grade GPUs (RTX5070Ti laptop & RTX5090), and it demonstrates a 2.29-3.38x speedup compared to the FP16 baseline."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "It is good to see a work that benefits from the new technologies on a cutting-edge GPU and demonstrates end-to-end gains and memory savings."}, "weaknesses": {"value": "- The mixed precision schema partitions the weight+activation into MXFP8+MXFP8, MXFP6+MXFP6, and MXFP4+MXFP4. Among them, MXFP6+MXFP6 does not make sense to me. I believe FP6 has the same peak performance on Blackwell GPUs (correct me if I'm wrong), and quantizing **activations** into FP6 does not reduce the model size, nor increase the inference speed. W6A8 might be a better choice here, and cutlass has an example for W6A8 kernel (https://github.com/NVIDIA/cutlass/blob/main/examples/79_blackwell_geforce_gemm/79c_blackwell_geforce_mixed_mxfp8_mxfp6_bf16_gemm.cu)\n\n- The real efficiency of the implemented kernel is not very clear to me. Although there is a comparison against TRT-FP16, W4A16, and FP8 in section 4.4, the actual utilization (achieved vs peak performance) is not provided.\n\n- Though this paper claims they conduct experiments on server-grade GPUs (RTX5090), the RTX5090 still has the same architecture as consumer GPUs (sm_120) instead of real compute cards like B200 with sm_100 architecture. It is totally understandable that the authors may not have access to these latest-generation GPUs, but I would not recommend claiming the work supports server-grade GPUs. Also, some explanations in the paper (for example, tcgen05.mma instructions) only apply to these compute cards and do not exist on RTX5090."}, "questions": {"value": "- What is the hardware utilization of the mixed precision kernel? I would recommend using the ratio of FP4/FP6/FP8 and the peak performance numbers to calculate the theoretical latency and compare it with the measured latency. (Be aware that on GeForce GPUs, the bitwidth of accumulation (FP32 vs FP16) will affect the peak performance.) I'm concerned about this because MicroMix uses 3 cutlass kernels to perform a single GEMM, and this might lead to some overhead. \n\n- In end-to-end throughput results, why does INT8 quantization provide little performance improvement? Also, what are the performances of the prefilling stage (TTFT) and decoding stage (token/s)?\n\n- I would recommend adding some accuracy+performance results in the same table. How do quantization methods in Table 1 perform?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NblOGUqiui", "forum": "P5OKoZdwlB", "replyto": "P5OKoZdwlB", "signatures": ["ICLR.cc/2026/Conference/Submission6469/Reviewer_MRQT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6469/Reviewer_MRQT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956704724, "cdate": 1761956704724, "tmdate": 1762918856703, "mdate": 1762918856703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a mixed-precision post-training quantization framework using MX formats with 4, 6, and 8-bit channels. The method adaptively assigns precision to channels based on quantization thresholds and fuses channel reordering into the quantization kernel. The approach is designed to leverage NVIDIA Blackwell FP4 tensor cores. Experiments on Llama and Qwen models show better accuracy with approximately five-bit effective precision and latency improvements."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors provide an end-to-end implementation and report real latency improvements on Blackwell GPUs, which increases the paper's practical value. \n\nThe use of flexible bit-width allocation per layer and integration with FP4 tensor core support matches new hardware capabilities. It is timely. \n\nThe empirical results show high accuracy retention across zero-shot, math, and code benchmarks. The fused reorder-and-quantize kernel design is efficient and demonstrates careful engineering."}, "weaknesses": {"value": "Despite the solid engineering contribution, the technical novelty appears limited. The paper extends prior mixed-precision ideas, such as Atom, by allowing flexible channel ratios. The threshold-based assignment and layer-wise granularity resemble existing mixed precision heuristics and tuning methods, so the algorithmic insight is incremental.  Fusing permutation into quantization kernel is primarily an engineering effort. \n\nThe end-to-end speedup analysis uses FP16 and FP8 baselines, but not the low-bit kernels like 4bit. A comparison against an equivalent or similar bit-width kernel would clarify relative benefits. Separating prefilling and decoding latency would also give better insights. The evaluation could extend to more settings of different batch sizes, context lengths and decoding lengths. Right now only a few settings are covered. The evaluated models are only Llama and Qwen language models.   \n\nFigure 6 compares kernel speedups against FP8, FP16, and W4A16, but it remains unclear how much of the speed improvement arises from the authors’ design versus underlying CUTLASS kernels. A breakdown of kernel time contributions would clarify the engineering contribution.\n\nFor the accuracy comparison, the proposed method MicroMix is only set to ~5 bit. However, there are baselines like QuaRot use ~4bit.  It would be fair to also show ~4bit result for MicroMix, as well as for the latency comparison."}, "questions": {"value": "Rather than only showing results around five bit width, could you please analyze results at different target bit widths such as approximately four, five, or six bits to demonstrate the flexibility of the method and its ability to adapt to different accuracy and latency budgets?\n\nCould you please analyze more model architecture like different attention modules and MoE etc to show its generality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h85hTUI2iW", "forum": "P5OKoZdwlB", "replyto": "P5OKoZdwlB", "signatures": ["ICLR.cc/2026/Conference/Submission6469/Reviewer_x7qv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6469/Reviewer_x7qv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003432466, "cdate": 1762003432466, "tmdate": 1762918856424, "mdate": 1762918856424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}