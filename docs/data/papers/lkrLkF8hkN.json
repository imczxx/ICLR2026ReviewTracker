{"id": "lkrLkF8hkN", "number": 16136, "cdate": 1758260448647, "mdate": 1759897259801, "content": {"title": "OlymBio-Bench: A Multimodal Challenge Towards Expert-Level Biological Reasoning", "abstract": "The evaluation of large language and multimodal models requires benchmarks that go beyond simple knowledge retrieval to assess complex reasoning, especially in scientific domains such as biology. Existing biology benchmarks fall short, either being text-based, too low-level, or lacking the integrative reasoning needed for graduate-level problems. To address this, we introduce OlymBio-Bench, a novel, graduate-level multimodal benchmark for biology. Sourced from over 220 frontier research papers in the life sciences and curated by a dedicated team of over 60 authors and reviewers, our benchmark is uniquely challenging, requiring sophisticated inference and multi-step reasoning within realistic research contexts. A key feature is its inherent multimodality, with a large majority of questions incorporating essential images, diagrams, and data plots that demand integrated visual and domain-specific understanding. We evaluate a range of state-of-the-art models on OlymBio-Bench, and our results reveal that even the most powerful models fail to achieve a passing score, highlighting critical deficiencies in their ability to perform complex, multimodal scientific reasoning. We further demonstrate a strong correlation between question complexity and model failure rates, with multimodal questions posing a more significant challenge than text-only ones. Our findings confirm that OlymBio-Bench is a formidable and unsolved challenge that can serve as a crucial resource to catalyze the development of next-generation AI models capable of more advanced scientific reasoning.", "tldr": "", "keywords": ["Biology", "Reasoning Model", "Multimodal Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/24bca36c687ff217edf857c72185387f4ef0a0eb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces  OlymBio-Bench, a multimodal benchmark for biology sourced from 220 research papers in life sciences and curated by a team of 60 authors and reviewers. The benchmark is used to evaluate a range of state-of-the-art models. Results reveal that sota models fail to achieve a passing score, highlighting critical deficiencies in their ability to perform complex, multimodal scientific reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The work addresses a relevant and important problem.\n\nThe authors evaluate several state-of-the-art language and multimodal models on the benchmark"}, "weaknesses": {"value": "**1.- Disregards previous literature**:\n\nThis work disregards prior relevant benchmarks. Specifically, it does not sufficiently acknowledge or position itself relative to recent unimodal and multimodal biology benchmarks such as MicroBench, MicroVQA, LAB-Bench, BioASQ, just to name a few. Without a clear comparative analysis or discussion of how the proposed benchmark differs from and improves upon these prior efforts, the novelty and incremental contribution of this work remain unclear.\n\n**2. This benchmark appears to focus primarily on chart, diagram, and plot interpretation rather than biological visual understanding.** None of the example questions in the appendix include images of biological specimens at either microscopic or macroscopic resolution. In cell biology, researchers routinely rely on diverse microscopy modalities (e.g., fluorescence microscopy, light microscopy, electron microscopy) to study biological systems. These imaging approaches are foundational to modern biology, yet this benchmark does not include such modalities. As a result, it is difficult to characterize this as a comprehensive “biology” benchmark when one of the core pillars of biological research (microscopy) is entirely absent.\n\n**suggestion**: Expand this benchmark to include microscopy.\n\n**3. Images are not required to answer the questions:** The example questions suggest that images are largely unnecessary for solving the tasks. For instance, in the first example (Animal Biology), the textual description alone appears sufficient to answer the question, indicating that visual inputs are superficial rather than essential. This raises questions about whether the benchmark meaningfully evaluates multimodal biological reasoning.\n\n**Suggestion:** Quantify the proportion of “multimodal” questions that can be answered correctly without access to the image, and report this statistic. This would help demonstrate that the benchmark genuinely requires visual understanding rather than relying primarily on text. I also recommend reviewing the Cambria benchmark and considering mechanisms used there to ensure that tasks are truly vision-centric; similar design principles could strengthen this work.\n\n**4. Missing essential dataset descriptors:** The paper omits key dataset details. For instance, the total number of questions in the benchmark is not clearly stated anywhere in the manuscript. Based on Table 1, one might infer that the dataset contains approximately 1,000 questions, but this should be explicitly reported. Clear dataset statistics are critical for evaluating the scale and significance of the benchmark.\n\n**Suggestion**: Please provide more datset stats."}, "questions": {"value": "Suggestions are shared above ^"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Nd8G3a1xsv", "forum": "lkrLkF8hkN", "replyto": "lkrLkF8hkN", "signatures": ["ICLR.cc/2026/Conference/Submission16136/Reviewer_iFPm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16136/Reviewer_iFPm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881255170, "cdate": 1761881255170, "tmdate": 1762926305517, "mdate": 1762926305517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OlymBio-Bench, an expert-level multimodal benchmark for biology. The benchmark contains 363 problems, most of them multimodal. It spans multiple subfields in biology. Questions are sourced from practice sets originally developed for participants in International Biology Olympiad. Evaluation shows that state-of-the-art models do not perform well on the benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The fact that state-of-the-art models underperform on OlymBio-Bench shows that it is of sufficient difficulty and points to weaknesses in current models.\n+ Its focus on \\textbf{multimodal} biology problems is a strength, since they are difficult to source."}, "weaknesses": {"value": "- The abstract claims questions are \"sourced from over 220 frontier research papers in life sciences\" but Section 3.1 says \"All questions were sourced from practice sets originally developed for participants in International Biology Olympiad\". This seems conflicting. Where do the questions come from exactly?\n- 363 questions is on the lower side for a benchmark.\n- As a benchmark paper, data quality is the most important factor for consideration. But this paper only includes a few examples. It would be better if there was a link to an anonymous site hosting the data for review.\n- The paper currently stands at only 7 pages. A lot more details on curation, dataset examples & analysis, and error analysis can be added. I also find the error analysis to be a little superficial."}, "questions": {"value": "- Rules permitting, would it be possible to provide a link to your an anonymized repository of the questions?\n- Could the authors provide additional details on dataset source and curation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dKvp6KulzK", "forum": "lkrLkF8hkN", "replyto": "lkrLkF8hkN", "signatures": ["ICLR.cc/2026/Conference/Submission16136/Reviewer_aBYe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16136/Reviewer_aBYe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913624627, "cdate": 1761913624627, "tmdate": 1762926305054, "mdate": 1762926305054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents **OlymBio‑Bench**, an expert‑level multimodal biology benchmark constructed from IBO‑style problems, emphasizing integrated reasoning over text and figures. The dataset contains **363 items** spanning five subdisciplines, with most items including at least one figure (Sec. 3.1, p.3; Fig. 2–3, p.4). Models (Gemini‑2.5, GPT‑5 family, Grok‑4, Qwen‑2.5‑VL) are evaluated zero‑shot with an exact‑match scoring pipeline based on a fixed final‑answer string (Sec. 3.2, p.5). **No model passes 60%;** the best is gemini‑2.5‑pro at 51.79% overall, with text‑only items generally easier than multimodal ones (Table 1, p.6; Sec. 4.1, p.5–6)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. **Expert‑level multimodal focus**\n- Majority of items are image‑dependent, aligning with authentic biological analysis (Fig. 2 right, p.4), which raises ecological validity for VLM evaluation. \n- Breadth across five subdisciplines supports cross‑topic reasoning (Fig. 3, p.4), increasing coverage beyond narrow tasks. \n- Examples in the appendix tie questions to research‑style scenarios, illustrating realism (A.1, pp.10–13). \n2. **Documented curation pipeline**\n- Multi‑stage process with three independent reviews plus a revision specialist (Fig. 1; Sec. 3.1, p.3) supports quality control and reduces single‑author bias. \n- Final selection of 363 items targets “high discriminatory power and strong reasoning demands” (Sec. 3.1, p.3), aligning with aims. \n- Additional human expert inspection after sampling provides an extra QA layer (Sec. 3.1, p.3)."}, "weaknesses": {"value": "1. **Similarity to existing biology benchmarks is under‑analyzed**\n- The paper itself cites LAB‑Bench (multimodal figure/table interpretation and protocol tasks), VCT (multimodal virology), and Bix‑Bench (computational‑biology agents) but does not systematically differentiate OlymBio‑Bench from these resources beyond general claims (Sec. 2.2, pp.2–3). This leaves novelty vs. prior art ambiguous. \n- No side‑by‑side comparison (tasks, item formats, difficulty, modality mix) or cross‑benchmark evaluation/transfer study is provided—e.g., overlap with LAB‑Bench FigQA/TableQA is asserted qualitatively but not quantified (Sec. 2.2, pp.2–3). No direct evidence found in the manuscript. \n- The claim that OlymBio covers “a more comprehensive range of biological subjects” than HLE is not accompanied by a definition of HLE or a tabulated coverage comparison (p.4). This reduces clarity of the distinct contribution. \n2. **Release and confidentiality are in tension**\n- The Abstract/Intro says “We release OlymBio‑Bench,” while Sec. 3.1 states questions come from IBO practice sets under confidentiality and are “not publicly available,” creating ambiguity about what is actually released (Abstract, p.2; Sec. 3.1, p.3). \n3. **Under‑specified evaluation setup**\n- Sampling parameters (temperature, top‑p, seed, max tokens) for each API are not reported (Sec. 3.2, p.5). No direct evidence found in the manuscript. Image preprocessing (resolution, cropping) is not described, though core to multimodal performance (Sec. 3.2, p.5). No direct evidence found in the manuscript. The paper’s own error analysis notes formatting errors in weaker models, showing the brittleness of regex‑only parsing (Fig. 5, p.7; Sec. 3.2, p.5)."}, "questions": {"value": "**Novelty vs. prior art:** Please provide a side‑by‑side comparison with LAB‑Bench, VCT, and Bix‑Bench (task types, item format, modality mix, difficulty) and define HLE to substantiate distinct contributions (Sec. 2.2, pp.2–3; p.4)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JxMg2AVIbK", "forum": "lkrLkF8hkN", "replyto": "lkrLkF8hkN", "signatures": ["ICLR.cc/2026/Conference/Submission16136/Reviewer_NMuL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16136/Reviewer_NMuL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987136744, "cdate": 1761987136744, "tmdate": 1762926304175, "mdate": 1762926304175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces OlymBio-Bench, a multimodal benchmark designed to evaluate expert-level biological reasoning. The dataset consists of 363 curated problems derived from International Biology Olympiad (IBO) materials and recent research topics. Each question is reviewed through a multi-stage pipeline involving item authors, multiple reviewers, and a revision specialist. Experiments show that even the strongest models fail to reach human-level accuracy with performance dropping significantly on multimodal and longer questions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The dataset’s construction pipeline comprises authoring, triple-reviewing, and expert revision, ensuring reliability and conceptual correctness, reducing annotation bias and improving scientific validity.\n2.The benchmark effectively exposes current LMMs’ limitations in visual-biological reasoning. The observed performance degradation with increased question length or image complexity provides actionable insights into model weaknesses."}, "weaknesses": {"value": "1.Insufficient transparency in data collection and annotation. Although the paper mentions multiple roles (authors, reviewers, revision specialists), it lacks key details: the total number of contributors in each role, their expertise levels, and how reviewer feedback was structured and incorporated. Annotation criteria and conflict-resolution protocols are not clearly defined.\n2.Limited dataset statistics and diversity analysis. Beyond distributions of question length and number of images, the paper does not quantify diversity across question types or the variety of image modalities.\n3.Unclear answer format. It remains unclear whether the evaluation is limited to multiple-choice questions or also includes short-answer and open-ended formats.\n4.Lack of comparison with existing benchmarks. While the paper discusses general benchmarks (e.g., GPQA), it does not provide empirical comparisons or difficulty calibration against them.\n5.Limited evaluation depth. Current analysis focuses mainly on modality (image vs. text). The paper should include breakdowns by biological subfields and cross-category performance, as well as tests with research-oriented agents (e.g., OpenDeepResearch [1]) to better reflect real scientific reasoning capabilities."}, "questions": {"value": "please refer to weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XlkehWwPw6", "forum": "lkrLkF8hkN", "replyto": "lkrLkF8hkN", "signatures": ["ICLR.cc/2026/Conference/Submission16136/Reviewer_AMLL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16136/Reviewer_AMLL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762407909296, "cdate": 1762407909296, "tmdate": 1762926303724, "mdate": 1762926303724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}