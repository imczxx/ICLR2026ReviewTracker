{"id": "nOcy5NvNI1", "number": 21032, "cdate": 1758313040505, "mdate": 1759896946059, "content": {"title": "Constantly Improving Image Models Need Constantly Improving Benchmarks", "abstract": "Recent advances in image generation, often driven by proprietary systems like GPT-4o Image Gen, regularly introduce new capabilities that reshape how users interact with these models. Existing benchmarks often lag behind and fail to capture these emerging use cases, leaving a gap between community perceptions of progress and formal evaluation. To address this, we present ECHO, a framework for constructing benchmarks directly from real-world evidence of model use: social media posts that showcase novel prompts and qualitative user judgments. Applying this framework to GPT-4o Image Gen, we construct a dataset of over 35,000 prompts curated from such posts. Our analysis shows that ECHO (1) discovers creative and complex tasks absent from existing benchmarks, such as re-rendering product labels across languages or generating receipts with specified totals, (2) more clearly distinguishes state-of-the-art models from alternatives, and\n(3) surfaces community feedback that we use to inform the design of metrics for model quality (e.g., measuring observed shifts in color, identity, and structure).", "tldr": "", "keywords": ["unified model", "image generation benchmark", "native multimodal model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f0d5a2f179a947b4e5323085f83472b951c8a4c6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the growing gap between the rapid progress of image generation models and the static nature of existing benchmarks. It introduces ECHO, where instead of relying on predefined and manually curated tasks, the framework builds its challenges by mining social media posts about new image models (the case study for this paper is GPT-4o Image Gen) to construct an in-the-wild, re-runnable benchmark. It then details an LLM and VLM-based processing pipeline, where the authors extract structured <input, output, feedback> tuples from messy social media threads and curate a benchmark of around 35K samples. The paper demonstrates that ECHO captures creative, complex, and evolving tasks not present in existing benchmarks, provides stronger differentiation among models, and motivates the development of new quantitative metrics (e.g., colour shift, text rendering accuracy, identity preservation) based on community feedback."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper's central premise (benchmarks must dynamically evolve with model capabilities and user behaviours) is a highly significant and timely contribution. The idea of using community-generated evidence from social media as a source is a novel solution. \n2. The paper is clearly written, visually rich, and easy to follow. The proposed pipeline (ECHO) is detailed and well-motivated. The analysis demonstrates how ECHO surfaces novel tasks, uncovers model failure modes, and differentiates state-of-the-art models.\n3. It is an important contribution to how we evaluate image generation models. The framework could inspire new adaptive benchmarking protocols in other modalities. \n4. Traditional benchmarks for image generation often rely on abstract metrics that correlate poorly with human preference. ECHO’s feedback-derived metrics aim to capture human-relevant error dimensions."}, "weaknesses": {"value": "1. Although ECHO collects a large and diverse dataset, the quality control process heavily depends on LLM and VLM filtering. The paper does not provide any quantitative validation on how accurate the pipeline is. How do the authors handle bias or noisy samples in their final benchmark? The fill-in-the-blank prompts (Section 3.3) could also introduce hallucinated prompts. \n2. The authors analyze bigrams to assess the linguistic diversity of the benchmark, but this is a coarse view of the benchmark. A more informative approach would be to use an LLM to cluster each datapoint into broader task categories, which will be more meaningful to understand the benchmark distribution. It would also reveal potential imbalances, there could be an overrepresentation of certain tasks, which is not currently captured in the analysis."}, "questions": {"value": "1. The framework currently relies on (presumably) Western-centric, English-language social media. How feasible or what challenges would arise to extend ECHO to non-English or region-specific platforms and mitigate this sampling bias. \n2. Can the authors provide a more granular breakdown of the benchmark's composition, as suggested in W2. An analysis of task clusters would be far more insightful than bigrams and would help identify which specific capabilities ECHO is truly testing and how balanced it is.\n3. Based on this analysis, which tasks in the benchmark are most difficult (i.e., highest failure rate for all models) and which are comparatively easier? This would be invaluable for guiding future research."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Lip6j2YCuQ", "forum": "nOcy5NvNI1", "replyto": "nOcy5NvNI1", "signatures": ["ICLR.cc/2026/Conference/Submission21032/Reviewer_eprr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21032/Reviewer_eprr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933806195, "cdate": 1761933806195, "tmdate": 1762940613312, "mdate": 1762940613312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a framework ECHO, that constructs benchmarks by directly probing social media posts from platforms like twitter. The framework is applied to GPT 4o Image Gen, and thereby a collection of 35000 prompts are curated, which are those that are directly used by real-life users. Resultantly, the benchmarks consists of complex and realistic tasks, not included in previous benchmarks. By applying such prompts, state-of-the-art generative models are re-evaluated, and the discovered indicators from the posts help evaluate the models further."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The benchmark is unique, and is of utmost importance, as it ties the general public opinion with model performance. \n2. The authors have put in substantial efforts to extract the prompts from the complex tweet chains.\n3. The framework uncovers significant model failures observed by the users, as shown in Fig. 4."}, "weaknesses": {"value": "1. While I agree that the large-scale images generated from the prompts cannot be manually evaluated, the VLM-human correlation seems quite low, raising questions on the evaluations.\n2. It is unclear how many images have been generated by each generative model for evaluation. \n3. The benchmark consists of several tasks, while Fig. 6 summarizes them. It would be good to capture how the different models on the individual tasks, as virtual try on, novel view synthesis are themselves quite significant as tasks.\n4. Fidelity, Faithfulness and Diversity - the three most important metrics related to text-to-image generations should have been discussed in more detail.\n5. [minor] The resulting observations are similar to the expected results - the closed-source models generally outperform the open-sourced ones. However, that being said, the curated prompts seem useful, as they are obtained from real-life users."}, "questions": {"value": "1. How many images per model did the authors evaluate on the constructed benchmark?\n2. Did the authors try the traditional fidelity and faithfulness metrics? Wherever applicable, how diverse are the generations by the different models? This question is important as the prompts are unique in the proposed benchmark.\n3. Counting and hallucinations have been raised by users as failure cases, and these have been well-established problems in literature. Did authors try measuring them, especially for newer models like FLUX Kontext, GPT etc?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uRmsZZzqjd", "forum": "nOcy5NvNI1", "replyto": "nOcy5NvNI1", "signatures": ["ICLR.cc/2026/Conference/Submission21032/Reviewer_dbsG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21032/Reviewer_dbsG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001993298, "cdate": 1762001993298, "tmdate": 1762940612252, "mdate": 1762940612252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ECHO (Extracting Community Hatched Observations), a framework for constructing adaptive, data-driven benchmarks for image generation models based on real-world user interactions on social media. Motivated by the observation that existing evaluation datasets lag behind rapidly evolving generative capabilities, the authors propose leveraging posts from platforms such as Twitter/X to capture emergent tasks, user prompts, and qualitative feedback surrounding newly released models (e.g., GPT-4o Image Gen). ECHO systematically extracts multimodal data, comprising textual prompts, reference images, and community commentary, using a pipeline that integrates large language and vision–language models (LLMs/VLMs) to filter, contextualize, and structure these inputs into benchmark-ready samples.\n\nEmpirically, the authors curate a dataset of over 35,000 social media posts and develop a benchmark subset containing approximately 1,700 text-to-image and image-to-image tasks. Using this dataset, they evaluate eight leading generative models, showing that ECHO better distinguishes performance differences than conventional benchmarks such as GEdit or CompBench. Beyond quantitative evaluation, the framework also transforms recurring user observations (e.g., color shift, identity drift, text rendering errors) into measurable diagnostic metrics, providing a mechanism to align benchmarking with authentic user concerns. The paper concludes that such community-grounded, continuously updatable benchmarks can serve as a scalable and dynamic alternative to static evaluation paradigms for assessing progress in generative modeling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper articulates a timely problem: benchmarks for generative models cannot keep pace with emerging user behavior. The introduction (pp. 1–2) effectively grounds this issue using the example of “Ghiblification” — a community-invented use case of GPT-4o that no prior benchmark captured. This framing convincingly motivates ECHO.\n\n2. The ECHO pipeline (Figure 2) is the paper’s technical core: Collect relevant social posts, Reconstruct context across replies, Process multimodal data (text + images + screenshots), Filter and classify for quality and benchmarking. The pipeline is modular, well-illustrated, and generalizable. The multimodal LLM+VLM processing steps (using GPT-4o and Qwen-2.5-VL) are particularly novel — they turn noisy social media content into structured benchmark samples.\n\n3 Empirical Validation: The authors benchmark 8 models (open-source and proprietary) across the new dataset. \n\n4 Turning user complaints into quantitative metrics (color shift, face identity, text rendering) is an interesting idea. Figure 8 (page 9) demonstrates these metrics’ power to confirm qualitative observations (e.g., GPT-4o’s “yellow tint” or identity drift). This “closing the loop” contribution elegantly connects community perception and model evaluation."}, "weaknesses": {"value": "1. ECHO’s exclusive reliance on Twitter/X introduces substantial platform- and demographic-specific biases. As acknowledged in *Appendix A*, trending phenomena such as the “Ghibli style” disproportionately influence the sample composition, leading to a skewed task distribution that may compromise benchmark representativeness and fairness. Consequently, models optimized for highly visual or viral content may appear to perform better under this framework. Furthermore, the pipeline is evaluated solely on Twitter/X, without empirical validation across alternative social platforms (e.g., Reddit, Discord, or YouTube), thereby limiting the generalizability of the proposed approach and undermining its claim to universality.\n\n2. Although ECHO is described as “re-runnable,” the paper lacks sufficient methodological transparency for full replication. Critical details concerning data acquisition—such as API endpoints, scraping protocols, temporal sampling strategies, and filtering heuristics—are not disclosed. While Figure D.1 enumerates example keywords, key parameters governing data retrieval, LLM-based relevance scoring, and post-filtering thresholds remain unspecified. This omission impedes reproducibility and may also raise compliance concerns regarding Twitter/X’s data use policies. Moreover, the reuse of user-generated content (including images and textual comments), even in anonymized form, poses potential ethical and legal challenges under data protection regulations such as the General Data Protection Regulation (GDPR).\n\n3. Despite the use of an ensemble of evaluators (GPT-4o, Gemini, and Qwen), the low Kendall’s τ correlation with human judgments (τ ≈ 0.10–0.12) indicates that current VLM-as-a-judge paradigms remain insufficiently reliable for fine-grained assessment of generative quality. Additionally, although approximately 35,000 posts are collected, only ~1,700 high-quality samples are retained for benchmarking. This limited subset, especially when contrasted with larger-scale datasets such as DiffusionDB (≈14 million prompts), raises concerns regarding statistical robustness and the stability of model performance differences. It remains unclear whether the reported distinctions reflect genuine capability gaps or are artifacts of small-sample variance."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1km79C7akI", "forum": "nOcy5NvNI1", "replyto": "nOcy5NvNI1", "signatures": ["ICLR.cc/2026/Conference/Submission21032/Reviewer_Fp5T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21032/Reviewer_Fp5T"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185617981, "cdate": 1762185617981, "tmdate": 1762940611808, "mdate": 1762940611808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing Benchmarks often mirror the capabilities of models to evaluate and understand how models\nperform on tasks of interest. These benchmarks contain short and simple instructions. Authors present ,\nECHO framework for constructing benchmarks using 35000 prompts created from real world (social\nmedia data- Twitter) and used GPT4o for image generation to identify SoTA model capabilities and its\nalternatives , design metrics , complex task which was absent in existing or previous benchmarks. Curated\nprompts were filtered by filtered posts by designing the relevant keywords and relevance of the post text.\nFurther online discussion may have messy conversation which leads to incorrect dataset collection . To\navoid this authors have used LLMs to turn every messy conversation into one clean, self-contained data\nsample that includes: the prompt, the responses, and how good it is. After text refinement , Multimedia\n(Images) requires VLM (Qwen 2.5 VL). After Analysis , 20% Data marked as High Quality\n(Benchmarking) , 66% Data as Medium Quality (Analysis). Due to policy of twitter and openai some\nposts were refused to download or generate and further data removed manually by the author . Following\ntypes of models used such as Unified Models , LLM+Diffusion , Image Editing Models. Instead Human\nEvaluation for large scale authors used VLM as a Judge. As a secondary Validation , authors present five\nexpert raters with outputs of 8 models for 200 samples and ask the annotators to rank the output from best\nto worst for both image to image and image to text splits. Authors framed ECHO could help to\ndifferentiate model performance in the fine grained ways such as Color Shift Magnitude , Face Identity\nSimilarity , Visual structure such as object positioning or human pose."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Paper primary goal is to create a new generative model structured dataset involving users sharing\ninteresting prompts and outputs, novel task ideas, or commentary on model behavior.\n2. Authors present , ECHO framework for constructing benchmarks using 35000 prompts created\nfrom real world (social media data- Twitter).\n3. Designed several specialized automated metrics: color shift magnitude, face identity similarity,\nstructure distance, and text rendering accuracy."}, "weaknesses": {"value": "1. Community perceptions and discussion topics evolve over time. Benchmarks derived from a\nsnapshot of community feedback may not adapt quickly, risking obsolescence or misalignment\nwith current priorities unless actively maintained.\n\n2. Since social media discussions are shaped by active communities, the collected prompts and\nfeedback may reinforce prevailing stereotypes, biases, or misconceptions. This can lead to\nbenchmarks that unintentionally favor certain demographic groups, styles, or cultural contexts,\nthereby limiting fair assessment across diverse user groups.\n3. Leveraging social media data introduces vulnerability to manipulation, such as spam posts,\ncoordinated misinformation, or artificially amplified feedback, which could skew the benchmark\ntoward certain failure modes or artificially elevate the perceived performance."}, "questions": {"value": "1. The success of your approach relies heavily on LLMs and CV models for classification and\nextraction. How sensitive are these models to inaccuracies or biases, and what measures are in\nplace to validate their outputs? Could errors in automated classification impact the reliability of\nthe derived metrics?\n2. Are there classes of failures or use cases that ECHO systematically misses due to its reliance on\nsocial media posts or community feedback?\n3. Are there risks that users might intentionally or unintentionally manipulate community feedback\n(e.g., spamming certain prompts or promoting specific outputs) to bias the benchmark? How does\nyour framework mitigate or detect such scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2ZzmqfphDq", "forum": "nOcy5NvNI1", "replyto": "nOcy5NvNI1", "signatures": ["ICLR.cc/2026/Conference/Submission21032/Reviewer_vt4s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21032/Reviewer_vt4s"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762836475417, "cdate": 1762836475417, "tmdate": 1762940611269, "mdate": 1762940611269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}