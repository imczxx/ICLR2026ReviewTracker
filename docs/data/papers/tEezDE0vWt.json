{"id": "tEezDE0vWt", "number": 16874, "cdate": 1758269788563, "mdate": 1759897214288, "content": {"title": "Temporal Slowness in Central Vision Drives Semantic Object Learning", "abstract": "Humans acquire semantic object representations from egocentric visual streams with minimal supervision. Importantly, the visual system processes with high resolution only the center of its field of view and learns similar representations for visual inputs occurring close in time. This emphasizes slowly changing information around gaze locations. This study investigates the role of central vision and slowness learning in the formation of semantic object representations in humans. We simulate five months of human-like visual experience using the Ego4D dataset and generate gaze coordinates with a state-of-the-art gaze prediction model. Using these predictions, we extract crops that mimic central vision and train a time-contrastive Self-Supervised Learning model on them. Our results show that combining temporal slowness and central vision improves the encoding of different semantic facets of object representations. Specifically, focusing on central vision strengthens the extraction of foreground object features, while considering temporal slowness, especially during fixational eye movements, allows the model to encode broader semantic information about objects.\nThese findings provide new insights into the mechanisms by which humans may develop semantic object representations from natural visual experience. Our code will be made public upon acceptance.", "tldr": "Prioritizing central, gaze-predicted regions and temporal context in egocentric videos improves semantic object representations in self-supervised models, aligning them more closely with human visual learning.", "keywords": ["bio-inspired learning", "human vision", "egocentric learning", "self-supervised learning"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/17ddc1b543b5282f44684e2be8e91575a605d393.pdf", "supplementary_material": "/attachment/f55d590cc76cae07bb2bb77c6111147254b6ef00.pdf"}, "replies": [{"content": {"summary": {"value": "This paper investigates the role of central vision and slowness learning in semantic object learning. Specifically, it simulates the human-like visual experience using ego videos. The experimental results show that the combination of temporal slowness and central vision can improve the encoding of different semantic facets of object representations. Experiments are conducted on various tasks, including object categorization, Fine-grained object categorization, instance-level object recognition, and scene recognition, to validate the image recognition capabilities of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors conducted extensive experiments to validate the effectiveness of the proposed bio-inspired learning strategy.\n\n2. The utilization fo Ego4D dataset and gaze coordinates to mimic central vision is a kind of novel\n\n3. This paper is well organized and easy to read."}, "weaknesses": {"value": "1. The proposed bio-inspired learning is inferior to frames learning on scene recognition under both the ResNet and ViT backbones. This result is not well discussed.\n\n2. The influence of gaze estimation is not thoroughly discussed. The authors leverage a state-of-the-art gaze estimation model to obtain cropped regions that mimic central vision. Would the accuracy of gaze estimation affect the performance of the proposed method? Furthermore, if the salient regions were cropped instead to construct the central vision data, would the method still perform well?\n\n3. This paper contains some typos, such as a missing period in line 093."}, "questions": {"value": "1. Would the accuracy of gaze estimation affect the performance of the proposed method? Furthermore, if the salient regions were cropped instead to construct the central vision data, would the method still perform well?\n\n2. The proposed bio-inspired learning is inferior to frame-based learning on scene recognition under both the ResNet and ViT backbones. Could the authors provide a comprehensive discussion of this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "unFSVU7ymG", "forum": "tEezDE0vWt", "replyto": "tEezDE0vWt", "signatures": ["ICLR.cc/2026/Conference/Submission16874/Reviewer_mtJa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16874/Reviewer_mtJa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761054444504, "cdate": 1761054444504, "tmdate": 1762926900258, "mdate": 1762926900258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the role of central vision and temporal slowness in the formation of semantic object representations in humans. It collects video data from Ego4D dataset and generates gaze coordinates with a existing gaze prediction model. It then crops image regions around gaze coordinates (central vision) and train a time-contrastive self-supervised learning model (temporal slowness).  The experiments assess the impact of learning visual representations with bio-inspired central vision and temporal slowness."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1) The paper aims to study the role of central vision and temporal slowness in the formation of semantic object representations in humans. The motivation is interesting.  \n2) Extensive experiments are conducted to evaluate effectiveness of the proposed feature learning on four downstream vision recognition tasks."}, "weaknesses": {"value": "1) **Lack of technical novelty**. The technical approach of the paper, that is the extraction of gaze-centered image crops with existing gaze prediction model and the self-supervised constrastive learning guided by temporal distance, are very basic technical processes. Thus there is lack of sufficient technical contributions of the paper.  \n2) **Lack of sufficient performance comparison**.  The paper didn't make sufficient comparison with SOTA feature learning methods to validate the superior performance of the proposed method. There are not a few work each year focusing on self-supervised feature learning in the computer vision community, however, the paper only compares with a baseline of “Frames Learning”."}, "questions": {"value": "1) Current gaze prediction methods based on video input are more like saliency prediction which produces heatmap for each image frame. How do you crop the image when such heatmap is distributed like multi-Gaussians?  \n2) Have you considered the cases of gaze transition in a video clip which causes dissimilarity of visual appearance between adjacent framews."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "brOgXVdAXI", "forum": "tEezDE0vWt", "replyto": "tEezDE0vWt", "signatures": ["ICLR.cc/2026/Conference/Submission16874/Reviewer_Qmz9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16874/Reviewer_Qmz9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571977261, "cdate": 1761571977261, "tmdate": 1762926899091, "mdate": 1762926899091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates whether combining central vision via gaze-centered crops with temporal slowness in self-supervised learning improves object-centric representation learning from long egocentric video. The pipeline predicts gaze where ground truth is unavailable, constructs gaze-centered crops, and trains a time-augmented SSL variant that aligns crops across nearby timesteps. Linear-probe evaluations indicate consistent gains for category, fine-grained, and instance recognition, while scene recognition can favor full-frame training. The analysis further explores context learning by comparing representation similarity to object co-occurrence embeddings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The integration of central vision with temporal slowness is a focused, biologically motivated idea that appears to yield practical improvements on object-centric tasks, suggesting value for egocentric and embodied learning communities.\n- The empirical exploration includes reasonable sweeps over crop sizes and temporal pairing that reveal interpretable behavior and replicate across backbones.\n- The end-to-end pipeline is presented clearly with intuitive figures that explain how gaze-centered cropping and temporal pairing interact."}, "weaknesses": {"value": "- There is a heavy reliance on the predicted gaze rather than ground-truth gaze, and it is not accompanied by any calibration/error metrics. The paper could benefit from reporting prediction error characteristics and relating them to performance when ground-truth gaze is substituted for the eye-tracked subset.\n- It remains unclear whether the gains arise from human-like fixations or from general object-biased views. The paper could benefit from controls using fixed center crops, saliency-only crops without temporal constraints, and motion-centric crops that ignore gaze, matched for compute and augmentations.\n-  Important details such as batch sizes, optimizer and learning-rate schedules, EMA settings, total updates, and effective tokens/pixels are not consolidated, making it difficult to rule out under- or over-training effects. The paper could benefit from a unified table and training curves across methods.\n- The method appears less favorable for scene recognition. The paper could benefit from experiments that blend full-frame and gaze-centered crops within batches or adjust crop scales during temporal pairing to probe whether object gains can be retained while narrowing the scene gap.\n- Effect sizes, multiple-comparison adjustments, and potential vocabulary-mapping biases are not fully discussed. The paper could benefit from explicit reporting and robustness checks that vary the mapping pipeline."}, "questions": {"value": "1. How does representation quality change on the eye-tracked subset when training with ground-truth gaze, predicted gaze, fixed center crops, and saliency-only crops? Providing accuracy deltas with confidence intervals would clarify the specific contribution of human-like fixation information.\n\n2. What are the calibration and error properties of the large-scale gaze predictor (e.g., angular error, spatial bias, per-activity breakdown), and how do these correlate with downstream linear-probe metrics when substituting ground truth?\n\n3. Were mixed training regimes attempted (for example, combining full-frame and gaze-centered crops within a batch or varying crop scale for temporally paired views), and how did these affect object-centric and scene-centric metrics?\n\n4. How sensitive are results to the fixation threshold and grouping rules? Histograms of fixation/saccade durations and speeds, plus an ablation over the threshold, would make the dependence more transparent."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ys06K6fGIb", "forum": "tEezDE0vWt", "replyto": "tEezDE0vWt", "signatures": ["ICLR.cc/2026/Conference/Submission16874/Reviewer_pvCu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16874/Reviewer_pvCu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857221368, "cdate": 1761857221368, "tmdate": 1762926898381, "mdate": 1762926898381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors explore the contribution of central vision and slowness learning in visual representations of objects within the human brain via bio-inspired self-supervised learning. Using gaze locations as estimated by a gaze prediction model, the method extracts crops that resemble human central vision and train a time-augmented SSL model that aligns crops from the same short time segments within Ego4D recordings. Through several experiments on different granularities of object and scene recognition using both ResNet and VIT architectures, the authors present evidence for their claim that central vision and slowness learning has a strong effect on learning semantic object-level visual representations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n\n2. The set of experiments and analyses done to present evidence of the claims is quite meticulous and impressive. I want to particularly appreciate the analysis on CKA similarity between learned representations and Glove-based object co-occurrence embeddings.\n\n3. The performance metrics for some of the tasks like fine-grained and instance-level recognition are quite impressive."}, "weaknesses": {"value": "1. Even though the paper focuses on central vision, simply ignoring peripheral vision might overlook many insights to the human visual system.\n\n2. Since the gaze crop consists of almost half of the scene (224/336), and the gaze location is heavily biased to the center (Fig. 10 of the appendix), I am not sure if a gaze location is necessary to crop the frame. What would the results look like if the crop was a 224X224 or 336X336 box centered in the frame?\n\n3. I am not convinced that a biologically plausible method would excel at fine-grained recognition but not as much at recognizing the coarser class labels. The analysis of the same is very hand-wavy in L.304-310, and overlooks the very small or non-existent gains in performance with the bio-inspired learning method."}, "questions": {"value": "1. The authors talk about central vision, i.e., the part of the scene that is fixated, and in L.449, they mention that the crop is parafovea-sized can they elaborate how they map the gaze crop size in pixels to visual angle to infer that it is parafovea-sized?\n\n2. What are the linear probe accuracies as reported in Table 1 for the ablation \"w/o Central Vision\" (provided in Table 2)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cYh8OXHOV5", "forum": "tEezDE0vWt", "replyto": "tEezDE0vWt", "signatures": ["ICLR.cc/2026/Conference/Submission16874/Reviewer_KB1f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16874/Reviewer_KB1f"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958722440, "cdate": 1761958722440, "tmdate": 1762926897835, "mdate": 1762926897835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}