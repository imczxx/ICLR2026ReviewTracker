{"id": "47l2z9Tgk6", "number": 10654, "cdate": 1758178684560, "mdate": 1763039593605, "content": {"title": "Blend-Aware Latent Diffusion: Mitigating Stitched Seams in Image Inpainting", "abstract": "Image inpainting aims to fill missing or masked regions of an image in a manner that blends with the surrounding context. While diffusion models have significantly improved the visual fidelity of inpainting, they still suffer from noticeable stitched seams, including \\textbf{boundary discontinuity} and \\textbf{content inconsistency} between the preserved and generated regions. We argue that these issues originate from a fundamental limitation: the latent blending of the two regions in inference, which unaccounted for in training, creates a piece-wise latent manifold. Firstly, the masked input encoded by VAE does not perfectly align with the resized mask, resulting in boundary discontinuity and the discontinuity will maintain in the reconstruction and denoising process. Second, the piece-wise latent manifold deviates from the assumption of data coherence in diffusion models since the two regions follow distinct distributions, leading to content inconsistency. In this work, we propose \\textbf{Blend-Aware Latent Diffusion}, a unified framework that explicitly resolves these issues by aligning the model's training dynamics with the blend nature of inference. Our framework consists of two complementary components: \\textbf{BlendRecon}, a blend-aware variational autoencoder that learns to decode blended latents continuously; and \\textbf{BlendGen}, a novel denoising loss that explicitly regularizes the generated content to harmonize with the surrounding context. Extensive experiments demonstrate that Blend-Aware Latent Diffusion effectively mitigates stitched seams and improves perceptual quality across various scenarios, including inpainting and outpainting.", "tldr": "", "keywords": ["Image Inpainting", "Seamless Blend", "Stable Diffusion", "VAE"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/4fba5957a6c240e7c1679df11ab184221a26564b.pdf", "supplementary_material": "/attachment/49aaebd4badfa7e82ab904416a32fd4b1812f1e4.pdf"}, "replies": [{"content": {"summary": {"value": "In this work, the authors dive into the stitched seams in diffusion-based image inpainting/outpainting and analyze the underlying causes. We argue that latent blending creates a mismatched mask gap and a piece-wise manifold, resulting in boundary discontinuity and content inconsistency. To address the above issues, the authors propose two key solutions: BlendRecon ensures boundary continuity by enabling the VAE to correct mismatched mask gaps, while the BlendGen simulates the blending operation during training, leading to smoother generation for image content. Extensive experiments demonstrate the effectiveness and robustness of our approaches, indicating the potential to be widely applied in tasks requiring seamless integration."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Easy to read and a promising extension of image inpainting."}, "weaknesses": {"value": "- The motivation of this paper is rather unclear. I believe that the two challenges proposed by the authors actually stem from improper dataset processing:\n\n  - Regarding the first challenge in the first row of Fig. 1, after downloading and examining the BrushBench dataset, I found that the issue (Boundary Discontinuity) actually arises from the dataset itself. Specifically, during the creation of BrushBench, only the main objects were segmented, while the edge details were not carefully processed, leading to boundary discontinuities. If a more appropriate dataset, such as EditBench, were used, this problem might not even exist. Therefore, I strongly question the validity of this “challenge.” Moreover, the authors did not empirically verify that their proposed method can resolve this issue—the experimental results mainly show regular mask occlusions or occlusions with relatively simple surrounding details. This suggests that the so-called challenge is not inherently related to image inpainting itself, but rather caused by the improper dataset construction.\n\n  - For the second challenge in the second row of Fig. 1, namely Content Inconsistency, I agree that this problem can exist. However, I believe the authors have conflated the challenges of traditional image inpainting with those of text-guided image inpainting. This challenge seems more reasonable to propose and address within the context of traditional inpainting. In text-guided inpainting, the masked regions typically remove an entire object along with its boundary areas, whereas traditional inpainting emphasizes arbitrary occlusions across the whole image, where maintaining local consistency with surrounding content is more critical. Therefore, the example shown in Fig. 1 appears to be just an isolated case from BrushBench. Consequently, the comparison with text-guided inpainting methods is highly questionable. Furthermore, the experiments fail to convincingly demonstrate that the proposed approach effectively resolves the Content Inconsistency problem—for instance, in the third row of Fig. 7, I do not consider the emergence of a new bird in the inpainted region to be an optimal or reasonable outcome.\n\n- Blend-Aware Denoiser process lacks necessary visualizations, making it unclear how the L2 term addresses the issue of content inconsistency during the denoising process. The experimental section fails to provide an analysis of computational complexity or a comparison across different models (e.g., SDXL, DiT, and Flow Matching). Moreover, since the role of text is not effectively demonstrated, I believe that comparisons with text-guided image inpainting methods are highly unfair. Instead, comparisons with traditional diffusion-based inpainting methods such as RePaint, CoPaint, DDNM, StrDiffusion and IR-SDE would be much more reasonable.\n\nBased on the above, I believe this submission requires a complete revision — including redoing the experiments and rewriting the manuscript — before resubmission to a venue"}, "questions": {"value": "- The details in Weaknesses\n\n- The motivation of this paper needs to be restated, and the experimental section should better demonstrate the rationale and validity of the proposed motivation.\n\n- The authors should clearly state whether the submission mainly addresses the problem of image inpainting or text-guided image inpainting, as the challenges faced by these two tasks are fundamentally different. Moreover, the paper fails to include a comparison with recent state-of-the-art work, such as CVPR 2025 “Towards Enhanced Image Inpainting: Mitigating Unwanted Object Insertion and Preserving Color Consistency.”\n\n- Additionally, several essential ablation and detail experiments are missing and should be supplemented to strengthen the empirical analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Ztam3mocEr", "forum": "47l2z9Tgk6", "replyto": "47l2z9Tgk6", "signatures": ["ICLR.cc/2026/Conference/Submission10654/Reviewer_cbiB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10654/Reviewer_cbiB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706367708, "cdate": 1761706367708, "tmdate": 1762921907826, "mdate": 1762921907826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "TMkrcYSrIy", "forum": "47l2z9Tgk6", "replyto": "47l2z9Tgk6", "signatures": ["ICLR.cc/2026/Conference/Submission10654/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10654/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763039592599, "cdate": 1763039592599, "tmdate": 1763039592599, "mdate": 1763039592599, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method to align the training dynamics of both the VAE and the denoiser with the piece-wise latent manifold inherent in inpainting tasks. It proposes two key components: BlendRecon and BlendGen. BlendRecon ensures boundary continuity by enabling the VAE to correct mismatched mask gaps, while BlendGen simulates the blending operation during training to produce smoother image content. Additionally, the paper introduces Seam Visibility (SV) to measure stitched seams. Extensive experiments demonstrate that the approach significantly outperforms existing methods in both image quality and seam visibility, effectively reducing boundary artifacts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method is intuitively clear and well-motivated. The design is simple yet effective.\nThe proposed Seam Visibility (SV) metric offers a clear, quantitative way to evaluate seam quality, filling a gap in existing assessments."}, "weaknesses": {"value": "The main concern is that key design choices and evaluation metrics lack clear alignment with their stated objectives. This raises questions about the individual contributions of the proposed components and the validity of the measurement. Please refer to the Questions section for more specific criticism and suggestions."}, "questions": {"value": "In L238, Eq. 7 enables the decoder to learn seamless reconstructions of blended latents, thereby reducing boundary discontinuity. However, the fine-tuning objective in Eq. 7 applies a reconstruction loss over the entire image, which does not explicitly focus on the boundary regions where discontinuities occur.\n\nIn L346, seam visibility (SV) is defined as the average L2 distance of RGB values between pixels along the mask boundary and their neighboring pixels. However, the calculation of Eq. 8 is to process the pixels at the same position of the inpainted images and the original images. \n\nIn the ablation study, neither BlendRecon nor BlendGen alone yields a significant improvement in Seam Visibility (SV), yet their combination leads to a substantial reduction in SV. This raises two questions: (1) Why does the joint use produce a synergistic effect that neither component achieves individually? (2) Since BlendRecon is specifically designed to reduce boundary discontinuities, why does it not lead to a measurable improvement in SV when applied alone?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v6uziDzn3I", "forum": "47l2z9Tgk6", "replyto": "47l2z9Tgk6", "signatures": ["ICLR.cc/2026/Conference/Submission10654/Reviewer_RJi9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10654/Reviewer_RJi9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900297162, "cdate": 1761900297162, "tmdate": 1762921907403, "mdate": 1762921907403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new LDM-based inpainting method. The core problem tackled in the paper is resolving the \"stitch seam\" problem in LDM-based inpainting. The main idea is to fine-tune the LDM components explicitly on blended inputs/latents. Two main components: (i) The VAE is fine-tuned on blended inputs. (ii) The denoiser is also fine-tuned on blended latents. However, the latter is done in a more carefully designed manner: The blended latents are constructed based on the prediction of $z_0$ where noise is then reapplied and true $z_0$ is subtracted to create the training target. Experiments demonstrate that the proposed method achieves state-of-the-art results on BrushBench and MISATO."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The design of the proposed method is plausible and convincing.\n\n- State-of-the-art results."}, "weaknesses": {"value": "- The level of innovation is somewhat low. What the method basically does is fine-tuning the components explicitly on \"seamed\" data.\n\n- A noticeable part is the training-target construction for the denoiser. I get the idea in this \"detouring,\" since the denoiser must have a valid target and input for the seamed latents. However, a question still remains: Is it really necessary to go all the way to the estimate of $z_0$ (I understand that it is a single-step estimate)? It seems to me that a valid target can still be similarly (and more simply) constructed based on $z_t$, $z_0^M$, and predicted $z_{t-1}$.\n\n[minor points]\n\n- The propositions on page 4 are more like remarks rather than propositions. They are quite obvious, and I suggest changing the presentation here.\n\n- The math notations in Algorithm 1 are somewhat poor. I suggest carefully reviewing them."}, "questions": {"value": "Please see the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NHy9wTaU4n", "forum": "47l2z9Tgk6", "replyto": "47l2z9Tgk6", "signatures": ["ICLR.cc/2026/Conference/Submission10654/Reviewer_fdKh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10654/Reviewer_fdKh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762018100666, "cdate": 1762018100666, "tmdate": 1762921906810, "mdate": 1762921906810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new method for fine-tuning latent diffusion models, such as stable diffusion, for inpainting. The authors focus specifically on addressing a common issue with previous approaches, which is visible discontinuities between the inpainted regions of the image and the original image. The approach proposed by the authors introduces separate updates to fine tune the VAE decoder and the denoising network used in the latent diffusion model. The update to the VAE fine-tuning uses a reconstruction loss that specifically reconstructs from a blended encoding of a masked image and the full original image, comparing it to the full original image. This loss is designed to force the decoder to learn to produce continuous images even with discontinuous latent codes. The second contribution is a \"blend aware denoiser\" training procedure, which similarly fine-tunes the denoiser using blended inputs. In their experiments, the authors provide results on BrushBench and MISATO datasets, showing improvements in perceptual metrics such as FID and LPIPS, as well as a user study. The authors also introduce a new metric that measures the L2 loss compared to the original image specifically around the mask boundaries and show that their method improves this as well."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "**Methodology**\n- The proposed method is very simple, directly fine-tuning the existing model rather than using an auxiliary network, and simply adjusting the loss function to account for discontinuities in the inputs. It appears to be easy to implement and applicable other similar latent diffusion models. \n- Despite its simplicity, it is novel to my knowledge. \n- Adding a metric to measure reconstruction specifically around the mask boarder is interesting and could be useful for other works investigating techniques for inpainting.\n\n**Results**\n- The results appear to be strong. On standard metrics the approach outperforms other recent works in inpainting, sometime seemingly by a significant margin.\n- The experiments are performed on recent, high-resolution benchmarks\n- The authors include an ablation study that justifies the use of both contributions for their results\n- The qualitative examples are compelling, clearly looking better than previous methods.\n\n**Paper**\n- The writing of the paper is mostly clear and well-motivated\n- There are numerous qualitative examples shown in the main text"}, "weaknesses": {"value": "**Experiments**\n- The version of stable diffusion (v1.5) used is now somewhat outdated. It makes sense as a benchmark against other methods implemented with that version, but it would be useful to see how this performs on more recent models with updated architectures.\n- The experiments don't compare to conditional models for inpainting or control nets. Some of the cited comparisons do, so this is not a huge issue, but it would be a good update. Similarly, it would also be useful to compare to zero-shot approaches like [1,2, 3] etc.\n- The corresponding prompts used for the results are not shown.\n\n**Writing**\n- It's not clear to me what the value of the theoretical insight section is, as the formalisms are not used elsewhere.\n- The definition of seam visibility is unclear. The text states it's: \"the average L2 distance of RGB values between pixels along the mask boundary and their neighboring pixels.\" but the equation suggests it's comparing the inpainted image to the ground truth.\n- There is no discussion of the computation used for training the model\n- There is no discussion of the potential negative social consequences of undetectable image editing models \n\n[1] Zhang, Bingliang, et al. \"Improving diffusion inverse problem solving with decoupled noise annealing.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n[2] Rout, Litu, et al. \"Solving linear inverse problems provably via posterior sampling with latent diffusion models.\" Advances in Neural Information Processing Systems 36 (2023): 49960-49990.\n[3] Chung, Hyungjin, et al. \"Diffusion posterior sampling for general noisy inverse problems.\" arXiv preprint arXiv:2209.14687 (2022)."}, "questions": {"value": "- Would this work with adaptor methods instead of fine-tuning the full model?\n- Does the fine-tuned model still have equivalent performance in generating full images? \n- How does the method perform with other types of masks. E.g. masks that are not as smooth and convex as the examples shown, such as dropping out random pixels?\n- Would there be any advantage to also using the mask and/or masked image as a conditioning input to this method?\n- Consider citing PixPerfect as concurrent work:\nYao, Yuan, et al. \"PixPerfect: Seamless Latent Diffusion Local Editing with Discriminative Pixel-Space Refinement.\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vgGss1Uy5E", "forum": "47l2z9Tgk6", "replyto": "47l2z9Tgk6", "signatures": ["ICLR.cc/2026/Conference/Submission10654/Reviewer_xB6p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10654/Reviewer_xB6p"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144624505, "cdate": 1762144624505, "tmdate": 1762921906324, "mdate": 1762921906324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}