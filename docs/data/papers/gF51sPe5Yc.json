{"id": "gF51sPe5Yc", "number": 14313, "cdate": 1758232747290, "mdate": 1763073662896, "content": {"title": "CinePile: A Large-Scale Video Question Answering Dataset and Benchmark", "abstract": "Current datasets for long-form video understanding often fall short of providing genuine long-form comprehension challenges, as many tasks derived from these datasets can be successfully tackled by analyzing just one or a few random frames from a video. To address this issue, we present a novel dataset and benchmark, CinePile, specifically designed for authentic long-form video understanding. This paper details our innovative approach for creating a question-answer dataset, utilizing advanced LLMs with human-in-the-loop and building upon human-generated raw data. Our comprehensive dataset comprises 305,000 multiple-choice questions (MCQs), covering various visual and multimodal aspects, including temporal comprehension, understanding human-object interactions, and reasoning about events or actions within a scene. Additionally, we fine-tuned open-source Video-LLMs on the training split and evaluated both open-source and proprietary video-centric LLMs on the test split of our dataset. The findings indicate that although current models underperform compared to humans, fine-tuning these models can lead to significant improvements in their performance.", "tldr": "", "keywords": ["Datasets and benchmarking", "Video understanding", "Multi-modal learning", "Visual question answering", "Long-form video", "Metrics and benchmarks"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/857e99107f45569d53616139b21709b66bdebd4c.pdf", "supplementary_material": "/attachment/0b48d3dee81adb3c113688e15c73d4b55479e82a.zip"}, "replies": [{"content": {"summary": {"value": "In this work, the authors introduce CinePile, a large-scale dataset and benchmark for long-form video understanding. It consists of 305,000 multiple-choice questions, covering various visual and multimodal aspects. Moreover, they evaluate various video-centric MLLMs and also finetune them for performance investigation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Clarity\n\nThe paper is well-written with good structure. Hence, the clarity is basically good.\n\n* Significance\n\nThis paper focuses on evaluating long video understanding of MLLMs, which is an important and practical problem. Hence, the significance is basically OK for video research community."}, "weaknesses": {"value": "Compared to the existing benchmarks, the key contribution of this work is not quite clear. I do appreciate the large scalibility of this dataset. However, the collection process and QA types of this benchmark are basically similar to the exsiting ones.  (Adding Theme type in Table 1  would not change my point too much). Hence, please further clarify the key contributions in this work from these aspects."}, "questions": {"value": "Please see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GnekSX977h", "forum": "gF51sPe5Yc", "replyto": "gF51sPe5Yc", "signatures": ["ICLR.cc/2026/Conference/Submission14313/Reviewer_pbBk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14313/Reviewer_pbBk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795664395, "cdate": 1761795664395, "tmdate": 1762924751034, "mdate": 1762924751034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "0Mh2c9Wdlx", "forum": "gF51sPe5Yc", "replyto": "gF51sPe5Yc", "signatures": ["ICLR.cc/2026/Conference/Submission14313/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14313/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763073662151, "cdate": 1763073662151, "tmdate": 1763073662151, "mdate": 1763073662151, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper builds a very large long-video QA benchmark, CinePile, by aligning short YouTube movie clips to audio description (AD) transcripts and dialogue, then generating multiple-choice questions (MCQs) with LLMs using a template pipeline distilled from prior, human-curated QA datasets (MovieQA, TVQA, Perception Test). The curation includes relevance filtering, adversarial refinement to reduce degenerate questions, and diagnostics such as “visual-dependence” (answerable from dialogue alone?) and “hardness” (answerable even when the model sees the scene text used for authoring). The result is ~303k QAs across 9,396 clips (avg ≈ 160 s), with ~4,941 MCQs in the test split and a taxonomy spanning character/relations, narrative/plot, temporal, setting/technical, and thematic questions. The authors benchmark 24 proprietary and open-source video-LLMs: humans top out ≈73%, the best proprietary model ≈60%, and the best open-source ≈49%. They also show substantial gains when fine-tuning an open-source baseline (Video-LLaVA) on the CinePile training split."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper presents a scale with valuable utility with 303k QAs (298k train, 5k test) as it fills gap between small benchmarks and provides a training resource that yields 71% relative improvement when fine-tuning Video-LLaVA, proving the dataset's practical value beyond evaluation.\n\n2) The paper presents an automated pipeline that combines audio descriptions as free visual annotations, and template extraction via clustering 30k human questions from MovieQA/TVQA/Perception Test + GPT-4 abstraction into 86 templates.\n\n3) The paper presents a multi-layered quality control with adversarial refinement, by implementing degeneracy filtering using three diverse models (Gemini, GPT-3.5, Phi-1.5), then iteratively repairs ~91% of weak questions using LLaMA-3.1-70B adversarial loop rather than simply discarding them, plus vision-dependence and hardness diagnostics.\n\n4) The paper includes a comprehensive evaluation as the benchmark includes 24 models (proprietary and open-source) with detailed breakdowns across 5 question categories, hard-split analysis showing 15-20% performance drops, frame-rate ablations, and dual human baseline results :73% non-authors, 86% authors.\n\n5) The paper presents a strong case by demonstrating automated generation rivals human-curated diversity despite using templates, by achieving a diversity score of 0.45 using semantic similarity metrics, matching Video-MME and exceeding MVBench (0.42) and IntentQA (0.37), \n\n6) The results show a significant human to model performance gap as it states a 24-point gap between best model (Gemini 1.5 Pro: 60%) and human performance (73%). This indicates that there exists a genuine challenge, with even larger 37-point gap to best open-source model, establishing scope for future progress."}, "weaknesses": {"value": "1) The dataset relies heavily on movie clips with audio descriptions (ADs), and QAs are generated from scene text combining dialogue and visual narration. While this approach seems scalable, it may impose stylistic or narrative biases specific to AD conventions or movie genres. Without a per-source or per-genre performance analysis, it is difficult to assess how such biases shape overall model behavior. The paper reports only high-level category distributions (e.g., 41% “Character & Relationship Dynamics”), leaving this aspect somewhat underexplored.\n\n2) The dataset generation pipeline depends on a small set of LMs (Gemini, GPT-3.5, Phi-1.5), and the “repair” phase uses only LLaMA-3.1-70B. Without any cross-model disagreement analysis, it is unclear whether quality-control decisions might be model-specific. If the authors can add a brief dual-judge consistency check, it could help to strengthen the claims of robustness.\n\n3) Proprietary systems (e.g., Gemini 1.5 Pro) process entire videos, while GPT-4o/4V are restricted to 10-frame samples due to API constraints. As a result, leaderboard comparisons may conflate model competence with input access. A fixed-frames or fixed-token evaluation track would offer a fairer, more interpretable comparison.\n\n4) The introduction of a “hard” test split is one of CinePile’s appealing features—it lowers performance significantly, but the criteria for difficulty labeling are only described qualitatively. If the authors can provide  inter-annotator agreement or per-category reliability for hardness tags, it would make this partition more reproducible and reliable.\n\n5) The visual-dependence analysis usefully distinguishes items answerable from dialogue alone versus those requiring visual cues. However, the finding that many items remain dialogue-solvable suggests that some subsets may still underemphasize video reasoning. If the authors can provide a complementary video-only evaluation, it could help quantify this imbalance."}, "questions": {"value": "1) Would you consider reporting performance and frame-scaling trends by source (MovieClips, AD datasets) and by genre? This would help reveal any residual bias introduced by the AD-driven curation pipeline.\n\n2) In the refinement and filtering stages, have you explored using two independent LMs (e.g., Gemini vs GPT-4) and keeping items on which they disagree? Reporting such disagreement rates would enhance the dataset’s reliability claims.\n\n3) Could you release a small category-wise analysis of distractor types (e.g., plausible vs lexical confounders) and the associated error breakdowns on the “hard” split? This would help determine whether model gains reflect reasoning or test-taking heuristics.\n\n4) What were the instructions, time budgets, and evaluation conditions for “author” and “non-author” human annotators? Reporting confidence intervals or variance across annotators would clarify the significance of the observed gap."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eh1yqpaObG", "forum": "gF51sPe5Yc", "replyto": "gF51sPe5Yc", "signatures": ["ICLR.cc/2026/Conference/Submission14313/Reviewer_vxTH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14313/Reviewer_vxTH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866372084, "cdate": 1761866372084, "tmdate": 1762924750619, "mdate": 1762924750619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors introduce a large-scale benchmark for long-form video question answering, trying to target actual multimodal and temporal understanding rather than frame-level recognition. It contains ~305K multiple-choice questions across 9,396 movie clips (~160 s each), derived from human audio descriptions of films (for the visually impaired) aligned with video and dialogue.The dataset was built using an LLM-driven automated pipeline."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Scalable Dataset Generation Pipeline:\nThe paper introduces LLM-driven, human-in-the-loop pipeline that uses audio descriptions and automated template generation to create diverse, long-video QA pairs which is scalable, reproducible, and cost-efficient compared to traditional human-only annotation.\n\nComprehensive Benchmarking and Analysis:\nCinePile is evaluated across 24 open-source and proprietary Video-LLMs, with detailed category-wise results (temporal, narrative, thematic, etc.) and human baselines, providing deep diagnostic insight into the current limitations of multimodal reasoning models."}, "weaknesses": {"value": "Limited domain diversity: CinePile relies almost entirely on movie clips, which constrains generalization to real-world, instructional, or egocentric videos where visual and narrative cues differ significantly.\n\n\nSynthetic question dependency: Despite human-in-the-loop checks, most QAs are LLM-generated, raising concerns about hidden biases, semantic leakage, and superficial reasoning patterns that may not reflect true human-level understanding."}, "questions": {"value": "no much questions at this point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Sg3VxW95jD", "forum": "gF51sPe5Yc", "replyto": "gF51sPe5Yc", "signatures": ["ICLR.cc/2026/Conference/Submission14313/Reviewer_SGML"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14313/Reviewer_SGML"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927362486, "cdate": 1761927362486, "tmdate": 1762924750221, "mdate": 1762924750221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "* Introduces CinePile, a large-scale long-form video understanding benchmark: ~305k MCQ QAs across 9,396 videos (train/test).\n\n* Targets temporal reasoning, human–object interactions, and narrative/plot understanding—explicitly aiming to defeat single-frame shortcuts.\n\n* Built via an LLM-driven, human-in-the-loop pipeline that leverages human audio descriptions aligned to public movie clips; questions are generated without direct video input.\n\n* At evaluation time, models get only raw video + dialogue.\n\n* Benchmarks 24 models; humans outperform best commercial models by ~25% and open-source by ~37%; instruction-tuning on CinePile yields large gains (e.g., Video-LLaVA +71% relative)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Clear problem motivation: Existing “long-video” datasets permit frame-based shortcuts; CinePile stresses genuine temporal/narrative comprehension.\n\n* Scale & utility: Large enough for both instruction-tuning and evaluation; already seeing community adoption.\n\n* Generalizable pipeline: Automated QA generation/verification shown to extend to longer (≤30 min) and non-movie videos with minimal prompt changes.\n\n* Question diversity: Broad coverage (temporal, perceptual, reasoning); includes quantitative diversity metrics and a hard split analysis."}, "weaknesses": {"value": "1.\tOrganization & clarity of exposition\nThe manuscript would benefit from tighter structure and clearer analysis. For example, Section 3 could be integrated as a subsection of Section 2. Section 2 currently reads as procedural rather than analytical; please quantify key elements (e.g., the proportion of “hard” questions detected and the share of weak Q&A filtered). Statements such as “we observed that some questions…” (L213) should be replaced with concrete statistics and definitions of “trivial/basic.” Reporting these ratios will turn the section from a workflow description into evidence-backed analysis.\n\n2.\tVisual reliance policy\nIf visual grounding is a primary objective, consider filtering out non–vision-centric items rather than merely down-weighting them. Alternatively, justify the scoring approach with ablations showing how the visual-reliance score correlates with model performance and dataset difficulty.\n\n3.\tRelated work coverage & comparative positioning\nThe related work and Table 1 under-represent recent benchmarks. Please update with stronger, more current baselines and include a detailed, side-by-side comparison with InfiniBench (released >1 year ago), covering scale, clip duration, domains, question types, and construction pipeline. If CinePile is smaller/shorter, clarify the intended complementary role; if larger/longer in some dimensions, make that explicit.\n\n4.\tTemplate-based generation: motivation & impact\nThe rationale for using templates needs clarification. What benefits do templates provide over free-form generation (e.g., control, coverage, difficulty)? Quantify any trade-offs: linguistic diversity, semantic variety, and question naturalness. If templates risk constraining the benchmark, consider expanding template sets or adding a free-form subset and report diversity metrics across both.\n\n5.\tEvaluation format (MCQ-only)\nLimiting the evaluation to MCQ may mask real reasoning ability. Adding an open-ended evaluation regime (short-answer/free-form with calibrated automatic or human grading) would substantially strengthen claims about long-form understanding and reduce the risk of distractor-driven shortcuts.\n\n6.\tInput-modality ablations\nPlease include a systematic analysis of modality contributions: (i) video-only (no subtitles), (ii) subtitles/dialogue-only (no frames), and (iii) full multimodal input. Reporting the deltas will clarify visual vs. textual reliance and help assess potential leakage or superficial cue exploitation.\n\n7.\tBaselines and recency in comparisons\nSeveral compared methods in Table 2 appear outdated. Please include major video(-LLM) releases from the past year and ensure consistent, transparent evaluation settings (frame rate, sampling strategy, input modalities, prompt formats) to support fair comparisons."}, "questions": {"value": "1- Compare against InfiniBench in detail, as it seems to offer more skills and longer videos.\n\n2- Please detail all the missing vague numbers in Section 2.\n\n3- Add more recent benchmarks and methods to polish the paper.\n\n4- What are the insights of the benchmark? A beneficial benchmark should shed light on the current limitations to guide future research. Therefore, highlighting interesting conclusions is essential."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tRAO1VBfPu", "forum": "gF51sPe5Yc", "replyto": "gF51sPe5Yc", "signatures": ["ICLR.cc/2026/Conference/Submission14313/Reviewer_yLyE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14313/Reviewer_yLyE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762826170257, "cdate": 1762826170257, "tmdate": 1762924749656, "mdate": 1762924749656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}