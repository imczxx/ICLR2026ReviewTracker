{"id": "Z091XLyVkJ", "number": 10454, "cdate": 1758172156840, "mdate": 1759897649951, "content": {"title": "Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception", "abstract": "Fine-grained perception of multimodal information is critical for advancing human–AI interaction. \nWith recent progress in audio–visual technologies, Omni Language Models (OLMs), capable of processing audio and video signals in parallel, have emerged as a promising paradigm for achieving richer understanding and reasoning. \nHowever, their capacity to capture and accurately describe fine-grained details remains limited explored. \nIn this work, we present a systematic and comprehensive investigation of omni detailed perception from the perspectives of the data pipeline, models, and benchmark. \nWe first identify an inherent ``co-growth'' between the level of detail and the degree of hallucination in current OLMs. \nTo address this, we propose \\textbf{Omni-Detective}, an agentic data generation pipeline integrating tool-calling, to autonomously produce highly detailed yet minimally hallucinatory multimodal data. \nBased on the data generated with Omni-Detective, we train two captioning models: \\textbf{Audio-Captioner} for audio-only detailed perception, and \\textbf{Omni-Captioner} for audio–visual detailed perception. \nUnder the cascade evaluation protocol, Audio-Captioner achieves the best performance on MMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and delivering performance comparable to Gemini 2.5 Pro. \nOn existing detailed captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and achieves the best trade-off between detail and hallucination on the video-SALMONN 2 testset.\nGiven the absence of a dedicated benchmark for omni detailed perception, we design \\textbf{Omni-Cloze}, a novel cloze-style evaluation for detailed audio, visual, and audio-visual captioning that ensures stable, efficient, and reliable assessment. \nExperimental results and analysis demonstrate the effectiveness of Omni-Detective in generating high-quality detailed captions, as well as the superiority and human preference alignment of Omni-Cloze in evaluating such detailed captions. \nAll the agentic data pipeline, models, and the benchmark will be open-source to facilitate further research and development for omni detailed perception.", "tldr": "In this work, we presented a complete framework for advancing omni detailed perception from three perspectives: Data, Model, and Benchmark.", "keywords": ["Omni model", "Multimodal large language model", "Detailed captioning", "Audio understanding", "Video understanding", "Benchmark", "Evaluation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/51eeea59b91f0b4d48a76dc3c3d1b5b36eafdc86.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Omni-Detective, an agentic data generation pipeline that uses iterative Query-Observation cycles to produce high-quality, low-hallucination multimodal captions. Based on this data, the authors train Audio-Captioner and Omni-Captioner models for audio and audio-visual detailed perception, respectively, and propose Omni-Cloze, a novel cloze-style benchmark for evaluation. Experimental results show that Omni-Captioner outperforms existing open-source models on benchmarks like VDC and video-SALMONN 2, achieving a strong balance between detail and hallucination."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "# Strengths:\n(+) Innovative data generation：Omni-Detective’s iterative approach with tool-calling effectively reduces hallucination, providing a novel solution for generating high-quality multimodal data.\n\n(+) Superior model performance：Omni-Captioner sets new state-of-the-art results on VDC and achieves the best detail-hallucination trade-off on video-SALMONN 2, surpassing most open-source models.\n\n(+) Robust evaluation framework：Omni-Cloze offers a comprehensive, efficient, and reliable cloze-style evaluation across audio, visual, and audio-visual modalities."}, "weaknesses": {"value": "# Weakness:\n(-) The reliance on a single detective module, while effective, lacks the depth of insight offered by multi-agent systems, potentially limiting its innovation in the generative AI field.\n\n(-) The paper lacks experimental data on the computational cost per iteration, leaving the method’s feasibility in resource-constrained environments unclear.\n\n(-) The paper does not specify how to assess the detective agent’s performance, raising concerns that poor agent output could result in longer, meaningless text rather than valuable information."}, "questions": {"value": "# Questions:\n1. Can Omni-Detective maintain low hallucination rates during multi-round iterations in highly complex scenarios?\n2. Are there plans to evaluate Omni-Captioner’s performance on devices with limited computational resources?\n3. How can specific metrics be designed to quantify the quality of information extracted by the detective agent in each round?\n4. Is there a mechanism to correct errors if the detective agent misidentifies information and mitigate subsequent impacts?\n5. Can the Omni-Cloze benchmark be extended to evaluate longer videos or more complex multimodal interactions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZaKb8V33cC", "forum": "Z091XLyVkJ", "replyto": "Z091XLyVkJ", "signatures": ["ICLR.cc/2026/Conference/Submission10454/Reviewer_AErd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10454/Reviewer_AErd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761216337171, "cdate": 1761216337171, "tmdate": 1762921752457, "mdate": 1762921752457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes Omni-Detective, an agent-based data generation pipeline that automatically produces highly detailed multimodal data with minimal hallucination by integrating tool-calling. Specifically, Omni-Detective interacts with diverse tools, including MLLMs, OCR, and ASR, to extract information from videos and summarize detailed perceptions. Using this information, the LLM integrates all perceptions to generate detailed captions, which can later be used for question–answer tasks. Two captioning models, Audio-Captioner and Omni-Captioner, are trained as part of Omni-Detective.\nTo further validate detailed perception capabilities, the authors introduce Omni-Cloze, a cloze-style evaluation dataset for detailed audio, visual, and audio-visual captioning. Experimental results show that Omni-Detective outperforms existing methods on both existing benchmarks and the Omni-Cloze dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Designing Omni-Detective is a valid and practical approach to fully utilize the perceptual capabilities of existing models for detailed captioning.\n- Introducing the Omni-Cloze evaluation dataset for detailed captioning is valuable to the community. It enables cloze-style evaluation, addressing current benchmark limitations that require multiple LLM calls for assessment."}, "weaknesses": {"value": "- Several important details are missing. For instance:\n  - Which dataset is used for Figure 2 and how are hallucinations detected in long captions?\n  - Which model serves as the detective agent in Figure 3?\n  - What metrics are used in Table 3?\n  - Which datasets are used for joint training and audio-only training?\n  - How is the detail rate evaluated in L406?\n\n- Although AudioCaps or Clotho are not specifically designed for detailed captioning, they can still be used to evaluate audio captioning capabilities. However, they are not included in the experiments.\n\n- In Figure 4, the Omni-Cloze samples appear to include audio-only, visual-only, and audio-visual QAs. In Table 4, for the audio-only model, was the evaluation conducted only on audio-only QAs within Omni-Cloze?\n\n- Error analysis of the data generation pipeline is missing. Even with human validation, such analysis is essential to assess the robustness and reliability of the generated dataset.\n\n- For training Omni-Captioner (and Audio-Captioner), is full finetuning reallly effective? Why not LoRA?"}, "questions": {"value": "### Questions and Suggested Experiments\n\n- Although AudioCaps and Clotho are not targeted for detailed captioning, evaluating on these datasets would help assess the model’s audio captioning capability.\n\n- Since the data generation pipeline involves automation, conducting error analysis after human verification would clarify its robustness.\n\n- Ablation on each modality to examine whether the audio module is necessary for the omni benchmark will help to understand that the model fully exploit all the modalities, and the benchmark requires all the modalities.\n\n### Minor Questions and Suggestions\n\n- In L606, VGGSound was used for training the audio-captioner. Does VGGSound provide ground-truth captions for training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q2tLUL12FE", "forum": "Z091XLyVkJ", "replyto": "Z091XLyVkJ", "signatures": ["ICLR.cc/2026/Conference/Submission10454/Reviewer_1S9o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10454/Reviewer_1S9o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761703992394, "cdate": 1761703992394, "tmdate": 1762921751918, "mdate": 1762921751918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Omni-Detective, an agentic data generation pipeline designed to enhance fine-grained multimodal perception in Omni Language Models (OLMs). It produces highly detailed yet low-hallucination audio–visual data and supports training of Audio-Captioner and Omni-Captioner, which achieve performance comparable to closed-source models (ex, Gemini). The authors also propose Omni-Cloze, a new cloze-style benchmark for evaluating detailed multimodal captioning across audio, visual, and audio–visual domains."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is well written and well structured, clearly presenting the motivation, methodology, and results.\n\n- It achieves competitive or superior performance to closed-source models (e.g., Gemini 2.5 Flash and Pro) across multiple benchmarks.\n\n- The experimental setup, dataset description, and evaluation protocol are presented in exceptional detail, ensuring transparency and reproducibility."}, "weaknesses": {"value": "There are no major weaknesses identified in the paper. The overall work is solid and well executed; only a few minor points of curiosity or clarification remain, which are addressed in the Questions section below."}, "questions": {"value": "- In Table 2, why are proprietary models such as Gemini 2.0 and 2.5 not included in the comparison, despite being used as baselines in other sections? Were these results unavailable or intentionally omitted for consistency?\n\n- Since the core models are fine-tuned from existing architectures, what specific aspects of Omni-Detective or Omni-Captioner represent the novel contribution beyond data quality improvements?\n\n- Are there plans to release the generated dataset and benchmark publicly, and if so, will accompanying annotation or tool-calling logs be shared for transparency?\n\n- When conducting these experiments, how sensitive are the results to hyperparameters such as gradient accumulation, number of training epochs, or learning rate? Even with a strong data pipeline like Omni-Detective, it would be valuable to understand how much small tuning variations influence the final outcomes, based on the authors’ experience."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pWxMMZitOk", "forum": "Z091XLyVkJ", "replyto": "Z091XLyVkJ", "signatures": ["ICLR.cc/2026/Conference/Submission10454/Reviewer_WAFR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10454/Reviewer_WAFR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920417261, "cdate": 1761920417261, "tmdate": 1762921751479, "mdate": 1762921751479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission introduces Omni-Detective, an agentic data pipeline for generating highly detailed yet low-hallucination multimodal captions via iterative tool-calling and evidence verification. \nUsing this data, the authors train Audio-Captioner (audio-only) followed by Omni-Captioner (audio–visual), and claimed that they achieve state-of-the-art results (but this reviewer got some concerns and this matter should be discussed further). \nTo evaluate fine-grained multimodal perception, they also propose Omni-Cloze, a cloze-style benchmark covering audio, visual, and audio–visual tasks, offering stable and efficient assessment with strong alignment to human preference. \nOverall, the work presents a whole stack, including data pipeline, models, and benchmark, for fine-grained, low-hallucination multimodal understanding.\n\nThis reviewer saw the contribution and potential of this submission to the community. However, this reviewer found that the comparative evaluations are conducted in unfair ways. This issue may nullify most of the claims made by comparing with the competing methods. This should be further discussed during the rebuttal."}, "soundness": {"value": 1}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Conceptual clarity: Clearly articulates the \"co-growth\" issue between detail and hallucination and provides an actionable framework to mitigate it.\n\n- Clear demonstration of the effectiveness of the Omni-Detective\n\n- Comprehensive evaluation: Omni-Cloze provides stable, low-cost, human-aligned evaluation across modalities.\n\n- Reproducibility: Extensive appendices with prompts, hyperparameters, and validation details enhance transparency.\n\nOverall, the paper structure, motivation, contribution, and analyses are well implemented."}, "weaknesses": {"value": "- Unfair comparison due to unfair training scheme: The comparison in Table 2 appears not fully fair, as the proposed Omni-Captioner is trained with the additional proposed large-scale, detailed captioning data (≈ 55k audio-only + 15k audio-visual samples) from VGGSound and FineVideo using a two-stage curriculum. In contrast, all baseline results are \"obtained from their original papers\" (as described in the caption of Table 2), meaning those models were applied without any chance to be fine-tuned with comparable detailed captioning datasets, but evaluated in their own (likely general or zero-shot) settings. Although the authors mentioned the proposed Captioners were applied in a zero-shot way, the zero-shot meanings are different (the proposed method is not fine-tuned on any training set related to the benchmark, but it still has an unfair advantage of fine-tuning on additional large-scale detailed captioning data, while the other competing methods do not).\nThus, the proposed model benefits from substantially more task-specific supervision, making the performance gains partly attributable to data scale rather than modeling improvements. The authors should clarify this mismatch or conduct controlled experiments with at least comparable training datasets, so that Table 2 clearly shows the effectiveness of the proposed dataset and training recipe. If there is no comparable dataset for the audio-only case, the authors should provide some baselines at least in an ablation way.\n\n- Due to the above concern, all the claims made with the comparisons against the competing methods are deemed to be nullified. For example, in Table 3, even if the proposed method is additionally fine-tuned, the performance gaps against the other competing methods seem marginal.\n\n- Scalability: While the design of Omni-Detective seems effective according to Fig. 6, it may be bulky and has a trade-off between the effectiveness and computational costs. The multi-step Omni-Detective pipeline may be computationally expensive for large-scale data generation, but no analysis and discussion have been reported."}, "questions": {"value": "Please check the weakness for further discussion.\n\nPersonally, this reviewer would like the work, but the following critical concern hinders giving a positive opinion. \nHowever, the unfair comparison is tightly entangled with the overall contribution claims. It's indeed unfortunate. \nThus, carefully addressing this reviewer's concern would be essential to re-evaluate the submission. Otherwise, the rate would not be changed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pVbu9UFoiF", "forum": "Z091XLyVkJ", "replyto": "Z091XLyVkJ", "signatures": ["ICLR.cc/2026/Conference/Submission10454/Reviewer_Mhp2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10454/Reviewer_Mhp2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984899746, "cdate": 1761984899746, "tmdate": 1762921751121, "mdate": 1762921751121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}