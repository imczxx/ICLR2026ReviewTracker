{"id": "vwgrHsmKuH", "number": 8378, "cdate": 1758080616296, "mdate": 1763113597643, "content": {"title": "Sim2Real-HOI: Sim-to-Real HOI Video Generation via Decoupled Motion–Appearance Diffusion", "abstract": "We present Sim2Real-HOI, a zero-shot framework that closes the sim-to-real gap for hand–object interaction (HOI) video generation using the initial and target poses of both hand and object. Controllable diffusion models like InterDyn and ManiVideo stumble at scale when moving simulation to reality: the quality of generated videos are suboptimal, and they rely on simulator-unobtainable cues such as the first frame. Sim2Real-HOI addresses the problem in two stages: (1) an appearance generator that models both appearance and background using a controllable image diffusion model, and (2) a motion transfer model that transfers motion, generated by a pretrained hand pose generator, to real-world video through a controllable video diffusion model. To improve performance, we incorporate multiple types of conditions that ensure the generated output aligns with the geometry, semantics, and fine details of the hand pose. Extensive experiments on DexYCB and OAKINK2 demonstrate that Sim2Real-HOI enhances the generated quality compared to the best prior work and results in a lower error rate when the generated videos are used to train downstream hand-pose estimators. The code and pre-trained weights will be made publicly available.", "tldr": "We propose a novel sim2real HOI video transfer pipeline that generates realistic real-world videos using only hand and object poses and object mesh, overcoming limitations of current methods, and setting a new benchmark for HOI video generation.", "keywords": ["Video Diffusion", "Hand Object Interaction"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/46b48862f7a647af4ef1e79be0c058102b5baefe.pdf", "supplementary_material": "/attachment/26cd3ad715b8fc336be4baab16bf27170ae0f72c.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a pipeline for HOI video generation, conditioned on the hand-object pose sequence. Experimental results show that the proposed method achieves better performance on an HOI dataset, i.e., DexYCB."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The method is easy to follow. The proposed pipeline achieves better video generation quality on the lab-captured DexYCB dataset."}, "weaknesses": {"value": "1. The technical contributions are severely insufficient. There is neither a newly-designed network architecture nor novel training strategies. The entire pipeline is just a combination of existing works and training on the HOI dataset.\n\n2. The paper claims the \"lack of visual diversity\" drawback of the first-frame conditioned video generation pipeline in the introduction; however, the proposed pipeline is still based on generating the first frame of the video. I don't think training on a lab-captured dataset, such as the DexYCB dataset, can result in a significantly improved diverse generation ability.\n\n3. I suspect the zero-shot generalization results on the OckInk2 dataset shown in Figure 7. Without training on the OckInk2 dataset, how to make the generated background exactly the same as the ground truth, even for some objects not in the conditions?\n\n4. Improving the performance of a single baseline (SimpleHand) in the downstreaming task is not enough to demonstrate the effectiveness.\n\n 5. The generation ability for more unseen grasping motions should be further analyzed, and more qualitative and quantitative results should be provided."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "M1MahtN6h8", "forum": "vwgrHsmKuH", "replyto": "vwgrHsmKuH", "signatures": ["ICLR.cc/2026/Conference/Submission8378/Reviewer_WDQg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8378/Reviewer_WDQg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867338307, "cdate": 1761867338307, "tmdate": 1762920285994, "mdate": 1762920285994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Ann6PKwZKl", "forum": "vwgrHsmKuH", "replyto": "vwgrHsmKuH", "signatures": ["ICLR.cc/2026/Conference/Submission8378/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8378/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763113596949, "cdate": 1763113596949, "tmdate": 1763113596949, "mdate": 1763113596949, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Sim2Real-HOI, a framework for hand–object interaction (HOI) video generation from the initial and target hand–object poses, designed to bridge the sim-to-real gap in video generation. Existing HOI video generation methods are less scalable due to their reliance on the availability of the first frame of the video, which introduces (1) an input bottleneck, as such data are difficult to obtain, and (2) a diversity bottleneck, since the first frame is fixed. In contrast, the proposed framework requires only the initial and target HOI poses, effectively mitigating both types of bottlenecks. To achieve this, it disentangles (1) appearance generation, which generates the first frame conditioned on the initial HOI pose, and (2) motion generation, which produces the full sequence of HOI poses. The resulting pose sequence is then fed into a controllable video generation model to synthesize the final HOI video. Experimental results show that the proposed method outperforms prior works in HOI video generation and also improves hand pose estimation performance when the synthesized data are used for additional training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**(1) Strong experimental results**\n\nThe proposed framework demonstrates strong experimental performance, outperforming prior works by a noticeable margin in controllable HOI video generation. In addition, the application results (sim-to-real transfer, downstream task validation, and zero-shot evaluation) are well presented, and they clearly demonstrate the practical effectiveness of the proposed method.\n\n**(2) Well-motivated method design**\n\nThe method is well motivated, and the rationale behind each component design is clearly justified. The overall motivation is coherent and effectively conveyed.\n\n**(3) Good presentation quality**\n\nOverall, the presentation quality of the paper is very good. It is easy to follow, and the motivation of the proposed method with respect to prior work (L73–80) is particularly well explained."}, "weaknesses": {"value": "**(1) Lack of technical novelty**\n\nAlthough the proposed method demonstrates strong experimental results and each component design is well justified, the technical novelty is somewhat limited, as it mainly involves adopting existing modules (e.g., ControlNet) and augmenting the conditioning modalities for video generation. Nevertheless, I still recognize the practical value of this work.\n\n**(2) Baseline comparisons for downstream tasks**\n\nThe proposed framework could be more convincingly validated if baseline comparisons for the downstream task (pose estimation) were included, as this represents the most practical application scenario."}, "questions": {"value": "L348–349: Could you confirm that the proposed method did not use the ground-truth first frame in the experiments (unlike the baselines)? This clarification is important, as the non-reliance on the ground-truth first frame is a key contribution of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Ly3O8oPfFB", "forum": "vwgrHsmKuH", "replyto": "vwgrHsmKuH", "signatures": ["ICLR.cc/2026/Conference/Submission8378/Reviewer_QqXY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8378/Reviewer_QqXY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969770986, "cdate": 1761969770986, "tmdate": 1762920285321, "mdate": 1762920285321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Sim2Real-HOI for Hand–Object Interaction (HOI) video generation. It aims to bridge the sim-to-real gap using only the initial and target hand–object poses and object geometry as inputs, without relying on the first frame. The approach decouples motion generation and appearance generation. For motion generation, a pretrained hand-motion model generates intermediate poses, which are rendered by a controllable video diffusion model to create full HOI sequences. For appearance generation, a controllable image diffusion model synthesizes the first frame from depth maps, semantic masks, and hand keypoints. The model is evaluated on the DexYCB and OAKINK2 datasets and shows improvements in FVD, SSIM, and MPJPE scores, etc."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and easy to follow.\n- The proposed framework decouples motion and appearance generation, which is simple and intuitive.\n- Sim2Real evaluations show that the model trained with synthetic data performs well on real data."}, "weaknesses": {"value": "- The method relies on several pretrained models (e.g., GraspXL, CogVideo-X) rather than an end-to-end training framework. This modular design could introduce cumulative errors between stages. A deeper analysis of these potential issues would strengthen the paper. In particular, the authors should justify their choice of pretrained models and discuss whether adopting more advanced or jointly trained base models could yield further improvements.\n\n- The paper lacks a user study or qualitative evaluation of realism. Including such an evaluation would provide stronger evidence of the model’s real-world applicability and perceptual quality beyond benchmark metrics."}, "questions": {"value": "- In Table 1, Sim2Real-HOI is marked as not using Object Appearance and Hand Appearance as input conditions. However, Figure 1 suggests that the HOI Pose Sequence implicitly reflects both object and hand appearance information. It would be good to clarify how Object Appearance and Hand Appearance are defined in Table 1, and to what extent the pose sequence contributes to appearance modeling. Please correct me if I have misunderstood this aspect."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PTw1fbBHuM", "forum": "vwgrHsmKuH", "replyto": "vwgrHsmKuH", "signatures": ["ICLR.cc/2026/Conference/Submission8378/Reviewer_d7o4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8378/Reviewer_d7o4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136955930, "cdate": 1762136955930, "tmdate": 1762920284510, "mdate": 1762920284510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Sim2Real-HOI for generating HOI videos from initial/target hand and object poses. It proposes a  two-stage process. First, an appearance model synthesizes a photorealistic first frame conditioned on depth, segmentation, hand keypoints. Second, a motion model animates this frame using a pre-trained pose generator and a video diffusion model. This paper demonstrates experimental results on DexYCB and OAKINK2. It also shows the application of using synthetic data to improve downstream hand pose estimation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well-written and easy to follow. The core ideas are communicated effectively. The proposed cascaded pipeline demonstrates its effectiveness on public datasets, DexYCB and OAKINK."}, "weaknesses": {"value": "1. The paper’s primary motivation focus on overcoming the \"first-frame bottleneck\" of prior work that conditions on a real first frame. This premise is unconvincing. A realistic first frame can be generated given the initial hand and object pose conditions, with appearance randomization handled by existing image generation models. \n2. The proposed pipeline glues a few existing model components together, lacking technical novelty and insights.\n3. Relying on a complex, cascaded pipeline of multiple heavy foundation models, including Flux and CogVideoX. This architecture is likely computationally expensive and slow, making it ill-suited for the large-scale data synthesis cited as a goal. The paper lacks a performance study reporting the inference time or computational cost required to generate video clips, which is essential for assessing practical scalability.\n4. The experimental validation is restricted to the test splits of the datasets used for training (DexYCB and OAKINK2). The paper lacks in-the-wild evaluation on completely unseen real-world data outside of these established benchmark distributions."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "P9dwUPeHHi", "forum": "vwgrHsmKuH", "replyto": "vwgrHsmKuH", "signatures": ["ICLR.cc/2026/Conference/Submission8378/Reviewer_CJT1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8378/Reviewer_CJT1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762376527780, "cdate": 1762376527780, "tmdate": 1762920284005, "mdate": 1762920284005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}