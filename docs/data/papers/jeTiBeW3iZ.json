{"id": "jeTiBeW3iZ", "number": 12621, "cdate": 1758209059524, "mdate": 1759897497697, "content": {"title": "Memorization Through the Lens of Sample Gradients", "abstract": "Deep neural networks are known to often memorize underrepresented, hard examples, with implications for generalization and privacy.  Feldman & Zhang (2020) defined a rigorous notion of memorization. \nHowever it is prohibitively expensive to compute at scale because it requires training models both with and without the data point of interest in order to calculate the memorization score.\nWe observe that samples that are less memorized tend to be learned earlier in training, whereas highly memorized samples are learned later. \nMotivated by this observation, we introduce Cumulative Sample Gradient (CSG), a computationally efficient proxy for memorization. CSG is the gradient of the loss with respect to input samples, accumulated over the course of training.\nThe advantage of using input gradients is that per-sample gradients can be obtained with negligible overhead during training. The accumulation over training also reduces per-epoch variance and enables a formal link to memorization. Theoretically, we show that CSG is bounded by memorization and by learning time.\nTracking these gradients during training reveals a characteristic rise–peak–decline trajectory whose timing is mirrored by the model’s weight norm. This yields an early-stopping criterion that does not require a validation set: stop at the peak of the weight norm. This early stopping also enables our memorization proxy, CSG, to be up to five orders of magnitude more efficient than the memorization score from  Feldman & Zhang (2020).  It is also approximately 140 $\\times$ and 10$\\times$ faster than the prior state-of-the-art memorization proxies, input curvature and cumulative sample loss, while still aligning closely with the memorization score, exhibiting high correlation. Further, we develop Sample Gradient Assisted Loss (SGAL), a proxy that further improves alignment with memorization and is highly efficient to compute. Finally, we show that CSG attains state-of-the-art performance on practical dataset diagnostics, such as mislabeled-sample detection and enables bias discovery, providing a  theoretically grounded toolbox for studying memorization in deep networks.", "tldr": "", "keywords": ["Memorization", "Sample Gradients"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c46609e7e942e274e188f534356eddf6449d60c2.pdf", "supplementary_material": "/attachment/ae1055533901d0d0b4857a29a864c96a775c1270.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes a computationally efficient way to approximate the degree of memorization in deep neural nets. Based on the observation that memorized samples tend to have longer training time, this work proposes cumulative sample gradient (CSG) as a proxy for memorization. Theoretical results show the relation between CSG and learning time & memorization. Empirical evaluations corroborates the findings and shows superior computational performance over previous state-of-the-art."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, this work is well-written and organized. It motivates the problem settings and draws fairly clear connection with previous work. The contribution is pertinent to the current challenge of ML. By providing a more efficient probe for the phenomenon of memorization, this work can accelerate future research in this field. The theoretical formulations are solid and with clear purpose: they do shed light into the construction of the practical proxy. The empirical improvements are encouraging. No major flaw with experiment design."}, "weaknesses": {"value": "The work doesn't have apparent weaknesses that might lead to clear reject. I do have a few questions about the assumption and the source of computation edge over previous work. Having them clarified can better help the reader understand the contribution and use the tool with confidence.\n\nThere are a few grammatic glitches here and there. For example, \"is plays\" at Line 310 and \"it's roots\" at Line 258. Can be fixed by proof reading."}, "questions": {"value": "1) What makes the computation of CSG faster than CSL? Seems that both metrics are cumulative and the computation of loss/gradient are not too different in general. Could you tell us more about the source of speedup?\n\n2) The theoretical results show that CSG is upper bounded by learning time and memorization. Does that mean high CSG -> high memorization? What about the opposite direction, does low CSG -> low memorization?\n\n3)  The theories are formulated against pure SGD. Does CSG reliably detect memorization/mislabeled samples for different optimizers? If time allows, could you show some evidence of CSG's success for other optimizer? \n\n4) Are previous SOTA's performance dependent on the choice of optimizer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AcLjSfGWdo", "forum": "jeTiBeW3iZ", "replyto": "jeTiBeW3iZ", "signatures": ["ICLR.cc/2026/Conference/Submission12621/Reviewer_maBN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12621/Reviewer_maBN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808548380, "cdate": 1761808548380, "tmdate": 1762923469449, "mdate": 1762923469449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CSG, a fast, theoretically grounded proxy for measuring memorization in deep networks by accumulating input loss gradients during training. CSG correlates strongly with true memorization scores while being up to five orders of magnitude more efficient, enabling validation-free early stopping, mislabeled-sample detection, and bias discovery."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "CSG offers a theoretically grounded and computationally efficient way to estimate memorization, achieving near-perfect correlation with true scores at a fraction of the cost.\n\nIt enables validation-free early stopping and state-of-the-art mislabeled data detection, making it both practical and interpretable for large-scale deep learning."}, "weaknesses": {"value": "Novelty:\nMy primary concern lies in the novelty of the work. The authors’ main observation that memorization tends to occur in the later stages of training is well established and has been extensively documented in prior studies [1,2]. Likewise, leveraging gradients to approximate or track memorization has been explored before [3,4], making the core idea appear incremental rather than groundbreaking. Can the authors show how their work performs in relation to [1,3,4].\n\nComputational Cost:\nWhile the proposed approach claims efficiency, computing cumulative sample gradients still requires forward and backward passes for each sample at every epoch, which can be prohibitive for large models and datasets. Prior work [3] already proposes strategies to reduce this overhead.\n\nLimited Optimizer Evaluation:\nThe experiments rely solely on the Adam optimizer. To demonstrate broader applicability, results should be validated across multiple optimizers such as SGD, RMSProp, and AdamW. Can authors provide more info on how their method would behave across optimizers?\n\n\n[1]. Agiollo, Andrea, Young In Kim, and Rajiv Khanna. \"Approximating Memorization Using Loss Surface Geometry for Dataset Pruning and Summarization.\" Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2024.\n[2] https://aclanthology.org/2024.blackboxnlp-1.4\n[3] https://arxiv.org/abs/2008.11600\n[4] https://arxiv.org/pdf/2002.08484"}, "questions": {"value": "Above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JygZB8rteV", "forum": "jeTiBeW3iZ", "replyto": "jeTiBeW3iZ", "signatures": ["ICLR.cc/2026/Conference/Submission12621/Reviewer_BKpC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12621/Reviewer_BKpC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844258798, "cdate": 1761844258798, "tmdate": 1762923469140, "mdate": 1762923469140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Cumulative Sample Gradient (CSG)—the loss gradient w.r.t. the input, accumulated over training—as a computationally cheap proxy for stability-based memorization (Feldman & Zhang, 2020). The authors provide theory that (i) expected CSG is upper-bounded by learning time (Theorem 4.2) and (ii) linearly bounded by memorization (Theorem 4.3). Empirically, they observe a characteristic rise–peak–decline trajectory for average per-sample input gradients that aligns with a peak in weight norm and the first minimum in validation loss (double-descent boundary), enabling validation-free early stopping. They also introduce SGAL (a loss accumulated only until the gradient-based stopping point) for further efficiency. Across CIFAR-100/ImageNet, CSG/SGAL correlates well with memorization scores, is substantially faster than curvature and CSL proxies, supports mislabeled-sample detection, and helps surface dataset biases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•\tOriginality & clarity: Using input gradients accumulated over training as a memorization proxy is elegant; the rise–peak–decline alignment with the weight norm and double-descent boundary is compelling and clearly presented. \n•\tQuality (theory): Theorems relating CSG to learning time and memorization provide formal grounding absent in many proxies; assumptions and proof sketches are transparent. \n•\tQuality (empirics): Consistent binned linear trends; strong correlation with F&Z scores; broad comparisons (CSL, curvature, forgetting events, loss sensitivity) on CIFAR-100/ImageNet; MIA and adversarial-distance analyses support the privacy link. \n•\tSignificance & practicality: Large speedups (0.1–0.3× of standard training vs. 3.6–14.3× for curvature; orders of magnitude vs. F&Z) lower the barrier to dataset diagnostics at scale; mislabeled-sample AUROCs are SOTA or competitive at all noise levels."}, "weaknesses": {"value": "•\tAssumption sensitivity and constant opacity. The theoretical bounds rely on β-stability, Lipschitz continuity, L-bounded losses, and learning-rate conditions, and exclude first-layer skip connections. Constants involving the pseudo-inverse of batch matrices (κ terms) may be large/ill-conditioned, making the bounds hard to interpret quantitatively. \n•\tCalibration claim is mixed. Table 2 shows lower ECE for the last epoch (0.1017) than for gradient-based stopping (0.1382), contradicting the blanket statement that early-stopped checkpoints have lower calibration errors; other metrics (MCE/MSCE/UCE) favor earlier stopping, so the narrative should be nuanced. \n•\tScope of datasets/models. Results are primarily on CIFAR-100/ImageNet with ResNet/Inception. It would strengthen generality to include modern architectures (e.g., ViT) and tasks beyond image classification, since input-gradient behavior and training dynamics may differ. (The Adam experiment is a useful first step.) \n•\tComparative coverage. While CSL/curvature/forgetting/loss-sensitivity are included, some adjacent proxies (e.g., EL2N, GraNd / importance-sampling-style difficulty measures) and influence-based approximations (e.g., TracIn) are not compared; these could provide a more complete picture of trade-offs.\n•\tTraining-access requirement. Like many proxies, CSG needs access to per-sample gradients during training; this limits pure post-hoc auditing scenarios (the limitation is acknowledged). \n•\tQualitative bias analysis. The bias discovery examples are informative but largely qualitative; quantitative fairness metrics (e.g., subgroup error rates) would make the case stronger."}, "questions": {"value": "1.\tHow sensitive is the weight-norm peak rule to weight decay, label smoothing, data augmentation strength, and optimizer hyperparameters? A small ablation across these knobs would clarify robustness.  \n2.\tPractically, do you compute input gradients every iteration for all samples, or on a schedule/subset? Please quantify wall-clock overhead vs. vanilla training across model sizes.  \n3.\tHave you tested CSG/SGAL on ViTs or transformers for NLP? If not, what obstacles (e.g., tokenization, augmentation) do you anticipate?\n4.\tCan you empirically estimate the constants in Lemma 4.1 (e.g., behavior of κ through training) to illustrate why the linear trends emerge despite potential ill-conditioning?  \n5.\tGiven Table 2 shows mixed results across ECE/MCE/MSCE/UCE, can you reconcile the claim “lower calibration errors than last epoch” and specify which metrics you prioritize and why?  \n6.\tSince you use precomputed F&Z scores, how sensitive are your correlations to training recipe variations (architectures different from F&Z)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dR4LtmlQfC", "forum": "jeTiBeW3iZ", "replyto": "jeTiBeW3iZ", "signatures": ["ICLR.cc/2026/Conference/Submission12621/Reviewer_gxif"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12621/Reviewer_gxif"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862030660, "cdate": 1761862030660, "tmdate": 1762923468870, "mdate": 1762923468870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Cumulative Sample Gradient (CSG) as a theoretically motivated and computationally efficient proxy for memorization in deep neural networks. The authors define CSG as the gradient of the loss with respect to input samples, accumulated across training, and claim it correlates strongly with Feldman & Zhang’s formal memorization score while being orders of magnitude cheaper to compute. They theoretically show that CSG is bounded by both learning time and memorization, then empirically validate this relation across CIFAR-100 and ImageNet. Moreover, they propose that the peak of the model’s weight norm corresponds to the optimal early-stopping point, eliminating the need for a validation set. The paper also introduces Sample Gradient Assisted Loss (SGAL) as an efficiency improvement, and reports strong performance on tasks such as mislabeled sample detection and dataset bias discovery"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work links input-space gradients to memorization and learning dynamics through formal theorems, extending prior work that primarily focused on weight gradients or loss-based proxies.\n\n2. The idea of accumulating input gradients incurs minimal additional computation during training and is potentially useful for large-scale data auditing, noisy-label detection, and privacy diagnostics.\n\n3. The observed “rise–peak–decline” trajectory in sample gradients and weight norms provides an intuitive link between optimization dynamics and generalization behavior."}, "weaknesses": {"value": "1. The main claim that “Cumulative Sample Gradient” represents a gradient of loss with respect to the input, accumulated over training, is conceptually questionable. The proposed CSG is essentially an aggregated gradient norm trajectory rather than a true differentiable functional of the loss. Treating it as a gradient object conflates sensitivity analysis (∇ₓℓ) with memorization, which lacks theoretical grounding in generalization theory. The derivations (Theorems 4.2–4.3) merely establish loose proportionality bounds without proving causality or sufficiency.\n\n2. The assertion that the peak of weight norm universally coincides with the minimum validation loss is overstated. This correspondence may depend on architecture, optimizer, and regularization strength, and may fail under strong augmentation or non-stationary data.\n\n3. While the authors claim that CSG generalizes across tasks, they only test standard supervised image classification. The theoretical link assumes uniform β-stability of SGD and bounded loss, which rarely holds in modern deep nets. It remains unclear whether CSG maintains predictive utility in other regimes such as self-supervised, generative, or multi-label settings."}, "questions": {"value": "1. Since CSG is defined as the accumulated input gradient norm, not the derivative of a loss functional over training trajectories, do we have a rigorous reason to treat it as a “gradient of loss with respect to input samples, accumulated over training”? How does this differ from simply tracking cumulative sensitivity?\n\n2. The paper asserts that stopping at the peak weight norm matches the minimum validation loss. Can this be proven under general conditions? How robust is this correspondence across architectures, datasets, or optimizers (e.g., Adam, Adagrad, adaptive schedulers)?\n\n3. Do you think the CSG–memorization relationship could generalize to regression, contrastive, or generative models, where accuracy or label definitions differ? You don’t need to perform new experiments—rather, please share your intuition on whether and why such generalization might hold."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z3GZqYPWjF", "forum": "jeTiBeW3iZ", "replyto": "jeTiBeW3iZ", "signatures": ["ICLR.cc/2026/Conference/Submission12621/Reviewer_P7vQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12621/Reviewer_P7vQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892883940, "cdate": 1761892883940, "tmdate": 1762923468432, "mdate": 1762923468432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}