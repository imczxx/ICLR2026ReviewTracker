{"id": "3FcjKYmNY5", "number": 11727, "cdate": 1758203356725, "mdate": 1763639725387, "content": {"title": "SALSA-V: Shortcut-Augmented Long-form Synchronized Audio from Videos", "abstract": "We propose SALSA-V, a multimodal video-to-audio generation model capable of synthesizing highly synchronized, high-fidelity long-form audio from silent video content. Our approach introduces a masked diffusion objective, enabling audio-conditioned generation and the seamless synthesis of audio sequences of unconstrained length. Additionally, by integrating a shortcut loss into our training process, we achieve rapid generation of high-quality audio samples in as few as eight sampling steps, paving the way for near-real-time applications without requiring dedicated fine-tuning or retraining. We demonstrate that SALSA-V significantly outperforms existing state-of-the-art methods in both audiovisual alignment and synchronization with video content in quantiative evaluation and a human listening study. Furthermore, our use of random masking during training enables our model to match spectral characteristics of reference audio samples, broadening its applicability to professional audio synthesis tasks such as Foley generation and sound design.", "tldr": "", "keywords": ["video-to-audio", "audio generation", "diffusion"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/780501365dd6cdc6dc1c38131015c6ed7be54b52.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a new video-to-audio model called SALSA-V that efficiently generates long-form audio given a target video. To perform long-form audio generation, the proposed method iteratively outpaints the previously generated audio, and for that purpose, the model is trained with masked ground-truth audio as conditional input. For efficient generation, the training objective includes a shortcut loss, which enables few-step generation during inference. The experimental results show that the proposed model outperforms existing models, especially when the duration of the target video is long or the number of sampling steps is small."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is simple and should be easy to implement.\n- The advantage of the proposed method over MMAudio is significant when the duration of the target video is long or the number of sampling steps is small."}, "weaknesses": {"value": "- The novelty of the methodology is marginal.\n  - The main modifications from MMAudio are training with masked audio and the use of shortcut loss, and both techniques appear to be adopted in a straightforward manner.\n  - It would be beneficial to clearly describe the particular challenges in applying these techniques to video-to-audio models and how the proposed method addresses them. Alternatively, the authors can mention any empirical insights that could be helpful for future studies in the field of video-to-audio generation.\n- The comparison in the experiments needs additional baselines.\n  - For efficient generation, Frieren (or applying reflow to the proposed model with a flow matching formulation), as mentioned in Section 2.3, would be a good baseline.\n  - For long-form generation, V-AURA (mentioned in Section 2.1) and LoVA (mentioned in Section 2.4) could be used as baselines.\n- The benefit of audio-conditional generation in SALSA-V is not clear in Figure 4.\n  - I understand that some audio patterns in the conditional audio faithfully appear in the generated audio, but it seems that the ground-truth audio does not actually contain such patterns within the generated timeline. Thus, it is not clear if the conditional audio contributes to accurate audio generation.\n- I could not access the demo page due to a timeout error."}, "questions": {"value": "- Is there any reason why the authors did not try SALSA-V with 1B parameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TbAG3g4qmn", "forum": "3FcjKYmNY5", "replyto": "3FcjKYmNY5", "signatures": ["ICLR.cc/2026/Conference/Submission11727/Reviewer_gTEV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11727/Reviewer_gTEV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761609364519, "cdate": 1761609364519, "tmdate": 1762922766490, "mdate": 1762922766490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SALSA-V, a shortcut-augmented latent flow matching model for video-to-audio generation that achieves high-fidelity and temporally synchronized audio in just a few sampling steps. By combining masked training for audio conditioning and outpainting with contrastively trained synchronization features, SALSA-V enables both efficient short-form and stable long-form audio generation without distillation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "**1. Enables Long-Form Audio Generation**\n\nThe model supports audio-conditioned outpainting, allowing stable and coherent generation of extended audio sequences (30 s+) from video inputs.\n\n**2. Improved Audio-Visual Synchronization**\n\nA contrastively trained synchronization encoder provides precise temporal alignment between visual motion and audio events, achieving SOTA synchronization performance.\n\n**3. Efficient Few-Step Sampling**\n\nThe shortcut-augmented flow matching formulation reduces sampling steps to ≤ 8 without quality degradation."}, "weaknesses": {"value": "**1. Low Readability and Clarity**\n\nThe paper’s presentation is occasionally hard to follow.\n\n**2. Inconsistency between Quantitative Results (Fig. 5 vs. Table 1)**\n\nThere appears to be a mismatch between the 10s performance trend in Figure 5 and the metrics in Table 1.\n\n**3. Insufficient Analysis of Inference Efficiency**\n\nWhile the paper emphasizes few-step generation, there is no thorough empirical analysis of inference speed. More concrete runtime results would strengthen the efficiency claims."}, "questions": {"value": "**1. Clarification on Feature Roles and Figure References**\n\nIn the Method section, the paper sequentially introduces the VAE encoder, visual-text representation, and synchronization feature.\nHowever, it is unclear how each of these corresponds to the components shown in Figure 1, and what specific roles they play in the generation pipeline. A clearer mapping between the described features and their positions in Figure 1 would be helpful.\nIn addition, Figure 5 is never explicitly referenced in the main text—please consider mentioning and explaining it within the corresponding experimental section.\n\n**2. Inconsistency between Figure 5 and Table 1 Results**\n\nIn Figure 5, the proposed model shows lower performance than MMAudio for 10-second generation, whereas Table 1 indicates a different trend. Could you clarify whether these results are based on different experimental setups or evaluation protocols, and explain what accounts for the discrepancy?\n\n**3. Quantitative Analysis of Sampling Efficiency**\n\nThe paper highlights sampling efficiency as a major advantage of SALSA-V, but does not provide concrete runtime comparisons.\nCould the authors include or discuss actual inference speed measurements (e.g., seconds per 10-second clip, GPU type, batch size), and how they compare to other models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "93rH6jCBIM", "forum": "3FcjKYmNY5", "replyto": "3FcjKYmNY5", "signatures": ["ICLR.cc/2026/Conference/Submission11727/Reviewer_7VCU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11727/Reviewer_7VCU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652106377, "cdate": 1761652106377, "tmdate": 1762922765960, "mdate": 1762922765960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SALSA-V, a model for video-to-audio (V2A) generation, designed to synthesize high-fidelity, long-form audio that is precisely synchronized with video content. The authors propose three main contributions: (1) a shortcut-augmented training objective to enable high-quality audio generation in very few sampling steps; (2) a masked flow matching approach that allows the model to perform audio-conditioned generation and outpainting, thereby enabling the creation of audio for long-form videos through iterative extension; and (3) a new contrastive audio-visual synchronization model built upon a strong pre-trained vision backbone to yield high-resolution alignment features."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "*   **Clear Presentation and Structured Evaluation**: The paper is clearly written, with a well-defined problem motivation and a structured presentation of its methods. The evaluation is methodical, employing a range of established objective metrics alongside a human listening study.\n\n*   **Focus on Practical V2A Challenges**: The work addresses relevant practical limitations in the video-to-audio (V2A) domain, particularly the efficiency of the sampling process and the synthesis of audio for longer-form videos. This focus is pertinent to improving the applicability of such generative models.\n\n*   **Achieves Strong Temporal Synchronization**: A notable strength of the proposed model is its ability to generate tightly synchronized audio. The paper reports state-of-the-art results on the DeSync metric, and this quantitative improvement in temporal alignment is also corroborated by the human evaluation study."}, "weaknesses": {"value": "The paper, while presenting a well-engineered system, suffers from several weaknesses that question the significance and novelty of its contribution.\n\n1.  **Limited Novelty and Incremental Contribution**: The primary weakness of this work is its reliance on existing techniques across its entire pipeline, from the architectural framework and training methods to the experimental conclusions. This makes the overall contribution feel incremental rather than innovative.\n    *   The model's core design, which combines semantic and high-resolution synchronization features for conditioning, directly follows the framework established by **MMAudio[1]**.\n    *   The use of a masked training objective for audio conditioning is a known technique, with similar approaches having been explored in works like **AudioX[2]** and **MultiFoley[3]**.\n    *   The key insight regarding the choice of batch size for training the synchronization model is also acknowledged to be a direct replication of findings from **Synchformer [4]**.\n    While the integration of these parts is functional, the paper does not introduce a new fundamental concept, algorithm, or a significant insight to the field.\n\n2.  **Insufficient Experimental Baseline and Missing Citations**: The paper's experimental comparison is narrow, failing to benchmark against a sufficient number of relevant contemporary models. This makes it difficult to accurately assess its performance in the broader context of the field.\n    *   The main quantitative comparison (Table 1) only includes two other models, **FoleyCrafter** and **MMAudio**. Other highly relevant V2A methods, such as the autoregressive model **V-AURA [5]** and other diffusion-based models like **AudioX** and **Frieren[6]**, are not included in the benchmark.\n    *   A more comprehensive comparison against a wider array of recent models is necessary to robustly support the claims of state-of-the-art performance.\n\n3.  **Contradictory Results and Lack of Significant Improvement**: Several claims in the paper are not fully supported by its own results, and the overall improvement is not compelling.\n    *   **Lower Subjective Audio Quality**: For a generative model, perceptual quality is paramount. The human evaluation in Table 1 shows that SALSA-V's **Audio Quality** score (2.96) is lower than the baseline MMAudio (3.16). This key result undermines the paper's claim of outperforming existing methods. The overall subjective improvement is not significant.\n    *   **Potentially Flawed Long-Form Evaluation**: The paper compares its iterative, chunk-based generation with MMAudio's one-shot, full-sequence approach. As the models operate with different context windows and inference strategies, this direct comparison of metrics could be misleading.\n    *   **Disconnect Between Main Contribution and SOTA Results**: The \"shortcut loss\" for few-step sampling is highlighted as a major contribution. However, the main results in Table 1, which establish the model's SOTA synchronization, are based on 32 sampling steps. The paper does not demonstrate that this claimed SOTA performance is retained in the few-step regime, thus disconnecting the novel claim from the primary comparative results.\n\n4.  **Lack of Clarity in Experimental Details and Reproducibility Issues**: The description of the experimental setup lacks the necessary detail for reproducibility.\n    *   The test set used for evaluation is vaguely described as a composition of \"a holdout set of in-the-wild videos, the VGGSound test set, and UnAV-100\". Without precise details on the composition, data splits, and preprocessing of this custom benchmark, the results are not verifiable or reproducible by the community.\n\n[1] Cheng H K, Ishii M, Hayakawa A, et al. MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 28901-28911.\n\n[2] Tian Z, Jin Y, Liu Z, et al. Audiox: Diffusion transformer for anything-to-audio generation[J]. arXiv preprint arXiv:2503.10522, 2025.\n\n[3] Chen Z, Seetharaman P, Russell B, et al. Video-guided foley sound generation with multimodal controls[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 18770-18781.\n\n[4] Iashin V, Xie W, Rahtu E, et al. Synchformer: Efficient synchronization from sparse cues[C]//ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2024: 5325-5329.\n\n[5] Viertola I, Iashin V, Rahtu E. Temporally aligned audio for video with autoregression[C]//ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025: 1-5.\n\n[6] Wang Y, Guo W, Huang R, et al. Frieren: Efficient video-to-audio generation network with rectified flow matching[J]. Advances in Neural Information Processing Systems, 2024, 37: 128118-128138."}, "questions": {"value": "1.  **On Novelty**: The paper's core components (architecture, masked training, batch size insights) appear to be adapted from prior work. Could the authors clarify the primary technical novelty beyond the successful integration of these known techniques?\n\n2.  **On Experimental Baselines**: The experimental comparison is limited. Could the authors justify the exclusion of several recent and relevant baselines like V-AURA, AudioX, and Frieren, and perhaps provide a comparative analysis on key metrics?\n\n3.  **On Few-Step Generation Performance**: The \"shortcut loss\" is a key claimed contribution, but SOTA results are shown at 32 steps. Could the authors provide key metrics (e.g., DeSync, FAD) for the 8-step generation case against baselines to demonstrate the practical effectiveness of this feature?\n\n4.  **On Reproducibility**: The evaluation benchmark is vaguely described. For reproducibility, could the authors provide a precise composition and list of identifiers for their custom test set?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YYtyogEjnV", "forum": "3FcjKYmNY5", "replyto": "3FcjKYmNY5", "signatures": ["ICLR.cc/2026/Conference/Submission11727/Reviewer_jLCz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11727/Reviewer_jLCz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761783174508, "cdate": 1761783174508, "tmdate": 1762922765497, "mdate": 1762922765497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SALSA-V, a novel video-to-audio generation model that synthesizes high-fidelity, temporally aligned long-form audio from silent videos. The model leverages a masked diffusion objective to support audio-conditioned generation and seamless outpainting, enabling the synthesis of audio sequences of arbitrary length. A key innovation is the integration of a shortcut loss during training, which allows for high-quality audio generation in as few as eight sampling steps without additional fine-tuning. Furthermore, the authors introduce a contrastively-trained synchronization module using a large-scale pretrained vision backbone, which significantly improves temporal alignment between visual events and generated sounds. Extensive evaluations demonstrate that SALSA-V outperforms existing state-of-the-art models in both objective metrics (e.g., DeSync, FAD) and human subjective ratings, particularly in synchronization and long-form generation quality. The model also maintains competitive performance in semantic alignment and audio fidelity, despite having fewer parameters than leading baselines. These contributions collectively address key limitations in current V2A systems, including generation speed, controllability, and scalability to long durations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* High-Quality Synchronization: The model demonstrates state-of-the-art performance in temporal alignment between audio and video, as evidenced by both objective metrics (DeSync) and human evaluation. This is a critical and challenging aspect of video-to-audio generation.\n* Efficient Few-Step Sampling: By incorporating a shortcut loss during training, the model achieves high-quality audio generation in as few as eight sampling steps, without requiring additional fine-tuning or distillation. This makes it suitable for near-real-time applications.\n* Long-Form Generation Support: Through a masked diffusion objective, the model supports audio conditioning and seamless outpainting, enabling the generation of synchronized audio for extended video sequences without significant performance degradation.\n* Strong Multimodal Conditioning: The model effectively leverages multiple conditioning sources—semantic visual features, high-resolution synchronization features, and text embeddings—using a modified multimodal transformer architecture, leading to improved semantic and temporal alignment."}, "weaknesses": {"value": "* Evaluation of Contrastive Pre-training Models: The CAVP model from Diff-Foley is also a pre-trained model for audio-visual alignment, yet the authors omitted a comparative analysis with it. Could an experiment be designed to compare the performance of these two models? Additionally, while the paper discusses the impact of different batch sizes on contrastive learning, it lacks an ablation study to substantiate these claims.\n* Masked Training Strategy: During inference, only the preceding segment of clean audio is prepended. Why, then, is the masking during training applied randomly rather than being restricted to the preceding portion as well? Is there any prior investigation or ablation experiment regarding this design choice?"}, "questions": {"value": "Please refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X0p38Obvfe", "forum": "3FcjKYmNY5", "replyto": "3FcjKYmNY5", "signatures": ["ICLR.cc/2026/Conference/Submission11727/Reviewer_7eea"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11727/Reviewer_7eea"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991870213, "cdate": 1761991870213, "tmdate": 1762922765079, "mdate": 1762922765079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}