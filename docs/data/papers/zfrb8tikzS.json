{"id": "zfrb8tikzS", "number": 4953, "cdate": 1757819039811, "mdate": 1759898003191, "content": {"title": "SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment", "abstract": "Fine-grained cross-modal alignment seeks to establish precise local correspondences between vision and language, serving as a fundamental building block for visual question answering and related multimodal tasks.\nHowever, existing approaches are fundamentally constrained by patch redundancy and ambiguity, stemming from the inherent information density disparity between modalities—visual inputs provide dense spatial information across numerous patches while textual descriptions offer sparse, discrete semantic anchors. To address these limitations, we argue that richer semantic guidance is key in this paper and propose the Semantic-Enhanced Patch Slimming (SEPS) framework. To our knowledge, this is the first work in fine-grained alignment to combine MLLM-generated dense text with original sparse captions for enhanced visual patch selection. Our framework aggregates both sparse and dense textual representations to identify semantically relevant patches, and employs top-k selection with mean value computation to emphasize critical patch-word correspondences. Extensive experiments on Flickr30K and MS-COCO datasets demonstrate that SEPS achieves state-of-the-art performance, outperforming existing methods by 23\\%-86\\% in rSum across various model backbones, with particularly significant improvements in text-to-image retrieval tasks. Our available code is at https://anonymous.4open.science/r/SEPS/.", "tldr": "SEPS improves cross-modal alignment via two modules reducing patch redundancy and semantic ambiguity, achieving 23%-86% retrieval improvements on standard benchmarks.", "keywords": ["image-text matching; cross-modal retrieval; fine-grained cross-modal aligment"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d8fbad287c5cd49934dcb94dea529764d4dc0818.pdf", "supplementary_material": "/attachment/6a729864ca214c3a98f97efac41d00303baf1ec7.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes SEPS (Semantic-Enhanced Patch Slimming), a framework that aims to mitigate two key issues in vision-language pretraining and cross-modal retrieval — patch redundancy and patch ambiguity. The method leverages dense textual descriptions generated by a multimodal large language model (LLaVA) to provide rich semantic supervision in addition to the sparse original captions. Specifically, SEPS consists of two main modules:\n\nSDTPS (Sparse and Dense Text-Aware Patch Selection) — it computes semantic relevance between image patches and both sparse/dense textual representations, then selects informative patches via a differentiable Gumbel-Softmax sampling mechanism.\n\nHRPA (Highly-Relevant Patch-Word Alignment) — it constructs fine-grained alignments between selected patches and textual tokens using a relevance-aware selection and mean aggregation strategy, followed by bidirectional triplet losses for optimization.\n\nExperiments on Flickr30k and MS-COCO show significant improvements over several vision-language baselines (including ViT, Swin, and CLIP backbones), with reported gains up to +23–86% in rSum. The authors further provide ablations and sensitivity analyses to demonstrate robustness and reproducibility."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces a two-stage mechanism that incorporates unified semantic representations derived from both dense and sparse textual modalities. This mechanism eliminates potential semantic inconsistencies, enabling more accurate identification of visual patches.\n\n2. This paper leverages external MLLM to generate dense textual descriptions. This approach effectively enriches the semantic features available for alignment, directly addressing the key limitation of sparse captions that hinders prior work.\n\n3. The method achieves substantial gains over existing baselines on two widely used retrieval benchmarks (MS-COCO and Flickr30K). Experimental results demonstrate that combining dual-text supervision and patch selection can effectively enhance cross-modal alignment quality.\n\n4. The paper includes multiple ablation variants to study the impact of dense text, aggregation, and relevance-aware selection, showing clear trends in how each module contributes to overall performance."}, "weaknesses": {"value": "1. Lack of Novelty: The proposed SDTPS bears a strong resemblance to the LAPS framework [1], which also performs patch selection guided by textual relevance computed via cross-attention between text and patch embeddings. SEPS claims novelty by incorporating dense captions from MLLMs (e.g., LLaVA) as additional textual supervision. However, beyond the inclusion of dense text, the mechanism of computing patch importance (semantic scoring + differentiable selection) closely parallels that of LAPS.\n\n2. Limited Evaluation of Generalization: Although the paper's title and motivation emphasize fine-grained cross-modal alignment, all experiments are restricted to image-text retrieval tasks. Including evaluations on other alignment tasks such as visual grounding or VQA, would strengthen the claim and demonstrate the framework’s generalization ability.\n\n3. The dense text used in SEPS is generated by LLaVA directly from the test images, which introduces a potential form of semantic data leakage, as the evaluation text side indirectly encodes visual information from the test samples. This leakage allows the image semantics to transfer into the text space, which can dramatically boost retrieval performance and thus lead to an overestimation of the model’s true capabilities.\n\n[1] Linguistic-Aware Patch Slimming Framework for Fine-grained Cross-Modal Alignment, CVPR, 2024"}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "y5hGvRDXJx", "forum": "zfrb8tikzS", "replyto": "zfrb8tikzS", "signatures": ["ICLR.cc/2026/Conference/Submission4953/Reviewer_jURx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4953/Reviewer_jURx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761661003133, "cdate": 1761661003133, "tmdate": 1762917789276, "mdate": 1762917789276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the SEPS (Semantic-Enhanced Patch Slimming) framework for fine-grained cross-modal alignment. It addresses the issues of patch redundancy and ambiguity in cross-modal retrieval by integrating dense text generated from MLLMs with sparse textual semantics in two stages. The core contributions include:\n1. A dense text generation module using LLaVA-13B to enrich textual representations, bridging the information density gap between vision and language.\n2. A two-stage mechanism that incorporates unified semantic representations derived from both dense and sparse textual modalities.\n3. The authors develop a relevance-aware selection mechanism augmented by mean value calculation, which enhances the emphasis on critical patch-word correspondences.\nThis paper performs experiments on Flickr30K and MS-COCO in text-to-image retrieval. However, the method’s novelty and comparison with state-of-the-art baselines are limited."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem formulation of fine-grained cross-modal alignment is well-motivated, addressing practical issues of patch redundancy and ambiguity in multi-modal retrieval."}, "weaknesses": {"value": "1. The paper fails to benchmark against recent state-of-the-art models (e.g., CLIP variants, SigLIP 2, FG-CLIP, FineCLIP). This makes it difficult to assess SEPS’s competitiveness in the current research landscape. Perhaps the author could validate the proposed approach by executing your fine-tuning strategy on these baselines.\n2. The evaluation is restricted to standard image-text retrieval datasets (Flickr30K, MS-COCO) without testing on more complex or domain-specific scenarios, limiting the effectiveness validation of the proposed method. More tasks and datasets should be evaluated to prove the fine-grained capability."}, "questions": {"value": "Please refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Weh0Mdx8LX", "forum": "zfrb8tikzS", "replyto": "zfrb8tikzS", "signatures": ["ICLR.cc/2026/Conference/Submission4953/Reviewer_DzJL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4953/Reviewer_DzJL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811132087, "cdate": 1761811132087, "tmdate": 1762917788945, "mdate": 1762917788945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SEPS, a Semantic-Enhanced Patch Slimming framework designed to improve fine-grained cross-modal alignment between vision and language. The method combines dense textual representations generated by Multimodal Large Language Models (MLLMs) with sparse human captions in a two-stage process: (1) unified semantic fusion for patch selection, and  (2) relevance-aware patch–word alignment.  The approach is evaluated on Flickr30K and MS-COCO, achieving notable improvements over state-of-the-art image–text retrieval methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "SEPS effectively merges MLLM-generated dense descriptions with human captions, offering unified semantic guidance for patch selection. This hybrid approach reduces redundancy and ambiguity, improving visual–textual grounding"}, "weaknesses": {"value": "w1. Dense text is generated offline using LLaVa, but the paper does not confirm whether this model has previously seen the test images or captions.\n\nw2. Besides the potential for information leakage, another problem with using dense text during testing is efficiency. Compared to d2s-vse, which also uses dense text, the method in this paper seems to use dense text during testing as well, and the efficiency of generating dense text in real time is very low."}, "questions": {"value": "refer to weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "83RHyaT9ou", "forum": "zfrb8tikzS", "replyto": "zfrb8tikzS", "signatures": ["ICLR.cc/2026/Conference/Submission4953/Reviewer_1zka"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4953/Reviewer_1zka"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921846787, "cdate": 1761921846787, "tmdate": 1762917788521, "mdate": 1762917788521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}