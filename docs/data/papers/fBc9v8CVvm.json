{"id": "fBc9v8CVvm", "number": 24044, "cdate": 1758352113893, "mdate": 1759896784602, "content": {"title": "TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows", "abstract": "Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation.\nThese models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)).\nWhile various few-step methods aim to accelerate the inference, existing solutions have clear limitations.\nProminent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE).\nMeanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models.\nTo this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need for distillation from pre-trained models and avoids standard adversarial training, making  it ideal for building large-scale, efficient models.\nOn text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework).\n**Notably, we demonstrate the scalability of TwinFlow by transforming Qwen-Image-20B---the current largest open-source multi-modal generative model---into an efficient few-step generator**. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by $100\\times$ with minor quality degradation.\nOur code and models will be made publicly available.", "tldr": "We propose a simple yet effective framework for training one-step generative models without the demand of pretrained models for distillation or standard adversarial training, which is helpful when training large few-step generative models.", "keywords": ["few-step generation", "text-to-image generation", "multi-modal generative models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/36aa7820b5f1203252c8ae09b87177b5d8ef7654.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new one-step generation method named TwinFlow. The method applies a self-adaversarial manner: First, it extend the common $[0,1]$ time interval into $[-1,1]$. Then it uses the $[0,1]$ and $[-1,0]$ interval for training a true and fake diffusion model, respectively. And a loss is used to minimize the difference of the true and fake velocities. Experiments show that the proposed method achieves strong performance on text-to-image tasks, and also verify its scalability on a 20B model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- I like its feature where only one model is trained, and that it achieves good performance on T2I models without GAN. Compared to SiD and VSD, it combines the pretrained teacher and the true score net, while makes the fake score net online, which is a good improvement.\n- The performance is strong, especially for the scalabity on large-scale (20B) models."}, "weaknesses": {"value": "- This method is similar to existing distillation methods like SiD, DMD and VSD, for example, DMD requires a trained diffusion teacher and a online-training generator, corresponding to the $t\\in [0,1]$ part of TwinFlow; the auxiliary fake score network corresponds to the $t\\in[-1,0]$ part. The authors should provide a detailed comparison in their paper.\n- Since this approach integrate the function of the three models in DMD into one model, there is potential performance degration since the total model capacity is limited.\n- Though with better performance, the forward and backward pass of TwinFlow seems much more complicated than consistency training, the authors are expected to provide the theoretical and practical training cost comparison (also see questions)."}, "questions": {"value": "- How do you generate $\\\\boldsymbol{x}^{\\text{fake}}$? I suppose $\\\\boldsymbol{x}^{\\text{fake}}=\\\\boldsymbol{x}\\_t+(0-t)\\\\boldsymbol{F}_{\\theta}(\\\\boldsymbol{x}\\_t,t,0)$ under the OT-FM framework. If so, is the sample generated by the diffusion ODE idential to that generated by the generator trained wiith DMD loss (rectify loss in the paper) in particle or just in distribution? How do this loss boost the 2 or more-step performance?\n- In DMD and VSD, the training is alternatively (fake score net and generator). Why can your method be free from alternate training?\n- I found that the authors did not discuss the usage of CFG in the method, and I also notice that in Figure 3, the TwinFlow results are without CFG. I wonder if CFG is applicable in TwinFlow, or can CFG improve the performance further if applicable?\n- Is the DiT model on ImageNet trained from scratch or with teacher weights loaded? And, what is the 1-step performance? Also, are the results with or without CFG? If the $2.05$ FID is arrived without CFG, that is impressive.\n- While there are multiple scores reported for 1-step generation, there are very few 1-step visualizations. Can you provide some (it won't affect my evaluation)?\n- In consistency training/finetuning, the additional training cost compared to diffusion models is one forward pass. In your method, the extra cost seems much more. Can you elabarate the extra cost, and compare the practical memory and speed with consistency training (e.g., on ImageNet)?\n- The $\\lambda$ parameter only adjust the effect of $\\mathcal{L}_\\text{TwinFlow}$; do you also need to adjust the effect between adv loss and rectify loss?\n- Is there a missing \"-\" in Eq. (6)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HsmiCC3xPj", "forum": "fBc9v8CVvm", "replyto": "fBc9v8CVvm", "signatures": ["ICLR.cc/2026/Conference/Submission24044/Reviewer_qqUP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24044/Reviewer_qqUP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761468546407, "cdate": 1761468546407, "tmdate": 1762942910660, "mdate": 1762942910660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TwinSteps is a framework that aims to reduce the number of inference steps in diffusion / flow matching models to as few as 1 step. It bypasses the need for distillation and avoids standard adversarial training (e.g. GAN style). The approach strongly builds on the RCGM method, but adds three main additional components: extending the time domain from [0,1] to [-1,1] and adding an adversarial as well as a rectification loss. On text-to-image tasks, the method outperforms multiple baselines and is shown to be scalable by applying it on the largest open-source multi-modal model, Qwen-Image-20B."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- While many distillation or few step approaches leverage additional trained or teacher networks, this method does not require any additional separate network.\n- While strongly leveraging ideas by the DMD (Distribution Matching Distillation paper) paper, the authors found an elegant way to implement the idea of an adversarial network component into the existing flow matching / diffusion frameworks by extending the time domain to [-1,1] where [-1,0] corresponds to the fake data domain.\n- The presented method shows strong results on the image generation benchmarks GenEval, DPG-Bench and WISE"}, "weaknesses": {"value": "- Intuition and explanation:\nWhile the intuition of an adversarial component is clear in DMD is clear, certain design choices had to be taken to make this method work: \na) Representing fake data by negative time steps\nb) Having the adversarial loss learning the negative trajectory\nc) A rectification loss that should straighten the trajectory\nFurther, during training N=2 is adopted, meaning 2nd order approximations are done in the RCGM framework, giving a strong few-step prior.\nThe paper does not fully explain why the negative time trajectory is required and why the rectification component ultimately converges to a single step solution. If this was not investigated but only empirically observed, this needs to be clearly pointed out\nThe strength of the method could be better explained in more simple settings such as CIFAR10 or Image1K.\n\n- Evaluation and Results:\nAlmost all evaluations have been done on text-to-image benchmarks. This comes with additional complexity in the evaluation as e.g. diversity of images for the same text prompt multiple times (as pointed out by the authors in Table 2). While not explicitly mentioned in the main paper, the method seems to perform worse on other (simpler) benchmarks as e.g. Image-1K 256x256. In fact the 2 step FID of Twinflow trained for 801 epochs mentioned in the appendix (FID=2.05) is larger than the pure RCGM FID value after 424 epochs (FID=1.92, see https://github.com/LINs-lab/RCGM/blob/main/assets/paper.pdf). \nQwen-Image-Lightning is 1 step leader on the DPG benchmark and should be marked like this in Table 2\n\n- Distillation / Fine Tuning vs. Full training method:\nQwen-Image-TwinFlow (and possibly also TwinFlow-0.6B and TwinFlow-1.6B, see question below) leverages a pretrained model that is fine-tuned. In this case the pretrained model somehow acts as a “teacher” adding stability to the fine-tuning i.e. as the fake data predictions. \nHowever, when training from scratch the optimization problem becomes significantly harder. Possible stability issues and their solutions are not discussed in the paper.\n\n- Unclear writing:\nIn several parts of the paper, i.e. the methodology section, it is not always easy to follow the equations as variable names are not always defined or used in a consistent manner (see questions below)"}, "questions": {"value": "- In Eq. 1 it is not defined what $f_\\Theta-$ is?\n- In Eq. 2, does $z^{fake}$ correspond to $z$ that is also used in the base component of the loss to ensure alignment?\n- Eq. 2 has the objective to learn the negative time time trajectory. Why is it chosen differently to the normal base loss?\n- In Eq. 8 and 9: Did I understand correctly that $x_t$ corresponds to $x_{t’}^{fake}$?\n- On intuition and explanation of weaknesses: Can you give more information on the addressed point?\n- Instead of using a negative time trajectory, would a simple boolean condition as model input that indicates fake or real data also work? \n- Given the adversarial components do help the 1 / 2 step predictions, do they also help to improve many step predictions e.g. 50 or 100 NFE’s ?\n- On the Qwen-Image-Lightning diversity discussion: The shown results in the appendix support the “almost identical image” claim, but do - not seem sufficient. Can you quantify or provide more examples to support this claim?\n- In Table 2: Why is the performance increase of the method better on Qwen-Image than on OpenUni for 1 and 2 steps?\n- Can you clarify if TwinFlow-0.6B and TwinFlow-1.6B leverage a pretrained backbone that is full parameter finetuned or if these two models are trained ab-initio / from scratch? What about the ImageNet-1K results mentioned in the appendix?\n\n- Minor comment:\nTypo in Line 201: “to match with us”, probably “to match each other”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "girfYCR4W4", "forum": "fBc9v8CVvm", "replyto": "fBc9v8CVvm", "signatures": ["ICLR.cc/2026/Conference/Submission24044/Reviewer_ESMU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24044/Reviewer_ESMU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721597280, "cdate": 1761721597280, "tmdate": 1762942910305, "mdate": 1762942910305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TWINFLOW, a novel framework for training one-step generative models that achieves efficient inference without requiring auxiliary discriminators or frozen teacher models. The key innovation is the \"twin trajectory\" concept, which extends the time interval from [0,1] to [-1,1], creating paired trajectories for real and fake data generation. By minimizing the velocity field differences between these trajectories, the method achieves self-adversarial training. The authors demonstrate TWINFLOW's effectiveness by successfully applying it to Qwen-Image-20B, achieving competitive 1-NFE performance that matches 100-NFE baselines while reducing computational costs by 100×."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Successfully applying the method to 20B parameter models demonstrates promising scalability.\n- The 1-NFE performance closely matching 100-NFE baselines is impressive."}, "weaknesses": {"value": "- Theoretical flaws. If I understand correctly, the paper trains a model with a single-timestep condition to handle generation at arbitrary steps. **However, such a model is no longer a score model, and using it in reverse KL to compute the gradient of logp is theoretically flawed**.\n- BLIP-3o-60K contains samples carefully generated using GenEval prompts, and training on BLIP-3o could yield very high GenEval scores. GenEval score is the primary metric used in this paper and highlighted in the abstract, which leads to an unfair comparison in this paper.\n-  I think the benefits of integrating the real score, fake score, and generator—three models together—have been overly claimed. Particularly in the Qwen 20B example cited by the authors, all models use LoRA. For the distillation by reverse KL method, integrating two LoRAs onto the same model backbone does not bring about notable memory costs. And the GAN loss is an additional benefit; we can directly remove it and still achieve high performance. \n- The combination of reverse KL and consistency-like RCGM is straightforward. Besides, similar combinations have been explored in sCM [a].\n- The \"twin trajectory\" has been explored in FACM [b]. However, no discussion is found in the paper.\n\n[a] Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models.\n[b] Flow-Anchored Consistency Models."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PWu6qJclKo", "forum": "fBc9v8CVvm", "replyto": "fBc9v8CVvm", "signatures": ["ICLR.cc/2026/Conference/Submission24044/Reviewer_xzEE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24044/Reviewer_xzEE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993435227, "cdate": 1761993435227, "tmdate": 1762942909571, "mdate": 1762942909571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}