{"id": "DkHt2K1g2Y", "number": 109, "cdate": 1756728828308, "mdate": 1763632278021, "content": {"title": "Reasoning as Representation: Rethinking Visual Reinforcement Learning in Image Quality Assessment", "abstract": "Reasoning-based image quality assessment (IQA) models trained through reinforcement learning (RL) exhibit exceptional generalization, yet the underlying mechanisms and critical factors driving this capability remain underexplored in current research. Moreover, despite their superior performance, these models incur inference energy usage and latency orders of magnitude higher than their earlier counterparts, restricting their deployment in specific scenarios. Through extensive experiments, this paper verifies and elaborates that through RL training, MLLMs leverage their reasoning capability to convert redundant visual representations into compact, cross-domain aligned text representations. This conversion is precisely the source of the generalization exhibited by these reasoning-based IQA models. Building on this fundamental insight, we propose a novel algorithm, RALI, which employs contrastive learning to directly align images with these generalizable text representations learned by RL. This approach eliminates the reliance on reasoning processes and even obviates the need to load an LLM. For the quality scoring task, this framework achieves generalization performance comparable to reasoning-based models while requiring less than 5% of their model parameters and inference time.", "tldr": "", "keywords": ["Image Quality Assessment", "Low Level Vision", "Multimodal Large Language Model"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/64502e2096c3504a59e4cee632b87b97e02a35bd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the generalization mechanism of reasoning-based reinforcement learning (RL) models for image quality assessment (IQA). The authors argue that the strong generalization of RL-trained multimodal large language models (MLLMs) stems from their ability to compress redundant visual representations into compact, cross-domain text representations through reasoning. Based on this insight, the paper introduces two novel frameworks:\n\n1. RACT (Reasoning-Aligned Cross-domain Training): aligns multiple IQA datasets via reasoning-generated text to improve cross-domain generalization.\n\n2. RALI (Reasoning-Aligned Lightweight IQA): leverages contrastive learning to directly align images with reasoning text embeddings, achieving comparable performance to reasoning-based MLLMs with only 4% of their parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper investigates the generalization of reinforcement learning–based IQA methods. Although I believe the experimental setup is not entirely sufficient, it represents a frontier study in this field and carries significant research value.\n\n2. The RALI framework is quite interesting. Essentially, it distills a lightweight CLIP model using the reasoning text data generated by a reasoning-based model, and it further establishes a mapping between reasoning texts and quality scores. This design effectively avoids the problem of inconsistent score scales across multiple datasets during joint training. Meanwhile, textual information, compared with direct supervision using single numerical scores, may enhance the model’s generalization ability. From this perspective, I think the approach is somewhat similar to that of Q-Align, except that Q-Align directly maps scores to a few fixed textual tokens. I think this work provides an insightful strategy for multi-dataset joint training."}, "weaknesses": {"value": "１.\tLine 70–71: The authors state that “Instead of relying on lengthy visual tokens, it now depends on concise and compact reasoning (quality description) text tokens.” I disagree with this statement because, for both reasoning and non-reasoning models, the inputs are images (i.e., visual tokens). The reasoning model additionally generates a group of text tokens during output, derived from score-level supervision through RL. Therefore, it is unclear why the authors claim that reasoning-based models “depend on text tokens.” In my view, those text tokens are outputs rather than inputs.\n\n\n２.\tLine 74–76: The authors state that “previous MLLMs typically predict image quality through visual representations (more than 1000 tokens), whereas reasoning-based models do so via textual representations (less than 100 tokens), resulting in a compression of more than 10 times.” Similar to my comment in Question 1, I do not understand why reasoning-based models are described as not relying on visual tokens. These models still take images as input and thus require visual tokens to generate reasoning text. They cannot directly output reasoning tokens without first processing the visual input. Therefore, I disagree with this statement as currently written.\n\n\n３.\tFigure 2: The caption refers to subfigures (a) and (b), but the figure itself does not contain such labels. Please revise the figure or caption for consistency.\n\n\n４.\tSection 3.2: The comparison between Q-Insight and Qwen-VL appears to be unfair, as Qwen-VL has not been trained on any image quality assessment (IQA) dataset and lacks the capability for quality scoring. It is therefore expected that its reasoning outputs are longer and less relevant than those of Q-Insight, which the t-SNE visualization also reflects. A more appropriate comparison would involve a version of Qwen-VL that has been SFT-trained on the same IQA dataset. Furthermore, I do not fully agree with the conclusion that “through reinforcement learning, the reasoning model shifted its dependency on image quality scoring from visual tokens to reasoning text tokens.” As mentioned in Questions 1 and 2, reasoning text tokens and scores are both outputs of the reasoning model. The text tokens serve to explain why the model assigns a particular score, but the claimed “dependency shift from visual tokens to reasoning text tokens” is conceptually confusing, since these belong to two different stages of the processing pipeline.\n\n\n５.\tFigure 4: The comparison between visual tokens and reasoning text tokens appears to be unfair. To ensure a fair comparison, the visual model should also be fine-tuned on the same dataset, and the visual features used for comparison should be extracted from the last layer of that model to ensure sensitivity to image quality.\n\n\n６.\tTable 1: A compression experiment is missing—specifically, results for Qwen-VL trained via supervised fine-tuning should be added to show its in-domain and out-of-domain performance for comparison.\n\n\n７.\tLine 239: The authors mention that “its PLCC on KonIQ decreased by 0.024 compared to standalone training.” Please provide a reference for this number. Moreover, according to Tables 2 and 3, the proposed model also suffers a 0.011 drop, which should be discussed for completeness.\n\n\n８.\tLine 293–295: The statement “Although high-dimensional visual embeddings can fit the feature space well, they may harm out-of-distribution generalization, so we compress the visual tokens and reduce the visual space.” Please clarify how this conclusion is derived.\n\n\n９.\tTable 2: The order of methods and corresponding references is inconsistent across entries; please standardize the formatting.\n\n10.Tables 2 and 3: An important baseline, LIQE, is missing. LIQE also uses CLIP as its backbone and performs multi-dataset training; it should be included for a fair comparison."}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eIvfGTSO8v", "forum": "DkHt2K1g2Y", "replyto": "DkHt2K1g2Y", "signatures": ["ICLR.cc/2026/Conference/Submission109/Reviewer_6GPX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission109/Reviewer_6GPX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission109/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753510248, "cdate": 1761753510248, "tmdate": 1762915452770, "mdate": 1762915452770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the generalization capabilities of reasoning-based IQA models, particularly those trained with reinforcement learning. It argues that these models achieve superior generalization by converting redundant visual representations into compact, cross-domain aligned text representations (quality descriptions). Based on this, the authors propose RALI, a new framework that uses contrastive learning to align images directly with these generalizable text representations, bypassing the need for explicit reasoning and LLMs during inference. The paper demonstrates that RALI achieves comparable performance to reasoning-based models with significantly reduced computational cost.  The authors also introduce RACT, a Reasoning-Aligned Cross-domain Training framework to improve co-training on multiple IQA datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The core idea of linking generalization in IQA to the compression of visual information into text is insightful.\n2) The proposed RALI framework is well-designed and technically sound. The combination of contrastive learning, PCA, and bucketed k-means is effective.\n3) The experimental results are compelling and clearly demonstrate the effectiveness of RALI. \n4) The paper is generally well-written and easy to follow. The figures and tables are well-designed and informative."}, "weaknesses": {"value": "- While the paper convincingly demonstrates the importance of text representations, it sometimes overstates the case by implying that reasoning itself is not important. Reasoning, even if implicit, is still involved in the process of generating the quality descriptions used to train RALI. The paper needs to acknowledge this nuance.\n- RALI relies on Q-Insight to generate the initial image-text-score triplets. This raises concerns about the potential for bias in the generated data. It would be helpful to explore the sensitivity of RALI to the quality of the Q-Insight model.\n- The improvement of RACT is not significant enough. The authors should provide more solid evidence to support the effectiveness of RACT.\n- Can you clarify the role of reasoning in your framework? While RALI doesn't explicitly perform reasoning during inference, the quality descriptions it relies on are generated by a reasoning-based model. How does the quality of the initial reasoning affect RALI's performance?"}, "questions": {"value": "- Have you considered applying RALI to other vision-language tasks, such as image captioning or visual question answering?\n- What are the limitations of RALI? What types of images or distortions does it struggle with?\n- You mention the reduced computational cost of RALI. Can you provide a more detailed breakdown of the computational cost of each step in the RALI pipeline, compared to Q-Insight?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hzYRm24GXW", "forum": "DkHt2K1g2Y", "replyto": "DkHt2K1g2Y", "signatures": ["ICLR.cc/2026/Conference/Submission109/Reviewer_Wmn3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission109/Reviewer_Wmn3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission109/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829821913, "cdate": 1761829821913, "tmdate": 1762915452525, "mdate": 1762915452525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors raise two critical questions: how generalization relates to reasoning in image quality assessment (IQA), and whether it is essential, motivated by the observation that IQA scores are largely determined by reasoning text tokens rather than visual tokens.\nTo answer the first question, the authors propose a reasoning-aligned cross-domain training (RACT) framework to address divergent data distributions by leveraging textual reasoning representations derived from unified cross-dataset labels.\nTo answer the second question, they propose a reasoning-aligned lightweight IQA (RALI) framework to learn multimodal large language models' (MLLMs) reasoning-based IQA capability by fine-tuning a small vision encoder which is further generalized via feature compression.\nIn experiments, RALI achieved comparable IQA-scoring performance using only 4% of the parameters and runtime of Q-Insight."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "In Section 3.2, the paper presents a very interesting observation: the reasoning-based MLLM IQA model, Q-Insight, primarily references text tokens when generating the image quality score token, while hardly referencing visual tokens.\n- Building on this observation, the paper's development—proposing a solution to the stated problem—is highly logical and well-grounded.\n\nIn particular, by effectively leveraging the relatively domain-invariant nature of text representations compared to visual representations, the method secures generalization performance on the IQA task with a simple approach.\n- Both proposed methods, RACT (label alignment and cross-domain SFT) and RALI (feature compression), incorporate solutions for cross-dataset adaptation.\n- The approach is validated by the experimental results (Table 2 and Table 3).\n\nMoreover, compared to the prior art Q-Insight, it has a significant advantage for practical deployment by maintaining performance while reducing parameters and runtime to only 4% of Q-Insight."}, "weaknesses": {"value": "Regarding L176: 95% of the attention weights are assigned to the previously generated reasoning text tokens.\n- The 95% shown in Figure 3 is for a single example. Reporting the corresponding statistic over the entire KonIQ test set would enhance credibility.\n- Performing the same analysis as in Figure 3 on Qwen-VL would help generalize the claim about the reasoning mechanisms of MLLMs in image quality scoring.\n\nFigure 4 may be somewhat self-evident.\n- Because KonIQ and SPAQ contain different images, the visual tokens differ substantially, meanwhile, the text space used to describe IQA is relatively constrained (the terms and expressions for IQA are limited within the broader text corpus).\n- As in the right plot of Figure 2, adding scores to Figure 4 to show clustering by IQA score regardless of dataset would be more informative.\n\nIn the multi-dataset co-training results (Table 3), to better highlight the effect of RACT, it would be preferable to use a combination of four datasets that are very different in nature as described at L335 (e.g., KonIQ, KADID, PIPAL, and AGIQA, rather than KonIQ, SPAQ, KADID, and PIPAL) if possible."}, "questions": {"value": "No questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EUMg67rZYH", "forum": "DkHt2K1g2Y", "replyto": "DkHt2K1g2Y", "signatures": ["ICLR.cc/2026/Conference/Submission109/Reviewer_fd2c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission109/Reviewer_fd2c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission109/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921921609, "cdate": 1761921921609, "tmdate": 1762915452403, "mdate": 1762915452403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that RL-trained reasoning-style IQA models, such as the Q-Insight, don’t generalize better because they’re “better models,” but because RL teaches them to compress rich visual features into short textual descriptions of quality. This text space is said to be both compact and more cross-domain aligned, and that is claimed to be the source of generalization of RL-based IQA models. Based on that, they introduce RACT, Reasoning-Aligned Cross-domain Training, which first does per-dataset RL to obtain quality descriptions, aligns those descriptions across datasets, and then does cross-domain SFT using those aligned image–text pairs. They claimed that the proposed RACT enables RL-style OOD robustness without instability when mixing datasets. Furthermore, they propose RALI, which discards step-by-step reasoning at inference, aligns a CLIP-style vision encoder to those descriptions via contrastive learning, compresses features via PCA + bucketed k-means, and then predicts scores via similarity to a learned bank of basis vectors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper has a clear attempt to answer new open questions, which is more valuable and challenging than reporting new performance numbers. It tries to explain why RL-style IQA models outperform SFT baselines in cross-dataset generalization, which is timely and valuable regarding the internal mechanism.\n\n2. The proposed RACT and RALI pipelines are conceptually neat and practically actionable. Their effectiveness is verified on diversified IQA datasets. RACT is empirically strong OOD, where it outperforms both SFT-style and RL-style baselines in average PLCC/SRCC, with only the quality descriptions as a unifying label space across different datasets."}, "weaknesses": {"value": "1. The paper repeatedly asserts that RL teaches the model to compress visual tokens into textual reasoning, and that “this conversion is precisely the source of the generalization.” Although interesting, all supporting evidence is correlational, not causal. For example, fig. 3 shows attention mass shifting from visual tokens to reasoning tokens when outputting the score. But this only proves where the model is looking at inference time after reasoning text exists, not that the reasoning step itself is required to learn generalizable features. Provide causal ablations that can quantitatively prove this claim, or this would be fatal for this paper.\n\n2. In section 4, the authors explicitly claim that the proposed RALI eliminates the reliance on reasoning processes and even obviates the need to load an LLM. However, it is also stated that RALI first uses a pre-trained RL IQA model to generate reasoning text between \\<think\\> and \\<think\\> and scores for each training image. Therefore, although RALI is cheap at inference, it cannot be trained or updated without first running an RL reasoning model like Q-Insight to produce the textual descriptions that define that space. Can I interpret the contribution in this way: this paper is actually proposing distillation (or compression) of an RL reasoning model into a CLIP-like scorer, instead of removing reasoning entirely.\n\n3. The empirical evidence for RACT’s cross-domain convergence fix is not complete enough in the main part. As claimed in Sec. 3.4, RL on mixed datasets shows a severe convergence problem; the VisualQuality-R1’s ranking-based RL mitigates this but still drops when jointly trained, and multi-dataset reward construction is unstable. However, the paper doesn't quantitatively show the alleged instability/failure modes of naive multi-dataset RL training that motivate RACT. Since RACT is the core contribution, this point is very important but missing.\n\n4. RALI intentionally discards reasoning at inference. That’s great for speed, but some downstream pipelines want textual justifications. The paper doesn’t quantify how often RALI’s score disagrees with the RL model’s score on failure cases where the RL model also gave a plausible, human-auditable explanation.\n\n5. Some of the parameters look tricky and potentially fragile. The score range [1,5] is divided into N=240 buckets, then bucketed k-means with kn clusters per bucket yields 250 basis vectors, and why? Is that engineered around KonIQ’s and Q-Insight’s output distribution?\n\n6. The paper uses “reasoning,” “quality descriptions,” and “text tokens” almost interchangeably. But some of the provided examples look less like multi-step logical reasoning and more like structured perceptual justifications. Calling that “reasoning” may oversell what RL actually learned. I think clearer terminology would help.\n\nIn summary, I highly value the motivation and the ambition to answer open questions of the IQA field in this paper. However, I think current experiments and argumentation are not enough to support both the motivation and contribution. Several core claims are currently stronger than the presented evidence supports, especially the causal claim that “reasoning-as-compression is the source of generalization,” and the claim that RALI “removes” reasoning despite relying on RL-generated reasoning text for supervision."}, "questions": {"value": "1. As claimed in the paper, RALI is trained only on KonIQ, and its cross-dataset performance is reported (indicated by Table 2). Are the reasoning texts generated only from KonIQ images?\n\n2. Continuing the previous question, if so, then RALI’s OOD performance hinges on how well Q-Insight’s KonIQ-derived descriptions generalize to distortions and semantic content in SPAQ, AGIQA, CSIQ, etc. But the paper argues earlier that cross-domain alignment of reasoning is what generalizes, which implies we might get even better generalization if we harvest reasoning texts from multiple datasets. However, RALI is not shown in that stronger setting. How would the authors explain this?\n\n3. Which single dataset’s numeric scores are actually used during cross-domain SFT in Table 3’s RACT row? KonIQ only? SPAQ only? How sensitive is RACT to that choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8g2Rw97Lmf", "forum": "DkHt2K1g2Y", "replyto": "DkHt2K1g2Y", "signatures": ["ICLR.cc/2026/Conference/Submission109/Reviewer_xqw8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission109/Reviewer_xqw8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission109/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923600097, "cdate": 1761923600097, "tmdate": 1762915452269, "mdate": 1762915452269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response"}, "comment": {"value": "Thank you to all the reviewers for their constructive feedback. All reviewers have acknowledged and praised our innovation, methodology, and experimental results. We have addressed all the concerns, responded to each point, and made revisions to the paper. The modified sections are marked in green. We sincerely hope you recheck and reconsider your decision."}}, "id": "xdZ2wkxlwG", "forum": "DkHt2K1g2Y", "replyto": "DkHt2K1g2Y", "signatures": ["ICLR.cc/2026/Conference/Submission109/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission109/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission109/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763632319425, "cdate": 1763632319425, "tmdate": 1763632319425, "mdate": 1763632319425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}