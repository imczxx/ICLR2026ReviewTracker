{"id": "oBXfPyi47m", "number": 12017, "cdate": 1758205203849, "mdate": 1763734639205, "content": {"title": "Efficient Reinforcement Learning by Guiding World Models with Non-Curated Data", "abstract": "Leveraging offline data is a promising way to improve the sample efficiency of online reinforcement learning (RL). This paper expands the pool of usable data for offline-to-online RL by leveraging abundant non-curated data that is reward-free, of mixed quality, and collected across multiple embodiments. Although learning a world model appears promising for utilizing such data, we find that naive fine-tuning fails to accelerate RL training on many tasks. Through careful investigation, we attribute this failure to the distributional shift between offline and online data during fine-tuning. To address this issue and effectively use the offline data, we propose two techniques: i) experience rehearsal and ii) execution guidance. With these modifications, the non-curated offline data substantially improves RL’s sample efficiency. Under limited sample budgets, our method achieves a 102.8% relative improvement in aggregate score over learning-from-scratch baselines across 72 visuomotor tasks spanning 6 embodiments. On challenging tasks such as locomotion and robotic manipulation, it outperforms prior methods that utilize offline data by a decent margin.", "tldr": "", "keywords": ["Reinforcement Learning", "Reinforcement Learning from offline data"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba4cc7df0d96fa84808773791d72c9fd3bb752b3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a two-stage RL from offline data methods, which can leverage mixed quality offline data without reward labels. A world model is trained on these unlabeled offline data at stage one. At stage two, offline data relevant to online tasks, measured by the neural feature distance between online samples and offline trajectories, is retrieved to enrich the initial state distribution sampling. A behavior cloning policy $\\pi_\\text{bc}$ is pretrained on offline data to alternate sampling with the target policy during online training for enriching exploration.\n\nComprehensive experiments on locovisual tasks were conducted to verify the empirical advantages of this approach, compared to baseline RL methods that use offline data, as well as RL methods that learn from scratch."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Pros:\n\n\\+ This paper tackles an intriguing question of how to utilize imperfect, unlabeled, yet largely available offline data to boost RL performance, which is of practical importance.\n\n\\+ Technically solid methods that were clearly presented.\n\n\\+ Strong empirical performance with a gain of large margins, compared with baseline methods.\n\n\\+ Comprehensive evaluations on a wide range of baseline methods and locomotion tasks"}, "weaknesses": {"value": "Cons:\n\n\\- Vague descriptions about related work, such as \"uses a series of complex techniques to stabilize training\" (line 129) or \"or carefully curated dataset\" (line 131) - authors could've briefly summarized what techniques were used, and how curated data is generated in these related works.\n\n\\- Unclear definition of mixed quality data. Figure 9 alone does not sufficiently indicate the data quality, but only shows that imitation learning on these data leads to limited performance. See my question below for a follow-up."}, "questions": {"value": "- Has the scaling world model to 280M parameters also been applied to baseline methods for fair comparison?\n- How does Figure 2 support this: \"The world model’s accuracy degrades on states visited by the early-stage policy, especially when the offline data distribution is narrow, hurting sample efficiency.\" (Line 213-214)\n- Does training a bc policy on offline data rely on an unmentioned prerequisite: the mix quality data is not poor data? As mentioned, \"offline data often contains valuable information like near-expert trajectories and diverse state-action coverage\" (line 260). In my intuition, if the offline data quality is low, leveraging a bc policy would not help much in this algorithm. Have the authors explored to what extent the quantity and quality of offline data impact the algorithm's performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iyCnaE52UJ", "forum": "oBXfPyi47m", "replyto": "oBXfPyi47m", "signatures": ["ICLR.cc/2026/Conference/Submission12017/Reviewer_dmPf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12017/Reviewer_dmPf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761596786727, "cdate": 1761596786727, "tmdate": 1762923004979, "mdate": 1762923004979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the question of how to improve sample efficiency of RL by using offline data. The specific situation this paper looks into is the case where there is a large amount of offline data but that data is unstructured and not annotated. The paper pre-trains a world model on such data, consisting of many embodiments and then uses fine tuning to optimize for a specific task. The novel methodology of this work is the enabling of fine tuning of the world model to optimize for specific tasks and improve performance. In the first stage they pre-training their world model on offline image-based data. In the fine tuning stage their agent interacts with the environment and collects data for world model training. Their results show that their pre-training is able to improve sample efficiency greatly and doesn't hurt final performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Great motivating question as well as a practical method.\n\nVery strong results using a single world model for multiple tasks. This is a difficult challenge and this paper provides impressive empirical results.\n\nPaper is well written and very clear. Method is described well and the paper is easy to read. Empirical results are displayed clearly.\n\nEmpirical results are impressive and seem to be statistically well done with confidence intervals and such."}, "weaknesses": {"value": "I do not have any strong weaknesses in this paper that I can find."}, "questions": {"value": "Around line 266 you describe using a behavior cloned policy to choose actions. Does this cause any issues with the data now being off policy? I'm not sure how the dreamer architecture handles off policy data (it might handle it completely fine) but I'm wondering if this was something you considered?\n\nThe central problem you identify is distribution shift between the pre train data and the fine tune data. I see why this could cause problems but would your method prevent or slow down (in some settings) convergence of the world model to the best points in the world space? I'm just thinking that keeping around old data is fine and prevents a distribution shift but what about in the cases where the distribution shift is actually required. Possible example would be long horizon tasks where we only have pre train data for a small portion of the beginning, in this case we might not learn as well the longer ends of the task.\n\nFor the results (280) did you increase the size of the dreamer world model as well? It is unclear to me if this would make any difference or if it might even make dreamer perform worse but I missed if you described this somewhere."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rZ7s3bBeTe", "forum": "oBXfPyi47m", "replyto": "oBXfPyi47m", "signatures": ["ICLR.cc/2026/Conference/Submission12017/Reviewer_Uq2c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12017/Reviewer_Uq2c"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843549880, "cdate": 1761843549880, "tmdate": 1762923004456, "mdate": 1762923004456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces NCRL (Non-Curated offline data for efficient RL), an algorithm that leverages non-curated, reward-free, mixed-quality, and multi-embodiment datasets to improve sample efficiency in offline-to-online reinforcement learning. NCRL first pretrains a multi-task, multi-embodiment world model on this data, then reuses the same offline trajectories during fine-tuning through two mechanisms:\n(1) experience rehearsal, which retrieves task-relevant trajectories to mitigate distribution shift, and\n(2) execution guidance, a behavior-cloned prior policy that intermittently steers the agent during online training.\nIn  Meta-World and DMControl tasks, NCRL outperforms baselines and matches the performance of models trained with 3.3–6.7× larger budgets. It also surpasses several offline-data and world-model pretraining baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clarity and motivation.** The paper is well-written. The contributions are clearly articulated, the relationship to prior work is carefully established, and the distinctions from existing approaches are explicit and well-justified. I especially appreciate that the authors explain the reasoning behind key design choices throughout. Figure 2 provides a particularly clear and compelling motivation for the proposed approach.\n1. **Experimental design** The experiments are thoughtfully designed and demonstrate consistent, meaningful improvements of NCRL over relevant baselines. The ablations are comprehensive and provide useful information (e.g. showing that both core components of NCRL contribute to performance gains)\n1. **Reproducibility.** The paper includes extensive implementation and hyperparameter details, making the method straightforward to reproduce and encouraging further study or application."}, "weaknesses": {"value": "I vote to accept, though I have a few minor comments and clarifying questions:\n\n1. At the end of Section 3, the paper refers to theoretical results in Appendix B, but the main text does not summarize what these results show or why they are relevant. It would help readers if the paper briefly explained, in the main body, what guarantees or insights the theory provides (e.g., how experience rehearsal or execution guidance theoretically improves performance or mitigates distribution shift), etc.\n1.  In Tables 3 and 4, NCRL solve most Meta-World tasks with high probability but not all of them. Could the authors clarify what makes these tasks challenging? Are these differences due to the inherent difficulty of the tasks, the limited training budget of 150k steps, or particular weaknesses in retrieval or guidance?\n\nTypos:\n1. “reuses the this data” --> “reuses this data.”\n2. “Comparsion” --> “Comparison” in Fig. 11 caption\n4. \"reutrn\" --> \"return\" in code snippet"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vbB6J4U65x", "forum": "oBXfPyi47m", "replyto": "oBXfPyi47m", "signatures": ["ICLR.cc/2026/Conference/Submission12017/Reviewer_LFwp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12017/Reviewer_LFwp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976753144, "cdate": 1761976753144, "tmdate": 1762923004019, "mdate": 1762923004019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work addresses the challenge of improving sample efficiency in reinforcement learning (RL) by leveraging non-curated offline datasets which are reward-free, of mixed quality, and collected from multiple embodiments during online reinforcement learning. They investigate this in the world modeling case, and observe that naively replaying this data fails because of distribution mismatch. To overcome this, the paper proposes NCRL (Non-Curated offline data for RL), a two-stage framework: first train a task-agnostic world model on the offline data, and then fine-tune it on the target task. Specifically, the authors introduce two techniques during the fine-tuning phase to take advantage of this data: (1) Experience Rehearsal, which retrieves task-relevant trajectories from the offline dataset (based on neural feature distance using the world model encoder) and replays it during fine-tuning, and (2) Execution Guidance via Prior Actors, which randomly alternates between a prior policy trained on the retrieved offline data and the online RL policy, improving exploration.\n\nExperiments demonstrate that in the low-sample regime, NCRL improves significantly over learning-from-scratch baselines and match their performance in the high-sample regime; it also help sin a continual learning scenario with a sequence of tasks. Careful ablation experiments show each component's contribution to the observed improvement and improvement over directly using uncertainty-aware rewards for the offline data."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is written with clarity; the exposition of methods and step-by-step motivation is reasonable / comprehensive. In particular, the high-level motivation for the setting is very clear.\n\nThe experimental methodology is thorough and high-quality: covers a wide range of environments, uses appropriate metrics (normalized scores, learning curves with confidence intervals), and includes ablations to justify the algorithm’s design.\n\nThe novelty is primarily in framing the setting of a mixed-quality offline dataset; the methods fall quite naturally from this, and has related work with different assumptions (e.g. JSRL)."}, "weaknesses": {"value": "While the high-level motivation of using mixed-quality offline dataset makes perfect sense, the details of the way the offline dataset is constructed is somewhat artificial (e.g. agents corrupted with noise) and has characteristics which naturally lends itself to the proposed method; specifically, it's likely to cover similar state-space as some of the tasks, which means smart retrieval methods from this dataset would naturally help in the low-data regime. Similarly, a prior policy trained on retrieved tasks would help exploration simply by virtue of having seen similar tasks before. Perhaps putting stronger explanation on why this kind of dataset is reasonable to assume / arises naturally (e.g. in real-world robotics settings) would better motivate the method.\n\nThe impact of showing improvements in the low-data regime with these particular simulation settings is less clear, given cheapness of simulation data and the asymptote to the same performance with more environment steps."}, "questions": {"value": "This line of work [1], [2] are relevant given the emphasis on using demonstrations in a world-modeling approach.\n\nFurther analysis of exactly how the offline dataset matches the current task distribution would be helpful, i.e. could we quantify or visualize how much of the improvement comes from retrieving trajectories matching the current task state-space? Does the fact that the dataset is cross-embodiment matter if all the retrieved trajectories are the same embodiment?\n\nAny further analysis on the ablations in Figure 6? Specifically, it seems that execution guidance is the main differentiator for achieving nonzero reward for some subset of the Meta-World tasks.\n\n[1] Hansen, Nicklas, et al. \"Modem: Accelerating visual model-based reinforcement learning with demonstrations.\" arXiv preprint arX\n[2] Lancaster, Patrick, et al. \"Modem-v2: Visuo-motor world models for real-world robot manipulation.\" 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B7EupRYJ2C", "forum": "oBXfPyi47m", "replyto": "oBXfPyi47m", "signatures": ["ICLR.cc/2026/Conference/Submission12017/Reviewer_RREe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12017/Reviewer_RREe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984256209, "cdate": 1761984256209, "tmdate": 1762923003556, "mdate": 1762923003556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response by Authors"}, "comment": {"value": "We sincerely thank all reviewers RREe, LFwp, Uq2c, and dmPf for the time they dedicated to reviewing our work. We are grateful for the comments, constructive suggestions, and detailed questions, all of which have directly contributed to strengthening and clarifying our manuscript. We truly view this process as a collaborative effort, and we appreciate the reviewers’ engagement with both the high-level motivation and the finer technical details of our approach.\n\nAcross the reviews, we are glad to receive several consistent points of positive feedback:\n- Clarity and motivation: RREe, LFwp, and Uq2c praised the clarity and motivation.\n- Strong and comprehensive experimental evaluation: RREe, LFwp, and dmPf each highlighted the breadth and quality of our experiments and ablations, with Uq2c describing the empirical results as “impressive.”\n- Practical relevance and contribution: dmPf underscored the practical importance of leveraging imperfect offline data, while Uq2c emphasized the significance of successfully using a single world model across many tasks.\n- Overall assessment: We are encouraged that RREe and LFwp recommended acceptance, and Uq2c recommended strong accept.\n\nThe reviewers’ constructive suggestions helped us substantially improve the paper. In direct response to their feedback, we have:\n- Refined the related-work discussion to clarify assumptions and techniques (addressing dmPf).\n- Included new ablations on retrieval relevance and robustness to low-quality offline data (addressing RREe and dmPf).\n- Clarified model-size considerations (addressing Uq2c and dmPf).\n- Added a succinct summary of the theoretical insights to the main text (addressing LFwp).\n- Provided a clearer characterization of mixed-quality data (addressing dmPf).\n- Added new training resutls for challenging MetaWorld tasks with a larger training budget (addressing LFwp).\n\nWe believe these changes have strengthened the paper, and we appreciate the reviewers’ guidance in arriving at a clearer, more rigorous, and more thoroughly justified presentation.\n\nOnce again, we thank all reviewers for their encouragement and help in improving the manuscript."}}, "id": "IyqKwANgzF", "forum": "oBXfPyi47m", "replyto": "oBXfPyi47m", "signatures": ["ICLR.cc/2026/Conference/Submission12017/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12017/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission12017/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763734903140, "cdate": 1763734903140, "tmdate": 1763736536390, "mdate": 1763736536390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}