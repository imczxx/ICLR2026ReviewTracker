{"id": "WPgaGP4sVS", "number": 11262, "cdate": 1758194605343, "mdate": 1759897597551, "content": {"title": "TamperTok: Forensics-Driven Tokenized Autoregressive Framework for Image Tampering Localization", "abstract": "Multi-modal Large Language Models (MLLMs) offer powerful reasoning for localizing tampering in images, yet existing MLLM-based approaches suffer from suboptimal localization due to the reliance on exogenous segmentation decoders.\nThe stitched pipeline introduces information bottlenecks during backpropagation, diluting spatial signals from the MLLM's hidden embeddings and lacking semantic priors for forensic tasks, which leads to imprecise masks and poor generalization in Image Manipulation Detection \\& Localization (IMDL).\nTo address those limitations, we propose TamperTok, which reformulates MLLM-based IMDL as an autoregressive sequence generation task.\nUnlike existing approaches relying on exogenous decoder for localization, TamperTok directly generates spatially grounded token sequences from the MLLM, enabling precise probabilistic mask prediction without intermediary supervisions.\nSpecifically, we introduce Kernel Splatting Decoder (KSD) to mitigate the sharp gradients caused by deterministic map in codebook-based detokenizer via clustering-aware code smoothing while mapping tokens to binary masks.\nIn addition, to compensate for the lacking priors of diverse tampering types, i.e., splicing and semantic forgeries, we propose a novel Scene-wise Expert Injection (SwEI) to select and inject multi-scale tampering-specific features from a forensic expert model into the MLLM.\nExtensive experiments show that TamperTok achieves state-of-the-art (SOTA) performance on multiple tampering localization datasets, with 20\\% improvements in IoU and F1 over existing MLLM-based models, while exhibiting stronger robustness to noise perturbations and cross-domain scenarios.\nCodes will be released.", "tldr": "", "keywords": ["Image Manipulation Detection & Localization", "Multimodal Large Language Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d1405f8c3b4aeb35d2f3b7eaa0dada35bac60c87.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents TamperTok, a novel framework that reframes Image Manipulation Detection & Localization (IMDL) as an autoregressive sequence generation task. This approach avoids the information bottlenecks of existing MLLM-based methods that rely on external decoders. TamperTok introduces two key components: the Kernel Splatting Decoder (KSD) for stable mask generation from tokens, and Scene-wise Expert Injection (SwEI) to fuse multi-scale forensic features into the MLLM's visual encoder. Extensive experiments show that TamperTok achieves state-of-the-art (SOTA) performance and demonstrates superior robustness on multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The framework introduces a fundamental architectural innovation by reformulating localization as a sequence generation task. This effectively solves the \"stitched pipeline\" problem common in existing methods, reducing interference from extraneous semantic information and enabling more precise, end-to-end localization.\n2. The model demonstrates state-of-the-art performance, significantly outperforming existing forensic and MLLM-based methods across multiple benchmarks. Furthermore, it shows exceptional robustness against common image distortions and strong cross-domain generalization, highlighting its potential for real-world applications.\n3. The SwEI module provides a novel and effective method for integrating domain-specific knowledge. By adaptively fusing multi-scale forensic features within the visual backbone, it allows the model to leverage the most relevant clues for different types of manipulations, from subtle splicing artifacts to semantic inconsistencies."}, "weaknesses": {"value": "1. The KSD module is a key innovation of this work, but the paper currently lacks a discussion on the selection of its critical hyperparameters (e.g., the number of clusters for the codebook) and a sensitivity analysis. Adding this would further strengthen the completeness and reproducibility of the module's design.\n2. The paper's focus is on methodological innovation and performance validation, where it achieves excellent results. A further discussion on the model's computational efficiency (e.g., parameter count, inference latency) would help readers more comprehensively evaluate its deployment potential in various application scenarios, thus enhancing the practical value of the paper."}, "questions": {"value": "1. The design of the KSD module is convincing, particularly its use of clustering to smooth gradients. We are interested to know how sensitive the model's performance is to key hyperparameters within the KSD, such as the number of clusters for the codebook. An ablation study on this would effectively reinforce the completeness of the module's design.\n2. The choice of SparseViT as the expert model for the SwEI module proves effective. Could the authors elaborate on the specific rationale behind this selection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZbL441OUsV", "forum": "WPgaGP4sVS", "replyto": "WPgaGP4sVS", "signatures": ["ICLR.cc/2026/Conference/Submission11262/Reviewer_8BYc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11262/Reviewer_8BYc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761703108132, "cdate": 1761703108132, "tmdate": 1762922421774, "mdate": 1762922421774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TamperTok, a novel framework for image tampering detection and localization (IMDL) that leverages Multi-modal Large Language Models (MLLMs). The authors identify key weaknesses in prior MLLM-based approaches, namely their reliance on \"stitched\" pipelines with exogenous segmentation decoders , which leads to information bottlenecks and imprecise localization due to a mismatch between semantic MLLM embeddings and the spatial nature of the task."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper's primary contribution, reformulating MLLM-based localization from a segmentation-based \"stitched\" pipeline to an end-to-end autoregressive token generation task, is highly novel and elegant.\n\n- The problems are clearly articulated, and the proposed solutions (KSD, SwEI) are well-motivated, technically sound, and directly target the identified weaknesses. The experimental evaluation is comprehensive and rigorous.\n\n- The fact that this MLLM-based model surpasses dedicated, SOTA forensic expert models like SparseViT in a challenging cross-domain setting, provides a new and much more effective blueprint for adapting MLLMs to fine-grained, non-semantic, and spatially-precise tasks."}, "weaknesses": {"value": "- The SwEI module's success relies on injecting features from SparseViT, which is itself a SOTA forensic expert. This raises a question of how much of the performance is simply a successful distillation of the expert model.\n\n- The proposed TamperTok architecture involves running a large MLLM and an expert model (SparseViT) in parallel to extract features, followed by an autoregressive decoding step. This is almost certainly more computationally expensive (in terms of parameters, GFLOPs, and latency) than the baselines (e.g., SparseViT alone, or FakeShield which uses a lightweight decoder)."}, "questions": {"value": "How crucial is the choice of SparseViT as the expert model for SwEI? Have you experimented with using a weaker or architecturally different expert (e.g., ManTra-Net, MVSS-Net)? Is the SwEI performance boost contingent on using a top-tier expert?\n\nOther questions may be referred from Weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YOpBmMIxhd", "forum": "WPgaGP4sVS", "replyto": "WPgaGP4sVS", "signatures": ["ICLR.cc/2026/Conference/Submission11262/Reviewer_KQPs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11262/Reviewer_KQPs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729353823, "cdate": 1761729353823, "tmdate": 1763011162937, "mdate": 1763011162937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel Multimodal Large Language Model (MLLM) framework named TamperTok for Image Manipulation Detection and Localization (IMDL). The authors astutely reframe the localization task as an autoregressive token generation problem, thereby creating an end-to-end trainable system that avoids the pitfalls of prior methods relying on separate, non-integrated segmentation decoders. The core contributions are twofold: the Kernel Splatting Decoder (KSD), which enables stable, direct supervision for mask generation by smoothing token gradients, and the Scene-wise Expert Injection (SwEI) module, which intelligently fuses multi-scale forensic features from an expert model into the MLLM's visual backbone. Comprehensive experimental results demonstrate that TamperTok not only sets a new state-of-the-art across numerous benchmarks but also exhibits impressive robustness and cross-domain generalization."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Elegant End-to-End Formulation: The paper's primary strength lies in its reformulation of a localization problem into a sequence generation task. This is an elegant solution that directly addresses the information bottleneck and semantic mismatch issues that plague existing MLLM-based methods. By making the entire pipeline from image to mask tokens directly supervisable and end-to-end differentiable, the framework fully leverages the MLLM's spatial reasoning capabilities.\n2. Rigorous and Meaningful Evaluation: The experimental design is exceptionally thorough. The authors go beyond standard in-domain tests and place a strong emphasis on cross-domain generalization by evaluating on six diverse, unseen datasets. This is a critical and often overlooked aspect in the forensics field, and the model's strong performance in these challenging scenarios convincingly demonstrates its real-world viability.\n3. Sophisticated Knowledge Integration: The Scene-wise Expert Injection (SwEI) module is a well-designed and highly effective method for integrating domain-specific knowledge. Unlike naive late-fusion approaches that can dilute important signals, SwEI's attention-based, multi-scale injection at intermediate stages of the visual encoder ensures that subtle forensic artifacts are preserved and effectively aligned with the visual features. This sophisticated fusion strategy is a key reason for the model's high performance, especially in detecting challenging manipulations like copy-move."}, "weaknesses": {"value": "1. Regarding the SwEI module, the paper mentions that it can fuse features at different levels to handle various types of tampering . However, the paper does not further explain whether the strength of the injected expert model features changes when facing different tampering types (e.g., splicing, copy-move).\n2. The paper does not provide an ablation study for the clustering hyper-parameters used in the KSD module's loss calculation . For instance, no justification is given for the choice of the number of code-book clusters, making it difficult for readers to fully understand the design decisions of the module and its sensitivity to the final performance.\n3. Some references have wrong information or format. For example, the reference in lines 560-562 should be [1].\n\n[1] Chenfan Qu, Yiwu Zhong, Chongyu Liu, et al. Towards modern image manipulation localization: A large-scale dataset and novel methods[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 10781-10790."}, "questions": {"value": "The authors are encouraged to address the points raised in the 'Weaknesses' section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "i4hldDKHd3", "forum": "WPgaGP4sVS", "replyto": "WPgaGP4sVS", "signatures": ["ICLR.cc/2026/Conference/Submission11262/Reviewer_Vn19"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11262/Reviewer_Vn19"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801867365, "cdate": 1761801867365, "tmdate": 1762922420179, "mdate": 1762922420179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TamperTok reframes image tampering localization (IMDL) as an autoregressive token generation task using multimodal LLMs.\nInstead of relying on external segmentation decoders, it introduces the Kernel Splatting Decoder (KSD) for direct spatial mask prediction and the Scene-wise Expert Injection (SwEI) module to inject multi-scale forensic features into the visual encoder.\nAcross six benchmarks, TamperTok outperforms both prior MLLM-based methods (FakeShield, SIDA) and expert forensic models (SparseViT), improving IoU/F1 by around 20% and showing strong robustness to compression, noise, and scaling. The appendix also shows promising explainability results with structured textual reasoning."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- TamperTok is the first fully end-to-end autoregressive framework for image tampering localization using multimodal LLMs.\n- It reformulates tampering detection as a token generation task, eliminating the need for an external segmentation head. This has benchmark-level significance for future multimodal tampering detection research and provides a strong foundation for further extensions.\n- The design and reasoning around the codebook similarity avoidance in the proposed KSD module are very sound. In particular, the mechanism to prevent gradient oscillation caused by similar codebook entries, and the methods to mitigate sharp gradients, appear novel both in theory and in practical implementation.\n- Extensive experiments demonstrate the solid performance of the proposed model."}, "weaknesses": {"value": "- The backbone choice (internVL in this paper) of the large model is quite important — it’s better to include this information in the main text to make the paper more self-contained, rather than mentioning it only in the appendix.\n- It would be helpful if the paper could clarify how the codebook is selected or trained, including the data and procedure used for its construction, which would make the methodology more transparent and reproducible."}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1CYNqehGDS", "forum": "WPgaGP4sVS", "replyto": "WPgaGP4sVS", "signatures": ["ICLR.cc/2026/Conference/Submission11262/Reviewer_Zn9c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11262/Reviewer_Zn9c"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989840107, "cdate": 1761989840107, "tmdate": 1762922419441, "mdate": 1762922419441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}