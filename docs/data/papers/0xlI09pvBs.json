{"id": "0xlI09pvBs", "number": 18492, "cdate": 1758288283038, "mdate": 1759897099810, "content": {"title": "PolicyRAG: Prompt-Guided Symbolic Graph Memory for Interpretable Multi-Hop Retrieval", "abstract": "Retrieval augmented generation is a powerful way to ground large language models in external knowledge, yet most pipelines still treat the prompt as instructions for text production rather than as a control surface for retrieval. We introduce PolicyRAG, a framework that recasts retrieval as an explicit, auditable policy operating over a symbolic graph memory. Text is organized into lightweight typed links between entities and passages, enabling transparent search and controllable evidence selection. Beyond accuracy, the policy is compact and human editable, supporting governance, domain adaptation, and safety review without retraining. At query time, the policy seeds candidate entities, invokes brief LLM calls only for disambiguation and local gating, and performs symbolic traversal with Personalized PageRank (PPR). The resulting scores are projected to passages and finalized with a small, transparent re-ranker, producing a per-query trace of seeds, paths, and scores for explainable evidence selection. Compared with long-context expansion, the policy keeps test-time compute modest while preserving answer quality. On multi-hop question answering benchmarks, PolicyRAG achieves state-of-the-art results on HotpotQA (F1 80.7), 2WikiMultiHopQA (F1 78.9), and MuSiQue (F1 55.9) while remaining fully auditable and training free. We also assess domain adaptability on domain-specific datasets. By coupling symbolic structure with prompt level control, PolicyRAG provides a practical route from question to verifiable evidence and advances accuracy, efficiency, and trust in retrieval augmented generation.", "tldr": "", "keywords": ["Large Language Models", "Retrieval Augmented Generation", "Graph Based Reasoning", "Personalized PageRank", "Knowledge Graphs", "Symbolic Graph Memory", "Multi-Hop Question Answering"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c88b8ea736e3618ac2d7afd4020fb58ad157d001.pdf", "supplementary_material": "/attachment/02d66f37450b1f2f21d0b8ae07d118229eda7f60.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposes PolicyRAG, which extracts entities and builds a KG at offline time, and leverages the KG for retrieval and QA in RAG, especially for multi-hop questions. My main concern for this paper is that I don't see how it is different from past work like HippoRAG and why the quality is better than HippoRAG"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper solves an important problem--how to answer multi-hop questions using RAG.\n\nS2. Extensive experiments on 5 benchmarks showing improvement of PolicyRAG over existing methods."}, "weaknesses": {"value": "W1. The description of how to construct the symbolic graph is not super clear.\n- How to discover salient entities? At what granularity?\n- How to identify factual assertions (triples)?\n- How are the typed relations defined? \n- How are the extractions from different passages connected?\n- How are dedups done?\n\nW2. Related to W1, it is unclear what's the quality of the extracted symbolic graph and how does that affect QA quality? What's the design choices in graph building and granularity selections?\n\nW3. For Composite transitions (Sec 3.1), what exactly is the goal and method? What's B and how is it computed? What's A_r and A_s and how to compute?\n\nW4. Similarly, a lot of description for run-time QA is unclear. In (2), how is spec(e) computed? Why compact set F_s and how does that affect retrieval recall and e2e results?\n\nW5. What's the latency impact for runtime PPR? \n\nW6. More importantly, the paper discussed HippoGraph, but it's unclear how it goes beyond HippoGraph and significantly improves the results? The methods sound very similar. \n\nW7. How much is the method constrained by corpus size? How well does it scale up?\n\nW8. What are limitations of the method?"}, "questions": {"value": "Please answer all questions in the weakness session."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A3KgZQK8bo", "forum": "0xlI09pvBs", "replyto": "0xlI09pvBs", "signatures": ["ICLR.cc/2026/Conference/Submission18492/Reviewer_MYAX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18492/Reviewer_MYAX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761432784752, "cdate": 1761432784752, "tmdate": 1762928187423, "mdate": 1762928187423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to utilize prompt-based policies to improve the performance of LLMs on multi-hop reasoning tasks. Several experiments were conducted to verify the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The experiments in this paper are relatively abundant, and many auxiliary experiments are conducted to further explore the issues.\n\n2. The proposed method has certain insights."}, "weaknesses": {"value": "1. The presentation should be improved. The Introduction section is somewhat redundant and does not clearly and concisely state the motivation for the proposed method, which is very confusing. I suggest that the paper use examples to illustrate the specific motivation, including the problems encountered in previous work and the motivation for the proposed method.\n\n2. Non-sentence-initial citations should use \\citep instead of \\citet.\n\n3. The Methods section, Section 3, is also difficult to understand. The descriptions of the pipeline are not clear enough. I suggest adding a few more examples to help clarify the details.\n\n4. No code is provided."}, "questions": {"value": "See weaknesses above. Please respond to these cons."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J4QjZv04Ha", "forum": "0xlI09pvBs", "replyto": "0xlI09pvBs", "signatures": ["ICLR.cc/2026/Conference/Submission18492/Reviewer_cZtt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18492/Reviewer_cZtt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717255834, "cdate": 1761717255834, "tmdate": 1762928186948, "mdate": 1762928186948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PolicyRAG, a framework for multi-hop retrieval that enables transparent, training-free, and interpretable multi-hop evidence selection. The paper reports experimental results on multiple benchmark datasets and claims the proposed framework achieves state-of-the-art performance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The paper addresses a relevant problem, and introduces a framework tailored to it.\n* The authors present experimental results across multiple benchmark datasets."}, "weaknesses": {"value": "* The paper asserts that the proposed approach achieves strong performance; however, the comparison with existing works appears to be unfair. For instance, in Table 2, the underlying base LLMs used for comparison differ, making it unclear whether the reported superiority of the proposed method is solely attributable to the strength of GPT-4.1 mini.\n\n* The paper only reports results based on GPT-4.1 mini, leaving the generalizability of the method unclear.\n\n* The presentation of the paper requires significant improvement. For example, the cited \"ACL\" paper \"Hipporag 2: Enhanced retrieval-augmented generation with neurobiological memory principles.\" does not exist. It appears that minimal effort was made to properly reformat the manuscript according to the ICLR template."}, "questions": {"value": "* Where can we find the paper \"Hipporag 2: Enhanced retrieval-augmented generation with neurobiological memory principles.\" in the references?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "KhfcJkCIp9", "forum": "0xlI09pvBs", "replyto": "0xlI09pvBs", "signatures": ["ICLR.cc/2026/Conference/Submission18492/Reviewer_ThPW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18492/Reviewer_ThPW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969701214, "cdate": 1761969701214, "tmdate": 1762928186393, "mdate": 1762928186393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}