{"id": "Qe7dKZOtWM", "number": 18499, "cdate": 1758288377046, "mdate": 1763702090226, "content": {"title": "Parameterization-Based Dataset Distillation of 3D Point Clouds through Learnable Shape Morphing", "abstract": "Recent attempt in dataset distillation has been made to compress large-scale training datasets into compact synthetic versions, significantly reducing memory usage and training costs. While parameterization-based approaches have shown promising results on image datasets, their application to 3D point clouds remains largely unexplored due to the irregular and unordered nature of 3D data. In this paper, we first introduce a parameterization-based dataset distillation framework for 3D point clouds that enables the use of more diverse synthetic samples than conventional methods under the same memory budget. We first construct an initial synthetic dataset containing multiple anchor samples with a coarser resolution than the original sample. We also generate new samples by morphing the shapes of the anchor samples with learnable weights to improve the diversity of synthetic dataset. Moreover, we devise a uniformity-aware matching loss to ensure the structural consistency when comparing the original and synthetic datasets. Extensive experiments conducted on five standard benchmarks—ModelNet10, ModelNet40, ShapeNet, ScanObjectNN, and OmniObject3D—demonstrate that the proposed method effectively optimizes both the synthetic samples and the weights for shape morphing, outperforming existing dataset distillation methods.", "tldr": "Distilled dataset parameterization method for 3D point clouds using learnable shape morphing.", "keywords": ["Dataset Distillation", "Distilled Dataset Parameterization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/06ee95b14b7c68b26a240b244c5b379c88924509.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a parameterized dataset distillation framework for 3D point clouds, which stores multiple coarse \"anchor\" point clouds for each synthetic sample and then generates additional synthetic samples through learnable shape deformation. To reduce the storage budget, the proposed method replaces the full-resolution synthetic sample with several low-resolution anchors plus lightweight blending weights. Experiments on ModelNet10/40, ShapeNet, and ScanObjectNN show substantial gains over prior DD and 3D-DD baselines, especially at low “points per class” (PPC) settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a novel 3D data parameterization method that expands the diversity of the synthetic dataset under a fixed budget. Replacing one full-res synthetic sample with M low-res anchors and L blended variants is reasonable.\n\n2. The proposed method has strong empirical gains, especially at extreme compression ratios. For example, at PPC=1 with PointNet evaluation, the proposed method improves SOTA by large margins across ModelNet10/40, ShapeNet, and ScanObjectNN. Moreover, it also shows the best cross-architecture transfer in most cases.\n\n3. The proposed method exhibits plug-and-play applicability. Applying it to DM and SADM has a significant performance boost."}, "weaknesses": {"value": "1. The experiments are conducted on the classification task only. Although effective, there are still some other important tasks in 3D data, e.g., segmentation and detection. Can the proposed method be generalized to other tasks?\n\n2. As the number of classes increases (e.g., ModelNet10 -> ModelNet40), the performance of the proposed method degenerates. Therefore, we may doubt whether the proposed method can be applied to large-scale datasets with more categories. It would be better if the author could provide more experimental results to validate its effectiveness.\n\n3. The number of PPC is limited within \\{1, 3, 10\\}, which cannot achieve lossless performance in most datasets. More experimental results are required.\n\n4. The source code is not provided. The reproducibility is not guaranteed."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K0VvWvlFUM", "forum": "Qe7dKZOtWM", "replyto": "Qe7dKZOtWM", "signatures": ["ICLR.cc/2026/Conference/Submission18499/Reviewer_mBpX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18499/Reviewer_mBpX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632064297, "cdate": 1761632064297, "tmdate": 1762928197697, "mdate": 1762928197697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the first parameterization-based dataset distillation (DDP) framework for 3D point clouds, enabling more diverse synthetic samples under fixed memory budgets. Rather than optimizing full-resolution synthetics (as in prior DD), it initializes with M coarse anchors per class (MN₂ ≤ N₁) and generates L additional samples via learnable convex morphing of aligned anchors (akin to 3D mixup). Optimization uses a uniformity-aware SADM loss, which partitions full-res originals into coarse subsets and downweights non-uniform partitions via CV-based penalties. Evaluated on ModelNet10/40, ShapeNet, and ScanObjectNN with PointNet (500 epochs), it achieves SOTA DD accuracy (e.g., 82.5% MN40@10PPC vs. prior 79.6% SOTA) and strong cross-arch transfer (e.g., 47.7% PN++ on MN40@1PPC). However, large gaps to oracle (\"Whole\") persist (e.g., 73% vs 89% MN40@1PPC)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. First to parameterize point clouds via coarse anchors + morphing, yielding ~M+L samples vs. 1 prior—simple, zero-extra-cost diversity boost (storage: 96MN₂KC + 32L(M-1)KC bits ≤ prior budget).\n2. Consistent SOTA over 2D-transferred (DM/DC/MTT), 3D-tailored (PCC/SADM), and coresets across 4 datasets/PPCs; excellent cross-arch gen (Table 2) shows robustness.\n3. Anchor alignment (Hungarian on dist matrices) + uniformity penalty elegantly handles irregularity/coarseness; ablation potential high (e.g., morphing ablation implied).\n4. Fig.1/2 crisp; budget math (Sec.3.4) rigorous; oracle baselines throughout."}, "weaknesses": {"value": "1. Morphing is point-wise linear interpolation post-alignmen fundamentally 3D mixup on anchors. Lacks novelty over image DDP (IDC/FreD/HaBa) or point cloud mixup; alignment brittle for topology-varying shapes (e.g., chair/table).\n2. DD perf drops ~15-40% vs. oracle (e.g., 32% vs 63% ScanObj@1PPC), understates distillation limits; no segmentation/transfer tasks (e.g., PartNet).\n3. Baseline fairness is unclear. Do priors (PCC/SADM) use data aug (jitter/rotate/scale/chroma std for PointNet)? Paper implies yes (\"standard\"), but unconfirmed; oracle likely uses aug, inflating gaps.\n4. Limited scope/ablation: No hyperparam sensitivity (M/L/N₂); fixed PointNet (not DGCNN/PT from start); no real-time distilling or downstream (e.g., detection)."}, "questions": {"value": "1. How does learnable morphing outperform fixed-uniform weights or point cloud mixup (e.g., PointMixup) baselines? Ablate alignment (e.g., w/o Hungarian)?\n2.Explicitly confirm aug for all methods/oracle (e.g., Table: jitter σ=0.01, rot [±15°], scale [0.8-1.2])—did SADM/PCC reimpls match your 500-epoch setup?\n3. Add \"Oracle (Full Dataset)\" column w/ aug details; report relative gap (e.g., 82% of oracle @10PPC)? Test on downstream (PartNet seg, KITTI det)?\n4. Ablate M/L/N₂ tradeoffs (Fig?); why N₂=256, M=4, L=3? Test higher-res anchors or topology-aware alignment (e.g., functional maps)?\n5. Show qualitative synthetics (morphing fails?); robustness to noisy ScanObj aug (occlusion/clutter)? Extend to dynamic scenes (nuScenes LiDAR)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YCUb4ZKtvv", "forum": "Qe7dKZOtWM", "replyto": "Qe7dKZOtWM", "signatures": ["ICLR.cc/2026/Conference/Submission18499/Reviewer_3htV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18499/Reviewer_3htV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750473627, "cdate": 1761750473627, "tmdate": 1762928197229, "mdate": 1762928197229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the first parameterization-based dataset distillation framework for 3D point clouds, enabling the creation of compact synthetic datasets under strict memory constraints. Unlike prior 2D methods, it introduces learnable shape morphing to generate diverse samples by blending coarser anchor shapes with learnable weights, maximizing diversity within the same memory budget. A uniformity-aware matching loss further enforces structural consistency by balancing contributions from different data partitions. Joint optimization of anchors and morphing weights minimizes this loss, yielding superior results on ModelNet10/40, ShapeNet, and ScanObjectNN, especially under tight memory conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper introduces a novel parameterization-based dataset distillation framework for 3D point clouds\n+ Outperforms existing dataset distillation techniques across all four standard benchmarks (ModelNet10, ModelNet40, ShapeNet, and ScanObjectNN) at various Point Clouds Per Class (PPC) settings.\n+ On ModelNet10 at PPC=1, it achieves an accuracy of 87.7% (previous SOTA 35.9%). \n+ On the challenging real-world dataset, ScanObjectNN improves the previous SOTA from 17.6% to 32.6% at PPC=1."}, "weaknesses": {"value": "- Comparison of cross-architecture generalization performance at PPC = 1 (Table 2) is made on relatively older models (PointNet++, PointConv, and PointTransformer) and not on recent models (e.g., PointMamba). Hence, it is unclear if the proposed approach works on recent methods.\n- Results limited to a few selected datasets: the paper does not mention if the results of the real-world dataset ScanObjectNN [1] presented in the paper are for the hardest variant PB_T50_RS. \n- Moreover, these datasets have already reached saturation points; recent real-world datasets (e.g., 3DGrocery100[2], OmniObject3D[3]) are not used to evaluate the proposed approach.\n\n[1] Uy, Mikaela Angelina, et al. \"Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data.\" Proceedings of the IEEE/CVF international conference on computer vision. 2019.\n\n[2] Sheshappanavar, Shivanand Venkanna, et al. \"A benchmark grocery dataset of real-world point clouds from single view.\" 2024 International Conference on 3D Vision (3DV). IEEE, 2024.\n\n[3] Wu, Tong, et al. \"Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KjLc6A0W3s", "forum": "Qe7dKZOtWM", "replyto": "Qe7dKZOtWM", "signatures": ["ICLR.cc/2026/Conference/Submission18499/Reviewer_tN6U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18499/Reviewer_tN6U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996622820, "cdate": 1761996622820, "tmdate": 1762928196417, "mdate": 1762928196417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel parameterization-based dataset distillation framework for 3D point clouds. The approach leverages learnable shape morphing to generate diverse synthetic samples within a constrained memory budget. The core idea involves using multiple coarser anchor samples and blending their shapes with learnable weights.  While the paper presents promising results on standard benchmarks, several critical issues need to be addressed before it can be considered for acceptance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The application of parameterization techniques to dataset distillation for 3D point clouds is a relatively unexplored area, and this work introduces a potentially efficient approach. The idea of using learnable shape morphing to generate diverse synthetic samples is innovative.\n2. The paper demonstrates promising performance on several benchmarks (ModelNet10, ModelNet40, ShapeNet, ScanObjectNN), outperforming existing dataset distillation methods in some cases.\n3. The paper is generally well-written and explains the proposed method clearly."}, "weaknesses": {"value": "1. Critical Constraint Violation: The paper states the memory constraint MN2 ≤ N1, where M is the number of distinct coarser samples (anchors), N2 is the number of points in each anchor, and N1 is the number of points in the original (full-resolution) sample. The experimental setup violates this constraint. For instance, the paper mentions N2 = 252, M = 4, and N1 = 1024. In this case, MN2 = 4 * 252 = 1008, which is LESS than the constraint, should be MN2 <= N1. This casts significant doubt on the validity of the experiments and the claims made about memory efficiency. \n2. Limited Backbone Architectures: The experimental evaluation primarily relies on PointNet as the backbone network. While PointNet is a common choice for point cloud processing, it's essential to demonstrate the generalization ability of the distilled datasets with various backbone architectures. Evaluating with PointNet++ (as reported in Table 2), PointConv, and Point Transformer is a good starting point, but further investigation may be warranted. The paper needs to provide a more comprehensive evaluation across different backbone architectures to demonstrate the robustness and general applicability of the proposed dataset distillation method. Section 4.2 states that for the cross-architecture analysis, 3 different architectures are used, and claims the method \"demonstrates strong generalization ability\" across most datasets and architectures, but Section 4.5 states that the datasets were distilled with a PointNet backbone. Therefore, it's only demonstrated that the distilled dataset works well with other architectures, but not that the method produces good datasets when other architectures are used to distill the data."}, "questions": {"value": "1. The authors need to provide a thorough justification for the choice of experimental parameters and explicitly demonstrate how the memory constraint MN2 ≤ N1 is satisfied (or revise the constraint to be mathematically correct). If the constraint is not satisfied, the experiments need to be re-run with valid parameters. Provide a clear explanation as to why a seemingly small violation of the constraint is acceptable and doesn't invalidate the comparisons.\n2. Expand the experimental evaluation to include more diverse and state-of-the-art backbone architectures. Consider including more recent Transformer-based networks or graph neural networks to assess the generalization capabilities of the distilled datasets.\n3. Elaborate on the reasons behind the lower accuracy observed on ScanObjectNN with PointNet++ (as mentioned in Section 4.3). Investigate whether the choice of the SADM loss is indeed a limiting factor and explore alternative loss functions that might be more suitable for this dataset and architecture combination."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MKQExgK5vB", "forum": "Qe7dKZOtWM", "replyto": "Qe7dKZOtWM", "signatures": ["ICLR.cc/2026/Conference/Submission18499/Reviewer_5FmN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18499/Reviewer_5FmN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155640589, "cdate": 1762155640589, "tmdate": 1762928195266, "mdate": 1762928195266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}