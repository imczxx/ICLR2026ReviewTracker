{"id": "N1kiOll2EN", "number": 16082, "cdate": 1758259591321, "mdate": 1759897262830, "content": {"title": "SVD Provably Denoises Nearest Neighbor Data", "abstract": "We study the Nearest Neighbor Search (NNS) problem in a high-dimensional setting where data originates from a low-dimensional subspace and is corrupted by Gaussian noise. Specifically, we consider a semi-random model where $n$ points from an unknown $k$-dimensional subspace of $\\mathbb{R}^d$ ($k \\ll d$) are perturbed by zero-mean $d$-dimensional Gaussian noise with variance $\\sigma^2$ on each coordinate. Without loss of generality, we may assume the nearest neighbor is at distance $1$ from the query, and that all other points are at distance at least $1+\\varepsilon$. We assume we are given only the noisy data and are required to find NN of the uncorrupted data. We prove the following results:\n\n1. For $\\sigma \\in O(1/k^{1/4})$, we show that simply performing SVD denoises the data; namely, we provably recover accurate NN of uncorrupted data (Theorem 1.1).\n2. For $\\sigma \\gg 1/k^{1/4}$, NN in uncorrupted data is not even {\\bf identifiable} from the noisy data in general. This is a matching lower bound on $\\sigma$ with the above result, demonstrating the necessity of this threshold for NNS (Lemma 3.1).\n3. For $\\sigma \\gg 1/\\sqrt k$, the noise magnitude ($\\sigma \\sqrt{d}$) is significantly exceeds the inter-point distances in the unperturbed data. Moreover, NN in noisy data is different from NN in the uncorrupted data in general.\n\\end{enumerate}\n\nNote that (1) and (3) together imply SVD identifies correct NN in uncorrupted data even in a regime\nwhere it is different from NN in noisy data. This was not the case in existing literature (see e.g. (Abdullah et al., 2014)). Another comparison with (Abdullah et al., 2014) is that it requires $\\sigma$ to be at least an inverse polynomial in the ambient dimension $d$. The proof of (1) above uses upper bounds on perturbations of singular spaces of matrices as well as concentration and spherical symmetry of Gaussians. We thus give theoretical justification for the performance of spectral methods in practice. We also provide empirical results on real datasets to corroborate our findings.", "tldr": "", "keywords": ["nearest neighbor", "planted models"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/20466bd8fec14cac659e1ec71fef4b89effe70ca.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the Nearest Neighbor Search (NNS) problem where data points, originating from an unknown $k$-dimensional subspace within a $d$-dimensional space ($k \\ll d$), are corrupted by Gaussian noise . The objective is to recover the nearest neighbor of the *uncorrupted* data, given only noisy observations and queries. The authors propose a simple SVD-based algorithm that involves splitting the data matrix, computing the top-$k$ subspace for each half, and then projecting the data from one half onto the subspace derived from the other to find the nearest neighbor . The primary contribution is a proof that this method successfully recovers the true nearest neighbor even when the noise variance $\\sigma$ is as large as $O(1/k^{1/4})$ . This is a significant finding, as it holds in a noise regime where the nearest neighbor in the noisy data may differ from the true nearest neighbor. The authors establish this as a sharp threshold by providing a matching information-theoretic lower bound, demonstrating that recovery is impossible for $\\sigma \\gg 1/k^{1/4}$."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a practical and fundamental problem in data analysis. The main strength is the substantial improvement over prior SOTA (e.g., Abdullah et al., 2014), which required the noise level $\\sigma$ to be bounded by an inverse polynomial in the *ambient* dimension $d$. This work's bound of $\\sigma = O(1/k^{1/4})$ depends only on the intrinsic dimension $k$, which is a major advancement for $k \\ll d$ scenarios .\n\nThe work extends our understanding of this problem and identifies critical thresholds for $\\sigma$ and providing both an algorithmic upper bound and a matching lower bound. This comprehensive analysis is a key strength. Another significant contribution is showing that the algorithm works even when the noise is large enough ($\\sigma \\gg 1/\\sqrt{k}$) to change the identity of the nearest neighbor in the observed data, a regime not handled by previous work.\n\nThe algorithm itself is simple and clearly explained. The theoretical claims are supported by experiments on both synthetic and real-world datasets (Glove and MNIST), which confirm the algorithm's practical benefits over a naive approach and validate the theoretical dependency on key parameters ."}, "weaknesses": {"value": "The primary weakness is the lack of an explicit discussion of the paper's technical novelty. The analysis appears to rely on standard matrix perturbation bounds (like Davis-Kahan and Wedin) and concentration inequalities. The authors do not clearly articulate what new analytical techniques or core technical innovation enables them to achieve the $O(1/k^{1/4})$ bound, which is the paper's central improvement. It is unclear if the novelty lies simply in the data-splitting algorithm design, which simplifies independence arguments.\n\nThis new bound comes at the cost of a dependency on $s_k(B)$, the $k$-th singular value of the unperturbed data matrix. Prior work did not require this assumption. While the authors argue in Section 2.3 that $s_k(B)/\\sqrt{n}$ is likely a non-zero constant for \"well-conditioned\" data, this is a significant trade-off, especially when data is approximately embedded in a subspace (which is one core motivation of the model considered in this paper); see the question below about overspecification. \n\nThe experimental comparison is made against a naive baseline, not against the (Abdullah et al., 2014) algorithm that serves as the main theoretical comparator. The authors state this was due to the implementation infeasibility of the prior work, but this omission makes it difficult to empirically assess the practical performance gain over the previous state-of-the-art.\n\nFinally, some typos and clarity suggestions\n* \"weel-known\" instead of \"well-known\".\n* In Section 3.1, the query point $\\tilde{q}$ is missing from the problem description (Line 062), which makes the notation for $q$ confusing."}, "questions": {"value": "1.  Could you please clarify the core technical novelty of your analysis? The data-splitting trick  simplifies the probabilistic argument, but is this the key element that allows you to break the dependency on the ambient dimension $d$ and achieve the $O(1/k^{1/4})$ bound? Or is there a new, non-standard bound or analytical step being used?\n\n2.  The discussion in Section 2.3 regarding the requirement to know $k$ is confusing . You state that using a \"larger dimensional SVD subspace projection\" (i.e., overspecifying $k$) \"may be of use if we want to work with weaker assumptions\" . This seems counter-intuitive. Your bounds in Theorem 1.1 depend on $s_k(B)$. If the true rank is $k$ and you use a $k' > k$, the $k'$-th singular value $s_{k'}(B)$ would be zero. This would make your noise bound infinitely restrictive, not weaker. Can you clarify how overspecifying $k$ could be helpful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GnFi2oV8BO", "forum": "N1kiOll2EN", "replyto": "N1kiOll2EN", "signatures": ["ICLR.cc/2026/Conference/Submission16082/Reviewer_U3vH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16082/Reviewer_U3vH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760914341131, "cdate": 1760914341131, "tmdate": 1762926267862, "mdate": 1762926267862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies nearest neighbor search problem for high-dimensional spaces where the data lies low-dimensional subspace and is coordinate-wise corrupted by Gaussian noise. The authors shows that when the noise have small variance i.e. $\\sigma = O(k^{-1/4})$ they can recover the correct nearest neighbor; for large variance, recovery becomes impossible."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper establishes tight upper and lower bounds on the noise threshold for nearest neighbor recovery, providing a clear theoretical characterization of when SVD-based denoising succeeds. The proposed algorithm is conceptually simple and well-presented, with a clean and transparent analysis that makes the results easy to follow."}, "weaknesses": {"value": "I think the main critism here is the setting is too ideal seems a bit far from practical: It assumes the points are exactly in a k-dimensional subspace.  Moreover, the guarantees depend on the singular value of the clean data matrix, which could be very large for ill-conditioned data."}, "questions": {"value": "1. The model assumes data drawn exactly from a low-dimensional linear subspace corrupted by isotropic Gaussian noise. Could you identify any realistic scenarios or application domains where this setting meaningfully reflects observed data distributions.\n2. Do you think the dependence on the s_k is an artifact of the analysis or it is actually tight?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1feqyGside", "forum": "N1kiOll2EN", "replyto": "N1kiOll2EN", "signatures": ["ICLR.cc/2026/Conference/Submission16082/Reviewer_7ikB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16082/Reviewer_7ikB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975981541, "cdate": 1761975981541, "tmdate": 1762926267453, "mdate": 1762926267453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies nearest neighbor problem while data is corrupted with random Gaussian noise. That is, given arbitrary $n$ points in $d$-dimensional space that can be embedded in a $k$-dimensional subspace, with zero-mean $d$-dimensional $\\sigma$-variance Gaussian noise, the algorithm is able to distinguish the nearest neighbor while all other points are at least $(1+\\epsilon)$ distance away. The paper gives detailed analysis on how large a $\\sigma$ may affect the distinguishability of the neighbor points. The algorithm is based on spectral method, specifically only two SVD calls, which outperforms the prior work that builds on a more complicated PCA tree, however, with an assumption that the $k$-th singular value is large enough."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Comparing to the prior work, i.e. Abdullah et al., 2014, this paper achieved an improved noise tolerance with a simple spectral method with two calls of SVD on randomly separated data points. While Abdullah et al., 2014 can tolerate up to Gaussian noise with variance of at most an inverse polynomial of $d$, this paper designs an algorithm that can handle $\\sigma=O(1/k^{1/4})$. On the other hand, the paper further extends the theoretical foundation for spectral methods that perform well on nearest neighbor search problems in many occasions, sometimes even better than the worst-case optimal random projection. The theoretical framework follows from Abdullah et al., 2014 by considering a semi-random model."}, "weaknesses": {"value": "The paper assumes a random Gaussian noise, which follows from Abdullah et al., seems strong. In this case, the algorithm is highly dependent on a high amount of randomness. Is it possible to find the nearest neighbor when corruptions on $d$ coordinates are no longer independent? On the other hand, given the large top-$k$ sigular values assumption, it is not obvious why random projection will necessarily fail in this case. Therefore, is $\\sigma=\\Theta(k^{-1/4})$ a necessary criteria for spectral methods to outperform random projection? It would be nice to list all noise level thresholds when or when not SVD would be preferred to random projection."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U2kCb9uzbf", "forum": "N1kiOll2EN", "replyto": "N1kiOll2EN", "signatures": ["ICLR.cc/2026/Conference/Submission16082/Reviewer_N6hW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16082/Reviewer_N6hW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762413076732, "cdate": 1762413076732, "tmdate": 1762926266951, "mdate": 1762926266951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the NNS problem in high dimensions and shows that if the noise is corrupted by Gaussian noise, then simply performing SVD recovers the NN of the uncorrupted data. This, I think, is a really nice result. \nThey study this phenomenon for various settings of variance of the Gaussian noise. It also improves the result of previous works that relied on the variance to be inverse polynomial in the ambient dimension."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The strength of the paper lies in the range of the variance for which they can show denoising using simple SVD."}, "weaknesses": {"value": "It is hard to parse Theorem 1.1. It would be great if the authors had added more discussion on it right after the theorem. \n\n219: lemma not theorem\nLine 237: well-known\nWhy the growth of singular values has to be distributed?\nI am a little confused. Data matrix being well conditioned is a normal assumption in real datasets, but is it also the case for geometric problems like NN?\nMy personal opinion is that having a worse noise assumption is better than that on the data matrix because we can control the latter. I would love to hear the authorsâ€™ perspective on this front. \n\nOne important issue with the submitted version is that it takes an unfair advantage of the page limit by making the margin smaller than the ICLR format (at least it looks like that to me; I might be wrong). I wanted to flag this in case other reviewers also have an objection with that. I understand that margin bound is not explicitly stated in the Call for Papers, but this feels wrong to me, mainly due to this line in the Call for Papers: \"Papers with main text beyond the page limit will be desk-rejected.\"\n\nI leave the last point to the meta reviewers and AC to make a judgement on, especially in regards to fairness to other submissions."}, "questions": {"value": "i would say that performing SVD is an expensive process, especially for high dimensional data points. Is there some other way one can do to speed up this proces?\n\nWhy should one believe that data matrix is well condition when the underlying data is the one used in NN? Is there any empirical evidence that the authors can point to?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zPyKwrKan9", "forum": "N1kiOll2EN", "replyto": "N1kiOll2EN", "signatures": ["ICLR.cc/2026/Conference/Submission16082/Reviewer_LHPh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16082/Reviewer_LHPh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762472346139, "cdate": 1762472346139, "tmdate": 1762926266356, "mdate": 1762926266356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}