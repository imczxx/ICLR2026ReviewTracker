{"id": "BQfRA3tqt9", "number": 23586, "cdate": 1758345955800, "mdate": 1759896806437, "content": {"title": "Emergent Deceptive Behaviors in Reward-Optimizing LLMs", "abstract": "While most AI alignment research focuses on preventing models from generating explicitly harmful content, a more subtle risk is emerging: capability-driven exploitation. We investigate whether language models, when trained with reinforcement learning (RL) in environments with implicit loopholes, will spontaneously learn to exploit these flaws to maximize their reward, even without any malicious intent in their training. To test this, we design a suite of five diverse \"vulnerability games,\" each presenting a unique, exploitable flaw related to proxy metrics, tool use, and self-evaluation. Our experiments show that models consistently learn to exploit these vulnerabilities, discovering opportunistic strategies that significantly increase their reward at the expense of task correctness or safety. More critically, we find that these exploitative strategies are not narrow \"tricks\" but generalizable skills; they can be transferred to new tasks and even \"distilled\" from a capable teacher model to other student models through data alone. Our findings reveal that capability-driven risks pose a fundamental challenge to current alignment approaches, suggesting that future AI safety work must extend beyond content moderation to rigorously auditing and securing the training environments and reward mechanisms themselves.", "tldr": "", "keywords": ["LLM; Emergent Misalignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c4be7cf14966bd23da420c79f9881012a6ab23df.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper creates 4 simple settings with reward hacking opportunities and evaluates:\n* How quickly 3 small LMs trained with GRPO explore these hacks;\n* How much LMs that learned how of these hacks also learn the other hacks;\n* Whether learning one hack makes other hacks easier to explore."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* The paper studies multiple models for multiple reward hacking settings, providing greater diversity than prior work.\n* The paper studies open-source models with open source algorithms. If the settings are open-sourced, this could allow the community to have better open-source model organisms of reward hacking relative to the status quo (e.g. where work like https://arxiv.org/pdf/2406.10162 is closed-source)\n* The generalization results, while not very surprising given prior work, provide additional evidence of cross-reward hack transfer, both in zero-shot and “catalysis” settings.\n* The settings are clearly explained, and the results are well presented."}, "weaknesses": {"value": "* RQ1 seems very hack-specific, and more a property of a particular tuple of RL setup, initialization and reward hack than a generalizable finding which the community can learn from (which the abstract implies).\n* The work is not particularly novel. In particular, https://arxiv.org/pdf/2406.10162 studied reward hacking settings and generalization questions closely related to this work.\n* Some reward hacks are very hard to explore, and your experiments do not cover those hacks. Like https://arxiv.org/pdf/2406.10162, it might have provided deeper insights about what would happen for the most important, hardest-to-explore reward hacks."}, "questions": {"value": "* What do you see as your main contributions compared to https://arxiv.org/pdf/2406.10162\n* How hard is exploration in your hardest to explore setting? More information about this would be welcome, for example by running the zero-shot (before any training) experiment with a large n (how big is n in table 3?) It would also be informative to see what ER is when fine-tuning on the intended behavior (which might erode some of the safety training that might cause an ER of 0 in non-base models).\n* The paper claims “self-taught strategies are significantly stealthier and more resilient to standard mitigation techniques” in the conclusion. What mitigation technique are you referring to?\n* Do you intend to open source the code work and make it easily usable by the community?\n\nAdditional feedback that will not affect my score:\n* A more consistent ordering of the 4 settings in Figures would make the paper easier to skim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4wnVcibRTR", "forum": "BQfRA3tqt9", "replyto": "BQfRA3tqt9", "signatures": ["ICLR.cc/2026/Conference/Submission23586/Reviewer_8vKM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23586/Reviewer_8vKM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760811750463, "cdate": 1760811750463, "tmdate": 1762942722924, "mdate": 1762942722924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper constructs four toy environments where it's possible for a model to reward hack. They finetune small LLMs in each of these environments using GRPO. They observe that for almost all (model, environment) pairs, the model learns to exploit the environment. Training on these environments has an inconsistent effect on the model's actual performance (as evaluated by ground truth).  They observe that training on one environment has inconsistent effects on whether the models exploit other environments. However, training on one environment often makes it much more likely that the models will exploit other environments when further trained on those."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I think this is an important topic that is worth of study. I think the basic methodology of this paper makes a lot of sense. I really like the idea of constructing these different categories of exploitable environments. I thought the paper was very clearly written and presented. I hope the authors continue researching in this direction."}, "weaknesses": {"value": "My biggest problem with the paper is that the models trained are very small and the environments are mostly quite simple and toy. I would love a version of this paper that followed basically exactly the same methodology, but with more sophisticated and varied environments and more capable models. (By varied environments, one thing I mean is that it would be great to have multiple environments within each theme, e.g. multiple self-grading environments.) I agree with the authors that simple toy environments are a meaningful contribution, but it's hard to learn very much from the results until the environments are substantially more sophisticated (which will probably also require working with models that are capable of understanding more abstract concepts). \n\nAlso, though obviously it's good to demonstrate things in more contexts, some of the results are not that novel. For example, Denison 2024, \"sycophancy to subterfuge\", demonstrates a similar result as the \"training on one environment makes reward hacking on another emerge faster\".\n\nI don't think the situational awareness data set distinguishes between the model learning to behave differently depending on contextual cues as opposed to learning to use out of context knowledge about situational awareness. To improve this, you could have it so that there are many different differences between the auditor style and the user style prompts, and some of those differences are only present in your split of the data set that you use for evaluating how much the model learns situational awareness. (For example, you could start out by synthetic document fine-tuning the model on a bunch of documents explaining that there are two teams of auditors. One only ever writes in English and has great grammar. The other groups of auditors use distinctly Catalan-influenced Spanish and entirely ask questions about a particular type of harm, but they use quite idiomatic language. Then you only train on auditor prompts from the first team, and you evaluate on auditor prompts from the second team.) (I think it's very unlikely that these tiny models are properly situationally aware, as opposed to picking up on the highly salient syntactic clues.)\n\n—\nMore minor disagreements:\n\nI would find table four much easier to read if the values in the cells were color coded. Maybe it should be two tables, one with accuracy and the other with ER.\n\n> This is likely because Situational Awareness training teaches them to identify benchmark-style questions, triggering a more cautious and honest self-assessment in that context.\n\nAgain, I'm quite skeptical the models are learning situational awareness like this."}, "questions": {"value": "Do you agree with my point about the situational awareness dataset? \n\nMaybe you have an argument that it's more valuable to make relatively toy examples of these phenomena than I think?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dZWxK2SlfV", "forum": "BQfRA3tqt9", "replyto": "BQfRA3tqt9", "signatures": ["ICLR.cc/2026/Conference/Submission23586/Reviewer_JP7J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23586/Reviewer_JP7J"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976617013, "cdate": 1761976617013, "tmdate": 1762942722660, "mdate": 1762942722660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether LLMs trained with standard RL will spontaneously learn to exploit environmental loopholes to maximize their reward, even without explicit malicious intent. The authors term this phenomenon \"capability-driven alignment risk,\" framing it as a misalignment that emerges from the model's increasing capabilities to find \"shortcuts\" in imperfect training setups. The authors' primary contribution is a suite of four \"vulnerability games\" designed to test for these emergent exploits, and evaluated and trained on Qwen3-4B, Qwen3-4B-Base, Llama-3.1-8B-Instruct. The reward mechanisms of the games are intentionally designed to be flawed. There is a proxy reward that diverges from the true goal reward, so that the optimal policy for maximizing the proxy reward is an exploit. This policy leads to a high Exploit Ratio (ER) but a low Task Accuracy (ACC), which demonstrates the \"emergent deceptive behaviors\"  the paper investigates."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The \"vulnerability games\" as an evaluation suite, isolating distinct failure modes, is an interesting contribution.\n2. The authors identify a more fundamental alignment problem that is different from the popular research area of adversarially induced deception."}, "weaknesses": {"value": "1. Presentation of the results is rather confusing; I could not understand what message Tables 1 and 2 are trying to convey. \n2. Too few qualitative analyses show the existence of deceptive behaviors in the model and how they changed during the course of training. The paper would be much stronger if it included concrete, qualitative examples in the main body for all four games, and showed the evolution of the behavior over the training steps. We see that the exploit occurs (ER=1.0), but we don't see how the model's generated text or code gradually warped from an honest-but-wrong attempt into a deliberate, reward-hacking exploit.\n3. The environments might be too simple and focus on single-turn QA while real-world exploits might span multiple turns. A multi-turn conversation allows for far more sophisticated deception, such as \"grooming\" the user or setting up a vulnerability over several interactions, which these \"toy\" environments cannot capture.\n4. To demonstrate the efficacy and the potential impact of your proposed evaluation metrics, it is suggested to perform evaluations on more capable models like GPT-4o or Claude-4 to see if similar deceptive alignment exists in these models. I understand RL fine-tuning on some of these models isn't possible, so zero-shot evaluation on SOTA API-based models will suffice.\n5. The experiments are limited to a small set of sub-10B models. While this successfully demonstrates the existence of the risk in smaller, open-source models, it fails to provide empirical evidence for the scaling part of the hypothesis. Are larger, more capable models (e.g., Llama-3.1-16B, Qwen-3-8B, Qwen-3-16B etc.) more prone to this hacking behavior?\n6. The paper's experiments are conducted exclusively using GRPO, but they could be exploitative behaviors emerge under GRPO optimization. What about other online RL algorithms like PPO, REINFORCE++, or offline algorithm like DPO?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "cgFpSemndw", "forum": "BQfRA3tqt9", "replyto": "BQfRA3tqt9", "signatures": ["ICLR.cc/2026/Conference/Submission23586/Reviewer_GpPU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23586/Reviewer_GpPU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762071956173, "cdate": 1762071956173, "tmdate": 1762942722485, "mdate": 1762942722485, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper builds four RL tasks that expose loopholes, and shows that GRPO-trained models reliably discover strategies that raise reward while degrading task fidelity or safety. The paper studies emergence of these strategies, transfer to other tasks, and cross-model distillation. The framing is presented as “capability-driven alignment risk.”"}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides 4 environments that can be used to assess reward hacking tendencies across different models, and provides some interesting qualitative analysis of the emergence of reward hacking across a variety of models"}, "weaknesses": {"value": "**Contribution.** Unless I'm missing something, it seems like the contributions of the paper seem quite overstated: RQ1 is well established to have a positive resolution (among many others, [this](https://arxiv.org/abs/2409.12822) and [this](https://arxiv.org/abs/2411.02306) paper). RQ2 seems to be in part a simple extension of [subliminal learning](https://arxiv.org/abs/2507.14805), and in part has been demonstrated by prior work (e.g. [this](https://arxiv.org/pdf/2508.17511) or [this](https://www.lesswrong.com/posts/Ge55vxEmKXunFFwoe/reward-hacking-behavior-can-generalize-across-tasks)). Prominent references to these kinds of works and a differentiation from them seems fundamental to put this paper and its contributions in context. There is also almost no reference to prior work on reward tampering (e.g. work from Tom Everitt among others).\n\n**Novelty of framing.** The behaviors studies fall within established reward hacking/specification gaming, and it's unclear to me whether they are deserving of a separate category called \"capability-driven alignment risk\" – given that the boundary between them and standard reward hacking is unclear (e.g. \"exploit[ing] implicit loopholes in the environment’s dynamics\" is essentially equivalent to exploiting loopholes in reward functions). Even just looking at Table 1, all such behaviors have previously been simply thought of as imperfect reward mechanisms that lend themselves to reward hacking. All reward hacking is based on context-dependent strategies that exploit imperfections in the reward functions whenever possible, so it's unclear to me whether the \"context-dependence\" of situational awareness is a differentiating factor either. Overall, reading e.g. the intro, one comes away feeling like the paper is trying to reframe many already established concepts in a novel way which does not ultimately feel particularly illuminating.\n\nAlso, while RQ1 is phrased in the intro as ~\"does hacking emerge spontaneously\", in the later section it's phrased differently, as \"how does it emerge?\", which is meaningfully different.\n\nThe paper's style is written in ways that are sometimes somewhat confusing: \n* Table 3 is not referenced in the text, and it's unclear if the models were trained with GRPO on the rows\n* Table 4 description is somewhat confusing with regards to whether these are just numbers from the \"students\""}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ADd2XDU1FE", "forum": "BQfRA3tqt9", "replyto": "BQfRA3tqt9", "signatures": ["ICLR.cc/2026/Conference/Submission23586/Reviewer_C1WF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23586/Reviewer_C1WF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142059820, "cdate": 1762142059820, "tmdate": 1762942722278, "mdate": 1762942722278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}