{"id": "EOV1q1U23N", "number": 21871, "cdate": 1758322916484, "mdate": 1763388545945, "content": {"title": "Convergence of Regret Matching in Potential Games and Constrained Optimization", "abstract": "Regret matching (RM)---and its modern variants---is a foundational online algorithm that has been at the heart of many AI breakthrough results in solving benchmark zero-sum games, such as poker. Yet, surprisingly little is known so far in theory about its convergence beyond two-player zero-sum games. For example, whether regret matching converges to Nash equilibria in potential games has been an open problem for two decades. Even beyond games, one could try to use RM variants for general constrained optimization problems. Recent empirical evidence suggests that they---particularly regret matching$^+$ (RM$^+$)---attain strong performance on benchmark constrained optimization problems, outperforming traditional gradient descent-type algorithms.\n\nWe show that alternating RM$^+$ converges to an $\\epsilon$-KKT point after $O_\\epsilon(1/\\epsilon^4)$ iterations, establishing for the first time that it is a sound and fast first-order optimizer. Our argument relates the KKT gap to the accumulated regret, two quantities that are entirely disparate in general but interact in an intriguing way in our setting, so much so that when regrets are bounded, our complexity bound improves all the way to $O_\\epsilon(1/\\epsilon^2)$. From a technical standpoint, while RM$^+$ does not have the usual one-step improvement property in general, we show that it does in a certain region that the algorithm will quickly reach and remain in thereafter. In sharp contrast, our second main result establishes a lower bound: RM, with or without alternation, can take an exponential number of iterations to reach a crude approximate solution even in two-player potential games. This represents the first worst-case separation between RM and RM$^+$. Our lower bound shows that convergence to coarse correlated equilibria in potential games is exponentially faster than convergence to Nash equilibria.", "tldr": "We show that regret matching+ converges quickly to approximate KKT points in constrained optimization while regret matching can take exponential time.", "keywords": ["regret matching", "no-regret learning", "potential games", "constrained optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/72a5bbf67ac966613bb454dead2cfbc4af777cf4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies regret matching variants, particularly alternating RM and RM+ and explores the convergence of these algorithms to first order optima in constrained optimization problems and potential games. For computing approximate KKT points, the authors show that alternating RM+ converges after $O(1/\\epsilon^4)$ iterations,  though a $O(1/\\epsilon^2)$ bound can be obtained when the regrets remain bounded. This leverages a novel connection between regret minimization and KKT gaps. Using this result, it is also shown that simultaneous RM+ with symmetric initializations converge to approximate NE with the same complexity as given above. Next, RM is investigated and shown to possess exponential lower bounds, alternation or not. This shows a stark separation between RM and RM+, despite their similarity in terms of the update rule."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The results of this paper are strong, and resolve some open problems in the literature. The separation between RM and RM+ in terms of convergence rate is the first result of its kind.\n- The connection between KKT gap and total regret is to my knowledge novel in this setting and results in an intriguing algorithmic consequence of regret accumulation. Future work exploiting this connection would be very interesting indeed.\n-  The introductory sections are very clear and readable, even if the technical statements are relatively dense. This makes understanding the ideas of the paper much easier, particularly for readers who are not familiar with the material."}, "weaknesses": {"value": "- While this is not the main point of the paper, perhaps more background and comparison between the convergence of RM+ and other online learning methods like FTRL/Mirror Descent can be added to the main text. This would help readers understand what is known about online learning in potential games, and why the result for alternating RM+ is important. For example, while MWU can exhibit chaotic behavior in potential games, while it seems that alternating RM+ avoids this.\n- The reliance on alternation seems to dissuade the practical use-case of learning in a decentralized, simultaneous environment (e.g. where players do not get to observe the strategies used by other players). Having some more results on the simultaneous case would greatly improve the paper. While it is clear why Lemma 3.3 leads to the alternating convergence result, the paper could do with more discussion on where the proof idea fails for the simultaneous case, or if there are further technical problems that arise."}, "questions": {"value": "- Have the authors investigated predictive/smooth predictive RM+ in potential games? Would the $O(1/\\epsilon^4)$ convergence remain in this case, or would there be some acceleration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PrlxNIiU2g", "forum": "EOV1q1U23N", "replyto": "EOV1q1U23N", "signatures": ["ICLR.cc/2026/Conference/Submission21871/Reviewer_GY2W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21871/Reviewer_GY2W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761419969874, "cdate": 1761419969874, "tmdate": 1762941964601, "mdate": 1762941964601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Revision summary"}, "comment": {"value": "We are grateful to all reviewers for their service and helpful feedback. We have posted a revision that addresses the main comments. Below we briefly explain the key additions in the revision; the individual points and questions of each reviewer are addressed as separate responses.\n\nFirst, all reviewers pointed out that extending the convergence bounds to simultaneous RM+ would make the paper stronger. We have indeed observed that, using our current techniques, similar convergence bounds can be shown for simultaneous RM+ as well. In particular, we have added two new results in the revision. First, when the regrets can be initialized at a certain constant, simultaneous RM+ converges at the same rate of $O_\\epsilon(1/\\epsilon^4)$ (Corollary C.11). This follows by directly extending Lemma 3.7 to the case of multiple simplices (Lemma C.10). Second, even when the regrets are initialized at zero, simultaneous RM+ still converges, albeit at a slower rate of $O_\\epsilon(1/\\epsilon^8)$ (Theorem 3.12); this is a direct consequence of Lemma 3.8. Both of these results are direct implications of the tools present in our submission, which is why we wanted to include them in our revision. We have also adjusted our introduction to point out this extension. We hope this addresses the questions of the reviewers concerning simultaneous RM+. \n\nWe also spotted a missing precondition in an earlier theorem statement concerning the initialization of the regrets, which has been corrected in the revision (Corollary 3.11); this doesn’t affect our results.\n\nIn terms of the writing, we expanded on the related work section to discuss in more detail prior work concerning the convergence of no-regret algorithms in potential games (Appendix A), and expanded the future research section to highlight the stochastic/bandit setting as a natural next step. We also corrected a typo spotted by a reviewer.\n\nFinally, concerning the lower bound, we added a figure (Appendix C.2) to help convey the intuition of the construction. For completeness, our revision also shows that the same lower bound applies even if the players initialize from the uniform random strategy, which is common in practice (last paragraph in Appendix C.2)."}}, "id": "uPJXruzAj2", "forum": "EOV1q1U23N", "replyto": "EOV1q1U23N", "signatures": ["ICLR.cc/2026/Conference/Submission21871/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21871/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21871/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763388964490, "cdate": 1763388964490, "tmdate": 1763388964490, "mdate": 1763388964490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the convergence properties of Regret Matching (RM) and its popular variant, Regret Matching+ (RM+), in the context of potential games and, more generally, constrained optimization over a product of simplices. The authors provide two main, contrasting results. First, they establish for the first time that an alternating version of RM+ converges to an approximate KKT point in polynomial time, with a rate of O(1/ε⁴), confirming it is a valid first-order optimization algorithm. Second, they show that standard RM, with or without alternation, can require an exponential number of iterations to converge to even a coarse approximate Nash equilibrium in a simple two-player potential game. This provides the first exponential separation between the performance of RM and RM+, offering strong theoretical justification for the empirical preference for RM+ in practice."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.  The paper tackles a long-standing open problem about the convergence of regret matching in potential games. Establishing the first polynomial-time convergence guarantee for alternating RM+ in this general setting is a major theoretical breakthrough.\n2.  The exponential lower bound for standard RM is a very strong result. It clearly and definitively separates RM from RM+, explaining the performance gap often observed empirically. This separation in potential games is novel and fundamentally different from prior work focused on zero-sum games.\n3.  The paper is very well-written and structured. The introduction does an excellent job of motivating the problem, and the \"Our Results\" section clearly lays out the key contributions and their implications. The main theorems are stated precisely, and the narrative guides the reader effectively through the technical arguments.\n4.  The connection established between the KKT gap and the accumulated regret is intriguing and provides a novel analytical tool. The proof technique for the RM+ convergence, which handles the lack of a standard one-step improvement property, is clever."}, "weaknesses": {"value": "1.  The proven convergence rate for alternating RM+ is O(1/ε⁴), which is quite slow compared to the O(1/ε²) rate often achievable with standard gradient-based methods for non-convex optimization. While being the first polynomial bound is a great achievement, it would be beneficial to discuss whether this rate is believed to be tight or if it is an artifact of the current analysis.\n2. The positive convergence result for simultaneous RM+ is restricted to symmetric potential games under a symmetric initialization. This is a fairly strong assumption and leaves the more general and arguably more practical case of simultaneous updates in arbitrary potential games as an open question.\n3.  The paper is entirely theoretical. While the introduction cites recent work showing the strong empirical performance of RM+, the paper would be strengthened by including even a small experiment."}, "questions": {"value": "1.  How do you reconcile the strong empirical performance of RM+ (sometimes outperforming gradient descent, as mentioned in the introduction) with the relatively slow O(1/ε⁴) theoretical convergence rate you establish? Does this suggest that the worst-case instances for RM+ are rare in practice?\n2.  What are the primary technical hurdles to extending your convergence proof for simultaneous RM+ from symmetric games to general potential games? Do you suspect that simultaneous RM+ might fail to converge in the general case, or is the analysis just significantly more challenging?\n3.  Could you provide some higher-level intuition for why the seemingly minor modification of truncating negative regrets (the key difference between RM+ and RM) prevents the algorithm from getting stalled for an exponential amount of time? The paper proves this formally, but an intuitive explanation would be very helpful.\n4.  In Theorem 3.4, the convergence rate for alternating RM+ is parameterized by the growth of the regret. Can you comment on what typical or worst-case regret growth one might expect for RM+ in potential games?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U5WHUqX1Xs", "forum": "EOV1q1U23N", "replyto": "EOV1q1U23N", "signatures": ["ICLR.cc/2026/Conference/Submission21871/Reviewer_bT7s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21871/Reviewer_bT7s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632338457, "cdate": 1761632338457, "tmdate": 1762941964084, "mdate": 1762941964084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the behavior of Regret Matching variants in potential games. The two main results are to establish that:\n1. Alternating Regret Matching+ (RM+) has last-iterate convergence to Nash at a T^{-1/4} rate.\n2. Vanilla Regret Matching (RM), with or without alternation, has exponentially slow last-iterate convergence on a certain identical interest game. \n\nTo prove (1), the authors more generally show that alternating RM+ converges to approximate KKT points of (nonconvex) optimization problems over a product of simplices, and they also establish a similar rate of convergence to Nash in *symmetric* potential games under simultaneous RM+."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Overall this is a nice work that offers new characterization of the last-iterate convergence properties of regret matching variants beyond zero-sum games. The positive convergence results for (alternating) RM+ for potential games is in contrast to non-convergence results of these algorithms in certain zero-sum games (Lee+2021 and Cai+2025). Establishing the connection between regret and KKT gap is also interesting. In general the paper is well-written and easy to follow. \n\n[Lee+2021]: \"Last-iterate Convergence in Extensive-Form Games\", NeurIPS 2021\n\n[Cai+2025]: \"Last-Iterate Convergence Properties of Regret-Matching Algorithms in Games\", ICLR 2025"}, "weaknesses": {"value": "There are few areas that could improve the presentation: for example, some additional explanations and fixing or clarifying several incorrect statements (see these listed below in Questions)."}, "questions": {"value": "High-level suggestions:\n+ It would be helpful to include some additional related work and discussion on the last-iterate convergence behavior of other families of learning algorithms in potential games (e.g., multiplicative weights / FTRL). See for example [Palaiopanos+2017, Cheung+2020, Anagnostides+2022]. \n\nHigh-level questions:\n+ Can you offer explanation as to where the proof of Theorem 3.4 fails when considering *simultaneous* RM+? Does the one-step improvement property of Lemma 3.3. still hold? In other words, is alternation necessary for Theorem 3.4 to hold?\n\n+ Similarly, for Corollary 3.10, are symmetric initializations necessary for the result to hold?\n\n+ Do you suspect faster convergence rates are achievable using other RM+ variants like predictive RM+, with or without alternation? \n\nLow-level questions/clarifications:\n+ In Proposition 3.1, the left-hand side of the final inequality seems like it should involve the function value of the average-iterate: e.g., $u(\\bar x^T) \\ge \\max_{x} u(x) - R(T)/T$ where $\\bar x^T = \\sum_{t=1}^T x^t/T$. Similarly, Line 265 should be adjusted to make it clear that, for concave u, vanishing average regret implies convergence of the average-iterate (ergodic convergence), and not last-iterate convergence as is currently written. \n\n+ L338: Do you mean to say that it is open whether RM+ can have $o(\\sqrt{T})$ regret in potential games? (we already know RM/RM+ has $O(\\sqrt{Tm})$ regret in general). \n\nOther typos:\n+ L265: extra superscript parenthesis in $u(x^t)$. \n+ L327: seems like bound should read $\\le m \\sqrt{t}$ (lower-case $t$, not $T$). \n\n----- \n\n[Palaiopanos+2017]: \"Multiplicative Weights Update with Constant Step-Size in Congestion Games: Convergence, Limit Cycles and Chaos\", NeurIPS 2017.\n\n[Cheung+2020]: \"Chaos, Extremism and Optimism: Volume Analysis of Learning in Games\", NeurIPS 2020.\n\n[Anagnostides+2022]: \"On Last-Iterate Convergence Beyond Zero-Sum Games\", ICML 2022."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C7v0rYwkpj", "forum": "EOV1q1U23N", "replyto": "EOV1q1U23N", "signatures": ["ICLR.cc/2026/Conference/Submission21871/Reviewer_Zjs4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21871/Reviewer_Zjs4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706566231, "cdate": 1761706566231, "tmdate": 1762941963754, "mdate": 1762941963754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the convergence behavior of regret matching (RM) and regret matching+ (RM+) in potential games and general smooth objectives over products of simplices. It proves that alternating RM+ converges to an $\\epsilon$-KKT point within $O(1/\\epsilon^4)$ iterations and achieves faster rates when per-simplex regret grows slower than $\\sqrt{T}$. The work establishes a formal link between accumulated regret and stationarity, showing how no-regret learning can yield approximate equilibrium in non-zero-sum settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides the first general-purpose guarantees for RM+ beyond zero-sum settings. The core technical contribution is a careful connection between KKT gap and accumulated regret, which enables non-asymptotic convergence rates for alternating RM+ in nonconvex constrained optimization\n\n- The algorithmic scope is broad: the results cover any smooth objective over a product of simplices, with potential games as a key special case. Proving $O(1/\\epsilon^4)$  convergence to $\\epsilon$-KKT for alternating RM+ is a strong and clean statement\n\n- The lower-bound side demonstrates that plain RM can be exponentially slower than RM+ in potential games explains persistent empirical gaps between the two"}, "weaknesses": {"value": "- While the $O(1/\\epsilon^4)$   guarantee is valuable, it may still be pessimistic relative to practice. The analysis leaves the leading constants implicit and depends on problem parameters (e.g., action-set size, smoothness), which could be large in realistic instance\n\n- The alternating-updates requirement is algorithmically meaningful but differs from the fully simultaneous updates common in large-scale systems\n\n- The theoretical development assumes full-information gradients of a smooth objective on simplices and a normalization of payoff ranges. These are natural for analysis but leave open how the guarantees translate to bandit/partial information, stochastic gradients, or imperfect-recall sampling regimes"}, "questions": {"value": "Can you expose the hidden constants in your $O(1/\\epsilon^4)$  rate for alternating RM+ and show their dependence on the number of players/actions, Lipschitz constants, and potential range?\n\nBeyond the symmetric setting, how far do your techniques extend to simultaneous RM+? Are there counterexamples where simultaneous RM+ fails to approach $\\epsilon$-KKT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sKreH97YCc", "forum": "EOV1q1U23N", "replyto": "EOV1q1U23N", "signatures": ["ICLR.cc/2026/Conference/Submission21871/Reviewer_QJyC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21871/Reviewer_QJyC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762114476462, "cdate": 1762114476462, "tmdate": 1762941963510, "mdate": 1762941963510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}