{"id": "ilhWL2WPvS", "number": 18229, "cdate": 1758285422818, "mdate": 1759897117835, "content": {"title": "High-Dimensional Online Change Point Detection with Adaptive Thresholding and Interpretability", "abstract": "Change point detection (CPD) identifies abrupt and significant changes in sequential data, with applications in human activity recognition, financial markets, cybersecurity, manufacturing, and autonomous systems. While traditional methods often struggle with the computational demands of high-dimensional data, they also fail to provide explanations for detected change points, limiting their practical usability. This paper introduces a CPD framework that enhances both interpretability and scalability by leveraging the Sliced Wasserstein (SW) distance. Our contributions are fourfold: (1) we present a method to transform multivariate data into one-dimensional time series using the SW distance, enabling compatibility with existing CPD methods; (2) we derive theoretical insights, demonstrating that random slices of the SW distance follow a Gamma distribution, which facilitates statistical hypothesis testing for CPD; (3) we propose a novel self-adapting online CPD algorithm based on an adaptive threshold for a given significance level $\\alpha$; and (4) we propose a model-specific framework for generating contrastive explanations for annotated change points. We find that our method outperforms popular (online/offline) change point detection methods by reducing the number of false positives by at least 63% while also providing interpretable change points and maintaining competitive or superior detection performance, making it practical for deployment in high-stakes applications.", "tldr": "We propose SWCPD, a robust and interpretable online change point detection framework that leverages random projections, enabling adaptive thresholding, and contrastive explanations for high-dimensional data streams.", "keywords": ["Interpretability", "Explainability", "High-dimensional data", "Contrastive Explanations"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b9dfc8c222c163e943f43461eb5e6a241e6f5b75.pdf", "supplementary_material": "/attachment/e589fd62e04adea4af0d011b9f6c8557d0f4d55d.zip"}, "replies": [{"content": {"summary": {"value": "The authors consider the problem of nonparametric online change point detection. They suggest an algorithm based on sliced Wasserstein distance. Because of the reduction to one-dimensional projections, the authors significantly reduce computational expenses. Moreover, they establish (see Theorem 3.1) that the Wasserstein distance between random projections tends to the gamma distribution as the dimension increases. This suggest an approach for automatic threshold choice based on asymptotic distribution of the sliced Wasserstein distance. The authors illustrate the performance of their algorithm on synthetic and real-world data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Theorem 3.1 about asymptotic distribution of the sliced Wasserstein distance (as $d \\rightarrow \\infty$). allows for adaptive choice of the threshold."}, "weaknesses": {"value": "1. While Proposition 3.2 gives an intuition about running length of the detection procedure, its detection delay remains unexplored. This leaves open the question of optimality of the suggested approach.\n\n2. Some important references are missing. A variant of CUSUM was also studied in [Yu et al., 2023]. Methods based on different f-divergences and density-ratio estimation were considered in [Liu et al., 2013] and [Hushchyn, A. Ustyuzhanin, 2021]. Their change-point detection methods are based on KLIEP [Sugiyama, 2008], uLSIF [Kanamori et al., 2009], and RuLSIF [Yamada et al., 2013]. Approaches based on deep neural networks (with a statistics similar to the one used in GANs) were suggested in [Hushchyn et al., 2020], [Puchkin, Shcherbakova, 2023]. Finally, in [Cao et al., 2018] and [Markovich, Puchkin, 2024] the authors proposed algorithms based on online convex optimization and prediction with expert advice. I would suggest the authors to incorporate these references into the literature review during revision.\n\n[Sugiyama, 2008] M. Sugiyama, T. Suzuki, S. Nakajima, H. Kashima, P. von Bunau, and M. Kawanabe. Direct importance estimation for covariate shift adaptation. Annals of the Institute of Statistical Mathematics, 60(4):699–\n746, 2008.\n\n[Kanamori et al., 2009] T. Kanamori, S. Hido, and M. Sugiyama. A least-squares approach to direct importance estimation. Journal of Machine Learning Research, 10:1391–1445, 2009.\n\n[Yamada et al., 2013] M. Yamada, T. Suzuki, T. Kanamori, H. Hachiya, and M. Sugiyama. Relative density-ratio estimation for robust distribution comparison. Neural Computation, 25(5):1324–1370, 2013.\n\n[Liu et al., 2013] S. Liu, M. Yamada, N. Collier, and M. Sugiyama. Change-point detection in time-series data by relative density-ratio estimation. Neural Networks, 43:72–83, 2013.\n\n[Cao et al., 2018] Y. Cao, L. Xie, Y. Xie, and H. Xu. Sequential change-point detection via online convex optimization. Entropy, 20(2):108, 2018.\n\n[Hushchyn et al., 2020] M. Hushchyn, K. Arzymatov, and D. Derkach. Online neural networks for change-point detection. Preprint, arXiv:2010.01388, 2020.\n\n[Hushchyn, A. Ustyuzhanin, 2021] M. Hushchyn and A. Ustyuzhanin. Generalization of change-point detection in time series data based on direct density ratio estimation. J. Comput. Sci., 53:Paper No. 101385, 8, 2021.\n\n[Puchkin, Shcherbakova, 2023] N. Puchkin and V. Shcherbakova. A Contrastive Approach to Online Change Point Detection. In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206 of Proceedings of Machine Learning Research, pages 5686–5713, 2023.\n\n[Yu et al., 2023] Y. Yu, O. H. M. Padilla, D. Wang, and A. Rinaldo. A note on online change point detection. Sequential Analysis, 42(4):438–471, 2023.\n\n[Markovich, Puchkin, 2024] Score-based change point detection via tracking the best of infinitely many experts. Preprint, arXiv:2408.14073, 2024."}, "questions": {"value": "1. According to my experience, the threshold choice based on asymptotic distribution of the test statistic may be not the best one (this agrees with one of limitations you mentioned in Section 5). For instance, the automatic threshold choice for the kernel-based test statistic [Li et al., Seq. Anal., 2019] lead to frequent false alarms. Has a similar problem occurred, for instance, on the Room Occupancy data (where the time series has only 4 components)? Could you elaborate on how you chose a threshold in that case?\n\n2. Could you add a comparison of your algorithm with any of [Liu et al., 2013], [Hushchyn et al., 2020], [Markovich, Puchkin, 2024]? All these papers considered the human activity recognition data set. Besides, [Hushchyn et al., 2020] checked performance of their procedure on the MNIST data set, while [Markovich, Puchkin, 2024] applied their approach to the Room Occupancy data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YQYWP8qPXB", "forum": "ilhWL2WPvS", "replyto": "ilhWL2WPvS", "signatures": ["ICLR.cc/2026/Conference/Submission18229/Reviewer_EQ8h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18229/Reviewer_EQ8h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959474478, "cdate": 1761959474478, "tmdate": 1762927967651, "mdate": 1762927967651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Sliced Wasserstein Change Point Detection (SWCPD), a novel framework for change point detection (CPD) in high-dimensional data.\n \nThe cornerstone of this framework is a new insight: the theoretical result showing that random Sliced Wasserstein slices follow a Gamma distribution for adaptive thresholding. This finding is leveraged by the authors practically for CPD. Multivariate data is transformed into univariate signals using Sliced Wasserstein (SW) distance and then a hierarchical procedure for generating contrastive explanations of detected changes is implemented.\n \nThe method is evaluated on synthetic and real-world datasets (MNIST, Human Activity Recognition, Occupancy) and compared against established baselines (BOCPD, e-divisive, KCP, OT-CPD, ClaSP)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel theoretical characterization connecting SW projections to Gamma distributions, enabling principled statistical hypothesis testing for CPD.\n\n- First method to combine Sliced Wasserstein distance with adaptive thresholding and contrastive explanations in a unified framework.\n \n- Theorem 3.1, Propositions 3.2, and supporting lemmas are seemingly sound mathematical foundations\n\n- Comprehensive experimental evaluation including synthetic and real datasets, with ablation studies and parameter sensitivity analysis\n\n- Reproducible methodology with clearly described experimental settings and baselines.\n- Strong empirical results, particularly in reducing false positives\n \n- Figures and tables generally well-designed; boxplots in Figure 10 effectively communicate parameter sensitivity.\n \n- Relevant to high-stakes domains (finance, healthcare, autonomous systems) where both accuracy and interpretability are critical.\n\n- SWCPD has computational efficiency, which is meaningful in practice for streaming applications."}, "weaknesses": {"value": "Theoretical Limitations\n- Assumption Gap (in Section 3.1): Theorem 3.1 requires d → ∞ for the Gamma distribution approximation, but practical datasets have moderate dimensions. While Table 5 shows empirical validation works for d ≥ 20, the paper would benefit from:\n- Providing finite-sample bounds or approximation error rates for moderate d\n- Specifying minimum dimension requirements for reliable performance\n- Discussing behavior when theoretical assumptions are violated\n- Temporal Correlation (in Section 3.2): The moving average smoothing (step 2 of the algorithm) applies to temporally correlated sliding windows, yet the paper assumes i.i.d. random projections for statistical validity. The paper acknowledges this, but dismisses it without rigorous justification.\n- Method of Moments Limitations: MoM estimation is sensitive to outliers and heavy-tailed distributions. \n\nThe paper should:\n- Compare MoM to maximum likelihood estimation empirically\n- Analyze robustness to distribution misspecification\n- Provide diagnostic criteria for when the Gamma approximation fails\n\nHyperparameter Selection (Section 4.2):\n- Tolerance criterion (Algorithm 2, line 3): The stopping criterion β̂ ≤ tol lacks justification. What is an appropriate tolerance value? How sensitive are explanations to this choice?\n- Window length w: Figure 3 shows high sensitivity to w, but no principled selection strategy is provided beyond grid search. Next steps should develop data-driven selection methods (e.g., cross-validation, information criteria).\n- Wasserstein order p: The choice between p=2,4,6 appears dataset-dependent (Table 2, Figure 3) with no clear guidance. Provide decision rules based on drift characteristics.\n\nExperimental Concerns\n- Limited Dataset Diversity: Four datasets (two relatively simple: MNIST, Occupancy; one tied: HAR with 0.59 e-divisive).\n- Missing challenging scenarios: high-noise environments, gradual drifts, multimodal distributions, very high dimensions (d > 100).\n- Financial time series and sensor data mentioned in introduction but not evaluated.\n\nStatistical Rigor:\n- Confidence intervals on metrics would strengthen claims (only AUC has std in Table 3).\n- No runtime comparisons with non-OT baselines (Table 8-9 only compare OT methods).\nWith L=5000 projections, practical speed claims need verification for streaming applications with strict latency requirements."}, "questions": {"value": "To strengthen the background around Wasserstein-based change detection, it would be helpful to frame the discussion in consideration of prior OT/WD approaches, including WATCH and its lifelong extension (LIFEWATCH), which perform unsupervised CPD by monitoring Wasserstein shifts in high-dimensional streams. Beyond these, Cheng et al. (ICASSP’20) proposed an optimal transport CPD and segment-clustering pipeline grounded in the Wasserstein two-sample test. From a statistical perspective, Horváth et al. (Annals of Statistics, 2021) analyze sequential monitoring via a weighted Wasserstein distance with asymptotic and finite-sample guarantees, which I believe is a useful context for your hypothesis-testing claims. \n\nCan you provide finite-sample error bounds or convergence rates for the Gamma approximation in Theorem 3.1? Specifically, what is the approximation error for moderate dimensions (d = 10-50) commonly seen in practice?\n\nWhat is the practical minimum dimension d for reliable performance? Table 5 shows empirical validation, but can you provide theoretical guidance or diagnostic criteria for when the method should or shouldn't be applied?\n\nYou acknowledge temporal correlation (in line 218) but dismiss concerns without rigorous justification. Can you provide theoretical analysis or simulation studies quantifying the impact on confidence interval validity? \n\nCan you show empirical evidence of robustness across different correlation structures? Can you provide guidelines for adjusting parameters when correlation is strong?\n\nHow should practitioners set the tolerance parameter “tol” in Algorithm 2? Can you provide a sensitivity analysis or specify the selection criteria?\n\nSeveral improvements appear modest given the reported uncertainty. For example, HAR shows 0.85 ± 0.12 (yours) vs. 0.84 ± 0.15 (ClaSP), and Occupancy shows 0.59 vs. 0.52-0.58 (baselines). Can you provide statistical significance tests (such as paired t-tests or Wilcoxon tests) to confirm these improvements are not due to random variation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FL2AYLeY89", "forum": "ilhWL2WPvS", "replyto": "ilhWL2WPvS", "signatures": ["ICLR.cc/2026/Conference/Submission18229/Reviewer_7mTU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18229/Reviewer_7mTU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970562325, "cdate": 1761970562325, "tmdate": 1762927967309, "mdate": 1762927967309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors present a framework for online change point detection in high-dimensional data with Sliced Wasserstein distance. The idea is to transform multivariate data into one-dimensional projections, derives that random SW slices follow a Gamma distribution for hypothesis testing. An adaptive thresholding algorithm was proposed based on a significance level, and aimed to offer contrastive explanations via geometric properties of projections."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "> The paper addresses scalability and explainability in high-dimensional CPD, which is a growing concern in sequential data analysis.\n\n> It includes a derivation showing SW slices approximate a Gamma distribution, enabling some statistical testing.\n\n> Experiments on synthetic and a few real datasets show minor reductions in false positives in certain setups.\n\n> The adaptive threshold and projection-based explanations offer a straightforward way to add interpretability."}, "weaknesses": {"value": "- While the paper evaluates on time series (e.g., HAR) and image-based streams (e.g., MNIST), it fails to compare against state-of-the-art CPD methods tailored to each modality. For time series, it misses deep learning-based SOTA like LSTM/Transformer CPD (e.g., recent kernel methods for multivariate series), relying instead on general baselines like CUSUM or OT-CPD. For images (MNIST as sequential data), it ignores computer vision-specific CPD techniques, such as video anomaly detection models (e.g., autoencoders or flow-based methods for change detection in image sequences), limiting claims of superiority.\n  - A general methodology for fast online changepoint detection\n  - DeepLocalization: Using change point detection for Temporal Action Localization\n  - Adaptive Block-Based Change-Point Detection for Sparse Spatially Clustered Data with Applications in Remote Sensing Imaging\n\n- Only a handful of datasets are used (synthetic, HAR, MNIST, Occupancy), with small sequences (e.g., 200 samples per class in MNIST). No broader testing on diverse high-stakes domains like finance (e.g., stock data) or cybersecurity (e.g., network traffic benchmarks), despite mentioned applications. Results may not generalize.\n\n- Relies on i.i.d. assumptions in windows, which may not hold for correlated time series or image streams. Adaptive thresholding requires tuning (e.g., α, window size), and computational costs for many projections (L=5000) aren't quantified for real-time online use.\nUnderdeveloped Interpretability: Contrastive explanations via projections are basic; no user studies or quantitative metrics (e.g., fidelity) to validate if they truly aid understanding in practice.\n\n- Theoretical insights (Gamma approximation) are asymptotic (d→∞), potentially weak for moderate dimensions. Overall, incremental over existing SW/OT-based CPD (e.g., Cheng et al., 2020b), without strong novelty."}, "questions": {"value": ">> The paper lacks comparisons to modality-specific CPD SOTA. Could the authors explain the reason? I understand it has used HAR (time series) and MNIST (image streams), but baselines are general (e.g., CUSUM, OT-CPD). For time series, why not include deep CPD methods like Transformer-based detectors (e.g., from recent works on multivariate series)? For images, why omit CV-specific SOTA like optical flow or autoencoder-based video CPD? This omission weakens claims of \"competitive or superior performance.\"\n\n>> How generalizable are results beyond the limited datasets? Experiments focus on synthetic data, HAR, MNIST (as streams), and Occupancy, with small scales (e.g., 200 samples/class). Have you tested on larger, domain-specific benchmarks like financial time series (e.g., S&P 500 drifts) or cybersecurity traces (e.g., KDD Cup anomalies)? Without this, applicability to \"high-stakes\" fields seems overstated.\n\n>> I wonder about the robust of the Gamma approximation in practice. Theorem 3.1 assumes d→$\\infty$ for SW slices ~ Gamma, but datasets like HAR (d~561) or MNIST (d=784) are finite. Appendix shows p-values, but could you provide ablation on approximation error for lower d, or when distributions violate assumptions (e.g., non-Gaussian)? Moreover, the experiment is not comprehensive. It doesn't consider how the adaptive thresholding is sensitive to parameter choices. Algorithm depends on α, window size, lookback Kmax. Appendix grid search on MNIST/Occupancy shows variations, but no sensitivity analysis across all datasets. For low-drift synthetics, how does it avoid false positives/negatives compared to fixed thresholds in OT-CPD?\n\n>> The discussion is not thorough. How about the computational trade-offs for online deployment? With L=5000 projections and window sizes w=50, online CPD might be slow for streaming data. Grid searches vary α, w, Kmax, but no runtime benchmarks vs. baselines. How does it scale for real-time apps (e.g., autonomous systems), and why no efficiency metrics?\n\n>> I am confused about the validation of the interpretability of explanations? Projection-based contrastive explanations identify \"discriminative features,\" but lack quantitative evaluation (e.g., explanation accuracy via perturbation tests) or qualitative user studies. On HAR/MNIST, do they align with domain knowledge (e.g., sensor features in activity shifts)? Could you add metrics like faithfulness? What about correlated data or non-i.i.d. windows? Windows assume independence, but time series (HAR) and image streams (MNIST) often have temporal correlations. This could bias SW distances or MoM estimates. Could you discuss mitigations, like decorrelation steps, or test on autocorrelated synthetics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WaGxLU3MHb", "forum": "ilhWL2WPvS", "replyto": "ilhWL2WPvS", "signatures": ["ICLR.cc/2026/Conference/Submission18229/Reviewer_rWAP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18229/Reviewer_rWAP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989551177, "cdate": 1761989551177, "tmdate": 1762927966711, "mdate": 1762927966711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel method for explainable online change-point detection in high-dimensional data using the sliced Wasserstein distance. By transforming a multivariate time series into a one-dimensional signal, the method both detects change points and identifies which univariate series contribute most to the change. The detector uses a self-adaptive threshold, updating it for each window."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles an interesting practical task: detecting change points and identifying which components of the time series contribute to those changes.\n\n2. It provides theoretical guarantees for the distribution of the Sliced Wasserstein distance.\n\n3. Visualizations and experiments on synthetic and real-world datasets demonstrate the method’s interpretability and change-detection ability, compared with multiple baselines.\n\n4. The paper includes an ablation study with various parameter settings and reports runtime across different time-series lengths."}, "weaknesses": {"value": "1. Although the paper targets high-dimensional data, the datasets in the experiments are relatively low-dimensional. The largest dimension is 50 in the change point task with synthetic datasets. It would be beneficial to include higher-dimensional settings in the explainability section as well. In addition, the runtime as a function of the dimension $d$ is not reported.\n2. The main paper claims that the Sliced Wasserstein distance follows a Gamma distribution, but the theory is proved only for $p=2$.\n3. In the synthetic-data explainability section, interpretability is shown only for Gaussian mean-shift cases. It would be more complete to include variance shifts and other distributions. In the synthetic change-point detection section, there is no baseline comparison.\n\nSome minor comments:\n1. Some notation appears before it is introduced. For example, $\\delta$ in line 233, $K_{max}$ on line 249.\n2. The notation of $p$ and $q$ is sometimes confusing and used interchangeably. For example, on lines 167 and 259. $p$ should denote the Wasserstein order, and $q$ the confidence level or quantile?"}, "questions": {"value": "1. In the experiments, what tolerance is used to terminate Algorithm 2 at line 3?\n2. The number of change points is relatively small compared with the data dimension and the series length. What happens if you simulate many change points and shift a larger subset of series, for example, more than 2 change points across more than 3 series when $d=50$? \n3. Is it challenging to provide a theoretical guarantee for change-point detection? For example, can the current theory be extended to quantify the uncertainty of correctly localizing the change point?\n4. Is the dimensionality of the HAR dataset used in the experiments 250? What does “selected 25 instances” mean?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uSqRwqkhot", "forum": "ilhWL2WPvS", "replyto": "ilhWL2WPvS", "signatures": ["ICLR.cc/2026/Conference/Submission18229/Reviewer_6jLv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18229/Reviewer_6jLv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762212487005, "cdate": 1762212487005, "tmdate": 1762927966317, "mdate": 1762927966317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}