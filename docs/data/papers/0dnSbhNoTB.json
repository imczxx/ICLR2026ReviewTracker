{"id": "0dnSbhNoTB", "number": 4932, "cdate": 1757812169610, "mdate": 1759898004420, "content": {"title": "Where is Motion From? Scalable Motion Attribution for Video Generation Models", "abstract": "Despite the rapid progress of video generative models, the role of data in shaping motion quality is poorly understood. We present MOTIVE (MOtion Training Influence for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which finetuning clips improve or degrade temporal dynamics. MOTIVE isolates temporal dynamics from static appearance via flow-weighted loss masks, yielding scalable influence scores practical for modern, large, and high-quality datasets and models. On text-to-video models, MOTIVE identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With MOTIVE selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 76.7% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework that attributes motion (not just appearance) in video generative models and uses it to curate finetuning data.", "tldr": "Our method, MOTIVE, is a scalable, motion-centric data attribution framework for video generative models.", "keywords": ["Data attribution", "Video generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ef04b706dfdf118768356193b6fedc62556c55cc.pdf", "supplementary_material": "/attachment/5c10ced4648d636af9099bfb7bcb9e4cddf28b3a.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes selecting a subset of motion-influential data for training video generative models. The goal is to choose data that strongly affects motion and to guide data curation in a way that improves temporal consistency and physical plausibility. A computational method is used to compare the gradients of query and training data points for this purpose."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n\n2. Both computational experiments and human studies are provided to support the claims."}, "weaknesses": {"value": "**Scalability concern:**  In the experiment, there are 50 query videos and 10k clips in the training subset. Adding just one more query requires computing influence scores 10k additional times. How to address this limit?\n\n**Equation (5) issue:** How can the exact cardinality of the sampled timestep-and-noise set \\\\( \\\\mathcal{T} \\\\) be computed? There are infinitely many choices for the noise vector and time step in the flow matching framework. Shouldn’t it be better represented as an expectation?  \n\n**Query data instance:** Can we use a self-generated video and what does it imply?\n\n**Equation (7) parameter:**   fixing $\\\\epsilon_{\\\\text{fix}}$ is sort of understandable. But why is the timestep $t_{\\\\text{fix}}$ fixed?  \n\n**Equation (8) – Structured projection:** Why select the structured projection in this way? Many terms involve random operators; why is a statistical operator (e.g., expectation) not needed?  \n\n**Comparison metrics:**  Suggest using other metrics such as FVMD for more comprehensive quantitative results [1].  \n\n**Human evaluation experiment:**  Only 20 videos are used (10 from a baseline, 10 from the proposed method).  This seems too few to yield convincing results.\n\n\nRef:\\\n[1] Liu, J., Qu, Y., Yan, Q., Zeng, X., Wang, L. and Liao, R., 2024. Fr\\'echet Video Motion Distance: A Metric for Evaluating Motion Consistency in Videos. arXiv preprint arXiv:2407.16124."}, "questions": {"value": "Please see the [Weakness] section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rjyDuW7yY1", "forum": "0dnSbhNoTB", "replyto": "0dnSbhNoTB", "signatures": ["ICLR.cc/2026/Conference/Submission4932/Reviewer_PTzY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4932/Reviewer_PTzY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639545344, "cdate": 1761639545344, "tmdate": 1762917776443, "mdate": 1762917776443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the underexplored issue of motion attribution in video generative models by proposing MOTIVE, a scalable gradient-based framework. Existing methods fail to separate motion from static appearance and lack scalability for large datasets/models. MOTIVE tackles this via three key steps: using flow-weighted masks to isolate temporal dynamics, correcting frame-length bias for fair scoring, and applying Fastfood projection for efficient gradient storage/computation. It calculates motion influence scores for training clips, selects top 10% high-impact data for finetuning. Experiments on VIDGEN-1M/4DNeX-10M with Wan2.1-T2V-1.3B show MOTIVE outperforms baselines: 89.4% dynamic degree on VBench (surpassing full-dataset finetuning’s 84.7%) and 76.7% human preference win rate vs. pretrained models, proving its value for targeted data curation and motion quality improvement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Critical and Well-Targeted Problem FormulationThe paper focuses on a pivotal, underexplored gap in video generative models: identifying training data that drives motion quality (a core video feature distinct from static appearance). Filtering high-quality motion data is vital for finetuning—where carefully selected clips significantly boost temporal coherence and physical plausibility—filling a key need for practical video model optimization.\n\n2. Intuitive and Principled Method DesignMOTIVE’s approach is highly logical and video-specific. It isolates motion from static content via flow-weighted loss masks (using optical flow for dynamic regions), corrects frame-length bias (avoiding spurious long-clip ranking), and uses Fastfood projection for scalability. These choices directly fix image-centric attribution limits, making the framework theoretically sound and practically feasible.\n\n3. Comprehensive and Rigorous ExperimentsExperiments are thorough: evaluations on VIDGEN-1M/4DNeX-10M, VBench for motion metrics, diverse baselines (random selection, full finetuning), and human evaluations (76.7% preference vs. base model). Ablations (single-timestep validity, projection dimension impact) validate components, ensuring robust, credible conclusions."}, "weaknesses": {"value": "All experiments in the paper are exclusively conducted on the Wan2.1-T2V-1.3B model, with no validation on other mainstream video generative architectures (e.g., 3D U-Nets, latent video VAEs with different temporal attention blocks, or non-DiT-based diffusion models). As the paper itself acknowledges, \"our evaluation centers on one open-source backbone due to compute; broader portability is future work\" — this single-model focus means the framework’s effectiveness, such as motion mask compatibility, gradient projection stability, and finetuning gain consistency, cannot be confirmed for other popular video diffusion models. This limitation reduces confidence in the framework’s general applicability to diverse video generation systems, weakening the persuasiveness of its universal utility."}, "questions": {"value": "This paper addresses a crucial problem in data of video diffusion training with detailed analysis, making it worthy of acceptance. As it only tests Wan2.1-T2V-1.3B, adding one more model test would further boost its quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QECxGzbYeb", "forum": "0dnSbhNoTB", "replyto": "0dnSbhNoTB", "signatures": ["ICLR.cc/2026/Conference/Submission4932/Reviewer_MAhn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4932/Reviewer_MAhn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845814435, "cdate": 1761845814435, "tmdate": 1762917775735, "mdate": 1762917775735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the under-explored problem of attributing motion in generated videos to specific training clips, introducing MOTIVE, a gradient-based, motion-aware data attribution framework for video diffusion models. The key idea is to isolate temporal dynamics from static appearance by re-weighting gradients with flow-derived motion masks, enabling scalable influence estimation over modern billion-parameter models. Extensive experiments on VIDGEN-1M and 4DNeX-10M show great performance of MOTIVE."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper opens an important new topic—motion-centric data attribution—that prior image-oriented methods cannot address.\n\n- The authors propose a simple yet effective idea: flow-weighted gradient masking that disentangles motion from appearance without changing the forward generative process.\n\n- Experiments are good, including large-scale datasets and human evaluations."}, "weaknesses": {"value": "I would like to begin by setting aside the specific technical details and share my perspective on the problem that this paper aims to address. Based on my research experience in video generation, I have observed a clear trade-off between motion dynamics and the occurrence of visual artifacts—in general, stronger motion dynamics tend to correlate with a higher probability of artifacts. I believe this trade-off represents one of the most fundamental challenges in current video generation research. From the experimental results presented in the paper (e.g., Table 1), it appears that the proposed method still suffers from this dilemma. In other words, while the approach seems to improve the Dynamic Degree metric significantly, this improvement may come at the cost of other important aspects such as Background Consistency and Imaging Quality. If the main contribution of this work is limited to enhancing motion dynamics at the expense of overall visual stability and quality, such improvements could arguably be achieved more simply by using training data with inherently higher motion. Overall, I appreciate the authors’ motivation to address the problem of video dynamism. However, the current method seems not to fundamentally resolve the underlying trade-off and not to tackle the core technical challenges of dynamic yet consistent video generation.\n\nIn addition, I believe the paper has the following shortcomings:\n(1) Writing quality—the overall writing could be improved. For example, each equation should be properly punctuated, and the presentation could benefit from more polished academic writing.\n(2) Experimental sufficiency—the experiments are relatively limited, as they are conducted only on the Wan-2.1 model. This raises concerns about the reliability and generalizability of the results. It would be more convincing if the authors could include additional models, such as HunyuanVideo, to validate the effectiveness and robustness of their approach."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "51R0VHNxZ5", "forum": "0dnSbhNoTB", "replyto": "0dnSbhNoTB", "signatures": ["ICLR.cc/2026/Conference/Submission4932/Reviewer_FHs6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4932/Reviewer_FHs6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893252937, "cdate": 1761893252937, "tmdate": 1762917774979, "mdate": 1762917774979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MOTIVE, a framework for motion-centric data attribution in video diffusion models. While existing attribution methods primarily analyze static appearance in image diffusion, MOTIVE aims to identify which finetuning clips most influence temporal dynamics in generated videos. The key idea is to compute motion-weighted gradients, where optical-flow–based masks emphasize dynamic regions while suppressing static backgrounds."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper identifies an underexplored but meaningful question: “Which training data drives motion learning in video diffusion models?” This focus on motion attribution provides a clear conceptual step beyond standard image-level attribution, and the method effectively extends existing data attribution frameworks to the video domain with appropriate modifications for temporal structure and scalability."}, "weaknesses": {"value": "**Limited evaluation and unclear attribution advantage**  \nThe proposed attribution-based selection may in practice act as a proxy for identifying motion-rich or high-quality clips, rather than truly capturing data that causally influences motion learning. This concern is amplified by the evaluation setup, where the method is compared only against random selection, a trivial baseline that cannot disentangle whether improvements stem from genuine attribution or simply from favoring dynamic, well-captured videos. A more meaningful comparison would involve finetuning with datasets selected by explicit motion-quality criteria, such as average motion magnitude, optical-flow statistics, or reward-model scores reflecting motion realism or physical plausibility. Without such baselines, it remains unclear whether the proposed approach provides any advantage beyond straightforward motion-saliency filtering."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "eTuTTfaGk5", "forum": "0dnSbhNoTB", "replyto": "0dnSbhNoTB", "signatures": ["ICLR.cc/2026/Conference/Submission4932/Reviewer_8yPp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4932/Reviewer_8yPp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762019491994, "cdate": 1762019491994, "tmdate": 1762917774573, "mdate": 1762917774573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}