{"id": "nsNpsCpVG1", "number": 8259, "cdate": 1758076630015, "mdate": 1763611358858, "content": {"title": "FrameThinker: Learning to Think with Long Videos via Multi-Turn Frame Spotlighting", "abstract": "While Large Vision-Language Models (LVLMs) have achieved substantial progress in video understanding, their application to long video reasoning is hindered by uniform frame sampling and static textual reasoning, which are inefficient and struggle to handle visually intensive video tasks. \nTo overcome these challenges, in this paper, we introduce \nthe concept of thinking with long videos and propose a novel framework FrameThinker. Within this framework, LVLMs are able to iteratively interrogate video content. \nDeveloping such video reasoning capabilities in LVLMs presents notable challenges, particularly in adapting the model to new video actions (e.g. select frame), and designing reward functions to guide LVLMs to adopt the newly introduced action. \nTo solve these challenges, \nwe propose a two-phase training strategy, first employing Supervised Fine-Tuning (SFT) to instill fundamental action capabilities, followed by Reinforcement Learning (RL) to optimize a strategic decision-making policy.\nNotably, in this RL phase, we conduct an in-depth and comprehensive exploration of the reward design for each action and format reward. \nExtensive experiments on reasoning benchmarks like Video-Holmes, LongVideo-Reason, and long-video understanding benchmarks such as LongVideoBench, MLVU, VideoMME, and LVBench, demonstrate that FrameThinker gets a significant average improvement of +10.4\\% over baselines while drastically reducing the number of processed frames. \nMost notably, our 7B model, FrameThinker establishes a new state-of-the-art on LongVideo-Reason, achieving 76.1\\% accuracy using an average of only 20.6 frames. This not only outperforms the competitive LongVILA-R1 (72.0\\%) but does so with over 20x fewer frames (vs. 512), demonstrating unparalleled efficiency and effectiveness. Our code is available at:\n\\url{https://anonymous.4open.science/r/FrameThinker-B5FD}.", "tldr": "FrameThinker is a framework that equips LVLMs with iterative frame selection for long video reasoning, achieving state-of-the-art accuracy with fewer frames.", "keywords": ["LVLMs", "Video Reasoning", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac7f96b85921cf781cb55b526a23abaa6bd2a9a6.pdf", "supplementary_material": "/attachment/0f3983e7b391e5bb7ee8821ce69e7cf6b6a86680.zip"}, "replies": [{"content": {"summary": {"value": "Uniform frame sampling and static textual reasoning are the key problems of inefficiency and ineffectiveness in long video reasoning. This paper, FrameThinker, proposes a two stage training approach which consists of a supervised fine-tuning stage to enable the model to make actions (i.e, selecting frames), and a reinforcement learning stage to learn a policy for action decision making. Experiments show that FrameThinker greatly improve the performance over the baseline with much less frames processed, demonstrating the strong effectiveness and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong performance over baselines presented in the paper, not only in accuracy, but also efficiency. Evaluations on diverse benchmarks show strong generalizations in the video reasoning.\n2. The two stage training, supervised fine-tuning and reinforcement learning, is typical applied in training test-time reasoning in large language models for math and coding problems. This paper expanded the domain to video understanding and reasoning.\n3. For reward design, a novel Cognitive Consistency Verification (CCV) module to verify that the actions from the model are logically grounded, interpretable and aligned with its reasoning. Ablation studies show that CCV is crucial to the performance.\n3. The figures are well-designed and the paper is easy to follow."}, "weaknesses": {"value": "Missing video agent baseline: there were already some papers adopted a similar high-level idea of selecting video segments/frames for long video reasoning in a coarse-to-fine manner [1]. Therefore, adding at least one video agent baseline can make the contribution stronger if showing superior performance over existing methods.\n\nReferences:\n[1] Yang, et al. Video Curious Agent for Long Video Understanding. 2024."}, "questions": {"value": "1. Benchmarks in the paper are all long-video understanding and reasoning. Will training the model (i.e., FrameThinker) negatively impact the short-video reasoning, and traditional tasks for vision language models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qQ6vTnCfp5", "forum": "nsNpsCpVG1", "replyto": "nsNpsCpVG1", "signatures": ["ICLR.cc/2026/Conference/Submission8259/Reviewer_Qf4G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8259/Reviewer_Qf4G"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761241791718, "cdate": 1761241791718, "tmdate": 1762920200034, "mdate": 1762920200034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the task of long-video reasoning in Large Vision-Language Models (LVLMs). The author’s key motivation is that most existing LVLMs process frames uniformly sampled at a fixed interval, which often leads to many irrelevant frames being processed. To address this, they propose FrameThinker to enable LVLMs to perform frame sampling through a learned reasoning process. FrameThinker is trained in two stages: Supervised Fine Tuning (SFT) to teach structured thought-action generation, and Reinforcement Learning (RL) with GRPO to refine capabilities learned in the SFT stage. They conduct an extensive study on the RL stage, showing that unconditional and format-based rewards cause training collapse, and propose the Cognitive Consistency Verification (CCV) module to enforce alignment between thoughts and actions to stabilize the RL training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The problem is well motivated as uniform sampling quickly becomes impractical as video tasks increase in complexity. As complexity increases, LVLMs will require the ability to reason about which frames to process rather than relying on dense or uniform sampling\n\nFrameThinker is sufficiently different from existing frame sampling methods the reviewer is aware of, which rely on heuristics/pre-trained models or decouple training of the frame sampler and LVLM. In contrast, FrameThinker directly optimizes the LVLM to perform frame sampling\n\nThe method is presented clearly though the writing, and its effectiveness is shown on well-chosen benchmarks"}, "weaknesses": {"value": "Concern with comparisons to baseline model\n* It seems like the baseline model (Qwen2.5-VL-7B) is evaluated zero-shot on the Video-Holmes and LongVideo-Reason datasets, but FrameThinker is first fine-tuned on these two datasets. The reviewer believes a true fair baseline in Table 1 would be Qwen2.5-VL fine-tuned on the Video-Holmes dataset, and In Table 2 and Table 3 it should be the Qwen2.5-VL fine-tuned on the SFT+RL instruction pairs without the thinking and action reasoning\n\nFrameThinker still relies on uniform sampling in its initial spare scan and uses them to decide where to “zoom in” and focus. Given that a max of 12 frames are sampled, it is not unlikely that all of these frames will be irrelevant in some cases, especially for queries like the one in Figure 11 that are not contextualized with temporal information. In these cases, FrameThinker will suffer from the same limitation of uniform sampling as existing LVLMs\n\nMinor comments on formatting:\n* Line 26: “FrameThinker get” should be “FrameThinker gets”\n* Notation: In Section 3.1 the query is defined as $i$ but in Section 3.2 the query is defined as $q$ (the $i$ also becomes a subscript instead of a superscript of Tau in Section 3.2). It seems to me like $i$ should correspond to a specific trajectory rollout and not the input query\n* Suggestion for Table 2 and Table 3: It might be better if the deltas are green instead of red as Red makes the differences appear negative. The best performing models can also be bolded (consistent with Table 1)"}, "questions": {"value": "Can the authors clarify the evaluation protocol of the Qwen2.5-VL baseline (see Weakness 1)? If it is zero-shot, is there a reason why Qwen2.5-VL cannot be fine-tuned under the same conditions as FrameThinker?\n\nHow does FrameThinker handle cases where uniformly sampled frames dont capture any query-relevant frames (see Weakness 2)? I imagine it would randomly select a frame interval on which to “zoom in”, does FrameThinker have the ability to “zoom out”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Neqk9WNbjr", "forum": "nsNpsCpVG1", "replyto": "nsNpsCpVG1", "signatures": ["ICLR.cc/2026/Conference/Submission8259/Reviewer_U4yG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8259/Reviewer_U4yG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023517563, "cdate": 1762023517563, "tmdate": 1762920199575, "mdate": 1762920199575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses long video reasoning by introducing FrameThinker, a framework that enables a vision-language model to iteratively select and analyze frames across multiple reasoning turns. The model begins with a coarse video scan and selectively retrieves frames for closer inspection based on its evolving reasoning. It is trained in two stages: supervised fine-tuning to learn tool syntax, followed by reinforcement learning to optimize its frame selection policy. Key contributions include new action primitives, a multi-turn reasoning paradigm, and carefully designed reward mechanisms for stable training. The method achieves state-of-the-art accuracy on video QA benchmarks while using far fewer frames—for instance, 76.1% on LongVideo-Reason with ~20 frames versus 72.0% from prior work with 512. Overall, FrameThinker delivers significant accuracy gains (+10.4% on average) with substantially higher efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novelty - The paper introduces a new multi-turn “thinking” paradigm for video understanding, which is notable in this context, although not a fundamentally new algorithm. By allowing the model to iteratively query the video (via learned actions) rather than passively reading a fixed set of frames, it bridges the gap between static video QA models and interactive video agent systems.\n- Methodology - The two-phase training—supervised fine-tuning followed by RL—is well designed, with thoughtful reward structuring to prevent pitfalls like mode collapse. The Cognitive Consistency Verification module adds robustness, and ablation studies confirm that each component, including multi-turn reasoning and CCV, contributes to performance. The use of GRPO and carefully tuned rewards further support the method’s effectiveness.\n- Empirical Performance - The paper shows substantial improvements in both accuracy and efficiency across multiple long video reasoning benchmarks. FrameThinker outperforms all baselines while using significantly fewer frames—for example, achieving 76.1% on LongVideo-Reason with ~20 frames versus 72.0% with 512. This efficiency makes it well-suited for scaling to longer videos.\n- Impact - This work addresses a key bottleneck in video AI – the inability to efficiently handle long videos by enabling models to focus on relevant frames, offering a scalable solution for long-horizon video analysis. Its integration of reinforcement learning into vision-language models introduces a flexible inference strategy with potential impact beyond video."}, "weaknesses": {"value": "- Related Work - This section should have an explicit section on video key frame selection/sampling, given the focus of the paper. Quite a few key papers are missing from the discussion [a,b,c,d] \n- Frame Selection Ablation - One potential concern is the lack of comparison to other frame selection strategies. The paper convincingly shows improvements over uniform sampling and static baselines, but we don’t see comparisons to any heuristic or learned frame selection method [a,b,c,d]\n- Efficiency - The paper does not report actual inference time or compute cost comparisons. It’s assumed fewer frames = faster, but the iterative process might introduce some overhead (multiple forward passes). Quantifying the real-time speedup (or trade-off) would strengthen the empirical claims of efficiency.\n- Evaluation Scope - The experiments, while extensive on the benchmarks provided, focus mainly on QA tasks. It’s not fully explored how the approach would perform on other types of long video understanding tasks\n- Failure Cases - The paper would benefit from a clearer discussion of failure cases, such as missed events due to poor initial scans or premature stopping. It’s unclear how often issues like over-exploration occur or whether the Cognitive Consistency Verification (CCV) module mistakenly blocks valid strategies. Providing insight into these patterns and how frequently CCV intervenes would help assess the method’s robustness and reliability.\n\nReferences. \n- [a] M-LLM Based Video Frame Selection for Efficient Video Understanding, CVPR 2025\n- [b] VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection, CVPR 2025\n- [c] Vila: Efficient videolanguage alignment for video question answering, ECCV 2024\n- [d]  Self-Chained Image-Language Model for Video Localization and Question Answering, NeurIPS 2023"}, "questions": {"value": "- Frame Selection Ablation - Have you considered comparing FrameThinker to heuristic or learned frame selection strategies beyond uniform sampling? Including such baselines would help clarify how much of the performance gain comes from the learned policy versus the general benefits of dynamic frame selection.\n- Speedup - What is the computational speedup from processing fewer frames? The paper shows a drastic reduction in frames (e.g., 20 vs 512), but due to the multi-turn approach, there may be multiple forward passes. How does the actual inference time or FLOPs compare to a single-pass baseline? Some discussion or measurement of runtime efficiency would strengthen the claim of “unparalleled efficiency.”\n- Generality - How adaptable is FrameThinker beyond QA tasks? Could it handle video captioning or anomaly detection without an explicit query, or is a well-defined question essential for guiding frame selection? Exploring this would strengthen the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Not Applicable."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eylE2to55G", "forum": "nsNpsCpVG1", "replyto": "nsNpsCpVG1", "signatures": ["ICLR.cc/2026/Conference/Submission8259/Reviewer_wput"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8259/Reviewer_wput"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762324220487, "cdate": 1762324220487, "tmdate": 1762920199008, "mdate": 1762920199008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a LVLM that actively reasons over long videos by alternating thoughts and actions in multiple turns, trained via SFT then RL with a rule-based Cognitive Consistency Verification (CCV) filter to keep thoughts and actions logically aligned. The SFT process is trained to learn observe-think-action process, and the RL is to learn what are the frames and when in the timestamp to sample. The integrated CCV is to surpass the illogical thought-action pairs. Results on LongVideo-Reason, Video-Holmes demonstrate the method's frame efficiency. However, there are some drawbacks such as reward shaping is fragile (unconditional or naive bonuses can cause collapse) and CCV is rule-based rather than learned, so behavior may hinge on handcrafted checks and hyperparameters"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Prior LVLMs, Qwen2.5-VL, LongVILA-R1, mostly do one-shot reasoning on a big uniformly sampled frame set. FrameThinker instead actively selects frames over multiple turns, so it hits higher accuracy with far fewer frames.\n2. Compared to Video-R1 and VideoChat-R1, which are also built upon Qwen2.5-VL-7B, FrameThinker consistently scores higher on long-video benchmarks while using similar or fewer frames, showing that the active frame reasoning policy actually adds value beyond the backbone. \n3. Video agents, e.g., VideoAgent, RIVET-like systems, typically rely on manually designed workflows or external tools and are not trained end-to-end; they follow a pre-defined pipeline for querying frames or detectors. While this method makes frame operations part of the LVLM’s own action space and trains them with RL, so the policy for \"where to look next\" is learned from data, not hard-coded. That’s a big step up in autonomy and adaptability vs those agent-style baselines.\n4. Compared to many video agents, FrameThinker adds Cognitive Consistency Verification (CCV) that checks redundancy and fidelity of thought-action pairs, so the proposed method can more easily spot and filter illogical exploration instead of trusting a black-box planner."}, "weaknesses": {"value": "1. The paper’s efficiency claims are measured only in terms of the number of frames processed before feeding to the fixed LVLM, which is not an end-to-end computation. Compared to prior RLVR and CoT baselines that perform a single-shot pass on a uniformly sampled frame set, FrameThinker introduces extra multi-turn reasoning with repeated LVLM calls over successively updated contexts. While this is likely more frame-efficient, it is unclear whether it is actually more computationally efficient overall. \n2. The method is not compared against any SOTA that also performs reasoning-based or active frame sampling on long videos, making it difficult to justify the benefits of the proposed design.\n3. Can the method be plug-and-play module before any LVLMs? The proposed FrameThinker requires an action grammar, a bespoke RL setup, a rule-based CCV module, and nontrivial reward tuning."}, "questions": {"value": "1. The authors measure efficiency mainly as \"frames processed per question.\" How does FrameThinker compare to LongVILA-R1 / Video-R1 in terms of actual FLOPs or wall-clock latency, given you do multiple LVLM passes per example?\n2. Why not compare against a baseline that uses uniform frames but also multi-turn CoT (repeated LVLM calls) to match your compute pattern more fairly?\n3. How would FrameThinker adapt to long-video tasks where the supervision is not QA-style (e.g., temporal localization, dense captioning, or video editing assistance) where rewards and clean correctness signals are harder?\n4. How many examples does the model answer correctly without really using new frames (i.e., from the question or initial sparse scan alone)? Do you detect significant \"shortcut\" behavior?\n5. How robust are results to the exact CCV rules and thresholds? If you relax or slightly perturb them, does performance drop sharply?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "styg8PCa4v", "forum": "nsNpsCpVG1", "replyto": "nsNpsCpVG1", "signatures": ["ICLR.cc/2026/Conference/Submission8259/Reviewer_gjov"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8259/Reviewer_gjov"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762394989805, "cdate": 1762394989805, "tmdate": 1762920198444, "mdate": 1762920198444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel framework, FrameThinker, that reframes long video understanding. Other than using a traditional single-pass processing, FrameThinker uses multi-turn frame reasoning process, which dynamically highlights the relevant frames. By generating textual thoughts hint guided by actions prior, it focus on promising segments by choosing specific frame ranges.\n\nIn specific, it didn’t load large set of frames but learn to engages in a multi-turn reasoning loop. Then it utilizes the dynamic actions strategies which the primary action is chosen with the start and end frame within the “highlighted” range, allowing to extract the critical information based on its reasoning. After that, they uses a two-phase pipeline for training: 1. Using a supervised fine-tuning tiny dataset as teaching model for the syntax and mechanics of actions; 2. Using a larger dataset with reinforcement learning to train a strategic policy on controlling the timing to use these actions for. They provides a comprehensive reward design correspondingly for this task in order to point out unconditional rewards that lead to collapse and set conditional action bonus only on final success. In addition, they introduce a cognitive consistency verification (CCV) module to suppose illogical executions and ensure the interpretability of the model’s actions.\n\nThe author demonstrates the performance of FrameThinker by conducting extensive experiments to gain superior accuracy while using significantly fewer frames."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "•\tThe paper tackles a critical challenge in long video understanding: the efficiency and interpretability of long video due to uniform frame sampling for long-term reasoning. The motivation is interesting, and the core idea of “multi-turn frame spotlighting”. The author successfully builds up a pipeline of combining thoughts and actions along with observations. It mimics human-like analysis (skimming, then focusing) and moves the field from passive processing to active, agentic reasoning.\n\n•\tCognitive Consistency Verification (CCV): The paper carefully addresses the reward collapse issue during training with an unconditional action bonus. The CCV module is a novel and effective build-up component that acts as a filter after the rollout process in order to validate every trajectory by checking possible redundancy, logical flow, and fidelity. CCV ensures the generated chain of thoughts during the decision-making process. \n\n•\tSOTA on extensive experiments: With fewer input frames as final feed, delta and performance are consistent. FrameThinker reaches SOTA performance on xis benchmarks(Video-Holmes, LongVideo-Reason, LongVideoBench, MLVU, VideoMME-Long, LVBench)."}, "weaknesses": {"value": "•\tAction Space Flexibility: The paper mentioned that the current action space will depend on the selected range. It can help pinpoint the action to make predictions more accurate, but will also limit the generalizability of the model if it fails to get enough action within the range.\n\n•\tCCV’s robustness: Since CCV is a rule-based module, it weakens its generalizability when facing a more complex, interactive scenario, which creates a more difficult reasoning path. CCV might lose its advantage in complex scenes."}, "questions": {"value": "•\tWould the CCV flag a valid but complex reasoning path as an error, and how do you plan to scale these rules to more complex reasoning?\n\n•\tDuring the SFT phase (Phrase 1 training), if the teacher failed to give a basic action syntax for the features of SFT, it might likely to significantly decrease the performance on the final performance since it mainly relies on the quality of the teacher model’s data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cO9YNOfK15", "forum": "nsNpsCpVG1", "replyto": "nsNpsCpVG1", "signatures": ["ICLR.cc/2026/Conference/Submission8259/Reviewer_jZYr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8259/Reviewer_jZYr"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762405534987, "cdate": 1762405534987, "tmdate": 1762920198033, "mdate": 1762920198033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}