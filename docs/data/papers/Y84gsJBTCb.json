{"id": "Y84gsJBTCb", "number": 14211, "cdate": 1758230355388, "mdate": 1759897383426, "content": {"title": "ToolRM: Outcome Reward Models for Tool-Calling Large Language Models", "abstract": "As large language models (LLMs) increasingly interact with external tools, reward modeling for tool use has become a critical yet underexplored area. Existing reward models, trained primarily on natural language outputs, struggle to evaluate tool-based reasoning and execution. To quantify this gap, we introduce FC-RewardBench, the first benchmark designed to systematically evaluate reward models in tool-calling scenarios. Our analysis shows that current reward models often miss key signals of effective tool use, highlighting the need for domain-specific modeling. To address this, we propose a training framework for outcome reward models using data synthesized from permissively licensed, open-weight LLMs. We train models ranging from 1.7B to 14B parameters and evaluate them across seven out-of-domain benchmarks. These reward models consistently outperform general-purpose baselines, yielding up to a 25% average improvement in downstream task performance, enhancing robustness to input noise, and enabling data-efficient fine-tuning through reward-guided filtering.", "tldr": "", "keywords": ["Reward Modeling", "Large Language Models", "Tool Use"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7047deb506cba3609fef253d624ed4a1a8785c1e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on reward modeling for tool-calling in LLMs. The authors introduce FC-RewardBench, a benchmark explicitly designed to evaluate outcome reward models (ORMs) in function and tool-calling contexts. They propose ToolRM, a new family of ORMs trained with data synthesized from open-weight LLMs, with experiments demonstrating that ToolRM offers significant improvements—up to 25% average gains—across multiple tool-use benchmarks. Additional analyses highlight ToolRM’s effectiveness for data filtering and increased robustness, especially in low-data regimes and noisy conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Reward modeling for tool use is an area that remains relatively unaddressed in existing reward modeling literature.\n- The paper is relatively clearly written."}, "weaknesses": {"value": "- The motivation for training a reward model for tool use is unclear. If tool use and task performance (e.g., QA), in both the training and inference settings, are verifiable, what is the motivation for training a reward model to evaluate the effectiveness of tool use?\n- The benchmark is mostly adapted from other open source benchmarks (specifically, BFCL-v3). The training set is made by constructing negative examples. Other than this, the training paradigm for ToolRM is well established by previous work. These made the contribution of the submission limited.\n- The experiments show that the performance of ToolRM underperforms inference scaling methods like majority voting or vanilla model with greedy decoding. This possibly demonstrates the ineffectiveness of the training set constructed in the paper."}, "questions": {"value": "- How sensitive is ToolRM’s performance to the choice of the reward centering coefficient $\\eta$ and to architectural or regularization hyperparameters? Can an ablation be provided?\n- How generalizable is ToolRM to entirely new tool schema (e.g., tools unseen during training or with radically different parameter signatures) or domains outside API calls?\n- The error analysis (Table 7) shows some error types (e.g., irrelevance errors) increase with ToolRM. What caused this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5zH7ZoUqW7", "forum": "Y84gsJBTCb", "replyto": "Y84gsJBTCb", "signatures": ["ICLR.cc/2026/Conference/Submission14211/Reviewer_p6AE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14211/Reviewer_p6AE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901870001, "cdate": 1761901870001, "tmdate": 1762924668299, "mdate": 1762924668299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on reward modeling for tool-based reasoning scenarios. The authors first propose FC-RewardBench, a benchmark for systematic evaluation of reward models in tool-calling contexts. Subsequently, they introduce a training framework for outcome reward models. Experimental results demonstrate the effectiveness of their approach and highlight the importance of reward modeling in tool-calling reasoning scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-structured with clear writing that facilitates comprehension of the proposed methodology.\n2. The proposed FC-RewardBench represents a significant contribution to the tool-calling domain, providing a systematic evaluation framework for reward models that will facilitate the development of reward modeling in this scenario.\n3. The authors conduct experiments on FC-RewardBench and multiple datasets, demonstrating the importance of reward modeling. Additionally, they evaluate the proposed ToolRM using best-of-n sampling, showing consistently strong performance."}, "weaknesses": {"value": "1. In constructing FC-RewardBench, correct tool calls are sourced from original data while incorrect tool calls are generated by various models. This approach may introduce structural or pattern-based differences between correct and incorrect tool calls, or result in errors that have minimal or negligible impact on the overall reasoning outcome. The paper lacks effective mechanisms to manage these potential biases.\n2. For tool-calling reasoning, many scenarios can be constrained through rule-based outcome rewards. Moreover, process rewards for constraining multi-turn tool calls are equally important. However, the paper exclusively focuses on outcome rewards without discussing process reward modeling approaches.\n3. The experimental evaluation lacks direct application of the reward models to downstream tasks, such as demonstrating the enhancement effects on tool-calling agents through integration with PPO or GRPO training procedures."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wnB4IiuOht", "forum": "Y84gsJBTCb", "replyto": "Y84gsJBTCb", "signatures": ["ICLR.cc/2026/Conference/Submission14211/Reviewer_3mfp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14211/Reviewer_3mfp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996892515, "cdate": 1761996892515, "tmdate": 1762924667657, "mdate": 1762924667657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ToolRM, a family of outcome-based reward models specifically designed for evaluating and improving large language models in tool-calling scenarios. The authors also propose FC-RewardBench, the first benchmark for assessing reward models on function-calling correctness. ToolRM is trained on synthetic preference data derived from multiple open-weight function-calling models and evaluated across seven out-of-domain benchmarks. Experimental results show that ToolRM outperforms general-purpose reward models and LLM-as-a-Judge baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles an urgent and important problem: reward modeling for tool-calling LLMs.\n2. The proposed FC-RewardBench is a well-constructed benchmark that fills a clear evaluation gap.\n3. Empirical results show that ToolRM consistently outperforms general-purpose reward models on several tool-use benchmarks, and that it can be used effectively for Best-of-n inference and data filtering."}, "weaknesses": {"value": "1. The motivation for designing a tool-specific, learned reward model is not clearly justified. Many aspects of tool correctness, such as format validity, AST equivalence, or parameter type checking, can already be handled by rule-based or symbolic evaluation metrics (which are very common). I suggest the author explain more limitations of these rule-based evaluations and further clarify why a learned LLM-based RM is necessary for this domain.\n\n2. An implicit assumption is that a tool-specific reward model will lead to a more capable tool-use agent when used for RLHF or preference optimization. However, this is not demonstrated experimentally. It would greatly strengthen the paper to include a fine-tuning or RL experiment showing that ToolRM, as a reward signal, indeed improves tool-use reasoning beyond what general or rule-based rewards achieve.\n\n3. The study focuses solely on tool-calling datasets. It remains unclear whether ToolRM generalizes beyond this domain. Evaluating on general-purpose reward modeling benchmarks such as RewardBench or RewardBench-2 would reveal whether the model’s learned signal captures general notions of quality or is over-specialized to tool-use syntax.\n\n4. I am concerned that the experimental comparisons might not be fully fair. ToolRM is trained on datasets that include tool-use trajectories similar to those in the evaluation benchmarks, while the baseline reward models are not exposed to such data. I suggest the authors provide more clarification on the experiment setup to ensure a fair comparison."}, "questions": {"value": "See above weakness\n\nI recognize the authors' efforts in doing this work. However, there are some weaknesses that could be considered to strengthen the overall work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cx2sRvTTEq", "forum": "Y84gsJBTCb", "replyto": "Y84gsJBTCb", "signatures": ["ICLR.cc/2026/Conference/Submission14211/Reviewer_68SC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14211/Reviewer_68SC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998979340, "cdate": 1761998979340, "tmdate": 1762924667253, "mdate": 1762924667253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ToolRM, a family of outcome reward models (ORMs) tailored for the tool-calling regime, and introduces FC-RewardBench, a 1.5k-example benchmark designed to evaluate reward models’ ability to distinguish correct versus incorrect function calls. ToolRM models are trained using pairwise Bradley–Terry preferences formed from model-generated incorrect tool calls versus ground-truth calls. The paper claims (i) the first benchmark for reward models in function-calling, (ii) specialized ORMs that outperform domain-agnostic RMs and some LLM-judges at far smaller sizes, and (iii) strong correlation between FC-RewardBench accuracy and downstream tool-use performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The research problem is importtant. Rewarding validity of function calls is practically important for inference-time reranking and data filtering.\n\n2. Outcome-only pairwise BT objective with reward centering and small backbones is easy to reproduce.\n\n3. Introduce the FC-RewardBench evaluation with downstream tool-use performance targets a lower-cost proxy."}, "weaknesses": {"value": "1. **Counter-evidence claims**: Tool-Augmented Reward Modeling (Themis, ICLR’24)  [1] explicitly equips RMs with external tool use and reports robust improvements, which aims to capture tool-based reasoning by letting the RM use tools during judgment. This directly contradicts the blanket claim that \"existing RMs fail to capture\" tool use the authors claimed in the introduction. The paper ignored previous tool-augmented RM literature.\n\n2. **Factually incorrect novelty statement**. The statement “this is the first work introducing ORMs for tool calling” is demonstrably false.  This shows insufficient literature review on reward modeling for tool use.\n\n3. **Missing comparison to tool-augmented RMs (IMPORTANT)**. The paper claims existing RMs fail to capture tool-based nuances, yet [1] have already studied tool-augmented reward models (TARM) across varying kinds of tools (covering both single-turn and multi-turn) and reports robust improvements on both RM and RL training. A fair assessment requires adding a TARM baseline (with tools enabled).\n\n4. **Over-stated “no benchmark” claim**. The field has mature function-calling benchmarks (API-Bank, ToolBench/StableToolBench, BFCL) and increasingly mature RM benchmarks (RewardBench). If the novelty is “an RM-focused benchmark for pairwise outcome preferences in function calling,” this must be stated precisely.\n\n\n## References:\n[1] Tool-Augmented Reward Modeling. ICLR 2024."}, "questions": {"value": "see above weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "D0bjMQf6Mo", "forum": "Y84gsJBTCb", "replyto": "Y84gsJBTCb", "signatures": ["ICLR.cc/2026/Conference/Submission14211/Reviewer_sR8x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14211/Reviewer_sR8x"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762017194445, "cdate": 1762017194445, "tmdate": 1762924666730, "mdate": 1762924666730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}