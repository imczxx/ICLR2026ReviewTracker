{"id": "6AD3JuMxxg", "number": 12535, "cdate": 1758208446690, "mdate": 1759897503473, "content": {"title": "Fast Visuomotor Policy for Robotic Manipulation", "abstract": "We present a fast and effective policy framework for robotic manipulation, named $\\textbf{Energy Policy}$, designed for high-frequency robotic tasks and resource-constrained systems. Unlike existing robotic policies, Energy Policy natively predicts multimodal actions in a single forward pass, enabling high-precision manipulation at high speed.  The framework is built upon two core components. First, we adopt the energy score as the learning objective to facilitate multimodal action modeling. Second, we introduce an energy MLP to implement the proposed objective while keeping the architecture simple and efficient.  We conduct comprehensive experiments in both simulated environments and real-world robotic tasks to evaluate the effectiveness of Energy Policy. The results show that Energy Policy matches or surpasses the performance of state-of-the-art manipulation methods while significantly reducing computational overhead. Notably, on the MimicGen benchmark, Energy Policy achieves superior performance with at a faster inference compared to existing approaches.", "tldr": "", "keywords": ["Robotics", "Imitation Learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/98268f22e00917f8ca83f45a2048e9eaa37f64f5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes using energy score-based policies as a way to obtain multi-modal continuous actions while keeping a reasonable inference speed. The main motivation is to bypass the need for lengthy diffusion online or the loss of accuracy via distillation or loss of granularity and accuracy based on discretization. The proposed approach is compared to multiple baselines in single and mutli-task settings in simulation as well as 3 real-world tasks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is very well written and is an enjoyable read\n- Finding methods that appropriately tradeoff inference speed and performance is very important for policy learning, specially in robotic manipulation\n- The proposed approach is very simple making it an elegant solution to the problem\n- The results are quite promising, almost consistently showing a mostly equivalent (sometimes worse, sometimes better) performance in comparison to the baselines, while improving on inference speed\n- Experiments include ablations of the introduced hyperparameters. This makes building on top of this work by properly understanding a lot better."}, "weaknesses": {"value": "- Novelty is limited\n- Table 4 is missing some values without an explanation for why this is the case\n- The papers lacks a deeper analysis of the experimental results, for instance in some cases the proposed approach outperforms the baselines in others it doesn't. It would be interesting if the authors can attempt to provide an understanding of which settings are most and least suited for their method"}, "questions": {"value": "Can you provide a deeper discussion based on your results, on what settings most favor your method, and which settings can be leading to drop in performance? to rephrase, do you have an intuition on what to blame the drop in performance in certain tasks on?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dcIICt5sYM", "forum": "6AD3JuMxxg", "replyto": "6AD3JuMxxg", "signatures": ["ICLR.cc/2026/Conference/Submission12535/Reviewer_yAJK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12535/Reviewer_yAJK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707424394, "cdate": 1761707424394, "tmdate": 1762923397815, "mdate": 1762923397815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors introduced a visuomotor manipulation policy named Energy Policy. Contrary to previous works like distilled diffusion policy or other autoregressive methods, Energy Policy naturally only requires one single forward pass to predict multi-modal actions. The whole framework is lightweight and simply implemented with an energy MLP and a transformer decoder, which are optimized by the energy score. Experiments are conducted in both simulation and real-world settings, through which Energy Policy exhibits its comparable performance and efficiency to some degree."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The whole idea and implementation of the Energy Policy is simple but quite effective. Besides, it naturally requires only one single pass to predict actions without the need for distillation.\n- Using the energy score as the learning objective, the model can not only generate samples close to the target action but also maintain a certain level of diversity between independent samples.\n- Experiments are conducted in both simulation and real-world settings. Simulation experiments include RoboMimic, Franka Kitchen, MimicGen, and PushT. Real-world experiments also have more than one setting. The speed advantage of Energy Policy is demonstrated through these settings."}, "weaknesses": {"value": "- Although experiments are conducted in various settings, the tasks that the authors choose are quite simple, and most of them are just some basic atomic skills. For example, in the tasks from RoboMimic, state-based methods have already achieved 1.0 success rates. Thus, they cannot truly reflect the proposed methods’ effectiveness. So are the cases for Franka Kitchen. \n- The selection of baselines across different environments is not consistent, and many comparisons are not complete. For instance, why are the original DP’s performances not shown in Table 4? Besides, multiple positions are blank and filled with dashes.\n- Similarly, the real-world experiments also have such kinds of problems. The so-called dynamic tasks are actually still static ones.\n- The writing of this work requires improving. Some expressions are just too over, e.g., in line 60 “...making it computationally prohibitive…”"}, "questions": {"value": "- Improvements on single-task policies may not be enough at current timesteps. Since the architecture is quite like some VLA models, do you have any plans to apply the energy policy to them to replace the flow matching method during post-training?\n- Could you please provide more experimental results involving more complex manipulation tasks during the rebuttal stage?\n- Can you provide some insights regarding your selection of the coefficient $\\alpha$ from the perspective of data distribution other than just experience?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SBEc9ryfxr", "forum": "6AD3JuMxxg", "replyto": "6AD3JuMxxg", "signatures": ["ICLR.cc/2026/Conference/Submission12535/Reviewer_JKTV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12535/Reviewer_JKTV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832386399, "cdate": 1761832386399, "tmdate": 1762923397295, "mdate": 1762923397295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"Energy Policy,\" a novel visuomotor policy designed to address the critical trade-off between inference speed, action precision, and the ability to model multimodal action distributions. The authors identify key limitations in existing approaches: autoregressive models sacrifice fine-grained precision by discretizing actions, simple L1/L2 regression policies are inherently unimodal (failing in tasks with multiple valid solutions), and diffusion models, while effectively multimodal, are computationally expensive due to their iterative denoising process.\n\nThe proposed Energy Policy tackles this by making two primary contributions:\n\nEnergy Score as Objective: It employs the energy score, a strictly proper scoring rule, as its learning objective. This allows the model to learn complex, multimodal distributions without an explicit density function.\n\nEnergy MLP Architecture: It introduces a simple yet effective \"Energy MLP\" head. This module takes the output of a standard Transformer decoder (a latent vector $z_t$) and a random noise vector $\\epsilon$ as input. By training this implicit generator (EnergyMLP($z_t, \\epsilon$)) with the energy score loss, the model learns to produce action samples from the desired multimodal distribution.\n\nCrucially, this design allows for action generation in a single forward pass during inference by simply providing one noise sample. The authors conduct a comprehensive evaluation across a wide array of simulated benchmarks (Robomimic, Franka Kitchen, MimicGen, PushT) and two real-world robotic setups. The results are compelling: Energy Policy consistently matches or exceeds the success rates of state-of-the-art baselines (including strong diffusion and autoregressive models) while being significantly faster, achieving speedups ranging from 3x to 70x."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Significant Performance (Speed): The primary claim of the paper is its speed, and it delivers unequivocally. The ability to generate continuous, multimodal actions in a single pass is a major advantage. The reported inference times (e.g., 7.02ms on PushT in Table 4, 3-7x faster than the next-fastest baseline CARP, and 20-70x faster than diffusion policies in Table 1) are a substantial contribution for real-world robotics, where low latency is paramount. The real-world \"Catch\" task (Sec 4.2.2) provides excellent practical validation of why this inference speed matters.\n\n2. Excellent Performance (Success Rate): The method does not sacrifice accuracy for speed. It demonstrates state-of-the-art or comparable success rates against a very strong and recent set of baselines (DP, CARP, OneDP, CP, etc.). The comparison in Table 4 is particularly strong, showing that while another fast policy (VQ-BeT) exists, it suffers a massive performance drop (0.87 vs 0.68 avg. success), whereas Energy Policy maintains high performance.\n\n3. Novelty and Elegance: The core idea of using an energy score to train an implicit generative model (the Energy MLP + noise injection) is an elegant solution to the problem. It cleverly sidesteps the iterative sampling of diffusion and the imprecision of autoregressive tokenization, effectively getting the best of both worlds. The architecture is simple and the method is well-motivated.\n\n4. Comprehensive Evaluation: The experimental validation is thorough."}, "weaknesses": {"value": "1. Missing Context in Related Work (Implicit Models): The core mechanism—a deterministic function $f(z, \\epsilon)$ that maps a latent code $z$ and a noise vector $\\epsilon$ to an output, trained with a loss that compares samples—is the definition of an implicit generative model. A discussion comparing the energy score loss to other implicit losses would significantly strengthen the paper's positioning.\n\n2. Depth of Multimodality Analysis: The paper claims to model multimodal distributions, and provides visual evidence in Figure 5 for a bimodal task (go left or go right). This is good, but the analysis is somewhat superficial. The \"mode\" is generated by a simple uniform or Gaussian noise vector. What is the capacity of this mechanism? Can it learn to represent, for example, three or four distinct, sharp modes (e.g., three different valid grasps for an object) with the correct probabilities?\n\n3. Baseline Selection in Real-World Tasks: In the real-world evaluation (Sec 4.2), the authors compare their method only against diffusion policies (DP-UMI and DP-C). While they convincingly win on speed and success, this is a slightly weak comparison, as diffusion is known to be slow. A much more compelling experiment would have included a comparison against the strongest fast baseline from simulation.\n\n4. Analysis of $\\alpha$ Hyperparameter: The ablation on the $\\alpha$ parameter for the energy loss (Fig 6b) is interesting. The paper notes that $\\alpha=1.0$ is used empirically, and that performance degrades for $\\alpha=1.5$ or $\\alpha=2.0$. The paper would be strengthened by a brief hypothesis for why this is the case. Is it an issue of gradient stability during training when using an L1.5 or L2 norm in this specific loss formulation (Eq 3)?"}, "questions": {"value": "1. Could you elaborate on the relationship between Energy Policy and the broader category of implicit generative models? The method of using a noise-conditioned MLP seems to be a form of implicit generator. How does training with the energy score (Eq 3) compare conceptually to other implicit objectives like IMLE?\n\n2. Following up on the multimodality claim, can you provide more insight into the capacity of the noise-injection mechanism? Have you tested its ability to model more complex, multi-modal distributions (e.g., 3+ discrete, valid solutions)?\n\n3. For the real-world experiments, why was the comparison limited to diffusion policies? A comparison against a fast autoregressive baseline like CARP would have been more comprehensive, as CARP was a strong competitor in simulation.\n\n4. Do you have a hypothesis for the performance degradation when $\\alpha > 1.0$ (Fig 6b)? Is this due to training instability, or a more fundamental property of the energy score in this application?\n\n5. In your dynamic \"Catch\" experiment (Sec 4.2.2), you demonstrate a key application for low-latency policies. Could you please briefly discuss relevant concurrent work in this specific area, such as \"Latent Adaptive Planner for Dynamic Manipulation\" (Noh et al., 2025)? It would be valuable to contrast your single-step action policy with their trajectory-level planning approach for this class of dynamic task."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KWwuCPzXev", "forum": "6AD3JuMxxg", "replyto": "6AD3JuMxxg", "signatures": ["ICLR.cc/2026/Conference/Submission12535/Reviewer_Lkr6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12535/Reviewer_Lkr6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762469271548, "cdate": 1762469271548, "tmdate": 1762923396616, "mdate": 1762923396616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors consider policy learning in a continuous action space for robotic manipulation tasks. Specifically, the authors propose a method for learning multi-modal action distributions using an architecture which has a lower inference time than other multi-modal policy learning techniques (including most “fast” ones). The key idea here is to predict a distribution representation with a transformer, and then sample actions from an Energy MLP conditioned on uniform random noise. This Energy MLP is trained using the Energy loss. When trained on existing benchmark tasks, this method matches the performance of other SOTA methods while reducing the parameter count and inference time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Decreases inference time substantially, making their method more suitable for real-time inference\n* Explores a novel loss formulation that I haven’t seen before in this context.\n* Many experiments in simulation and real are provided, showing that the method is applicable in multiple scenarios.\n* Well-written and clear."}, "weaknesses": {"value": "* The benchmark tasks chosen are in some ways too easy - meaning their distributions are relatively narrow and not particularly multimodal. It’s not clear whether the proposed MLP-based architecture scales to rather complex distribution\n* Missing an ablation where parameter size is held constant compared to some of the larger models - e.g. could the larger models be reduced in parameter size with the same training technique but yield the same results? Speedup is presumably from that inference time reduction due to fewer parameters.\n* The multimodal behavior “T” example is not really sufficient to demonstrate multimodality, as the task is a bit too simple (2 modes)\n* Necessity of the energy loss (as compared to other distribution losses) is not well-motivated\n* nitpick: flow-matching is not mentioned in the related work"}, "questions": {"value": "* in section 3.2.1, in equation (3) why isn’t a 3rd independent sample drawn, e.g. to match the formulation in (2)? X1, x2, and y are used in (2).\n* Why wasn’t the sampling done at the input to the Transformer? Instead of in a separate MLP head.\n* Can the authors please describe, at a fundamental level, how this differs from a standard CVAE architecture? What would happen if the uniform noise was switched to Gaussian, and the loss became a standard ELBO loss? Is there any benefit from using the Energy loss in that case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iATKpkAY1t", "forum": "6AD3JuMxxg", "replyto": "6AD3JuMxxg", "signatures": ["ICLR.cc/2026/Conference/Submission12535/Reviewer_ehTc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12535/Reviewer_ehTc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762876574083, "cdate": 1762876574083, "tmdate": 1762923394946, "mdate": 1762923394946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}