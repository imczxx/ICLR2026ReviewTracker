{"id": "OCkJKMhIzV", "number": 10818, "cdate": 1758182603418, "mdate": 1759897627064, "content": {"title": "Rational Irrationality: Evaluating LLMs In Games With Strategic Behavior Discrepancies", "abstract": "Large language models (LLMs) are increasingly deployed in complex decision-making environments. Consequently, evaluating their strategic reasoning abilities is becoming increasingly important.  A growing body of research investigates their performance in multi-objective settings, often based on or inspired by game theory, with evaluation typically focusing on the models' ability to align with theoretical expectations. This paper shifts the focus to evaluating the alignment between LLM behavior and human strategic thinking by analyzing LLM responses in a well-established game theory testbed. We revisit three notable games--- Rock, Paper, Scissors (RPS), the Centipede Game (CG), and the Traveler’s Dilemma (TD), all of which are characterized by substantial discrepancies between empirical human behavior and theoretical predictions. For each game, we record the choices made by LLM agents and compare them with historical data from human subject experiments to uncover commonalities and particularities in their underlying strategic reasoning patterns. Our results indicate that LLMs are, in general, more aligned with game-theoretical expectations and show limited sensitivity to game hyperparameters. In RPS, most LLMs imitate rational behavior, but perform sub-optimally. In CG, likewise, LLMs adopt rational strategies, learning from past interactions. Finally, in TD they cooperate toward a better payoff, adopting, however, a more prudent strategy plan than humans.", "tldr": "", "keywords": ["llm", "game theory", "alignment", "rock paper scissors", "centipede game", "traveler's dilemma"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2bd1693dff2dd478078592c7fa41ce8805b1c500.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper evaluates how LLMs make strategic decisions by comparing their behavior to both game-theoretic predictions and human experimental data. Using three classic games, i.e., Rock-Paper-Scissors, Centipede, and Traveler’s Dilemma, the authors find that LLMs generally behave more rationally than humans, aligning more closely with theoretical equilibria. However, they show limited sensitivity to payoff changes and exhibit prompt-dependent variability, indicating shallow or unstable strategic reasoning. Overall, the study reveals that LLMs tend to favor safe, equilibrium-like strategies over the cooperative or risk-taking behaviors typically observed in humans."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. They have open-source code for better reproducibility\n2. Evaluate prompt sensitivity\n3. Collect documented human behaviors for reference"}, "weaknesses": {"value": "1. Only open-source models are evaluated. Could you please also evaluate some common proprietary models like GPT, Gemini, and Claude for an easier comparison with previous work?\n2. Game selection. The current three games are all two-person zero-sum or quasi-zero-sum games. The conclusion may be limited. For example, the conclusion may not generalize to cooperative games. The limitation should be reflected either in a limitation section, the introduction, or in the title.\n3. The key conclusion in RPS is that LLMs do not align with NE, which is different from some previous findings where LLM choices are very skewed toward rock [1]. Could you please explain?\n4. In my understanding, the prompt variants differ only in the symbols. There are not any rephrases. I think the change is very minimal and may not reflect the actual prompt sensitivity.\n5. Studying whether LLMs prefer optimal strategies or average human choices has been studied in this area. Some papers are not discussed sufficiently, especially for those with different conclusions. For example, Brookins and DeBacker, 2023 concluded that “the LLM replicates human tendencies towards fairness and cooperation. It does not choose the optimal strategy in most cases.” Additionally, Huang et al, (2025 concluded that “its predictions are more aligned with human behavior than the game’s NE,” in section 4.3. I think more discussions with these conclusions should be put into the paper.\n\n[1] Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game. ICML 2024."}, "questions": {"value": "1. Line 105: “This results in models performing better than the NE strategy, but worse than humans.”\n2. Line 253: “the number of participants to 100.” Do you randomly pair the 100 LLMs into 50 pairs for each round?\n3. DeepSeek-V3 performs differently in CG and TD, one against NE and the other toward. How to understand such a discrepancy?\n4. Do you examine whether LLMs know the optimal strategy for each game? You can simply ask LLMs to analyze and see whether their answers are correct. Also, you can vary the game settings and rephrase the sentences to avoid game leakage.\n5. I really want to see more discussions about the broader impact of LLMs if they align more with GT-predicted optimal strategies and are used in some financial decision scenarios.\n6. The human references are from 1999, 1992, and 2014. They are 20~30 years ago. How much do you think human behaviors nowadays have changed?\n7. How will models behave if you apply CoT? A deeper analysis of LLM behaviors using their CoT is needed for this paper.\n\nMinor suggestions and typos:\n1. Fig 4: figures are too small.\n2. Table 1: can add a row of “random.” Will it be all 33.3?\n3. Table 1: the reference to Figure 2 should be Figure 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nEkLfxkINr", "forum": "OCkJKMhIzV", "replyto": "OCkJKMhIzV", "signatures": ["ICLR.cc/2026/Conference/Submission10818/Reviewer_UBaE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10818/Reviewer_UBaE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761200496758, "cdate": 1761200496758, "tmdate": 1762922023862, "mdate": 1762922023862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines large language models (LLMs) in three classic game-theoretic environments—Rock-Paper-Scissors, Centipede Game, and Traveler’s Dilemma—to compare their strategic behavior against both game-theoretic (GT) rationality and empirical human data. The authors conclude that LLMs generally behave more rationally according to GT predictions than humans, are relatively insensitive to payoff hyperparameters, and sometimes exhibit partial human-like traits."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The experimental design is **systematic and reproducible**, with clear descriptions of protocols, baselines, and data sources.\n2. The paper includes **comprehensive empirical comparisons** across several models and games, supported by detailed statistical results."}, "weaknesses": {"value": "1. The **technical novelty is minimal.** The work largely reproduces existing studies on LLM behavior in classic games without introducing new theoretical, methodological, or empirical contributions.\n2. The **research motivation is weak**, providing little justification for the significance or necessity of re-evaluating LLMs on these already well-studied game scenarios.\n3. The **analysis remains descriptive**, lacking deeper interpretation of why certain models behave as they do or how prompt sensitivity influences outcomes.\n4. The **paper is overly lengthy and unbalanced**, spending excessive space on textbook-level explanations of game theory while offering limited new insights."}, "questions": {"value": "1. Rock–Paper–Scissors and Traveler’s Dilemma have been widely explored in prior LLM and behavioral-game-theory studies, whereas the Centipede Game is less examined. Could the authors clarify the rationale for choosing exactly these three games, and what new insight their combination provides beyond existing findings?\n2. The paper reports notable variance across verbalizations but provides limited interpretation. Could the authors elaborate on what this variability indicates about model reasoning consistency, symbolic sensitivity, or policy stability? Would additional ablations (e.g., varying label semantics or framing) help disentangle these effects?\n3. While Appendix D.1 briefly mentions temperature settings, the paper does not examine how temperature, model size, or multi-round context length affect behavioral stability. Would a systematic sensitivity analysis help confirm the robustness of the reported findings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xH5eDHEBqo", "forum": "OCkJKMhIzV", "replyto": "OCkJKMhIzV", "signatures": ["ICLR.cc/2026/Conference/Submission10818/Reviewer_g9qD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10818/Reviewer_g9qD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658132073, "cdate": 1761658132073, "tmdate": 1762922023382, "mdate": 1762922023382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors examine the alignment between LLMs and humans in strategic behavior through a game-theoretic perspective. Specifically, they evaluate LLM performance in three games where empirical findings diverge from theoretical predictions. Their analysis indicates that LLMs tend to imitate rational behavior, blending sometimes with human-like strategies. Moreover, LLMs appear mostly insensitive to numerical payoff-related hyperparameters in prompts, contrasting with typical human behavior."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. In this paper, the authors examine the alignment between LLMs and humans in strategic behavior through a game-theoretic perspective.\n2. The authors conduct several experiment on this topic, including LLMs like Llama-3.2-3B, Llama-3.1-8B, Llama-4-Scout, OLMo-2-13B, OLMo-2-32B, Gemma-3-12B, Gemma-3-27B, and DeepSeek-V3."}, "weaknesses": {"value": "1. For me, using game theory to evaluate large models is an ancient topic, and this article should have appeared two years ago instead of now. Therefore, in my view, this paper lack of novelty in some extend. The authors could this paper with some old paper, like Economics Arena for Large Language Models\n2. In my view, I don't see the necessity of the alignment of LLMs with empirical human reasoning, why we need to do that, since humans are also irrational, why should we make LLMs irrational? Shouldn't we look for a way to make LLMs more rational? \n3. In my view, using game theory alone to evaluate LLM is trivial and simple. How to make LLMs win in the game is important and difficult."}, "questions": {"value": "Na"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dCbgyUIxXB", "forum": "OCkJKMhIzV", "replyto": "OCkJKMhIzV", "signatures": ["ICLR.cc/2026/Conference/Submission10818/Reviewer_zhgV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10818/Reviewer_zhgV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914821260, "cdate": 1761914821260, "tmdate": 1762922022966, "mdate": 1762922022966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Large language models (LLMs) are increasingly deployed in complex decisionmaking environments. Recently researchers are focusing on purely game theory theoretical expectations, but this work analysis in other way that evaluates the strategic reasoning of Large Language Models (LLMs) by assessing their alignment with human strategic thinking. The authors test LLMs in three specific games - the ROCK,PAPER,SCISSORS(RPS),the CENTIPEDE GAME (CG), and the TRAVELER'S DILEMMA(TD) — which are all characterized by significant discrepancies between game-theoretic predictions and actual human behavior. Based on the study, the experimental results indicate that LLM behavior aligns more closely with game-theoretic rationality than with human strategies. Furthermore, the study found that LLMs exhibit limited adaptability to changes in payoff-related game hyperparameters, a trait that contrasts with typical human behavior."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "S1. The perspective from which this job discusses issues is very novel.\nS2. The paper is built on a thorough theoretical grounding, clearly contrasting classical game-theoretic predictions with established findings from human-subject experiments.\nS3. The experimental design is innovative, notably using multiple \"verbalizations\" to control for and measure the impact of prompt phrasing on strategic choices."}, "weaknesses": {"value": "W1. The motivation and conclusions lack clear directive significance, offering little guiding value for future research.\nW2. The models used in the experiment have relatively lagging capabilities and do not represent the latest or most powerful available models.\nW3. The model selection is limited to open-source models and omits comparisons with leading closed-source counterparts."}, "questions": {"value": "Q1. In the Traveler's Dilemma (TD), why did the LLMs fail to follow any consistent trend under asymmetric conditions, and how can this phenomenon be explained?\nQ2. Have you attempted to use any reasoning models to analyze how the models are actually making these decisions?\nQ3. What are the internal factors that cause different models to make different decisions? Furthermore, when different parameter-sized versions of the same model produce different results, what is the underlying cause for this discrepancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ui0HofVHIj", "forum": "OCkJKMhIzV", "replyto": "OCkJKMhIzV", "signatures": ["ICLR.cc/2026/Conference/Submission10818/Reviewer_A1QA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10818/Reviewer_A1QA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762328073264, "cdate": 1762328073264, "tmdate": 1762922022601, "mdate": 1762922022601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}