{"id": "dpPW15y3n8", "number": 20444, "cdate": 1758306225771, "mdate": 1759896977302, "content": {"title": "ATLAS: Learning to Optimally Memorize the Context at Test Time", "abstract": "Transformers have been established as the most popular backbones in sequence modeling, mainly due to their effectiveness in in-context retrieval tasks and the ability to learn at scale. Their quadratic memory and time complexity, however, bound their applicability in longer sequences and so has motivated researchers to explore effective alternative architectures such as modern recurrent neural networks (a.k.a long-term recurrent memory module). Despite their recent success in diverse downstream tasks, they struggle in tasks that requires long context understanding and extrapolation to longer sequences. We observe that these shortcomings come from three disjoint aspects in their design: (1) limited memory capacity that is bounded by the architecture of memory and feature mapping of the input; (2) online nature of update, i.e., optimizing the memory only with respect to the last input; and (3) less expressive management of their fixed-size memory. To enhance all these three aspects, we present Atlas, a long-term memory module with high capacity that learns to memorize the context by optimizing the memory based on the current and past tokens, overcoming the online nature of long-term memory models. Our experimental results on language modeling, common-sense reasoning, recall-intensive, and long-context understanding tasks support the effectiveness of Atlas compared to other modern recurrent neural networks.", "tldr": "", "keywords": ["Online Optimization", "Sequence Modeling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9b87c05cbd5c31e47ebe77f448c0df172d10f445.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper investigates into ways to improve recurrent architectures. More precisely, for recurrent architectures, the paper investigates into the memory modelling capacity, the paper made the following contributions in this area:\n- theoretical understanding of memory capacity and a novel way to increase this capacity via kernels (polynomial kernels implemented)\n- using sliding window updates instead of updates over single tokens for memory update\n- adopt a more suitable optimizer (Muon) for the proposed architectures\n\nExperimental results show that the proposed changes translate to general task improvements as well as to the long context task, measured via some RULER benchmarks. The proposed changes are further ablated to show their effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper has made contributions with a focus of long term tasks for recurrent architecture memories. The investigation into expressivity seems to be the main novelty of the paper since it contains a novel technical solution (polynomial kernel) with solid theoretical motivations. The sliding window idea has already been implemented in architectures such as MAG and MAL but the paper makes further steps to integrate the idea into a single architectures. \n\nAll the contributions have been tested not only in long context tasks (which is the focus of the paper in motivations) but also on other popular benchmarks to give an overview of the model performance. The proposed components and contributions have been mostly properly ablated."}, "weaknesses": {"value": "From Table 2, Muon does not offer great improvement and it seems this is not a central contribution from the paper. If my understanding is correct, it seems this part can be removed from the main section of the paper. \n\nMaybe this is more of a question, why Atlas++ results are not shown in Table 2?"}, "questions": {"value": "MAG and MAL architectures contain sliding window improvement (in spirit similar to Omega rule proposed in this paper), so:\n- Why MAG and MAL gives further improvement over Atlas which already has this sliding window aspect included in its design (i.e. Table 1 comparinsg MAG/MAL to Atlas)\n- Is there any computational advantages that the current paper can claim over MAG/MAL which are hybrid models? It seems to me the answer is positive."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pou2w7pFid", "forum": "dpPW15y3n8", "replyto": "dpPW15y3n8", "signatures": ["ICLR.cc/2026/Conference/Submission20444/Reviewer_Fg6k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20444/Reviewer_Fg6k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761511396291, "cdate": 1761511396291, "tmdate": 1762933887516, "mdate": 1762933887516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Atlas, a new long-term memory module for sequence models based on test-time optimization of memory states. Atlas tries to fix three common limits of modern recurrent networks. First, limited capacity of memory states by using polynomial kernels. Second, it writes local updates to memory with a context-aware sliding window called Omega, recurrent update rule based not only the current token. Third, more expressive memory update rule with updates from Muon optimizer instead of GD. The authors also propose a family of related models (DLA, SWLA, Omega, Atlas) and compare them with previous works such as Titans and modern recurrent models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Nice idea and clean motivation. The paper clearly explains the problems with memory state capacity, state update locality, and state optimization and how Atlas addresses them.\n- Systematization of prior work. It provides a helpful, unified perspective on earlier memory modules through attentional bias and test-time optimization (particularly Table 3)\n- Results on language modeling, language understanding and needle-in-a-haystack tasks show strong performance. Ablation study supports design choices."}, "weaknesses": {"value": "- Impact of polynomial mapping (value of p) on benchmarks results is not clear, especially on long context ones and MAD tasks, where larger memory capacity should show its benefit.\n- Features claimed for Omega (sliding window, global context-aware memory updates) are already in Block-Recurrent transformers, RMT, ARMT, and MELODI, which use the Transformer as a recurrent cell. Therefore, the novelty in this case is unclear. The text still lacks this discussion.\n- On BABILong, results are shown (Fig. 6) but ARMT is missing. The BABILong paper (Figure 1, Table 2) reports ARMT as the top model (~77 average accuracy on QA1-5 at 10M tokens) and shows its scaling to 50M tokens. Please include and discuss ARMT for completeness.\n- Evaluations on long-contexts, which are one of the key stated features of Atlas model, are only up to 16k tokens on the RULER benchmark. Currently, 16k tokens is not a very long context. More impressive Atlas results are shown on the BABILong benchmark in the appendix, with strong performance on up to 10 million tokens.\n- No results are reported with standard deviations, and the authors do not indicate how many runs were performed. Many reported numbers are close to those of other methods."}, "questions": {"value": "- Please address limitations as questions.\n- what exact value of p (polynomial mapping) is used in experiments, it is not mentioned in the paper.\n- several Titans variants exist (MAC, MAG, MAL); why was a weaker one used? Could you add discussion and comparison with the strongest published versions. Add Atlas MAC, MAG, MAL vs Titans MAC, MAG, MAL comparison.\n- SWDT, what is it? Table 1, Table 5. Where are the results of SWLA?\n- Are reported scores zero-shot or after fine-tuning (e.g., Table 1, 2)? Please specify for each task.\n\n\ntypos:\n- L210, l^(1) misses non-linearity \\phi applied to key ?\n- Fig. 11, y label should be perplexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9vDi07zE4p", "forum": "dpPW15y3n8", "replyto": "dpPW15y3n8", "signatures": ["ICLR.cc/2026/Conference/Submission20444/Reviewer_KJpp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20444/Reviewer_KJpp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988187941, "cdate": 1761988187941, "tmdate": 1762933886530, "mdate": 1762933886530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ATLAS, a recurrent long-term neural memory module that (i) increases capacity via higher-order (polynomial) key/query features, (ii) replaces online token-wise updates with an Omega sliding-window rule intended to memorize the context, instead of tokens, and (iii) improves memory management by using a Muon/second-order–like inner optimizer. Models (OMEGANET, ATLAS, and hybrid MAG/MAL variants) are evaluated on language modeling, common-sense reasoning, RULER NIAH, and several recall tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Theory is interesting and clearly framed. The capacity analysis (matrix vs deep memory; effect of polynomial mappings) and the Omega sliding-window objective are neat, well-motivated contributions.\n\n2. Broad empirical sweep with solid baselines from modern RNN families; results on RULER NIAH and standard LM/CS tasks are competitive, often best among non-hybrids."}, "weaknesses": {"value": "1. Key claims are not directly stress-tested. The headline contribution - “a long-term neural memory module with high capacity and the ability to memorize the context, instead of tokens\" - is asserted, but experiments do not isolate these mechanisms. Per-benchmark wins and one ablation (showing c=1 hurts) are helpful, yet there is no targeted evaluation that would uniquely validate context-level memorization vs token-wise memorization (e.g., tasks constructed so token-level memorization provably fails while context-level succeeds), nor a controlled capacity–vs–memory-size curve demonstrating the super-linear capacity predicted by the theory. The paper cites MAD/MQAR/recall results in the appendix, but these still read as broad benchmarks rather than diagnostic tests tied to the two claims above; the manuscript does not analyze why Omega/polynomial/Muon are winning on those axes.\n\n2. Baseline gap for Transformers. Table 1 uses an internal Transformer++ control, but omits widely used, publicly available ~1B open-weight Transformers (e.g., LLaMA-/Qwen) trained on comparable token budgets. If ATLAS is posited as a RNN alternative for Transformers, it should be compared head-to-head with similarly sized, well-known Transformer baselines under matched data/compute. \n\n3. Sparse discussion/analysis. The results sections are super brief and largely descriptive (“we attribute this to memorizing context / polynomial kernels”). There is little error analysis (where ATLAS fails vs Titans/DeltaNet/Transformers), no probing of Omega gates γ, and limited introspection on the Muon inner optimizer beyond citing second-order intuition.\n\n4. Muon ablation paradox (Fig. 2). Muon is introduced to avoid poor local optima that hurt long-context performance, yet removing Muon improves perplexity on the LM task in Fig. 2. This apparent contradiction is neither analyzed nor discussed.\n\n5. Hybrids (MAL/MAG) lack description in this paper. Scores are reported for MAL and MAG hybrids of ATLAS, but the paper does not explain what MAL/MAG are or why they matter beyond citing prior work. Including hybrid results without even a brief motivation and description makes them difficult to interpret and dilutes the empirical story. Either describe them succinctly (what they add, how they interact with ATLAS) or move them to appendix with clearer rationale.\n\n6. Related work coverage. Given the positioning, I expected a clearer discussion of Transformer variants with segment-level recurrence/sliding-window attention as alternative ways to achieve context-level optimization (e.g., SWA/Recurrent Memory Transformer/Associative Recurrent Memory Transformer and others). Segment-recurrent transformers have memory for context and thus directly related to one of the core contributions of the paper - token vs. context memorisation. The paper notes the connection in passing but does not compare empirically to such Transformer baselines."}, "questions": {"value": "1. Can you design diagnostic tasks that decisively require context-level memorization (not token-wise), and report ATLAS vs c=1 and token-memorizing RNNs on them?\n\n2. Please provide a capacity study: number of independent (key,value) pairs recovered vs memory size/degree-p features, with and without Muon. This would directly test the “high capacity” claim.\n\n3. Why are LLaMA/Qwen-scale Transformer baselines absent for ~1B models? Could you add matched-compute comparisons (same tokens/context window) to strengthen the “Transformer alternative” narrative?\n\n4. What are MAL/MAG, why include them, how do they interact with Omega/polynomial features?\n\n5. Why does removing Muon yield better LM perplexity in Fig. 2 if Muon is meant to avoid bad local optima in long contexts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HJh3jxIA3p", "forum": "dpPW15y3n8", "replyto": "dpPW15y3n8", "signatures": ["ICLR.cc/2026/Conference/Submission20444/Reviewer_Bhve"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20444/Reviewer_Bhve"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994892371, "cdate": 1761994892371, "tmdate": 1762933885819, "mdate": 1762933885819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}