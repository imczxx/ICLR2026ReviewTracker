{"id": "gSNq3jcNCW", "number": 20041, "cdate": 1758301812209, "mdate": 1759897004644, "content": {"title": "ImbalancE: Inference-Time Latent Search against Degree Imbalance in Link Prediction", "abstract": "Knowledge Graph Embedding models have been extensively used to learn representations of entities and relations in Knowledge Graphs for predicting missing links. However, the quality of the learned representations varies a lot across different areas of the same Knowledge Graph. If previous research efforts have loosely linked the problem to relation types or degree bias, we show that it is much more widespread, and it more precisely lies in the degree imbalance of the entities in test triples. In particular, we show that the prediction of a target entity that has a degree much smaller than the degree of the anchor entity is extremely problematic. This is critical in use cases like \\textit{drug target discovery}, where these triples are predominant, or \\textit{recommender systems}, where they represent important corner cases.\nTo address this issue, we propose an inference-time latent search optimization method capable of significantly improving model predictions on the most imbalanced triples. Built on top of a pre-trained model, it explores the embedding space at evaluation time, blending known and out-of-band information to mitigate the degree imbalance bias. We show the value of our approach on imbalanced triples from common benchmark datasets, where we outperform conventional methods, opening the door to the successful adoption of Knowledge Graph Embedding models on these critical corner cases.", "tldr": "We scope the degree imbalance problem that affects knowledge graph embedding models for knowledge graph completion and we propose ImbalancE, an inference-time latent search method to address this widespread issue", "keywords": ["Knowledge Graph", "Link Prediction", "Degree Imbalance", "Knowledge Graph Completion", "Latent Search", "Knowledge Graph Embeddings"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d528a50914bc2a9beeaf16f22542edbfcdd04899.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper identifies degree imbalance as a key weakness in Knowledge Graph Embedding (KGE) models—low-degree entities are poorly predicted when paired with high-degree ones. To fix this, the authors propose IMBALANCE, an inference-time latent search that refines entity embeddings using query-specific context and oracle-generated triples. Experiments on benchmark datasets show that IMBALANCE significantly improves link prediction for imbalanced triples."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This paper addresses Knowledge Graph Completion (KGC) under conditions of large degree differences between entities, which is a novel and practically valuable problem.\n- This paper introduces IMBALANCE, a practical inference-time optimization method that enhances pretrained models without requiring retraining. \n- The proposed method can be applied to existing KGE models and demonstrates good scalability.\n- Extensive experiments on benchmark datasets show significant and consistent improvements, proving the method’s effectiveness."}, "weaknesses": {"value": "- The datasets evaluated in this paper are all standard benchmark KGC datasets, while the introduction mentions practical scenarios such as drug–target discovery and recommender systems. It would be valuable to further investigate the method’s performance on knowledge graphs from these real-world applications.\n- Since the proposed approach introduces external knowledge through an oracle or LLM-based augmentation, the comparison with baselines may not be entirely fair. It is recommended to include baseline models that also leverage external knowledge for a more equitable evaluation.\n- The training-context term only incorporates related (s,p,?) triples. How does the method handle potential noise in the knowledge graph? How is the importance of these triples determined?\n- Apart from the FB15k‑237 dataset, the performance gains on the other two datasets are relatively small. Additional ablation studies on these datasets would help strengthen the experimental evidence.\n- The number of collected test samples seems limited. \n- It is suggested to express the results in percentage form to clarify the proportion of improved cases."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eF6f0I7mIt", "forum": "gSNq3jcNCW", "replyto": "gSNq3jcNCW", "signatures": ["ICLR.cc/2026/Conference/Submission20041/Reviewer_Hjxb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20041/Reviewer_Hjxb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20041/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761106243578, "cdate": 1761106243578, "tmdate": 1762932938135, "mdate": 1762932938135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies degree imbalance (large head–tail degree gaps) as one of the key reason that the current KGE models underrank low-degree entities and their performance collapses as the normalized degree difference grows. They propose IMBALANCE, a test-time latent search plug-in: for a query q, run a few gradient steps only on the anchor and a few oracle-suggested candidate targets, optimizing a two-term positive-only loss 1) training-context triples for the anchor and 2) oracle triples. After that, they rank with the updated embeddings. The oracle uses text embeddings of entity labels/descriptions to retrieve plausible candidates (precomputed; cosine-based). On hard High↔Low subsets, IMBALANCE markedly boosts MRR/Hits@k across ComplEx/RotatE/TransE/DistMult (e.g., FB15k-237) and both loss terms matter in ablations. An qualitative example shows rank improving 67 → 3 after the latent search."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis paper represents a new test-time adaptation method in the domain of KGE. Without any further training on all embeddings, they locally and temporally fine-tune the anchor embeddings for each test query. \n2.\tClear diagnosis of the failure mode. They bin test triples by normalized degree difference and show performance drops as the gap grows (Figure 2), cleanly isolating the imbalance effect beyond relation-type heuristics. They track per-epoch embedding movement and uncover two low-degree failure modes-overfitting (premature convergence) and non-convergence (oscillation), which explain why low-degree answers are misranked. (Figure 3, analysis.) \n3.\tPerformance gains on the hardest High-Low cases across multiple models/datasets. Ablations show both context and oracle are needed.\n4.\tOracle triples are precomputed; the method is model-agnostic and lightweight compared to retraining."}, "weaknesses": {"value": "1.\tOracle dependency. The improvements partially come from text (labels/descriptions). No comparison to text-augmented training baselines is provided, so the share of gain from the LLM side vs. the latent search isn’t isolated. \n2.\tMBALANCE's effectiveness depends on the choice and size of the training-context subset. The paper defines C_q (triples sharing the anchor and relation) but does not provide a principled selection policy or sensitivity analysis. For high-degree anchors, C_q can be large; different subsampling strategies (diversity-, hardness-, or degree-aware) may materially change both accuracy and latency. Please report results as |C_q| and the selection rule varies\n3.\tEven though no heavy re-training is needed for each inference, the proposed method still needs a few steps of gradient updates for each test query. This may cause significant latency compared to the other baselines."}, "questions": {"value": "1.\tPlease specify the exact construction of the training-context set C_q: how many triples, how selected (by relation, recency, degree, similarity?), and whether selection differs by model/dataset. Your Algorithm 1 references C_q, but the selection criteria aren’t fully explicit. \n2.\tAfter the per-query latent search, do you re-rank over all entities or only over the oracle candidates? Algorithm 1 says \"extract ranked list of candidates\" but does not spell out the candidate set used for evaluation. Please clarify. \n3.\tYou avoid negatives for efficiency. Did you observe any degenerate behaviors (e.g., indiscriminate score inflation) without a contrastive term, and how do you regularize against that? You note that Oracle triples may include false positives that can surface in ranking. \n4.\tHow often did that occur in practice, and what guardrails (e.g., confidence thresholds, re-scoring) reduce harm? Any quantitative analysis? \n5.\tYou acknowledge IMBALANCE requires a non-empty training context for the anchor. How does performance degrade as context shrinks, and can a \"pure-oracle\" fallback (no context) recover some of the gains? \n6.\tSince the oracle injects textual knowledge, please compare against text-aware KGE (e.g., adding description embeddings at training) to isolate the unique value of test-time latent search vs. simply having more information. \n7.\tPlease provide per-query latency for a typical T, and how cost scales with |\\Omega_q|=m and context size."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6kaIju6SxC", "forum": "gSNq3jcNCW", "replyto": "gSNq3jcNCW", "signatures": ["ICLR.cc/2026/Conference/Submission20041/Reviewer_Vq7R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20041/Reviewer_Vq7R"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20041/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761400590554, "cdate": 1761400590554, "tmdate": 1762932937605, "mdate": 1762932937605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies and analyzes the degree imbalance problem in Knowledge Graph Embeddings (KGEs)—the phenomenon where test triples with large degree differences between entities (e.g., high-degree head vs. low-degree tail) yield poor link prediction performance.\nIt introduces IMBALANCE, an inference-time latent search procedure that fine-tunes entity embeddings using a dual-term objective: (1) a training-context term to re-optimize the anchor entity, and (2) an oracle-enhanced term leveraging external LLM-derived triples to bias low-degree entities toward the query. Experiments on FB15k-237, WN18RR, and YAGO3-10 show substantial improvements in MRR and Hits@N for imbalanced triples, especially on FB15k-237."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1) The authors systematically connect prediction degradation to degree imbalance, offering solid empirical evidence (e.g., Figure 2, showing sharp MRR drop-offs by degree difference bins).\n\n2) Simple yet general idea: The method can be plugged into any pretrained KGE without retraining, adding post-hoc refinement.\n\n3) Dual-objective design: Separating the anchor refinement and oracle biasing terms is conceptually sound and empirically validated through ablations.\n\n4) Application relevance: Addresses a realistic pain point for long-tail link prediction (e.g., drug discovery, recommender systems)."}, "weaknesses": {"value": "1) Limited novelty. The inference-time latent search idea is borrowed almost directly from prior optimization-based refinement methods (e.g., Bonnet & Macfarlane 2024). The contribution here is more contextual adaptation to KGEs than methodological innovation. The optimization objective is simplistic (a score-sum without any new regularization or causal insight)\n\n2) Outdated backbone models. The work only tests on ComplEx-N3, and RotatE. State-of-the-art KGE methods in 2020-2025 such as NBFNet are absent, though I see that the method can be pluged into more models. \n\n3) The authors evaluate IMBALANCE only as a plug-in to their own reimplementation, not against competing post-hoc correction or de-biasing methods (e.g., Shomer et al. 2023.Toward Degree Bias in Embedding-Based\nKnowledge Graph Completion). Even simple baselines such as degree-aware normalization or dropout fine-tuning are missing.\n\n4) Datasets are small and classical (FB15k-237, WN18RR, YAGO3-10). The improvements on YAGO3-10 are marginal (MRR unchanged, +0.02 H@10). The oracle fails on text-only datasets, suggesting brittleness. The gains largely come from low-baseline models (ComplEx/RotatE underperforming due to degree bias)."}, "questions": {"value": "1) How does IMBALANCE compare to other debiasing or reweighting methods (e.g., degree-regularized KGEs or post-hoc calibration)?\n\n2) Why restrict to classical models? Could modern transformer-based KGE models also benefit?\n\n3) How sensitive is the method to oracle noise? Have you tested with random oracle triples?\n\n4) What is the runtime cost per query, and how does it scale to large KGs (e.g., Wikidata5M)?\n\n5) Would a simple degree-normalized reweighting baseline achieve similar improvements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CLWsGHlmfW", "forum": "gSNq3jcNCW", "replyto": "gSNq3jcNCW", "signatures": ["ICLR.cc/2026/Conference/Submission20041/Reviewer_VZ92"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20041/Reviewer_VZ92"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20041/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763038121, "cdate": 1761763038121, "tmdate": 1762932937228, "mdate": 1762932937228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors focused on the systematic performance degradation on triples involving entities with high degree imbalance. The proposal of an inference-time latent search method, IMBALANCE, to mitigate this bias is novel within the KGE domain. The empirical results showing significant gains on the hardest subset of test triples are compelling. However, the proposed objective function, combined with the core mechanism of gradient updates, introduces several critical theoretical and practical concerns that need to be addressed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a good analysis, defining the problem not merely as \"degree bias,\" but specifically as degree imbalance between head and tail entities in test triples. The analysis linking this issue to two distinct learning failures (overfitting and failed convergence) in low-degree nodes is insightful.\n\n2. The reported improvements on the highly imbalanced splits of FB15k-237 (metrics improving by factors up to x6) demonstrate the effectiveness on critical corner cases."}, "weaknesses": {"value": "## Weakness 1\n\nMy primary theoretical concern lies in the structure of the dual-term objective function:\n\n$$L(q = (s, p, ?)) = \\sum_{t^+ \\in \\mathcal{C}_q} f(t^+) + \\sum_{t \\in \\Omega_q} f(t)$$\n\nSince the objective is purely maximization over a set of known positive facts ($\\mathcal{C}_q$) and oracle-suggested positive facts ($\\Omega_q$), and there are no negative samples (or margin/regularization terms that push away incorrect entities), this optimization procedure is fundamentally unstable and prone to collapse or severe overfitting during the inference-time fine-tuning.\n\nThe gradient updates aim to maximally increase the similarity (score $f(t)$) between the refined anchor embedding $s$ and all associated target embeddings ($o$ in $\\mathcal{C}_q$ and $o_j$ in $\\Omega_q$). If the number of refinement steps $T$ is too large, the embeddings could converge to a state where $s$ and all related target entities are pushed maximally close together. This would result in poor discrimination against other entities in the graph (i.e., non-target entities not included in $\\Omega_q$) when the final ranking is performed.\n\nThe authors' defense that \"our approach does not require any negatives, thus circumventing the problem of their synthetic generation\" is not a justification for stability. The inclusion of a stability mechanism (like a margin, or pushing away local negatives) is essential so as to prevent convergence to trivial solution (that is move all embeddings into the same exact location). \n\nThe paper needs a detailed discussion or an ablation study showing how the parameters for training (e.g., number of epochs, and learning rate) affects the stability of the optimized embeddings.\n\n## Weakness 2\n\nThe reliance on LLMs as an oracle to generate the set of additional introduces critical source of bias and limitations. The problem is that LLM is used as Filter for finding candidate queries.  The paper justifies the oracle by needing to enhance the small amount of knowledge available for low-degree entities. However, by selecting the top-$m$ entities closest to a barycenter, the LLM effectively acts as a gatekeeper for what constitutes a plausible answer. Any correct, low-degree answer that the misses will be excluded from $\\Omega_q$ and therefore will not benefit from the refinement process. Instead of allowing the KG structure to guide the prediction and merely correcting the embedding landscape, the method constrains the space of beneficial answers to those already deemed similar or plausible by the LLM. This is a significant limitation, especially if the KG is used to \"complement\" the LLMs.\n\nThe lack of specific technical details on the LLM's performance (e.g., precision/recall of the generated triples $\\Omega_q$ against the actual test set) makes it difficult to assess the quality of this filtering step.\n\n## Weakness 3\n\nThe primary evidence (Figure 2) on the degree imbalance issue (performance systematically decreases as the normalized degree difference increases) is not clearly monotonic and appears to be heavily dependent on both the dataset and the underlying KGE model. No quantitative analysis is performed on this statement. While the overall trend is that the highest imbalance bins (e.g., the far left and far right) have the lowest MRR (on either object or subject corruption), there are significant counter-examples for FB15k-237, this trend did not hold true for YAGO3-10 (bins in far right, for example).  The lack of alignment in the patterns imply that there are different mechanisms at play that explain the low performance of these queries."}, "questions": {"value": "1. What happens when we increate the number of iterations (W1)\n2. How sensitive the proposed solution is to the LLM's knowledge. What if we have queries that are imbalanced and LLMs do not know."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8PNaO4O30n", "forum": "gSNq3jcNCW", "replyto": "gSNq3jcNCW", "signatures": ["ICLR.cc/2026/Conference/Submission20041/Reviewer_YMSQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20041/Reviewer_YMSQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20041/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761770189083, "cdate": 1761770189083, "tmdate": 1762932936736, "mdate": 1762932936736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}