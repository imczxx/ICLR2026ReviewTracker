{"id": "1QaTq2NXD6", "number": 2529, "cdate": 1757136527904, "mdate": 1759898142731, "content": {"title": "ACTIVE-o3 : Empowering MLLMs with Active Perception via Pure Reinforcement Learning", "abstract": "Active vision, also known as active perception, refers to actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. With the rise of Multimodal Large Language Models (MLLMs) as central planners in robotic systems, the lack of methods for equipping MLLMs with active perception has become a key gap. We first provide a systematic definition of MLLM-based active perception tasks and show that GPT-o3's zoom-in strategy can be viewed as a special case, though it suffers from low efficiency and inaccurate region selection. To address these issues, we propose Active-o3, a reinforcement learning framework built on GRPO that equips MLLMs with active perception capabilities. Leveraging a modular sensing-action design and a dual-form reward, Active-o3 autonomously learns efficient and stable region selection strategies without explicit supervision. We further establish a comprehensive benchmark covering both open-world tasks (small/dense-object grounding) and domain-specific scenarios (remote sensing, autonomous driving, interactive segmentation). Experimental results demonstrate that Active-o3 significantly enhances active perception capabilities compared to Qwen2.5-VL-CoT. Moreover, we show that our RL framework not only preserves the model’s general understanding ability but can also serve as a proxy task for leveraging perception data, further improving performance on benchmarks such as RealWorldQA. We hope that our work can provide a simple codebase and unified evaluation protocol to facilitate future research on active perception with MLLMs.", "tldr": "", "keywords": ["vlm", "RL", "reasoning", "MLLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc2ca93498216e0592deb06651ae9f20cdcab1ab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to develop a novel framework and methodology for active object detection on CoT reasoning with MLLM and online adaption. However, when moving to core technological parts, it scales down to visual grounding topic on 2D static image without update of camera pose. It presents a novel prompt format for CoT object detection on MLLM. In the experiment, it shows the effectiveness by comparison with just one baseline (Qwen2.5-VL-CoT), lack complete benchmarking on SOTA performance on leaderboard benchmark datasets (e.g., RefCOCO/+/g)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Investigating novel approach of Visual CoT on frontier MLLM for object grounding."}, "weaknesses": {"value": "The research is interesting but not completed. First, once scaled-down to scope of CoT for visual grounding, it may have to review and compare with recent progresses of Visual CoT and visual grounding, and in experiments, formal and systematic evaluations on representative benchmarks (e.g., RefCOCO/+/g) have to be performed and reported. Related papers like: “ARGUS: Vision-Centric Reasoning with Grounded Chain-of-Thought”, “Visual Chain-of-Thought Prompting for Knowledge-Based Visual Reasoning”."}, "questions": {"value": "What are the relations of this paper with recent researches on Visual CoT visual grounding? What are the distinctive novel parts in Prompting the visual CoT and reinforcement learning? Have the experiments and evaluations on Visual Grounding benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LklUeLYwE3", "forum": "1QaTq2NXD6", "replyto": "1QaTq2NXD6", "signatures": ["ICLR.cc/2026/Conference/Submission2529/Reviewer_YXgk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2529/Reviewer_YXgk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843929191, "cdate": 1761843929191, "tmdate": 1762916269265, "mdate": 1762916269265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reinforcement learning approach to identifying regions on an image on which a multi-modal LLM should zoom to obtain the information needed for its task, which can be framed under the problem of active perception. Specifically, the RL agent is trained through GRPO to produce several regions in parallel in a single-step RL environment, and the reward is a mix of task-based score (how well did the MLLM answer the task after having seen the zoomed images) and heuristics (region size, overlap, …) to guide the process. This is tested over several datasets and shows a good performance compared to CoT baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The approach in this paper is appropriate for the problem at hand. The paper is clearly written and conveys the method, justification and results both pleasantly and precisely. The main figure clearly explains the method’s core elements (although the colour style and the use of emojis are slightly unconventional for an academic paper). The experiments are well designed and sound, and the results support the claim. They are also well specified and should be reproducible given that the code is released as promised.\nOverall, without bringing groundbreaking novelty, this paper is a good example of adapting existing methods to tackle a problem in a new way, with good enough results that it is relevant for the community in this field.\nI also want to underline the Ethics Statement, which shows a good reflection about the potential issues related to this and other approaches to active perception."}, "weaknesses": {"value": "There is no strong weakness for this paper, except that the contribution is generally limited to adapting an existing method to a specific use-case. \nThe main issue I see is the angle the paper takes - framing the problem as a general active perception but then making several assumptions in a row to focus in the end on the problem of zooming for MLLMs. The method that is proposed is too tailored for this specific problem to be applicable for many other instances of active perception, to the point that I do not think the general definitions in section 3 are relevant. For example: the modular view of active perception separates the effects on the environment from the effects of moving the camera. This does not apply for example to robots which must change position to move the camera. Then, in section 4.1, the key property of 2D visual scenarios is used to fix the task model and focus on the sensing policy. This means that one part of the sequential nature of the problem is broken based on this specific use case. Finally, the authors decide to completely eliminate it by producing parallel region selections, instead of sequential ones. These choices are OK for the specific task of task-related region identification in images, but the whole sequential model described is not relevant, because the approach would not fit it at all.\n\nThis is a bit frustrating as a reader as the scope of the paper seems to shrink as we read it, and could be better handled by simply defining the specific problem as the one being tackled from the beginning. Yet, this is clearly not a major flaw."}, "questions": {"value": "It has not happened often to me as a reviewer that I don’t have any questions after reading a paper, but this is the case here. This is in part due to the incremental/applied nature of the contribution, but also and mainly because I found it very clear, coherent and self-consistent.\n\nSmall comments:\n- Figure 1: The *Think* output says “I will define three distinct regions”, but the *Answer* text and the image only have two. \n-  Section 5.1, “*a variant of ACTIVE-O3 to conduct a comparison. (see Figure 8 for visualization result […])*” there is a misplaced “.”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "As well mentioned in the Ethics Statement provided by the authors, this approach could lead to misuse for privacy and surveillance, as well as be subject to biases or lead to over-reliance/misbehaviour.\nYet, I believe these issues are generally true for most computer vision and MLLM research and I do not believe them to be specific to the approach they propose."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mkG0lsg8Wo", "forum": "1QaTq2NXD6", "replyto": "1QaTq2NXD6", "signatures": ["ICLR.cc/2026/Conference/Submission2529/Reviewer_8T9r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2529/Reviewer_8T9r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925933648, "cdate": 1761925933648, "tmdate": 1762916269046, "mdate": 1762916269046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Active-o3, a RL framework built on GRPO, equiping VLMs with active perception capabilities. The authors decompose active perception into a sensing module that selects informative regions and a task module that executes specific actions. Active-o3 uses a dual-form reward to train the sensing policy. Experiments across multiple benchmarks demonstrate improvements over baselines. The authors also show that the trained model preserves and even enhances general reasoning capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a clear and systematic definition of active perception, which is important for VLM research.\n- The paper includes extensive ablations on reward design, training data combinations, and robustness to random seeds."}, "weaknesses": {"value": "- The framework is GPRO with a specific reward design. It seems just like an application of GRPO.\n- Most baselines are training-free. A reasonable baseline is training the model directly with GRPO on the same data and comparing both results.\n- Recent RL-based VLM thinking with image works should also be considered as baselines."}, "questions": {"value": "1. Authors argue Active-o3 is \"the first reinforcement learning framework for active perception with MLLMs\" (Line 122-123). I am a little confused why works like [1] and [2] are not considered as this category.\n2. Can authors provide computational cost comparisons between Active-o3 training and directly training the VLM with GRPO?\n\nLine 53: Figures -> Figure\n\n[1] Grounded Reinforcement Learning for Visual Reasoning\n[2] GRIT: Teaching MLLMs to Think with Images"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qSRoHPB6Zh", "forum": "1QaTq2NXD6", "replyto": "1QaTq2NXD6", "signatures": ["ICLR.cc/2026/Conference/Submission2529/Reviewer_iQkB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2529/Reviewer_iQkB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980397546, "cdate": 1761980397546, "tmdate": 1762916268734, "mdate": 1762916268734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper solve the MLLM-based active perception with a reinforcement learning framework built on GRPO that equips MLLMs with active perception capabilities named ACTIVE-O3. This framework autonomously learns efficient and stable region selection strategies without explicit supervision. Experimental results demonstrate that ACTIVE-O3 significantly enhances active perception capabilities compared to Qwen2.5-VL-CoT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces an interesting pipeline to achieve active perception, and the results prove that this method significantly improves perception quality and task performance across both general-purpose and domain-specific visual tasks.\n\n2. This paper provides a list of hierarchical reward functions to guide model learn how to achieve active perception."}, "weaknesses": {"value": "1. This paper carefully designs the reward function for RL, however, it only provides ablation results to prove the effectiveness of these reward function. Could this paper provide more qualitative experimental results to demonstrate the effect of different rewards on the model's active perception capability?\n\n2. This paper only utilizes GRPO for active reinforcement learning. What is the performance of other RL methods in the active perception task?"}, "questions": {"value": "As shown in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OC8FXXjXXg", "forum": "1QaTq2NXD6", "replyto": "1QaTq2NXD6", "signatures": ["ICLR.cc/2026/Conference/Submission2529/Reviewer_pfeh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2529/Reviewer_pfeh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762143336888, "cdate": 1762143336888, "tmdate": 1762916268498, "mdate": 1762916268498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}