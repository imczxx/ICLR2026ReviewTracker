{"id": "3GNaXptttK", "number": 7813, "cdate": 1758037253579, "mdate": 1759897830870, "content": {"title": "Head-Level Mechanistic Attribution for Hallucination Control: Training-Free Counteractive Pruning in LVLMs", "abstract": "Large vision-language models (LVLMs) excel at multimodal tasks but often generate instance-level object hallucinations, describing nonexistent objects.\nSince existing methods overlook functional conflicts within attention heads and lack principled, fine-grained attribution and intervention at the head level, hallucination suppression is often accompanied by a substantial loss of semantic informativeness.\nTo overcome these limitations, we propose HACP, a unified framework that enables fine-grained internal hallucination control via precise intervention at the attention head level. Specifically, we introduce InfoSpectralScore, a novel attribution metric based on eigen-decomposition with spectral variance and entropy penalties, which allows for the accurate identification of hallucination-inducing heads. We further develop a dynamic, training-free pruning strategy that adaptively suppresses hallucination-prone heads while reinforcing faithful heads during inference. \nExtensive experiments across multiple LVLMs and benchmarks demonstrate that HACP achieves state-of-the-art hallucination mitigation, substantially reducing hallucinations while better preserving caption informativeness compared to existing approaches, thus offering a robust and transferable solution for controllable and interpretable multimodal generation. The source code will be released upon acceptance.", "tldr": "We introduce a fine-grained attribution and pruning method for vision-language models that substantially reduces object hallucinations while preserving caption informativeness, without additional training.", "keywords": ["Vision-Language Models;Object Hallucination;Attention Head Attribution;Dynamic Pruning;InfoSpectralScore"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7248f7b52196587e31a1cae113b7cd388913c666.pdf", "supplementary_material": "/attachment/bd614ff8de72c7c6dc0c3b091c1a64f93d4f5245.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a training-free method to mitigate hallucinations in LVLMs. The approach includes head-level pruning by identifying head-level semantic attribution via a self-defined infospectral score and reweighting the output vector of each head by pushing it away from hallucinated directions and toward faithful directions."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a principled, fine-grained attribution and intervention framework at the head level to mitigate object hallucination, which is an interesting direction."}, "weaknesses": {"value": "1. The writing in both the method and experimental sections is quite confusing. A major revision is recommended to improve clarity.\n2. The method itself is not intuitive, well motivated, or mathematically solid. The method is limited in downstream application. The estimation of statistical variables (e.g., hallucinated direction, infospectral score threshold) depends on a batch of samples, which makes it difficult to adapt to cases where only a single image is available.\n4. It would be helpful to include more recent LVLMs such as Qwen2.5-VL and LLaVA-v1.6 in the experimental results.\n5. Both CHAIR and POPE focus only on object existence hallucination. Including a more extensive benchmark such as MME[1] would strengthen the paper.\n\n[1] MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"}, "questions": {"value": "1. Line 122: “LVLMs still suffer from instance-level hallucinations, largely due to functional conflicts among attention heads.” Is this supported by prior work, or is it an overclaim?\n2. How do you define the hard100 subset of POPE evaluation? Why report only this subset instead of the full set as in previous papers?\n3. Line 363: “For each model, we define three attribution layer groups: LLaVA uses [5,18], [19,26], and a merged [5,26]; Shikra-7B uses [3,13], [14,28], and a merged [3,28].” Could you clarify what this grouping means and how it’s used in the method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r3ACv6ufTu", "forum": "3GNaXptttK", "replyto": "3GNaXptttK", "signatures": ["ICLR.cc/2026/Conference/Submission7813/Reviewer_5mJG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7813/Reviewer_5mJG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760743490028, "cdate": 1760743490028, "tmdate": 1762919854391, "mdate": 1762919854391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the critical problem of instance-level object hallucination in Large Vision-Language Models (LVLMs), where models describe objects not present in the visual input. The authors argue that current mitigation techniques often force a trade-off, reducing hallucinations at the cost of semantic informativeness (i.e., caption quality and detail). \n\nTo address this, the paper proposes HACP (Head-level Attribution for Counteractive Pruning), a unified, training-free framework that operates at inference time. The framework has two core innovations:\n\n1. InfoSpectralScore: A novel, semantics-based attribution metric designed to identify both \"hallucination-inducing\" and \"faithful\" attention heads. This metric is derived from an eigen-decomposition of head-level output embeddings and is regularized by spectral variance and entropy penalties to capture semantic capacity.\n\n2. Dynamic Counteractive Pruning: A new intervention strategy. Instead of just ablating (zeroing out) problematic heads, this method dynamically \"suppresses\" the output of hallucination-prone heads while simultaneously \"reinforcing\" the output of faithful heads during inference.\n\nThe authors conduct extensive experiments on multiple LVLMs (e.g., LLaVA, Shikra, Qwen) and benchmarks (CHAIR, POPE). The results demonstrate that HACP achieves a new state-of-the-art in hallucination mitigation, significantly reducing hallucination scores while, crucially, preserving or even improving semantic informativeness (F1 score) compared to prior methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a significant and high-impact weakness of LVLMs. The core idea of \"functional conflicts\" among attention heads is an insightful way to frame the hallucination problem, and the goal of breaking the trade-off between faithfulness and informativeness is a key challenge for the field.\n\n2. The proposed method is well-motivated and novel. The InfoSpectralScore is a principled, semantics-based metric that goes beyond simpler attribution methods like KL divergence. Its construction from spectral decomposition, variance, and entropy (Eq. 7) is a solid methodological contribution. The Counteractive Pruning strategy is a clear improvement over conventional ablation. The idea of not just silencing bad heads but also amplifying good ones (Eq. 12) is intuitive and, as shown by the results, highly effective.\n\n3. The experimental results are the paper's strongest point. The method is shown to be effective across multiple models and benchmarks. The key finding, highlighted in Tables 3-5, is that HACP breaks the faithfulness-informativeness trade-off. While competing methods like SPIN and MLIH achieve low hallucination scores at the expense of massive drops in F1, recall, and caption length, HACP reduces hallucinations while increasing the F1 score and preserving recall."}, "weaknesses": {"value": "1. The paper is not clear about the practical computational overhead. \n\n- The attribution step (Algorithm 1) requires running inference on an \"attribution set $D$\" for every head in the target layers. This seems to be a very expensive pre-computation.\n\n- More concerning is the \"Task-Specific Automated Pruning Pipeline\" (Algorithm 3), which suggests running a Bayesian optimization loop for $T$ iterations per-instance. It is unclear if this was used for the SOTA comparisons (Tables 3-5). If it was, this would represent a massive, per-sample computational cost not incurred by the baselines, making the comparison unfair.\n\n- The per-token latency added by the dynamic pruning (Algorithm 2) is not quantified.\n\n2. The method introduces several new hyperparameters: $\\alpha$ and $\\gamma$ for the InfoSpectralScore, $\\mu$ and $\\lambda$ for pruning, and the choice of target layers. The paper states that \"Grid search for hyperparameters ($\\mu$ ,$\\lambda$) is conducted independently on each split\". This suggests the method may be highly sensitive to these settings and would require a costly, task-specific tuning process to work, undermining the \"plug-and-play\" benefit.\n\n3. The quality of the method seems highly dependent on the \"attribution set $D$\". Section 4.3 and Table 2 explicitly show that using a \"hallucination-focused\" attribution set is far more effective than a random one. This creates a practical chicken-and-egg problem: to fix hallucinations, one must first curate a dataset of inputs that are known to cause hallucinations.\n\n4. The paper contains minor but confusing inconsistencies in its mathematical formulation.\n\n- $\\alpha$ Overload: The symbol $\\alpha$ is used for two different purposes: first as a threshold (e.g., 0.9) to determine $k^{*}$ for the EigenScore, and second as a regularization weight for the SpectralVar term in Equation. This reuse of a key symbol is confusing.\n\n- Entropy Formulation: There is an inconsistency in the \"Spectral Entropy\" formulation. Equation 6 correctly defines SpectralEntropy with a negative sign, consistent with Shannon entropy. However, Equation 7, which claims to add an \"entropy penalty\", adds the term $+\\gamma \\cdot [\\sum p_i \\log(p_i + \\epsilon)]$. This is the negation of the defined SpectralEntropy term. While the intent (penalizing low-entropy/sparse distributions) is clear, the inconsistent naming and sign in the final equation is notationally imprecise."}, "questions": {"value": "1. Regarding Computational Cost (Weakness #1):\n\n- Was the \"Automated Pruning Pipeline\" (Algorithm 3) with its per-instance Bayesian optimization used to generate the SOTA comparison results in Tables 3-5?\n\n- If yes, how is this a fair comparison to baselines? If no, how were the hyperparameters (which are noted to be grid-searched independently for each split 29) actually set for the SOTA comparison?\n\n- What is the one-time, per-model setup cost (in hours/GPU) for the attribution step (Algorithm 1)?\n\n- What is the per-token latency (in ms) added by the dynamic pruning mechanism (Algorithm 2) during generation?\n\n2. Regarding Practicality (Weakness #2 & #3):\n\n- Given the apparent hyperparameter sensitivity, how would a practitioner realistically set ($\\mu$, $\\lambda$) for a new model or task without a costly grid search or the per-instance optimization from Algorithm 3?\n\n- How is the \"hallucination-focused\" attribution set created in a general setting? Does this not assume one has already solved the problem of identifying hallucination-prone inputs?\n\n3. Regarding Methodology (Weakness #4):\n\n- Can the authors clarify the reuse of the symbol $\\alpha$?\n\n- Can the authors correct the sign inconsistency between the definition of SpectralEntropy in Equation 6 and its use in Equation 7?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U0itVIIry9", "forum": "3GNaXptttK", "replyto": "3GNaXptttK", "signatures": ["ICLR.cc/2026/Conference/Submission7813/Reviewer_icyf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7813/Reviewer_icyf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774690809, "cdate": 1761774690809, "tmdate": 1762919853853, "mdate": 1762919853853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces HACP, a training-free framework for controlling object hallucinations in large vision–language models (LVLMs) at the attention-head level. It identifies that individual attention heads can have conflicting roles, some contributing to faithful grounding and others to hallucinations, and addresses the lack of fine-grained head-level intervention mechanisms. The authors propose InfoSpectralScore, a semantics-based attribution metric combining eigenvalue analysis with spectral variance and entropy regularization, to distinguish hallucination-prone from faithful heads. Using this attribution, they implement dynamic counteractive pruning that suppresses selected hallucination heads and reinforces faithful ones during inference, with an adaptive, task-specific pipeline."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed approach operates in a training-free manner, thereby avoiding additional training cost and data requirements.\n2. The work explicitly addresses the issue of functional conflicts among attention heads, which has been largely overlooked in prior studies."}, "weaknesses": {"value": "1. In line 305, the authors state “We evaluate HACP on LLaVA-1.5 (7B, 13B), Shikra-7B, and Qwen2.5-VL-7B-Instruct.” However, I could not find corresponding experimental results for Qwen2.5-VL-7B-Instruct in the subsequent sections. It would be helpful to include these results to assess the method’s effectiveness on this more recent model.\n2. The evaluation relies on a relatively limited set of datasets. It would strengthen the work to include additional hallucination benchmarks such as HallusionBench[1] or CRPE[2] for a more comprehensive assessment.\n3. The paper lacks experiments on out-of-domain data. It would be valuable to understand whether the proposed method provides generalization benefits when applied to other datasets beyond those used in the current evaluation.\n4. Although the method is claimed to offer interpretability, the presented evidence is rather limited. The attribution of functional roles to attention heads is mostly supported by quantitative metrics and a small number of qualitative cases, without more systematic visualizations or behavioral analyses to substantiate this claim.\n\n[1] Guan T, Liu F, Wu X, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 14375-14385.\n\n[2] Wang W, Ren Y, Luo H, et al. The all-seeing project v2: Towards general relation comprehension of the open world[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024: 471-490."}, "questions": {"value": "As described in Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "84lNrvjwt5", "forum": "3GNaXptttK", "replyto": "3GNaXptttK", "signatures": ["ICLR.cc/2026/Conference/Submission7813/Reviewer_7Vx8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7813/Reviewer_7Vx8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902888444, "cdate": 1761902888444, "tmdate": 1762919852689, "mdate": 1762919852689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets the hallucination reduction problems in large vision-language models (LVLMs). It proposes to intervene the attention heads at a fine-grained level to mitigate LVLM hallucinations. The proposed strategy dynamically pruning or modifying the hallucination-prone attention heads, which is training-free. Experiments demonstrate improved performance in LVLM hallucination reduction."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a mechanistic approach to control the VLM hallucination on the attention head level, through effective inference time intervention.\n\n- Experimental results across three VLMs and two benchmarks show improved performance of the proposed approach."}, "weaknesses": {"value": "- The paper writing could be improved. The novelty may also be limited given prior research in interpreting hallucination with attention heads and general mechanistic interpretability methodologies.\n\n- Computational cost may be high. The identification of specific attention heads that are hallucination prone needs to be computed on a batch of images before inference, requiring specific head intervention.\n\n- Different images may have different optimal head configurations. Instance level optimization on the attention head identification and intervention is time consuming, compromising the applicability of the proposed approach.\n\n- The hyper-parameters are selected via grid search, which is also expensive considering the procedure of the approach.\n\n- For experiments such as on CHAIR, there is no sensitivity/statistical significance analysis showing the randomness of the results, as we know the scores could vary with a large variance. The paper could benefit from testing on more recent benchmarks and models, such as InternVL, PaliGemma, Phi vision, Meta PLM, AMBER, MME, THRONE.\n\n- The approach is image/benchmark specific, as the optimal attention heads may be different for different styles and sources of images. This impedes the practical usage of the approach."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F8bNIQuwEi", "forum": "3GNaXptttK", "replyto": "3GNaXptttK", "signatures": ["ICLR.cc/2026/Conference/Submission7813/Reviewer_jmP4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7813/Reviewer_jmP4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979605653, "cdate": 1761979605653, "tmdate": 1762919852042, "mdate": 1762919852042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}