{"id": "v5qb8BG18G", "number": 7232, "cdate": 1758012401068, "mdate": 1759897864696, "content": {"title": "Rewiring Experts on the Fly: Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models", "abstract": "Mixture-of-Experts (MoE) models achieve efficient scaling through sparse expert activation, but often suffer from suboptimal routing decisions that create performance bottlenecks during inference due to distribution shifts between training and test data. While existing test-time adaptation methods could potentially address these routing issues, they primarily focus on dense models and require access to external data, thereby limiting their practical applicability to MoE architectures. We propose a data-free, online test-time rerouting framework that optimizes MoE expert selection using only the input prompt itself. Our method operates through a continuous optimization approach that first adapts routing based on the input prompt, then maintains this adaptation throughout the generation process. We implement this through lightweight additive vectors that update only router logits in selected layers, maintaining computational efficiency while preventing over-adaptation. Experimental results show consistent performance gains on challenging reasoning tasks. For example, our method achieves a 5.5\\% improvement on HumanEval with OLMOE. Furthermore, the approach combines effectively with other test-time techniques, producing 6\\% average improvements when integrated with self-consistency on DeepSeek-V2-Lite.", "tldr": "", "keywords": ["LLM", "MOE", "Testtime"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d0c5fc2fa3d0d5d1c50fe11daa06e554ed70cd0e.pdf", "supplementary_material": "/attachment/20337e1da8e611c4051cac1f0abfcaee2868da61.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a data-free, online test-time adaptation framework for MoE LLMs. It continuously optimizes expert routing decisions during text generation by introducing lightweight additive parameters to router logits. To further improve efficiency and stability, the framework updates only a subset of MoE layers determined by confidence-based layer selection, which identifies task-relevant layers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an important problem of test-time adaptation for MoE models and presents an effective solution. The proposed method is data-free, computationally efficient, and easily applicable to existing models. Experiments show consistent performance gains across diverse reasoning and generation tasks."}, "weaknesses": {"value": "(1) The paper lacks theoretical analysis explaining why optimizing routing decisions on the current context leads to better performance on future generation. What guarantees that reducing cross-entropy loss on already-generated tokens improves routing for subsequent reasoning steps? This is particularly concerning since the optimization objective (past context) differs from the actual goal (future generation quality).\n\n(2) The evaluation focuses primarily on relatively short reasoning tasks (coding problems, math word problems). The method's effectiveness on longer-context scenarios (multi-turn dialogues, document summarization, extended reasoning chains) remains unclear. Given that the optimization interval is set to 128 tokens, how does performance scale with generation length? Does the method maintain benefits after generating more tokens, or do cumulative routing adjustments lead to drift?\n\n(3) While the ablation study (Table 2) shows that confidence-based selection outperforms alternatives, the underlying assumption (high-confidence layers are more \"important\" for adaptation) lacks empirical validation. High confidence could equally indicate that these layers have already converged to good routing decisions and need less adjustment. Do high-confidence layers exhibit more task-relevant expert specialization? Are low-confidence layers fundamentally noisy or processing difficult parts of the task?"}, "questions": {"value": "Do the reported hyperparameters (η=0.05, n=5, m=128) transfer across different models and tasks, or does each setting require task-specific tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "llkyg4dnuH", "forum": "v5qb8BG18G", "replyto": "v5qb8BG18G", "signatures": ["ICLR.cc/2026/Conference/Submission7232/Reviewer_ufCS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7232/Reviewer_ufCS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761580947164, "cdate": 1761580947164, "tmdate": 1762919374881, "mdate": 1762919374881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a fundamental challenge in Mixture-of-Experts (MoE) models: **suboptimal routing decisions during inference**. MoE models achieve efficiency by selectively activating only a subset of experts for each token, but the quality of these routing decisions directly impacts performance. The authors draw an analogy to neuroplasticity in the human brain, arguing that MoEs need similar adaptive mechanisms during deployment.\n\nChallenges addresseed in this work in regards to routing:\n\n- Routers are simple linear functions that must approximate the anticipated utility of activating each expert\n- Distribution shifts between training and test data lead to poor routing choices\n- During standard inference, there's no mechanism to reinforce successful routing or reduce routing to unhelpful experts\n- Existing test-time adaptation methods require external reference data, limiting practicality\n\n### Key Contributions and Methodology\n\nThe paper introduces a **data-free, online test-time rerouting framework** with three main components:\n\n1. Router Logits Modification (§ 3.2)\n    - Introduces lightweight additive parameter vectors $δ^{(l)}$ for each MoE layer\n    - Modifies router logits: $z̃^{(l)} = z^{(l)} + δ^{(l)}$\n    - These vectors **steer expert selection** without modifying the underlying model weights\n2. Dynamic Layer Selection (§ 3.3)\n    \n    The framework selectively updates only high-confidence layers rather than all layers:\n    \n    - **Confidence metric**: $C_i = -\\frac{1}{k} \\sum_{j=1}^{k} \\log p_{i,j}$ (confidence is calculated as the negative log probability of selected experts; higher values indicate more decisive routing)\n    - **Two strategies**:\n        - Hard selection: Choose top-r proportion ($S_t = \\text{TopK}(\\{C_i\\}_{i=1}^L, r)$) of highest confidence layers\n        - Soft weighting: Apply confidence-based weights to gradient updates\n3. Two-Phase Optimization Procedure (§ 3.4)\n    \n    The method alternates between:\n    \n    - **Phase 1 - In-Context Routing Optimization**: Uses the current context as self-supervised training data, optimizing  to minimize cross-entropy loss over n=5 iterations\n    - **Phase 2 - Steered Generation**: Generates m=128 tokens with optimized routing, then returns to Phase 1 with extended context"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Data-free self-supervised adaptation**: The method treats each input as its own training sample, computing cross-entropy loss on the current context (prompt + generated text) to optimize routing parameters $\\delta^{(l)}$ without any external data or retrieval overhead. This achieves better performance than baselines that use external data\n- **Self-Supervised Routing Refinement**: The framework treats each input prompt as a self-supervised learning opportunity for dynamic expert selection. During inference, the method alternates between two phases: In-Context Routing Optimization, where the current context itself becomes the training data for computing gradient updates to router logits, and Steered Generation, where text is generated using these optimized routing decisions. This creates a dynamic feedback loop - as the model generates text, that very text provides the supervision signal for improving subsequent routing choices. The model continuously refines its understanding of what the task requires based on its own generation progress, leading to increasingly informed expert routing decisions as generation proceeds.\n- **Dynamic continuous optimization during generation**: Unlike static approaches, the method alternates between optimization phases (updating $\\delta^{(l)}$) and generation phases at 128-token intervals, allowing routing to adapt as the model's understanding of the task evolves. This creates feedback loop where generation quality informs subsequent routing.\n- Mechanistic insights that validate the approach:\n    - Edit distance analysis shows task-specific pathway modifications concentrated in deeper layers\n    - Expert utilization heatmaps reveal strategic redistribution rather than uniform changes\n    - Confidence-based layer selection outperforms other form of updates\n- The paper follows a natural progression from problem identification through methodology, experiments, and analysis, making it easy to follow the authors' reasoning and working."}, "weaknesses": {"value": "- The paper optimizes routing by adding bias vectors to router logits: $\\tilde{z}^{(l)} = z^{(l)} + \\delta^{(l)}$ where $z^{(l)} = W_r^{(l)} h^{(l)}$. However, this approach doesn't address the inherent constraint that MoE routing remains a linear function of the hidden state. The modification merely shifts decision boundaries in the existing linear space.\n    \n    The relatively small improvements despite optimizing on the exact test context suggest that the linear routing architecture itself **may** be the limiting factor.\n    \n- This paper fundamentally assumes that different experts have learned distinct, task-relevant capabilities worth optimizing routing for. However, it never validates this critical assumption. Recent research has shown that MoE experts often exhibit significant redundancy and lack clear specialization, which raises questions about what this routing optimization actually achieves.\n    \n    There is no clear anaylsis on the following: \n    \n    - Whether experts are meaningfully specialized\n    - What the \"preferred\" experts actually compute differently\n    - If routing changes actually correlate with expert capabilities\n- The paper's central approach has a fundamental flaw: it optimizes routing to minimize cross-entropy loss on **already-generated text**, then uses those parameters to generate **new text**. This could create problems like:\n    - The routing that best explains existing tokens may be entirely different from routing that generates good future tokens\n    - The model is essentially being trained to \"retroactively justify\" its past decisions rather than improve future ones\n    - No evidence is provided that minimizing reconstruction loss on context correlates with better generation quality"}, "questions": {"value": "How much performance gain can realistically be achieved by steering the router which is fundamentally a linear layer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cbNB3wTdEm", "forum": "v5qb8BG18G", "replyto": "v5qb8BG18G", "signatures": ["ICLR.cc/2026/Conference/Submission7232/Reviewer_Vpis"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7232/Reviewer_Vpis"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857222704, "cdate": 1761857222704, "tmdate": 1762919374385, "mdate": 1762919374385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a data-free and online test-time framework that continuously adapts MoE routing decisions during text generation without external supervision or data."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The method demonstrates consistent and significant performance gains across multiple, distinct MoE architectures and a diverse set of challenging reasoning benchmarks.\n- Excellent Analysis and Ablations. The ablation studies in Section 5.2 thoroughly validate the key design choices: the superiority of confidence-based layer selection over alternatives and the benefit of continuous refinement."}, "weaknesses": {"value": "- Concerns regarding Latency. The paper's claim of a \"modest\"  computational overhead is a significant understatement. According to Table 3, the proposed method nearly doubles the inference time on HumanEval (20.12s) compared to the baseline (10.71s). Also, the FLOPs for proposed method is 1.96e+12, which is **larger** than 1.93e+12 of ICL(3-shot). \n\n- Lack of sensitivity analysis on optimization steps T."}, "questions": {"value": "In the abstract, this paper claim that existing test-time adaptation methods could potentially address these issues, but they primarily focus on dense models and require access to external data. Also, in the related work, this paper claim that these methods assume accessible training data during deployment and introduce significant retrieval overhead, limiting real-world practicality. It would be better to showcase detailed performance of these methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HXgOylKLdF", "forum": "v5qb8BG18G", "replyto": "v5qb8BG18G", "signatures": ["ICLR.cc/2026/Conference/Submission7232/Reviewer_94hV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7232/Reviewer_94hV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969988603, "cdate": 1761969988603, "tmdate": 1762919373886, "mdate": 1762919373886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a test-time adaptation method for MoE models, which optimizes the router logits based on the cross-entropy loss of the input sequence and current generation itself. This allows the MoE routing to be adjusted based on the characteristics of the actual input data. Furthermore, by adapting continuously, the model can acquire more task-specific routing. The experimental results show modest performance gains across various tasks, with a notable improvement confirmed on HumanEval in particular, compared to the baseline."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This is a very simple and lightweight method, and it could be a promising option in scenarios where test-time adaptation proves effective. Since significant performance gains are achieved on highly specialized tasks like HumanEval, it suggests that this lightweight optimization alone may be sufficient even in situations requiring significant domain adaptation. Furthermore, because it is data-free and can be used even when reference data is not available, it offers many practical advantages."}, "weaknesses": {"value": "- There are some points that seem unclear regarding the fairness of the experimental setup. Please refer to the Questions.\n- It appears there are two errors in the optimization strategy. First, the paper states a policy of updating layers that performed 'high-confidence' routing. It seems this should be the opposite; layers with 'low' confidence should be updated. Second, regarding Equation (2), the paper claims, \"higher confidence values indicate more decisive routing decisions.\" This formula, however, is equivalent to entropy, where a higher value actually represents greater uncertainty. It looks as though these two errors might be acting complementarily, coincidentally leading to the correct optimization strategy (i.e., updating the uncertain layers). However, this makes the paper's reasoning unreliable. The authors should probably re-verify that the results shown in the paper align with their intended methodology."}, "questions": {"value": "- When applying this method, are the adaptation parameters (the $\\delta$ vectors) re-initialized to zero for each test example? If it's not the case, it suggests that for all examples after the first one, a form of task-related leakage might be occurring. This could violate the fundamental assumption of evaluating samples independently and would also imply that performance becomes dependent on the evaluation order of the examples.\n- If each example is completely independent, the proposed method should perform better in the latter half of long sequences. Are there any experimental results that have been verified for different lengths of examples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DiFxnpG87s", "forum": "v5qb8BG18G", "replyto": "v5qb8BG18G", "signatures": ["ICLR.cc/2026/Conference/Submission7232/Reviewer_ccv4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7232/Reviewer_ccv4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008539125, "cdate": 1762008539125, "tmdate": 1762919373447, "mdate": 1762919373447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}