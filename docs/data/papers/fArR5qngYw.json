{"id": "fArR5qngYw", "number": 21404, "cdate": 1758317189191, "mdate": 1759896923770, "content": {"title": "From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning", "abstract": "Real-world graph datasets often consist of mixtures of populations, where graphs are generated from multiple distinct underlying distributions. However, modern representation learning approaches, such as graph contrastive learning (GCL) and augmentation methods like Mixup, typically overlook this mixture structure. In this work, we propose a unified framework that explicitly models data as a mixture of underlying probabilistic graph generative models represented by graphons. To characterize these graphons, we leverage graph moments (motif densities) to cluster graphs arising from the same model. This enables us to disentangle the mixture components and identify their distinct generative mechanisms. This model-aware partitioning benefits two key graph learning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a data augmentation technique that interpolates in a semantically valid space guided by the estimated graphons, instead of assuming a single graphon per class. 2) For GCL, it enables model-adaptive and principled augmentations. Additionally, by introducing a new model-aware objective, our proposed approach (termed MGCL) improves negative sampling by restricting negatives to graphs from other models. We establish a key theoretical guarantee: a novel, tighter bound showing that graphs sampled from graphons with small cut distance will have similar motif densities with high probability. Extensive experiments on benchmark datasets demonstrate strong empirical performance. In unsupervised learning, MGCL achieves state-of-the-art results, obtaining the top average rank across eight datasets. In supervised learning, GMAM consistently outperforms existing strategies, achieving new state-of-the-art accuracy in 6 out of 7 datasets.", "tldr": "We model graph datasets as a mixture of underlying generative graphons, identified via motif-based clustering, to create superior data augmentation and contrastive learning frameworks.", "keywords": ["Graphon", "Graphon mixture", "Moment", "Graph Contrastive Learning", "Graph Mixup"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a2e692ea308e24d4fd35799d8395dad5a4c3f5ca.pdf", "supplementary_material": "/attachment/755d197e4ce669cf82f9b6c45a6c021134bb6f1f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a unified framework for inferring multiple underlying generative models (i.e., graphon mixtures) from observed graph data and leverages this structure to enhance downstream tasks such as graph mixup augmentation and graph contrastive learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The work elevates \"graph augmentation\" from the observation space to the generative space, which is logically self-consistent. Once the graphon estimation is completed, the per-unit training cost is weakly coupled with K, making the computational overhead appear manageable and facilitating easy integration into existing contrastive learning pipelines."}, "weaknesses": {"value": "1. A core idea of this paper is modeling dataset heterogeneity via multiple latent generative factors, which closely resembles the concept of latent factors in disentangled graph representation learning [1, 2]. However, the article lacks comparisons with baselines from this related line of work.\n\n 2. The paper claims to obtain a more disentangled representation but lacks corresponding visualizations or experiments using quantitative disentanglement metrics. For example, visualizations like feature correlation matrices or comparative analyses are missing.\n\n 3. Several choices in the pre-modeling stage (e.g., the selection of K, potential bias in graphon estimation) likely influence the results, yet the paper lacks ablation studies examining these aspects.\n\n\n[1] Disentangled Graph Contrastive Learning. NeurIPS 2021\n\n\n[2] Disentangled Graph Convolution Networks. ICML 2019"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Br03cWTC48", "forum": "fArR5qngYw", "replyto": "fArR5qngYw", "signatures": ["ICLR.cc/2026/Conference/Submission21404/Reviewer_zrKm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21404/Reviewer_zrKm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655938397, "cdate": 1761655938397, "tmdate": 1762941750083, "mdate": 1762941750083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a framework for graph representation learning that models datasets as mixtures of underlying generative processes represented by graphons. The key idea is to represent each latent generative mechanism by a graphon, a continuous function that defines connection probabilities between nodes. To uncover these mechanisms, the authors propose to characterize graphs using motif densities (graph moments), which serve as structural fingerprints. Graphs with similar motif statistics are clustered together, and a distinct graphon is estimated for each cluster. Building on this mixture model, the authors propose two applications: Graphon Mixture-Aware Mixup (GMAM) for semantically consistent data augmentation, and Model-aware Graph Contrastive Learning (MGCL) for reducing false negatives in unsupervised learning. The approach is supported by theoretical analysis and achieves competitive results across several benchmark datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Conceptual novelty** Clearly identifies and formalizes the overlooked “mixture of graphons” problem, which challenges the single-distribution assumption in existing graph learning frameworks.\n\n**Strong theoretical contribution** Introduces a novel, tighter motif concentration bound and provides complete proofs.\n\n**Empirical validation** Demonstrates improvements on both synthetic and real datasets, with extensive ablation and visualization.\n\n**Interpretability** Motif-based clustering yields interpretable “graph fingerprints” and meaningful estimated graphons.\n\n**Clarity and reproducibility** The presentation is very clear, and the appendices provide all implementation details."}, "weaknesses": {"value": "W1: The framework is only evaluated within two settings — Mixup augmentation and contrastive learning. There is no discussion or experiment on extending the mixture-aware framework to other learning paradigms, such as semi-supervised node classification, which essentially corresponds to a subgraph classification task over ego-networks across different hops.\n\nW2: While the proposed methods achieve the best overall results, the performance gains over strong baselines are small, often below 1%, raising concerns about the practical significance of the improvement.\n\nW3：There are minor typos, such as “Equation equation 10” in line 208.\n\nW4: Experiments are confined to small- and medium-scale TUDatasets. It remains unclear how the proposed methods perform on large graphs.\n\nW5: The paper sets the number of mixture components as log of the number of graphs, but the ablation in Appendix shows that performance is quite sensitive to the choice of $K$, suggesting that this prior strategy requires further investigation. In addition, no such ablation is reported for the Mixup setting, where similar sensitivity may arise.\n\nW6: While Appendix presents an ablation on the number of motifs, the paper does not explore how different combinations or types of motifs affect clustering or downstream performance. This leaves open whether the proposed results are robust to motif choice."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q1riPUEjBP", "forum": "fArR5qngYw", "replyto": "fArR5qngYw", "signatures": ["ICLR.cc/2026/Conference/Submission21404/Reviewer_MyDk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21404/Reviewer_MyDk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986431880, "cdate": 1761986431880, "tmdate": 1762941749819, "mdate": 1762941749819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission presents a framework for graph representation learning that models data as a mixture of graphons, using motif density-based clustering to disentangle generative models. It introduces two methods: GMAM (for supervised mixup augmentation) and MGCL (for unsupervised contrastive learning with model-aware sampling). \nA theoretical result provides a bound linking the cut distance between graphons and differences in empirical motif densities. Experiments show improved performance over existing mixup and contrastive learning methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation to address graph heterogeneity via graphon mixtures is reasonable and intuitively appealing.\n2. The paper is clearly written and well-organized, with good visual aids (e.g., Figure 1) explaining the workflow.\n3. Empirical results are generally positive, demonstrating improvements on standard benchmark datasets."}, "weaknesses": {"value": "1. The theoretical component (Theorem 1) is incremental and largely reuses existing concepts from graph theory (e.g., motif density concentration). \nThe bound provided, although claimed to be tighter, does not appear to yield any substantial new theoretical insight or algorithmic design.\n2. Both GMAM and MGCL are relatively straightforward extensions of existing approaches such as G-Mixup, SIGL, and GraphCL. \nThe modifications mainly add a clustering step based on motif statistics, followed by standard mixup or contrastive loss. \nThis design is incremental and lacks conceptual depth.\n3. The paper only briefly mentions computational complexity in Appendix A.1, without any comparison to baselines or quantitative analysis (e.g., runtime, GPU hours, or scaling with graph size). \nSince the proposed methods require motif counting and multiple graphon estimations, the computational overhead is likely significant.\nWithout this analysis, it is unclear whether the performance gains stem from higher computational cost rather than algorithmic improvement.\n4. Further experimental evaluations are needed. 1) No ablation on the number of mixture components or motif types. 2) No sensitivity study to clustering quality or graphon estimation accuracy. 3) The datasets used are relatively small and may not sufficiently stress-test scalability. 4) Missing discussion on training efficiency and memory requirements."}, "questions": {"value": "Could the authors provide an ablation study for GMAM that compares it against a baseline that uses SIGL to estimate a single graphon per class (instead of a mixture)? This would help quantify the specific contribution of the mixture model idea."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iiUL2lMRwx", "forum": "fArR5qngYw", "replyto": "fArR5qngYw", "signatures": ["ICLR.cc/2026/Conference/Submission21404/Reviewer_z9jy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21404/Reviewer_z9jy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762087830813, "cdate": 1762087830813, "tmdate": 1762941749218, "mdate": 1762941749218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}