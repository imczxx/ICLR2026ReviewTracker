{"id": "5TEjuCm91H", "number": 16164, "cdate": 1758260843161, "mdate": 1759897257396, "content": {"title": "Equipping LLMs with Self-Awareness for High-Stakes Tasks", "abstract": "Overconfidence in large language model responses has emerged as a critical barrier for deploying these systems in high-stakes tasks such as cyber threat intelligence, financial analysis, and clinical decision support.\nThis issue stems from reward-optimal behavior, as LLMs are trained to produce answers even under uncertainty.\nNevertheless, most approaches in high-stakes domains continue to treat these tasks as primarily knowledge-intensive, focusing on scaling, retrieval, or fine-tuning, while leaving the problem of overconfidence unresolved.\nRecent studies have begun to highlight this gap, calling for solutions that move beyond superficial calibration or knowledge expansion.\nBuilding on these challenges, we identify self-awareness as a missing capability for LLMs in high-stakes deployment: the ability to recognize the limits of what they know and to assess the certainty of how well they know it.\nTo this end, we propose a framework that trains LLMs to cultivate self-awareness via reinforcement learning and decouples awareness learning from task performance, pairing it with adaptive inference-time strategies such as retrieval-augmented generation and low-confidence regeneration.\nWe evaluate our framework in the cybersecurity domain.\nResults demonstrate that our method substantially reduces confidently wrong outputs, surpassing the strongest baseline by up to 95.6\\%, while achieving competitive performance.\nOur implementations are available at https://anonymous.4open.science/r/SelfAwareLLM.", "tldr": "", "keywords": ["Self-Awareness", "Large Language Model"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b615653a1e517df266ae1153edc90a933e37b01.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper highlights the importance of enhancing LLM self-awareness in high-stakes domains.\nIt proposes a novel reinforcement learning framework that incorporates two awareness-based rewards, boundary-awareness and confidence-awareness, together with adaptive inference-time strategies.\nExperimental results show that it significantly reduces the proportion of confidently wrong outputs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Equipping LLMs with _self-awareness_ for **high-stakes applications** is practically meaningful and timely.\n- Using reinforcement learning purely with awareness rewards, without optimizing correctness, is a novel and thought-provoking exploration.\n- The experiments are relatively comprehensive, covering 4 datasets and 5 metrics."}, "weaknesses": {"value": "- The paper introduces new notions (self-awareness, boundary-awareness, confidence-awareness) but lacks definitions and fails to clarify how they relate to areas like _confidence calibration_ or uncertainty estimation.\n- The conceptual distinction between “what it knows” and “how well it knows” is unclear and not empirically separated. If confidence estimation is accurate, it could already imply boundary awareness, so the necessity of decoupling these two objectives is not well-justified.\n- Experimental results (Table 3) show that the proposed method underperforms strong baselines such as **Primus-Merged** and **RLVR** on most standard metrics (Accuracy, Brier, ECE, AUROC), making it difficult to claim clear advantages.\n- The proposed **CWR** metric is highly sensitive to threshold choice. Improvements appear only at high thresholds (≥ 0.8); the paper does not explain why this threshold is chosen or how many samples fall into this region.\n- Important calibration baselines (e.g., verbalization[1], self-consistency[2]) are missing, which weakens the empirical comparison.\n- The IB/OOB classification based on “at least one correct sample” is questionable. This can be affected by sampling frequency or prompt design.\n\n[1] Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence\nScores from Language Models Fine-Tuned with Human Feedback\n[2] Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j4VG9zvylA", "forum": "5TEjuCm91H", "replyto": "5TEjuCm91H", "signatures": ["ICLR.cc/2026/Conference/Submission16164/Reviewer_LaYM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16164/Reviewer_LaYM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16164/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761466092020, "cdate": 1761466092020, "tmdate": 1762926327671, "mdate": 1762926327671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an approach to explicitly train models to recognize the limits of what they know and to assess how well they know it (i.e., assigning confidence). The main innovation is considering this as a separate task from providing the model with more knowledge. Experiments are conducted on 6 benchmarks from the cybersecurity domain and compared with several baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Originality: the approach proposed by the authors is simple and elegant, but most likely original\n- quality: the way in which the method is framed and tested is of high quality\n- clarity: the explanation of the method and presentation of the results is mostly clear\n- signitficance: the paper’s main signiicance is in arguing that “faithfulness must be treated as a first-class objective rather than a by-product of knowledge expansion”, beyond the specifics of the implemented method."}, "weaknesses": {"value": "The paper has a few minor weaknesses: \n\n- overselling the significance of the results: it is true that the method proposed by the authors has good performance, comparable or better than the other baselines. However, even with this method, the scores for AUC, ECE, and CWR@0.8 are, in absolute terms, bad (ie, the models have still poor confidence quantification abilities!). This does *not* reduce the significance of the authors’ work, as the limited strength of a method on the existing set of models should not work against how worth of publication the method is (particularly when the method is comparable to other baselines). But I invite the authors to not oversell the results explicitly or implicitly, such as by saying “enhances reliability” without stressing that this is still poor as this may have dangerous consequences, for instance inducing practitioners to adopt models that are not suitable in high-stakes scenarios.\n- a few sentences are unclear:\n    - the opening in the abstract ‘Overconfidence in large language model responses’: not clear if overconfidence of the models or users\n    - line 68 is unclear: “[figure 1b] shows that…”\n    - line 69: ‘ordinary hallucinations’: it would be good to define this.\n- the authors introduce a new term ‘self-awareness’, but meta-cognition is used in the cognitive sciences for the same aim. Is there need to introduce such additional term?\n- the related works section can touch a few additional points:\n    - papers discussing fundamental limits on LLM’s confidence and correctness, as for instance https://arxiv.org/abs/2408.02357, https://arxiv.org/abs/2509.04664\n    - works to measure the ability to determine high-confidence and high-correctness regions for high-stakes AI usage, such as https://aclanthology.org/2025.findings-acl.790/\n    - empirical measures of overconfidence: https://www.nature.com/articles/s41586-024-07930-y\n    - the ‘assessor’ class of methods using surrogate models to estimate uncertainty: https://scholar.google.com/citations?view_op=view_citation&hl=en&user=VV76Eo8AAAAJ&citation_for_view=VV76Eo8AAAAJ:Y0pCki6q_DkC\n- while the experiments include OOD datasets, it would be interesting to see how the methods affects completely different domains. Moreover, I would be interesting in seeing how this training method, that is likely applied using benchmarks with well-defined ground-truth questions, affects the model’s behaviour on more open-ended tasks where a single ground truth may not exist."}, "questions": {"value": "- is it fair to say that a very high ability to “know how well it knows” implies “know what it knows?”. Or that, put otherwise, the former cannot exist without the second?\n- Table 1 mentions that, if the model determines a question is out of its knowledge boundary, it shoudl use RAG. But what if there is nothing that is not relevant to the quesiton at hand in the knowledge database?\n- Can one applied the suggested approach to a surrogate (smaller) model that accompanies a larger one which is tasked with producing the actual answer?\n- Do the authors have any intuition on what the effect would be on completeyl different domains, or on open-ended tasks with no single ground truth? Ie, does the model learn its knowledge boundary and confidence for the specific considered domain, or does the training method unlocks an “introspection” capability (as in https://arxiv.org/abs/2410.13787)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PIpXpCNOLa", "forum": "5TEjuCm91H", "replyto": "5TEjuCm91H", "signatures": ["ICLR.cc/2026/Conference/Submission16164/Reviewer_NfsF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16164/Reviewer_NfsF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16164/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683035355, "cdate": 1761683035355, "tmdate": 1762926327147, "mdate": 1762926327147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework for “equipping large language models with self-awareness,” proposing reinforcement learning objectives that explicitly train two complementary capabilities: boundary-awareness and confidence-awareness. The central idea is to optimize for awareness signals—the model’s ability to recognize when it knows or does not know—rather than directly for correctness. The resulting “self-aware” LLM can avoid producing confidently wrong answers even when uncertain. Crucially, the paper separates awareness learning (trained independently of task incentives) from downstream inference-time strategies, such as retrieval-augmented generation and low-confidence regeneration. Experiments on cybersecurity benchmarks—a domain chosen for its high stakes and data scarcity—show that the framework significantly reduces confidently wrong outputs compared to strong baselines, while maintaining competitive accuracy. When combined with the adaptive inference strategies, the method further improves robustness and task performance, suggesting that explicitly modeling self-awareness offers a principled route to safer model deployment in critical settings"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper identifies an important and underexplored goal—reducing confidently wrong model behaviors—by directly training self-awareness instead of relying on external calibration or confidence post-processing. The conceptual framing is clear and well-motivated: awareness learning as an independent signal decoupled from correctness offers a theoretically elegant way to improve reliability without distorting model incentives. The implementation is also thoughtfully designed: reinforcement learning objectives are introduced for boundary-awareness and confidence-awareness, and these are combined with adaptive inference-time mechanisms that modulate generation based on the learned signals. The experiments are extensive within their chosen domain, covering both in-distribution and out-of-distribution cybersecurity datasets."}, "weaknesses": {"value": "While the paper presents self-awareness as broadly applicable, the practicality of the approach is limited by its dependence on internal confidence scores and gradient-based access, which are often unavailable in real deployment settings. The framework assumes the ability to read and train confidence-bearing logits, compute awareness rewards, and integrate custom reinforcement learning objectives. However, many widely used LLMs—especially proprietary API-based models—do not expose raw probabilities, hidden activations, or confidence estimates in a stable or well-defined manner. Even when approximate probabilities are available, they are frequently temperature- or decoding-dependent and thus not reliable indicators of epistemic uncertainty. As a result, the proposed approach cannot be applied to many realistic scenarios where organizations rely on closed-weight models that allow only text-in/text-out interaction.\n\nAnother concern is that the paper treats confidence as a coherent and interpretable signal, yet the relationship between logit magnitude and genuine model uncertainty is not theoretically grounded. Modern LLMs often produce overconfident distributions due to training dynamics unrelated to knowledge boundaries, and reinforcement training on confidence signals may amplify this effect rather than mitigate it. The paper claims that its awareness objectives produce a “pure” measure of model uncertainty, but provides no mechanistic or representational analysis to support this, nor does it examine whether the trained score aligns with human judgments of uncertainty. Without probing how or where this awareness is encoded in the model’s internal representations, it remains unclear whether the system is genuinely learning to recognize when it does not know something, or simply learning a new pattern of confidence suppression conditioned on dataset artifacts.\n\nA further limitation is that the evaluation design does not reflect natural failure modes. The cybersecurity datasets are constructed with explicit in-boundary and out-of-boundary distinctions, creating a relatively sharp and learnable separation in input space. Many real-world uncertainty cases—e.g., ambiguous facts, partially overlapping knowledge domains, multi-step reasoning failures—do not exhibit such clean boundaries, and the method’s performance under these more subtle conditions is not demonstrated. The balanced train/test splits further simplify the decision setting and may encourage the model to rely on distribution memorization rather than genuine introspection.\n\nThe inference-time adaptive strategies—retrieval and low-confidence regeneration—likely contribute significantly to the observed improvements, yet the paper does not disentangle their effects from those of the self-awareness rewards. Retrieval-augmented generation, in particular, already encourages models to hedge or reconsider initial claims when uncertain. If most of the benefit comes from retrieval or regeneration, then the core contribution—self-awareness learning—may be less central than claimed. The existing ablations gesture toward this question but do not isolate it convincingly."}, "questions": {"value": "How robust are the learned awareness signals when applied to tasks beyond cybersecurity, particularly in open-domain reasoning or dialogue generation, where uncertainty manifests differently?\n\nCan the authors provide evidence that the awareness signals correspond to interpretable internal confidence or epistemic uncertainty, rather than being indirect by-products of reward optimization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dBXNfeejZP", "forum": "5TEjuCm91H", "replyto": "5TEjuCm91H", "signatures": ["ICLR.cc/2026/Conference/Submission16164/Reviewer_zAPH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16164/Reviewer_zAPH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16164/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872470069, "cdate": 1761872470069, "tmdate": 1762926326654, "mdate": 1762926326654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a self-awareness RL-tuning framework for LLMs in high-stakes domains, decomposed into boundary-awareness and confidence-awareness. It operationalizes both objectives via a binary reward for in/out-of-boundary classification and a calibration reward based on Brier score over verbalized confidence, and optimizes using GRPO with a curriculum. On six cybersecurity benchmarks, the approach substantially lowers the Confident Wrong Rate at a high confidence threshold, but gains over baselines on other calibration metrics or task accuracy are not consistent nor significant."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Evaluating on real-world high-stake domains (in this paper, cybersecurity) is valuable (which is not often seen from previous works). \n* Optimizing jointly for boundary-awareness and confidence-awareness is a clean approach, and is a promising direction for increasing trustworthiness in high-stake and critical applications, which is a significant topic.\n* The ablation studies on different reward terms and inference-time strategies are comprehensive, and presented with good visualization and analysis."}, "weaknesses": {"value": "* The authors should make it clearer their unique contributions compared with prior works. In particular, RLCR [1] which this paper cites and compares as a baseline, shares the exact same confidence-aware reward formulation, Brier score based scoring rule, and also uses GRPO for the RL algorithm. While the utility term in RLCR directly optimizes for correctness and this paper uses an indirect measure of knowledge boundary based on ensembles (additional overhead), the general goal and formulation is also similar. There is no theoretical analysis on the reward optimality or different choices of scoring functions in this paper. The authors could further elaborate on why rewarding knowledge boundary awareness instead of directly correctness could provide performance gains (based on claims from this paper), especially in OOD settings.\n* While cybersecurity is certainly a representative high-stakes and knowledge-intensive domain to test on (see strengths), it would be interesting to show how this generalizes and transfers to other domains, beyond looking at distribution shifts within one domain.\n* The results in both in- and out-of-distribution settings do not seem consistently strong, often not even beating the second-best, in terms of both accuracy and calibration. This is even true when augmented with adaptive strategies, which creates additional overhead at inference time, but benefits seem diminishing. Results on the self-proposed CWR metric show good gains. However, I think this metric can be flawed/gamed: 1). It can hinge on the choice of tau - what is the distribution of the overall confidence estimates, and how sensitive is the CWR when tau varies around 0.7-0.8, and 2). It may not capture the full picture: it ignores the coverage, or how often the model is highly confident - say a model is / learns to be only highly-confident for very few of all inputs (possible for difficult tasks), it can achieve a low CWR by almost never being confident at all, yet another more useful model could have the same or slightly higher CWR, but actually keep very few of its mistakes at high-confidence region.\n* The in-boundary vs. out-of-boundary is determined by resembling multiple answers and see if the correct answer appears at least once. This seems to be a simplification following [2] as the paper mentioned, but the authors should elaborate on the motivations of this specific adaptation. To me, this simple approach could mis-label hard but in-boundary queries (false OOB) or lucky guesses (false IB) which induces noise, and quality might be sensitive to N and temperature. Requiring multiple samples also adds overhead (N=16 in the experiments, which seems quite significant).  \n\n---\n\n[1]. Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty\n\n[2]. Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent"}, "questions": {"value": "* In OOD, why does CWR improve considerably while Brier / ECE remain mixed?\n* How sensitive the result is to the ensemble size N and the rollout and inference temperatures? How were the rest parameters tuned?\n* The Primus series models are based on Llama-3.1, but your experiments are with Qwen-2. Did you fine-tune Qwen-2 using Primus datasets, or was the result based on Llama-3.1? What exactly is the difference between Primus-Merged and Primus-Reasoning? These details, especially a brief overview of the Primus variants, could be included when you introduce the baselines (same for SecurityLLM)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OGvubQ8LTB", "forum": "5TEjuCm91H", "replyto": "5TEjuCm91H", "signatures": ["ICLR.cc/2026/Conference/Submission16164/Reviewer_tSAQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16164/Reviewer_tSAQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16164/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991171078, "cdate": 1761991171078, "tmdate": 1762926326025, "mdate": 1762926326025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}