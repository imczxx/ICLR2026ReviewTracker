{"id": "HA8KSQW7SO", "number": 9599, "cdate": 1758129250427, "mdate": 1759897709923, "content": {"title": "VideoPhy-2: A Challenging Action-Centric Physical Commonsense Evaluation in Video Generation", "abstract": "Large-scale video generative models, capable of creating realistic videos of diverse visual concepts, are strong candidates for general-purpose physical world simulators. However, their adherence to physical commonsense across real-world actions remains unclear (e.g., playing tennis, backflip). Existing benchmarks suffer from limitations such as limited size, lack of human evaluation, sim-to-real gaps, and absence of fine-grained physical rule analysis. To address this, we introduce VideoPhy-2, an action-centric dataset for evaluating physical commonsense in generated videos. We curate 4000 diverse and detailed prompts for video synthesis from modern generative models. We perform human evaluation that assesses semantic adherence, physical commonsense, and grounding of physical rules in the generated videos. Our findings reveal major shortcomings, with even the best model achieving only $47.7\\%$ joint performance (i.e., high semantic and physical commonsense adherence) on the hard subset of VideoPhy-2. We find that the models particularly struggle with conservation laws like mass and momentum. Finally, we also train VideoPhy-2-AutoEval, an automatic evaluator for fast, reliable assessment on our dataset. Overall, VideoPhy-2 serves as a rigorous benchmark, exposing critical gaps in video generative models and guiding future research in physically-grounded video generation. We will release the dataset, videos, auto-rater model, and code in the camera-ready version.", "tldr": "We introduce VideoPhy-2, an action-centric dataset for evaluating physical commonsense in generated videos.", "keywords": ["physical commonsense", "semantic adherence", "video generation", "benchmark", "auto evaluator"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c05275f8a7deb49524afdb3a40275d11698e86e6.pdf", "supplementary_material": "/attachment/97f373e77be7aecd3eb51db59167e696118bd05c.pdf"}, "replies": [{"content": {"summary": {"value": "Video generation models can produce realistic videos, making them ideal candidates for simulating the physical world. However, evaluating how well these models adhere to physical commonsense in real-world actions remains unclear. Existing models face issues such as limited size and lack of human evaluation, making it difficult to assess their ability to align with physical laws. In this work, the authors propose the Videophy-2 benchmark, which includes a dataset of 4000 diverse and detailed prompts, along with an automatic evaluator. Experimental results show that, under the Videophy-2 standard, even the best model achieves only 47.7% joint performance, exposing critical gaps in video generative models and providing direction for future research in physically-grounded video generation."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- It provides a challenging benchmark for physically-grounded video generation.\n\nIn the hard subset of Videophy-2, even the highest-performing model, Wan2.2-27B-A14B, only achieves a score of 47.7%. This means that Videophy-2 is a stringent benchmark, setting a standard for future evaluations of video generation models in terms of physical alignment.\n\n- Detailed evaluation dimensions.\n\n Unlike existing work that combines semantic and physical evaluations, leading to potential biases, Videophy-2 uses three distinct dimensions: Semantic Adherence (SA), Physical Commonsense (PC), and Physical Rules (PR). These dimensions evaluate whether the video aligns with the prompt, whether the video is physically aligned with the real world, and whether the video follows a specific physical law (e.g., gravity). This approach minimizes evaluation biases as much as possible.\n\n- An effective automated evaluation approach.\n\nAlthough human judgments can serve as the gold standard, the cost of human annotation limits the widespread application of evaluations. Therefore, Videophy-2 introduces an automatic evaluation method, using the VideoCon-Physics fine-tuned Videophy-2-autoeval (7B parameters) as the annotator. Experiments show that Pearson’s correlation of Videophy-2-autoeval exceeds that of several VLMs, including Gemini-2.0-Flash-Exp, and surpasses VideoCon-Physics in F1 score."}, "weaknesses": {"value": "- The lack of comparison with more advanced VLMs as annotators.\n\nThe most advanced closed-source VLM used in this paper is Gemini-2.0-Flash-Exp, but there is no comparison with stronger models like Gemini-2.5. Given the relatively small evaluation dataset, using stronger VLMs as annotators would increase the credibility of the conclusions drawn in the paper."}, "questions": {"value": "- Is it possible to prove that Videophy-2-autoeval does not suffer from evaluation bias?\n\nIn this paper, the authors collect labeled data sampled from HunyuanVideo-13B, Cosmos-Diffusion-7B, and CogVideoX-5B as the training set for Videophy-2-autoeval and evaluate its performance on unseen prompts and unseen video models. However, the videos generated by HunyuanVideo-13B, Cosmos-Diffusion-7B, and CogVideoX-5B may contain biases in terms of adhering to or violating physical laws. Given that the annotator is trained on data generated by these models, can it be considered a fair standard for evaluating future video models?\n\n- Would incorporating real videos into the training set of Videophy-2-autoeval improve its evaluation performance?\n\nThe training set of Videophy-2-autoeval only includes synthetic data, whereas real videos inherently possess the best physical commonsense. It is a question that adding real videos to the training data and constructing a diverse training set by providing different prompt-video and rule-video pairs could help make Videophy-2-autoeval closer to human annotators.\n\n- Can I2V models be evaluated using VideoPhy-2?\n\nIn this paper, the authors mainly compare the performance of text-to-video (T2V) models in terms of physical alignment. For image-to-video (I2V) models, the initial image should significantly influence the evaluation score. The authors evaluate the performance of the SVD-I2V model by first generating an image using Stable Diffusion and then guiding the SVD-I2V model to generate a video based on that image. Is there a better method to assess the physical alignment performance of I2V models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KVk54l7ix4", "forum": "HA8KSQW7SO", "replyto": "HA8KSQW7SO", "signatures": ["ICLR.cc/2026/Conference/Submission9599/Reviewer_tmU3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9599/Reviewer_tmU3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816073771, "cdate": 1761816073771, "tmdate": 1762921143740, "mdate": 1762921143740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "VIDEOPHY-2 is a large-scale benchmark for evaluating the physical common sense of video generation, centered on real-world \"actions\". It includes 3940 detailed prompts, multi-human evaluation, and fine-grained physics rule annotations, and provides an automated evaluator. Experiments show that even the strongest model achieves only about 47.7% of the joint semantic and physical metrics on hard sets, particularly weak in conservation laws (mass, momentum), highlighting the significant shortcomings of current video generation in terms of physical consistency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The benchmark is based on 197 real-world actions and 3940 long text prompts, covering diverse scenarios that closely resemble everyday physical situations.\n\nIt includes human-annotated semantic and physical scores, along with compliance and violation annotations at the level of physical rules/laws, supporting detailed diagnostics.\n\nIt provides an AUTOEVAL model to improve evaluation efficiency and consistency with human judgment, facilitating large-scale, rapid model comparisons."}, "weaknesses": {"value": "This paper has limited noverty, and the only difference between it and VideoPhy seems to be the addition of more prompts; it is not suitable as a standalone conference paper.\n\nTable 5 tests the OOD generalization ability of the eval model, but lacks comparisons using some cutting-edge VLMs, such as Qwen and GPT.\n\nThe paper lacks insight into how to enhance the ability of T2V models to generate physically consistent videos. Several necessary experiments could provide further insights, such as SFT, using an Eval model as a reward model for RL training, or RFT."}, "questions": {"value": "see the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1tRZWHAO30", "forum": "HA8KSQW7SO", "replyto": "HA8KSQW7SO", "signatures": ["ICLR.cc/2026/Conference/Submission9599/Reviewer_74jg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9599/Reviewer_74jg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823136607, "cdate": 1761823136607, "tmdate": 1762921143428, "mdate": 1762921143428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VIDEOPHY-2, a large-scale benchmark for testing whether text-to-video generation models can follow basic physical commonsense. It expands the earlier VIDEOPHY dataset with 197 actions and 3,940 detailed prompts, focusing on action-centric, physics-rich scenarios. Authors also train VideoPhy-2-AutoEval, an automatic evaluator for fast assessment on their proposed dataset."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a large-scale, carefully annotated benchmark for assessing the physical understanding ability of video generation models.\n2. An automatic evaluator is trained to approximate human judgments, which is useful in principle, though not always effective in practice."}, "weaknesses": {"value": "1. I really dislike the presentation. Figure 2 does not clearly correspond to Section 2 (for example, the dense captioning component is not reflected in the figure). In Section 3.1, the distinction between PC and PR is unclear. Several parts are verbose and poorly organized; for instance, around line 275, experimental descriptions appear within the evaluation methodology section. Similar issues occur throughout the paper.\n2. The experimental setup is unfair, as some models use upsampled captions while others do not. The authors should generate and evaluate videos separately for fair comparison.\n3. The experimental result and tables should be more detailed. The main results table (Table 2) should explicitly report all metrics such as SA and PC.\n4. According to Table 4, the pearson's correlation between VIDEOPHY-2-AUTOEVAL and human ratings is below 0.5, indicating that the proposed automatic evaluator does not generalize well to unseen scenarios. Therefore, I have concerns about the benchmark’s reliability for automatic evaluation, especially since human evaluation of video generation models is labor-intensive and inefficient."}, "questions": {"value": "1.See Weaknesses.\n2. What is the inter-annotator agreement among human evaluators? The paper should include an analysis of annotation consistency to verify the reliability of human evaluation.\n3. The amount of work in this paper is impressive, and I appreciate the effort. However, the overall writing quality is poor. The authors should seriously reconsider how to present and organize their work more clearly and coherently."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CIFrr8Aj6L", "forum": "HA8KSQW7SO", "replyto": "HA8KSQW7SO", "signatures": ["ICLR.cc/2026/Conference/Submission9599/Reviewer_VSb4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9599/Reviewer_VSb4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908865795, "cdate": 1761908865795, "tmdate": 1762921143110, "mdate": 1762921143110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VIDEOPHY-2, a large-scale benchmark for evaluating physical commonsense in text-to-video generation. The dataset consists of 3,940 LLM-generated prompts covering 197 real-world actions, with detailed human annotations on semantic adherence, physical commonsense, and rule violations. It includes a hard subset emphasizing physics-rich and multi-event scenarios (e.g., backflip, throwing discus). The authors also develop VIDEOPHY-2-AUTOEVAL, a fine-tuned video-language model trained on 50K human annotations for scalable automatic evaluation. Extensive experiments on leading open and closed video models (e.g., Wan2.2-27B, Sora, CogVideoX) show that current models struggle with physical plausibility—particularly with conservation laws of mass and momentum—achieving only 47.7% joint adherence on the hard subset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed benchmark addresses an important gap in evaluating physical realism in video generation, a key step toward world-modeling AI, it extends the scale with more fine-grained physical law annotations compared to previous version VIDEOPHY.\n\n2. The proposed VIDEOPHY-2-AUTOEVAL is valuable and achieves large correlation gains over strong baselines (e.g., Gemini-2.0-Flash-Exp).\n\n3. Thorough experiments and insightful analysis: The fine-grained breakdown of violated physical laws (e.g., momentum, elasticity) provides deep diagnostic insight."}, "weaknesses": {"value": "1. Missing related work discussion: The paper does not reference Impossible Videos, which also proposes a benchmark including evaluating violations of physical laws in video generation. A detailed comparison and discussion are needed to clarify the conceptual and methodological differences between VIDEOPHY-2 and that work.\n\n2. Prompt ambiguity in multi-event scenarios: The authors encourage LLMs to generate prompts depicting multiple events within a single description. While this increases task difficulty, it also introduces ambiguity when evaluating semantic adherence—for instance, when a video generator accurately follows only part of the prompt. How is semantic adherence rated in such cases? Is there a standardized or clearly defined evaluation guideline to handle partial adherence?"}, "questions": {"value": "1. Does training on VIDEOPHY-2 improve a model’s physical commonsense generation?\n2. Could the authors release example failure cases and annotation guidelines to help the community standardize future evaluation protocols?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qp7moIGRcf", "forum": "HA8KSQW7SO", "replyto": "HA8KSQW7SO", "signatures": ["ICLR.cc/2026/Conference/Submission9599/Reviewer_QRAy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9599/Reviewer_QRAy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9599/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928340594, "cdate": 1761928340594, "tmdate": 1762921142792, "mdate": 1762921142792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}