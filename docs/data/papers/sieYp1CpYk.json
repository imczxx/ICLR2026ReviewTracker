{"id": "sieYp1CpYk", "number": 21165, "cdate": 1758314433289, "mdate": 1759896938799, "content": {"title": "The Unreasonable Effectiveness of Randomized Representations in Online Continual Graph Learning", "abstract": "Catastrophic forgetting is one of the main obstacles for Online Continual Graph Learning (OCGL), where nodes arrive one by one, distribution drifts may occur at any time and offline training on task-specific subgraphs is not feasible.\nIn this work, we explore a surprisingly simple yet highly effective approach for OCGL: we use a fixed, randomly initialized encoder to generate robust and expressive node embeddings by aggregating neighborhood information, training online only a lightweight classifier.\nBy freezing the encoder, we eliminate drifts of the representation parameters, a key source of forgetting, obtaining embeddings that are both expressive and stable.\nWhen evaluated across several OCGL benchmarks, despite its simplicity and lack of memory buffer, this approach yields consistent gains over state-of-the-art methods, with surprising improvements of up to 30\\% and performance often approaching that of the joint offline-training upper bound.\nThese results suggest that in OCGL, catastrophic forgetting can be minimized without complex replay or regularization by embracing architectural simplicity and stability.", "tldr": "", "keywords": ["continual learning", "online learning", "graph neural network", "randomized feature extraction", "streaming graph learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f6afaea1b51171e875deb35befa1d5d30be66465.pdf", "supplementary_material": "/attachment/606af887ad46e02b6e8cf568c4d02560534a3c3e.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the surprising effectiveness of randomized representations in Online Continual Graph Learning (OCGL). The authors propose to replace the learned graph encoder with a fixed random projection encoder, and only train a lightweight streaming linear classifier (SLDA) during continual learning. This design aims to eliminate representation drift—the major source of catastrophic forgetting—while preserving expressive power through high-dimensional random embeddings. Extensive experiments on several graph continual learning benchmarks show that such a simple approach not only matches but often surpasses state-of-the-art methods, and even approaches the jointly trained oracle performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow, with clear motivation and logical presentation.\n2. Extending the concept of random representations from image-based continual learning to graph case is sound.\n3. The method is elegant and simple—freezing the encoder effectively prevents forgetting while keeping the system lightweight and stable."}, "weaknesses": {"value": "1. The novelty is limited. As the method mainly transfers the existing idea from [1] to the graph domain. \n2. The paper lacks theoretical analysis on how well the random graph embeddings can approximate the oracle representations or under what conditions they remain expressive.\n3. Fixing the encoder reduces model plasticity, limiting adaptability to evolving or dynamic graph structures.\n4. The baselines used for comparison are relatively old, making it difficult to fully assess performance against the latest continual learning models."}, "questions": {"value": "Compared to the approach in [1], which studies the effectiveness of random representations in general online continual learning, what are the unique challenges or characteristics of the graph continual learning scenario that make this problem distinct?\n\n[1] Prabhu A., Sinha S., Kumaraguru P., et al. Random Representations Outperform Online Continually Learned Representations. arXiv preprint arXiv:2402.08823, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jFbdHVhsdX", "forum": "sieYp1CpYk", "replyto": "sieYp1CpYk", "signatures": ["ICLR.cc/2026/Conference/Submission21165/Reviewer_Nni1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21165/Reviewer_Nni1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823169402, "cdate": 1761823169402, "tmdate": 1762941528401, "mdate": 1762941528401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies Online Continual Graph Learning (OCGL), where nodes arrive sequentially and the model must adapt to evolving distributions without access to past data. The authors combine a frozen randomized GNN encoder with a lightweight online classifier, claiming improved stability and reduced forgetting. Experiments on several node-classification graph datasets report competitive performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Motivates the challenge of catastrophic forgetting when nodes arrive sequentially and past data cannot be stored.\n\n2. Uses a simple and intuitive strategy: freezing the encoder to avoid representation drift.\n\n3. Empirical evaluation across multiple node-classification graph datasets, including some real-world application datasets."}, "weaknesses": {"value": "1. Problem definition lacks clarity. The introduction states that *“nodes arrive sequentially and undergo distribution drifts,”* and *\"an incremental graph G, induced by a stream of nodes v1, v2, . . . , vt, . . . that are added one by one,\"* but this scenario also exists in the temporal graph continual learning setting. It is unclear what concrete challenge OCGL uniquely poses beyond existing temporal GNN continual learning methods.  Authors must clarify why temporal graph continual learning baselines are excluded and whether those models already address the studied challenge.\n\n2. Motivation for real-time online learning is underdeveloped. The paper claims offline training is infeasible, yet it does not provide practical evidence or runtime constraints where online learning is strictly required. A discussion contrasting the cost-benefit vs. offline retraining is needed.\n\n3. Limited technical novelty. The core method is freezing a randomized encoder and training a streaming classifier. Each component has been previously studied (random features, SLDA, UGCN, GRNF). The contribution appears to be an empirical combination rather than a principled algorithmic innovation.\n\n4. Method description is ambiguous in Table 1. Table 1 lists baselines but does not clearly show where the proposed method sits in the taxonomy. This table is very confusing, it does not even highlight the proposed method or mark the best-performing results.\n\n5. Baseline coverage is insufficient. There are several continual graph learning baselines (e.g., ER-GNN [1], TWP [2]) and temporal graph continual learning (N-ForGOT [3], OTGNet [4]) works that should be compared. Current baselines do not convince that the method advances the state of the art.\n\n\n\n[1] Fan Zhou and Chengtai Cao. Overcoming catastrophic forgetting in graph neural networks with experience replay. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI, pp. 4714–4722, 2021.\n\n[2] Huihui Liu, Yiding Yang, and Xinchao Wang. Overcoming catastrophic forgetting in graph neural networks. In Thirty-Fifth AAAI Conference on Artificial Intelligence, pp. 8653–8661. AAAI Press, 2021.\n\n[3] Wang, Liping, et al. \"N-ForGOT: Towards Not-forgetting and Generalization of Open Temporal Graph Learning.\" The Thirteenth International Conference on Learning Representations, ICLR, 2025.\n\n[4] Kaituo Feng, Changsheng Li, Xiaolu Zhang, and Jun Zhou. Towards open temporal graph neural networks. In The Eleventh International Conference on Learning Representations, ICLR, 2023."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ayg6OcNC2J", "forum": "sieYp1CpYk", "replyto": "sieYp1CpYk", "signatures": ["ICLR.cc/2026/Conference/Submission21165/Reviewer_yf4i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21165/Reviewer_yf4i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827890221, "cdate": 1761827890221, "tmdate": 1762941527712, "mdate": 1762941527712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the problem of Online Continual Graph Learning (OCGL), where nodes arrive sequentially and models must learn from a single pass of data while retaining past knowledge. The authors propose a simple decoupled approach that combines randomized, untrained graph feature extractors (UGCN or GRNF) with a Streaming Linear Discriminant Analysis (SLDA) classifier. This design removes the need for training a GNN backbone and aims to prevent catastrophic forgetting while keeping computation and memory low. Experiments on seven benchmarks show that the method achieves state-of-the-art or near-upper-bound accuracy, outperforming existing OCGL and CL baselines. Overall, the paper argues that fixed randomized representations can provide strong and stable performance for online graph learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper proposes a decoupled OCGL framework using untrained randomized graph features with a lightweight SLDA classifier. The approach is easy to implement yet very effective.\n\n2. The method is computationally lightweight, requires no replay buffer, and scales well to streaming graph data. This makes it practically appealing for real-time or resource-limited continual learning scenarios.\n\n3. The method is tested on seven benchmarks and consistently achieves state-of-the-art or near-upper-bound performance without using replay buffers."}, "weaknesses": {"value": "1. In the proposed method, node embeddings are generated once and never updated. However, as new nodes arrive and the graph structure evolves, the context of old nodes may change, making their fixed embeddings potentially outdated. This raises concerns about the reliability of predictions for earlier nodes. The paper does not provide experiments or analysis to assess this effect.\n\n2. The authors emphasize that the method is lightweight, but there are no quantitative comparisons of runtime, memory usage, or computational complexity against other OCGL approaches. \n\n3. The paper experiments with two random feature extractors (UGCN and GRNF) without explaining why these were chosen or whether other random graph encoders would behave similarly. A deeper analysis of why these random features perform so well would make the contribution more convincing."}, "questions": {"value": "1. The embeddings of old nodes are fixed once generated. When new nodes and edges arrive, the graph structure changes — could this make old embeddings outdated? Have you tested how this affects performance?\n\n2. You claim the method is lightweight and efficient. Can you provide comparisons of training time, memory usage, or complexity with other OCGL methods?\n\n3. Why did you choose only UGCN and GRNF as random feature extractors? Would other randomized graph encoders work similarly?\n\n4. How would the method perform in real dynamic graphs where node attributes or edges change frequently?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "02TJlPUEGt", "forum": "sieYp1CpYk", "replyto": "sieYp1CpYk", "signatures": ["ICLR.cc/2026/Conference/Submission21165/Reviewer_3MG8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21165/Reviewer_3MG8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904272651, "cdate": 1761904272651, "tmdate": 1762941526921, "mdate": 1762941526921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studied the research problem of online continual graph learning, where nodes sequentially show up in the graph, resulting a continual learning problem where the graph (dataset) keeps changing and drifting. The proposed method is very simple, just a untrained (randomly initialized) feature extractor model, followed by a continuously trained linear classifier. The authors also provided insights on why this simple model works so well on this task. The authors evaluated the proposed method against multiple baselines on a couple benchmarks, and showcased very good performances."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is very simple and easy to implement.\n2. This paper is overall clearly written and easy to follow.\n3. The authors conducted fairly comprehensive evaluations, with the proposed method showcasing good improvements."}, "weaknesses": {"value": "1. I'm not super familiar with the literature of OCGL, and found it interesting that the whole setup assumes only new nodes showing up along with their edges, but not consider new edges between old nodes. If we consider social platforms with user-user graphs and/or user-video/product/ads graphs, IMO it would make much more sense to consider an edge-based streaming update, where a new node shows up along with the first edge that connects to it. I'd appreciate if the authors can discuss more regarding this difference in the setup, as well as how would it affect the proposed method's effectiveness/efficiency etc.\n2. The datasets seem small. I wonder are there larger datasets with timestamps exist that the authors can evaluate with."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "614RUHDH3G", "forum": "sieYp1CpYk", "replyto": "sieYp1CpYk", "signatures": ["ICLR.cc/2026/Conference/Submission21165/Reviewer_re4q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21165/Reviewer_re4q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971984963, "cdate": 1761971984963, "tmdate": 1762941526030, "mdate": 1762941526030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}