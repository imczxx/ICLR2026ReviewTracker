{"id": "EcbUaXJb9s", "number": 11000, "cdate": 1758186536124, "mdate": 1759897615411, "content": {"title": "MARWA: Multi-agent retrieval-augmented framework for reliable bioinformatics workflow automation", "abstract": "The rapid growth of multi-omics data has driven the expansion of bioinformatics analysis tools. Common bioinformatics tasks often rely on workflows, which link multiple tools into structured pipelines for reproducibility and scalability. Yet, building workflows manually is slow and error-prone, motivating efforts toward automation. However, bioinformatics workflow automation remains difficult due to the need to clarify vague analytical objectives, coordinate heterogeneous tools, and generate intricate tool commands. Despite the potential of large language models (LLMs) to aid bioinformatics workflow recommendation through advanced semantic understanding and logical reasoning, current agent frameworks often rely on one-shot generation, weak tool retrieval solution, and limited evaluation scheme, resulting in fragile workflow automation. We propose MARWA, a Multi-Agent Retrieval-augmented framework for reliable bioinformatics Workflow Automation. The framework emphasizes a step-by-step generation process with error handling at each stage to ensure robustness. We introduce a retrieval-augmented framework to strengthen tool command accuracy, which incorporates multi-perspective LLM-augmented descriptions and employs contrastive learning. We further design a two-stage evaluation framework, combining expert-verified execution on 40 curated tasks with large-scale benchmarking on 2,270 tasks using LLM-based evaluation. Our experiments demonstrate that MARWA consistently outperforms baselines in pass rate, workflow quality and scalability. Our work provides a foundation for trustworthy bioinformatics workflow automation. Project Page: https://anonymous.4open.science/r/MARWA-7D30.", "tldr": "We present MARWA, a multi-agent retrieval-augmented framework that improves reliability of bioinformatics workflow automation.", "keywords": ["Bioinformatics; Workflow Automation; Multi-Agent Systems; Retrieval-Augmented Generation; Large Language Models"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d9055b3507fbd9f0223026fe1373e09b7e9204c4.pdf", "supplementary_material": "/attachment/e38d90b6331dce8634e89243d3b9d2cb2a3c0b68.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a multi-agent retrieval-augmented framework call MARWA for reliable bioinformatics Workflow Automation. MARWA's architecture is composed of six specialized LLM-based agents (Analyzing, Planning, Selecting, Generating & Executing, Debugging, and Judging) that operate in a step-by-step, closed-loop fashion. The authors also design an embedding method called LAFT, based on contrastive learning fine-tuning on the pretrained BERT model. Experiments show that MARWA consistently outperforms baselines like AutoBA and BioMaster, particularly in generating correct installation commands and file paths, leading to higher workflow success rates."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1.The six-agent, step-by-step framework is a well-reasoned and significant improvement over one-shot generation. By breaking the complex problem of workflow creation into discrete and verifiable stages, the system introduces robustness and error-handling capabilities that are critical for this domain.\nS2.Experiments show that MARWA and LAFT outperform other baseline methods."}, "weaknesses": {"value": "W1. The paper emphasizes that the proposed method is specifically designed for bioinformatics workflow automation. However, although the evaluation datasets are related to bioinformatics, the architectures of the proposed MARWA and LAFT methods do not appear to have domain-specific optimizations for bioinformatics. Could the authors leverage the characteristic structures of bioinformatics data to optimize the model framework itself (rather than only the prompts)?\nW2. In the embedding section, fine-tuning using contrastive learning is already a well-established approach for training embedding models. This work merely uses LLM-generated synthetic data to fine-tune the embedding model(BERT), without introducing a novel method. In addition, state-of-the-art embedding models are often based on decoder-only architectures with larger parameter scales, such as BGE-EN-ICL and Qwen3 Embedding.\nW3. The experiments show that MARWA achieves a higher success rate compared to baseline methods. However, given its highly complex workflow structure (including six agents and a loop structure), it is expected to consume significantly more tokens and inference time than other methods. The experiments, however, do not evaluate MARWA’s token costs, inference time, or similar metrics."}, "questions": {"value": "1. Based on Table 1, can the authors compare retrieval performance when replacing the BERT model with more recent open-source base embedding models, such as BGE-EN-ICL or Qwen3 Embedding?\n2. Can the authors design experiments to evaluate the cost of MARWA, such as the number of tokens consumed and the average inference time?\n3. Can the authors compare MARWA with some general-purpose agent methods, such as ReAct?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8OQHhruyqS", "forum": "EcbUaXJb9s", "replyto": "EcbUaXJb9s", "signatures": ["ICLR.cc/2026/Conference/Submission11000/Reviewer_CtBg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11000/Reviewer_CtBg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886676786, "cdate": 1761886676786, "tmdate": 1762922184960, "mdate": 1762922184960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge of **automating bioinformatics workflows**, where existing LLM-based systems often fail due to **ambiguous task definitions, heterogeneous tools, and unreliable one-shot generation**.\n\nTo address these issues, the authors propose **MARWA**, a multi-agent retrieval-augmented framework that **decomposes workflow construction into modular stages with dedicated agents** for task clarification, tool retrieval, command synthesis, and error correction.\n\nMARWA enhances tool selection through contrastive-learning-based retrieval and validates reliability via a two-stage evaluation combining expert execution and large-scale LLM-based assessment.\n\nExperiments demonstrate that MARWA substantially improves workflow accuracy, robustness, and scalability over existing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper designs a comprehensive end-to-end execution pipeline that covers the entire process，including task analysis, tool selection and workflow execution and validation.\n\nThis holistic design ensures not only that each stage is logically grounded and traceable, but also that potential errors can be detected and corrected through contextual feedback, significantly enhancing overall reliability."}, "weaknesses": {"value": "The paper mainly proposes solutions to surface-level problems without uncovering the deeper reasoning gaps between the task requirements and the chosen methods.\n\nFor instance, it claims that replacing a one-shot generation process with a step-by-step approach improves workflow reliability, yet it never explains *why* one-shot generation fails in this task setting, *what specific reasoning capabilities* are lacking, or *why* step-by-step reasoning would inherently address them.\n\nSince step-by-step generation is already a standard configuration for large language models, this modification only strengthens a weak baseline rather than constituting a principled innovation.\n\nMoreover, the framework appears as an incremental extension or a more fine-grained reactive pipeline, without demonstrating true multi-agent cooperation or emergent division of labor. Consequently, the proposed contributions lack clear conceptual novelty and task-specific motivation, resulting in a framework that feels more like a layered adaptation of existing paradigms than a genuinely innovative approach."}, "questions": {"value": "1. Have previous studies already introduced benchmark datasets or evaluation protocols for workflow automation, and how were these used to assess reliability or execution quality?\n2. What specific improvements or novelties do the proposed dataset and evaluation metrics in this paper provide beyond existing ones — in terms of coverage, realism, or reproducibility?\n3. Has the paper reported the computational or financial cost (e.g., model inference time, agent coordination overhead, or GPU usage) associated with the multi-agent setup, and how does this compare to the baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LptBLzETzd", "forum": "EcbUaXJb9s", "replyto": "EcbUaXJb9s", "signatures": ["ICLR.cc/2026/Conference/Submission11000/Reviewer_AmHM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11000/Reviewer_AmHM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959892831, "cdate": 1761959892831, "tmdate": 1762922184476, "mdate": 1762922184476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multi-agent retrieval enhancement framework, MARWA, to address the robustness and scalability issues in bioinformatics workflow automation. \nWith the rapid growth of multi-omics data, bioinformatics analysis workflows are becoming increasingly complex, and manually constructing workflows is both time-consuming and error-prone. \nExisting automation methods based on Large Language Models (LLMs) suffer from issues such as \"one-off generation,\" inaccurate tool retrieval, and insufficient evaluation. MARWA significantly improves the accuracy of tool retrieval and command generation by employing six collaborative agents (analysis, planning, selection, generation and execution, debugging, and judgment), combined with retrieval enhancement (RAG), multi-perspective LLM tool description, and comparative learning. \nFurthermore, the paper proposes a two-stage evaluation system that combines expert execution and large-scale LLM evaluation. Experiments demonstrate that MARWA outperforms existing methods across pass rate, workflow quality, and scalability, laying the foundation for reliable bioinformatics automation workflows."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This work employs a multi-agent collaborative architecture, involving a series of intricate processes including analysis, planning, selection, execution, debugging, and judgment, thereby significantly improving process robustness and flexibility.\n\n- The method uses LLM to generate multi-perspective tool descriptions and optimizes tool embedding through BERT contrastive learning, achieving tool retrieval accuracy higher than mainstream baselines.\n\n- Real-world execution and large-scale evaluation are combined: 40 expert validation tasks and 2270 LLM evaluation tasks provide a comprehensive evaluation system, making the results highly convincing."}, "weaknesses": {"value": "- While the evaluation system is comprehensive, large-scale tasks primarily rely on automated evaluation using LLM, resulting in a limited number of actual tasks executed. Furthermore, for biomedicine, does over-reliance on automated evaluation accurately reflect real-world usability?\n\n- The tool's database expansion primarily relies on manual verification and command logging, leaving room for improvement in automation and scaling up.\n\n- The detailed description of contrastive learning is rather brief, lacking sufficient disclosure of hyperparameters, training set size, and other details."}, "questions": {"value": "- Will the multi-perspective generation of tool descriptions in MARWA's search enhancement feature lead to semantic drift due to LLM illusion? How can description consistency be guaranteed?\n\n- Are there compatibility solutions for the file system interface across different operating systems (e.g., Windows, macOS)? How general is its practical deployment?\n\n- In large-scale evaluations, is there more detailed statistical analysis of the accuracy of LLM automatic scoring and its consistency with expert scoring?\n\n- How adaptable is MARWA to new tools or parameter changes? Does it support automatic tool version identification and compatibility?\n\n- Will multi-agent collaboration lead to significant computational resource consumption? What are the actual hardware requirements for deployment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3FsNzgiloc", "forum": "EcbUaXJb9s", "replyto": "EcbUaXJb9s", "signatures": ["ICLR.cc/2026/Conference/Submission11000/Reviewer_fs7m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11000/Reviewer_fs7m"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762226622450, "cdate": 1762226622450, "tmdate": 1762922184098, "mdate": 1762922184098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an innovative Multi-Agent Retrieval-Augmented framework aimed at enhancing bioinformatics workflow automation. The approach leverages multi-perspective LLM-enhanced tool descriptions combined with contrastive representation learning to achieve robust semantic representations of bioinformatics tools, ultimately improving tool retrieval accuracy. The evaluation dataset constructed for this purpose could serve as a valuable resource for the research community."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe integration of multi-perspective LLM-enhanced tool descriptions is a promising approach for tool selection in complex scientific domains, potentially benefiting agent systems.\n2.\tThe two proposed datasets could significantly aid in the evaluation of bioinformatics agents."}, "weaknesses": {"value": "1.\tThe claim of complete automation in the current workflow seems somewhat overstated. Is there any algorithmic illustration provided? Is the sequence of operations predefined?\n2.\tThe framework includes six cooperative LLM-based expert agents. Are the same models used across all six components, or are there distinct characteristics for different experts? Insights into model selection would be beneficial.\n3.\tThere is a sentence structure issue on lines 263-264 that needs clarification.\n4.\tIs the file system intended to be multimodal?\n5.\tWhich specific LLMs are used as evaluators? What distinguishes the evaluation process from the judging operation?\n6.\tA more detailed presentation of the dataset's difficulty and characteristics would enhance clarity. Additionally, what are the cost differences between MARWA and test-time scaling with powerful LLM-only methods in solving the task?"}, "questions": {"value": "Identical to the 'Weaknesses' noted"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QvwgNrDtqK", "forum": "EcbUaXJb9s", "replyto": "EcbUaXJb9s", "signatures": ["ICLR.cc/2026/Conference/Submission11000/Reviewer_SPqT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11000/Reviewer_SPqT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762258254046, "cdate": 1762258254046, "tmdate": 1762922183738, "mdate": 1762922183738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}