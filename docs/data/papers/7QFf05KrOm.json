{"id": "7QFf05KrOm", "number": 10531, "cdate": 1758174796220, "mdate": 1759897645440, "content": {"title": "Enhancing Multi-Image Understanding through Delimiter Token Scaling", "abstract": "Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input.\nOne major reason is the cross-image information leakage, where the model struggles to distinguish information across different images.\nExisting LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage.\nTo enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens.\nThis enhances the model’s ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions.\nConsequently, the model is better able to distinguish between images and reason over them more accurately.\nExperiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB and QBench2. \nWe further evaluate our method on text-only tasks that require clear distinction. \nThe method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews and WCEP-10. \nNotably, our method requires no additional training or inference cost.", "tldr": "", "keywords": ["LVLM", "Multi-Image Understanding", "Training-free"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f136522b97343b85433c25e2e09f3f4e3415c678.pdf", "supplementary_material": "/attachment/636314a36a71572153eb1a404e0c0a57a4c76714.zip"}, "replies": [{"content": {"summary": {"value": "The method is evaluated on a wide range of models and benchmarks, demonstrating consistent performance improvements on multi-image understanding tasks (Mantis, MuirBench, etc.). Crucially, the authors show the method's generality by applying it to text-only multi-document and multi-table tasks, again achieving performance gains. The proposed approach requires no additional training or inference costs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's primary strength is its clear and insightful analysis of how delimiter tokens function and where they fall short. The \"image-wise tagging\" concept provides a strong theoretical motivation for the proposed solution.\n\nThe method is remarkably simple (a single scaling operation) and highly efficient (no training, no inference overhead, compatible with optimizations like FlashAttention). This makes it a very appealing and practical technique.\n\nThe effectiveness of the method is demonstrated across a wide variety of models (Qwen2.5-VL, InternVL3, LLaVA-OV), model sizes (0.5B to 32B), and task domains (multi-image, multi-document, multi-table). This shows it is a general principle for improving multi-instance understanding, not a model-specific trick."}, "weaknesses": {"value": "The paper states that the scaling layer and factor λ are tuned for each model. The appendix mentions the scaling layer is fixed for each model, but the process for choosing this layer is not detailed. A more principled explanation or heuristic for selecting the optimal layer(s) to apply scaling would make the method even more robust and easier to adopt. The current approach feels slightly post-hoc.\n\nThe method involves scaling a single chosen layer's hidden states. A deeper analysis on why a particular layer is optimal or an exploration of scaling multiple layers could provide further insights into the model's internal workings."}, "questions": {"value": "please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O48VZAgF5D", "forum": "7QFf05KrOm", "replyto": "7QFf05KrOm", "signatures": ["ICLR.cc/2026/Conference/Submission10531/Reviewer_Fgst"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10531/Reviewer_Fgst"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761586544490, "cdate": 1761586544490, "tmdate": 1762921809918, "mdate": 1762921809918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical limitation of LVLMs, i.e., cross-image information leakage in multi-image tasks, by scaling delimiter token hidden states, a simple yet effective training/inference-cost-free method. Experiments across multi-image (Mantis, MuirBench) and text-only (MultiNews, TQABench) benchmarks show consistent gains, validating its robustness and generality. The analysis of delimiter tokens’ two key properties also provides valuable insights into LVLM attention mechanisms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Solves cross-image leakage in LVLMs via delimiter scaling, with gains in multi-image/text tasks, no extra cost.\n- Analyzes delimiter tokens’ key properties, offering clear theoretical basis for the method.\n- Generalizes to text multi-instance tasks, works across models (0.5B–32B), and fits optimized kernels."}, "weaknesses": {"value": "- Though claiming minimal impact on text-image interaction, it only mentions a 10% drop in text-to-image attention scores without detailing how this drop affects downstream cross-modal tasks (e.g., image-text retrieval), leaving uncertainty about real-world cross-modal performance"}, "questions": {"value": "- Your work shares similarities with parallel context encoding [1] and attention sink, both focusing on context separation/attention regulation. How does your delimiter scaling method relate to attention entropy (a core factor in Zhang et al.’s work)? Does scaling delimiter states reduce cross-image attention entropy, and if so, how? \n- Is there a run-time analysis to show the exact additional inference cost? \n\n\n[1] Attention Entropy is a Key Factor: An Analysis of Parallel Context Encoding with Full-attention-based Pre-trained Language Models\n, Z Zhang et al, ACL 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BRyU5lBsiH", "forum": "7QFf05KrOm", "replyto": "7QFf05KrOm", "signatures": ["ICLR.cc/2026/Conference/Submission10531/Reviewer_4BeG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10531/Reviewer_4BeG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718116558, "cdate": 1761718116558, "tmdate": 1762921809494, "mdate": 1762921809494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a training-free method for limiting cross-image interactions in LVLMs by scaling the hidden states of delimiter tokens across multiple images. Comprehensive evaluation results on multi-image, multi-document, and multi-table benchmarks are provided to demonstrate the effectiveness of the proposed approach. Both qualitative and quantitative analyses are included to illustrate the role and impact of image delimiter tokens."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem of cross-image information leakage in multi-image LVLM settings is important and worth investigating.\n2. The prior analysis on delimiter tokens and the characterization of their key properties is clear and insightful, helping readers better understand the mechanism.\n3. The experiments are extensive, covering multiple LVLM families and sizes, four multi-image benchmarks, two multi-document benchmarks, and one multi-table benchmark."}, "weaknesses": {"value": "1. The concept of “sink tokens” has been studied in prior works, and there are also existing methods addressing cross-image leakage. Thus, the novelty and significance of the current findings appear limited.\n2. The technical contribution of the proposed method is relatively weak. Scaling the hidden states of delimiter tokens provides only marginal performance gains. For instance, when applied to larger LVLMs such as InternVL3-14B or Qwen2.5-VL-32B, the improvements are minimal (e.g., 42.42 → 42.58)."}, "questions": {"value": "1. Could the authors propose a more technically substantial method or extend the current approach with additional mechanisms? Further ablation studies building on the discovered delimiter token properties may help strengthen the contribution and highlight the novelty of the work.\n2. The writing could be further polished for clarity, and several figures and tables could be improved for readability. Enhancing the presentation quality would significantly improve the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bnWUnwhdkZ", "forum": "7QFf05KrOm", "replyto": "7QFf05KrOm", "signatures": ["ICLR.cc/2026/Conference/Submission10531/Reviewer_pZZk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10531/Reviewer_pZZk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725429266, "cdate": 1761725429266, "tmdate": 1762921809076, "mdate": 1762921809076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on interleaved inputs for multi-modal large language models. \n\nIt is motivated by the hypothesis that **adjacent frames carry stronger contextual relevance.** \n\nUnlike previous interleaved approaches that rely on special textual tokens, this work introduces a delimiter token reweighting mechanism."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The interleaved image–text processing still lacks clarity in terms of how information flows across modalities. The explanation within MLLMs remains underexplored, though this work makes a valuable attempt to highlight the importance of key regions.\n\nThe motivation is clear.\n\nThe core idea is **very simple yet interesting**, and the method section is clearly presented. The method do not bring additional training cost and just reweight the hidden states."}, "weaknesses": {"value": "1. The main concern is the **limited scope of evaluation**. The paper focuses primarily on math and multi-view benchmarks, whereas multi-image input represents a special case of __interleaved data__ that can be applied to a broader range of scenarios. The performance under few-shot settings, where multiple instances are concatenated together, remains unclear and differs from the explored benchmarks.\n\n2. The performance improvements are sometimes marginal, suggesting limited generalization.\n\n3. The proposed reweighting operation introduces inconvenience during inference, as it requires modification of hidden states and is therefore applicable only to open-source models."}, "questions": {"value": "1. In Appendix A.1, several details require clarification. For instance, what are the four images and the corresponding text shown? How many samples were used to construct this figure? If the images share visual similarity, would the corresponding results change?\n\n2. Can the proposed method be extended to interleaved inputs (e.g., alternating image–text sequences)?\n\n3. For multi-image inputs, could there be potential attention sink issues similar to those discussed in “When Attention Sink Emerges in Language Models: An Empirical View”?\n\nI may reconsider my evaluation after reading the authors’ rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pful1hgqxz", "forum": "7QFf05KrOm", "replyto": "7QFf05KrOm", "signatures": ["ICLR.cc/2026/Conference/Submission10531/Reviewer_exND"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10531/Reviewer_exND"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762135598025, "cdate": 1762135598025, "tmdate": 1762921808384, "mdate": 1762921808384, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}