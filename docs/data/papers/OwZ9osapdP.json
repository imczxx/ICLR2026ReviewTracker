{"id": "OwZ9osapdP", "number": 18317, "cdate": 1758286353223, "mdate": 1759897111337, "content": {"title": "Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields", "abstract": "Semantic distillation in radiance fields has spurred significant advances in open-\nvocabulary robot policies, e.g., in manipulation and navigation, founded on pre-\ntrained semantics from large vision models. While prior work has demonstrated the\neffectiveness of visual-only semantic features (e.g., DINO and CLIP) in Gaussian\nSplatting and neural radiance fields, the potential benefit of geometry-grounding in\ndistilled fields remains an open question. In principle, visual-geometry features\nseem very promising for spatial tasks such as pose estimation, prompting the ques-\ntion: Do geometry-grounded semantic features offer an edge in distilled fields?\nSpecifically, we ask three critical questions: First, does spatial-grounding produce\nhigher-fidelity geometry-aware semantic features? We find that image features\nfrom geometry-grounded backbones contain finer structural details compared to\ntheir counterparts. Secondly, does geometry-grounding improve semantic object\nlocalization? We observe no significant difference in this task. Thirdly, does\ngeometry-grounding enable higher-accuracy radiance field inversion? Given the\nlimitations of prior work and their lack of semantics integration, we propose a novel\nframework SPINE for inverting radiance fields without an initial guess, consisting\nof two core components: (i) coarse inversion using distilled semantics, and (ii)\nfine inversion using photometric-based optimization. Surprisingly, we find that the\npose estimation accuracy decreases with geometry-grounded features. Our results\nsuggest that visual-only features offer greater versatility for a broader range of\ndownstream tasks, although geometry-grounded features contain more geometric\ndetail. Notably, our findings underscore the necessity of future research on effective\nstrategies for geometry-grounding that augment the versatility and performance of\npretrained semantic features.", "tldr": "We explore the geometry-grounded semantic features in distilled radiance fields and find that although these features provide finer geometric detail, they do not outperform purely visual semantic features.", "keywords": ["Distilled radiance fields", "Robotics", "Geometry-grounded visual semantics", "Gaussian Splatting", "NeRFs"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/24795a8b7efa6dc3007eb66468ffad1e4f774a66.pdf", "supplementary_material": "/attachment/0d7c71e7169eacdb036478fa49fadd9c7cc7f108.pdf"}, "replies": [{"content": {"summary": {"value": "This paper conducts an empirical study to compare the effectiveness of visual-only semantic features (from DINOv2, DINOv3) against visual-geometry semantic features (from VGGT) when distilled into 3D radiance fields (Gaussian Splatting and NeRFs). The investigation is structured around three core questions relevant to robotics applications:\n1. Do visual-geometry features contain higher-fidelity spatial content?\n2. Does geometry-grounding improve semantic object localization?\n3. Can visual-geometry features enable higher-accuracy radiance field inversion (i.e., camera pose estimation)?\n\nTo facilitate the third question, the authors propose SPINE, a novel framework for radiance field inversion that does not require an initial pose estimate. The key findings are that while geometry-grounded features (VGGT) do capture finer geometric details, they do not offer an advantage in semantic localization and surprisingly underperform visual-only features (DINOv2) in the task of pose estimation. The authors conclude that visual-only features currently offer greater versatility for downstream tasks in distilled fields."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a fundamental and highly relevant question: what kind of pretrained features are most effective for 3D scene understanding in the context of radiance fields? Comparing modern visual-only foundation models like DINO with emerging visual-geometry models like VGGT provides valuable insights for the community.\n- The study is well-organized, comparing two types of features on two different radiance field representations (NeRF and GS) across three distinct and important downstream tasks.\n- Beyond a comparative study, the paper introduces SPINE, a novel method for radiance field inversion that works without an initial camera pose guess.\n- The results are somewhat counter-intuitive; one might expect geometry-grounded features to excel at a geometric task like pose estimation. The finding that visual-only features from DINOv2 perform better is surprising and important."}, "weaknesses": {"value": "- **Overly Broad Claims** from a Single Model: The paper's claims about \"visual-geometry features\" as a general class are based on experiments with a single model, VGGT. While VGGT is a strong representative, it is possible that its specific pre-training objective is what limits its versatility, rather than the principle of geometry-grounding itself. The conclusions should be carefully worded to reflect that the findings are specific to the models tested, avoiding generalization to all possible geometry-grounding techniques.\n- The paper presents SPINE as a method for pose estimation, but its training protocol and generalization capabilities are not clearly described.\n- The authors use perceptual metrics (SSIM, PSNR, LPIPS) to evaluate semantic localization accuracy. While the rationale is noted, this is a departure from standard practice for localization/segmentation tasks. These metrics measure the similarity of the relevancy heatmaps, not how accurately the target object is isolated. Reporting results across a range of thresholds with mIoU would make the results more convincing."}, "questions": {"value": "The paper tackles a very interesting problem with a well-structured set of experiments and introduces a novel pose estimation framework (SPINE). The findings are surprising and thought-provoking. However, the work is held back by a few significant weaknesses: the conclusions about geometry-grounded features are drawn from a **single model**, and the evaluation for semantic localization uses non-standard metrics in my opinion. These issues prevent a confident recommendation for acceptance in its current form. I believe the paper has high potential, and I would be willing to reconsider my rating if the authors can address these points in a revision."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YI0Faa3jsa", "forum": "OwZ9osapdP", "replyto": "OwZ9osapdP", "signatures": ["ICLR.cc/2026/Conference/Submission18317/Reviewer_x4tX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18317/Reviewer_x4tX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760755477117, "cdate": 1760755477117, "tmdate": 1762928034349, "mdate": 1762928034349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits the role of geometry-grounded vision backbones (e.g., VGGT) versus visual-only ones (e.g., DINOv2/v3) in semantic distillation for radiance fields. It examines three key questions: Do geometry-grounded features contain richer spatial information? Do they improve semantic object localization? And do they enable more accurate radiance field inversion (pose estimation)?\nThe authors propose SPINE, a novel inversion framework using semantic features to estimate camera poses without initialization, followed by photometric refinement. Surprisingly, results show that geometry-grounded features do not outperform visual-only ones on localization or inversion, though they contain more geometric detail."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. This is the first systematic analysis comparing visual-only vs. visual-geometry semantic embeddings in radiance fields, a gap overlooked by existing works like LERF, CLIP-NeRF, or DFF.\n2. The study spans multiple datasets (LERF, 3D-OVS, Robotics), two radiance field types (NeRF, GS), and multiple metrics (GFF, SSIM, PSNR, LPIPS, SE(3) error)."}, "weaknesses": {"value": "1. The description of SPINE’s inverse model is conceptually clear but lacks comparative or sensitivity analyses: How critical are the semantic embeddings vs. the photometric refinement? How do different backbone dimensions (e.g., CLIP 512 vs 768) affect inversion? What is the runtime/efficiency cost of SPINE relative to baseline pose estimators?\n\n2. The paper concludes that geometry-grounded semantics hurt versatility but offers no concrete explanation beyond “supervised inductive bias.”\n\n3. Semantic localization results are mostly relative (DINO vs VGGT). It would be more informative to include absolute comparisons against: CLIP-only localization (as in LERF), Geometry-only cues (depth or SDF features). \n\n4. The term \"semantic embedding\" is misleading in L161. VGGT is not trained with semantic supervision, and I don't believe its intermediate features can be called semantic embedding. \n\n5. The core idea, comparing geometry-grounded vs. visual-only features in semantic radiance fields, is primarily evaluative.\nSPINE, the only algorithmic contribution, is a straightforward combination of: a shallow MLP mapping semantic embeddings to poses, and standard PnP-based refinement. Both steps are well established in prior literature (e.g., iNeRF [Yen-Chen 2021], CatNIPS [Chen 2024], and Splat-NAV [Chen 2025]). SPINE’s novelty lies mostly in not requiring an initial guess, but the authors never prove that it consistently converges without one — the results appear scene-specific and qualitative.\nOverall, the paper feels like an empirical case study rather than a fundamentally new framework or theoretical advance.\n\n6. Mathematical sections contain excessive exposition of standard concepts (e.g., SVD, Sobel operator). Section 4 devotes a full paragraph to PCA projection details, which are trivial and distract from the main analysis."}, "questions": {"value": "1. Could you provide any intuition or analysis for why geometry grounding degrades downstream task performance?\n2. Does SPINE generalize across unseen scenes, or is it trained per scene like a NeRF?\n3. You conclude that “visual-only features offer greater versatility.” How general is this conclusion? Do you believe this holds for all geometry-grounded models (e.g., MVDream, Depth-DINO, GeoCLR), or only VGGT? What would you recommend for future work — redesigning geometry-grounding losses or simply abandoning geometry-grounded semantics for robotics applications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "k5oMwqbMAf", "forum": "OwZ9osapdP", "replyto": "OwZ9osapdP", "signatures": ["ICLR.cc/2026/Conference/Submission18317/Reviewer_uFrD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18317/Reviewer_uFrD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761530535414, "cdate": 1761530535414, "tmdate": 1762928033865, "mdate": 1762928033865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether geometry-grounded vision backbones, specifically VGGT, can provide advantages over purely visual backbones such as DINOv2/DINOv3 when distilled into radiance fields for robotics-centric tasks. The authors evaluate three downstream capabilities: semantic content fidelity, open-vocabulary object localization, and radiance-field inversion. The study concludes that geometry-grounded features exhibit sharper geometric structure but do not improve semantic localization performance and even degrade pose inversion accuracy. The paper additionally introduces SPINE, a scene-specific inversion module that leverages semantic cues to recover camera pose without initialization.\n\nThe central message is that geometry-enhanced features do not necessarily translate into broader utility in 3D-aware semantic radiance fields, and that purely visual embeddings remain more versatile for downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses a timely and relevant question regarding the real-world value of geometry-grounded semantics for 3D robotics perception.\n\n2. Provides systematic comparisons across multiple semantic backbones and tasks.\n\n3. Experimental setup is generally clear and the negative results are informative for the community."}, "weaknesses": {"value": "1. **Novelty is limited**\n   - The core technical pipeline largely reuses existing radiance-field semantic distillation approaches, with the primary modification being the substitution of pretrained feature sources.\n   - SPINE follows a standard design (semantic prior + photometric refinement + PnP/RANSAC) and is trained per scene, further reducing novelty in system design.\n\n2. **Scope of geometry-grounded models is insufficient**\n   - Only VGGT is evaluated. Contemporary spatially grounded models such as DUSt3R, MASt3R, CroCo, or other geometric transformer variants are omitted.\n   - The conclusion that geometry-grounding harms versatility is based on a narrow model sample and may not generalize.\n\n3. **Lack of mechanistic insight**\n   - The paper observes performance degradation with geometry-grounded features but does not provide clear hypotheses or analysis explaining why geometry hurts semantic versatility.\n   - Without deeper investigation, the conclusions may appear anecdotal.\n\n4. **Incomplete evaluation of scalability and practicality**\n   - SPINE is trained per scene, similar to NeRF-style pipelines, raising concerns about scalability in real robotic deployments.\n   - No demonstration on larger-scale scenes, dynamic environments, or cross-scene generalization.\n\n5. **Overall maturity not yet sufficient**\n   - Although the question is valuable, the current implementation resembles an exploratory empirical study rather than a fully developed methodology.\n   - Lack of ablations on distillation design (e.g., shared hashgrids, separate semantic heads, language-only vs geometry-only distillation) limits interpretability."}, "questions": {"value": "1. Would the observed trend hold for other geometry-grounded models such as DUSt3R, MASt3R, or other spatial transformers?\n2. Can the authors provide more in-depth analysis explaining why geometry-grounding compromises semantic versatility? For example, changes in embedding smoothness, gradient stability, or photometric consistency?\n3. Is SPINE capable of generalizing across scenes, or can it be made scene-agnostic? If not, how do the authors envision scaling it in real robotic deployments?\n4. Could separate encodings rather than shared hash-grids improve geometry-grounded feature distillation?\n5. Do any tasks exist where the geometry-grounded semantic fields *do* offer measurable benefits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HWlB8Z6vzv", "forum": "OwZ9osapdP", "replyto": "OwZ9osapdP", "signatures": ["ICLR.cc/2026/Conference/Submission18317/Reviewer_mf1Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18317/Reviewer_mf1Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892516940, "cdate": 1761892516940, "tmdate": 1762928033404, "mdate": 1762928033404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors explore the effectiveness of vision-only and visual-geometry features on downstream tasks including edge computation, object localization, etc. \n\nAdditionally, the authors propose a SPINE framework for inverting radiance fields. More precisely, SPINE predicts poses directly from learned image features and then refines the poses by solving a PnP problem.\n\n\nExperiments on public datasets demonstrate that (1) visual-geometry features contain higher fidelity spatial content than visual-only features; (2) both features give close performance on semantic object localization; (3) visual-only features give higher accuracy radiance field inversion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1.\tInteresting topic. Recently, distilling semantic knowledge from foundation models (e.g. CLIP) into NeRFs or Gaussian Splatting is a hot research topic as also mentioned in the introduction. However, comparisons of the effectiveness of the visual-only features (e.g. DINO v2/v3) and visual-geometry features (e.g. VGGT) on different tasks are still open problems. Therefore, I believe the topic of this paper is interesting and the conclusions obtained from the experiments are useful. \n\n2.\tThe paper is well-organized. \n\n3.\tThe experiments, although conducted on a limited number of datasets, are relatively convincing."}, "weaknesses": {"value": "1.\tLimited contribution. The main contribution of this paper is the comparison of the effectiveness of visual-only and visual-geometry features on downstream tasks. Although it is interesting and useful, the contribution is not enough as an ICLR paper.\n\n2.\tThe second contribution of the paper is the SPINE framework for inverting radiance fields. However, the SPINE is essentially a pipeline for end-to-end relocalization task which takes features as input and predicts the 6-DoF camera poses. There are many works in this area [R1, R2, R3]. It would be better to give a discussion of this task.   \n\n[R1] PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization, Kendall  et al., ICCV 2015.\n\n[R2] Learning multi-view camera relocalization with graph neural networks, xue et al., ICCV 2020.\n\n[R3] The NeRFect Match: Exploring NeRF Features for Visual Localization, zhou et al., ECCV 2024."}, "questions": {"value": "It seems like some details of the method are missing, which causes the low readability, for example:\n\n1.\tThe details of f_l, f_s.\n\n2.\tHow to the match before PnP for pose estimation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dQuu8nTMsI", "forum": "OwZ9osapdP", "replyto": "OwZ9osapdP", "signatures": ["ICLR.cc/2026/Conference/Submission18317/Reviewer_HjDG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18317/Reviewer_HjDG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101909593, "cdate": 1762101909593, "tmdate": 1762928033042, "mdate": 1762928033042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}