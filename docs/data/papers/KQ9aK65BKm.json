{"id": "KQ9aK65BKm", "number": 18130, "cdate": 1758284203937, "mdate": 1759897130378, "content": {"title": "Graph-Refined Representation Learning for Few-Shot Classification via CLIP Adaptation", "abstract": "Few-shot image classification remains a fundamental challenge, as learning transferable representations from only a handful of examples often fails to generalize to unseen concepts. Recent advances benefit pre-trained vision-language models such as CLIP, yet their inherent biases and limited task adaptability hinder robust performance. We propose a novel **graph-driven cache refinement framework** that improves CLIP's prior knowledge with task-specific representation learning while preserving its lightweight inference. The first stage, **Inductive Statistical Subspace Aggregation (ISSA)**, partitions each feature into subspaces, builds fully connected intra-sample graphs, and applies statistical aggregation to capture robust subspace-level dependencies. The second stage, **Feature Subspace Propagation (FSP)**, globally diffuses contextual signals across subspaces while preserving their individuality, resulting in enriched embeddings that drive cache-based retrieval models. In particular, this refinement branch is active only during training, producing enhanced cache keys while ensuring graph-free, efficient inference. Across multiple benchmarks, our method consistently outperforms state-of-the-art approaches, establishing new performance standards in few-shot learning while retaining computational efficiency. Source code will be released to support reproducibility and further research.", "tldr": "", "keywords": ["Few-shot representation learning", "Vision-language models", "Graph neural networks", "CLIP adaptation", "Knowledge representation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c1fb4cf13f49136387e4d256c99348dbc647264c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a novel graph-driven cache refinement framework that improves CLIP’s prior knowledge for lightweight few-shot adaptation onto downstream tasks. The proposed method designs a two-stage graph learning approach. For each feature sub-vector (named subspace in this paper), in the first stage, the Inductive Statistical Subspace Aggregation (ISSA) updates it by summarizing neighborhood statistical information. Then, in the second stage, each subspace is further enhanced by absorbing contextual signals from other subspaces through the Feature Subspace Propagation (FSP) operation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well organized and easy to follow. In addition, the motivation of this work is well clarified.\n\n2. The idea of dividing the whole feature vector into different subspaces and constructing the graph between those subspaces is interesting.\n\n3. The authors have conducted extensive experiments, and the experimental results indicate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. One of my major concerns lies in the training process of the proposed method. I cannot understand how the graph-driven knowledge influences the optimization of the cache keys. Specifically, the authors claim that the keys $\\Theta_{train}$ “are initialized with the L2-normalized CLIP visual features from the support set, and updated via gradient during training”, but it can be observed from Eq. (4) that the graph-enhanced features $\\tilde f_{train}$ do not influence the training of $\\Theta_{train}$, which is only optimized by the first term of Eq. (4). In addition, the second term in Eq. (4) only trains the parameters of the constructed graph, while those parameters of CLIP are frozen, but this graph is not utilized in the inference stage, so I do not know how does this term affect the cache keys and final performance.\n\n2. In the Inductive Statistical Subspace Aggregation operation, the authors claim that the aggregation of neighborhood statistical information can “capture fine-grained relational dependencies and discover robust feature representations”. I am wondering why the neighborhood statistical information could help to extract robust feature representations.\n\n3. For the experimental results, the competing benchmarks appear to be outdated. Could the authors compare their proposed method with those published in 2025? Such as in CVPR 2025, ICML 2025, ICCV 2025, or ICLR 2025?\n\n4. I am confused about the results in Table 6. If I am misunderstanding, please correct me. From Table 6, we can find that the cosine similarities between different partitioned subspaces are nearly equal to 1. Does this mean that those subspaces are becoming more redundant since they become more similar? If so, it may indicate that the graph refinement operation can lead to information loss, since different subspaces (i.e., feature dimensions) cannot represent distinct visual information. Furthermore, from the results, we can see that the graph refinement will modify the space of visual features, it may degrade the vision-language alignment of CLIP model, so will the graph refinement influence the effectiveness of the pre-trained CLIP text-based classifier? Could the authors provide more explanations and discussions for these results?"}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZNiZpolZOW", "forum": "KQ9aK65BKm", "replyto": "KQ9aK65BKm", "signatures": ["ICLR.cc/2026/Conference/Submission18130/Reviewer_JNSy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18130/Reviewer_JNSy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18130/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982595490, "cdate": 1761982595490, "tmdate": 1762927891642, "mdate": 1762927891642, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework for few-shot adaptation of CLIP, building upon the \"cache model\" paradigm (e.g., Tip-Adapter-F). The authors argue that existing methods treat CLIP's global feature embedding as a monolithic, unstructured vector, thereby ignoring the rich relational information within the feature's dimensions. To address this, the proposed method introduces a \"graph-driven cache refinement\" process that is active only during training. The result is a method that (in theory) distills the rich, intra-feature relational knowledge from the graph into the cache keys, while maintaining the high efficiency (no GNN, no extra parameters) of the cache model at inference time. The authors show SOTA performance on 11 benchmarks, especially in the 1-8 shot regime."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Excellent Motivation and Core Idea: The paper's key insight—that a global CLIP embedding is not a monolithic vector but a structured representation whose internal subspaces can be refined—is excellent. The idea of using a GNN to model intra-feature relationships, rather than the typical inter-sample (transductive) relationships, is novel and clever.\n\nStrong \"Best of Both Worlds\" Design: The framework's design is its strongest point. It intelligently \"distills\" the knowledge from the complex graph refinement process into the simple cache keys. This allows the model to leverage deep, relational feature learning during training while paying zero computational overhead for it at inference time. This is a very elegant solution to the usual trade-off between performance (adapters, prompts) and efficiency (cache models).\n\nComprehensive Empirical Validation: The paper presents SOTA results across 11 diverse datasets (Table 1), with particularly large gains in the low-shot (1-4) regime (e.g., +5% avg. at 1-shot over the next-best adapter). This is a very strong empirical contribution."}, "weaknesses": {"value": "My concerns are minor and are mostly related to clarifying the exact mechanism of ISSA and the training.ISSA Aggregation (Eq. 1):\n\n1.  The formulation of the ISSA aggregation in Eq. 1 is confusing and seems non-standard. It appears to define $H_u$ as a $3 \\times F$ matrix of the three statistical aggregations (mean, max, std), and then applies a full self-attention mechanism within this tiny $3 \\times 3$ \"graph\" of statistics.\n\nQ1: Is this interpretation correct? If so, what is the intuition behind applying self-attention to the statistics themselves? A more standard GNN approach would be to concatenate the statistics ($[h_{mean}, h_{max}, h_{std}]$) into a single $1 \\times 3F$ vector and apply a linear layer. The ablation in Table 3 shows \"attn\" is best, but the justification for why it's better than concatenation (\"cat\") or projection (\"proj\") is missing.\n\n2. Training Procedure (Eq. 4): There is a potential ambiguity in the training loss (Eq. 4). The first term (cache retrieval) uses the original feature $f_{train}$, while the second term (CLIP classification) uses the graph-refined feature $\\tilde{f}_{train}$.\n\nQ2: Does the gradient from the $\\tilde{f}_{train}$ term flow back to update $\\Theta_{train}$? The text says: \"This residual-style formulation ensures that gradients from the CLIP classifier stream guide the adaptation of cache keys\". This implies $\\tilde{f}_{train}$'s loss also updates $\\Theta_{train}$, but the equation logits = ... + \\tilde{f}_{train}W_c^T does not contain $\\Theta_{train}$.\n\nPlease clarify the exact gradient flow. Is $\\Theta_{train}$ only updated by the $f_{train}$ term? Or is $\\Theta_{train}$ updated by both terms? Or, as a third possibility, is the refined feature $\\tilde{f}_{train}$ used to also calculate the cache retrieval term (i.e., replacing $f_{train}$ in the first term)? This last option seems most logical but is not what Eq. 4 says.\n\n3. Partitioning Strategy (Table 6): The authors claim that \"contiguous\" partitioning is better than \"random,\" but the results in Table 6 show a negligible difference (e.g., 47.2 vs 47.0; 97.9 vs 97.8). This slightly undermines the motivation that the contiguous chunks have some inherent structure. Suggestion: The authors should tone down this claim. The real takeaway from this ablation seems to be that the method is robust to the partitioning strategy, as long as all feature dimensions are covered. The strong performance of both contiguous and random splits suggests the model is truly learning the relationships, rather than exploiting a pre-existing spatial structure in the feature vector."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rJ7ZV1hAMu", "forum": "KQ9aK65BKm", "replyto": "KQ9aK65BKm", "signatures": ["ICLR.cc/2026/Conference/Submission18130/Reviewer_eVfs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18130/Reviewer_eVfs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18130/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999847205, "cdate": 1761999847205, "tmdate": 1762927891277, "mdate": 1762927891277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper improves few-shot image classification by enhancing CLIP’s task adaptability while keeping inference efficient. The authors introduce a training-only graph-based refinement framework that strengthens CLIP’s representations without adding test-time cost. It operates via two steps: subspace-level statistical aggregation to capture robust intra-sample dependencies, and subspace propagation to enrich contextual information. The refined cache keys are used at inference, but the graph module is discarded. Experiments across multiple benchmarks show consistent gains over CLIP-based methods and competitive state-of-the-art performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Proposes a subspace-level refinement strategy for CLIP-based few-shot learning, offering a different angle compared to prompt/adaptor tuning.\n\n2. Training-only graph refinement is a practical design that improves representations without adding inference cost.\n\n3. Method is clearly presented and experimentally validated across multiple benchmarks, showing consistent improvements."}, "weaknesses": {"value": "1. The introduction quickly jumps into the technical proposal without clearly framing the core challenge, limitations of CLIP, and intuition behind subspace graph reasoning.\n\n2. Although the empirical section is quite thorough, some ablations do not cover multiple datasets, slightly limiting conclusions about generality. More detailed computation-vs-gain analysis would strengthen claims about efficiency and practicality.\n\n3. Missing comparisons with very recent (2024–2025) CLIP-adaptation or FSL works, especially those exploring subspace modeling or cache refinement approaches. Authors should add the latest baselines if possible.\n\n4. While attractive, the paper could better articulate why training-time subspace propagation leads to robust inference-time performance without graphs, e.g., theoretical perspective or more analytical visualization."}, "questions": {"value": "1. Could the authors more explicitly describe why subspace partitioning addresses CLIP’s bias in few-shot regimes? Any theoretical or empirical intuition beyond empirical gains?\n\n2. Please further explain why the propagated subspace enhancement can be discarded at inference with minimal loss. Are there scenarios where this fails (e.g., domain shift, higher-shot scenarios)?\n\n3. Can you provide a more detailed computational breakdown (training time, memory, inference speed) relative to prompt tuning and adapter baselines?\n\n4. Are there 2025 FSL methods the authors can compare against, such as the latest CLIP-tuning or cache-refinement works?\n\n5. Some ablations appear limited to a subset of datasets. Any results across additional benchmarks to verify robustness?\n\n6. How sensitive is performance to the number of subspaces and graph connectivity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9G85xLcqvL", "forum": "KQ9aK65BKm", "replyto": "KQ9aK65BKm", "signatures": ["ICLR.cc/2026/Conference/Submission18130/Reviewer_1zWJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18130/Reviewer_1zWJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18130/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762097843305, "cdate": 1762097843305, "tmdate": 1762927890835, "mdate": 1762927890835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}