{"id": "hdErPCTui6", "number": 5275, "cdate": 1757885485545, "mdate": 1759897983769, "content": {"title": "Attribution-Guided Exit Policy for Reliable Early-Exit Inference", "abstract": "Early-exit neural networks enhance efficiency by producing predictions at intermediate layers, but their utility depends critically on the exit policy that decides when to stop inference. Conventional policies, typically based on confidence thresholds, lack transparency and provide little justification for their choices. We address this limitation with two contributions. First, we introduce Progressive Feature Attribution Maps (PFAM), which capture the network’s evolving focus through both exit-wise and cumulative attributions. Second, we propose the Interpretability-Based Early-Exit Score (IEES), a composite metric that unifies confidence, attribution progression, and feature relevance. To eliminate attribution overhead at deployment, we further develop a lightweight proxy predictor that estimates IEES from inexpensive, attribution-free features. Experiments on ResNet-18 (CIFAR-10), MSDNet (CIFAR-100), and MobileNetV3 (ImageNet) show that our approach improves accuracy by up to 3\\% while reducing computation by up to 26\\% compared to full-depth inference. Analyses demonstrate that IEES-driven exits are semantically coherent and reliable, while PFAM achieves near-perfect monotonicity, convergence alignment, and strong faithfulness across benchmarks. Together, these advances enhance the transparency of anytime prediction and establish a foundation for explainable early-exit inference.", "tldr": "We propose IEES and PFAM, two modules that guide early-exit decisions in DNNs by combining confidence, attribution cues, and consistency—improving efficiency and explainability for real-world deployment.", "keywords": ["Early-Exit Neural Networks", "Interpretable Deep Learning", "Explainable AI (XAI)", "Exit Policy Optimization", "Attribution Maps", "Semantic Consistency", "PFAM", "Anytime Prediction"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e468c948646d56d67dda0afe0eed2d6300422323.pdf", "supplementary_material": "/attachment/6e9f55bccf93a867a7f0020df13f29e3dd7243ef.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces an attribution-guided framework for making early-exit neural networks both more efficient and interpretable. Instead of deciding when to exit purely based on prediction confidence, the authors propose an Interpretability-Based Early-Exit Score (IEES) that combines three signals: confidence, attribution strength (how strongly class-relevant features are activated), and attribution stability (how consistent the model’s focus remains across layers). To make this practical at inference time, they train a lightweight proxy regressor. The accompanying Progressive Feature Attribution Maps (PFAM) provide a visual and quantitative explanation of how model attention evolves with depth. Their method achieves better accuracy-latency tradeoffs than tuned confidence-based policies across a few different datasets. PFAM also outperforms standard attribution methods in coherence and stability, and a small user study finds the explanations more semantically meaningful."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear and modular framework that integrates explainability directly into the decision mechanism for early exits.\n- Consistent empirical gains in both accuracy and efficiency across multiple datasets and architectures.\n- Careful control experiments (e.g., “confidence (IEES-matched)”) that isolate the effect of the scoring function from exit distribution.\n- PFAM provides interpretable visualizations and quantitative evidence of progressive, stable attributions.\n- Ablations and diagnostic analyses that clarify how each IEES component contributes."}, "weaknesses": {"value": "-  The conceptual contribution mainly lies in combining confidence-based early exits and attribution methods. While the unification through the IEES score is elegant, it represents an empirical heuristic more than a theory-based improvement.\n- All experiments are in the image classification benchmarks (CIFAR-10/100, ImageNet) domain. Despite claims of being attribution and architecture-agnostic, PFAM and IEES are only demonstrated on CNNs using Grad-CAM.\n- The reported gains in accuracy and efficiency are small and rely on dataset-specific grid searches over multiple hyperparameters. This tuning process limits scalability, and it remains unclear how robust the method is under real-world use cases without extensive recalibration."}, "questions": {"value": "1) How did you decide on a linear combination of terms for IEES? Did you try nonlinear combinations or learned functions (e.g., small MLP or decision trees) to see if the linearity assumption limits performance?\n2) Since the grid search for hyperparameters $w$ and $\\tau$ is dataset-specific, do you see stable ratios across datasets, or do the weights need to be recalibrated each time?\n3) What happens if you replace Grad-CAM with another attribution method, i.e. does IEES still improve over confidence-based exits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4NmOiuQpbu", "forum": "hdErPCTui6", "replyto": "hdErPCTui6", "signatures": ["ICLR.cc/2026/Conference/Submission5275/Reviewer_7FyA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5275/Reviewer_7FyA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934927583, "cdate": 1761934927583, "tmdate": 1762917984544, "mdate": 1762917984544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission introduces IEES, an exit policy for early-exit image classification models. The proposed approach first designs a novel metric inspired from the explainable AI domain and based on semantic reliability, that combines commonly used confidence with class-relevant activation strength and stability in attribution focus. At deployment time the proposed metric is approximated through a random forest predictor, to reduce the computational burden of attribution computation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Revisiting the exit policy in input-dependent computation approaches is a very timely and interesting problem.\n- The proposed unification of efficiency and explainability through the proposed attribution-stability based metric (combined with previous confidence solutions) is insightful and demonstrated effectiveness in the experimental evaluation under the examined assumptions. \n- The manuscript is very polished and easy to follow."}, "weaknesses": {"value": "1. Some of the design choices of the proposed approach are not adequately justified. For example, since the proposed metric is based on stability, why does the proxy predictor in Eq.7 only consider features from the current exit as inputs?\n2. Given the fact that a predictor is trained to serve as exit policy (approximating the proposed metric), it is unclear why the authors chose to go with prediction of a semantic reliability metric, rather than direct prediction of the early-exit decision itself. A clear comparison between the two is required to justify the contribution of this work in the online (inference-time) setting. \n3. Since stability is a key property of the proposed exit policy, an obvious baseline would be to consider the stability of logits, or predictions between successive early exits, rather than the proposed attribution-based metric. Further comparisons are required to demonstrate the effectiveness of the proposed components in this context. \n4. Although the proposed results showcase the effectiveness of the proposed early exit scheme across some CNN models and image classification datasets, an application of the proposed approach to Transformer architectures for the same tasks is important to showcase the generality of the proposed components and applicability to SoTA approaches. \n5. Although briefly demonstrated in the appendix, further experiments are required to compare the proposed metric with its proxy prediction through random forests in terms of out-of-distribution sample handling and other important properties that the original metric would be able to capture, but the proposed approximation is most likely not exposed to during training."}, "questions": {"value": "Please consider replying on the comments raised above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7lFgs0ZCeK", "forum": "hdErPCTui6", "replyto": "hdErPCTui6", "signatures": ["ICLR.cc/2026/Conference/Submission5275/Reviewer_diB3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5275/Reviewer_diB3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948416756, "cdate": 1761948416756, "tmdate": 1762917984096, "mdate": 1762917984096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work presents an explainability-aware early exit framework that built using three components: 1) Progressive feature attribution maps that visualize how model focus evolves across exits, producing localized and cumulative attributions. 2) It also provides an interpretability-based Early-Exit score (IEES) that quantifies the exit reliability by unifying model confidence, feature relevance and attribution stability. 3) Finally the method trains a lightweight IEES predictor that can provide a pseudo-IEES score during inference. The method is backed by experimental analysis on ResNet-18, MSDNet and MobileNetV3 architectures."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The method proposes an explainability-aware early exit framework, which is important. \n\n2) The IEES scores proposed in the paper seem to be a reliable metric for making an exit decision, which makes the prediction reliable and trustworthy. \n\n3) The paper is well written and easy to follow."}, "weaknesses": {"value": "1) Multiple baselines that work on improving the trustworthiness of EE frameworks are missing [1], [2], [3], [4], [6], they should not only be cited but also be compared. \n\n2) The method heavily depends on the validation dataset, as it sets multiple hyperparameters using that, it restricts the generalization of the method. Also, as the method has a number of exits + (w_1, w_2, w_3) as hyperparameters, the grid search seems extremely costly. Also, the method trains the lightweight IEES score predictor on the validation set, which again tells of the dependence on the validation set. \n\n3) The authors only experiment with the image classification tasks, which is a major concern as the performance needs to be shown on the recent language generation and other GenAI models to make the work worthy. \n\n4) The paper does not provide any model training details what loss function to use, what weights to give for exits, etc., is left for the reader to assume. \n\n5) The author missed multiple relevant papers, some of them are below:\n\n\n[1] https://papers.neurips.cc/paper_files/paper/2022/file/6fac9e316a4ae75ea244ddcef1982c71-Paper-Conference.pdf\n\n[2] https://proceedings.neurips.cc/paper_files/paper/2024/file/ea5a63f7ddb82e58623693fd1f4933f7-Paper-Conference.pdf\n\n[3] https://openaccess.thecvf.com/content/WACV2024/papers/Meronen_Fixing_Overconfidence_in_Dynamic_Neural_Networks_WACV_2024_paper.pdf\n\n[4] https://aclanthology.org/2024.findings-acl.101.pdf\n\n[5] https://aclanthology.org/2023.emnlp-main.362.pdf"}, "questions": {"value": "1) How authors decide the layers in which exits are attached. Also, based on the design, the method chooses different thresholds for each exit and performs a grid search to fix them; it will become infeasible when the model becomes large (32-layer language model). Any thoughts?\n\n2) A suggestion will be, please don't write sentences like \"provide little justification for their choices\" (line 015), methods usually develop scores which are theoretically grounded."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cnjCcsho4P", "forum": "hdErPCTui6", "replyto": "hdErPCTui6", "signatures": ["ICLR.cc/2026/Conference/Submission5275/Reviewer_TTQm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5275/Reviewer_TTQm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989999581, "cdate": 1761989999581, "tmdate": 1762917983853, "mdate": 1762917983853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes IEES (Interpretability-based Early Exit Score), a method for determining when to terminate inference in early-exit neural networks. Instead of relying only on prediction confidence, IEES integrates three complementary cues into a single decision score: prediction confidence, class-weighted activation strength, and attribution stability. The accompanying Progressive Feature Attribution Maps (PFAM) visualize how the model’s semantic focus evolves across exits. Experiments on CIFAR-10, CIFAR-100, and ImageNet show improved accuracy–latency trade-offs over confidence-based policies and competitive performance with recent early-exit baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Clear motivation for combining multiple cues (confidence, activation, and stability) to guide exit decisions.\n+ Good experimental evidence across three datasets, including ablations and robustness tests under threshold perturbation and distribution shift.\n+ Quantitative evaluation of interpretability via faithfulness and alignment metrics, and introduction of PFAM as an intuitive visualization tool.\n+ Well-written and well-structured paper with clear definitions and improved presentation quality."}, "weaknesses": {"value": "- The evaluation scope remains limited to CNN-based image classifiers; no results for ViTs, transformers, or non-vision domains.\n- The claim of “anytime prediction” is only approximate since exits occur at discrete layers. \n- The computational overhead of attribution computation is not quantified, leaving the efficiency benefit somewhat incomplete.\n- Comparisons could include more diverse dynamic routing methods (e.g., RANet, DeeBEE) to better contextualize IEES performance.\n- Some hyperparameter tuning details (e.g., validation protocol for threshold search) could be explained more precisely.\n- A highly relevant work is missing from related work: \"Early-exit Convolutional Neural Networks\" by Demir et al. \n- Fonts in plots (Fig2& Fig3) are too small. \n- I am not sure why evaluation on domain-shift needed. \n\nMinor: \n- It is not clear what f is in Eq.1. \n- L49: I think the definition of F_i(x) is important to understand the method. It should be moved to the main text from Appendix. \n- \"standard augmentation\" should be clarified (L201). \n- In Table 1, are the numbers under E1...E4 accuracies of examples exiting that specific gate? This is not clear from the caption of main text."}, "questions": {"value": "- Since there are only three or four hardcoded exits, why do you call it \"anytime prediction\"? \n- L214: What is the point of comparing PFAM with GradCAM? Isn't PFAM Grad CAM as mentioned in Eq.2? \n- Table 1 shows that early-exit accuracy is better than no-early-exit accuracy. While authors provide an explanation in Section 5.1, this is not a consistent result in the literature (as also evidenced in Table 2 column \\delta accuracy). Can you explain this further or provide more visual results to support your explanation (e.g. showing that examples exiting at later gates are more complicated examples)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PRjVwd6HuS", "forum": "hdErPCTui6", "replyto": "hdErPCTui6", "signatures": ["ICLR.cc/2026/Conference/Submission5275/Reviewer_X2q6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5275/Reviewer_X2q6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992693897, "cdate": 1761992693897, "tmdate": 1762917983640, "mdate": 1762917983640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}