{"id": "2IftRjRB07", "number": 2873, "cdate": 1757295387869, "mdate": 1763256051829, "content": {"title": "FlashWorld: High-quality 3D Scene Generation within Seconds", "abstract": "We propose FlashWorld, a generative model that produces 3D scenes from a single image or text prompt in seconds, $10 \\sim 100\\times$ faster than previous works while possessing superior rendering quality.\nOur approach shifts from the conventional multi-view-oriented (MV-oriented) paradigm, which generates multi-view images for subsequent 3D reconstruction, to a 3D-oriented approach where the model directly produces 3D Gaussian representations during multi-view generation.\nWhile ensuring 3D consistency, 3D-oriented method typically suffers poor visual quality.\nFlashWorld includes a dual-mode pre-training phase followed by a cross-mode post-training phase, effectively integrating the strengths of both paradigms.\nSpecifically, leveraging the prior from a video diffusion model, we first pre-train a dual-mode multi-view diffusion model, which jointly supports MV-oriented and 3D-oriented generation mode. \nTo bridge the quality gap in 3D-oriented generation, we further propose a cross-mode post-training distillation by matching distribution from consistent 3D-oriented mode to high-quality MV-oriented mode. \nThis not only enhances visual quality while maintaining 3D consistency, but also reduces the required denoising steps for inference.\nAlso, we propose a strategy to leverage massive single-view images and text prompts during this process to enhance the model's generalization to out-of-distribution inputs.\nExtensive experiments demonstrate the superiority and efficiency of our method.", "tldr": "a generative model that produces high-quality 3D scenes from a single image or text prompt in seconds", "keywords": ["3D Scene Generation", "Multi-view Diffusion Models", "World Models", "Distribution Matching Distillation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b14a6c72a4044d9b08b20fbcc61442f5d88f977b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses text- and image-to-scene generation. Previous methods that leverage powerful video foundation models (VDMs) face a trade-off: multi-view-oriented approaches lack 3D consistency, while 3D-oriented methods often yield poor visual quality. This paper introduces pre-training and post-training strategies for VDMs to improve 3D consistency and accelerate video generation. For pre-training, the authors add a 3DGS decoder (the \"3D-oriented\" branch) to inject 3D priors and enhance the latent video diffusion model’s 3D consistency. For post-training, they distill a multi-view-oriented teacher into a 3D-oriented student to speed up generation. Comprehensive experiments show the proposed methods outperform prior state-of-the-art approaches in both qualitative and quantitative evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The two proposed training strategies are novel, efficient, and effective for 3D scene generation.\n\n- The authors present comprehensive experiments that convincingly support their claims.\n\n- Comparison results show the proposed method outperforms both image-to-3D and text-to-3D approaches, producing finer-detail renderings and faster generation times."}, "weaknesses": {"value": "- The writing flow is poor and Sec. 3.2 feels chaotic. For example, the sentence To generate a 3D scene, the 3D-oriented multi-view generation process alternates between denoising and noise injection steps to enhance sample quality.'' is confusing: is the goal to generate a 3D scene, to enhance sample quality, or both? \n\n- For Sec. 3.3, the reason the model improves the quality of out-of-domain (OOD) data is unclear. As I understand it, the approach uses camera-trajectory augmentation to enhance the model’s generalizability and also discards the GAN loss during training. Which of these two changes is more important for OOD performance? Data augmentation is easy to see as a way to improve generalization, but why does including a GAN loss lead to poor generalizability?"}, "questions": {"value": "1. There may be a typo on lines 254--255. You refer to λ in relation to Eq.(6), but Eq.(6) does not contain λ. \n\n2. Missing some relative references:\n- VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model,  which proposes a 3D-aware sampling strategy;\n- AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction, using 4DGS to alleviate the inconsistency from multi-view video generation. \n\nPlease consider add the above relative references.\n\nIn summary, the authors propose interesting pre-training and post-training strategies for video-diffusion model training to improve 3D consistency and quality in scene generation, achieving state-of-the-art results in both quantitative and qualitative experiments. However, the writing flow is poor: some sentences are confusing, and I have concerns about the model’s generalizability. As a result, I am inclined to give a borderline-accept score, though I would be happy to raise it if the authors address these main concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Shl3Yt8T1r", "forum": "2IftRjRB07", "replyto": "2IftRjRB07", "signatures": ["ICLR.cc/2026/Conference/Submission2873/Reviewer_mRkm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2873/Reviewer_mRkm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2873/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761544902691, "cdate": 1761544902691, "tmdate": 1762916423835, "mdate": 1762916423835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles scene generation from text and images. Existing video foundation model approaches trade off between multi-view methods, which struggle with 3D consistency, and 3D-focused methods, which often compromise visual fidelity. To overcome this, the authors propose pre-training and post-training techniques for VDMs: a 3DGS decoder (the “3D-oriented” branch) is added during pre-training to inject geometric priors and improve 3D coherence, and a teacher–student distillation step transfers knowledge from a multi-view-oriented teacher to a 3D-oriented student to speed up generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clearly written and easy to follow.\n\n- The organization is logical, and most design choices are validated with ablation studies.\n\n- The method shows notable improvements over prior work, particularly in preserving fine-grained details."}, "weaknesses": {"value": "The paper is novel and effective; the following points are offered as constructive suggestions for further improvement.\n\n- The diversity and scale of generated scenes are still constrained by the coverage of existing datasets.\n\n- The model currently struggles with accurately generating fine-grained geometry, mirror reflections, and articulated objects. These issues may be alleviated by incorporating depth priors and more 3D-aware structural information\n\n- Although FlashWorld does not use explicit depth supervision, its 3D Gaussian Splatting (3DGS) outputs can be used to extract depth maps. However, the quality of the resulting depth information could be improved."}, "questions": {"value": "There may be an error in Eq. (6), where is \\lambda ?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns"]}, "details_of_ethics_concerns": {"value": "There are no obvious ethics issues."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7Eu2z5w465", "forum": "2IftRjRB07", "replyto": "2IftRjRB07", "signatures": ["ICLR.cc/2026/Conference/Submission2873/Reviewer_m4Bg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2873/Reviewer_m4Bg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2873/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970649502, "cdate": 1761970649502, "tmdate": 1762916423408, "mdate": 1762916423408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a generative model that produces high-quality 3D scenes from a single image or text prompt in just seconds—10-100× faster than prior methods. FlashWorld’s key innovation is a distillation strategy that transfers high visual fidelity from a multi-view-oriented diffusion teacher to a 3D-oriented student, improving 3D consistency. The authors also introduce an out-of-distribution co-training strategy to improve generalization to scenes beyond the training distribution. Extensive experiments show FlashWorld outperforms state-of-the-art methods in both generation quality and inference speed."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clearly written and easy to follow.\n- Experimental validation is thorough, with a comprehensive benchmark against many competing methods.\n- The out-of-distribution co-training approach is an effective and efficient way to improve generalization and robustness."}, "weaknesses": {"value": "-  Although the model demonstrates strong and generalizable generation capabilities, its video-rendering performance is not extensively evaluated in the main paper.\n- As noted in the limitations, the model still struggles with fine-grained geometry, mirror reflections, and articulated objects."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VIaWHKe44g", "forum": "2IftRjRB07", "replyto": "2IftRjRB07", "signatures": ["ICLR.cc/2026/Conference/Submission2873/Reviewer_N491"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2873/Reviewer_N491"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2873/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980672130, "cdate": 1761980672130, "tmdate": 1762916422981, "mdate": 1762916422981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FlashWorld, a framework for fast and high-quality 3D scene generation that addresses the critical speed-quality trade-off in current methods. The authors propose a cross-mode distillation approach that leverages both MV-oriented (multi-view) and 3D-oriented generation modes. The key innovation lies in using dual-mode pre-training followed by cross-mode post-training, where the MV-oriented mode serves as a teacher to provide visual quality while the 3D-oriented mode acts as a student to ensure geometric consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of training a dual-mode model and using cross-mode distillation for post-training is creative and well-motivated. Using the MV-oriented mode as teacher for visual quality while training the 3D-oriented mode for consistency is an elegant solution to the speed-quality trade-off.\n\n- Achieving ~9 second generation time while maintaining SOTA quality represents a significant practical contribution. The 10-100× speedup over prior work (CAT3D, Wonderland, etc.) makes this approach much more suitable for real-world applications."}, "weaknesses": {"value": "- Section 3.2 (cross-mode post-training) needs more detail. This stage appears to integrate DMD2 with the dual-mode diffusion model, but the step-by-step procedure is unclear. Please add a training algorithm box or pseudocode to clarify the process.\n- Include NVS comparisons with video-based methods such as TrajectoryCrafter, GEN3C, and ViewCrafter.\n- Provide depth visualizations of generated scenes and compare against baselines."}, "questions": {"value": "- Does the Out-of-Distribution (OOD) data improve only text-to-3D, or does it also help image-to-3D? The gains for text-to-3D are clear, but the impact on image-to-3D is not.\n- Line 249 is confusing: “we additionally update an MV-oriented student model at a lower frequency.” From Fig. 3, it seems the 3D- and MV-oriented modes share the same DiT backbone, whose latent output feeds the 3DGS decoder. If you update the 3D-oriented model, does that also update the DiT (and therefore the MV-oriented model)? Please clarify which parameters are shared or frozen during training and how updates are applied.\n- How does the method handle scenes with significant occlusions or transparent objects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6ogfbtiLME", "forum": "2IftRjRB07", "replyto": "2IftRjRB07", "signatures": ["ICLR.cc/2026/Conference/Submission2873/Reviewer_wnKK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2873/Reviewer_wnKK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2873/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993247927, "cdate": 1761993247927, "tmdate": 1762916422071, "mdate": 1762916422071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear Reviewers,\n\nWe would like to sincerely thank you for all your valuable suggestions, which have helped us make the paper more comprehensive and complete. In this revision, we have updated the following content:\n\n1. Detailed workflow and pseudocode of few-step generation and cross-mode post-training in **[Appendix B, Lines 816-898]**.\n2. More comparisons:\n- Depth comparison with Director3D and Prometheus in **[Fig. 9, Lines 1065-1079]**.\n- NVS comparison with ViewCrafter  in **[Fig. 10, Lines 1080-1010]**.\n- Video quality assessment on Text-to-3D as follows:\n\n**Table: Q-align VQA scores of rendered videos of different methods (higher is better)**\n\n|  | T3Bench-200 | DL3DV-200 | WorldScore-200 |\n| --- | --- | --- | --- |\n| Director3D | 0.4841 | 0.6682 | 0.5388 |\n| Promtheus | 0.4168 | 0.5088 | 0.5839 |\n| Ours | **0.7676** | **0.8286** | **0.7219** |\n\n3. An ablation study on image-to-3D scene generation task with WorldScore Benchmark as follows:\n\n**Table: Ablation study of OOD co-training on WorldScore Benchmark**\n\n|  | 3DConsistency | PhotometricConsistency | ObjectControl | ContentAlignment | StyleConsistency | SubjectiveQuality | Average |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| w/o OOD | **87.35** | **90.12** | 34.94 | 44.87 | **83.16** | 46.24 | 64.45 |\n| Full model | 85.87 | 86.72 | **49.61** | **53.96** | 81.52 | **54.63** | **68.72** |\n\n---\n\nRegarding other concerns and questions you have raised, we have provided detailed and point-to-point responses under the corresponding comments.\n\nWe look forward to your feedback on our responses, and we also welcome you to raise new questions and engage in discussions.\n\nSincerely,\n\nFlashWorld Authors"}}, "id": "thq0zACD8V", "forum": "2IftRjRB07", "replyto": "2IftRjRB07", "signatures": ["ICLR.cc/2026/Conference/Submission2873/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2873/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission2873/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763340568755, "cdate": 1763340568755, "tmdate": 1763340568755, "mdate": 1763340568755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}