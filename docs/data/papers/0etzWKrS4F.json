{"id": "0etzWKrS4F", "number": 2167, "cdate": 1757007063068, "mdate": 1763406902290, "content": {"title": "Beyond the Linear Separability Ceiling: Aligning Representations in VLMs", "abstract": "A challenge in advancing Visual-Language Models (VLMs) is determining whether their failures on abstract reasoning tasks, such as Bongard problems, stem from flawed perception or faulty top-down reasoning. To disentangle these factors, we introduce a diagnostic framework centered on the Linear Separability Ceiling (LSC), the performance achievable by a linear classifier on a VLM's raw visual embeddings. Applying this framework to state-of-the-art VLMs, we uncover a pervasive ``alignment gap'', where most models fail to generatively outperform the linear separability of their own representations. We find that the few models surpassing this ceiling do so via two mechanisms: by further refining visual representations into a more linearly separable format or by executing non-linear decision logic. We demonstrate that this bottleneck is not a fundamental limitation but a solvable alignment issue. By augmenting standard next-token prediction with a contrastive objective, our method forces the model to reason via image-to-image comparison rather than textual mediation. This improves the linear structure of representations, enabling the model to significantly surpass the LSC on abstract binary classification tasks.", "tldr": "next token + contrastive objective in vlms helps resolve alignment gap between perception and reasoning", "keywords": ["visual-language models", "abstract visual reasoning", "Bongard problems", "representation alignment", "contrastive learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/43a5a27105aead1e36ad6db92f257f435644badf.pdf", "supplementary_material": "/attachment/9be52cdac7771ed1751028b3a513a8185bbf8b53.zip"}, "replies": [{"content": {"summary": {"value": "The paper investigates why VLMs fail on abstract visual reasoning tasks such as the Bongard problems. The paper proposes the Linear Separability Ceiling (LSC), a diagnostic measure of how well a simple linear classifier performs on a VLM’s visual embeddings. It serves as a baseline to test whether a model’s reasoning pipeline adds non-linear reasoning beyond what its visual features already provide.\nThe authors evaluate several vLMS (e.g. Gemma, Pixtral, Phi, Qwen, InternVL) and realize that most models fail to exceed their own LSC, implying that their reasoning components are not effectively leveraging visual representations.\nThey also identify two strategies by which the VLMs can surpass the LSC\n1) Enhanced linear separability: improving internal representations to be more discriminative\n2) Non-linear decision logic: leveraging deeper reasoning pathways beyond linear readouts \n\nThe paper also proposes a finetuning method that combines next-token prediction and a contrastive loss to improve representation alignment. They show that this object allows models to surpass LSC and improve in-domain and cross-domain reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The concept of an “alignment gap” between perception and reasoning reframes common VLM failures through a geometric and statistical lens\n2) The paper’s decomposition of reasoning into linear vs. non-linear computational mechanisms (perceptual refinement vs. reasoning) provides conceptual clarity that could generalize beyond VLMs"}, "weaknesses": {"value": "1) The experimental validation is limited to binary image-to-text retrieval variants of Bongard-style tasks (Bongard OpenWorld and Bongard HOI), where the model must choose between two options given a query image. While these are well-defined tests for abstract reasoning, they represent a highly specific and simplified evaluation setup. It remains unclear whether the proposed framework and fine-tuning method would generalize to broader vision-language reasoning tasks such as open-ended visual question answering, caption generation. The improvements demonstrated may partially stem from optimizing for the same binary discriminative signal introduced by the contrastive objective, rather than from a genuinely general enhancement of multimodal reasoning.\n2) Because the proposed contrastive objective directly optimizes for improved separability between positive and negative examples, part of the observed performance gain could be attributed to alignment with the evaluation metric itself (linear separability), rather than improved reasoning. Additional experiments on independent tasks not directly linked to the contrastive loss would help verify generalization.\n3) The paper notes that the combined objective (L_combined) can induce catastrophic forgetting and prompt-format overfitting. It is unclear whether the performance of the finetuned model on general vision-language benchmarks (e.g. VQA benchmarks) is preserved after finetuning.\nFurthermore, the paper provides little quantitative analysis of how the weighting between the next-token prediction and contrastive loss terms affects this trade-off. A sensitivity study or learning dynamics analysis would strengthen the claims."}, "questions": {"value": "1) The paper mentions LSC could be used as a “live diagnostic” during model training. Could the authors outline how such an online LSC metric might be integrated into training pipelines (e.g., as a stopping criterion or auxiliary signal)?\n2) How sensitive are the results to the relative weights w_m and w_c in L_combined? Does a small contrastive component already improve alignment, or is a strong contrastive signal necessary?\n3) Have the authors evaluated whether the LSC framework predicts reasoning performance on tasks that are less abstract (e.g., visual question answering or commonsense reasoning)? How consistent is the alignment gap in these settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PG24h30xD3", "forum": "0etzWKrS4F", "replyto": "0etzWKrS4F", "signatures": ["ICLR.cc/2026/Conference/Submission2167/Reviewer_jDo8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2167/Reviewer_jDo8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761507488674, "cdate": 1761507488674, "tmdate": 1762916084368, "mdate": 1762916084368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates failures of VLMs on abstract visual reasoning tasks like Bongard problems, questioning whether the bottleneck lies in visual perception or higher-level reasoning. The authors introduce a diagnostic framework centered on the Linear Separability Ceiling (LSC): the maximum performance achievable by a linear classifier on the VLM's initial visual embeddings. Applying this framework, they discover an alignment gap: most state-of-the-art VLMs fail to generatively outperform their own LSC, suggesting their reasoning capabilities are poorly aligned with their visual representations. They propose a fine-tuning method using LoRA with a combined objective, adding a contrastive loss to the standard next-token prediction loss. This method improves the linear separability of final embeddings, successfully allowing models to systematically surpass the LSC and achieve higher performance on abstract reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The graphs and tables are clear and easy to understand.\n2. Experiments are thorough, covering multiple VLMs, datasets, PEFT methods, objectives, and generalization scenarios."}, "weaknesses": {"value": "1. While effective, the LSC relies solely on linear separability. It's possible that representations hold complex non-linear structures useful for reasoning that the LSC metric fails to capture.\n2. The core observation that VLM generative performance often fails to surpass a linear probe on its visual features is not entirely new. Similar gaps between representation quality and end-to-end performance have been previously studied, showing VLMs can underperform linear probes on classification or generally overlook information in their visual representations.\nSome related works like:\n[Why are Visually-Grounded Language Models Bad at Image Classification?](https://arxiv.org/pdf/2405.18415.pdf)\n[Hidden in plain sight: VLMs overlook their visual representations](https://arxiv.org/pdf/2506.08008.pdf)"}, "questions": {"value": "1. Can the fine-tuning approach be successfully applied to improve VLM performance on other challenging reasoning domains beyond Bongard problems, such as VQA or complex instruction following?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pE93cW1VoV", "forum": "0etzWKrS4F", "replyto": "0etzWKrS4F", "signatures": ["ICLR.cc/2026/Conference/Submission2167/Reviewer_3cSn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2167/Reviewer_3cSn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761534101384, "cdate": 1761534101384, "tmdate": 1762916083234, "mdate": 1762916083234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why Visual–Language Models (VLMs) often fail on abstract visual reasoning tasks and whether such failures arise from perception or reasoning deficits. To diagnose this, the authors propose the Linear Separability Ceiling (LSC) framework, which quantifies the performance achievable by a linear probe on a model’s visual embeddings. Analyses across state-of-the-art VLMs reveal a pervasive alignment gap, where models rarely exceed their own LSC. To address this, the study augments next-token prediction with a contrastive objective that enhances the linear structure of visual representations. Fine-tuned models consistently surpass the LSC, achieving human-level accuracy on Bongard-OpenWorld and significant gains on relational reasoning benchmarks, demonstrating that reasoning limitations stem from misalignment rather than intrinsic capacity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1)\tThe paper introduces the Linear Separability Ceiling framework to disentangle perception and reasoning in VLMs.\n2)\tThrough large-scale analysis, the authors reveal a pervasive alignment gap: most VLMs fail to outperform their own LSC, highlighting a fundamental but previously unmeasured bottleneck in vision–language reasoning.\n3)\tThe approach attains or surpasses human-level accuracy on OpenWorld and narrows the gap on HOI reasoning, demonstrating that the limitation in current VLMs stems from misalignment, not innate capacity."}, "weaknesses": {"value": "1)\tWhile the Linear Separability Ceiling is intuitively defined, the paper lacks a rigorous theoretical justification for why linear separability should represent the upper bound of perceptual quality. A more formal link between LSC and model capacity or information-theoretic limits is missing.\n2)\tThe claim that failures arise from “alignment gaps” rather than perception deficits is mostly correlational. The experiments show association but not causal evidence that reasoning misalignment causes underperformance.\n3)\tThe evaluation focuses mainly on Bongard-style reasoning and a single compositional benchmark. Broader validation on diverse abstract reasoning or real-world multimodal tasks would strengthen the generality of conclusions.\n4)\tThe “nonlinear decision logic” mechanism is described conceptually but not visualized or quantitatively analyzed. Without feature attribution or attention-map evidence, the interpretation remains speculative.\n5)\tThis paper is lengthy and conceptually dense. Core ideas like “alignment gap” and “surpassing the ceiling” could be more precisely illustrated. Figures (e.g., Fig. 2) lack clear axis descriptions, and some tables overflow with statistical detail without clear takeaway messages.\n6)\tThe paper’s structure is somewhat diffuse, with diagnostic and intervention sections interleaving and key concepts repeated across sections. This weakens the logical flow from problem to solution and makes the main argument harder to follow."}, "questions": {"value": "Please refer to the weak points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "phkeaOia3h", "forum": "0etzWKrS4F", "replyto": "0etzWKrS4F", "signatures": ["ICLR.cc/2026/Conference/Submission2167/Reviewer_XeiP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2167/Reviewer_XeiP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735368372, "cdate": 1761735368372, "tmdate": 1762916081154, "mdate": 1762916081154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the LSC to diagnose whether VLMs fail on abstract reasoning tasks due to perception or reasoning. The authors identify an alignment gap where reasoning fails to surpass the model’s own representational limit. They further proposed a contrastive fine-tuning objective that can close this gap on Bongard-style reasoning tasks.\n\nOverall, the paper is thoughtfully motivated but overinterprets linear-probe diagnostics as mechanistic evidence for reasoning non-linearity. The statistical evidence is somewhat weak, and the causal link between representational geometry and reasoning behavior remains speculative. I will consider moving up the score, depending on how the authors clarify statistical robustness and theoretical grounding during discussion."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well motivated and identifies an important problem: diagnosing perceptual–reasoning misalignment in VLMs.\n2. The proposed LSC is a clear, interpretable metric that operationalizes representational quality in a meaningful way.\n3. The paper is well written and organized, with 2 dataset on 8 models coupled with various promptings.\n4. The paper introduced a contrastive fine-tuning objective that simultaneously improves generative accuracy and final-layer separability."}, "weaknesses": {"value": "1. The paper defines “non-linearity” as “cases where linear probes fail.”  The non-linearity can be an artifact of your measurement of cosine similarity of euclidean averaged embeddings, not a measured representational property. The claim would be stronger with direct evidence of curvature or manifold structure.\n2. Important evaluation details are underspecified—for instance, how generative accuracy is computed relative to the probe-based classification accuracy. \n3. Some of the results (e.g., Section 7.2) appear cherry-picked without consistent statistical treatment. A group-level comparison across models or prompt conditions would increase confidence in the claims.\n4. The causal interpretation—that contrastive fine-tuning resolves misalignment—remains speculative. The improvements may simply reflect more linearly organized feature geometry rather than deeper mechanistic reasoning.\n5. The paper notes catastrophic forgetting as a limitation but does not fully analyze why L_{combined} causes this — an area that could benefit from ablation or regularization experiments."}, "questions": {"value": "1. How sensitive are your results to pooling strategy (mean pooling vs. attention pooling vs. CLS token)?\n2. How stable is the LSC metric across different random seeds or mini-batch samplings?\n3. How exactly is the statistical comparison between generative accuracy and LSC performed in Fig. 2—are these paired comparisons over test trials or aggregated accuracies? For models that pass LSC, the advantage of generative accuracy is low.\n4. Could this framework be applied to tasks requiring fine-grained perceptual reasoning (e.g., gaze direction or social interaction), rather than Bongard tasks that suit better for linear separation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VxwHd7RNuZ", "forum": "0etzWKrS4F", "replyto": "0etzWKrS4F", "signatures": ["ICLR.cc/2026/Conference/Submission2167/Reviewer_jWGQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2167/Reviewer_jWGQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931953837, "cdate": 1761931953837, "tmdate": 1762916080689, "mdate": 1762916080689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}