{"id": "et2JRQYkGA", "number": 5226, "cdate": 1757870043599, "mdate": 1763337675834, "content": {"title": "Beyond Redundancy: Diverse and Specialized Multi-Expert Sparse Autoencoder", "abstract": "Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting large language models (LLMs) by decomposing token activations into combinations of human-understandable features. While SAEs provide crucial insights into LLM explanations, their practical adoption faces a fundamental challenge: better interpretability demands that SAEs' hidden layers have high dimensionality to satisfy sparsity constraints, resulting in prohibitive training and inference costs. Recent Mixture of Experts (MoE) approaches attempt to address this by partitioning SAEs into narrower expert networks with gated activation, thereby reducing computation. In a well-designed MoE, each expert should focus on learning a distinct set of features. However, we identify a *critical limitation* in MoE-SAE: Experts often fail to specialize, which means they frequently learn overlapping or identical features. To deal with it, we propose two key innovations: (1) Multiple Expert Activation that simultaneously engages semantically weighted expert subsets to encourage specialization, and (2) Feature Scaling that enhances diversity through adaptive high-frequency scaling. Experiments demonstrate a 24\\% lower reconstruction error and a 99\\% reduction in feature redundancy compared to existing MoE-SAE methods. This work bridges the interpretability-efficiency gap in LLM analysis, allowing transparent model inspection without compromising computational feasibility. \nOur code is publicly available at https://anonymous.4open.science/r/scale_sae-C6D0/.", "tldr": "We resolve feature redundancy in the Mixture of Experts Sparse Autoencoder by co-activating multiple experts and scaling high-frequency features, which results in more diverse and specialized dictionaries.", "keywords": ["Sparse Autoencoder", "LLM Interpretability", "Mixture-of-Experts", "Representation Learning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e060bbf9c350e164b4be5719dd20c0969e556795.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces multi expert SAEs (as opposed to single expert SwitchSAEs). They further introduce feature scaling as a mean of encouraging specialisation and reducing feature redundancy. They run evaluations on gpt2 on fraction of loss recovered, mse, interpretability, and redundancy of features. They show improvements again some competing architectures of SAEs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The introduction of weighted MoE SAEs is novel, and feature scaling is effective as a way to enhance specialisation (which could be tied to interpretability, as an interesting future direction). \nThere are many evaluations criteria, and improvements are shown across them. \nThe problem of SAE feature interpretability, and the improvement of SAEs are well located problems within the literature."}, "weaknesses": {"value": "The main limitations are:\n- evaluations restricted to gpt2\n- the improvements on L0 against MSE/loss look incremental\n- only.3 architectures (topk/switch/gated) are tested against (more baselines could help, such as matryoshka or jumprelu saes)\n\nI would increase my score if evaluations were successfully (more than incremental improvements) run on more models/families.\n\nStyle:\n- figure 3 looks cluttered (make all your saes one color and all the baselines another)"}, "questions": {"value": "Have you tried expanding evaluations to other models or families (even small ones like pythia 70m)?\n\nIs fraction of loss recovered the LLM's loss or the SAE's EV?\n\nHave you tried experiments linking the specialisation of features to their interpretability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TXWolwWFYo", "forum": "et2JRQYkGA", "replyto": "et2JRQYkGA", "signatures": ["ICLR.cc/2026/Conference/Submission5226/Reviewer_S4XQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5226/Reviewer_S4XQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760568939605, "cdate": 1760568939605, "tmdate": 1762917959237, "mdate": 1762917959237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors suggest an improvements to SAEs by using MoEs extending Mudide et al 2025. They suggest that the original approach to MoE SAEs was flawed and had polysemanticity which undermines the interpretability that the approach was intended to provide. Instead of choosing a single expert, they choose multiple experts with their Multiple Expert Activation scheme. They then have a DSP-inspired technique Feature Scaling which amplifies the component of the expert encoder's weights which is far from the mean of the encoder vectors. They refer to this as \"amplifying the high-frequency component\". With these changes their SAEs outperform TopK SAEs and the Switch SAE. The primary obstacle to performance from previous work that they solve is the problem of feature redundancy. The authors perform ablations to confirm that both of these changes are important in training more performant SAEs.\n\nOverall this is a solid and well argued interpretability paper, though possibly incremental in terms of its impact to the interpretability (and broader ML) community."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear writing style\n- Generally readable figures\n- Clearly notes the limits of the Switch SAE and how they overcome these\n- Useful to see the performance on two different datasets across both the MSE and Loss Recovered metrics\n- Interesting exposition when detailing how the two architecture changes help the overall performance of the SAE\n- Valuable use of the signal processing literature which is a literature that is not always leveraged in interpretability research (and where interpretability research could likely learn a lot more from)"}, "weaknesses": {"value": "- For interpretability researchers reading the authors might want to be careful about using the term \"high frequency\" without explaining what this means. In this case it seems to be mostly an analogy to the signal processing literature but within the SAE literature a high frequency feature is typically a feature which activates very often which is a quite different concept. Clarifying this would be useful.\n  - It's also not totally clear why this analogy is a good analogy - exploring why \"high-frequency\" is the right term here (possibly with spectral plots or similar) could be valuable to help with readers' intuition. \n- Though the authors talk about computational efficiency being an advantage of their approach, they do not show any charts or tables tracking the computational efficiency of their approach relative to others. Seeing this would be useful to validate that claim. \n- The authors do not evaluate their results on downstream tasks. Doing so (possibly using SAE-Bench or a similar benchmark) would be an improvement to the work demonstrating the downstream usefulness across other metrics. \n- The work is somewhat incremental. Though I believe their results are an improvement over prior methods, it's not clear that the level of improvement that is presented will meaningfully impact the research community. This is not to take away from the interesting methodology, clear presentation and reasonable ablations. \n- All of the experiments are using GPT-2. Having at least one plot in the paper with results from a larger model would show that this method works well on larger models and that it scales well. in particular this is important because one of the main claims of the paper is about efficiency."}, "questions": {"value": "- What is happening in Figure 5a)? The title of the charts suggest an L0 of 2 and 32 yet the bar and line charts show different values of L0 on the x axis. Is the x axis actually varying the number of experts?\n- In 3.2.1, why would there be diminishing returns to having more than 2 experts active at once? Given that you're not actually using all of the features but are instead using another TopK filter afterwards (so the FLOPs are not increasing) it seems still to explain why 2 experts is markedly better than 1 but 4 isn't seemingly better than 2. \n    - This seems to be somewhat mitigated in 3.2.2 but I would like to know more of why this mitigation ought to work from the authors\n- Does Feature Scaling ever amplify noise or cause training instability?\n- What are the training and inference time memory implications for having the MoE layer? Is there a way to reduce the footprint of this compared to naive methods?\n- Is the idea that the low frequency features are more coarse-grained and high-frequency ones flesh out more complex details? If so is there any evidence for this?\n- The abstract states that the approach gives 99% less feature redundancy - where in the paper is this claim justified?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vvq9lEU1GY", "forum": "et2JRQYkGA", "replyto": "et2JRQYkGA", "signatures": ["ICLR.cc/2026/Conference/Submission5226/Reviewer_t2ME"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5226/Reviewer_t2ME"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761525872157, "cdate": 1761525872157, "tmdate": 1762917958986, "mdate": 1762917958986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a critical limitation in Mixture-of-Experts Sparse Autoencoders: the failure of experts to specialize, leading to high feature redundancy that undermines both interpretability and performance. The authors propose Scale SAE, a novel framework with two core innovations:\n- Instead of routing an input to a single expert, a subset of experts is activated. \n- Encoder weights are decomposed into low-frequency and high-frequency components, and the high-frequency parts are adaptively amplified.\n\nThrough extensive experiments on GPT-2, the paper demonstrates that Scale SAE significantly outperforms strong baselines (TopK SAE, Gated SAE, Switch SAE) under a FLOPS-matched paradigm."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The methodology is explained with precise mathematical notation, and the results are presented with effective visualizations.\n\nThe experimental evaluation is thorough and convincing. The use of FLOPS-matched comparisons, multiple datasets (in-domain and cross-domain), and a suite of complementary metrics leaves little doubt about the superiority of the proposed method. The ablation studies and mechanistic analysis are executed to a high standard."}, "weaknesses": {"value": "> W1. The Mechanistic Rationale for Multiple Expert Activation Requires Deeper Justification. \n\nTo be very honest, activating more than one expert in MoE is standard practice. The difference here is that we select the Top-K experts across all experts.\n\nThe paper shows that activating multiple smaller experts outperforms a single larger expert under a FLOPS-matched budget (e.g., 2 experts of size 128 vs. 1 expert of size 256). However, the fundamental reason for this performance boost is not sufficiently explained. A key question remains: is the benefit primarily due to the modularity and finer granularity of the experts, or is it the interaction and joint sparsification across experts that is crucial? For example, consider a 4-expert SAE (activating 2 at a time, each with 128 hidden units) versus a 2-expert SAE (activating 1 at a time, each with 256 hidden units). The computation is identical, so why should reconstruction quality, accuracy, and stability improve? The paper shows results in Figures 3 and 4, but does not clearly explain the underlying logic.\n\n> W2. Insufficient Discussion and Comparison for Feature Scaling. \n\nFeature Scaling appears primarily as a load-balancing technique, both at the expert and neuron levels. Adding high-frequency components increases directional diversity, but alternative methods exist. \n\nAt the expert level, how does the implicit balancing effect of Feature Scaling compare to more explicit load-balancing losses used in MoE literature?\n\nAt the neuron/feature level, orthogonal initialization or updates could achieve similar effects.\n\nThese alternatives are not discussed or compared. While the quantitative results in Section 3.3 make sense, the paper lacks a deeper analysis of why this approach is preferable."}, "questions": {"value": "From Figure 6(a), adding more than two experts does not further improve specialization—in fact, performance worsens. This seems inconsistent with the stated property. Why is that?\n\n\nI wonder if adding high-frequency components requires longer training to converge, since it perturbs the optimization direction. Is that correct? Could you also compare training iterations and time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s0koHmQc4m", "forum": "et2JRQYkGA", "replyto": "et2JRQYkGA", "signatures": ["ICLR.cc/2026/Conference/Submission5226/Reviewer_vCWP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5226/Reviewer_vCWP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717047252, "cdate": 1761717047252, "tmdate": 1762917958799, "mdate": 1762917958799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of feature redundancy and poor expert specialization in Mixture-of-Experts Sparse Autoencoders (MoE-SAEs), which are used to make large language models more interpretable while reducing computational cost. Prior work such as Switch SAEs often suffers from redundant experts learning overlapping features, limiting interpretability and efficiency. The authors propose two key innovations: (1) Multiple Expert Activation, which activates several experts per input and applies a global Top-K sparsity constraint to encourage expert specialization, and (2) Feature Scaling, a learnable high-frequency amplification mechanism that promotes feature diversity and stabilizes training. Experiments on GPT-2 activations show that these methods improve reconstruction error, feature diversity, and automated interpretability scores relative to baseline SAEs. Ablation studies attribute these gains to the proposed mechanisms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well motivated, clearly written, and deeply engages with prior work. The authors correctly identify a central limitation of existing MoE-SAEs and propose two simple, conceptually coherent mechanisms to address it. Both techniques are well defined and integrated cleanly into the SAE framework. The experimental evaluation is thorough, including ablation studies that isolate the contribution of each innovation. The results convincingly demonstrate reductions in feature redundancy and improved interpretability metrics. Overall, the work provides a solid step toward making large-scale sparse autoencoders more computationally feasible for interpretability research."}, "weaknesses": {"value": "The experiments are limited to GPT-2, which is now a dated architecture. Including results on more recent models such as Gemma or LLaMA would strengthen the empirical claims and test generality.\nThe FLOPs-matching procedure is not clearly justified. The authors write that “to match the computational load of activating a fixed number of experts, the hidden dimension is set to 768” for dense SAEs, while Scale SAEs use a total hidden dimension of 24,576. It is unclear how this setup maintains computational parity, and a more detailed explanation of this comparison is needed.\nFinally, the discussion of automated interpretability results is too brief. The paper would benefit from a few qualitative examples of discovered features or a quantitative measure of feature diversity across experts, to demonstrate that higher automated interpretability scores correspond to genuinely more distinct, human-understandable features."}, "questions": {"value": "1. Could the authors clarify how the FLOPs-matching setup ensures a fair comparison between dense SAEs with 768 hidden units and Scale SAEs with a total of 24,576 dimensions? Is the compute matched per forward pass, per batch, or by total parameter count?\n\n2. Did you observe any side effects of the Feature Scaling mechanism, such as instability, changes in sparsity dynamics, or degraded interpretability for certain expert configurations? Since this mechanism directly modifies encoder weights, a brief discussion of possible unintended effects would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "urZH9rL5wM", "forum": "et2JRQYkGA", "replyto": "et2JRQYkGA", "signatures": ["ICLR.cc/2026/Conference/Submission5226/Reviewer_vN4Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5226/Reviewer_vN4Q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820646393, "cdate": 1761820646393, "tmdate": 1762917958514, "mdate": 1762917958514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}