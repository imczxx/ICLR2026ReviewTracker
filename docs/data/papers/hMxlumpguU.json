{"id": "hMxlumpguU", "number": 17647, "cdate": 1758278742984, "mdate": 1759897162765, "content": {"title": "Accelerating first-order methods for nonconvex-strongly-convex bilevel optimization under general smoothness", "abstract": "Bilevel optimization is pivotal in machine learning applications such as hyperparameter tuning and adversarial training. While existing methods for nonconvex-strongly-convex bilevel optimization can find an $\\epsilon$-stationary point under Lipschitz continuity assumptions, two critical gaps persist: improving algorithmic complexity and generalizing smoothness conditions. This paper addresses these challenges by introducing an accelerated framework under Hölder continuity—a broader class of smoothness that subsumes Lipschitz continuity. \nWe propose a restarted accelerated gradient method that leverages inexact hypergradient estimators and establishes theoretical oracle complexity for finding $\\epsilon$-stationary points. Empirically, experiments on data hypercleaning and hyperparameter optimization demonstrate superior convergence rates compared to state-of-the-art baselines.", "tldr": "", "keywords": ["Bilevel Optimization", "Hölder Continuity", "Accelerated Gradient Method"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fb9007ba89216dca2e88fc50e4e243be547295c2.pdf", "supplementary_material": "/attachment/75b632edbb67e335b945904f36f65a4bda3db323.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes RAGD-GS, a restarted accelerated first-order framework for nonconvex–strongly-convex bilevel optimization that works through a penalized surrogate $L_{\\lambda}^{\\*}(x)$ and an inexact hyper-gradient estimator computed by two inner AGD solvers (for $y^\\*(x)$ and $y_{\\lambda}^{*}(x)$). It claims an accelerated complexity of $\\tilde{O}(\\epsilon^{-7/4})$ in the Lipschitz case while extending to Hölder-continuous higher-order smoothness, and reports small-scale experiments on data hyper-cleaning and hyperparameter optimization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The RAGD outer loop plus two inner AGDs is easy to implement, and the estimator of $\\nabla L_{\\lambda}^{*}$ is explicitly stated and purely first-order.\n\n- Restart condition, potential-function descent, and complexity theorems are self-contained and align with recent nonconvex AGD analyses.\n\n- Experiments compare against AID/ITD/BA/RAHGD/F2BA/AccF2BA and support the theoretical convergence guarantees."}, "weaknesses": {"value": "- The paper claims to improve the first-order oracle complexity to $\\tilde{O}(\\ell^{3/4}\\kappa^{13/4}\\epsilon^{-7/4})$ without requiring second-order oracles. However, this result has already been achieved by the AccF$\\^2$BA method in prior work [1] using first-order oracles only.\n\n- While this work considers the general Hölder continuity setting, it does not empirically explore or validate the Hölder case in Section 5.\n\n- Condition 1 and Theorem 2 require inner solves tight enough that costs are hidden in $\\tilde{O}$, but the experiments do not report the actual inner iteration counts that satisfy (8).\n\n\n\n[1] Chen, L., Ma, Y., & Zhang, J. (2025). Near-optimal nonconvex-strongly-convex bilevel optimization with fully first-order oracles. Journal of Machine Learning Research, 26(109), 1-56."}, "questions": {"value": "- How do you estimate $H_{\\nu}$ and $D$ to run the restart test in equation (5) without oracle knowledge in experiments?\n\n- Regarding the Hölder regime, could you include synthetic experiments where $\\nu_f, \\nu_g < 1$ (controlled via constructed objectives) to empirically validate the exponents predicted in Theorem 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sSzILusbjS", "forum": "hMxlumpguU", "replyto": "hMxlumpguU", "signatures": ["ICLR.cc/2026/Conference/Submission17647/Reviewer_8Fo8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17647/Reviewer_8Fo8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969462382, "cdate": 1761969462382, "tmdate": 1762927503684, "mdate": 1762927503684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an \"accelerated first-order\" framework for nonconvex–strongly-convex bilevel optimization under generalized Holder continuity assumptions on higher-order derivatives. The method combines the penalty-based bilevel method with a restarted accelerated gradient descent (RAGD-GS). The proposed algorithm achieves an $\\tilde{O}(\\ell^{3/4}\\kappa^{13/4}\\epsilon^{-7/4})$ first-order oracle complexity, matching the state-of-the-art accelerated results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a technically correct extension of prior “fully first-order” acceleration frameworks to a slightly more general smoothness class (Hölder instead of Lipschitz).\n- The convergence rate can cover match the state-of-the-art accelerated results under the standard Lipschitz continuity setting."}, "weaknesses": {"value": "- This paper lacks technique novelty for bilevel optimization problem, and the entire framework is a combination of existing papers:\n    - The penalty-surrogate method and its gradient construction are directly inherited from [F$^2$SA](https://arxiv.org/pdf/2301.10945) and [F$^2$BA](https://www.jmlr.org/papers/volume26/23-1104/23-1104.pdf).\n    - The restart mechanism is essentially a parameterized variant of the [RAHGD](https://arxiv.org/pdf/2307.00126) restart rule, rewritten to include a Hölder-dependent constant instead of a fixed Lipschitz threshold.\n    - The complexity result reproduces $\\tilde{O}(\\epsilon^{-7/4})$ rate already established by F$^2$BA/AccF$^2$BA.\n\n- The new assumption \"Holder continuous in higher-order derivatives\" is not a relaxation for the prior assumption in F$^2$SA. In my mind, the new assumptions are different, not weaker. They replace the standard third-order derivative Lipschitz conditions with Holder-continuous condition but still require bounded constants that are functionally equivalent for practical analysis. The proof is almost the same when handling the higher-oder smoothness. They don't provide theoretical or empirical evidence that the Holder setting broadens the applicability to any class of bilevel problems previously excluded. \n\n- The experimental section provides no empirical justification for the new assumptions. The proposed algorithm achieves nearly identical performance to AccF$^2$BA in Figure 3.\n\n- There is no stochastic analysis. In real bilevel problems—hyperparameter tuning, meta-learning, or data hypercleaning, gradients are noisy or sampled.\n\nAbove all, the paper does not provide substantive theoretical or empirical advancement over existing first-order bilevel acceleration methods."}, "questions": {"value": "Please check Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "meIx1uHZgw", "forum": "hMxlumpguU", "replyto": "hMxlumpguU", "signatures": ["ICLR.cc/2026/Conference/Submission17647/Reviewer_UCqE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17647/Reviewer_UCqE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973719783, "cdate": 1761973719783, "tmdate": 1762927502898, "mdate": 1762927502898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a restarted accelerated gradient method for nonconvex-strongly-convex bilevel optimization problems under generalized Hölder continuity assumptions. The method builds upon a penalty-based reformulation to create a single-loop algorithm that uses only first-order oracles. The authors provide a theoretical analysis showing an improved oracle complexity of $\\tilde{\\mathcal{O}}(\\epsilon^{-7/4})$ under Lipschitz continuity, matching concurrent work, and more general rates under Hölder continuity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper extends the analysis of accelerated bilevel methods from the standard Lipschitz continuity setting to the more general Hölder continuity framework.\n\n2. The proposed RAGD-GS is a fully first-order method, avoiding the need for computationally expensive Hessian-vector or Jacobian-vector product oracles."}, "weaknesses": {"value": "1. The primary claimed contribution of the paper is weakening the smoothness requirements for bilevel optimization from Lipschitz continuity of second- and third-order derivatives to Hölder continuity. However:\n\nThere already exist bilevel optimization methods that do not assume Lipschitz continuity of \n$\\nabla^2 f$ or $\\nabla^3 g$, such as [14] and the following relevant reference omitted by the authors:\nChen, Xuxing, Tesi Xiao, and Krishnakumar Balasubramanian. \"Optimal algorithms for stochastic bilevel optimization under relaxed smoothness conditions.\" JMLR 25.151 (2024): 1–51.\n\nThe paper does not sufficiently motivate why Hölder continuity is important in machine learning contexts. Are there common, practical ML applications where Hölder smoothness holds but Lipschitz smoothness does not?\n\nThere is no empirical evaluation demonstrating value in the Hölder setting. As a result, the contribution risks appearing as a purely theoretical generalization without practical relevance.\n\n\n2. The $\\tilde{O}(\\epsilon^{-7/4})$ rate (Theorem 2, for $\\nu_f=\\nu_g=1$) matches the rate of AccF2BA [13] exactly, as shown in Table 1. The authors acknowledge this, stating their result is \"consistent with the concurrent findings of [13]\". This means the paper essentially proposes a different algorithm (using a different restart scheme) to achieve the same result that is already present in the literature. The contribution is therefore methodological (a different path) rather than a new state-of-the-art complexity, which significantly lessens its impact.\n\n\n3. The algorithmic components (penalty formulation, AGD subroutines, restart condition) are not novel. The primary novelty lies in the theoretical analysis under a different set of assumptions. The restart condition, while different from prior work, is not motivated by a clear, fundamental insight that leads to a qualitative improvement.\n\n\n4. The experiments in Figures 1, 2, and 3 are not compelling. They show that the proposed RAGD-GS performs very similarly to AccF2BA [13] and RAHGD [15]. This is not surprising, as it shares the same theoretical rate as AccF2BA. The experiments merely confirm that the algorithm works as expected, but they do not demonstrate any practical advantage over the existing state-of-the-art. Given that the main result is not novel, stronger empirical performance (e.g., significantly better constants, more stability) would be needed to justify the contribution, but this is not shown."}, "questions": {"value": "1. Can you provide a clear example of a practical bilevel optimization problem in machine learning that fails the Lipschitz smoothness assumption but satisfies the Hölder one, where your generalized theory would be necessary?\n\n2. Since your complexity under Lipschitz assumptions is identical to [13], what is the practical, empirical, or theoretical advantage of your proposed restart scheme (Eq. 5) over the one used in AccF2BA [13]? Your experiments do not seem to show a clear difference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BMjsKNoy8I", "forum": "hMxlumpguU", "replyto": "hMxlumpguU", "signatures": ["ICLR.cc/2026/Conference/Submission17647/Reviewer_U2GS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17647/Reviewer_U2GS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981004287, "cdate": 1761981004287, "tmdate": 1762927502417, "mdate": 1762927502417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies nonconvex–strongly-convex bilevel optimization problems and proposes an accelerated first-order framework under general (Hölder-continuous) smoothness assumptions. The authors introduce a penalty reformulation that removes the need for exact implicit differentiation and combine it with a restarted accelerated gradient descent scheme (RAGD-GS). The analysis establishes an improved first-order oracle complexity of $\\tilde O(\\varepsilon^{-7/4})$ in the Lipschitz-smooth case, extending acceleration theory to bilevel settings. Empirical validation on the MNIST hypercleaning task demonstrates faster convergence than existing bilevel algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The theoretical development is rigorous and technically strong. The paper unifies several existing bilevel frameworks under a single smoothness-generalized setting and achieves provable acceleration. The presentation of the penalized objective and the restart mechanism is clear and mathematically elegant. The convergence proofs are complete and well organized."}, "weaknesses": {"value": "The analysis assumes deterministic nonconvex–strongly-convex regime and exact gradients in both levels. \n\nThe paper does not extend to stochastic or weakly convex inner problems, which limits its relevance for large-scale or deep learning applications. \n\nThe complexity bound hides heavy dependence on the condition number $\\kappa$ and smoothness constants, which may offset the practical benefit of the improved $\\varepsilon$-rate. \n\nEmpirically, only the data-hypercleaning benchmark on MNIST is presented, so the experimental evidence remains narrow."}, "questions": {"value": "1. The analysis assumes strong convexity of the lower-level objective $g(x, y)$. Could the authors clarify whether the acceleration guarantees extend to weakly convex or nonconvex inner problems?\n2. The penalty parameter $\\lambda$ plays a central role in the convergence proof. How should it be selected in practice, and is there any adaptive rule ensuring stability without prior knowledge of problem constants?\n3. The improved complexity $\\tilde{O}(\\varepsilon^{-7/4})$ depends on Hölder smoothness. Could the authors specify the exact constants hidden in the $\\tilde{O}$ notation and discuss their dependence on the condition number $\\kappa$?\n4. The restart condition for the RAGD-GS algorithm is theoretically motivated. How sensitive is the method to the choice of restart interval or stopping criterion in empirical implementations?\n5. The analysis is deterministic. Can the authors comment on whether similar acceleration rates could be achieved under stochastic gradients or sampling noise?\n6. How does the proposed penalty formulation compare theoretically to implicit differentiation or Moreau-envelope-based bilevel methods in terms of approximation bias?\n7. Are there lower bounds suggesting that $\\tilde{O}(\\varepsilon^{-7/4})$ is optimal for nonconvex–strongly-convex bilevel problems under the assumed smoothness conditions?\n8. The empirical evaluation is limited to the MNIST hypercleaning task. Could the authors provide evidence that the acceleration behavior persists in higher-dimensional or large-scale settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AYdwCa1phq", "forum": "hMxlumpguU", "replyto": "hMxlumpguU", "signatures": ["ICLR.cc/2026/Conference/Submission17647/Reviewer_KzPz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17647/Reviewer_KzPz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762133031258, "cdate": 1762133031258, "tmdate": 1762927501941, "mdate": 1762927501941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}