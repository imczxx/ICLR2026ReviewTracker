{"id": "kdeiRledV6", "number": 3706, "cdate": 1757500737307, "mdate": 1763360470116, "content": {"title": "Overthinking Reduction with Decoupled Rewards and Curriculum Data Scheduling", "abstract": "While large reasoning models trained with critic-free reinforcement learning and verifiable rewards (RLVR) represent the state-of-the-art, their practical utility is hampered by ``overthinking'', a critical issue where models generate excessively long reasoning paths without any performance benefit. Existing solutions that penalize length often fail, inducing performance degradation due to a fundamental misalignment between trajectory-level rewards and token-level optimization. In this work, we introduce a novel framework, DECS, built on our theoretical discovery of two previously unaddressed flaws in current length rewards: (1) the erroneous penalization of essential exploratory tokens and (2) the inadvertent rewarding of partial redundancy. Our framework's innovations include (i) a first-of-its-kind decoupled token-level reward mechanism that surgically distinguishes and penalizes redundant tokens, and (ii) a novel curriculum batch scheduling strategy to master the efficiency-efficacy equilibrium. Experimental results show DECS can achieve a dramatic reduction in reasoning tokens by over 50\\% across seven benchmarks while simultaneously maintaining or even improving performance. It demonstrates conclusively that substantial gains in reasoning efficiency can be achieved without compromising a model's underlying reasoning power.", "tldr": "theoretical unveil the underlying limitations of length reward and propose D$^2$yOR to achieve supreme efficiency without performance degradation", "keywords": ["efficient reasoning; curriculum sampling with decoupled reward"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fe8f39957304a65f1c5ddd381a13bc3a31c72479.pdf", "supplementary_material": "/attachment/c202828c9679c05b4a745102b1c7ddce8b4da755.zip"}, "replies": [{"content": {"summary": {"value": "Handling RLVR model’s overthinking behavior by separating tokens according to their role and penalizing redundant tokens, and scheduling batch with curriculum.  \n–\nPrior approaches incorporating length penalties (e.g., Eq. 4) broadcast trajectory-level rewards to all tokens via GRPO's advantage estimation (Eq. 2). When all responses in a group are correct but vary in length, shorter sequences receive positive advantages while longer ones receive negative advantages uniformly across every token. Consequently, high-entropy tokens like \"Wait\" or \"However\"—which initiate productive exploration—are suppressed simply because they appear in longer trajectories, even when they are part of the necessary reasoning process (Lemma 2, Theorem 1). DECS addresses this by decoupling rewards at the token level: it assigns maximum reward (r₊) to all NRP tokens including high-entropy exploratory tokens, while inversely penalizing only the redundant tokens generated after the NRP (Eq. 9). This surgical approach maintains exploratory capacity (preserving Pass@K performance, Fig. 3c) while achieving superior compression (>50% token reduction, Table 1)."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The idea is backed by theoretical claims, and they are logically correct. The two flaws revealed by Lemma 2 and Theorem 2 (suppression of high-entropy tokens + failure to penalize redundant tokens) are addressed by DECS through curriculum scheduling (Theorem 1) and decoupled rewards.\n- In addition, the paper make each claim irrefutable through quantitative verification at every level - from theory to core analysis (Fig 1, 3), then validate broadly with 7 benchmarks x 2 models x 5+ baselines, with robustness checks (Tables 5-7)\n- The paper organized their contents very nicely, well distributed content between main text and appendix. \n- Good generalization to out-of-domain tasks (LiveCodeBench, GPQA-D)"}, "weaknesses": {"value": "Limited accessibility for non-expert readers: While the paper presents rigorous theoretical analysis and strong empirical results, the presentation could be more accessible. Figures 1-2, which are central to understanding the core contributions, are dense with notation that is not immediately intuitive without careful reading of Section 3. For a paper introducing novel concepts like \"Necessary Reasoning Prefix\" and \"decoupled token-level rewards,\" more pedagogical support would strengthen impact.\n\nSuggestions for improvement:\n- Add a concrete, annotated example in Appendix showing token-by-token reward assignment for a simple problem (e.g., \"What is 2+3?\"). Compare how (a) base GRPO, (b) length penalty, and (c) DECS assign advantages to specific tokens including high-entropy ones.\n- Figure 2, while comprehensive, relies heavily on abstract labels (\"NRP\", \"Redundancy\", \"Correct Thinking Process\") that may not be \nimmediately intuitive to readers unfamiliar with the concepts. Consider replacing these placeholders with concrete text snippets from an actual model response (e.g., showing \"Wait, let me verify... 2+3=5\" as NRP vs \"Let me triple-check... yes, 5\" as redundancy) within the same figure space. This would make the core contribution—that DECS protects exploratory tokens while pruning redundancy—immediately graspable without requiring back-reference to Section 3.\n- Include an intuitive explanation of why high-entropy tokens matter, possibly with a probability distribution visualization showing how entropy differs between \"5\" (low H) vs \"Wait\" (high H)."}, "questions": {"value": "* Q1: The paper presents strong technical contributions, but accessibility could be improved with concrete worked examples (e.g., token-by-token walkthrough of the \"2+3\" problem showing advantages under baseline vs DECS). Given the paper is already dense, where do you envision such pedagogical content fitting best?\n* Q2: Table 1 shows 7B has less overthinking than 1.5B initially. Does this trend continue? Would larger models need DECS?  (Scalability to 30B+ models unverified)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sV4IhrkWTG", "forum": "kdeiRledV6", "replyto": "kdeiRledV6", "signatures": ["ICLR.cc/2026/Conference/Submission3706/Reviewer_tgZx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3706/Reviewer_tgZx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3706/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761229799904, "cdate": 1761229799904, "tmdate": 1762916936204, "mdate": 1762916936204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the \"overthinking\" problem in large reasoning models trained with reinforcement learning, where models generate excessively long reasoning paths without performance benefits. The authors propose DECS (Decoupled rewards and Curriculum data Scheduling) to reduce the reasoning length while not harming the performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The studied problem is important;\n- The writing is easy to follow;"}, "weaknesses": {"value": "- The proposed reward and curriculum designs are quite artificial. In particular, the decoupled reward depends on pre-training an effective NRP detection model, which introduces practical challenges given that the definition of NRP is ambiguous, let alone the difficulty of reliably detecting it with another model.\n- The method is not straightforward to implement, as it requires pre-training a separate detection model whose accuracy cannot be guaranteed or easily verified.\n- The curriculum design component lacks novelty and, as such, does not constitute a substantial contribution to the paper."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Nr4M4vlgK8", "forum": "kdeiRledV6", "replyto": "kdeiRledV6", "signatures": ["ICLR.cc/2026/Conference/Submission3706/Reviewer_YPs8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3706/Reviewer_YPs8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3706/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761672875650, "cdate": 1761672875650, "tmdate": 1762916936022, "mdate": 1762916936022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DECS, a novel framework designed to mitigate \"overthinking\", the generation of excessively long reasoning paths, in large reasoning models (LRMs) trained with reinforcement learning. The authors first provide a theoretical analysis identifying two key flaws in current length-penalty methods: the erroneous penalization of essential exploratory tokens and the inadvertent rewarding of redundant tokens. To address these, DECS incorporates three main components: (1) a lightweight judge model to detect the Necessary Reasoning Prefix (NRP), (2) a decoupled token-level reward that specifically penalizes tokens generated after the NRP, and (3) a scheduling strategy to balance efficiency and performance during training. The authors conduct extensive experiments, demonstrating that DECS can reduce the number of reasoning tokens while maintaining or even improving performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper provides a clear theoretical analysis of why existing length-reward mechanisms are suboptimal.\n2. The DECS offers a carefully designed approach grounded in theoretical reasoning.\n3. The authors evaluate their method across diverse benchmarks, use two different model scales, and compare against multiple relevant and strong baselines.\n4. DECS achieves a reduction in token count while simultaneously improving performance."}, "weaknesses": {"value": "1. The framework's reliance on a separate, fine-tuned NRP detector adds complexity to the training pipeline. A deeper analysis of its robustness would strengthen the paper.\n2. The decoupled reward is used to differentiate between \"Leading Redundant tokens\" and other redundant tokens, but this does not appear to be reflected in Eq.9."}, "questions": {"value": "1. I am intrigued by the parallels between the \"erroneous penalization\" issue and concepts like relative overgeneralization in MARL; a brief comment on this could enrich the paper's context within the broader RL literature.\n2. Could the authors elaborate on the reward design shown in Fig.2 and Eq.9?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "badFFPtmxH", "forum": "kdeiRledV6", "replyto": "kdeiRledV6", "signatures": ["ICLR.cc/2026/Conference/Submission3706/Reviewer_fsTY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3706/Reviewer_fsTY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3706/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735096580, "cdate": 1761735096580, "tmdate": 1762916935850, "mdate": 1762916935850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets the “overthinking” phenomenon on LRMs. Based on the fundamental misalignment between trajectory-level rewards and token-level optimization. The authors propose a DeCS framework based on theoretical discoveries that are erroneous penalization of essential exploratory tokens and  inadvertently rewarding partial redundancy. The DeCS framework includes (1) decoupled token-level reward mechanism and (2) curriculum batch scheduling strategy. Through comprehensive experiments the authors demonstrate effectiveness on their proposed framework"}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "**\\[S1\\] Solid Theoretical Foundation**  \nThe proposed framework is developed through strong theoretical foundation that enhance its novelty and provide convincing justification for its effectiveness. Also the paper's clear definition of concepts like NRP will likely provide significant value to the field by establishing precise terminology\n\n**\\[S2\\] Comprehensive Experimentation and Analysis**  \nThe paper presents a satisfying experimental structure organized around research questions, including thorough comparisons with baseline methods and detailed ablation studies. This approach effectively addresses potential questions about the proposed framework while convincingly demonstrating its effectiveness."}, "weaknesses": {"value": "**\\[W1\\] Readability Issues**  \nWhile the logical flow of the paper is solid, it was difficult to understand due to the extensive use of abbreviations and mathematical notations. Additionally, the token length comparison in Table 1 would have been more effectively conveyed through visualization. \n\n**\\[W2\\] Limited Model Generalization**  \nThe experiments are confined exclusively to the DeepSeek model family. The paper would be strengthened by extending experimentation to model families with different exploration capabilities to demonstrate broader generalization of the proposed framework.\n\n**\\[W3\\] Domain Dependency of the NRP Classification Model.**   \nWhile the NRP classifier trained on OpenR1-Math-220K demonstrates strong reliability through human validation within the mathematical domain, the paper lacks analysis of its performance in non-mathematical domains (e.g., coding, science, etc.). It will be great for the authors to additionally verify whether the NRP classification remains robust and consistent across these domains."}, "questions": {"value": "**\\[Q1\\]** In the RQ2 experiment, I understand that the exploration potential maintains similar with the base model. I'm curious about how exploration compares between DeCS  and GRPO-trained models. What differences or similarities exist in the exploration behavior between DeCS and models trained with GRPO?\n\n**\\[Q2\\]** Table 1 indicates that the proposed DeCS framework demonstrates lower average performance metrics compared to GRPO-trained models. Furthermore, the manuscript predominantly focuses on comparisons with baseline methods and analyses of individual DeCS components, while direct comparative analysis with GRPO appears somewhat limited. Given these observations, are there any theoretical reasons why DeCS shows a lower performance compared to GRPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aKHyrnqHgI", "forum": "kdeiRledV6", "replyto": "kdeiRledV6", "signatures": ["ICLR.cc/2026/Conference/Submission3706/Reviewer_XB3N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3706/Reviewer_XB3N"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3706/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981914810, "cdate": 1761981914810, "tmdate": 1762916935511, "mdate": 1762916935511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets the “overthinking” phenomenon on LRMs. Based on the fundamental misalignment between trajectory-level rewards and token-level optimization. The authors propose a DeCS framework based on theoretical discoveries that are erroneous penalization of essential exploratory tokens and  inadvertently rewarding partial redundancy. The DeCS framework includes (1) decoupled token-level reward mechanism and (2) curriculum batch scheduling strategy. Through comprehensive experiments the authors demonstrate effectiveness on their proposed framework"}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "**\\[S1\\] Solid Theoretical Foundation**  \nThe proposed framework is developed through strong theoretical foundation that enhance its novelty and provide convincing justification for its effectiveness. Also the paper's clear definition of concepts like NRP will likely provide significant value to the field by establishing precise terminology\n\n**\\[S2\\] Comprehensive Experimentation and Analysis**  \nThe paper presents a satisfying experimental structure organized around research questions, including thorough comparisons with baseline methods and detailed ablation studies. This approach effectively addresses potential questions about the proposed framework while convincingly demonstrating its effectiveness."}, "weaknesses": {"value": "**\\[W1\\] Readability Issues**  \nWhile the logical flow of the paper is solid, it was difficult to understand due to the extensive use of abbreviations and mathematical notations. Additionally, the token length comparison in Table 1 would have been more effectively conveyed through visualization. \n\n**\\[W2\\] Limited Model Generalization**  \nThe experiments are confined exclusively to the DeepSeek model family. The paper would be strengthened by extending experimentation to model families with different exploration capabilities to demonstrate broader generalization of the proposed framework.\n\n**\\[W3\\] Domain Dependency of the NRP Classification Model.**   \nWhile the NRP classifier trained on OpenR1-Math-220K demonstrates strong reliability through human validation within the mathematical domain, the paper lacks analysis of its performance in non-mathematical domains (e.g., coding, science, etc.). It will be great for the authors to additionally verify whether the NRP classification remains robust and consistent across these domains."}, "questions": {"value": "**\\[Q1\\]** In the RQ2 experiment, I understand that the exploration potential maintains similar with the base model. I'm curious about how exploration compares between DeCS  and GRPO-trained models. What differences or similarities exist in the exploration behavior between DeCS and models trained with GRPO?\n\n**\\[Q2\\]** Table 1 indicates that the proposed DeCS framework demonstrates lower average performance metrics compared to GRPO-trained models. Furthermore, the manuscript predominantly focuses on comparisons with baseline methods and analyses of individual DeCS components, while direct comparative analysis with GRPO appears somewhat limited. Given these observations, are there any theoretical reasons why DeCS shows a lower performance compared to GRPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aKHyrnqHgI", "forum": "kdeiRledV6", "replyto": "kdeiRledV6", "signatures": ["ICLR.cc/2026/Conference/Submission3706/Reviewer_XB3N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3706/Reviewer_XB3N"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3706/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981914810, "cdate": 1761981914810, "tmdate": 1763536977478, "mdate": 1763536977478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}