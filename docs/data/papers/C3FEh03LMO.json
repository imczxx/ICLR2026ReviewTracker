{"id": "C3FEh03LMO", "number": 23615, "cdate": 1758346373771, "mdate": 1759896804550, "content": {"title": "Opal: An Operator-Algebra View of RLHF Objectives", "abstract": "We present Opal, an operator-algebra view of RLHF objectives as ladders acting on pairwise margins. For a broad reducible subclass, we prove a terminating and confluent rewrite system with a unique normal form and an $O(m)$ canonicalization algorithm. On the learning side, we establish calibration and regret transfer, and give an oracle reduction that collapses all reducible ladders to a single canonical learner. We also show gap-preserving separations for violations (score-dependent weights, gating, pair-dependent references) with an $\\Omega(1/\\gamma^{2})$ testing lower bound. Finally, we provide a one-pass tester that outputs either a canonical hash and certificate or a finite witness, yielding a minimal GKPO semantics for decidable equivalence and proof-carrying objectives.", "tldr": "Opal provides decidable equivalence for RLHF objectives via canonicalization, certificates, and finite witnesses.", "keywords": ["RLHF", "canonicalization", "objective equivalence", "property testing", "reproducibility"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/db11948bf876cc767864862d0978d4634b03b8c0.pdf", "supplementary_material": "/attachment/a48d4b71bac062a9f2455554f056c65aa32af113.zip"}, "replies": [{"content": {"summary": {"value": "The submission provides a new view of Reinforcement Learning from Human Feedback (RLHF) objectives. This contribution is supported by theoretically establishing a unique normal form, learning guarantees and testing lower bounds, among others."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The submission provides a broad range of contributions; these are primarily foundational, but are also supported by a light-weight demo implementation."}, "weaknesses": {"value": "The submission is not at all suited to the general ICLR community, as it lacks any sort of introduction to the studied area. Naturally, not every ICLR paper needs to provide contributions that are central to everyone in the conference's community, but as a general rule I would expect that at least the central concepts and contributions are explained on a level that would be understandable to at least a reasonable portion of the attendees. Yet in this particular case, it feels as if the authors have intentionally omitted anything that would help non-experts understand the submission's contributions. There is no general introduction to the topic - in fact, even the central abbreviation (RLHF) is never explained. All the discussion of prior work is concentrated in a half-page Related Work section on page 8, which just summarizes the main take-aways from previous articles but does not explain the general research direction or provide any context. Apart from that, the whole summary of the research area and desiderata is concentrated into the first two essentially empty sentences in the Introduction. Central concepts are not introduced with sufficient rigor and assume knowledge with concepts such as \"margins\" without providing any references for where a reader could at least theoretically obtain the necessary background.\n\nI have no idea why the authors chose to present their results with this style, but it makes the submission essentially useless to all but the few experts who work on RLHF. The way the manuscript entirely ignores the need to convey information to researchers outside of its specific sub-area contrasts not only the general structure of ICLR submissions in other subfields, but also the much more welcoming style employed in many of the previous works on the topic cited by the submission. In its current form, I simply cannot see the submission as being ready to appear in the ICLR proceedings."}, "questions": {"value": "None, but the authors are welcome to respond to the weaknesses listed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qkq6f60hrX", "forum": "C3FEh03LMO", "replyto": "C3FEh03LMO", "signatures": ["ICLR.cc/2026/Conference/Submission23615/Reviewer_ybHd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23615/Reviewer_ybHd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761517098171, "cdate": 1761517098171, "tmdate": 1762942735793, "mdate": 1762942735793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Honestly, I could not understand what this paper is about -- see the weaknesses section"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "once again, it is hard to judge strengths of the paper in the current form, see weaknesses section."}, "weaknesses": {"value": "The paper lacks many components that are expected from an ICLR submission -- motivation, introduction for non-specialists, proper discussion and comparison with the existing work. The introduction is rather short and right away starts operating with terminology like RLHF, ladders, margins and so on, which is never properly defined (for instance, it is not even defined what RLHF stands for). As a result, I am not even sure to which area this paper belongs to... If I'm not mistaken, there are no references until page 8, where a rather breve survey of the existing work appears (which looks more like a list of key words with references).\n\nAs a summary, the current submission looks at best like a draft, but it is definitely not ready for a publication."}, "questions": {"value": "no questions at this stage"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iwMdQIzGbv", "forum": "C3FEh03LMO", "replyto": "C3FEh03LMO", "signatures": ["ICLR.cc/2026/Conference/Submission23615/Reviewer_hps1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23615/Reviewer_hps1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761661617016, "cdate": 1761661617016, "tmdate": 1762942735540, "mdate": 1762942735540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the question of determining when two objective functions used in RLHF are equivalent. It takes a formal verification approach, and the main result appears to be a canonicalizing algorithm for RLHF objectives.\n\nUnfortunately the paper is written in a completely impenetrable way and is unsuitable for publication in this state."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "N/A"}, "weaknesses": {"value": "The paper is written in an highly impenetrable and obscure fashion, seemingly with extensive usage of an LLM. The problem is never properly defined, motivation is barely provided, terminology is used without explanation, exposition and organization is absent, and the writing is extremely terse and superficially mathy to the point of deliberate obscurantism. Sample the first sentence of Section 3: \"This section formalizes an equational theory for RLHF ladders and shows that, within the reducible class R (Assumptions (R1)â€“(R3) in Preliminaries), a terminating and locally confluent rewrite system yields a unique normal form (up to a fixed gauge)\".\n\nRegardless of contribution, the paper simply cannot be read in this state, much less accepted for publication."}, "questions": {"value": "If the authors are serious about their contribution, I would recommend rewriting the paper entirely from scratch and making it accessible to a general ML audience."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5sD9eGUQQd", "forum": "C3FEh03LMO", "replyto": "C3FEh03LMO", "signatures": ["ICLR.cc/2026/Conference/Submission23615/Reviewer_xc5G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23615/Reviewer_xc5G"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762328112362, "cdate": 1762328112362, "tmdate": 1762942735293, "mdate": 1762942735293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper uses tools from logic and rewrite theory to show that many RLHF objectives are algebraically equivalent and can be canonicalized in a certain way. For irreducible methods, the theory can produce finite witnesses that show irreducibility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The theory is novel and interesting. It is also correct as far as the reviewer can tell. The paper is comprehensive and cover most of the questions that could be asked in this line of work, with limitations clearly explained. Examples are performed on latest methods of RLHF, and key takeaways from each sections are cleanly stated."}, "weaknesses": {"value": "While expected from a more algebraic approach, the main drawbacks of the paper include:\n1. Lack of immediate actionable takeaway: At the heart of the paper is a reduction technique to rewrite one method of RLHF to another, base around preserving the margin. However, this does not necessarily mean that one method is preferred to another, or that the canonical method is superior to reducible methods that it is reducing to. With their reduction, the authors are able to canonicalize many ideas, from margin to regrets, etc, but it would be more valuable to demonstrate, at least experimentally why a canonicalized version would be preferred (e.g. numerical stability).\n2. Margin-equivalent may obfuscate finer details in optimization and generalization: the canonicalization that serves as one of the key contributions of the paper is based around defining methods to be equivalent when they are (Bayes) margin-equivalent. However, it is known from margin theory of classification optimization that the value of the margin induced by the hypothesis function has impact on, say, implicit biases of gradient descents (e.g. Lyu and Li 2019 \"Gradient Descent Maximizes the Margin of Homogeneous Neural Networks\" or Ji and Telgarsky 2019, 2020 \"The implicit bias of gradient descent on nonseparable data\", etc.). There are also margin-based generalization bounds (Bartlett, Foster and Telgarsky 2017 \"Spectrally-normalized margin bounds for neural networks\", etc.). While these works are for supervised classification task, it highlights the fact that even if two methods produce the same margin, they may have different optimization and generalization properties altogether.\n3. Guarantees are rather loose (which is expected from a more algebraic/general approach): for instance, the regret transfer bound is only guaranteed under some nondecreasing phi, which can be quite large."}, "questions": {"value": "1. Does the proposed canonicalization works for binary classification in the supervised setting? \n2. Can you use this method to canonicalize deep learning architectures, instead of just lose functions? Perhaps easier, can you apply this to different regularizers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o3FtBWtmMz", "forum": "C3FEh03LMO", "replyto": "C3FEh03LMO", "signatures": ["ICLR.cc/2026/Conference/Submission23615/Reviewer_caKT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23615/Reviewer_caKT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762351391056, "cdate": 1762351391056, "tmdate": 1762942734925, "mdate": 1762942734925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}