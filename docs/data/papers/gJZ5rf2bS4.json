{"id": "gJZ5rf2bS4", "number": 22419, "cdate": 1758330820394, "mdate": 1763626386750, "content": {"title": "Multiple-Prediction-Powered Inference", "abstract": "A core challenge in modern AI model development is obtaining high-quality evaluation metrics in a cost-effective way. Such evaluation often involves tradeoffs between expensive, high-quality measurements and a variety of lower-quality proxies. We introduce Multiple-Prediction-Powered Inference (MultiPPI), a general framework for constructing statistically efficient estimates by optimally allocating resources across these diverse data sources. We provide theoretical guarantees about the minimax optimality, finite-sample performance, and asymptotic normality of the MultiPPI estimator. Through experiments across three diverse large language model (LLM) evaluation scenarios, we show that MultiPPI consistently achieves lower estimation error than existing baselines. This advantage stems from its budget-adaptive allocation strategy, which strategically combines subsets of models by learning their complex cost and correlation structures.", "tldr": "This paper introduces MultiPPI, an optimal procedure for mean estimation by combining expensive, high-quality data with cheap, lower-quality proxies.", "keywords": ["Prediction-powered inference", "Statistical estimation", "Model evaluation", "LLM as judge", "PPI"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/219f5c267d8dc87bea318f2598aacb007c75177a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles cost-efficient estimation when a high-quality but expensive signal coexists with cheaper proxies. The method formulates variance-minimizing estimators with constraints. With known covariance \\(\\Sigma\\) this becomes a tractable SOCP/SDP and is minimax-optimal. With \\(\\hat{\\Sigma}\\) from a small burn-in, it retains finite-sample bounds and asymptotic normality. Then, the authors show in experiments that, on LLM evaluation (arena wins, reasoning budgets, factuality), their proposed method achieves lower MSE and tighter CIs, adapting from cheap proxies at low budgets to accurate raters as budgets grow, in multiple settings."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.  A major strength is that the paper frames the complex allocation problem in a manner that makes it solvable using standard optimization tools.\n2.  The paper provides finite-sample bounds on the estimator's performance for the practical scenario where the covariance $\\Sigma$ must be estimated from data.\n3.  The effectiveness of MultiPPI is demonstrated across diverse and relevant large LLM evaluation scenarios."}, "weaknesses": {"value": "1.  The practical algorithm relies heavily on an a priori estimate of the covariance matrix $\\Sigma$, which is derived from an initial \"burn-in\" set of $N$ fully-labeled samples. The performance may degrade if the size $N$ is improperly chosen or if $\\Sigma$ is ill-conditioned.\n2.  The MultiPPI framework allows sampling from any subset $I$ of $k$ variables. However, enumerating or considering many such subsets and solving the corresponding SOCP/SDP problems can become computationally expensive as $k$ increases. The authors should include a discussion on the scalability of the MultiPPI estimator.\n3.  The finite-sample bounds in Theorem 4.3 assume that $|X_i| \\leq 1$, which is a restrictive assumption. Some evaluation signals (e.g., scores/logits) may violate this bound in other application scenarios.\n4.  MultiPPI is designed to estimate a linear function of the mean. This may limit its applicability for estimating a broader scope of non-linear functions of the mean. (This is noted as a potential limitation, perhaps minor given the paper's focus.)"}, "questions": {"value": "Please see the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qWbhq0IKiY", "forum": "gJZ5rf2bS4", "replyto": "gJZ5rf2bS4", "signatures": ["ICLR.cc/2026/Conference/Submission22419/Reviewer_pGiG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22419/Reviewer_pGiG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750637480, "cdate": 1761750637480, "tmdate": 1762942211309, "mdate": 1762942211309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response: New Experiments on Small-Sample Sizes, Robustness and Shrinkage"}, "comment": {"value": "We thank the reviewers for their thoughtful and detailed feedback. We were encouraged by the consensus that the problem formulation is \"clean,\" \"interesting,\" and \"practical.\"\n\nA primary question raised across reviews (by R-bTBc, R-YZU1, R-pGiG) concerned the robustness of MultiPPI to covariance misspecification, particularly in regimes with very few labeled samples.\nWe have taken this feedback to heart and significantly expanded our experimental and theoretical analysis to address this. We are excited to report that in new stress tests with **as few as $n=10$ labeled samples, MultiPPI (using shrinkage) consistently outperforms all baselines and achieves up to a 50% reduction in Mean Squared Error (MSE)** compared to classical sampling. The revised manuscript (changes in red) includes the following updates:\n\n---\n\n## 1. Robustness via Shrinkage Estimation (New Section D.4)\n\nA key strength of the MultiPPI framework is its flexibility: it is agnostic to the specific choice of covariance estimator (see Theorems E.1 and F.3) and can utilize any \"plugin\" estimate. While our original submission used the simplest empirical covariance for clarity, we have now demonstrated—following the excellent suggestion of Reviewer bTBc—that more advanced estimators can be easily swapped in. Specifically, we implemented MultiPPI using Ledoit-Wolf shrinkage covariance estimation. We find that this enhances performance across all scenarios without requiring any changes to the core optimization framework, proving that MultiPPI can effectively accommodate advanced estimation techniques to handle low-data regimes.\n\n---\n\n## 2. Best Performance in Low-Data Regimes (New Section D.3)\n\nWe performed extensive new stress tests varying the number of labeled samples $n$ from 10 to 250.\n\n* **Empirical Results:** We find that MultiPPI with shrinkage remains the best method even with extreme data scarcity, when compared to all baselines (classical sampling, PPI++). With as few as $n=10$ samples, MultiPPI achieves up to a **50% reduction in Mean Squared Error (MSE)** compared to classical sampling.\n\n---\n\n## 3. Strengthened Theoretical Guarantees (Theorem 4.4 & D.1)\n\nWe have formalized the theoretical basis for these empirical gains:\n\n* **Sensitivity Analysis:** We highlight **Theorem 4.4,** which provides finite-sample bounds explicitly quantifying the sensitivity of our estimator to errors in the covariance estimate.\n\n* **Shrinkage Corollary:** We added **Theorem D.1,** which specializes our finite-sample bounds to the case of Ledoit-Wolf shrinkage. This confirms that the improved condition number of the shrinkage estimator translates directly to tighter theoretical error bounds.\n\nWe believe these additions conclusively address the concerns regarding robustness and practical applicability in low-resource settings. We invite the reviewers to examine these new results in Appendix D."}}, "id": "nakILD9WrB", "forum": "gJZ5rf2bS4", "replyto": "gJZ5rf2bS4", "signatures": ["ICLR.cc/2026/Conference/Submission22419/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22419/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22419/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763626492981, "cdate": 1763626492981, "tmdate": 1763626492981, "mdate": 1763626492981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a method to estimate the resource allocations for evaluation, e.g., assessing the qualities of model predictions in a task using large language models, under the budget constraint. Basic idea is to formulate the task setting as a gold human label with \"auorators\" by language models, and to trade the number of queries to the models and the expected qualities with the maximum budget constraint. It is now treated as an optimization problem assuming the accurate covariance of labeled samples are obtained."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This work introduce a task setting to maximize the expected evaluation qualities under the resource budget so that the number of queries to language models could be minimized. It is an interesting yet practical setting especially when language models are employed for evaluation, i.e., LLM-as-a-judge."}, "weaknesses": {"value": "- Clarity is an issue. The task setting assumes $n$ samples with gold labels by human and $n$ samples without gold and thus estimated by autorators only, i.e., language models, in Equation 3. Further discussion introduces the cascade modeling in Equation 4. However, their relation to the proposed approach is not discussed in section 4. As a result, the contribution of this work is not clear.\n- Given the task setting assumes incomplete labels in term of the lack of human ratings, it is not clear how that is reflected in the experimental settings. It is also not clear what optimizer was used in the experiment."}, "questions": {"value": "See the comments regarding weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "mxqyRtFe9G", "forum": "gJZ5rf2bS4", "replyto": "gJZ5rf2bS4", "signatures": ["ICLR.cc/2026/Conference/Submission22419/Reviewer_LyCP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22419/Reviewer_LyCP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762040599201, "cdate": 1762040599201, "tmdate": 1762942211031, "mdate": 1762942211031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical challenge in modern AI model development: balancing the cost and quality of evaluation metrics. It proposes MultiPPI, a statistical framework that optimally allocates resources across diverse data sources (high-cost, high-quality \"gold\" metrics and low-cost, low-quality proxies) to estimate linear functions of population means (e.g., LLM win-rates, factuality accuracy) under a fixed budget. The framework provides theoretical guarantees (minimax optimality, finite-sample bounds, asymptotic normality) and validates performance across three LLM evaluation tasks, consistently outperforming baselines like classical sampling, PPI++, and vector PPI++."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The authors provide a complete and well-structured theoretical framework, which is rare in applied AI evaluation work:  \n- Minimax Optimality: When the covariance matrix of data sources is known, MultiPPI achieves the minimax risk lower bound (Theorem 4.2), ensuring it is statistically optimal among all budget-feasible estimators. This anchors the method in fundamental statistical principles, rather than heuristic design.  \n- Practical Guarantees: For real-world scenarios where covariance matrices are unknown, the paper derives finite-sample bounds (Theorem 4.3) that quantify performance degradation when using empirical covariance matrices. It also proves asymptotic normality (Theorem 4.4), enabling valid confidence interval construction—critical for practical decision-making (e.g., comparing model versions).  \n- Optimization Tractability: The authors skillfully transform the resource allocation problem into solvable programs: second-order cone programming (SOCP) for single budgets and semi-definite programming (SDP) for multiple budgets. This ensures MultiPPI can be implemented with off-the-shelf tools (e.g., cvxpy), bridging theory and practice."}, "weaknesses": {"value": "The Method Rely on Accurate Covariance Estimation\n\nIn practice, MultiPPI requires a set of \"fully labeled samples\" (containing both gold metric$X_1$ and all proxies $\\(X_2,...,X_k\\)$ to estimate the empirical covariance matrix $\\(\\hat{\\Sigma}\\)$. The paper uses N=250 or 1000 such samples, but :  \n- Fully labeled samples may be scarce: Gold metrics like human annotations are often extremely costly—collecting 250 fully labeled samples could be prohibitive for small teams or niche tasks (e.g., low-resource language LLM evaluation).  \n- Cost-effectiveness of estimation: The paper does not sufficiently explore whether an accurate covariance matrix can be robustly estimated with a smaller, more cost-effective sample."}, "questions": {"value": "1. Are the golden samples, (i.e., the fully-labeled samples that contain both the high-quality metric $\\(X_1\\)$ and all low-cost proxy metrics $\\(X_2, ..., X_k\\)$), prepared to obtain an accurate covariance matrix sufficient for well estimating the mean of the high-quality score, $\\(E[X_1]\\)$,  thereby avoiding the need to compute the low-cost proxy metrics??  \n2. Evaluation metrics are often used in the model training phase. If the model is modified, can the empirical covariance matrix $\\(\\hat{\\Sigma}\\)$ obtained under the old model still be applied to the new model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DpIBzO8EuU", "forum": "gJZ5rf2bS4", "replyto": "gJZ5rf2bS4", "signatures": ["ICLR.cc/2026/Conference/Submission22419/Reviewer_YZU1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22419/Reviewer_YZU1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762885007285, "cdate": 1762885007285, "tmdate": 1762942210571, "mdate": 1762942210571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Multiple-Prediction-Powered Inference (MultiPPI), an extension of Prediction-Powered Inference (PPI++) to settings with multiple predictive models of varying cost and accuracy. The authors frame the problem of estimating a population mean under budget constraints as an optimization over which predictors to sample and how to weight them. They derive a minimax-optimal solution assuming known covariance, propose a practical version using an estimated covariance matrix, and provide asymptotic and finite-sample guarantees. Experiments on LLM-evaluation tasks (Chatbot Arena, ProcessBench, factuality benchmarks) suggest improved efficiency over baseline PPI methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The formulation is clean and the mathematics appear correct.\n2. Good Application. The use of realistic LLM cost scenarios is interesting for the LLM evaluation tasks.\n3. The optimization framing (SOCP/SDP) is good and potentially generalizable."}, "weaknesses": {"value": "I don't see much weakness of the paper. The main contribution is clear: MultiPPI is a straightforward generalization of PPI++: it replaces a single predictor with a vector and adds a *cost-weighted* sampling constraint. The estimator remains linear, the theory is a direct extension of standard control variates, and the minimax-optimality proof follows textbook arguments once the covariance structure is fixed. I am not sure if this contributions reaches the bar of ICLR though. I would also think that Cost-aware PPI would be a better name of the method, since the main contributions is not from aggregating multiple ML predictions, but  aggregate them in a cost-aware way."}, "questions": {"value": "1. How sensitive are the theoretical results to covariance misspecification?\n2. How sensitive is MultiPPI’s performance to errors in the covariance estimate? Could shrinkage or robust covariance estimation improve stability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MXJjvNdfrr", "forum": "gJZ5rf2bS4", "replyto": "gJZ5rf2bS4", "signatures": ["ICLR.cc/2026/Conference/Submission22419/Reviewer_bTBc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22419/Reviewer_bTBc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762906658809, "cdate": 1762906658809, "tmdate": 1762942210089, "mdate": 1762942210089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}