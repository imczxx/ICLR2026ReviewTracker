{"id": "nLVmi5OKql", "number": 2097, "cdate": 1756988574717, "mdate": 1759898169728, "content": {"title": "TiMi: Empowering Time Series Transformers with Multimodal Mixture of Experts", "abstract": "Multimodal time series forecasting has garnered significant attention for its potential to provide more robust and accurate predictions than traditional single-modality models by leveraging rich information inherent in other modalities. However, due to fundamental challenges in modality alignment, existing methods often struggle to effectively incorporate multimodal data into predictions, particularly textual information that has a causal influence on time series fluctuations, such as emergency reports and policy announcements.\nIn this paper, we reflect on the role of textual information in numerical forecasting and propose **Ti**me series transformers with Multimodal **Mi**xture-of-Experts, **TiMi**, to unleash the causal reasoning capabilities of LLMs. Concretely, TiMi utilizes language models to generate inferences on future developments, which then serve as guidance for time series forecasting.\nTo seamlessly integrate both exogenous factors and time series into predictions, we introduce a Multimodal Mixture-of-Experts (MMoE) module as a lightweight plug-in to empower Transformer-based time series models for multimodal forecasting, eliminating the need for explicit representation-level alignment. Experimentally, our proposed TiMi demonstrates consistent state-of-the-art performance on sixteen real-world multimodal forecasting benchmarks, outperforming advanced unimodal and multimodal baselines while offering strong adaptability and interpretability.", "tldr": "", "keywords": ["deep learning", "machine learning", "time series forecasting"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e9e55bab46e8041ad2e0503b48a0efa78c151d47.pdf", "supplementary_material": "/attachment/caeec3570165e81aec256d526e86a6164cf6a1cb.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes TiMi, a time-series–centric Transformer enhanced with a Multimodal Mixture-of-Experts (MMoE) that injects causal guidance from text. A frozen LLM first extracts structured inferences about future developments (trends/periodicity/shocks) from exogenous text. Then they are routed via a Text-informed MoE (TMoE), while historical series form a global representation that routes a Series-aware MoE (SMoE). This design avoids explicit representation-level alignment and aims to guide prediction instead of fusing features. Across multiple real-world multimodal forecasting benchmarks, TiMi reports consistent SOTA over unimodal and multimodal baselines, with claims of adaptability and interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear problem reframing and architecture: The paper articulates why text–series semantic misalignment makes standard early/late fusion suboptimal and instead uses text as guidance through MoE routing (TMoE + SMoE), which is a clean, modular idea that fits common TS Transformers.\n\n2. Good experiment results: The proposed TiMi method consistently achieves superior forecasting performance on multiple datasets than baselines, with notable error reductions."}, "weaknesses": {"value": "1. The proposed TiMi presumes that available text causally informs future series. However, when text is noisy, off-topic, or adversarial, TMoE routing might misguide the forecaster. Besides, the proposed method does not thoroughly stress-test this with ablation on text quality, noise level, or contradictory narratives. \n\n2. While TiMi leverages LLM causal reasoning, the experiments don’t include causal identification/diagnostics (e.g., interventions, counterfactual text deletion, or do-calculus-style tests). Instead, current case studies are suggestive but do not disentangle correlation from causation."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "L886qHYoFS", "forum": "nLVmi5OKql", "replyto": "nLVmi5OKql", "signatures": ["ICLR.cc/2026/Conference/Submission2097/Reviewer_ukpQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2097/Reviewer_ukpQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760666048498, "cdate": 1760666048498, "tmdate": 1762916024868, "mdate": 1762916024868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose Time-series Transformers with multimodal Mixture of experts (TiMi) for multimodal time series forecasting. By incorporating a Multimodal Mixture-of-Experts as a plug-in module into Transformer-based time series forecaster, it enables the seamless integration of structured extracted knowledge into context-based prediction, rather than performing vague feature-level fusion. The authors demonstrated the effectiveness of their algorithm through comparative experiments with unimodal and multimodal baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The model architecture demonstrates a certain degree of innovation. Leveraging the concept of selective routing of MoE, it introduces TMoE and SMoE modules, which enable adaptive representation learning of historical time series and extracted future textual guidance, respectively."}, "weaknesses": {"value": "1. With the continuous advancement of research in multimodal time series forecasting, many new SOTA models have emerged in the past year. The selected baselines do not cover the latest SOTA models. It is recommended to include more recent SOTA models for comparison, particularly with a greater focus on multimodal time-series forecasting models.  \n\n2. There is a lack of error bar analysis.  \n\n3. The explanation for Figure 6 is not sufficiently clear and should be revised."}, "questions": {"value": "Using large language  models to extract information may, on one hand, involve potential data leakage issues, and on the other hand, result in extracted information that is irrelevant or misleading to the time series data. \n\nRegarding the data leakage problem, how can we ensure that no information leakage exists, and how do we evaluate whether the model's performance improvement stems from its architecture or from leaked future information? \n\nAs for irrelevant or misleading extracted information, part of the issue arises from the inherent noise in textual data itself, while another part stems from the hallucination problems inherent in large language models. I am curious about how the TiMi framework addresses these two aspects respectively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JhFS3Y7H6R", "forum": "nLVmi5OKql", "replyto": "nLVmi5OKql", "signatures": ["ICLR.cc/2026/Conference/Submission2097/Reviewer_5MRV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2097/Reviewer_5MRV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967865855, "cdate": 1761967865855, "tmdate": 1762916024300, "mdate": 1762916024300, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TiMi tries to solve the problem of aligning textual data with time-series for forecasting tasks. They propose a guidance based approach where the textual embeddings generated from LLMs are used to guide some of the MoE experts of the transformer backbone whereas other experts are guided by the backbones' temporal embeddings. This approach is claimed to be more efficient than early or late fusion approached of past works and enables better performance on wide range of domains. The ablations show the importance of mixed multimodal MoE approach, with case studies showing examples of textual information guiding the forecasts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The methodology is well motivated is non-trivial\n2. helps solves an important problem in time+text forecasting\n3. Results are promising across wide range of benchmarks from past datasets"}, "weaknesses": {"value": "1. Can the models adapt to varying degree of textual information? In many applications text data is sparse and not as well aligned as those used in say Time-MMD which the paper uses. Does the model handle such situations by adapting to use more of temporal MoEs when necessary?\n2. How does the model compare against other integrated solutions such as using both time and text inputs as part of single embedding representations (like OpenTSLM https://arxiv.org/abs/2510.02410) ?\n3. How does the performance degrade with increase in horizon lenght? How are different MoEs used at longer horizons?"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M8fQ6iu7n8", "forum": "nLVmi5OKql", "replyto": "nLVmi5OKql", "signatures": ["ICLR.cc/2026/Conference/Submission2097/Reviewer_Kf6m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2097/Reviewer_Kf6m"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762114061577, "cdate": 1762114061577, "tmdate": 1762916023137, "mdate": 1762916023137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is about a multimodal model to process text and numerical data for time series forecasting. Time series transformers with multimodal MoE (TiMi) uses language models toto guide the time series forecasting.  To integrate exogenous text + numerical data, the authors introduce a TMoE (text MoE) and SMoE (series aware MoE), in order to circumvent text-series representation alignment. Experiments are performed on the Time-MMD and Time-IMM datasets (16 datasets total). The model is evaluated against unimodal and multimodal approaches and achieves superior performance compared to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed approach embeds an MoE modules for both the series and the text instead of explicitly aligning both modalities. The LLM then guides the prediction of the primary time-series branch. To the best of my knowledge this is novel. The design is well-motivated for semantically misaligned series–text pairs common in practice.\n\n2. The modular approach allows authors to plug in the MMoE into other transformer based methods (PatchTST, autoformer etc, table 2). In all cases, the MMoE improves predictive performance which clearly shows the advantage of the methods.\n\n3. The ablation on LLMs also seems to suggest that stronger/larger LLMs improve the model performance, which make the method general.  \n\n4. The Mann-Kendall trend test to detect monotonic trends adds a layer of interpretability to the model."}, "weaknesses": {"value": "1. The work misses providing finer details of the exogenous text data (how it is obtained or generated), and how the LLM is prompted for guiding the time-series prediction. The “causal knowledge” claim hinges on how text is curated, and aggregated; beyond average pooling of LLM embeddings, key preprocessing choices and robustness to noisy/off-topic text are not thoroughly stress-tested. \n\n2. Given many works in the (multimodal) TSF community and experimental settings and implementations often differing between papers, I believe submission of code (ideally for both the method and baselines) would strengthen the paper. Currently I see a single python file submitted for the supplementary material containing just the model class. But this misses details on how data processing was performed and how experiments were run on baselines etc."}, "questions": {"value": "- How sensitive is the TMoE routing to noisy or irrelevant text?\n- L446 says figure 4.3 but I am unable to find it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RbzJxcV5Rb", "forum": "nLVmi5OKql", "replyto": "nLVmi5OKql", "signatures": ["ICLR.cc/2026/Conference/Submission2097/Reviewer_GtWW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2097/Reviewer_GtWW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762121818670, "cdate": 1762121818670, "tmdate": 1762916022715, "mdate": 1762916022715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}