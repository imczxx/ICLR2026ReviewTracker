{"id": "9WJoD0iDig", "number": 5623, "cdate": 1757923560805, "mdate": 1763553150638, "content": {"title": "Foresight Diffusion: Improving Sampling Consistency in Predictive Diffusion Models", "abstract": "Diffusion and flow-based models have enabled significant progress in generation tasks across various modalities and have recently found applications in predictive learning. However, unlike typical generation tasks that encourage sample diversity, predictive learning entails different sources of stochasticity and requires sampling consistency aligned with the ground-truth trajectory, which is a limitation we empirically observe in diffusion models. We argue that a key bottleneck in learning sampling-consistent predictive diffusion models lies in suboptimal predictive ability, which we attribute to the entanglement of condition understanding and target denoising within shared architectures and co-training schemes. To address this, we propose **Foresight Diffusion (ForeDiff)**, a framework for predictive diffusion models that improves sampling consistency by decoupling condition understanding from target denoising. ForeDiff incorporates a separate deterministic predictive stream to process conditioning inputs independently of the denoising stream, and further leverages a pretrained predictor to extract informative representations that guide generation. Extensive experiments on robot video prediction and scientific spatiotemporal forecasting show that ForeDiff improves both predictive accuracy and sampling consistency over strong baselines, offering a promising direction for predictive diffusion models.", "tldr": "We propose Foresight Diffusion to improve the sampling consistency in predictive diffusion models through decoupling conditional understanding from target denoising.", "keywords": ["diffusion models", "flow-based models", "predictive learning", "generative models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ffa0e5fc3cf75b2b15b24074c5a1e35283d1c070.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Diffusion models have achieved remarkable success in generative tasks, but they struggle with predictive learning, where sampling consistency with real trajectories is crucial. The authors identify this limitation as stemming from the entanglement of condition understanding and target denoising in shared architectures. To address this, they propose Foresight Diffusion (ForeDiff), which separates deterministic condition processing from stochastic denoising, leading to improved predictive accuracy and sampling consistency across robotic and spatiotemporal forecasting tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This paper is overall well-written and easy to follow. The authors start from the discussion of the difference between standard generation tasks and predictive tasks via diffusion model. The predictive tasks require more accuracy on the dynamic, while diffusion model’s generation process has a large uncertainty. The investigated problem is interesting and practical.\n\n* The motivation of the proposed method, which is to detangle the generation and conditional guidance, is reasonable and clear.\n\n* The empirical results demonstrate the good results over vanilla diffusion model baseline on predictive tasks."}, "weaknesses": {"value": "* While the motivation is clear and conceptually sound, the qualitative justification remains weak. For instance, in Figure 3(b), the authors use LPIPS to measure the similarity between generated and ground-truth samples, concluding that diffusion models exhibit greater uncertainty as they produce both the best and worst cases compared to iVideoGPT. However, it is unclear whether LPIPS is an appropriate metric for assessing dynamic prediction quality. LPIPS primarily measures perceptual similarity, which may not capture temporal or dynamic consistency crucial for predictive tasks.\n\n* The proposed methodological framework appears rather simple and incremental. The main difference lies in injecting the conditioning information into the first DiT block instead of concatenating it with the noised target input. Similar conditioning mechanisms have been used in many existing diffusion models (e.g., Stable Diffusion). If the primary novelty lies only in altering where the condition is injected, the technical contribution seems quite limited.\n\n* The experimental evaluation also lacks breadth. The authors only compare their method against a vanilla diffusion baseline. Without comparisons to other relevant or stronger baselines, it is difficult to convincingly demonstrate the effectiveness of the proposed approach."}, "questions": {"value": "1. What is the difference between the conditioning injection mechanism in Stable Diffusion and that in ForeDiff-Zero? If they are similar, does Stable Diffusion already possess the capability to disentangle generation and conditional guidance?\n\n2. How does the proposed method compare to using a deterministic predictor directly for prediction tasks?\n\n3. Many text-to-diffusion methods also use textual information as conditional input. How would textual conditioning be incorporated into ForeDiff-Zero?\n\n4. The predictive task discussed here resembles the concept of a world model, which has recently gained significant attention. What is the conceptual and practical difference between your method and existing world models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tlALDRdk1J", "forum": "9WJoD0iDig", "replyto": "9WJoD0iDig", "signatures": ["ICLR.cc/2026/Conference/Submission5623/Reviewer_Cbg4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5623/Reviewer_Cbg4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559078336, "cdate": 1761559078336, "tmdate": 1762918163645, "mdate": 1762918163645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Foresight Diffusion, which decouples a deterministic predictive stream (ViT blocks processing the condition $y$) from a generative/denoising stream (DiT blocks processing $x_t$), and then adopts a two-stage training scheme: first train the predictive stream as a standalone deterministic predictor with a prediction head, then freeze it and feed its internal representation $g_M$ to guide diffusion generation. The claim is that this design improves “predictive ability” and sampling consistency (lower across-seed variance of PSNR/SSIM/LPIPS) on datasets such as RoboNet, RT-1, and HeterNS."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method is simple and relatively easy to reproduce: the architectural split and the two-stage schedule are clearly described, and the paper includes ablations on the prediction head and the number of ViT blocks in the predictive stream. Some datasets show moderate improvements in PSNR/LPIPS and reduced reported variability, and removing the prediction head appears beneficial across tasks per the appendix tables."}, "weaknesses": {"value": "(1) Limited novelty and weak theory. The main idea—strengthening a condition encoder via standalone pretraining and then freezing its features to condition the denoiser—tracks common practice in conditional diffusion and teacher-feature guidance. Theoretical support is thin: the central formal argument reduces the $t=1$ case to a deterministic model by zeroing the first-layer weights, which does not yield general consistency or error bounds for multi-step diffusion.\n\n(2) Consistency is proxied narrowly and may conflate “agreement” with “accuracy.” Using the STD of PSNR/SSIM/LPIPS across repeated samples risks rewarding collapse or overly strong reliance on the condition signal. The paper lacks calibration-oriented metrics (e.g., NLL/CRPS/coverage) or uncertainty diagnostics to demonstrate that reduced variability corresponds to better probabilistic modeling, rather than just more concentrated—but possibly biased—outputs.\n\n(3) Distributional quality gains are not robust. On RT-1, the reported FVD degrades as predictive blocks increase (e.g., baseline 11.7 vs. 12.0 at the default $M=6$, undermining the claim that decoupling and two-stage training consistently improve generative quality. The paper should transparently analyze this trade-off.\n\n(4) Ablations do not fully isolate training-scheme and compute confounds. Although the appendix contrasts “training-only decoupling” (pretrain at $t=1$ then fine-tune) with the architectural variant, the study still does not establish equalized budgets (steps/FLOPs/memory/denoising steps) between end-to-end shared vs. decoupled + frozen designs. Nor are throughput/latency or memory overheads reported for the extra predictive stream and the freeze-and-feed pipeline.\n\n(5) Baselines and task breadth are insufficient. The paper mostly compares to its own variants and standard conditional diffusion. For scientific/physical forecasting, head-to-head evaluations against strong PDE/forecasting models (e.g., modern operator-learning and transformer baselines) are missing; for video prediction, broader masked/autoregressive competitors would better position the claimed advantages.\n\n(6) Method details and limits remain under-explained. The Fusion mechanism and temporal conditioning choices are specified but not systematically stress-tested for long-horizon prediction or out-of-distribution conditions; appendix tables list numbers (e.g., PredHead effects) without enough analysis about when and why the components help or hurt."}, "questions": {"value": "Can you report calibration metrics (e.g., NLL/CRPS/coverage) and analyze their relationship to the STD of PSNR/SSIM/LPIPS, on the same hardware and sampling budgets?\n\nPlease provide equal-budget comparisons (same steps, FLOPs, memory, sampling steps) between (i) end-to-end shared models and (ii) your decoupled two-stage pipeline, along with throughput/latency and memory use at training and inference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0RnmCQVMOV", "forum": "9WJoD0iDig", "replyto": "9WJoD0iDig", "signatures": ["ICLR.cc/2026/Conference/Submission5623/Reviewer_SCh5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5623/Reviewer_SCh5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831101056, "cdate": 1761831101056, "tmdate": 1762918163374, "mdate": 1762918163374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Foresight Diffusion (ForeDiff), a framework designed to improve sampling consistency in predictive diffusion models. The authors identify that vanilla diffusion models, while effective for diverse generation tasks, suffer from high sample variance when applied to predictive learning where consistency is crucial. ForeDiff addresses this by architecturally decoupling condition understanding from target denoising through separate predictive and generative streams, and employing a two-stage training scheme with deterministic pretraining."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a novel architectural approach by explicitly separating condition processing from denoising, which is a creative departure from standard conditional diffusion models.\n- The focus on sampling consistency as a distinct requirement for predictive tasks versus generative tasks is an important problem formulation.\n- Clear mathematical formulation and proof of the key lemma connecting diffusion and deterministic models\n- Comprehensive experimental evaluation across three diverse datasets (RoboNet, RT-1, HeterNS) covering both real-world robotics and scientific computing domains\n- Shows consistent (but modest) improvements across multiple metrics and datasets\n- Well-structured paper with clear motivation through visual examples (Figures 1 and 2) and illustration of architectural differences (Figure 4)"}, "weaknesses": {"value": "- As acknowledged by the authors, experiments are limited to moderate-scale settings (64×64 resolution, relatively small models). \n- Only DiT-based architectures are evaluated; generalization to U-Net or other diffusion backbones is assumed but not demonstrated\n- The connection between predictive ability and sampling consistency could be more rigorously established\n- The improvement margins, while consistent, are sometimes modest\n- The two-stage training could be seen as unfair comparison since vanilla diffusion does not benefit from deterministic pretraining"}, "questions": {"value": "- Do you anticipate ForeDiff will scale well to higher-resolution datasets and larger models (e.g., 256×256 or video generation)?\n- Have you attempted to adapt ForeDiff to non-DiT backbones, such as U-Net or latent diffusion architectures? If not, what are the main obstacles?\n- Since ForeDiff benefits from deterministic pretraining, could you provide a control where the baseline diffusion model is pretrained with a similar deterministic phase? \n- Could you clarify why the deterministic predictor must be trained separately rather than jointly fine-tuned with the diffusion model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V00JasLrfK", "forum": "9WJoD0iDig", "replyto": "9WJoD0iDig", "signatures": ["ICLR.cc/2026/Conference/Submission5623/Reviewer_6hoQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5623/Reviewer_6hoQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911893997, "cdate": 1761911893997, "tmdate": 1762918162946, "mdate": 1762918162946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a key discrepancy between the typical use of diffusion models for generative tasks and their application to predictive learning. While generative tasks often value sample diversity, predictive tasks (like video forecasting) require sampling consistency, where generated samples are tightly clustered around a single, physically plausible ground-truth future. The authors hypothesize that the suboptimal consistency of standard predictive diffusion models stems from the entanglement of condition understanding and target denoising within shared architectures and co-training schemes.\n\nTo solve this, the paper proposes Foresight Diffusion (ForeDiff), a framework that disentangles these two roles. ForeDiff consists of two main contributions:\n\n1. A Decoupled Architecture consists of a condition understanding branch and a generative branch.\n\n2. A Two-Stage Training Scheme that enables the predictive branch develops a strong foresight which can later provide a strong conditioning signal to the generative branch.\n\nExperiments on robotic video prediction and scientific spatiotemporal forecasting demonstrate that ForeDiff significantly improves both predictive accuracy and sampling consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is a clear and well-motivated paper. It identifies an important issue with applying diffusion models to predictive tasks and proposes an effective solution. The experimental results on various tasks and multiple ablations studies provide evidence for the authors' claims. This work can potentially be applied to various fields such as forecasting, robotics, and scientific ML."}, "weaknesses": {"value": "1. The proposed two-stage training scheme, while effective, introduces additional complexity to the training pipeline compared to a single-stage, end-to-end model. It requires a separate pretraining phase for the predictive branch, which may add to the overall engineering effort, training time, and need for hyperparameter tuning. A discussion of this trade-off (implementation complexity vs. consistency gain) would be beneficial.\n\n2. Using a frozen, deterministic predictor could also be a potential limitation. Predictive learning is not always purely deterministic; there can be legitimate stochasticity or multi-modality in the dataset. Forcing the model to foresee the future and training with a simple $L_2$ loss might suppress the model's ability to capture valid multi-modality, collapsing all potential futures into some form of averaged future prediction.\n\n3. The generative network is fundamentally conditioned on the output of the frozen predictive stream. If the predictive stream makes a significant error, the generative network has no mechanism to correct this. It is forced to denoise towards a faulty premise. Are there any ways to mitigate such scenarios?"}, "questions": {"value": "1. Following on Weakness 1, how sensitive is the final ForeDiff model to the quality of the pretrained deterministic predictor from Stage 1? For instance, if the predictive branch is overfit, or trained for too few steps, does this significantly harm the generative stream's performance or consistency?\n\n2. Could the authors comment on the potential of the setting of jointly training the full architecture with a composite loss, i.e.,\n$\\\\mathcal{L}$ = $\\mathcal{L}\\_{denoise}$ + $\\lambda \\mathcal{L}\\_{deter}$? This ablation studies can further demonstrate the effectiveness of the two-stage training method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vvl5p4yHGn", "forum": "9WJoD0iDig", "replyto": "9WJoD0iDig", "signatures": ["ICLR.cc/2026/Conference/Submission5623/Reviewer_bbPz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5623/Reviewer_bbPz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984612127, "cdate": 1761984612127, "tmdate": 1762918162688, "mdate": 1762918162688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}