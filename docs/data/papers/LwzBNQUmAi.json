{"id": "LwzBNQUmAi", "number": 14180, "cdate": 1758229800561, "mdate": 1763760597073, "content": {"title": "Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment", "abstract": "Existing paradigms for ensuring AI safety, such as guardrail models and alignment training, often compromise either inference efficiency or development flexibility. We introduce Disentangled Safety Adapters (DSA), a novel framework addressing these challenges by decoupling safety-specific computations from a task-optimized base model. DSA utilizes lightweight adapters that leverage the base model's internal representations, enabling diverse and flexible safety functionalities with minimal impact on inference cost. Empirically, DSA-based safety guardrails substantially outperform comparably sized standalone models across hate speech classification, detecting unsafe model inputs and responses and hallucination detection with relative improvements of up to 53\\% in AUC.  Furthermore, DSA-based safety alignment allows dynamic, inference-time adjustment of alignment strength and a fine-grained trade-off between instruction following performance and model safety. Importantly, combining the DSA safety guardrail with DSA safety alignment facilitates context-dependent alignment strength, boosting safety on StrongReject by 93\\% while maintaining 98\\% performance on MTBench—a total reduction in alignment tax of 8 percentage points compared to standard safety alignment fine-tuning. Overall, DSA presents a promising path towards more modular, efficient, and adaptable AI safety and alignment.", "tldr": "We introduce Disentangled Safety Adapters (DSA), lightweight modules that leverage a base model's representations to significantly improve AI safety and alignment with minimal computational overhead.", "keywords": ["ResponsibleAI", "AI Safety", "Safety Guardrails", "Model Alignment", "Hallucination Detection", "Inference Efficiency", "Model Adapters", "Parameter-Efficient Fine-tuning", "Factorized Representations"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/43c92edfc7f5d569455273849a5c70a5ae431d75.pdf", "supplementary_material": "/attachment/5d52f0c77dfae7e86a64f848a62da31fded3917f.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces Disentangled Safety Adapters (DSA), a modular approach to improving LLM safety without the trade-offs of guardrails or full alignment fine-tuning. DSA uses lightweight adapters that tap into the base model’s internal representations, enabling efficient and flexible safety control at inference time. Experiments on models like Qwen, Mistral, Gemma, and Zephyr show strong results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly structured and highly readable. I think this paper effectively motivates the problem of AI safety trade-offs between guardrails and alignment fine-tuning, and communicates the key ideas in an intuitive way. \n\n1. The concept of Disentangled Safety Adapters (DSA) offers a compelling alternative to existing paradigms; that is, separate guardrail models and alignment fine-tuning. By decoupling safety-specific computations while leveraging base model representations, DSA provides a modular and efficient framework for safety interventions at inference time. This represents a potentially important step forward in designing adaptive and resource-efficient safety systems for large language models.\n\n1. The proposed method is technically solid and easy to follow. The authors provide a clear mathematical formulation for both DSA-based guardrails and alignment  and a thorough comparison to related architectures such as LoRA, Res-Tuning, and Ladder Side-Tuning. The description of targeted alignment effectively shows how context-dependent safety control can be achieved without retraining the base model.\n\n1. The empirical results are impressive and demonstrate clear superiority over baseline methods across multiple datasets and model families."}, "weaknesses": {"value": "1. The paper provides insufficient detail for reproduction. Although the appendices briefly list hyperparameters and dataset splits, the source code and trained models are not provided, and several important training settings (e.g., exact adapter dimensions, implementation details for cross-attention, initialization strategies, and data preprocessing) are only briefly summarized. Given the complexity of multi-module training (guardrail + alignment + targeted interpolation), reproducing the experiments from the text alone would be extremely difficult. The absence of public code limits the practical verification of the claimed efficiency and scalability benefits.\n\n1. The paper indirectly evaluates helpfulness via MTBench scores, showing that DSA maintains most of the model’s helpfulness while improving safety. However, there is no direct human evaluation of response quality or real-world usefulness."}, "questions": {"value": "1. Did the authors examine how the actual generated outputs change after applying DSA? If so, what qualitative characteristics or behavioral differences were observed in the model’s responses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kcF7XUZu7A", "forum": "LwzBNQUmAi", "replyto": "LwzBNQUmAi", "signatures": ["ICLR.cc/2026/Conference/Submission14180/Reviewer_Qsrg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14180/Reviewer_Qsrg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760946536468, "cdate": 1760946536468, "tmdate": 1762924638442, "mdate": 1762924638442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Disentangled Safety Adapters (DSA), a novel framework that decouples safety-specific computations from the base model by attaching lightweight adapters that reuse internal representations. DSAs are applied to both safety guardrails and safety alignment. The authors show empirical results on various benchmarks, demonstrating that DSA guardrails outperform standalone classifiers with similar cost and that targeted alignment reduces the alignment tax compared to standard LoRA fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "•⁠  ⁠Clear and Novel Concept: The DSA framework is an intuitive design that reuses the base model's computations for safety tasks, enabling low-cost, disentangled safety modules.\n•⁠  ⁠Guardrail Performance: DSA guardrails match the performance of full-sized, high-cost models while adding minimal compute overhead (<1% FLOPs). They outperform comparable low-cost models, with up to a 53% relative AUC improvement on hallucination detection.\n•⁠  ⁠Targeted Alignment Results: The method achieves safety levels comparable to standard LoRA fine-tuning while preserving ~98% of the base model's general task performance, effectively mitigating the alignment tax.\n•⁠  ⁠Architectural Analysis: The paper provides clear justification for its design choices by comparing adapter architectures and iterating on alignment adapters."}, "weaknesses": {"value": "•⁠  ⁠Novelty: While presented as a novel framework, DSA is largely an incremental adaptation of existing adapter and side-tuning ideas. The contribution lies mainly in applying these methods to safety tasks, which is valuable but not conceptually deep.\n•⁠  ⁠Weak Baselines: Comparisons rely on low-cost guardrails rather than competitive guardrails (e.g., Llama-Guard 2). DSAs also receive richer initialization and distillation, compromising fairness. Stronger baselines and matched compute would be needed to validate the claimed efficiency advantage.\n•⁠  ⁠Limited interpretability and diagnostic analysis: The paper offers no qualitative analyses, error breakdowns, or visualization of adapter activations. Without understanding what DSAs actually learn or when they fail, it is hard to assess their safety reliability or transparency."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uKrMaKkMsL", "forum": "LwzBNQUmAi", "replyto": "LwzBNQUmAi", "signatures": ["ICLR.cc/2026/Conference/Submission14180/Reviewer_rFTf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14180/Reviewer_rFTf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909956222, "cdate": 1761909956222, "tmdate": 1762924637822, "mdate": 1762924637822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on two current technical approaches for LLM safety: guardrail models and safety alignment, with an emphasis on the trade-off between development flexibility and inference efficiency. It proposes a Disentangled Safety Adapter (DSA), which enhances safety by training independent guardrail and alignment adapter modules, without modifying the base model's parameters or altering its core inference path for general tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies the crucial trade-off between development flexibility and inference efficiency in current guardrail models and safety alignment methods, particularly in LLM deployment.\n2. The DSA framework proposed in this paper unifies the training of guardrail models and safety alignment, which is an innovative approach."}, "weaknesses": {"value": "1. Lack of Generalization Verification for the DSA Method:\n\n 1.1. The experiments were conducted on only one base model, which itself has poor safety performance. It would be beneficial to see experiments on at least three different models, not just the base model, as models fine-tuned with instructions may be more prone to performance degradation after safety alignment.\n \n1.2. The paper uses only one training dataset, but the introduction mentions one of the advantages of DSA as \"updating safety policies without retraining or redeploying the base model.\" This claim requires validation with more diverse training data.\n\n2. Missing Key Baselines:\n\nThe DSA framework proposed in this paper essentially adds additional modular parameters to the model, improving safety through optimization. There is a similar prior work, “MoGU: A Framework for Enhancing Safety of Open-Sourced LLMs While Preserving Their Usability,” but no comparison is made in the experiments.\n\n3. Lack of Hyperparameter Analysis:\n\nFrom Figure 2, it appears that the DSA method is quite sensitive to hyperparameters. Whether this impacts the method's practicality needs further analysis and clarification.\n\n4. The necessary ablation analysis is lacking：\n\nThe proposed DSA Targeted Alignment framework includes two trainable modules: a guardrail module and an alignment module; however, there are no experiments directly comparing which of the two modules is more important."}, "questions": {"value": "1. In the introduction, one of the advantages of DSA is mentioned as \"updating safety policies without retraining or redeploying the base model,\" but the experiments lack additional validation for this point. I carefully reviewed the paper, and the authors only trained one model on one dataset.\n2. Table 3 primarily compares DSA with LoRA-based DPO, but in fact, DSA increases the number of parameters compared to the base model, whereas LoRA-based methods do not add parameters during inference. This seems to contradict the motivation of the paper. \n3. Additionally, LoRA's specific settings, such as hyperparameters like rank, can impact performance. It's unclear whether the authors considered this in their experiments. Why not consider full model fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8jz781PCgx", "forum": "LwzBNQUmAi", "replyto": "LwzBNQUmAi", "signatures": ["ICLR.cc/2026/Conference/Submission14180/Reviewer_yZaC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14180/Reviewer_yZaC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991374644, "cdate": 1761991374644, "tmdate": 1762924637224, "mdate": 1762924637224, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Disentangled Safety Adapters (DSA), a framework that decouples safety-specific computations from task-optimized base models using lightweight adapters. The authors propose DSA for two applications: (1) safety guardrails that classify unsafe inputs/outputs by reusing base model representations, and (2) safety alignment that steers generation by interpolating between base model and adapter logits. The key innovation is \"targeted alignment\" that dynamically adjusts alignment strength based on input safety classification. Experiments show DSA guardrails outperform comparably-sized standalone models (up to 53% AUC improvement on hallucination detection) while adding <1% computational overhead, and DSA targeted alignment reduces alignment tax by 8 percentage points compared to standard fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-motivated problem: The paper clearly articulates the trade-off between separate guardrails (flexible but expensive) and alignment training (efficient but inflexible), positioning DSA as an elegant solution combining advantages of both approaches.\n2. Good experimental evaluation:\n    - Multiple base models (Qwen, Mistral, Gemma) and scales (7B-14B)\n    - Diverse safety tasks (hate speech, unsafe prompts/responses, hallucination detection)\n    - Systematic architecture comparison (PLR → RTB → LST)\n    - Thorough ablations and multiple benchmarks\n3. Good empirical results: DSA:LST matches full-sized guardrails at <1% overhead. Consistent improvements across model families and tasks"}, "weaknesses": {"value": "1. Limited architectural novelty: The paper primarily applies existing adapter architectures (Side-Tuning, LST, Res-Tuning) to safety tasks. While the application is novel, the core technical contribution is incremental. The DSA:LST+ and DSA:LLD variants introduced for alignment are relatively minor modifications.\n\n2. Binary switching mechanism: Using a step function ($\\lambda \\in \\{0,1\\}$) is simplistic. The paper mentions smooth interpolation as future work, but this seems essential for robustness.\n\n3. DSA:LST and DSA:LST+ significantly underperform LoRA baseline (Figure 2). And I don't agree the statement that\n```\nThis method (Fig. 2, \"LORA\") incurs no additional inference cost but entangles safety with\nbase model computations, precluding flexible alignment strength adjustment without running both\naligned and unaligned models\n```\nActually, LoRA can be only used in the last layer and most computaions can be used. \n\n4. Limited baseline comparisons: No comparison with other efficient guardrail methods. Is there any other methods for similar tasks? For example, a good and simple baseline could be just prompt engineering: Prompting LLM itself to output whether the input/output is safe or not, and most computations could be reused due to prompt KV caching mechanism."}, "questions": {"value": "1. What is the actual latency overhead in your experiments and in production settings with batching?\n2. Can adapters trained for one safety policy be efficiently adapted to new policies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WtJpP7AF6o", "forum": "LwzBNQUmAi", "replyto": "LwzBNQUmAi", "signatures": ["ICLR.cc/2026/Conference/Submission14180/Reviewer_7YFw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14180/Reviewer_7YFw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005395629, "cdate": 1762005395629, "tmdate": 1762924636590, "mdate": 1762924636590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}