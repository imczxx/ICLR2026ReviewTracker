{"id": "kbxjkoF42x", "number": 25509, "cdate": 1758368735846, "mdate": 1763736941382, "content": {"title": "Ensemble Learning for AUC Maximization via Surrogate Loss", "abstract": "In classification tasks, the area under the ROC curve (AUC) is a key metric for evaluating a model’s ability to discriminate between positive and negative samples. An AUC-maximizing classifier can have significant advantages in cases where ranking correctness is valued or when the outcome is rare. While ensemble learning is a common strategy to improve predictive performance by combining multiple base models, direct AUC maximization for aggregating base learners leads to an NP-hard optimization challenge. To address this challenge, we propose a novel stacking framework that leverages a linear combination of base models through a surrogate loss function designed to maximize AUC. Our approach learns data-driven stacking weights for base models by minimizing a pairwise loss-based objective. Theoretically, we prove that the resulting ensemble is asymptotically optimal with respect to AUC. Moreover, when the set of base models includes correctly specified models, our method asymptotically concentrates all weight on these models, ensuring consistency. In numerical simulations, the proposed method reduces the AUC risk by up to 20\\% compared to existing ensemble methods, a finding that is corroborated by real-data analysis, which also shows a reduction of over 30\\%.", "tldr": "This paper proposes a novel stacking framework that linearly combines base models via a surrogate loss function designed to maximize AUC. The resulting ensemble is asymptotically optimal and its effectiveness is verified by empirical studies.", "keywords": ["AUC Maximization", "Ensemble Learning", "Machine Learning", "Binary Classification", "Surrogate Loss", "Asymptotic Optimality"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9ee967d96af6a9efac51d353960d45a8a838da18.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors proposed an optimization framework named Ensemble Learning method for AUC Maximization **(ELAM)**. To be specific, ELAM aims to learn the optimal weight (mixture weight) of an ensemble of models so that the \"surrogate\" AUC is maximized. Here, surrogate means replacing the indicator function in the definition of AUC with some continuous approximations such as quadratic, exponential, or logistic functions. The base models in the ensemble are learned in a cross-validated manner.\n\nIn terms of theory, under technical assumptions, the authors show the consistency of the learned mixture weight in terms of:\n\n- Theorem 1: Asymptotically optimal with respect to the surrogate risk objective.\n- Theorem 2: Asymptotically optimal with respect to the original AUC.\n- Theorem 3: Convergence with probability one to the optimal solution.\n\nNumerical experiments show improved AUC compared to baselines like BIC, AIC, or other stacking methods such as AUCW and simple averaging."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Directly maximizing AUC is meaningful and practically relevant, particularly in scenarios where the data are imbalanced.  \n- The paper is generally well written."}, "weaknesses": {"value": "- The paper has limited novelty. The idea of replacing the indicator function by smoother ones should be very standard."}, "questions": {"value": "- The authors used a handful number of assumptions: 5 assumptions to derive Theorem 1; 9 assumptions to derive Theorem 2; and 5 assumptions to derive Theorem 3. Are these assumptions easy to verify in practice?\n\n- I wonder how the method stands against other methods outside the ensemble family, i.e., those that maximize the surrogate AUC and are not necessarily given as a weighted sum of pretrained models.\n\n- The authors mentioned that Assumption 4 is stronger than those in the literature because they allow weights to be upper-bounded by $C$ instead of 1. In the case where the upper bound is exactly $1$, can Assumption 4 be alleviated to milder assumptions as in the literature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9ENriLmrpG", "forum": "kbxjkoF42x", "replyto": "kbxjkoF42x", "signatures": ["ICLR.cc/2026/Conference/Submission25509/Reviewer_zy6d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25509/Reviewer_zy6d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25509/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761580377758, "cdate": 1761580377758, "tmdate": 1762943457174, "mdate": 1762943457174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "mp8jHc4eel", "forum": "kbxjkoF42x", "replyto": "kbxjkoF42x", "signatures": ["ICLR.cc/2026/Conference/Submission25509/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25509/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763736939880, "cdate": 1763736939880, "tmdate": 1763736939880, "mdate": 1763736939880, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an ensemble method named ELAM, designed to directly optimize the AUC via a surrogate loss. The core idea is to use K-fold cross-validation to generate out-of-sample predictions for each base learner and then learn an optimal linear combination by minimizing a differentiable pairwise surrogate loss (e.g., logistic loss) under box constraints. Experiments show that this method has certain effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The approach is straightforward: generate out-of-sample predictions via cross-validation and learn combination weights by minimizing a pairwise surrogate loss. It can be easily applied to any collection of base models without architecture modification.\n2. The paper presents a comprehensive asymptotic analysis, proving surrogate risk and AUC risk optimality as well as consistency of the learned weights."}, "weaknesses": {"value": "1. The theoretical results rely on several restrictive and technical assumptions (e.g., bounded gradients, specific convergence rates, and conditions on $\\xi_n$ and $M$). These assumptions may not hold in high-dimensional or deep-learning settings. The paper would benefit from intuitive explanations of these assumptions and examples of when they are satisfied or violated.\n2. Experiments are conducted only on small-to-medium UCI datasets with logistic regression as base learners. No comparison is made with modern deep AUC optimization methods or large-scale ensemble approaches.\n3. The robustness of ELAM with respect to class imbalance or hyperparameter variation is unclear."}, "questions": {"value": "1. Several assumptions (e.g., Assumptions 4 and 9) restrict the growth of M relative to n. Could the authors provide intuition or numerical examples showing these are realistic in practice?\n2. Could the authors compare ELAM with recent deep AUC optimization methods [1,2,3,4]?\n\n[1] LibAUC: A Deep Learning Library for X-risk Optimization\n\n[2] Algorithmic Foundation of Empirical X-risk Minimization\n\n[3] Learning with Multiclass AUC: Theory and Algorithms\n\n[4] AUCSeg: AUC-oriented Pixel-level Long-tail Semantic Segmentation"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CMmYFTXB9f", "forum": "kbxjkoF42x", "replyto": "kbxjkoF42x", "signatures": ["ICLR.cc/2026/Conference/Submission25509/Reviewer_V8g7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25509/Reviewer_V8g7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25509/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791577897, "cdate": 1761791577897, "tmdate": 1762943456679, "mdate": 1762943456679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ELAM (Ensemble Learning for AUC Maximization), a stacking-based ensemble method that directly targets AUC maximization through a surrogate loss.\nDirect optimization of AUC over ensemble weights is NP-hard, so the authors propose to replace the non-differentiable AUC indicator with a smooth, convex surrogate trained using K-fold cross-validation to obtain unbiased out-of-sample predictions from base learners.\nTheoretical results establish asymptotic optimality under both the surrogate risk and the true AUC risk, as well as weight consistency, showing that the method asymptotically concentrates weights on correctly specified base models.\nEmpirical results on real-world and synthetic datasets suggest that ELAM achieves modest but consistent improvements in AUC compared to existing ensemble and model averaging baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is generally well-written and clearly structured. The problem is well-motivated, and the proposed method (ELAM) is technically sound, though largely builds on existing surrogate loss ideas for AUC optimization. The theoretical analysis is thorough, with detailed proofs that support the main claims. Overall, the presentation is clear, and the technical development is sounds."}, "weaknesses": {"value": "- Limited Novelty.\nReplacing the AUC indicator with a convex surrogate and training weights via cross-validation is well-known in AUC optimization (Gao & Zhou 2015; LeDell et al. 2016). The contribution largely builds on these ideas within a stacking setup, adding theoretical justification but limited methodological novelty.\n- Restrictive Setting and Scalability Concerns.\nThe method is essentially a linear stacking with logistic surrogate loss; it scales poorly with the number of base models and samples due to quadratic pairwise terms."}, "questions": {"value": "How does the proposed surrogate differ fundamentally from prior logistic/exponential AUC-consistent surrogates used in ranking optimization?\nWhat is the computational complexity of optimizing the pairwise surrogate (Eq. 11) in terms of n and M? Can it scale to large datasets?\nHow sensitive are results to the choice of surrogate loss (logistic vs. exponential) and the number of folds K in cross-validation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xQNbO3JfdN", "forum": "kbxjkoF42x", "replyto": "kbxjkoF42x", "signatures": ["ICLR.cc/2026/Conference/Submission25509/Reviewer_8GWi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25509/Reviewer_8GWi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25509/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922312979, "cdate": 1761922312979, "tmdate": 1762943456440, "mdate": 1762943456440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ELAM, an ensemble learning framework for maximizing the AUC for binary classification problems by combining base classifiers through a stacking approach. ELAM formulates the ensemble weighting problem as minimizing a surrogate pairwise loss based on K-fold cross-validation predictions to approximate direct AUC optimization. The authors provide theoretical guarantees demonstrating asymptotic optimality of the ensemble in surrogate and true AUC risk."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Theoretical Guarantees: The paper rigorously proves that the stacking weights computed by minimizing the surrogate loss converge asymptotically to the optimal solution in terms of both surrogate and actual AUC risk."}, "weaknesses": {"value": "1. The novelty of the proposed method is questionable. A learning scheme is a standard stacking method with k-fold cross-validation. Authors should refer to previous literature in Section 3, e.g., “Using Stacking Approaches for Machine Learning Models” by Bohdan Pavlyshenko. A surrogate loss is adopted from (Gao & Zhou, 2015). So what is the novelty of the method?\n\n2. In the introduction, authors state: “We address the NP-hard challenge of direct AUC optimization”. But no statement about the particular algorithmic problem and its NP-hardness do not follow after that.\n\n3. line 161: R_{\\phi} is not defined."}, "questions": {"value": "1. When you say “the NP-hard nature of direct AUC optimization”, what do you mean specifically? When you make such a statement, I expect either a reference to previous literature or at least a particular formulation of an algorithmic problem that is NP-hard. Real-valued optimization problems in equations (5), (6) do not belong to a class of discrete problems which could be NP-hard or not NP-hard.\n\nNote that exponentially complex ALGORITHMS and NP-hard PROBLEMS  are different concepts; please do not conflate them. There are previous papers with similar claims like “One-Pass AUC Optimization” by Gao et al., however, without any specification (“Direct optimization of AUC often leads to an NP-hard problem as it can be cast into a combinatorial optimization problem”). What is the formal input data for the problem you name NP-hard?\n\n2. line 161: R_{\\phi} is not defined."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ll3lPE7FxL", "forum": "kbxjkoF42x", "replyto": "kbxjkoF42x", "signatures": ["ICLR.cc/2026/Conference/Submission25509/Reviewer_KsJN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25509/Reviewer_KsJN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25509/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016285667, "cdate": 1762016285667, "tmdate": 1762943456006, "mdate": 1762943456006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}