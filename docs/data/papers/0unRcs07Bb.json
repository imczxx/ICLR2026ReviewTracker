{"id": "0unRcs07Bb", "number": 3657, "cdate": 1757492358506, "mdate": 1763119955005, "content": {"title": "MoRe4D: Joint 3D Motion Generation and Geometry Reconstruction for 4D Synthesis from a Single Image", "abstract": "Generating interactive, dynamic 4D scenes from *a single static image* remains a core challenge. Most existing *generate-then-reconstruct* and *reconstruct-then-generate* methods decouple geometry from motion, causing spatiotemporal inconsistencies and poor generalization. To overcome these limitations, we extend the reconstruct-then-generate framework to jointly couple \\textbf{Mo}tion generation with geometric **Re**construction for **4D** Synthesis (**MoRe4D**). We first introduce TrajScene-60K, a large-scale dataset of 60,000 video samples with dense point trajectories, addressing the scarcity of high-quality 4D scene data. Based on this, we propose a diffusion-based 4D Scene Trajectory Generator (4D-STraG) to jointly generate geometrically consistent and motion-plausible 4D point trajectories. To leverage single-view priors, we design a depth-guided motion normalization strategy and a motion-aware module in 4D-STraG for effective geometry and dynamics integration. We then propose a 4D View Synthesis Module (4D-ViSM) to render videos with arbitrary camera trajectories from 4D point track representations. Extensive experiments show that MoRe4D generates high-quality 4D scenes with multi-view consistency and rich dynamic details from a single image.", "tldr": "", "keywords": ["4D generation", "dense point track", "video diffusion model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/f1244e8de9cae40089d0ce267a8f99f1795cd6fa.pdf", "supplementary_material": "/attachment/439fff180865f22f31d59f923a8ee3171dc4bb76.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes MoRe4D, a \"reconstruct-then-generate\" framework that jointly couples 3D geometry with motion for single-image to 4D scene synthesis. The paper puts forth 3 main contributions:\n(1) TrajScene-60K, a curated dataset of ~60k videos with dense 4D point trajectories derived from WebVid using VLM-based filtering, monocular depth, and tracking.\n(2) 4D-STraG, a diffusion/flow-matching model that predicts relative, depth-normalized point motions from a single image, using a Motion Perception Module.\n(3) 4D-ViSM, a video diffusion inpainting stage to render novel-view videos from the generated 4D point trajectories, filling holes where projections are sparse.\n\nExperiments emphasize qualitative results and VBench scores, with ablations for depth-guided motion normalization and the MPM."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to follow. The roles of 4D-STraG vs. 4D-ViSM are well separated and explained.\n- The proposed dataset seems highly beneficial for 3D/4D fields, as well as motion tracking fields."}, "weaknesses": {"value": "- The proposed pipeline seems to be a cleverly integrated vision system that combines many different pretrained models. It is unclear what the contributions here are other than smart system integration. It is also unclear if any of this transfers if we use other pretrained priors.\n- Evaluation does not target true 4D consistency/geometry. Given the paper’s central claim (joint geometry–motion), this is a mismatch: the method should be measured on whether the generated 4D is geometrically coherent across novel views and time. With TrajScene-60K in hand (pseudo-GT tracks and depths), it’s feasible to report 2D endpoint error (EPE) of tracks, occlusion accuracy, 3D Chamfer/EPE3D against pseudo-GT or self-consistency checks across rendered views. There are also other synthetic datasets that can be employed to measure this.\n- The paper does not report on established dynamic NVS/4D datasets (even with caveats), nor does it propose a public evaluation split for TrajScene-60K with released annotations to allow reproducible comparisons. Without public release/eval code, the benchmark story is weak.\n- Baseline evaluations and experiment design is very confusing. Different models are benchmarked in different ways, without a clear explanation on why these evaluations make sense. To me it seems like the authors are cherry-picking evaluations.\n- Ablations are qualitative only. Not even VBench evaluations are included.\n\nMinor points:\n- Feature-wise conditioning reminds me of FiLM/AdaIN/T2I-Adapter style conditioning. They should be credited.\n- Depth-scaled normalization: commonly used in geometric vision models and scale-invariant depth or flow models. Should also be cited."}, "questions": {"value": "In additional to the weakness section above:\n- How often does 4D-ViSM “hallucinate” content outside the projected points, and how does that affect multi-view consistency over time?  - What are the dominant failure modes?\n- How sensitive is performance to depth noise?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns"]}, "details_of_ethics_concerns": {"value": "The dataset relies on WebVid plus monocular depth/tracking and LLM/VLM filters, but the paper doesn’t clarify the bias induced by LLM/VLM filtering."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n4X4Gztr3p", "forum": "0unRcs07Bb", "replyto": "0unRcs07Bb", "signatures": ["ICLR.cc/2026/Conference/Submission3657/Reviewer_KDft"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3657/Reviewer_KDft"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857192575, "cdate": 1761857192575, "tmdate": 1762916902815, "mdate": 1762916902815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "tHfhQLzD28", "forum": "0unRcs07Bb", "replyto": "0unRcs07Bb", "signatures": ["ICLR.cc/2026/Conference/Submission3657/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3657/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763119952801, "cdate": 1763119952801, "tmdate": 1763119952801, "mdate": 1763119952801, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MoRe4D, a novel framework for joint 3D motion generation and geometry reconstruction to synthesize a dynamic 4D scene from a single static image. The core innovation is to move beyond the traditional decoupled \"generate-then-reconstruct\" (GtR) or \"reconstruct-then-generate\" (RtG) pipelines, which often lead to spatiotemporal inconsistencies. MoRe4D introduces a 4D Scene Trajectory Generator (4D-STraG), which is a spatiotemporal diffusion model (finetuned from Wan 2.1) trained to predict a dense 4D point trajectory ($\\Delta P_t$) coupled with an initial 3D point cloud ($P_0$). To achieve this geometric awareness, the method incorporates a Depth-Guided Motion Normalization and a Motion Perception Module (MPM) with MAdaNorm to inject motion-aware features and enforce consistency. The method demonstrates superior performance in generating coherent 4D dynamic point clouds and views, validated on a newly introduced, large-scale TrajScene-60K dataset."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The introduction of the TrajScene-60K dataset, a large-scale collection of 4D trajectories coupled with 3D geometry, is a valuable contribution that will facilitate future research in this domain.\n\t2. generated dynamic scenes and resulting dynamic point clouds exhibit high fidelity, fine-grained details, and superior spatiotemporal consistency compared to baseline methods, particularly those relying on sequential reconstruction."}, "weaknesses": {"value": "1. The image-to-motion generation ability comes from Wan-I2V. What is the difference with the pipeline that first generating a video then conducting 4D reconstruction with methods like TTT3R or MASt3R? This pipeline could also get a dynamic point cloud as output, expressing the unnecessary need to do the complex joint trajectory generation proposed by the authors.\n\n2. Regarding MPM, integrating an external feature extractor (OmniMAE) with a conventional conditional injection mechanism has few genuinely novel technical contributions and could be considered a relatively simple way to inject conditional information into the diffusion transformer architecture.\n\n3. Difference with 4DNeX [1]: The paper needs to clearly articulate the technical superiority of its joint motion/geometry prediction architecture (MoRe4D) compared to other feed-forward models like 4DNeX, which also fine-tunes a pretrained video diffusion model to predict a unified 6D (RGB+XYZ) representation from a single image. The claimed benefit of coupling motion and geometry must be empirically shown to surpass other unified, feed-forward approaches.\n\n4. (minor, misclassification) In L148, 4dfy first conforms the \"reconstruct -then-generate\" instead of the previous one paradigm. It first generate canonical NeRF with SDS. Instead, SV4D [2], EG4D [3] conforms the \"reconstruct -then-generate\" scheme.\n\n\n[1] 4DNeX: Feed-Forward 4D Generative Modeling Made Easy\n\n[2] SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View Consistency, ICLR 2025\n\n[3] EG4D: Explicit Generation of 4D Object without Score Distillation, ICLR 2025"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A4xAasEnkd", "forum": "0unRcs07Bb", "replyto": "0unRcs07Bb", "signatures": ["ICLR.cc/2026/Conference/Submission3657/Reviewer_LyB9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3657/Reviewer_LyB9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989289208, "cdate": 1761989289208, "tmdate": 1762916902207, "mdate": 1762916902207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose to generate geometry and motion jointly for 4D synthesis from a single image. Specifically, the authors first construct a large 4D dataset with 4D annotations from a pre-trained 3D point trajectory method. Then, they finetune a video diffusion model to predict point cloud sequence conditioned on the input image and depth. They also train a 4D view synthesis module to predict video from the projection of the generated point cloud sequence. Extensive experiments on public and private dataset demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1)The motivation of jointly predicting geometry and motion makes sense.\n\n2)The paper conduct extensive experiments.\n\n3)Generated results seems great."}, "weaknesses": {"value": "1) Lack of evaluation for 4D consistency: it is suggested to reconstruct the 4D scene using nerf or gaussian to better evaluate the 4D consistency of the proposed method as well as other methods\n\n2) The authors are suggested to validate their joint prediction design by comparing with video gen from Wan2.1 + DELTA point tracking + 4D-ViSM result (as a generation-reconstruction method, close to the paper setting), since they use DELTA for video annotation\n\n3) It is recommended to incorporate point trajectories in conjunction with point cloud rendering in 4D ViSM"}, "questions": {"value": "See weaknessnes"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mxfAhPZ9ax", "forum": "0unRcs07Bb", "replyto": "0unRcs07Bb", "signatures": ["ICLR.cc/2026/Conference/Submission3657/Reviewer_nPQj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3657/Reviewer_nPQj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010643885, "cdate": 1762010643885, "tmdate": 1762916900388, "mdate": 1762916900388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a  4D scene, i.e., temporally dynamic 3D scene, synthesis method, which adopts reconstruct-then-generate strategy.  It first generates 4D point trajectories conditioned on input image and text, and then renders the 4D point trajectories with 3D Gaussian splatting as the initial video.  Finally, the initial video is used as the guidance for the final video generation. Since  this method reconstructs 4D point trajectories as the underlying representation of the 4D scene, it supports the editing of camera trajectories during video generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  The 4D point trajectory representation is novel, and it is effective to serve as an intermediate representation to improve geometric consistency in video generation.\n2.  This paper constructs TrajScene-60K, with 60,000 video samples with dense point trajectories. This dataset is valuable to the video processing research. \n3.  the point trajectory normalization step is effective to facilitate the training of VAE for 4D point trajectories."}, "weaknesses": {"value": "The generated video is relatively short, and in its current form, the proposed method would likely struggle to handle the appearance or disappearance of objects mid-sequence."}, "questions": {"value": "Suppose a video sequence recorded for a man takes out his mobile phone from his pocket at mid-sequence, but the mobile phone is not visible in the first frame because it is still in the pocket,  how do you construct the 4D point trajectories for this sequence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FMZCRVcGST", "forum": "0unRcs07Bb", "replyto": "0unRcs07Bb", "signatures": ["ICLR.cc/2026/Conference/Submission3657/Reviewer_CNBJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3657/Reviewer_CNBJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762019992484, "cdate": 1762019992484, "tmdate": 1762916900010, "mdate": 1762916900010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}