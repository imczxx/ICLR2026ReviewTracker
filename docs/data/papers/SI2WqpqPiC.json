{"id": "SI2WqpqPiC", "number": 18603, "cdate": 1758289415094, "mdate": 1759897092783, "content": {"title": "Group-Wise Optimization for Self-Extensible Codebooks in Vector Quantized Models", "abstract": "Vector Quantized Variational Autoencoders (VQ-VAEs) leverage self-supervised learning through reconstruction tasks to represent continuous vectors using the closest vectors in a codebook. However, issues such as codebook collapse persist in the VQ model. To address these issues, existing approaches employ implicit static codebooks or jointly optimize the entire codebook, but these methods constrain the codebook's learning capability, leading to reduced reconstruction quality. In this paper, we propose Group-VQ, which performs group-wise optimization on the codebook. Each group is optimized independently, with joint optimization performed within groups. This approach improves the trade-off between codebook utilization and reconstruction performance. Additionally, we introduce a training-free codebook resampling method, allowing post-training adjustment of the codebook size. In image reconstruction experiments under various settings, Group-VQ demonstrates improved performance on reconstruction metrics. And the post-training codebook sampling method achieves the desired flexibility in adjusting the codebook size.", "tldr": "", "keywords": ["VQ-VAE", "codebook"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc77fc84b5e40e290134ff8e078d6df780014921.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "VQ-VAEs often suffer from codebook collapse, and \"joint VQ-VAEs\" have been proposed to improve the utilization of code vectors. The authors point out that increased codebook usage does not necessarily result in better reconstruction performance compared to vanilla VQ-VAEs. To address this, they propose a group-wise optimization of the codebook to balance codebook utilization and reconstruction quality. VQ-VAEs trained with this framework achieve better reconstruction performance than previous approaches and enable resampling and self-extension of the codebook."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Bridging the gap between vanilla VQ-VAE and current joint VQ-VAEs offers an interesting perspective.\n- The concept of enabling codebook upsampling is novel and intriguing."}, "weaknesses": {"value": "- The most critical weakness is that the proposed method is not validated on any generation tasks. Application to generative tasks is important, as this is a primary use case for VQ-VAE in the vision domain (although VQ-VAE is also actively used as a neural codec in the audio domain).\n- While Figure 2 is interesting problem posing, motivation of proposing Group-VQ is not so clear. It is not so straightfoward for me to think generalizing vanilla VQ and joint VQ with the concept of group is helpful.\n- In Table 4, the improvement in reconstruction quality by varying the number of groups is marginal.\n- Although self-extension and upsampling of the codebook are interesting, there is no comparison with results from Group-VQ trained from scratch with a doubled codebook size.\n- The benefit of codebook resampling is unclear. Additionally, readers may wonder whether reconstruction performance improves if a fixed codebook core is used during training and inference.\n- Figure 4 has no error bar despite that the resampling and self-extension involves random seeds.\n- The section for related work lacks some works for \"Improvement to the codebook\" that is based on such as [1,2].\n\n[1] Takida et al., ``SQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization,'' ICML2022.\n\n[2] Vuong et al., ``Vector Quantized Wasserstein Auto-Encoder,'' ICML2023.\n\n**Minor**\n- In Line 212, $b_0$ should be $b_j$?\n- Visualizing similarities between projections for different groups, in addition to Figures 5 and 6, would be helpful."}, "questions": {"value": "- Why do the trends of rFID and PSNR with respect to codebook size differ in Figure 4? I did not understand the interpretation of these results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Potential ethic concerns are discussed in a section."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RlvJmurXYJ", "forum": "SI2WqpqPiC", "replyto": "SI2WqpqPiC", "signatures": ["ICLR.cc/2026/Conference/Submission18603/Reviewer_oSUk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18603/Reviewer_oSUk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760792411503, "cdate": 1760792411503, "tmdate": 1762928319323, "mdate": 1762928319323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "They partition the VQ codebook into disjoint groups and reparameterize each group with its own projector/bias, so codes update jointly within a group but independently across groups (“Group-VQ”). Claimed benefit: balance high utilization with better reconstruction quality than “Joint VQ” (one shared projector for all codes) or “Vanilla VQ” (each code updated alone). They also propose a post-training “resampling/self-extension” trick to change codebook size without retraining."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1.Simple, modular design that interpolates between Vanilla and Joint VQ; the gradient-isolation argument for group-wise updates is clear\n2.Practical knob (group count) with a reported sweet spot and utilization dynamics."}, "weaknesses": {"value": "1. The paper frames Group-VQ as a middle ground between k=1 Joint VQ and k=n Vanilla VQ, but the core mechanism is “multiple independent shared-parameter blocks.” This is very close to simply sharding SimVQ/VQGAN-LC into independent sub-projectors—i.e., architecture factorization rather than a fundamentally new training principle. The paper needs a sharper theoretical or empirical claim that grouping yields qualitatively new optimization behavior beyond trivial partitioning.\n2.Results show linear projectors outperform MLP projectors at same codebook size, but the study is brief and could reflect under-tuned MLPs or capacity/regularization mismatches. Without stronger controls (same params/FLOPs; weight decay; norm; init), it’s hard to attribute gains to “grouping” vs. “simpler projector works better here.”\n3. While convenient, newly sampled codes aren’t trained; authors note quality eventually degrades when adding too many. This seems like a brittle knob unless coupled with light finetuning or usage-aware pruning. The paper should test a short finetune vs. pure resampling."}, "questions": {"value": "1. If you fix total projector capacity (params/FLOPs), does grouping still win over a single global projector?\n2. What happens if groups are data-driven (e.g., k-means over code usage) instead of uniform ranges? Any routing bias?\n3. Does a brief post-resampling finetune (e.g., 1–5 epochs) recover the degradation when doubling/quadrupling codebook size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OofYBLuFbW", "forum": "SI2WqpqPiC", "replyto": "SI2WqpqPiC", "signatures": ["ICLR.cc/2026/Conference/Submission18603/Reviewer_wmzt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18603/Reviewer_wmzt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761039797440, "cdate": 1761039797440, "tmdate": 1762928318787, "mdate": 1762928318787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Group-VQ, a variant of VQ-VAE models designed to prevent codebook collapse and improve reconstruction performance.\nThe authors first show that JointVQ, a previous technique that addresses codebook collapse by using shared learnable parameters to reparameterize the entire codebook, has limitations in reconstruction performance.\nThey conjecture that sharing parameters across the entire codebook may lead to interference between code updates.\nBased on this observation, Group-VQ divides the codebook into multiple independent groups, each optimized jointly within the group but independently across groups.\nThe paper also introduces a training-free codebook resampling technique, which allows flexible adjustment of codebook size after training.\nExperiments on image reconstruction tasks demonstrate that Group-VQ outperforms existing methods, and that codebook resampling/self-extension is effective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and the proposed method is well-motivated.\n2. The experimental evaluation is comprehensive, including careful comparisons to strong baselines, extensive ablation studies on the number of groups and projector types, as well as detailed visualizations and analyses of the learned codebook."}, "weaknesses": {"value": "1. The main idea, to my understanding, is dividing the codebook of SimVQ [Zhu et al., 2025] into groups and optimizing each group independently, and it feels relatively incremental. Grouping parameters in neural networks is a common strategy, as seen in grouped convolution [Krizhevsky et al., 2012], group normalization [Wu & He, 2018], and multi-head attention [Vaswani et al., 2017]. Thus, the novelty and impact of the work seem limited. This paper would benefit from a more explicit discussion of the unique advantages of Group-VQ over prior work, a clearer positioning with respect to existing literature, and the inclusion of theoretical guarantees.\n\n2. While Section 4.1 shows that codebook expansion via self-extension/self-expansion is possible, it is unclear how effective the resulting codebook is compared to retraining. Results comparing against retraining Group-VQ and SimVQ or other codebook expansion methods would strengthen the claims.\n\nMinor comment:\n* The terms \"self-expansion\" and \"self-extension\" are used inconsistently throughout the paper. It would be better to unify the terminology for clarity (e.g., Figure 4 caption vs. Section 3.3).\n\n\n[Zhu et al., 2025] Yongxin Zhu, et al. \"Addressing representation collapse in vector quantized models with one linear layer.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025.\n\n[Krizhevsky et al., 2012] Alex Krizhevsky, et al. \"Imagenet classification with deep convolutional neural networks.\" Advances in neural information processing systems 25. 2012.\n\n[Wu & He, 2018] Yuxin Wu and Kaiming He. \"Group normalization.\" Proceedings of the European conference on computer vision (ECCV). 2018.\n\n[Vaswani et al., 2017] Ashish Vaswani, et al. \"Attention is all you need.\" Advances in neural information processing systems 30. 2017."}, "questions": {"value": "1. Regarding Weakness 1: Could you provide further discussion about the unique advantages that Group-VQ offers over existing VQ approaches? In particular, how does Group-VQ fundamentally differ from these prior works, and are there specific scenarios where it provides clear benefits?\n2. Regarding Weakness2: Do you have any comparisons between the proposed training-free codebook resampling techniques and other methods or retraining?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CYa7tje5i3", "forum": "SI2WqpqPiC", "replyto": "SI2WqpqPiC", "signatures": ["ICLR.cc/2026/Conference/Submission18603/Reviewer_ScTN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18603/Reviewer_ScTN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895805650, "cdate": 1761895805650, "tmdate": 1762928318360, "mdate": 1762928318360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Group-VQ, an extension of Vector Quantized Variational Autoencoders (VQ-VAEs) that aims to mitigate the common problem of codebook collapse. Instead of using a single jointly optimized or static codebook, the authors propose a group-wise optimization strategy where the codebook is divided into multiple groups optimized independently but with intra-group coordination. This design seeks to enhance codebook utilization while maintaining high reconstruction quality. The paper also presents a training-free codebook resampling technique, enabling flexible post-training adjustment of codebook size. Experimental results on various image reconstruction tasks show that Group-VQ achieves improved reconstruction metrics and demonstrates adaptability through its resampling mechanism."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel Codebook Optimization Strategy:\nThe proposed group-wise optimization introduces a fresh perspective on improving codebook utilization in VQ-VAEs, effectively addressing the long-standing issue of codebook collapse.\n\n2. Training-Free Flexibility:\nThe post-training codebook resampling technique is innovative and practical—it allows dynamic adjustment of codebook size without retraining, improving adaptability to different computational or performance requirements.\n\n3. Potential Broad Applicability:\nThe framework could be easily extended to other domains (e.g., speech, video, or representation learning tasks) where quantized latent representations are useful."}, "weaknesses": {"value": "1. The study is primarily inspired by the well-established VQ-VAE framework, and the authors introduce a new variant of the variational autoencoder. However, the absence of image generation results is a notable limitation of the paper.\n\n2. The proposed approach shares conceptual similarities with rq-vae [1] and SC-VAE [2], as both employ multiple atoms or vector representations. Demonstrating improvements solely on image reconstruction tasks is insufficient; additional downstream evaluations are necessary to better highlight the advantages and generalizability of the proposed model.\n\n[1] Lee, Doyup, et al. \"Autoregressive image generation using residual quantization.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n\n[2] Xiao, Pan, et al. \"SC-VAE: Sparse coding-based variational autoencoder with learned ISTA.\" Pattern Recognition 161 (2025): 111187."}, "questions": {"value": "Could you show image generation results? I am open to improve the score if the work demonstrates better image generation results compared to other vq-vae based models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8dxUh7ccwV", "forum": "SI2WqpqPiC", "replyto": "SI2WqpqPiC", "signatures": ["ICLR.cc/2026/Conference/Submission18603/Reviewer_Pu5M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18603/Reviewer_Pu5M"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958747949, "cdate": 1761958747949, "tmdate": 1762928317978, "mdate": 1762928317978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}