{"id": "wK9rTqYY0e", "number": 3915, "cdate": 1757567524886, "mdate": 1759898062788, "content": {"title": "Second-Order Bounds for [0,1]-Valued Regression via Betting Loss", "abstract": "We consider the $[0,1]$-valued regression problem in the i.i.d. setting. In a related problem called cost-sensitive classification, Foster and Krishnamurthy (2021) have shown that the log loss minimizer achieves an improved generalization bound compared to that of the squared loss minimizer in the sense that the bound scales with the cost of the best classifier, which can be arbitrarily small depending on the problem at hand. Such a result is often called a first-order bound. For $[0,1]$-valued regression, we first show that the log loss minimizer leads to a similar first-order bound. We then ask if there exists a loss function that achieves a variance-dependent bound (also known as a second-order bound), which is a strict improvement upon first-order bounds. We answer this question in the affirmative by proposing a novel loss function called the betting loss. Our result is “variance-adaptive” in the sense that the bound is attained without any knowledge about the variance, which is in contrast to modeling label (or reward) variance or the label distribution itself explicitly as part of the function class, such as distributional reinforcement learning.", "tldr": "", "keywords": ["Regression", "Second-Order Bounds"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bbdf8ff94d86c9df4e93faf38902354c26ea54ec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a theoretical study on generalisation error bounds for [0, 1]-valued regression in the stochastic setting. The main contribution is a “second-order” bound for a proposed novel loss function. First, a first-order bound is shown for log-loss. This bound scales with a term that upperbounds the conditional variance. Next, a novel betting loss is proposed, which has two hyperparameters $\\bar{\\phi}$ and $c$. A second-order bound is shown for this loss, with the bound depending on the conditional variance $\\sigma_x$. This result is initially shown for a finite hypothesis class and then extended to a broader family of functions defined as “parametric” class. Computational experiments have been performed on a synthetic dataset. Improved generalisation using betting loss has been shown in comparison to the squared and logit loss. The experiments were conducted with varying levels of conditional variance in the data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "*) A novel loss function, referred to as betting loss, is proposed for the regression problem. \n\n*) The minimiser of this loss guarantees a second-order generalisation error bound. The second-orderness is implied by dependence on conditional variance. The minimiser can be attained without knowledge of the conditional variance\n\n*) Second-order bounds have been established for the finite hypothesis class case and a subclass of the infinite hypothesis class, specifically the parametric class. A lack of such bounds for the non-parametric class is conjectured, and a supporting theory for it is detailed in the appendix. \n\n*) Experimental validation of improved generalisation using betting loss is done using a synthetic dataset. Experiments are shown with varying levels of conditional variance. The experiments are repeated multiple times, and the standard errors of the performances are reported.\n\n*)The related works have been discussed in an organised manner, with the distinction of the current work highlighted."}, "weaknesses": {"value": "*) The betting loss function has two parameters \\bar{\\phi} and c. There is a lack of discussion on the effect of these parameters.\n\n*) More discussion of this loss function within the broader class of loss functions will help readers.\n\n*) The minimiser of the betting loss cannot be accurately computed because of $\\max_h \\max_\\phi \\max_c$. As done in the experiments, it requires performing a discretised search. This increases computations.  \n\n*) No experiments on a real-world dataset."}, "questions": {"value": "*) Can a discussion be provided on the parameters $\\phi$ and $c$?\n\n*) Can some experiments be shown on a real-world dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yIXMbGGkKR", "forum": "wK9rTqYY0e", "replyto": "wK9rTqYY0e", "signatures": ["ICLR.cc/2026/Conference/Submission3915/Reviewer_YX2n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3915/Reviewer_YX2n"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723732104, "cdate": 1761723732104, "tmdate": 1762917096958, "mdate": 1762917096958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper gives a strong generalization bound for the basic task of $[0, 1]$-valued regression (or more generally settings where the target variable is a bounded real value). The main feature of these bounds is that they scale naturally with the intrinsic conditional variance (or noise level) $Var(y|x)$ of the target variable, so that the bound on the test loss of the ERM estimator grows tighter as the noise level decreases. Such a bound is described as a \"variance-adaptive\" or \"second-order\" generalization bound. This is similar in spirit to known \"first-order\" generalization bounds (aka \"optimistic rates\") that scale favorably with the optimal achievable loss (and variants thereof). Such bounds are stronger than naive uniform convergence bounds, which do not in general adapt to the optimal loss or variance at all. \n\nThe main contribution of the paper is a new method that achieves such a second-order guarantee for the $L^1$ loss by minimizing a surrogate \"betting loss\". The theoretical results are also supported by empirical experiments on synthetic data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem studied by this paper, namely giving variance-adaptive / second-order generalization bounds, is a nice theoretical problem in statistical learning theory. The method proposed (minimizing a surrogate betting loss) seems appealingly simple and practical to use. The paper is generally written in a clear way."}, "weaknesses": {"value": "The main weakness of the paper is that it does not seem to be aware of / engage with a large body of closely related prior work in generalization theory for supervised learning. This prior work is usually associated with keywords such as \"optimistic rates\", \"optimal / localized generalization bounds\", \"benign overfitting\", and \"Moreau envelope theory\", among others. The authors may claim that these works focus on so-called \"first-order\" bounds, whereas they are concerned with \"second-order\" bounds. But I would strongly contest this distinction in the setting considered in this paper, which is the _realizable_ setting (i.e. there exists $f^{\\*}$ in the class such that $\\\\mathbb{E}[y|x] = f^{\\*}(x)$). When you assume realizability, there is no longer an important distinction between \"first-order\" and \"second-order\" generalization bounds, because the expected conditional variance $\\\\mathbb{E}[\\sigma_x^2]$ is precisely the best achievable square loss $\\\\min_{f \\in F} \\\\mathbb{E}[(f(x) - y)^2] =: L_{sq}(f^{*})$. The latter is ostensibly a \"first-order\" quantity. Accordingly, it is not at all clear that the bounds in this paper are truly novel in light of existing theory.\n\nIt is true that the exact statement of the theorem differs in certain minor ways from typical statements of optimal generalization bounds (specifically, the LHS has $\\\\mathbb{E}[|f(x) - f^{*}(x)|]$ as opposed to the naive $L^1$ loss $\\\\mathbb{E}[|y - f(x)|]$, and the RHS has the optimal $L^2$ loss or variance). It is also possible that the exact distributional / model assumptions are a bit different. I am not necessarily claiming that the main theorem is a trivial corollary of existing bounds (I have not carefully checked this either way). But the main point is that the paper's contributions need to be carefully contextualized and compared against this existing work to be considered novel / significant.\n\nHere are references to some particularly relevant related works:\n\nOptimistic rates:\n- Nathan Srebro, Karthik Sridharan, and Ambuj Tewari (2010). “Optimistic Rates for Learning with a Smooth Loss.”\n  - The main theorem in this reference is very similar to the one considered in this paper, and may very well imply it with some additional work.\n- Lijia Zhou, Frederic Koehler, Danica J. Sutherland, and Nathan Srebro (2021). “Optimistic Rates:\nA Unifying Theory for Interpolation Learning and Regularization in Linear Regression.”\n- Lijia Zhou, Frederic Koehler, Pragya Sur, Danica J. Sutherland, and Nathan Srebro (2022). “A\nNon-Asymptotic Moreau Envelope Theory for High-Dimensional Generalized Linear Models.”\n  - Please see the discussion of related work within this reference for a discussion of other related papers\n  - [This Simons talk](https://www.youtube.com/watch?v=h1TGvwxRSd8) by Frederic Koehler discusses some of the results in this line of work\n\nOlder work on localized generalization bounds:\n- Bartlett, Bousquet, Mendelson (2005). \"Local Rademacher Complexities.\"\n- Koltchinskii (2006). \"Local Rademacher complexities and oracle inequalities in risk minimization.\""}, "questions": {"value": "As discussed above, my recommendation to the authors would be to include a much more substantive discussion of and comparison with the prior work referenced above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SzOASiVAbi", "forum": "wK9rTqYY0e", "replyto": "wK9rTqYY0e", "signatures": ["ICLR.cc/2026/Conference/Submission3915/Reviewer_ZQA9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3915/Reviewer_ZQA9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877878027, "cdate": 1761877878027, "tmdate": 1762917096749, "mdate": 1762917096749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies regression problems where the target value is always between 0 and 1. Standard methods like squared loss aren't always the best fit for this specific case. While using log loss is an improvement and provides what's known as a \"first-order bound\"—meaning its performance guarantee is linked to the magnitude of the values being predicted—it still doesn't fully leverage the data's underlying characteristics, particularly when the amount of noise or uncertainty isn't uniform across all data points.\nTo address this, the authors propose a novel approach inspired by betting losses. The main result is that by minimizing this betting loss, their algorithm achieves a \"second-order bound\". Thus, the method's performance guarantee is tied to the actual variance in the data ($\\sigma_x^2$), which can be much smaller than what first-order bounds depend on.\nInterestingly, the algorithm is variance-adaptive; it achieves these tighter, more accurate results without needing any prior information about the noise levels in the data. The authors back up their theory with experiments, showing that their betting loss method consistently results in a lower mean absolute error (MAE) when compared to both traditional squared loss and the improved log loss, especially when the data has low variance."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors proved improved bounds (in certain regimes) for a fundamental problem in learning theory.\n\n- The results seem to be following in a non-trivial way."}, "weaknesses": {"value": "- I found the intuition behind the betting loss function a bit unclear.\n\n- The result requires that the label space $y$ is bounded in $[0,1]$."}, "questions": {"value": "- Can you provide some more intuition behind the betting function?\n\n- Can you say anything about unbounded losses? Relatedly, how does the bound scale with an upper bound on $y$ (different than 1)?\n\n- One advantage that your result has is that it scales nicely with the noise of the label; for instance, assume that $y = f^*(x)$ w/ prob 1. Then, the error bound is $O(1/n)$ and increases smoothly as the noise in the label $y$ increases. Is my interpretation correct?\n\n- Can you say how close to being optimal your bound is?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3FZWgJFtjt", "forum": "wK9rTqYY0e", "replyto": "wK9rTqYY0e", "signatures": ["ICLR.cc/2026/Conference/Submission3915/Reviewer_yM3S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3915/Reviewer_yM3S"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762076368786, "cdate": 1762076368786, "tmdate": 1762917096478, "mdate": 1762917096478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers the problem of $[0,1]$-valued regression, where the goal is to learn a function $f^\\star \\in \\mathcal{F} \\subset \\{f: \\mathcal{X} \\rightarrow [0, 1]\\}$ that predicts $\\mathbb{E}_x [y \\mid x]$, under realizable setting. The authors argue that the standard squared loss is insensitive to variance, while the log loss (CE) can only achieve a first-order bound (scale with the variance proxy $f^*(x)(1 - f^*(x))$). The latter can be loose in some cases.\n\nThe above is also the motivation of this paper, which is trying to define a new loss for this problem that can actually achieve a second-order bound, scaling with the true conditional variance $\\sigma_x^2 = \\mathbb{E}_{y \\mid x}[(y - f^*(x)^2]$. The authors give an affirmative answer by using the betting loss, which is well-known in the hypothesis testing framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clear and easy to follow."}, "weaknesses": {"value": "Honestly, this paper gives me a hard time evaluating it fairly. On one hand, the __theoretical__ finding of this paper, though niche, is interesting enough. The proof technique is simple and natural, nothing to write home about. On the other hand, one might argue that the result only makes sense theoretically, and in practice, it is absolutely intractable due to the multi-level (not even bi-level) optimization nature. As a result, the experiments are very simple and only for demonstration purposes (the function class $\\mathcal{F}$ is finite and contains only 21 hypotheses, the inner $\\max_{\\phi, c}$ cannot even be solved analytically, and has to use a grid-search). However, I would not consider it a weakness due to the theoretical nature of the results - the proposed loss is naturally intractable! \n\n\nGiven the above, I can only have lukewarm support for this paper with a low-confidence score. I leave the decision for the AC and other reviewers to evaluate the significance and appropriateness of this paper in a conference like ICLR. That being said, if I review this paper in AISTATS, I will definitely give it a better score with high confidence, and I truly believe that this draft should be submitted to AISTATS or ALT instead. But don't be disappointed! This draft is in good shape, and maybe other reviewers and AC will value this draft differently!"}, "questions": {"value": "No further questions. The problem setting is clear, and the result is well-presented. I only consider the experiments as add-ons, and in my view, even no experiment is acceptable for a paper with theoretical implications only. The proof technique is natural and sound - I checked it and found no issues."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "Tn5YUlDoYb", "forum": "wK9rTqYY0e", "replyto": "wK9rTqYY0e", "signatures": ["ICLR.cc/2026/Conference/Submission3915/Reviewer_7NdS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3915/Reviewer_7NdS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762322382807, "cdate": 1762322382807, "tmdate": 1762917096217, "mdate": 1762917096217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}