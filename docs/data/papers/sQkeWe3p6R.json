{"id": "sQkeWe3p6R", "number": 11093, "cdate": 1758189012457, "mdate": 1759897609270, "content": {"title": "NO DARK DATA REQUIRED: BRIDGING THE GAP BETWEEN NORMAL AND LOW-LIGHT DETECTION VIA RETINEX DECOMPOSITION", "abstract": "Conventional low-light object detection approaches typically involve distinct image enhancement modules before the detection process. This can lead to compromised performance due to misaligned objectives and reduced robustness in challenging visual contexts. Many existing methodologies either do not optimize both tasks jointly or overlook significant latent features that are essential for accurate detection. To address this issue, a novel end-to-end framework was proposed that was exclusively trained on normal-light images, eliminating the need for low-light data during the training phase. This approach drew inspiration from the Retinex theory, which separated images into reflectance (representing scene structure) and illumination (indicating lighting conditions). The proposed framework approximates this decomposition within the feature space. The architecture utilises deep multi-scale feature aggregation along with a reflectance-guided fusion pathway, enabling the adaptive integration of illumination-aware representations through element-wise modulation. Despite being trained on normal-light images, the framework demonstrates effective generalisation to low-light and visibility compromised environments. Comprehensive experiments conducted on both synthetic datasets (Pascal VOC) and real-world benchmarks (ExDark, RTTS) indicate that this method achieves enhanced detection accuracy and robustness, particularly in adverse lighting conditions, and outperforms current state-of-the-art techniques.", "tldr": "We propose a robust Retinex-guided framework that learns illumination-aware features for accurate object detection under low-light and foggy conditions.", "keywords": ["Low-light images", "Adversarial Visibility Condition", "Retinex theory", "Image Processing."], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0a9a7ef13880415e52d531591b7bb4a1ca81dcc9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents an end-to-end object detection architecture inspired by Retinex theory. The design features a decomposition module that estimates reflectance and illumination in feature space, with a novel fusion approach to produce illumination-invariant features for object detection. The key distinguishing property is that the model is trained solely on normal-light data but tested robustly on both synthetic and real low-light/foggy datasets. The method is benchmarked on Pascal VOC and ExDark and RTTS."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The model is trained exclusively on normal-light images which is practically significant to reduce data curation burdens.\n2. The decomposition of deep features into reflectance and illumination via Retinex-inspired, feature-level processing seems a distinctive integration."}, "weaknesses": {"value": "1. The paper is difficult to read due to a combination of disorganized content and poor layout. The logical progression of ideas is unclear, and the unprofessional typesetting, evidenced by significant blank space on page 3, detracts from the work's credibility.\n2. The mathematical descriptions in Section 3.2–3.3 (especially around the decomposition and fusion) are rather high level. Crucial details such as the explicit forms of the aggregation $\\mathcal{A}(\\cdot)$, sampling method for constructing $L(x, y)$ and $R(x, y)$, channel alignment techniques, normalization procedures, and whether the element-wise fusion is normalized or bounded, are unspecified.\n3. The paper does not adequately differentiate its proposed methodology from existing Retinex decomposition/fusion techniques.\n - Deep Retinex Decomposition for Low-Light Enhancement, BMVC18\n - IniRetinex: Rethinking Retinex-type Low-Light Image Enhancer via Initialization Perspective, AAAI25"}, "questions": {"value": "1. Can the authors provide explicit mathematical details for the aggregation and fusion steps, particularly a precise functional form for $\\mathcal{A}$ and the normalization/activation used in $F_{i}^{\\text{fused}}$? Are there learned or fixed weights, or dynamic selection mechanisms in fusion?\n2. What is the process and rationale for selecting RepNCSPELAN4 blocks, and do alternative feature processing blocks (e.g., C2f, ELAN) materially affect performance? Quantitative ablation here would strengthen the architectural justification.\n3. What are the failure points under extreme conditions (e.g., mAP vs. fog/darkness level)? Is there a critical threshold below which the proposed method degrades significantly earlier or later than the SOTA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PExx4lVRGa", "forum": "sQkeWe3p6R", "replyto": "sQkeWe3p6R", "signatures": ["ICLR.cc/2026/Conference/Submission11093/Reviewer_8714"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11093/Reviewer_8714"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923867160, "cdate": 1761923867160, "tmdate": 1762922270461, "mdate": 1762922270461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript attempts to introduce Retinex theory into the YOLO framework, but in reality, it is merely a patchwork of pre-existing concepts and methods."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The title of the manuscript is decent."}, "weaknesses": {"value": "1. There are significant formatting problems, including large blank spaces on several pages (e.g., pages 3, 4, and 6) and improperly scaled tables (e.g., Tables 1, 2, and 3).\n2. The proposed method is an unjustified assembly of classic methods from the vision field (YOLO and Retinex) with virtually no original design contributions.\n3. The contributions summarized in the introduction are all based on existing methods, lacking any original design from the authors, and are poorly written. The author describes the method as an \"AI model.\" The core of the work is merely applying different processing to feature maps of different scales within YOLO and claiming this constitutes a Retinex decomposition. This claim is unsubstantiated, and the author fails to provide a clear explanation, instead just restating concepts from Retinex theory and YOLO object detection.\n4. The author's writing suggests a lack of familiarity with standard academic terminology in this field. For instance, summarizing their method as \"an Artificial Intelligence (AI) solution\" is not a phrasing I have encountered in computer vision or related fields.\n5. The experiments in this manuscript are primarily compared against baseline and older methods, failing to include comparisons with the latest state-of-the-art (SOTA) approaches. Furthermore, only limited results are presented, which is insufficient to validate the effectiveness of the proposed method."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UPtNvrvIHB", "forum": "sQkeWe3p6R", "replyto": "sQkeWe3p6R", "signatures": ["ICLR.cc/2026/Conference/Submission11093/Reviewer_u2EL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11093/Reviewer_u2EL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977226192, "cdate": 1761977226192, "tmdate": 1762922269893, "mdate": 1762922269893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript proposes a new end-to-end framework based on normal-light images for low-light image detection. The proposed method separates images into reflectance and illumination, which approximates this decomposition within the feature space. A multi-scale feature aggregation is introduced to learn illumination-aware representation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Compared to multiple baselines, the method exhibits enhanced generalization under challenging lighting and weather conditions.\n2. The framework attains high inference speed, enabling effective support for real-time applications."}, "weaknesses": {"value": "1. The work as a whole lacks a distinct innovative core. Its technical approach largely manifests as a direct integration of the YOLO model with Retinex theory, without presenting substantial original theoretical advancements, nor conducting in-depth exploratory research on key technical bottlenecks. \n2. The introduction fails to provide an explicit and structured summary of the study’s contributions. In academic writing, a clear statement of contributions serves as a \"guide\" for readers to quickly identify the work’s core value and differences from prior studies. \n3. The paper’s formatting is extremely poor. This lack of rigor in the submission attitude has raised doubts about the quality of the paper’s content and the authenticity of its experiments"}, "questions": {"value": "Poor readability of the paper\nPoor formatting of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HxJAYMESX0", "forum": "sQkeWe3p6R", "replyto": "sQkeWe3p6R", "signatures": ["ICLR.cc/2026/Conference/Submission11093/Reviewer_TjUq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11093/Reviewer_TjUq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980582381, "cdate": 1761980582381, "tmdate": 1762922269325, "mdate": 1762922269325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}