{"id": "AJMgU3Rspx", "number": 16482, "cdate": 1758265020218, "mdate": 1759897237961, "content": {"title": "SpeechQC-Agent: A Natural Language Driven Multi-Agent System for Speech Dataset Quality", "abstract": "Ensuring the quality of large-scale datasets is a prerequisite for reliable machine learning, yet current verification pipelines are static, domain-specific, and heavily reliant on human experts. We introduce **SpeechQC-Agent**, the first natural language-driven agentic framework for dataset quality control that generalizes across modalities, vendors, and languages. A central planner LLM decomposes user queries into directed acyclic graph (DAG) workflows executed by modular sub-agents that combine reusable tools with LLM-synthesized functions, enabling flexible and scalable verification. Unlike rule-based scripts, this design supports parallelism, dependency management, and adaptive extension to novel schemas. To benchmark verification systems, we release **SpeechQC-Dataset**, a multilingual speech corpus with controlled perturbations spanning audio, transcripts, and metadata, allowing systematic evaluation of 24 verification tasks. Experiments show that SpeechQC-Agent achieves 80-90\\% of expert level accuracy while operating at less than 20\\% of cost and time and generalizes from synthetic perturbations to real vendor-supplied corpora. Comparative analysis across multiple planner LLMs highlights trade-offs between fidelity (GPT-4.1-mini), efficiency (LLaMA-3.3-70B), and reasoning strength (DeepSeek-R1). Beyond speech, our approach establishes a general paradigm for LLM-driven workflow generation in dataset quality assurance, with implications for the curation of multimodal and multilingual resources on scale.", "tldr": "SpeechQC-Agent is a natural language-driven multi-agent system that transforms user instructions into structured validation workflows for scalable and automated speech dataset quality checks.", "keywords": ["low resource", "multi agent", "large language model", "automatic speech recognition"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0084fc72860fd6942d58e690a862d130a7d87965.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "SpeechQC-Agent proposes a multi-agent LLM system that converts natural language quality control requirements into executable workflows for speech dataset validation. The system uses DAG-based task decomposition, combines LLM-synthesized and pre-defined tools, and evaluates 5 LLM planners across 24 verification tasks. While addressing a genuine practical problem, the paper suffers from significant methodological weaknesses and insufficient experimental rigor that prevent acceptance at ICLR in its current form."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Addresses Real Problem with Clear Motivation: Speech dataset quality control is genuinely expensive and time-consuming.\n2. Novel Application Domain for Multi-Agent Systems: This appears to be the first integrated system combining natural language-driven workflow generation with multi-agent orchestration specifically for speech dataset quality control. \n3. New Benchmark Contribution: SpeechQC-Dataset"}, "weaknesses": {"value": "1. Synthetic Benchmark Validity (Critical): Controlled perturbations ≠ real-world complexity. Prior research shows synthetic benchmarks often underestimate task difficulty and yield rankings inconsistent with real-world evaluations. The authors should validate their data by training speech processing systems on it and comparing performance with systems trained on real-world data.\n\n2. Missing Baseline: No human expert baseline is provided. Where are multiple expert annotations with inter-rater agreement (e.g., Cohen’s kappa, Krippendorff’s alpha)? How is “expert-level” defined? There’s also no comparison with existing tools or ablations against single-agent LLM or simple prompt-engineering baselines.\n\n3. Limited Applicability: Evaluation is restricted to a single language (Hindi) and only 15 hours of generated data—less than the amount available in existing benchmarks (e.g., OpenSLR 118 https://www.openslr.org/118/).\n\n4. Limited Novelty: The contribution is primarily system integration of existing components rather than any substantive algorithmic innovation."}, "questions": {"value": "Check weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v1nf5YEUCm", "forum": "AJMgU3Rspx", "replyto": "AJMgU3Rspx", "signatures": ["ICLR.cc/2026/Conference/Submission16482/Reviewer_bkFa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16482/Reviewer_bkFa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761712743101, "cdate": 1761712743101, "tmdate": 1762926584774, "mdate": 1762926584774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SpeechQC-Agent, a natural language-driven multi-agent system for large-scale speech dataset quality control. It addresses the limitations of static, domain-specific QC pipelines by leveraging a LLM planner to decompose user queries into dynamic directed acyclic graph workflows. The system utilizes modular sub-agents that execute specific QC tasks and features an iterative, LLM-guided task selection and refinement loop to ensure completeness. From a multimodal perspective, the framework implicitly performs a high-level fusion of control and domain-specific tools, allowing human intent to govern the orchestrated execution of modality-specific analysis tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1) The concept of a natural language-driven, generalized, multi-agent framework for data QC can be employed for reducing the dependence of human experts.\n\nS2) Employing modular sub-agents and a DAG execution structure facilitates easy incorporation of new QC tools and enables generalization across different modalities, vendors, and languages.\n\nS3) The system demonstrates a successful fusion of the LLM's symbolic reasoning/planning modality with the computational execution modality."}, "weaknesses": {"value": "W1) This study is vague on how the outputs of the various sub-agents are ultimately fused and synthesized into a final QC report. Are they merely appended sequentially, or is there a subsequent LLM step for semantic fusion/summary?\n\nW2) Depending on the LLM for up to three iterations of task refinement introduces significant latency and computational cost. This can hinder adoption for real-time or massive-scale QC applications.\n\nW3) The structure seems focused on workflow. Explicit discussion of how fine-grained modality analysis is integrated beyond simple ASR or WER tasks is missing."}, "questions": {"value": "Q1) How is the final QC result generated from the multiple sub-agent outputs? Is there a dedicated Fusion Agent responsible for synthesizing disparate metrics into a single, cohesive human-readable quality score or report?\n\nQ2) Could the authors elaborate on which of the 24 predefined tasks specifically fall under fine-grained modality analysis versus simple data-level computation?\n\nQ3) In a scenario where two different sub-agents produce conflicting results, how does the LLM planner or the execution loop manage this conflict before finalizing the DAG output?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W6VhPbBdcX", "forum": "AJMgU3Rspx", "replyto": "AJMgU3Rspx", "signatures": ["ICLR.cc/2026/Conference/Submission16482/Reviewer_aioD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16482/Reviewer_aioD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756126941, "cdate": 1761756126941, "tmdate": 1762926584442, "mdate": 1762926584442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SpeechQA-Agent, which is an agent system that intends to conduct speech data quality verification automatically. \n\nThe agent would first understand the user's intention, making it into an action list. The action list is then transformed into a graph and conducted in parallel. \n\nIn experiments, the authors report the success rate of the design with varying LLMs. The authors claim the proposed agent can improve work efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The reviewer appreciates the authors' intention to design an agent to conduct speech data verification. It also proposes a theoretically feasible workflow for such an intention."}, "weaknesses": {"value": "I would appreciate it if the reviewers could respond to the following concerns:\n\n(1) It is claimed \"SpeechQC-Agent achieves 80-90% of expert-level accuracy while operating at less than 20% of the cost and time\" in the abstract, but I cannot find the support material in the main content. Section 5.3 also mentioned \"Human Annotation baseline\", which cannot be found in Tables 2,3,4,5. The paper also doesn't compare with any prior agent baseline methods, or the previous \"static, domain-specific, and heavily reliant on human experts\" method.\n\n(2) Although Table 1 mentions many different check items, Tables 2 and 3 only report these items selectively. \n\n(3) There is considerable duplication between Section 3 and Section 5.1\n\n(4) For the proposed SpeechQC-dataset, many details are missing. E.g., although the data could contain various flaws (which the agent intends to detect), there are no statistics about the distribution of each error category. The whole data simulation pipeline leverages many tools that could contain error propagation, such as a multilingual LLM for low-resource languages and TTS. These errors should be properly handled and analyzed when building a benchmark (as this benchmark is used to justify the effectiveness of the proposed agent)\n\n(5) Tables 4 and 5 compare various LLM planning capabilities. Although this is important to an agent, such capability is intrinsic to the LLMs and is not a contribution to this work. Instead, the author claims that the parallel planning (together with the graph workflow) is a major benefit of this agent, but didn't provide experimental support for it."}, "questions": {"value": "As in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U8Yi7z9TcQ", "forum": "AJMgU3Rspx", "replyto": "AJMgU3Rspx", "signatures": ["ICLR.cc/2026/Conference/Submission16482/Reviewer_ouXL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16482/Reviewer_ouXL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978929151, "cdate": 1761978929151, "tmdate": 1762926583896, "mdate": 1762926583896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SpeechQC-agent, a natural language driven agent for speech dataset quality control. Given a natural language user input, the agent produces code that checks the quality of paired speech data, such as audio length, sample rate, normalization, and domain classification. The authors test the agent using SpeechQC-Dataset, a synthetic benchmark developed using LLM-driven dialogue generation, translation, and synthesized into speech via TTS. Results suggest that SpeechQC-agent can achieve strong verification accuracy while being more efficient than human annotation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- This paper presents a novel method for speech dataset qc using LLM agents\n- The authors evaluated different ways to implement agent-based speech qc, showing the weakness of single-agent systems"}, "weaknesses": {"value": "- Parts of the paper are difficult to understand. Figure 3 is too dense to be easily parse and its difficult to capture what the numbers in the results tables are measuring / intended to convey. Minor formatting issues such as the inconsistent use of sig figs in each row.\n- I am not convinced about the usefulness of the proposed method. While a general speech QC agent would indeed be useful for open-ended tasks (like coherence evaluation) , it seems excessive to employ one to simply check the sample rate of an audio file. Considering how dense some of these prompts are for extremely simple programs (like `Sample Rate Check Prompt` and `Audio Length Calculation Prompt`), why rely on an agentic framework that must re-write the code every time and not just call a pre-written function? Most of the evaluated tasks are not open ended and can be done with static pre-written functions. The results in Table 2 even indicate that the agents often fail to even perform these operations. Using the agent to call pre-written functions should perform just as well while being more efficient."}, "questions": {"value": "- For a paper that focuses primarily on low-resource languages, it is quite surprising that there is no mention of this aspect in the title and abstract, making the intro seem very disjoint from the above elements.\n- Similarly, why not just stress-test verification systems directly on English? All the synthetic dialogues are originally generated in English anyways. Using LLMs to translate to low-resource languages (and then synthesizing into speech with TTS) increases the risk of hallucination, which risks making the shown results less reliable.\n- Missing citation in line 82 after \"planning\"?\n- In Table 4, what are QC1-1, 1-2, and 1-3? Why only evaluate on 9 audio samples total if the benchmark set has thousands of examples?\n- I do not understand what the authors are trying to show in Table 3 and the caption is unhelpful. Is it measuring the accuracy of the model in detecting Roman characters, WER of the synthetic audio (does 0.12 mean 12% WER here or 0.12%?), and if the counted number of HTML tags is correct?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XxYOhXQyf6", "forum": "AJMgU3Rspx", "replyto": "AJMgU3Rspx", "signatures": ["ICLR.cc/2026/Conference/Submission16482/Reviewer_FF7f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16482/Reviewer_FF7f"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762123682403, "cdate": 1762123682403, "tmdate": 1762926583583, "mdate": 1762926583583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}