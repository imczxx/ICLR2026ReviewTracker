{"id": "kyzHM557gE", "number": 4498, "cdate": 1757690210849, "mdate": 1759898029708, "content": {"title": "Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning", "abstract": "Any entity in the visual world can be hierarchically grouped based on shared characteristics and mapped to fine-grained sub-categories. While Multi-modal Large Language Models (MLLMs) achieve strong performance on coarse-grained visual tasks, they often struggle with Fine-Grained Visual Recognition (FGVR). Adapting general-purpose MLLMs to FGVR typically requires large amounts of annotated data, which is costly to obtain, leaving a substantial performance gap compared to contrastive CLIP models dedicated for discriminative tasks. Moreover, MLLMs tend to overfit to seen sub-categories and generalize poorly to unseen ones. To address these challenges, we propose Fine-R1, an MLLM tailored for FGVR through an R1-style training framework: (1) Chain-of-Thought Supervised Fine-tuning, where we construct a high-quality FGVR CoT dataset with rationales of \"visual analysis, candidate sub-categories, comparison, and  prediction”, transition the model into a strong open-world classifier; and (2) Triplet Augmented Policy Optimization, where Intra-class Augmentation mixes trajectories from anchor and positive images within the same category to improve robustness to intra-class variance, while Inter-class Augmentation  maximizes the response distinction conditioned on images across sub-categories to enhance discriminative ability. With only 4-shot training, Fine-R1 outperforms existing general MLLMs, reasoning MLLMs, and even contrastive CLIP models in identifying both seen and unseen sub-categories, showing promise in working in knowledge-intensive domains where gathering expert annotations for all sub-categories is arduous.", "tldr": "", "keywords": ["Multimodal Large Language Model", "Fine-Grained Visual Recognition", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d05142352cef5afa192c0043fb42237c5629dce9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Fine-R1, a multi-modal large model (MLLM) designed to excel at fine-grained visual recognition (FGVR). The authors propose a two-stage framework, starting with Chain-of-Thought Supervised Fine-tuning (CoT SFT) to teach the model structured reasoning, followed by Triplet Augmented Policy Optimization (TAPO). TAPO is an RL algorithm based on DAPO that uses anchor, positive, and negative image triplets to handle the FGVR challenges of high intra-class and low inter-class variance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper's primary strength lies in its impressive empirical results. Fine-R1 achieves state-of-the-art performance on six FGVR datasets, outperforming general MLLMs, reasoning-focused MLLMs, and even strong contrastive CLIP models. The model shows particularly strong generalization to unseen categories, which is a key challenge in FGVR. The analysis (hypotheses H1-H3) provides an insightful conclusion that the gains come from an improved ability to deploy existing knowledge, rather than learning new features or knowledge."}, "weaknesses": {"value": "Weakness\n* Limited Novelty of TAPO: The core algorithmic contribution, TAPO, does not appear to be a novel RL algorithm. It feels like a forced \"splicing\" of positive ($x^{pos}$) and negative ($x^{neg}$) sampling techniques onto an existing baseline (DAPO/GRPO). The use of a $D_{\\text{KL}}$ loss for the $x^{neg}$ sample is a common regularization technique in standard (non-RL) fine-grained classification, which calls into question its novelty as a policy optimization method.\n* Unclear Ablation: The paper fails to clearly disentangle the individual contributions of the $x^{pos}$ and $x^{neg}$ components. It is unclear if the $x^{pos}$ (hybrid rollouts) provides any significant benefit on its own. The ablation study is missing crucial comparisons (e.g., Baseline + $x^{pos}$ only, Baseline + $x^{neg}$ only) and also lacks an analysis of the $n_1:n_2$ ratio (anchor vs. positive rollouts)."}, "questions": {"value": "If the authors can clearly address the following points during the rebuttal, I am open to reconsidering my score:\n1. Can you further justify the novelty of TAPO as an RL algorithm, distinguishing it from simply applying a known FGC regularizer ($x^{neg}$) and a data augmentation strategy ($x^{pos}$) to a DAPO baseline?\n2. Can you provide a decoupled ablation study that shows the individual performance contributions of $x^{pos}$ (Intra-class Augmentation) and $x^{neg}$ (Inter-class Augmentation)? I am particularly interested in the (Baseline + $x^{pos}$ only) result.\n3. Please provide an ablation study on the ratio of anchor-to-positive rollouts ($n_1$ vs $n_2$)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no concerns"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mptHnFhkoT", "forum": "kyzHM557gE", "replyto": "kyzHM557gE", "signatures": ["ICLR.cc/2026/Conference/Submission4498/Reviewer_ENRW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4498/Reviewer_ENRW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4498/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540360810, "cdate": 1761540360810, "tmdate": 1762917402407, "mdate": 1762917402407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Fine-R1, a multimodal large language model (MLLM) framework for fine-grained visual recognition (FGVR). It introduces a two-stage R1-style training pipeline: (1) Chain-of-Thought Supervised Fine-Tuning (CoT-SFT), where the model learns structured reasoning steps from synthesized CoT data; and (2) Triplet Augmented Policy Optimization (TAPO), a reinforcement-learning method that augments intra- and inter-class examples (anchor, positive, negative) to improve robustness and discrimination. Fine-R1 achieves superior accuracy on six FGVR benchmarks, outperforming both general and reasoning MLLMs (e.g., Qwen2.5-VL, DeepPerception) and even contrastive CLIP models, especially in few-shot and unseen-category settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Clear motivation**: The paper clearly articulates the limitations of existing MLLMs in fine-grained visual recognition and motivates the need for improved reasoning and generalization capabilities.\n\n**Methodological novelty**: The proposed TAPO combines reinforcement learning with triplet-based augmentation, conceptually bridging contrastive and policy-optimization paradigms.\n\n**Strong empirical results**: Fine-R1 consistently surpasses prior MLLMs and CLIP baselines across multiple FGVR datasets and evaluation settings (closed/open-world, seen/unseen).\n\n**Well-structured analyses**: The ablation studies and hypothesis testing (H1–H3) are thoughtful, showing that Fine-R1’s gains stem from better knowledge deployment rather than merely improved visual features or memorization."}, "weaknesses": {"value": "**Limited data scale for CoT-SFT**: The CoT dataset reportedly contains only 404 samples, raising doubts about generalization and potential overfitting to synthetic patterns.\n\n**Evaluation bias toward Qwen-based baselines**: All base models are Qwen-VL variants; cross-model validation (e.g., on LLaVA or InternVL foundations) is missing, which might limit claims of generality.\n\n**Complexity vs. gain**: TAPO adds considerable training and sampling overhead (triplet construction, multi-rollout reward computation), but the improvement over DAPO (+1.6%) is relatively modest.\n\n**Conceptual overlap**: While well-positioned as an R1-style method, the framework’s connection to previous RL-based reasoning systems (e.g., Visual-RFT, VLM-R1) could be made more precise to clarify incremental novelty.\n\n**Interpretability of CoT generation**: The reasoning chains are auto-synthesized by another MLLM (Qwen2.5-VL-32B), but no human verification or quality metrics are provided, leaving uncertainty about rationale faithfulness."}, "questions": {"value": "**1. Data efficiency and generalization:**\nThe CoT-SFT dataset contains only 404 samples. Could the authors elaborate on how they ensure generalization beyond this limited synthetic set? For instance, were any experiments conducted to test scaling behavior when using larger or more diverse CoT data?\n\n**2. On cross-model validation:**\nSince all base models are Qwen-VL variants, have the authors attempted to reproduce the results on alternative architectures (e.g., LLaVA or InternVL) to confirm that the proposed TAPO framework generalizes across backbones?\n\n**3. On training efficiency and computational cost:**\nTAPO introduces triplet sampling and additional rollouts. Could the authors quantify the training-time or GPU-hour overhead compared to DAPO or standard GRPO?\n\n**4. On incremental novelty and relation to prior work:**\nThe paper positions Fine-R1 as an R1-style framework. Could the authors clarify the specific conceptual or algorithmic distinctions from prior RL-based reasoning methods such as Visual-RFT, Vision-R1, or VLM-R1? What unique design choices make TAPO fundamentally different rather than a variant?\n\n**5. On CoT faithfulness and quality control:**\nThe CoT rationales are generated automatically by Qwen2.5-VL-32B. Did the authors evaluate their correctness or consistency? How sensitive is model performance to potential noise in these synthesized CoTs?\n\nI will adjust my score based on the authors’ response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k4H690ZBvU", "forum": "kyzHM557gE", "replyto": "kyzHM557gE", "signatures": ["ICLR.cc/2026/Conference/Submission4498/Reviewer_8sko"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4498/Reviewer_8sko"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4498/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723327269, "cdate": 1761723327269, "tmdate": 1762917402168, "mdate": 1762917402168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets FGVR by proposing a two-stage approach to improve MLLMs. The first stage, CoT SFT, uses supervised fine-tuning with a structured Chain-of-Thought to teach the model an interpretable, fine-grained reasoning process. The second stage, TAPO, employs a triplet-augmented policy optimization to sharpen the model's ability to distinguish between highly similar classes. In a 4-shot, base-to-new setting across six FGVR datasets, the method reports significant gains over both general-purpose MLLMs and strong contrastive models like SigLIP."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1、t's widely acknowledged that general MLLMs underperform contrastive models like CLIP/SigLIP on fine-grained tasks. The paper's attempt to close this gap using structured CoT and reinforcement learning, rather than relying solely on massive labeled datasets, is a practical and appealing direction.\n\n2、The two-stage approach is well-motivated. The CoT SFT stage provides an interpretable reasoning framework (\"visual analysis → candidate subclasses → comparison → prediction\"), while the triplet-based policy optimization (TAPO) directly targets the core challenge of FGVR: maximizing inter-class variance while minimizing intra-class variance.\n\n3、The experiments are extensive, covering both closed-set and open-set scenarios. The use of multiple evaluation metrics (including semantic similarity) and thorough ablations helps to clearly identify the sources of performance improvement.\nWeaknesses & Suggestions"}, "weaknesses": {"value": "1、The method feels like an application of existing CoT and RL techniques, not a new paradigm for FGVR. A direct comparison against a generic CoT prompt is needed to prove the proposed reasoning structure is truly beneficial. \n\n2、Using a SigLIP encoder to calculate a key metric while also comparing against SigLIP is a potential conflict. The results should be cross-verified with another encoder (like CLIP's) to ensure fairness. \n\n3、The reported gains are marginal and lack error bars, making them unconvincing given the high variance of RL and CoT methods. \n\n4、It's unclear if the gains come from the reasoning structure or just from generating longer text, a known confounder. The paper needs length-controlled experiments to prove its central claim. Weak Baselines. \n\n5、The CLIP/SigLIP baselines seem under-tuned, as they lack standard optimizations like prompt ensembling."}, "questions": {"value": "1、Novelty of CoT Application Needs Clarification. The core idea is to combine structured CoT with policy optimization. However, using CoT to enhance reasoning is already a well-explored area in LLMs. The paper needs to more clearly articulate the fundamental difference between its \"CoT SFT\" and existing work on few-shot CoT prompting or standard CoT-based supervised fine-tuning.\n\n2、Details on Semantic Similarity Metric are Lacking. The paper relies on a SigLIP text encoder to calculate semantic similarity, which is a key metric. However, it needs to provide more details on the threshold selection, the metric's sensitivity to different class granularities, and the potential impact of text normalization or synonyms.\n\n3、Clarity on Acronyms. Several new acronyms (\"CoT SFT,\" \"No-Thinking-RL,\" \"TAPO,\" etc.) should be defined with their full names upon first use to improve readability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "aWmC2ahVyz", "forum": "kyzHM557gE", "replyto": "kyzHM557gE", "signatures": ["ICLR.cc/2026/Conference/Submission4498/Reviewer_Ace4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4498/Reviewer_Ace4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4498/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953236334, "cdate": 1761953236334, "tmdate": 1762917401889, "mdate": 1762917401889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}