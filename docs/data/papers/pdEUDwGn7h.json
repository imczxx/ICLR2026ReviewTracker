{"id": "pdEUDwGn7h", "number": 23380, "cdate": 1758342923133, "mdate": 1759896818083, "content": {"title": "Enhancing structural consistency of 3D Human Pose Estimation through Trainable Loss Function", "abstract": "3D human pose estimation (3D HPE) is a challenging task due to complex structural constraints that are not well captured by standard training objectives such as mean squared error (MSE). Previous studies have attempted to enforce structural consistency by incorporating manually designed priors, rule-based constraints, or specialized architectures, which often limit adaptability. In this paper, we propose SCoTL-pose (Structural Consistency via Trainable Loss for Pose Estimation) framework that enables pose estimation models (pose-net) to learn structural dependencies directly from data, through a trainable loss function (loss-net), without explicit priors. Our approach introduces a graph-based loss-net that captures both local and global joint relationships, ensuring anatomically plausible pose predictions. While inspired by the idea of Structured Energy As Loss (SEAL), we extend it to tackle 3D human pose estimation, a task with more complex and high-dimensional structural dependencies than those considered in previous applications. To this end, we employ a graph-based model as loss-net architecture, tailored to capturing the intricate local and global dependencies among joints. SCoTL-pose can be combined with diverse backbones, from single-frame lifting networks to state-of-the-art multi-frame temporal models, without additional inference cost. To assess whether SCoTL-pose enhances structural plausibility in a quantitative manner, we also introduce Limb Symmetry Error (LSE) and Body Segment Length Error (BSLE) as evaluation metrics. Experimental results on Human3.6M, MPI-INF-3DHP, and Human3.6M WholeBody datasets demonstrate that SCoTL-pose not only reduces per-joint pose estimation errors but also generates more plausible poses, with increasing gains under more challenging settings such as single-frame or in-the-wild scenarios.", "tldr": "", "keywords": ["Trainable Loss function", "Structural Consistency", "3D Human Pose Estimation", "Structured Energy network"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab8a479146d11b554604f33fd89b5ba8dc7fdca8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SCoTL-Pose, an extension of the Structured Energy As Loss (SEAL) framework, tailored for the high-dimensional and structurally complex domain of 3D human pose estimation. Additionally, it proposes two metrics‚ÄîLimb Symmetry Error (LSE) and Body Segment Length Error (BSLE)‚Äîand demonstrates performance improvements across the Human3.6M, MPI-INF-3DHP, and H3WB datasets.."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed loss network (loss-net) is employed only during training, thus incurring no additional inference cost. \n2. The framework is compatible with various backbone architectures, including SimpleBaseline, VideoPose, and MixSTE. Extensive experiments validate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. This work represents an incremental extension of SEAL-Pose [1], which was accepted at the ICCV 2025 Workshop on SP4V. The primary modification lies in replacing the loss-net with a Graph Network to better capture skeletal topology. While this change offers structural advantages, it is largely engineering-level rather than conceptual or theoretical.\n2. The reported performance differences between SCoTL-Pose (MLP) and SEAL-Pose (Margin) are inconsistent‚Äîsome results improve, while others remain similar. These inconsistencies raise concerns regarding reproducibility and experimental reliability.\n3. Evaluate cross-dataset generalization, e.g., training on MPI-INF-3DHP and testing on Human3.6M.\n4. Extend the framework to other structured prediction tasks, such as hand pose or animal pose estimation.\n\n[1] SEAL-Pose: Enhancing 3D Human Pose Estimation through Trainable Loss Function."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zSEzLjEmIT", "forum": "pdEUDwGn7h", "replyto": "pdEUDwGn7h", "signatures": ["ICLR.cc/2026/Conference/Submission23380/Reviewer_xYLb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23380/Reviewer_xYLb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905564100, "cdate": 1761905564100, "tmdate": 1762942635664, "mdate": 1762942635664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all reviewers for their thoughtful and detailed feedback, especially for the recurring questions regarding the novelty and positioning of our method relative to SEAL. Below, we clarify how our work concretely extends and instantiates the SEAL paradigm in the context of 3D human pose estimation.\n\nWe fully agree that our work is conceptually inspired by the ‚Äústructure-as-loss‚Äù idea introduced by SEAL. **SEAL, however, is a generic framework:** it shows that a trainable energy-based loss can, in principle, enforce structural plausibility, but **it does not provide a concrete recipe for complex, continuous, high-dimensional regression tasks** such as 3D human pose estimation. **Our contribution is to instantiate and validate this paradigm in a genuinely challenging, real-world setting.**\n\nConcretely,\n\n(1) We are the first to apply a SEAL-style trainable loss to 3D human pose estimation and show that it works **robustly across multiple datasets and six backbones** without any test-time overhead. We are also adding results on two additional strong backbones ‚Äî a diffusion-based model and the state-of-the-art KTPFormer ‚Äî which demonstrate that the proposed loss-net is also compatible with high-performing HPE architectures.\n\n(2) We design a **graph-based loss-net that mirrors the human skeleton topology, which is not present in SEAL.** This architecture is designed to capture limb couplings and structural violations that are specific to HPE, and our experiments show that it effectively learns such structure.\n\n(3) We also adopt a **simple but systematic training protocol**: we perform a greedy hyperparameter search where we first tune the pose-net learning rate, then the loss-net learning rate, and finally the energy weight ùõº. **This procedure makes it easier to find a stable and effective configuration in practice.** We also experimented with explicit negative samples to further guide training; although this has not yet led to clear improvements, it is something we are actively exploring and refining.\n\nWe will revise the main contributions to make this positioning clearer: our goal is not to propose an entirely new generic framework on top of SEAL, but to **bridge the SEAL to a realistic, complex structured prediction task,** and to show that, with an appropriate graph-based loss-net design and training protocol, **it becomes a practical, broadly applicable module for 3D human pose** estimators."}, "title": {"value": "general response1:Regarding novelty beyond SEAL"}}, "id": "yt4vrzBjV9", "forum": "pdEUDwGn7h", "replyto": "pdEUDwGn7h", "signatures": ["ICLR.cc/2026/Conference/Submission23380/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23380/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23380/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763619395962, "cdate": 1763619395962, "tmdate": 1763634095008, "mdate": 1763634095008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to enhance 3D Human Pose Estimation (HPE) through structural constraints. The framework consists of two main components:\n1Ôºâa graph-based trainable loss network \n2Ôºâtwo types of evaluation metrics (LSE and BSLE) used to assess the structural quality of predicted poses.\nOverall, the proposed structure is applied to some baseline models and leads to certain performance gains. However, the paper‚Äôs organization and presentation are not sufficiently clear, and several concerns are noted below:\n\n1. The authors state that the loss network is only required during training and does not introduce additional inference cost. However, it is unclear how the loss network can be used only during training.\n2. The Pairwise Temporal Loss is not clearly formulated, and while LSE and BSLE are introduced as evaluation metrics, the training targets used to guide the optimization are not explained.\n3. The relationship between the pose network and the loss network is not clearly described. It is recommended that the paper include a diagram illustrating the overall framework, showing how the pose-net and loss-net interact and what specific roles they play in improving the model.\n4. The backbone models chosen for comparison are relatively outdated. The proposed SCoTL-Pose framework has not been integrated with more recent strong baselines such as MotionAGFormer or KTPFormer, which limits the significance of the reported performance. Furthermore, the MixSTE results reported in Table 1 appear to be incorrect, as the MPJPE for MixSTE should be around 40.9 mm according to the original paper. \n5.The comparative methods listed in the figures and tables should include proper citations to help readers locate and verify the referenced works, as well as the \"single frame\" or \"multi-frame\" setting marks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed structure is applied to several baseline models and achieves certain performance gains. It demonstrates a feasible design that can be integrated into both single-frame and multi-frame architectures."}, "weaknesses": {"value": "Refer to the Summary part."}, "questions": {"value": "Refer to the Summary part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "avDdEZIktb", "forum": "pdEUDwGn7h", "replyto": "pdEUDwGn7h", "signatures": ["ICLR.cc/2026/Conference/Submission23380/Reviewer_eSb3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23380/Reviewer_eSb3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911380613, "cdate": 1761911380613, "tmdate": 1762942635430, "mdate": 1762942635430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SCoTL-pose (Structural Consistency via Trainable Loss for Pose Estimation), a framework to address a key challenge in 3D Human Pose Estimation, i.e., standard losses like MSE optimize for per-joint accuracy but often produce anatomically implausible poses. Instead of using manually designed, rule-based constraints, this work introduces a trainable loss function (loss-net) that learns to assess the structural plausibility of a predicted pose. This loss net is trained jointly with the main pose estimation model in an alternating, dynamic fashion, inspired by the Structured Energy As Loss (SEAL) framework. The authors introduce two evaluation metrics, i.e., Limb Symmetry Error (LSE) and Body Segment Length Error (BSLE) to quantify structural plausibility. Experiments are performed on Human3.6M, MPI-INF 3DHP, and H3WB datasets to show the effectiveness of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The design of the loss-net as a graph-based model is a key strength. Using a network architecture that mirrors the output structure, i.e., a skeleton, is an intuitive and good way to enforce structural consistency.\n2. The authors validate their method on multiple datasets and six different backbone models (three single-frame, three multi-frame), demonstrating that the model is agnostic of the framework.\n3. The introduction of LSE and BSLE as metrics allows for a direct, quantitative evaluation of the plausibility problem, which standard metrics like MPJPE fail to capture. The analysis in Figure 2 shows that SCoTL-pose improves LSE/BSLE even for samples with similar P-MPJPE.\n4. Loss-net is only used during training and so adds no computational overhead at test time, which I think is a major practical advantage."}, "weaknesses": {"value": "1. The framework involves an alternating training procedure for two networks, which is essentially a minimax game and similar to GANs which is usually difficult to stabilize. The paper admits this in its limitations (\"broad hyperparameter search space,\" \"less straightforward\"). I think that this is a major practical weakness.\n2. There‚Äôs no interpretability or visualization of what the loss-net focuses on. As an example, which limbs or joint dependencies dominate the structural energy? Understanding what the loss is penalizing (symmetry violations, limb length deviations, joint rotations) would make the contribution more interpretable and reliable.\n3. The paper mainly compares SCoTL-Pose with vanilla supervised models or simple regularizers. It omits direct comparisons with other structure-aware or constraint-based approaches, such as kinematic tree priors, limb or bone length constraints, graphical model based pose estimators (example Pose-GCNN, kinematic refinement modules). Without such comparisons, it‚Äôs hard to understand whether the learned loss is actually superior to explicit structure enforcement.\n4. Since the learned loss may rely on dataset specific geometry such as Human3.6, this limits claims of broad generalizability.\n5. The temporal pairwise loss is an appealing idea but the results in Table 5 in the appendix aren‚Äôt convincing. The improvement is marginal."}, "questions": {"value": "1. Please clarify the \"+ Constraint\" baseline in Table 4.\n2. The paper states SEAL cannot be directly applied. However, the described method (alternating training, margin/NCE loss) appears to be a direct implementation of the SEAL dynamic framework. Could you clarify if the primary novelty is the application of SEAL to this complex regression task and the novel graph-based architecture for the loss-net, rather than a simple modification of the SEAL framework itself?\n3. Please answer points arising from the Weakness section as well."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "26aFdTo0h9", "forum": "pdEUDwGn7h", "replyto": "pdEUDwGn7h", "signatures": ["ICLR.cc/2026/Conference/Submission23380/Reviewer_TBiB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23380/Reviewer_TBiB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966548749, "cdate": 1761966548749, "tmdate": 1762942635068, "mdate": 1762942635068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SCoTL, the framework introducing a trainable loss network, loss-net, for 3D human pose estimation. Unlike prior approaches relying on manual priors or rule-based constraints, SCoTL-pose learns structural dependencies from data. Extensive experiments on Human3.6M, MPI-INF-3DHP, and Human3.6M WholeBody shohw consistent improvements in both single- and multi-frame settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive experiments across multiple datasets and backbones.\n2. Well-written and easy to understand; clear motivation and methodology.\n3. No additional inference cost, making the approach practical for integration."}, "weaknesses": {"value": "1. Limited novelty beyond SEAL. The main contribution lies in applying a known concept (trainable energy-based loss) to 3D pose estimation, with relatively straightforward modifications.\n2. Lack of theoretical analysis: The paper would benefit from a more rigorous examination of why the learned energy function improves plausibility or generalizes.\n3. Training instability and sensitivity: The paper acknowledges a large hyperparameter search space but provides little guidance or empirical analysis of its effect."}, "questions": {"value": "1. How sensitive are the results to the Œ± coefficient balancing MSE and the learned energy term?\n2. Could the loss-net trained on one dataset transfer to another without re-training?\n3. How does this approach compare with explicit constraint-based or manifold-regularized methods in terms of efficiency and robustness?\n4. SImilar to the first question, could the loss-net overfit to dataset-specific skeletal proportions or noise patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vcDqdyFXE7", "forum": "pdEUDwGn7h", "replyto": "pdEUDwGn7h", "signatures": ["ICLR.cc/2026/Conference/Submission23380/Reviewer_7hLc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23380/Reviewer_7hLc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762768400921, "cdate": 1762768400921, "tmdate": 1762942634367, "mdate": 1762942634367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}