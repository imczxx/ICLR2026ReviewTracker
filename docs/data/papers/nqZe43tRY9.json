{"id": "nqZe43tRY9", "number": 985, "cdate": 1756827136860, "mdate": 1759898232788, "content": {"title": "LogicXGNN:  Grounded Logical Rules for Explaining Graph Neural Networks", "abstract": "Existing rule-based explanations for Graph Neural Networks (GNNs) provide global interpretability but often optimize and assess fidelity in an intermediate, uninterpretable concept space, overlooking the grounding quality of the final subgraph explanations for end users. This gap yields explanations that may appear faithful yet be unreliable in practice. To this end, we propose LogicXGNN, a post hoc framework that constructs logical rules over reliable predicates explicitly designed to capture the GNN's message-passing structure, thereby ensuring effective grounding. We further introduce data-grounded fidelity ($Fid_D$), a realistic metric that evaluates explanations in their final-graph form, along with complementary utility metrics such as coverage and validity. Across extensive experiments, LogicXGNN improves $Fid_D$ by over 20% on average relative to state-of-the-art methods while being 10-100 times faster. With strong scalability and utility performance, LogicXGNN produces explanations that are faithful to the model's logic and reliably grounded in observable data.", "tldr": "", "keywords": ["Graph Neural Networks", "Interpretability", "Explainability", "Neural-symbolic", "Logical Rules", "AI for Science", "XAI"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/289fd0016dd7efd20b130929b9cdfa6a58f340cd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles post-hoc rule-based model-level GNN explainabiltiy, aiming to ground the extracted global rules to human understandable data-level. They propose LOGICXGNN, which constructs logical rules directly grounded in data, ensuring that explanations correspond to real subgraphs. It introduces a new metric, data-grounded fidelity (FidD), which measures how well explanations match the GNN’s outputs in the actual input space, not in an abstract one."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. Quantitative results are consistently strong. Strong empirical performance on fidelity, coverage, stability, validity and efficiency.\n2. Well motivated. The paper identifies a valid and underexplored problem. Clear identification of an overlooked issue in prior rule-based explainers. \n3. The framework is complete and reproducible."}, "weaknesses": {"value": "1. **Missing clear discussion and comparison on an important baseline.** The paper briefly mentions GCNeuron (Xuanyuan et al., AAAI 2023) in the related work section, categorizing it as a “concept-based global explanation”. However, it does not clearly articulate the methodological differences or justify the absence of a direct comparison. Given that GCNeuron is also a rule-based, model-level GNN explainer that produces logical rules to characterize model behavior, a clearer discussion of methodological distinctions, or at least a brief justification for not comparing, would strengthen the completeness of the evaluation.\n2. **Overly domain-specific evaluation. Lack of validation on simple synthetic tasks.**\nThe paper claims to propose a general and scalable global rule-based GNN explainer, but almost all examples and visualizations are from molecular datasets (Mutagenicity, BBBP, NCI1). This makes the work look domain-specific rather than general. The authors did not include enough simple synthetic datasets where the ground-truth reasoning patterns are known. On such datasets, an effective rule-based method should ideally achieve near 100% data-grounded fidelity, clearly showing that it can capture the model’s true decision rules. Without this type of controlled experiment, it remains unclear whether the proposed rules genuinely reflect model reasoning or are just fitting chemical regularities in the data.\n3. **Excessive complexity of grounded rules without well-clarifications.** While the proposed orbit-based grounding mechanism improves formal precision, it significantly increases the structural and logical complexity of the resulting rules. The grounded rules involve multi-level orbit decomposition, nested logical conjunctions, and numerical thresholds, which make them difficult for non-expert users to interpret. Compared to prior rule-based explainers such as GLGExplainer and GraphTrail that generate concise and human-readable logic, the grounded rules here are overly abstract and cumbersome. This undermines one of the central goals of grounding in this paper, which was explicitly framed as aiming for \"human-understandable\" explanations.\n4. **Insufficient justification for the rule-based paradigm.** Although the paper emphasizes the importance of rule grounding, it does not convincingly demonstrate why a rule-based approach is preferable to other established global explanation paradigms, such as generation-based (XGNN, GNNInterpreter) or subgraph-based (TreeX) methods. There is no human study, no qualitative comparison of interpretability, and no synthetic benchmark explicitly designed to evaluate rule quality. As a result, the claimed advantage of rule-based grounding remains conceptually appealing but empirically unproven.\n5. **Questionable novelty of the proposed evaluation metric.** The proposed “Data-grounded Fidelity (Fid$^D$)” metric appears conceptually aligned with the instance-level fidelity evaluation already used in prior work such as TreeX, where global explanations are mapped back to instances to check whether they reproduce the original GNN’s predictions. The paper reframes this idea under a new term “data-grounded fidelity”, but this seems more like a terminological reframing than a fundamentally new metric design. Moreover, TreeX also provides motif-level global explanations evaluated on instances, making it a highly relevant baseline. However, the authors neither compare with TreeX nor acknowledge this conceptual overlap. This omission weakens the claimed novelty of Fid$^D$ and leaves the evaluation incomplete. \n6. **Limited technical novelty.** The proposed framework largely combines existing ideas: constructing logical rules from learned predicates (as in GLGExplainer, GCNeuron and GraphTrail), grounding them to data (as in motif-based explainers like TreeX and PAGE), and evaluating instance-level fidelity (similar to TreeX)."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oa75c4IEHl", "forum": "nqZe43tRY9", "replyto": "nqZe43tRY9", "signatures": ["ICLR.cc/2026/Conference/Submission985/Reviewer_kSni"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission985/Reviewer_kSni"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762063977391, "cdate": 1762063977391, "tmdate": 1762915653148, "mdate": 1762915653148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a real limitation in current global explainability methods for GNNs: they derive logical rules in a latent or concept space, and only afterwards associate these latent concepts with illustrative (sub)graphs; as a result, the generated example (sub)graphs often fail to correspond to real structures in the dataset (in molecular datasets, some are even chemically invalid). \nThe author propose a multi-step framework that learns logical rules whose predicates are directly grounded in subgraphs observable in the input data. The method is compared against two relatively recent baselines (GLGExplainer and GraphTrail), which the authors re-evaluate by substituting their latent concepts with the representative subgraphs provided by those methods. A new evaluation metric called data-grounded fidelity is introduced to assess how well the logical explanations reproduce the model’s behavior on real graphs rather than on latent representations. \n\nOverall, the motivation is clear and the addressed problem appears real and relevant. However, the paper is occasionally difficult to follow, mainly because the pipeline uses several dense steps, sometimes similar (e.g. different decision trees for different purposes). The provided implementation also raises minor concerns: the released code explicitly skips the grounded part of the pipeline for all datasets except BBBP, Mutagenicity, and NCI1, and it appears not to run correctly for IMDB-BINARY."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper identifies a genuine limitation of current global explainability methods for GNNs: the lack of grounding of logical explanations in real graph instances. The proposed framework offers a systematic and sound solution. The proposed metric is reasonable. The presentation of the baseline results seems rigorous: tha appendix states that the authors of the original methods were consulted to verify the correctness of the reproduction."}, "weaknesses": {"value": "The proposed procedure is quite convoluted, and the absence of ablation studies makes it difficult to understand which components of the pipeline are essential and which could be simplified. For similar reasons, the methododgical explanation is hard to follow: the paper requires several readings to fully grasp the role of each step (e.g., the multiple decision trees used for different purposes). The experimental comparison includes only two baselines."}, "questions": {"value": "1. The released code explicitly skips the grounded part of the pipeline for all datasets excepts BBBP, Mutagenicity, and NCI1. Could the authors clarify whether this was intentional and whether the grounded evaluation can be extended to other datasets (e.g., IMDB-BINARY doesn’t seem to work)?\n2. The procedure is difficult to follow. Have the authors considered simplifying the exposition, for example through a schematic overview?\n3. An ablation study could help isolate which steps in the pipeline are most important. In addition, testing against more baselines would strengthen the empirical evidence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wguFS2EDiI", "forum": "nqZe43tRY9", "replyto": "nqZe43tRY9", "signatures": ["ICLR.cc/2026/Conference/Submission985/Reviewer_kLwi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission985/Reviewer_kLwi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762085165501, "cdate": 1762085165501, "tmdate": 1762915652946, "mdate": 1762915652946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework to extract logic rules as explanation structures for GNN outputs. The method encodes node receptive fields using a WL kernel-based hash function and trains a decision tree to generate regulations that consistently classify the model's predictions. Conjunctive logic rules are extracted as grounding rules to link predicates with the input feature space. An experimental study verified the methods' efficiency, fidelity, and scalability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. There is novelty in WL hashing-based structural encoding and decision tree for generating logic rules. \nS2. The overall method is justified with a cost analysis. \nS3. Results support the major claims."}, "weaknesses": {"value": "W1. The link between the quality of the generated rules and their closeness to real-world evidence remains unevaluated. \nW2. The time-cost analysis lacks a more rigorous elaboration. Important steps seem overly simplified or omitted. \nW3. More baselines are needed, such as motif-based GNN explanation, which can directly generate explanations as subgraphs."}, "questions": {"value": "D1. A main challenge is how to ensure the explanation structures are better grounded in genuine real-world evidence, while the process yields rules based on GNNs that still seem to focus on the models' faithfulness. Have any human experts or authorities assisted in evaluating the generated rules?  How will such a measure be quantified, if possible? \n\nD2. The time cost omits several sources of overhead, such as training decision trees; a more complete analysis is not in place. \n\nD3. Is the method model-specific? Meaning: if one changes to another test set or another model, does the method need to be restarted from scratch, even when the graph is not changed? How may the method respond to larger-scale analysis?\n\nD4. There is a lack of in-depth analysis of how likely the rules are to be redundant or logically entailed by others—a missed opportunity for optimization? \n\nD5. Other approaches, including motif-based GNN explanation, directly output graph patterns or subgraphs, which can be readily converted to conjunctive triple patterns or rules. Representative work needs to be compared with."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4wmFFbyxoP", "forum": "nqZe43tRY9", "replyto": "nqZe43tRY9", "signatures": ["ICLR.cc/2026/Conference/Submission985/Reviewer_nxGX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission985/Reviewer_nxGX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762180955583, "cdate": 1762180955583, "tmdate": 1762915652711, "mdate": 1762915652711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel approach to post-hoc explanations of graph classification, based on concepts. Differently from previous methods, however, it grounds the explanations into actual patterns in the data by means of Weisfeiler–Lehman (WL) graph hashing."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is well written and presented. \n- The methodology is not particularly complex, but it allows creating a very effective method\n- Section 3.4 reports a very nice Analysis subsection, theoretically describing the computational complexity of the method and its applicability to different gnn architectures\n- The experiments show a clear advantage of the proposed method against a couple of state-of the-art baselines"}, "weaknesses": {"value": "General weaknesses:\n- The paper does not have a limitation paragraph, which is now considered almost mandatory in top-level conferences\n- The scope of the proposed method is focused to explanations regarding graph classification only (similarly to the baselines). It would be very interesting if the authors could show or even just describe whether the proposed method could be applied to node classification, or if any methodological assumptions fail to hold in that case. \n- Also in several parts there are mentions of graph tasks, suggesting that different types of tasks have been considered, which does not seem the case: only multiple instances of the graph classification task have been tested. I would recommend rephrasing as it is currently misleading.\n- One of the main result of the paper is that previous baselines provide explanations that are not grounded. However, in the main paper (I saw them in the appendix) there are no mentions regarding how the baselines have been reproduced. Without at least a footnote saying how the explanations for the baselines have been extracted, Figure 1 results too strong and may rise critiques.\n\n\nSpecific weaknesses:\n- The sentence \"As a result, LOGICXGNN not only generates a rich set of representative subgraphs but also learns generalizable grounding rules for each predicate, addressing unreliable grounding in existing methods.\" is not very clear I would suggest rephrasing.\n- $p_j$ and $P$ are not properly defined"}, "questions": {"value": "My main question is regarding the applicability of the proposed method to node classification task:\n- is it feasible to consider the same framework also in this case? \n\nMy guess is that the hashing could be re-used similarly but possibly also the decision trees to select the patterns over the embeddings and the one over the activation matrix."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UVdDUJxwoh", "forum": "nqZe43tRY9", "replyto": "nqZe43tRY9", "signatures": ["ICLR.cc/2026/Conference/Submission985/Reviewer_AscN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission985/Reviewer_AscN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762184555079, "cdate": 1762184555079, "tmdate": 1762915652605, "mdate": 1762915652605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}