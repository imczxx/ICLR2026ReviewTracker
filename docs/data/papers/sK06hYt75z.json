{"id": "sK06hYt75z", "number": 23359, "cdate": 1758342591688, "mdate": 1759896819105, "content": {"title": "Localized Cultural Knowledge is Conserved and Controllable in Large Language Models", "abstract": "Just as humans display language patterns influenced by their native tongue when speaking new languages, LLMs often default to English-centric responses even when generating in other languages. Nevertheless, we observe that local cultural information persists within the models and can be readily activated for cultural customization. We first demonstrate that explicitly providing cultural context in prompts significantly improves the models' ability to generate culturally localized responses. We term the disparity in model performance with versus without explicit cultural context the explicit-implicit localization gap, indicating that while cultural knowledge exists within LLMs, it may not naturally surface in multilingual interactions if cultural context is not explicitly provided. Despite the explicit prompting benefit, however, the answers reduce in diversity and tend toward stereotypes. Second, we identify an explicit cultural customization vector, conserved across all non-English languages we explore, which enables LLMs to be steered from the synthetic English cultural world-model toward each non-English cultural world. Steered responses retain the diversity of implicit prompting and reduce stereotypes to dramatically improve the potential for customization. We discuss the implications of explicit cultural customization for understanding the conservation of alternative cultural world models within LLMs, and their controllable utility for translation, cultural customization, and the possibility of making the explicit implicit through soft control for expanded LLM function and appeal.", "tldr": "Steering improves LLM abilities at culturally localizing model generations.", "keywords": ["multilinguality", "mechanistic interpretability", "semantic accents", "steering", "evaluations"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b3b65feb83799a4f62550d87df63f110087e2ddb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates where large language models (LLMs) store local cultural information by examining their internal representations. It first explores the effects of explicit cultural context prompts (i.e., providing explicit country information) and implicit cultural context prompts (i.e., using multilingual input), demonstrating that cultural knowledge requires explicit context to surface in multilingual settings. The paper then identifies an explicit cultural customization vector that steers LLMs toward non-English cultures. Finally, it shows the generality of these findings across seven models and four cultures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: The paper investigates an interesting and timely problem of how to steer language models toward specific cultures. The framing of the implicit vs. explicit localization gap provides a nice way to conceptualize differences in model behaviour.\n\nS2: The paper presents an extensive investigation of the localization gap, layer-wise importance, and steering experiments, using both synthetic and real datasets.\n\nS3: Comparing localization and cultural adaptation through steering vectors (instead of prompting) is a good approach that goes beyond black-box prompting. The hypothesis of universal steering vectors is particularly interesting and shows strong potential."}, "weaknesses": {"value": "W1: There are several potential evaluation concerns, including:\n\n- Lack of human evaluation: The use of cosine similarity between embeddings serves only as a proxy for semantic similarity, rather than a direct human judgment of cultural appropriateness. The paper would be stronger if human raters from the target cultures were involved in evaluating the open-ended generations, especially given the relatively small number of prompts used (24, L186).\n- Using GPT embeddings with GPT as the judge: This raises concerns about circularity and potential bias in evaluation.\n\nW2: While the hypothesis of a universal cultural steering vector is enticing, the supporting evidence is relatively limited. More ablations (how robust across tasks, sparsity, what negative side-effects occur) would strengthen the claim. Extending the evaluation to include more low-resource languages would also enhance the findings. Finally, expanding the study to cases where cultures differ but the language is roughly the same (e.g., the U.S., U.K., Australia, and Singapore) would further strengthen the work.\n\nW3: Despite using a definition of culture in L078, the concept of culture remains vaguely *operationalized* in the chosen evaluation data. It is unclear which aspects of culture each evaluation dataset examines, or how these collectively provide a comprehensive view of the intended test."}, "questions": {"value": "- Are authors planning on releasing all the synthetic data for reproducibility? \n- Did you examine whether the steering vector introduces any negative side effects on unrelated tasks, such as factual correctness or language fluency?\n- In general, how should one choose which layer to steer for universal steering?\n\nOther comments:\n- L087 - citation should be added the first time mentioning CulturalBench.\n- While Aya-8B is one explicitly multilingual model, Aya-8B-Expanse is a much stronger variant. I am very curious to see whether the results and findings also hold for Aya-8B-Expanse."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sLcEnCSO5w", "forum": "sK06hYt75z", "replyto": "sK06hYt75z", "signatures": ["ICLR.cc/2026/Conference/Submission23359/Reviewer_LYar"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23359/Reviewer_LYar"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761056190058, "cdate": 1761056190058, "tmdate": 1762942626489, "mdate": 1762942626489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates cultural localization in large language models, meaning to identify which internal layers encode culture-specific information and how to steer model behavior toward certain cultural traits. The authors introduce a cultural localization benchmark with four datasets and a method to locate culturally sensitive layers using activation patching. They propose benchmarks based on names and city-related questions and evaluate explicit vs. implicit cultural elicitation, as well as open-ended generation tasks. The study’s main claim is that large models contain latent cultural knowledge that can be mechanically accessed."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The attempt to systematically localize cultural traits within model layers and define a linear “steering” mechanism is a technically interesting direction. Understanding and controlling culturally grounded behaviors is important for fairness and model transparency. \n\n- The proposed framework could open avenues for better evaluation of cultural competence and for designing more adaptive models.\n\n- The overall motivation and broader implications are clearly articulated in the introduction and conclusion."}, "weaknesses": {"value": "- The paper is hard to follow. Dataset descriptions are confusing. Several key sentences are grammatically incorrect or incomplete (e.g. \"Mathematically, the steering vector at layer j steers away from the behavior displayed by negative examples from D− and towards the behavior of positive examples D+ is defined as\", \"If the average similarity is higher than answers we consider those answers more semantically similar.\")\n\n- The procedure for activation patching and steering is not sufficiently explained. It is unclear what _last token position_ $t_{source}$ refers to, which tokens are replaced in what layers, or how source-target pairs are constructed. Likewise, “positive” and “negative” examples ($D^+$ and $D^-$) are never clearly described.\n\n- The data labels of “correct English answer” or “correct translated answer” in the explicit–implicit localization gap are not defined and thus confusing. As a result, Figure 1 and some quantitative results cannot be interpreted.\n\n- The cosine similarity measure between text embeddings of different answers are not informative of homogeneity, the interpretation of values (e.g., 0.333 vs. 0.359) is unclear, in what sense is different?  the words? the topic? Also, reliance on ChatGPT as a cultural judge is questionable.\n\n- Reported results (e.g., 64% and 32% refusal rates from GPT-4o to answer city and name-related questions) are difficult to interpret without clear dataset sizes.\n\n- The central claim that the method finds layers where “cultural localization happens” is intriguing but difficult to understand due to the lack of clarity in the methodology."}, "questions": {"value": "1. Please clarify the activation patching procedure: what are $t_{source}$ and $t_{target}$? Which tokens are replaced in what layers, and how are the source–target pairs selected from the dataset?\n\n2. Please clarify the datasets. How do you define “positive” and “negative” examples when constructing $D^+$ and $D^-$ for the steering vector? What does it mean for a response to be “correct” or “incorrect” in the explicit–implicit localization task? Could you provide dataset statistics (number of samples per culture, domain, etc.) to assess the robustness of your results?\n\n3. How to interpret the cosine similarity score? Two pairs of sentences could have the same similarity score for two very different reasons.   How should we interpret it in semantic terms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MIMVDSy8lh", "forum": "sK06hYt75z", "replyto": "sK06hYt75z", "signatures": ["ICLR.cc/2026/Conference/Submission23359/Reviewer_sbiU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23359/Reviewer_sbiU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761300908916, "cdate": 1761300908916, "tmdate": 1762942626221, "mdate": 1762942626221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper evaluates whether LLM-generated responses vary when the prompt is in English vs a specific language. They evaluate if prompting in a specific language generates a more culturally appropriate response than a socio-demographic prompting setup in English. They call this gap the explicit-implicit performance gap. Next, they attempt to steer the responses using vectors, which they find are better at reducing culture-specific stereotypes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper asks an interesting question about the impact of language in implicitly conditioning open-ended generation. Furthermore, they explore steering vectors as a way to generate localized responses, which also minimize stereotyping."}, "weaknesses": {"value": "1. The paper hypothesizes that language should implicitly indicate a preference for localized response. Hence, the same task, if prompted in a socio-demographic setup using English, should yield semantically similar output if prompted in a regional language without the explicit socio-demographic setup. The assumption that language should succinctly inform the model about the socio-demographic setup might be problematic. This is reflected in the results, which indicate that models possibly maintain a consistent semantic space and need explicit prompting (due to instruction tuning/alignment) to get culture-specific responses. Also, the paper makes a 1:1 language-country assumption by mapping language to a single country, which might be untrue. The example in lines 95-96 might not hold for Bengali spoken in India, where the name \"Mohammed\" is highly unlikely.\n2. The paper makes claims about model stereotyping and diversity (\"Despite the explicit prompting benefit, ... toward stereotypes,\" in the abstract, Table 1). However, (i) they do not formally define stereotyping, and (ii) they only measure stereotyping using LLM as a judge. A formal definition of what entails a generation to be classified as a stereotype should be provided. Human evaluations on a subset of generations should be conducted to enhance the reliability of the LLM-based evaluations. Also, a statistical significance test should be performed on Table 1 to check if the differences are significant. The prompt used with LLMs (Figures 11 and 12) does not contain an option if none of the generated excerpts are stereotypical. Also, would we see the same scores if instead of using a comparative setup, where two excerpts are compared against each other for stereotypicality, each excerpts are scored individually and then their aggregated scores are compared?\n3. Section 2.4 defines the EI gap as a difference in probability distributions. What does the subtraction of probability distributions mean in the formula? How do the authors measure distributional difference as a single metric in Figure 1?\n4. The paper is very difficult to read, as the ideas and experiments could be conveyed in simpler terms. For example, Sections 3.2 and 3.3 are difficult to read and lack clarity. The authors talk about \"option labels\" in line 269, without explaining which options they are referring to. I think the issue is that the paper tries to do a lot many things without succinctly studying each thing.  Also, I was consistently getting lost between the jargon - They introduce new terms, such as \"non loc prob\", \"loc prob\", etc, without mentioning how they tie to the earlier part of the paper. Figure 3: The formatting of the images should be consistent. The font size and y-axis range should be kept consistent to enhance readability.\n5. The paper claims representational universality in lines 312-314, which must be validated statistically. Solely observing the graphs side-by-side does not entail such conclusions."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FCMRT2QUfU", "forum": "sK06hYt75z", "replyto": "sK06hYt75z", "signatures": ["ICLR.cc/2026/Conference/Submission23359/Reviewer_Ffxc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23359/Reviewer_Ffxc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737646299, "cdate": 1761737646299, "tmdate": 1762942625897, "mdate": 1762942625897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper demonstrates the presence of an explicit-implicit cultural localization gap, where the models fail to generate the correct answers in a culture’s language, even when they do when the culture is explicitly mentioned in the prompt. The paper proposes a steering based approach to improve this localization. The results show that for 5 cultures, culture-specific vectors help reduce this gap while also maintaining diversity of answers in open-ended generations (which explicit prompting reduces)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper clearly shows a gap and proposes a well grounded solution\n- There is thorough analysis of the relevant cultural information within the model, with individual steering vectors as well as combined\n- The paper is clearly written and easy to follow"}, "weaknesses": {"value": "- 3/4 benchmarks on which the results are shown are new, created as part of this study but not enough information is provided to validate their effectiveness.\n- The choice of language/cultures aren't ones that are localised to one culture - for instance Bengali is spoken in both Bangladesh and West Bengal in India, which could explain the difference in performance"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0H26PTBQx4", "forum": "sK06hYt75z", "replyto": "sK06hYt75z", "signatures": ["ICLR.cc/2026/Conference/Submission23359/Reviewer_gLzr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23359/Reviewer_gLzr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762251881105, "cdate": 1762251881105, "tmdate": 1762942625637, "mdate": 1762942625637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}