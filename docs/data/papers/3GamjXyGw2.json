{"id": "3GamjXyGw2", "number": 12584, "cdate": 1758208784541, "mdate": 1759897500275, "content": {"title": "Rethinking Fair Anomaly Detection From The Group Imbalance Perspective", "abstract": "Anomaly detection (AD) has been widely studied for decades in many real-world applications, including fraud detection in finance, intrusion detection for cybersecurity, etc. Existing anomaly detection methods struggle in imbalanced group scenarios, where the unprotected group is significantly larger than the protected group. Specifically, fairness-unaware methods achieve high overall performance by misclassifying more protected group examples as anomalies, while fairness-aware methods overcompensate fairness by labeling excessive unprotected group examples as anomalies, sacrificing overall performance. To address these issues, we propose FADIG, a fairness-aware contrastive learning-based anomaly detection method designed for imbalanced groups. FADIG consists of two key modules: (1) an adaptively re-balanced autoencoder module that dynamically adjusts group contributions to balance fairness with performance and (2) a fairness-aware contrastive learning module that maximizes similarity between protected and unprotected groups to ensure fairness. Moreover, we provide a theoretical analysis showing our proposed contrastive learning regularization guarantees group fairness. Extensive experiments across multiple real-world datasets demonstrate the effectiveness and efficiency of FADIG in achieving both accurate and fair anomaly detection.", "tldr": "", "keywords": ["Anomaly Detection; Fairness"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/840ae07695669548c211269e97206d5a7f631b42.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces FADIG, a new method for fair anomaly detection considering imbalanced protected and unprotected groups. It tackles the issue through two main components: an adaptively re-balanced autoencoder that adjusts group contributions to the loss function and a fairness-aware contrastive learning module to align the data representations across groups."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The considered problem is essential in fairness studies.\n\n2. The presentation is overall smooth and easy to follow.\n\n3. The experiments are extensive."}, "weaknesses": {"value": "1. In the experiment part, fairness-accuracy trade-off comparisons are missing.\n\n2. The re-balancing loss is a well-established technique. The author mentions that the novelty lies in its hyperparameter-free and parameter-free adaptive weight.  While the proposed formulation in Equation (4) is interesting, the idea of dynamically adjusting weights based on training losses or model performance on different data subsets is not new in the fields of fairness, class imbalance, or hard-sample mining.\n\n3. The paper's motivation hinges on the claim that existing fairness-aware methods \"often overlook the underlying group imbalance that gives rise to such unfairness\". This framing is confusing, as any method designed to enforce group fairness inherently must consider group imbalance. The limitation is not that prior work overlooks imbalance, but how it attempts to solve it but insufficient.\n\n4. The motivation of the proposed methods is not well presented. The paper does not provide a clear explanation for why FADIG's specific components succeed where others fail. The experimental results show that FADIG outperforms the baselines, and the ablation study confirms its components are necessary. However, it does not show how/why the proposed methods can achieve their superior outcomes by resolving what kinds of limitations.\n\n5. Authors should consider widely adopted fairness metrics such as equalized odds and equalized opportunity for results comparisons."}, "questions": {"value": "1. At the beginning, the model is largely unfitted, meaning reconstruction errors are high and potentially close to the initial estimates. This could make the numerator and denominator small, potentially leading to unstable or erratic behavior."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zi6c4NZQPx", "forum": "3GamjXyGw2", "replyto": "3GamjXyGw2", "signatures": ["ICLR.cc/2026/Conference/Submission12584/Reviewer_TAVv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12584/Reviewer_TAVv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12584/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760598305387, "cdate": 1760598305387, "tmdate": 1762923434333, "mdate": 1762923434333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies fairness in unsupervised anomaly detection under group imbalance. Existing fairness-unaware models often misclassify protected-group samples as anomalies, while fairness-aware methods overcompensate and degrade overall performance. To address this, the authors propose FADIG, a fairness-aware anomaly detection method that is inspired by reconstruction-based autoencoders. The authors define a new loss function for reconstruction error that has two components: an adaptively rebalanced autoencoder reconstruction loss that dynamically adjusts group contributions during training, and a fairness-aware contrastive learning loss that aligns the representations of protected and unprotected groups while maintaining within-group diversity. A theoretical analysis based on f-divergence shows that minimizing their contrastive regularization reduces group recall disparity. The authors provide extensive experiments on image, tabular, and graph datasets demonstrating that FADIG achieves higher recall with lower disparity than baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Introduces a novel adaptive reweighting mechanism that balances contributions from protected and unprotected groups, and a fairness-aware contrastive learning module that promotes cross-group alignment and within-group diversity.\n- Provides extensive experiments across image, tabular, and graph datasets, showing improvements in both fairness and accuracy (higher recall)."}, "weaknesses": {"value": "- The core contribution is relatively modest, as FADIG ultimately modifies the training objective through a reweighted reconstruction loss and additional fairness regularizer.\n- The fairness analysis only provides an indirect link between the theoretical risk bounds and the empirical fairness metric (recall disparity), limiting the strength of its claims."}, "questions": {"value": "- The authors say that the training and the test datasets are the same. I don't know how common this is in anomaly detection, but this requires more justification than the task being unsupervised.\n- I am quite surprised about the recall statistics of fairness-unaware methods. In general, there is a tradeoff between accuracy and fairness, but the algorithm proposed by the authors achieves higher accuracy and fairness simultaneously. This raises questions about whether the baselines are sufficiently strong or well-tuned, how authors explain FADIG's ability to achieve higher fairness without compromising accuracy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QuHoKtj3ul", "forum": "3GamjXyGw2", "replyto": "3GamjXyGw2", "signatures": ["ICLR.cc/2026/Conference/Submission12584/Reviewer_ufXC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12584/Reviewer_ufXC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12584/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939575821, "cdate": 1761939575821, "tmdate": 1762923433989, "mdate": 1762923433989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on fairness in outlier detection in unsupervised learning specifically addressing imbalance data that naturally arises in presence of minority protected groups. The paper presents a method for addressing representation disparity due to imbalance by proposing a fairness-aware contrative learning criterion and a weighted reconstruction based network module to account for patterns from minority groups. Empirical results show the effectiveness of the proposed method across multiple real-world datasets when compared to exciting fairness-aware methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Rebalancing autoencoder with learnable weight for reconstruction loss is a simple way to encourage learning patterns from minority groups. I like the analytical calculation of \\epsilon. \n2. Adapting contrastive learning for comparing the groups induced by protected attributes is simple and effective approach.\n3. Paper is easy to read and follow."}, "weaknesses": {"value": "1. Appendix G notes that the paper focuses on binary groups. How does the method scale with multi valued multiple protected attribute setup?\n2. Paper shows the robustness of choices of hyperparameters. However, a discussion on how initial choice was arrived at would be helpful."}, "questions": {"value": "I do not have specific questions. I reviewed this paper in an earlier cycle, and the authors have significantly updated it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J2mZeRhwnz", "forum": "3GamjXyGw2", "replyto": "3GamjXyGw2", "signatures": ["ICLR.cc/2026/Conference/Submission12584/Reviewer_bFEz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12584/Reviewer_bFEz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12584/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996625505, "cdate": 1761996625505, "tmdate": 1762923433485, "mdate": 1762923433485, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies fairness in unsupervised anomaly detection with group-wise imbalance. The authors argues that standard AD methods skew toward majority patterns and over-flag the protected group as anomalous, while prior fairness-aware methods often over-correct and hurt overall recall. To tackle the group imbalance, the authors propose FADIG, which combines (i) an adaptively re-balanced autoencoder that learns group weights to balance utility and fairness, and (ii) a fairness-aware contrastive learning regularizer that aligns group-wise representations without collapsing anomaly separability. Experimental results across multiple datasets shows that FADIG improves detection performance and group parity over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed FADIG framework elegantly integrates adaptive re-weighting and fair contrastive learning, addressing both representation bias and imbalance without requiring group labels during inference.\n\nThe authors derive a provable bound showing that minimizing their contrastive regularizer reduces group-risk differences, lending theoretical support to the fairness claims."}, "weaknesses": {"value": "The overall problem setup lacks clarity: the method assumes full access to sensitive attribute labels during training, while anomaly labels remain unavailable. Is this a common real-world scenario? Specifically, addressing partial or missing sensitive information seems to be more reasonable.\n\nThe proposed re-balanced reconstruction loss seems to rely on the strong assumption that the minority group also shares worse performance. In practice, however, this is likely to not hold true, which poses concerns on the applicability of the proposed method.\n\nIt remains unclear how the proposed method aligns with conventional fairness notions such as disparate impact or equalized odds. The paper primarily evaluates fairness through group-wise performance gaps in anomaly scores, but does not explicitly examine whether FADIG improves or preserves fairness under these established definitions.\n\nSeveral claims are misleading or inaccurate. For example, the authors claim \"a hyperparameter-free and parameter-free adaptively reweighted autoencoder\" in line 481, which is clearly not the case.\n\nIt is unclear how the proposed method would perform relative to thresholding-based or post-processing fairness baselines, especially given the superiority of post-processing as suggested in existing work [1].\n\n[1] Cruz, Andr√© F., and Moritz Hardt. \"Unprocessing seven years of algorithmic fairness.\" arXiv preprint arXiv:2306.07261 (2023)."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EJKrvkCLPk", "forum": "3GamjXyGw2", "replyto": "3GamjXyGw2", "signatures": ["ICLR.cc/2026/Conference/Submission12584/Reviewer_Zhba"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12584/Reviewer_Zhba"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12584/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762138809627, "cdate": 1762138809627, "tmdate": 1762923432874, "mdate": 1762923432874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}