{"id": "NCecQKw1Ni", "number": 6778, "cdate": 1757995338541, "mdate": 1759897894549, "content": {"title": "CollectiveKV: Decoupling and Sharing Collaborative Information in Sequential Recommendation", "abstract": "Sequential recommendation models are widely used in applications, yet they face stringent latency requirements. \nMainstream models leverage the Transformer attention mechanism to improve performance, but its computational complexity grows with the sequence length, leading to a latency challenge for long sequences.\nConsequently, KV cache technology has recently been explored in sequential recommendation systems to reduce inference latency.\nHowever, KV cache introduces substantial storage overhead in sequential recommendation systems, which often have a large user base with potentially very long user history sequences.\nIn this work, we observe that KV sequences across different users exhibit significant similarities, indicating the existence of collaborative signals in KV. \nFurthermore, we analyze the KV using singular value decomposition (SVD) and find that the information in KV can be divided into two parts: the majority of the information is shareable across users, while a small portion is user-specific.\nMotivated by this, we propose CollectiveKV, a cross-user KV sharing mechanism.\nIt captures the information shared across users through a learnable global KV pool. During inference, each user retrieves high-dimensional shared KV from the pool and concatenates them with low-dimensional user-specific KV to obtain the final KV.\nExperiments on five sequential recommendation models and three datasets show that our method can compress the KV cache to only 0.8\\% of its original size, while maintaining or even enhancing model performance.", "tldr": "", "keywords": ["KV cache", "Sequential recommendation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d8d258d41df7308d487319b53c6f1f42e537dc48.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes CollectiveKV, a cross-user KV cache sharing mechanism for sequential recommendation models. The key idea is to decouple the KV cache into a high-dimensional global KV shared across users and a low-dimensional user-specific KV. CollectiveKV employs a router network to retrieve shared KV items from a global pool and concatenates them with user-specific KV for attention computation. Experimental results on five sequential recommendation models and three datasets show large cache compression rates while reportedly maintaining or even improving accuracy metrics compared to baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses a relevant efficiency problem in sequential recommendation — KV cache storage and latency — with a novel sharing-based compression approach.\n- The method design is conceptually simple yet elegant, with clear separation of shared and user-specific KV components.\n- Experimental evaluation spans multiple models and datasets, reporting consistent compression gains and performance improvements.\n- The work offers practical value for large-scale industrial recommendation systems where KV cache size is a bottleneck."}, "weaknesses": {"value": "- While the work is innovative, the overall innovation point is relatively simple and may not be sufficient for a strong theoretical contribution. It is more like a balance between the existing group clustering-based attention and the ordinary attention.\n- There is a theoretical concern: since global modeling is used to compress the KV cache, the actual recommendation performance should, in principle, be at most the same as before compression. However, the reported experiments show that the proposed method is significantly superior to the baseline, which raises questions about the cause. Further exploration experiments and deeper analysis are needed to explain why and under what conditions the method can outperform the standard uncompressed KV cache."}, "questions": {"value": "- The novelty is effective but relatively minor. It is more like a balance between the existing group clustering-based attention and the ordinary attention.\n- Further exploration experiments and deeper analysis are needed to explain why and under what conditions the method can outperform the standard uncompressed KV cache."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sbDGbQQOVI", "forum": "NCecQKw1Ni", "replyto": "NCecQKw1Ni", "signatures": ["ICLR.cc/2026/Conference/Submission6778/Reviewer_peMb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6778/Reviewer_peMb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816012584, "cdate": 1761816012584, "tmdate": 1762919054432, "mdate": 1762919054432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper, \"CollectiveKV,\" addresses the significant challenge of storage overhead and inference latency introduced by the Keys and Values (K/V) cache mechanism in sequential recommendation systems, particularly those leveraging Transformer attention. Sequential recommendation models, unlike LLMs, serve a massive user base with potentially long interaction histories, leading to KV cache sizes that exceed GPU capacity and necessitate costly offloading.\n\nThe core insight driving this work is the observation that K/V sequences across different users exhibit substantial similarities, suggesting the presence of collaborative signals embedded within the K/V representations. Through singular value decomposition (SVD) analysis, the authors confirm that the majority of K/V information is shareable (principal component subspace), while only a small fraction is user-specific (residual subspace).\n\nMotivated by this, CollectiveKV proposes a novel compression paradigm that decouples the K/V into a high-dimensional collective KV shared via a learnable global pool, and a low-dimensional user-specific KV. This decomposition and sharing mechanism, governed by a router network trained with specific losses (peak and load balance loss), allows for drastic compression. The experimental results demonstrate that CollectiveKV achieves remarkable compression and latency reduction while maintaining or improving recommendation accuracy across five models and three datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Groundbreaking Compression Rate and Performance Preservation: The method achieves an exceptional level of compression, reducing the KV cache size to as low as 0.8% of its original size (a compression ratio of 0.008). Crucially, this compression is achieved while simultaneously maintaining or even enhancing model performance across various target-attention and self-attention models (SIM, SDIM, ETA, TWIN, HSTU) and three long-sequence datasets.\n\n2. Significant Inference Latency Reduction: CollectiveKV effectively addresses the stringent latency requirements of real-time sequential recommendation. Comparative experiments show that the proposed method yields significantly lower latency than the standard KV cache, particularly for larger batch sizes. For instance, at a batch size of 512, the standard KV cache latency was 32.991 ms, whereas CollectiveKV achieved 0.695 ms. This result confirms the practical deployment benefit of the approach in mitigating transfer latency caused by offloading large caches.\n\n3. Novel and Well-Motivated Compression Paradigm: The paper introduces a new compression paradigm for K/V caches by sharing information across users, a unique approach that exploits the strong collaborative signals inherent in recommendation scenarios. This is a fundamental distinction from existing methods primarily designed for LLMs, which focus on compressing individual sequences"}, "weaknesses": {"value": "1. Empirical Determination of User-Specific KV Dimensionality: A primary limitation acknowledged by the authors is the difficulty in determining the optimal dimensionality ($d_u$) for the low-dimensional user-specific KV. The choice of this dimension critically affects both the overall compression rate and the model's ability to capture sufficient personalized signals; if set incorrectly, performance may degrade or compression benefits may be lost.\n\n2. Lack of Adaptive Strategy for $d_u$: Related to the above, the current optimal setting for the user-specific KV dimension relies on empirical observations (e.g., typically set within 4% of the attention head dimension). The paper highlights that developing an automated or adaptive strategy to determine this dimension remains a crucial direction for future work, indicating that the current method relies on a tuning process for optimal results.\n\n3. Inconsistent Robustness of Training Losses: While the peak loss and load balance loss are generally effective, the ablation study shows that they perform suboptimally on the KuaiVideo dataset under the standardized loss weight settings. This suggests that the hyperparameter weights for these auxiliary losses might need substantial dataset-specific tuning to maintain optimal performance across diverse recommendation environments.\n\n4. Dataset-Dependent Optimal Sharing Strategy: The ablation study demonstrates that the best sharing configuration (sharing keys only, values only, or both) varies across different datasets. For example, sharing only values (CollectiveV) yielded the best AUC/Logloss on MicroVideo, while sharing both (CollectiveKV) was best on KuaiVideo and Ebnerd. This suggests an inherent trade-off between key and value sharing that is not universally optimized by the current CollectiveKV framework, potentially requiring empirical selection per dataset"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XAYDsrbpFW", "forum": "NCecQKw1Ni", "replyto": "NCecQKw1Ni", "signatures": ["ICLR.cc/2026/Conference/Submission6778/Reviewer_uWXd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6778/Reviewer_uWXd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971053298, "cdate": 1761971053298, "tmdate": 1762919053940, "mdate": 1762919053940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "It is hard to satisfy the latency requirement for Transformer based sequential recommendation models. CollaborativeKV analyzes the collaborative signals in KV caches and divides it into inter-user and intra-user components by SVD. The proposed method achieves better effectiveness and efficiency than state-of-the-art baselines on benchmark video recommendation datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "--CollaborativeKV improve the efficiency of KV cache and at the same time the recommendation accuracy without tradeoff.\n\n--Practically, it solves the urgent latency problem by low-rank factorization for Transformer based sequential recommendation models.\n\n--The massive reduction in storage overhead has been verified on real-world datasets in Table 1."}, "weaknesses": {"value": "-The proposed method highly relies on the user similarities, which might not be effective for applications with non-overlapping user interests.\n\n-The recommendation accuracy in terms of GAUC is increased by 0.005 or so with the increase of the global KV pool size to 10000 according to Figure5(a). This performance improvement cannot be ignored compared with its improvement in Table 1. So how to derive the conclusion in the last passage of Page 8: …a relatively small global KV pool is SUFFICIENT to capture shared KV information…."}, "questions": {"value": "--Though the cache efficiency improvement is obvious, the effectiveness improvement of CollaborativeKV is relatively small.\n\n--According to the choice of benchmark datasets, it is better to change the title to clarify that the proposed method is designed for video recommendation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zeSv4fuOys", "forum": "NCecQKw1Ni", "replyto": "NCecQKw1Ni", "signatures": ["ICLR.cc/2026/Conference/Submission6778/Reviewer_U1XG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6778/Reviewer_U1XG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094022218, "cdate": 1762094022218, "tmdate": 1762919053329, "mdate": 1762919053329, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CollectiveKV, a novel approach for compressing the KV cache in sequential recommendation systems by leveraging collaborative signals across users. The authors first analyze and identify significant similarities in KV sequences across different users, and through Singular Value Decomposition (SVD), confirm that KV information can be decoupled into largely shareable collaborative information and a small portion of user-specific information. Based on this, they design the CollectiveKV method, which centers on a learnable global KV pool to capture cross-user shared information and a router network for dynamic retrieval of shared KV. During inference, low-dimensional user-specific KV is concatenated with high-dimensional shared KV to drastically compress the cache size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation is clear. Through data-driven analysis, the existence of collaborative signals in KV caches is demonstrated from the perspectives of similarity and information decomposition, thus leading to the core idea of \"decoupling and sharing\".\n- The paper pioneers the incorporation of cross-user collaborative signals into KV cache compression, distinguishing it from traditional methods focused on individual sequences in LLMs. By decoupling KV information into shared and user-specific components, it establishes a novel paradigm that could be extended to other recommendation scenarios."}, "weaknesses": {"value": "-  The paper notes that the selection of the user-specific KV dimension lacks an automated strategy (as discussed in Section 6) and currently relies on manual setting (e.g., dimensions within 4% of the attention head dimension). This could necessitate iterative experimentation across different datasets or models.\n- As shown in Figure 4, increasing the user-specific KV dimension can degrade performance, but the paper does not deeply explore the redundancy threshold.  \n-  Ablation experiments reveal that the effectiveness of the sharing strategy varies by dataset (e.g., suboptimal results of loss functions on KuaiVideo), suggesting sensitivity to data characteristics like sequence length and task difficulty. This may limit robustness in diverse scenarios, requiring further cross-domain validation.\n- The method is effective on various models and datasets, but its performance is affected by data characteristics (such as sequence length), indicating the need for scenario adaptation. Testing on diverse datasets (such as Amazon product data or Spotify sequences) verifies the method's robustness to heterogeneous behavioral patterns.\n- Although the paper provides intuitive support through SVD and similarity analysis, the theoretical basis of the sharing mechanism (e.g., the integrity of shared information after decoupling) is not explored in depth. This may affect the trustworthiness of the method in more complex systems."}, "questions": {"value": "- The global K/V pool size is fixed, which determines the upper limit of information capacity. Experiments show that the global K/V pool size has a relatively small impact on AUC. So, does there exist a lower bound for the pool size?\n- If a lower bound exists, how would recommendation performance be affected when long-tail user behaviors cannot be adequately covered?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T3Ii3p0tUG", "forum": "NCecQKw1Ni", "replyto": "NCecQKw1Ni", "signatures": ["ICLR.cc/2026/Conference/Submission6778/Reviewer_tHC6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6778/Reviewer_tHC6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762232629158, "cdate": 1762232629158, "tmdate": 1762919052948, "mdate": 1762919052948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}