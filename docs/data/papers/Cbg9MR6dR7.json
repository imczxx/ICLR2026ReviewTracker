{"id": "Cbg9MR6dR7", "number": 15946, "cdate": 1758257439608, "mdate": 1763696991643, "content": {"title": "The State of Reinforcement Finetuning for Transformer-based Generative Agents", "abstract": "Reinforcement finetuning (RFT) has garnered significant attention in recent years, particularly for enhancing large reasoning models such as OpenAI o1 and Deepseek R1. The appeal of RFT largely stems from its ability to refine model knowledge, better align outputs with user intent, and address challenges associated with limited finetuning data. Despite these advantages, the application of RFT in large Transformer-based generative agents remains relatively underexplored. Although these agents are designed to address multiple tasks through large-scale autoregressive pretraining and share many properties with large reasoning models, current adaptation strategies predominantly rely on supervised finetuning (SFT). In this work, we conduct a systematic investigation of several RFT techniques across a variety of finetuning parameter configurations and meta-reinforcement learning (meta-RL) environments, employing few-shot offline datasets. We provide a comprehensive analysis of RFT algorithm performance under diverse experimental conditions and, based on our empirical findings, introduce a lightweight enhancement to existing RFT methods. This enhancement consistently improves outcomes by combining the strengths of both SFT and RFT. Our findings provide valuable insights for advancing the effectiveness of RFT approaches and broadening their applicability to meta-RL tasks with large Transformer-based generative agents, motivating further research in broader domains.", "tldr": "", "keywords": ["Finetuning", "Meta-RL", "Generative Agents"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a8d16990a7bf67e12f80153410b9ab460162971.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper “The State of Reinforcement Finetuning for Transformer-based Generative Agents” aims to study how reinforcement finetuning (RFT) techniques, commonly used for aligning large language models such as through RLHF, can be applied to Transformer-based “generative agents.” It positions itself as a comprehensive evaluation of RFT methods in offline reinforcement learning settings, integrating variants such as reward modeling and policy-gradient finetuning. The authors claim that RFT can enhance sample efficiency and reasoning ability in pre-trained Transformer agents and present a benchmark framework comparing different RFT configurations. However, the paper’s objectives remain vague—it is unclear whether it introduces a new algorithm, proposes a benchmark, or provides theoretical insights—and the distinction between “Transformer-based generative agents” and standard LLMs or RL agents is not well defined."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper touches on a important topic. The motivation to unify reinforcement-based alignment with multi-task or offline RL adaptation reflects an original attempt to connect two active research directions. The authors make an effort to survey existing RFT approaches and to frame them within a consistent experimental setting, which could, in principle, contribute to clarifying the landscape of RL-based post-training methods. The writing is generally clear at a surface level, and the paper demonstrates awareness of recent developments in RLHF and RFT, attempting to position itself at the intersection of language modeling, reinforcement learning, and meta-adaptation. Overall, while the execution is weak, the underlying idea of systematically analyzing reinforcement-style finetuning for large Transformer-based agents shows conceptual ambition and topical relevance."}, "weaknesses": {"value": "The main weakness of this paper lies in its lack of conceptual clarity and methodological grounding. It is unclear whether the paper’s goal is to propose a benchmark, a new algorithm, or an analytical study—and this ambiguity undermines its overall contribution. The term “Transformer-based generative agent” is used extensively without a precise definition or boundary relative to standard large language models, prompting confusion about what specific architecture or behavior distinguishes these agents from common RLHF-trained LLMs.\n\nFrom a technical standpoint, the paper’s use of offline reinforcement learning tasks is poorly justified. RFT and RLHF are inherently online or preference-based alignment methods, not baselines for static offline RL datasets. The decision to evaluate RFT in this context suggests a fundamental misunderstanding of the underlying paradigms. Furthermore, the experimental setup is under-specified: there is no mention of which base model was used, what datasets were employed, or how the finetuning protocols were configured. Without this information, the reported results lack reproducibility and interpretability.\n\nThe evaluation design is also inadequate—the authors omit strong baselines such as CQL, IQL, or other modern offline RL methods, and they provide no ablations or sensitivity studies to validate their claims. The comparison across RFT variants remains superficial and largely descriptive, without clear analysis of why certain methods perform differently. Finally, the paper’s writing gives the impression of a survey-like overview rather than a focused, hypothesis-driven study; key references are discussed only at a surface level (e.g., RLHF and GRPO), and theoretical or empirical novelty is minimal."}, "questions": {"value": "- Could the authors clarify the main objective of the paper — is it intended to serve as a benchmarking effort, a new RFT algorithm, or a conceptual analysis of reinforcement finetuning for Transformer-based agents? Clear positioning would help readers understand how to interpret the experimental results and contributions.\n\n- How do the authors define a “Transformer-based generative agent” in contrast to standard LLMs fine-tuned with RLHF or RFT? Are these agents expected to operate in simulated environments, handle sequential decision-making, or simply produce text-conditioned reasoning? A more precise definition would make the scope of the study much clearer.\n\n- Why were offline RL tasks chosen as the evaluation domain for RFT, which is traditionally an online or human-feedback-based training process? Are there theoretical or practical motivations for believing that RFT can meaningfully improve offline RL policy learning?\n\n- What base model and pretraining setup were used for finetuning? Without information about the backbone architecture, scale, or initial performance, it is difficult to interpret whether the observed improvements come from RFT or model capacity.\n\n- Could the authors provide stronger experimental baselines or ablation studies (e.g., comparing against standard offline RL algorithms such as CQL, IQL, or decision-transformer-based approaches) to substantiate the empirical claims?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nSCOTfWpTd", "forum": "Cbg9MR6dR7", "replyto": "Cbg9MR6dR7", "signatures": ["ICLR.cc/2026/Conference/Submission15946/Reviewer_RqYc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15946/Reviewer_RqYc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760925494709, "cdate": 1760925494709, "tmdate": 1762926157155, "mdate": 1762926157155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work attempts to provide an earnest and comprehensive comparison view on the state of reinforcement fine-tuning (RFT) compared to supervised fine-tuning (SFT) of RL agents. The authors identify several RFT and SFT methods commonly used in the literature for RFT used for LLMs and apply them to RL agents on different environments (Metaworld, Mujoco) in the few-shot scenario to sparse and dense reward settings. Moreover based on the current observation of methods the authors propose a new method that combines RFT and SFT. Across a wide range of experiments the authors demonstrate the benefits of their newly proposed method, which is usually among the best performing methods. Finally, the authors try to provide useful takeaways beneficial for practitioners."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper attempts to provide an earnest and comprehensive comparison of SFT vs RFT on RL tasks.\n- Plenty of experiments for different fine-tuning methods (SFT vs RFT)\n- The proposed method seems to generally perform pretty well on the different settings."}, "weaknesses": {"value": "**Inconsistent results and takeaways**\n\nI like the approach of the authors to try to break down plenty of results into key takeaways, however I found multiple contradictions to the key takeaways, some of which are listed below. This is the main factor why I am currently leaning towards rejection at the moment, if they can be resolved I'd consider increasing my rating.\n\nThe paragraph on line 312 states the superiority of QP algorithms and their robustness and broad applicability. Taking a closer look at Table 1, however, the gap to competitors is not large and there is usually a competitor that lies within one standard deviation of QP (e.g. DPO vs QP-SFT for Adapters and Decorators). This means that if a test for statistical significance was performed, the result would be that there is no statistically significant difference. Therefore I recommend to tone down the claiming on superiority and robustness of QP.\n\nIn line 368 states that increasing the number of trajectories consistently improves performance for all methods, however looking at e.g. PPO this is not the case. Similarly performance on DPO for expert is sometimes worse with 100 trajectories than with 50 for expert data. Furthermore, Figure 1 is utterly confusing to me, why is there a distinction in dataset quality for different methods? Does that mean the different methods were trained on different fine-tuning data? Or does it just show the same scores as Figure 2, but normalized? If the latter is the case, please correct the caption because at the moment it states \"Comparison of finetuning dataset quality\" which led to my confusion. Furthermore, if I interpreted it correctly and they essentially show the same information, one of those figures could be moved to appendix to accomodate space for further interpretation/results.\n\nAnother example are takeaways in line 372: \"Supervised approaches yield stable, imitation-driven performance—particularly\nwhen data quality is mixed\", however SFT is one of the worst methods on HalfCheetah (Medium). Yet another example: \"(ii) regardless of the algorithmic family, more finetuning trajectories monotonically enhance performance\", however CQL on Metaworld (Expert) gets worse with more trajectories.\n\n**Novelty**\n\nOne important aspect of the contributions of this work is the investigation of the data quality for fine-tuning, however this has been investigated in prior work already [1]. In particular, this prior work introduces metrics quantifying trajectory quality and state-action coverage and their influence on performance for several offline RL algorithms. It would be helpful to clarify the distinction to this work.\n\nFurthermore, [2] also provides a comprehensive comparison between different parameter-efficient fine-tuning approaches for single-task fine-tuning on Metaworld and DMControl, including a wider variety of PEFT methods than this work. Though they only investigate SFT and no RL fine-tuning. It would be helpful to outline the difference to those works concretely.\n\n[1] A dataset perspective on offline reinforcement learning, Schweighofer et al., CoLLAs 2022\n\n[2] Learning to Modulate pre-trained Models in RL, Schmied et al., NeurIPS 2023\n\n**Conceptual framing**\n\nThe authors mention several times throughout the paper that they investigate a metaRL setup, however to me it seems it is more like an actual transfer learning/ multitask fine-tuning setup as in [1]. There are also no conventional metaRL algorithms compared which makes me wonder whether the \"metaRL\" framing is the correct terminology. Furthermore the terminology of \"generative agents\" is rather confusing to me, as this terminology is usually employed for LLM-based agentic frameworks [2]. Also the focus on \"transformer-based\" is not necessary as all the tested algorithms are agnostic to the architecture.\n\n[1] Learning to Modulate pre-trained Models in RL, Schmied et al., NeurIPS 2023\n\n[2] Generative Agents: Interactive Simulacra of Human Behavior, Park et al., UIST 2023\n\n\n\n**Choice of methods**\n\nIt might be interesting looking into [1] for a LoRA-variant that has been shown to significantly improve upon LoRA based single-task finetuning on the MetaWorld tasks in [2]. This could potentially affect the ranking of the methods in the experiments.\n\n[1] Parameter Efficient Fine-tuning via Explained Variance Adaptation, Paischer et al., ENLSP workshop at NeurIPS 2024\n\n[2] Learning to Modulate pre-trained Models in RL, Schmied et al., NeurIPS 2023"}, "questions": {"value": "- Eq 15: why is the expectation only over (s,a) tuples? why can we not just as in regular double Q-learning use (s,a,s’) tuples for training? \n - Eq 16: would it make a difference if you sampled multiple actions from the policy and take the max instead of only using $\\hat{a}$? I know $\\pi_\\theta$ already maximizes the q-value, but it might help wich counteracting approximation errors.\n - Line 261 mentions that the fine-tuning dataset is much smaller than the pretraining dataset, it would be helpful to provide actual numbers here. While Table 1 mentions 50 finetuning trajectories, this should be made more explicit in the text and not just in the caption.\n - Table 1: Why is the performance on MetaWorld for Prompt tuning equal across all methods? \n - Line 320 mentions that PPO and CQL lack inductive priors, what does that mean?\n - Line 323 mentions that switching from Adapters to Decorator for PPO yields 3% improvement, however the average score of Adapters for PPO is larger than the one of Decorator, am I missing something?\n - The final reached reward for the sparse setting (Figure 3) is around the same (sometimes even higher) than for the dense setting, why? What is the delay in the reward? For the same number of update steps I would expect massive differences if there was a substantial delay in the reward as shown in [1], which does not seem to be the case for e.g. PPO or CQL. \n - Any intuition as to why PPO trained on expert data is better in sparse reward settings than in dense reward settings?\n   \n [1] RUDDER: Return Decomposition for Delayed Rewards, Arjona-Medina et al., NeurIPS 2019"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JT1agf0fE3", "forum": "Cbg9MR6dR7", "replyto": "Cbg9MR6dR7", "signatures": ["ICLR.cc/2026/Conference/Submission15946/Reviewer_EYuE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15946/Reviewer_EYuE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812318367, "cdate": 1761812318367, "tmdate": 1762926156649, "mdate": 1762926156649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a comprehensive study on Reinforcement Fine-Tuning (RFT) for Transformer-based Generative Agents (TGAs) within meta-reinforcement learning (meta-RL) settings. The authors explore various finetuning algorithms, parameter configurations, and their interplay in adapting TGAs to new tasks. They propose a lightweight enhancement combining Supervised Fine-Tuning (SFT) and RFT, providing empirical evidence of its effectiveness across multiple meta-RL environments. The study’s results show that RFT can improve performance, especially in few-shot and sparse reward settings, and the proposed QP-based methods outperform traditional approaches like SFT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper systematically evaluates different RFT methods, providing valuable insights into the trade-offs between various finetuning strategies.\n\n2.The introduction of a lightweight enhancement that combines SFT and RFT is a notable innovation, and the proposed QP-based finetuning methods offer a promising direction for improving model performance in meta-RL settings.\n\n3.The authors conduct extensive experiments across multiple environments (MuJoCo, MetaWorld) with varying dataset qualities and sizes, demonstrating the robustness of their methods.\n\n4.The research addresses a practical challenge in adapting large pre-trained models to real-world tasks with limited data, an important topic given the increasing use of Transformer-based agents."}, "weaknesses": {"value": "1.While the paper highlights the success of RFT in non-RL domains, the motivation for applying RFT to TGAs in meta-RL environments lacks a strong theoretical justification. The analogy to non-RL models feels speculative, without clearly explaining the structural or optimization similarities that would make RFT effective in RL settings. A more principled explanation would strengthen the rationale.\n\n2.The introduction of QP (Q-guided Policy Optimization) is promising, but it requires more clarification. The explanation of how QP combines RL with SFT is not immediately clear, and the potential advantages and challenges of QP are not adequately addressed. A more concise and focused summary would help readers better understand its benefits.\n\n3.The paper focuses on models with up to 40M parameters but does not address the applicability or efficiency of RFT in larger models, such as those with billions of parameters, which are common in current large language models. A discussion on scalability would provide a more complete picture of the method's practical limitations.\n\n4.The conclusion summarizes the contributions well but could benefit from explicitly discussing the broader implications of the proposed RFT method for meta-RL and real-world applications. Additionally, acknowledging potential limitations, such as the method's dependency on smaller model scales, would offer a more balanced view."}, "questions": {"value": "1.The paper mentions a “...lightweight improvement DP...” that integrates the advantages of SFT and RFT. However, the term \"DP\" is not clearly defined.\n\n2.Can the authors provide a more detailed theoretical explanation for why RFT should benefit TGAs in meta-RL, beyond analogy with non-RL models?\n\n3.How does the QP method combine RL and SFT? What are its key advantages and potential challenges in practical applications?\n\n4.Given the increasing size of modern language models, how scalable is the proposed RFT method to models with billions of parameters, and what computational challenges might arise?\n\n5.Could the authors discuss the broader impact of their RFT method on meta-RL and real-world applications, and highlight any potential limitations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ejSQPjlUg8", "forum": "Cbg9MR6dR7", "replyto": "Cbg9MR6dR7", "signatures": ["ICLR.cc/2026/Conference/Submission15946/Reviewer_CvWD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15946/Reviewer_CvWD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901248206, "cdate": 1761901248206, "tmdate": 1762926156256, "mdate": 1762926156256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how to fine-tune a Decision Transformer-style generative agent on a new task with RL in a few-shot way, as existing works mainly investigate supervised fine-tuning of such a generative agent. They compare several existing methods and a newly proposed method called QP which adds an additional term of maximizing the Q-value of the policy to either SFT or DPO. They conduct extensive experimental analysis on Mujoco and MetaWorld to investigate how different fine-tuning algorithm choices and adaptation methods influence the fine-tuning performance, which provide useful insights for future work on RL fine-tuning of generative agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly motivated with a research gap, i.e., RL fine-tuning of generative agents, to fill. \n2. The paper thoroughly investigate many possible RL ways for fine-tuning, and different ways parameter-efficient fine-tuning methods. \n3. The experiments extensively investigate many problem setting and algorithm choices that may influence the performance of RL fine-tuning."}, "weaknesses": {"value": "1. My main concern is with the significance of novelty of this paper. It's more like a benchmarking paper instead of proposing a new idea. The QP method proposed by the authors is more like a direct extension of the QT (Hu et al. 2024) algorithm to a multi-task setting, which is limited in originality. \n2. As discussed by the authors, this paper only considers moderate-size models on relatively simple benchmarks like Mujoco locomotion and MetaWorld. Whether the lessons learned from these benchmarking results can be extended to larger-scale models on more realistic tasks or not remains unknown."}, "questions": {"value": "1. Which checkpoint's performance is reported for each method? The last one after training for a fixed amount of steps or the one with the best evaluation performance?\n2. Why not do \"real\" PPO learning? Is it because you want to do few-shot adaptation?\n3. The CQL regularization term in equation 11 seems to be inverse?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wgcEZZpYJt", "forum": "Cbg9MR6dR7", "replyto": "Cbg9MR6dR7", "signatures": ["ICLR.cc/2026/Conference/Submission15946/Reviewer_RG77"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15946/Reviewer_RG77"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996020232, "cdate": 1761996020232, "tmdate": 1762926155928, "mdate": 1762926155928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "# Summary\n\n**We would like to thank all the reviewers for their thoughtful suggestions on our submission, and appreciate that the reviewers have multiple positive impressions of our work, including:**\n\n- **A clear motivation and well-identified research gap** in exploring reinforcement finetuning (RFT) for generative agents (RG77, RqYc).\n- **A systematic and comprehensive evaluation**, covering diverse RFT and SFT methods, multiple parameter-efficient finetuning configurations, and extensive ablations across environments, data regimes, reward structures, and model scales (RG77, CvWD, EYuE).\n- **Insightful comparative analysis** that sheds light on the trade-offs among RFT strategies and their practical implications for few-shot meta-RL adaptation (CvWD).\n- **A promising hybrid approach (QP)** that unifies supervised and reinforcement finetuning and demonstrates robust performance across varied settings (CvWD, EYuE).\n- **Strong experimental breadth** across MuJoCo and MetaWorld tasks with varying dataset qualities and sizes, highlighting robustness of the methods (CvWD).\n- **Relevance to practical real-world constraints**, especially the challenge of adapting large pretrained Transformer-based agents with limited data (CvWD).\n- **Conceptual ambition and topical significance**, connecting reinforcement-based alignment with multi-task/offline RL adaptation for large Transformer-based agents (RqYc).\n\nWe summarize below the key revisions we will incorporate into the manuscript. **All corresponding clarifications, analyses, and additional results will be integrated into the revised submission.**\n\n**Introduction, Related Works, and Conclusions:**\n\n- We will clarify the **motivation, novelty, and research gap** addressed by the paper, explicitly positioning our work within the landscape of RFT for decision-making agents (RG77, CvWD, RqYc).\n- We will expand the discussion of the **broader impact of RFT**, its relevance to real-world embodied or robotic applications, and its **potential limitations** (CvWD).\n- All typographical issues noted in the introduction will be corrected (CvWD).\n- We will integrate additional **related work**, including recent studies on data quality, PEFT methods, and multi-task RL, to better contextualize our contributions (EYuE).\n- We will provide a clearer justification for evaluating RFT in the **offline meta-RL** setting and explain its relevance to domains where online interaction is infeasible (RqYc).\n\n**Methods:**\n\n- We will refine the mathematical presentation of baseline methods and correct all notational inaccuracies (RG77, EYuE).\n- We will more clearly articulate the **design, intuition, and advantages** of the proposed QP method, including its connection to regularized policy improvement (CvWD).\n- We will provide a **formal and precise definition of Transformer-based generative agents**, distinguishing them from LLM-based agents and clarifying their operational domain (EYuE, RqYc).\n\n**Experiments:**\n\n- We will incorporate **additional large-scale experiments on the LIBERO benchmark**, demonstrating QP’s applicability to more complex environments and larger models (RG77, CvWD).\n- We will provide full experimental details, including: the backbone architecture and model scales, pretraining datasets and number of trajectories, finetuning dataset sizes, evaluation protocol and checkpoint selection criteria (RG77, EYuE, RqYc).\n- We will **revise and tighten the key takeaways** to avoid overstatement, ensuring they reflect the empirical results with appropriate nuance (EYuE).\n- We will include additional **Q-learning ablations**, including multi-action sampling for QP updates (EYuE).\n- We will add deeper analysis of **counterintuitive behaviors**, such as: the identical performance of prompt tuning on MetaWorld, PPO performance in sparse vs. dense reward settings. These analyses will provide clearer insights into algorithmic behavior (EYuE, RqYc).\n\n**Once again, we sincerely thank all reviewers for their detailed feedback and constructive suggestions. We will integrate all corresponding discussions, analyses, and additional experimental results into the revised manuscript.**"}}, "id": "2AL9wtt9NB", "forum": "Cbg9MR6dR7", "replyto": "Cbg9MR6dR7", "signatures": ["ICLR.cc/2026/Conference/Submission15946/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15946/Authors"], "number": 16, "invitations": ["ICLR.cc/2026/Conference/Submission15946/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763696927580, "cdate": 1763696927580, "tmdate": 1763696927580, "mdate": 1763696927580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}