{"id": "a15cDnzr6r", "number": 11538, "cdate": 1758201154601, "mdate": 1759897569262, "content": {"title": "DirMoE: Dirichlet-Routed Mixture of Experts", "abstract": "Mixture-of-Experts (MoE) models have demonstrated exceptional performance in large-scale language models. Existing routers typically rely on non-differentiable Top-$k$+Softmax, limiting their performance and scalability. We argue that two distinct decisions, which experts to activate and how to distribute expert contributions among them, are conflated in standard Top-$k$+Softmax. We introduce Dirichlet-Routed MoE (DirMoE), a novel end-to-end differentiable routing mechanism built on a Dirichlet variational autoencoder framework. This design fundamentally disentangles the core routing problems: expert selection, modeled by a Bernoulli component, and expert contribution among chosen experts, handled by a Dirichlet component. The entire forward pass remains fully differentiable through the use of Gumbel-Sigmoid relaxation for the expert selection and implicit reparameterization for the Dirichlet distribution. Our training objective, a variational ELBO, includes a direct sparsity penalty that precisely controls the number of active experts in expectation, alongside a schedule for key hyperparameters that guides the model from an exploratory to a definitive routing state. Moreover, our DirMoE router matches or exceeds other methods while improving expert specialization.", "tldr": "An intretable probablisitc sparse mixture of expert based Dirichlet distribution.", "keywords": ["MoE", "Mixture of experts", "sparsity", "intrepretability"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/abd232731e227507fd2f538e6df8606edb5c2d43.pdf", "supplementary_material": "/attachment/2ccfd43a08a0258e9fe61e112fb89bb19e1c9505.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Dirichlet-Routed Mixture of Experts (DirMoE), a novel, end-to-end differentiable routing mechanism designed to address the limitations of existing methods like Top-k+Softmax, which conflate expert selection and contribution. DirMoE models routing using a Dirichlet variational autoencoder framework that fundamentally disentangles these two decisions: (i) Expert Selection ($z$) is handled by a Bernoulli component (relaxed via Gumbel-Sigmoid), and (ii) Expert Contribution ($\\theta$) among chosen experts is handled by a Dirichlet component. The training objective is a variational ELBO, which includes a direct sparsity penalty ($R_{sparsity}$) controlling the expected number of active experts $k$. A core theoretical contribution is the introduction of the sparsity knob ($\\lambda$), derived from the Dirichlet concentration, which provides explicit and calibrated control over sparsity, decoupled from the active set size $k$. The authors demonstrate that DirMoE achieves competitive zero-shot performance and improves expert specialization on a LLaMA-185M backbone."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "**Originality**: The idea of factorizing routing using a spike-and-slab prior implemented via Gumbel-Sigmoid (selection) and Dirichlet (contribution) is highly original. \n\n**Quality**: The use of Gumbel-sigmoid and implicit reparameterization ensures the entire forward pass remains fully differentiable, avoiding the gradient bottlenecks of standard Top-k routing. This white-box design provides superior interpretability by explicitly controlling how many experts are active ($k$) and how concentrated their probability mass is ($\\lambda$). Empirically, DirMoE demonstrates better expert specialization across different layers compared to Vanilla MoE, attributed to its mechanism which discourages homogenization often caused by explicit load-balancing losses.\n\n**Clarity**: The paper is generally clear, well-structured, and effectively contextualizes the work relative to prior art in Top-k routing (Switch, GShard) and continuous routers (ReMoE, Soft-MoE). Figure 1 concisely illustrates the complex flow of the DirMoE router. The mathematical formulation is precise, particularly concerning the spike and slab prior and the training objective. A few inconsistencies in variable naming (listed in the Typos section) slightly detract from perfect clarity, but overall, the concepts are presented insightfully.\n\n**Significance**: Given the high importance of MoE models in industry, this is a very nice contribution to the sparse MoE literature.\nAlthough there have been fully differentiable routers proposed in literature e.g., DSelect-k [Hazimeh et al. (2021)], COMET (Ibrahim et al. (2023)], these routers do not support sparse training. DirMoE appears to be an interesting routing approach to hold end-to-end differentiability. Although the complexity of the method (in terms of tuning) makes it unlikely to be adopted by the industry to have a significant impact. \n\nSome related references to include in related work on differentiable routing:\n- Hussein Hazimeh et al. DSelect-k: differentiable selection in the mixture of experts with applications to multi-task learning. In NeurIPS'21.\n- Shibal Ibrahim et al. 2023. COMET: Learning Cardinality Constrained Mixture of Experts with Trees and Local Search. In KDD'23.\n- Michael E. Sander et al. Fast, differentiable and sparse top-k: a convex analysis perspective. In ICML'23."}, "weaknesses": {"value": "**Architectural and Optimization Complexity**: DirMoE relies on a complex stack of techniques: Gumbel-Sigmoid relaxation, implicit reparameterization for Dirichlet samples, a full VAE objective, and multiple scheduled hyperparameters ($\\tau_z$, $\\alpha_{lo}$, $\\lambda^{(p,t)}$). While mathematically elegant, this complexity may lead to significant tuning overhead compared to simpler, fully continuous approaches like ReMoE, which should be explicitly discussed and benchmarked for tuning difficulty.\n\n**Persistent Need for Explicit Cardinality Penalty**: Despite the principled probabilistic design intended to control sparsity, the method still requires a direct sparsity penalty ($R_{sparsity}$) (Eq. 9) to encourage $\\sum_{i=1}^E \\tilde{z}_i(x) \\approx k$ in expectation. Although the authors justify this term as a smooth surrogate, its necessity slightly dampens the claim that the VAE framework inherently solves the controllable sparsity problem entirely through parameter calibration (like $\\lambda$).\n\n**Limited Scalability Comparison**: The scalability evaluation (Table 1) only compares DirMoE against Vanilla MoE (Switch) on a small LLaMA-185M model. Given that other recent differentiable methods (ReMoE, Soft-MoE, Lory) also address the differentiability bottleneck, a comprehensive efficiency comparison showing throughput, memory, and latency overheads against these direct architectural competitors is missing and necessary to validate the robustness of the proposed router at industrial scales. \n\n**Typos**:\n- In Line 30, the citation \"Du & et al., 2022\" (and in references) should be standardized, to \"Du et al., 2022.\"\n- Line 216, delete \"either\".\n- Reference to Eq 8 in line 10 of algorithm 1 is missing.\n- I see a number of places of the format “ Eq. equation x”. Either use equation x or Eq. x.\n- In Section 4.3 and Appendix A.1, there is inconsistent usage of the regularization term's coefficient: $\\lambda_{sparsity}$ in Equation (9) and the caption of Figure 2, but $\\lambda_{card}$ in the derivation in Appendix A.1.\n- In Figure 6 captions: $\\lambda_{sparsity}$ is consistently written as $\\lambda_{sparse}$ in the legend keys in parts (a), (b), and (c). Please make it consistent.\n- In Appendix A.2, the parameters for the prior slab (Eq. 5 revisited in Eq. 17) uses tildes on $\\alpha$'s. The Appendix uses bars on $\\alpha$'s, suggesting slight notational inconsistency between the main text and appendix. Please double check if this is not a typo."}, "questions": {"value": "**Complexity Management**: The proposed method relies heavily on four distinct scheduled parameters ($\\tau_z$, $\\alpha_{lo}$, $\\alpha_{hi}$, and $\\lambda^{(p)}$). Could the authors elaborate on the practical sensitivity of DirMoE to these schedules? Is there a risk that poor schedule tuning leads to instability or poor specialization?\n\n**Efficiency vs. Other Differentiable Routers**: Please provide a comparison of training iteration time and throughput against other recent fully differentiable routers, such as ReMoE or Lory, ideally using the same LLaMA-185M setup or larger models, to fully contextualize the runtime overhead of the Dirichlet VAE structure and implicit reparameterization gradients compared to alternative continuous gating functions.\n\n**Results on O(10B) models**: It would be good to see the method is effective on larger model sizes in the order 10B. Can the authors show zero-shot results at this scale?\n\n**Role of $\\lambda_{sparsity}$**: Given that the Dirichlet concentration $\\lambda$ is theoretically presented as the primary sparsity knob for contribution, what is the minimum necessary role of the explicit cardinality penalty $R_{sparsity}$ (Eq. 9) in achieving a target expected $k$ without compromising model stability or specialization? Does the model fail to converge to sparse selections without $R_{sparsity}$? Figure 2 only shows that lower $\\lambda_{sparsity}$ yields less sparsity compared to desired $1-k/E$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5i7hfnJt4k", "forum": "a15cDnzr6r", "replyto": "a15cDnzr6r", "signatures": ["ICLR.cc/2026/Conference/Submission11538/Reviewer_cmFj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11538/Reviewer_cmFj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761507830020, "cdate": 1761507830020, "tmdate": 1762922631984, "mdate": 1762922631984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel routing mechanism for Mixture-of-Experts (MoE) models called DirMoE. It aims to solve two key problems with standard Top-k+Softmax routing: the non-differentiability of its Top-k operation and its entanglement of two distinct decisions (which experts to select vs. how to weight their contributions). The proposed method explicitly disentangles the routing process into:\n\n1. **Expert Selection:** A Bernoulli distribution determines which experts to activate for a given token.\n    \n2. **Expert Contribution:** A Dirichlet distribution determines the probability weights assigned to the *active* experts.\n    \n\nThe final routing vector is a normalized product of these two components. This entire mechanism is end-to-end differentiable, using Gumbel-Sigmoid for selection and implicit reparameterization for the Dirichlet sampling. The router is trained using a variational ELBO objective, which includes a reconstruction loss, a KL divergence, and a direct sparsity penalty loss. The authors also provide a theoretical justification for a \"calibrated sparsity,\" linking the Dirichlet concentration parameter $\\lambda$ to the expected expert contribution distribution.\n\nEmpirical results on a 185M parameter model show that DirMoE achieves better performance against baselines like Switch MoE and ReMoE. Critically, it also demonstrates significantly improved expert specialization at this scale, while introducing negligible computational overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novelty:** The core idea of disentangling expert selection (Bernoulli/Gumbel) from expert contribution (Dirichlet) is well-motivated and novel. Framing this as a probabilistic spike-and-slab model within a VAE framework is a novel approach to MoE routing.\n    \n2. **Controllable Sparsity:** I really like the theoretical analysis introduced in Section 5, which connects the Dirichlet concentration parameter $\\lambda$ to the expected contribution sparsity. It provides a principled calibrated knob approach for controlling the *contribution sparsity* (i.e., the dispersion of weights among *active* experts), which is a more nuanced control than what is offered by standard temperature scaling in Softmax routing.\n    \n3. **Competitive Empirical Performance and improved specialization (at small scale):** The method achieves results that are competitive with or slightly better than baselines on the 185M model benchmark (Table 2). This demonstrates the viability of the approach in a small-scale setting. The method achieves much clearer expert specialization than the baseline (vividly shown in Figure 5).\n    \n4. **Computational Efficiency:** Despite the apparent complexity of the VAE framework, the authors demonstrate that its computational overhead is negligible (<1% slowdown in iteration time compared to a vanilla MoE, as shown in Table 1). This makes it more practical."}, "weaknesses": {"value": "1. **Clarity of the Training Objective:** The paper's primary weakness is its lack of clarity regarding the overall training objective. The VAE objective (Eq. 8) is presented, but it's not explicitly stated how this loss is combined with the main LM loss. This is a critical detail for reproducibility and understanding.\n    \n2. **Justification of the VAE Objective:** The VAE's reconstruction task—reconstructing the token embedding $x$ from the routing vector $r(x)$—is non-obvious. The authors could provides more intuition and ablation about this design choice. This auxiliary reconstruction task seems not well-justified.\n    \n3. **Model Complexity:** While computationally fast, the DirMoE router is significantly more complex to implement and tune than Top-k routing. It introduces a VAE (encoder, decoder, KL loss) *for every MoE layer*, along with a host of new hyperparameters and schedules (as seen in Table 4). This complexity could be a barrier to adopt this method in large-scale MoE training.\n    \n4. **Limited Ablation and Comparison to Simpler Alternatives:** Following the previous two weaknesses, **my greatest concern** for this submission is the lack of the comprehensive ablations. For example, the VAE framework is a major part of the contribution, but the router is perfectly trainable without it (using only the main LM loss and the $\\mathcal{R}_{sparsity}$ penalty). It's plausible that the method also achieves good performance without the reconstruction loss. The paper lacks a crucial ablation study that removes the VAE objective to prove that this added complexity is necessary. Furthermore, the paper fails to compare against other, potentially simpler, methods that also aim to improve router training. For example, recent work like RMoE [1] has also shown that adding extra gradient paths and information sharing via layer-wise recurrence can be effective, suggesting that DirMoE's highly complex VAE is not the only (or necessarily best) way to solve this problem (though RMoE does not offer the controllability of DirMoE, but it is still unclear whether such controllability will improve or hurt the final performance when scaled up).\n    \n5. **Limited Scale of Experiments:** The experiments are conducted on a 185M parameter model with 8 experts. This is a significant weakness that *directly undermines* the claims of \"strong performance\" and \"improved specialization.\" The primary benefits of MoE are at much larger scales. Crucially, the paper's visual argument for specialization (Figure 5) relies on a small-scale baseline that appears to fail to specialize. Recent work[2] has demonstrated that standard Top-k+Softmax routers *can* also achieve strong specialization when properly trained at scale with appropriate load balancing implementation. This suggests the paper's central claim of superior specialization is not convincingly proven against a strong, scaled baseline. The stability and effectiveness of this complex VAE router at such scales also remain unproven.\n\n[1] https://arxiv.org/abs/2408.06793\n[2] https://arxiv.org/abs/2501.11873"}, "questions": {"value": "1. **Training Objective:** Could the authors please clarify the full training objective for the entire model?\n    \n2. **Justification for VAE:** What is the intuition behind forcing the routing vector $r(x)$ to be able to reconstruct the token embedding $x$? Does this encourage the router to learn a richer representation, or is it primarily a regularization?\n    \n3. **Ablation Study:** What is the performance of a model that removes the VAE objective (reconstruction loss and slab KL) and is trained *only* with the main LM loss and the $\\mathcal{R}_{sparsity}$ penalty from Eq. 9? This would help isolate the contribution of the VAE framework.\n    \n4. **Hyperparameter Sensitivity:** The method introduces many new hyperparameters (Table 4). How sensitive is the model's final performance and specialization to these settings (e.g., the KL weight $\\beta_{\\theta}$, the posterior scale $\\lambda^{(q)}$, and the prior mean mass $m$)?\n    \n5. **Scalability (optional):** Do the authors have any insights or preliminary results on how DirMoE performs at larger scales (e.g., >1B parameters, more training tokens)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jQ0z1XGOba", "forum": "a15cDnzr6r", "replyto": "a15cDnzr6r", "signatures": ["ICLR.cc/2026/Conference/Submission11538/Reviewer_xEic"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11538/Reviewer_xEic"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761682789617, "cdate": 1761682789617, "tmdate": 1762922631259, "mdate": 1762922631259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DirMoE, a probabilistic, fully-differentiable MoE router that explicitly separates (i) expert selection (a Bernoulli “spike” trained via Gumbel-Sigmoid) from (ii) expert contribution (a Dirichlet “slab” sampled with implicit reparameterization). Routing weights are the normalized Hadamard product of the relaxed mask and a Dirichlet draw. Training uses a β-VAE-style ELBO with a sparsity penalty to target ~k active experts; a “sparsity knob” schedules Dirichlet concentration and gate temperature, with a Beta/Simpson-index calibration giving interpretable control over mass on the active set. Experiments (LLaMA-185M, ~30B tokens) show similar training efficiency to Switch and zero-shot gains over several baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "-  **Interpretable sparsity control.** The Beta/Dirichlet calibration + Simpson-index theory offers a principled knob for expected mass on the active set and dispersion, beyond temperature heuristics.\n- **Light systems delta.** Reported iteration time/throughput match Switch under compute parity; the router adds negligible overhead.\n- **Empirical improvements.** On 7 zero-shot tasks, DirMoE slightly outperforms Switch/ReMoE/SparseMixer on average, and shows stronger expert specialization visualizations."}, "weaknesses": {"value": "- **Load-balancing risk.** The router relies on near-binary masks and dispersion calibration without an explicit balancing mechanism; the paper itself notes potential utilization skew.\n- **Limited scale & evaluation breadth.** All results are on a ~185M-param LLaMA with ~30B tokens; there is no evidence at larger scale/more experts or on standard reasoning/coding suites (e.g., MMLU, HumanEval/HellaSwag). External validity for modern LLMs remains unclear.\n- **Questionable reuse of prior results.** Several zero-shot numbers appear copied from ReMoE rather than reproduced under your setup, making them non-comparable and weakening the empirical claim. Please re-run these baselines and report variance.\n- **Added complexity with unclear necessity.** The router introduces multiple moving parts (near-binary masks, dispersion calibration, auxiliary objectives/schedules) without a clear motivation for each. A component-wise ablation is needed to establish necessity versus incidental regularization."}, "questions": {"value": "- With the quadratic penalty on $\\sum \\tilde z - k$, how often do you deviate from exactly k actives? Does this variability harm batching efficiency or increase variance in gradient estimates?\n- For the throughput numbers in Table 1, specify the full systems setup: world size and data/tensor/pipeline/expert parallel configurations (plus capacity factor and drop vs. dropless). Under expert parallelism, does DirMoE exhibit poorer balance and lower throughput?\n- The domain specialization in Figure 5 is interesting. Please detail the protocol (corpora, measurement window...) and explain the observed patterns—for example, why some experts are entirely inactive on certain corpora? (dead experts vs. specialization)\n- Theoretically, Simpson-index control is monotone in $λ$. Empirically, how robust are downstream metrics to mis-set $m/λ$? Please provide heatmaps of $(m, λ)$ vs. zero-shot accuracy and utilization."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mAUsm35Scj", "forum": "a15cDnzr6r", "replyto": "a15cDnzr6r", "signatures": ["ICLR.cc/2026/Conference/Submission11538/Reviewer_qUTF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11538/Reviewer_qUTF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880480360, "cdate": 1761880480360, "tmdate": 1762922630565, "mdate": 1762922630565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}