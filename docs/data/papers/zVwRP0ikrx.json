{"id": "zVwRP0ikrx", "number": 22708, "cdate": 1758334606903, "mdate": 1759896851256, "content": {"title": "Scaling Laws of SignSGD in Linear Regression: When Does It Outperform SGD?", "abstract": "We study scaling laws of signSGD under a power-law random features (PLRF) model that accounts for both feature and target decay.\nWe analyze the expected population risk of a linear model trained with one-pass signSGD on Gaussian-sketched features. \nWe express the risk as a function of model size, training steps, learning rate, and the feature and target decay parameters.\nComparing against the SGD risk analyzed by Paquette et al. (2024), we identify a drift-normalization effect and a noise-reshaping effect unique to signSGD.\nWe then obtain compute-optimal scaling laws under the optimal choice of learning rate. Our analysis shows that the noise-reshaping effect can make the compute-optimal slope of signSGD steeper than that of SGD in regimes where noise is dominant.\nFinally, we observe that a stable-decay schedule—a simplified variant of the widely used warmup-stable-decay (WSD) schedule—further reduces the noise term and sharpens the compute-optimal slope, when feature decay is fast but target decay is slow.", "tldr": "signSGD sharpens compute-optimal scaling in PLRF for noise bottleneck regime.", "keywords": ["scaling laws", "signSGD", "SGD", "compute-optimal curves", "power-law random feature", "stable-decay schedule"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3d13b3f29a133217fb0f5b2d83cebc1351672a19.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the scaling law of SignSGD in linear regression. It shows that the compute-optimal scaling of SignSGD is better than SGD in certain regimes, and the stable-decay schedule can further benefit the scaling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A clear benefit of SignSGD compared with SGD is shown.\n2. The authors made an important attempt towards understanding the learning rate schedule by studying the stable-decay schedule.\n3. Presentation is clear overall."}, "weaknesses": {"value": "1. The assumption of $H$ being diagonal and $(w_*)\\_i$ being $i^{-\\beta}$ can be problematic for SignSGD. We omit the projection $S$ for convenience. In existing works about SGD, assumptions are usually about the eigenvalue spectrum of $H$ as well as the inner product of $w_*$ with the eigenvectors of $H$. Meanwhile, we can assume that $H$ is diagonal to simplify analysis because in SGD, we can apply rotations to $x$ and $w$: If $H$ is not diagonal, then assume that $H=UDU^\\top$ where $U$ is orthogonal and $D$ is diagonal. We can then apply the mapping $x'=U^\\top x$ and $w'=U^\\top x$, then the SGD update is\n$$\nw'\\_{t+1}=U^\\top w_{t+1}=U^\\top(w_t-\\gamma_tg_t)=w_t'-\\gamma_tU^\\top g_t.\n$$\nSince $g_t=x_t(x_t^\\top w_t-y_t)$, we have $U^\\top g_t=x_t'((x_t')^\\top w_t'-y_t)\\coloneqq g'_t$, indicating that the SGD update does not change under rotation. However, this property does not hold for SignSGD because $U^\\top\\mathrm{sign}(g_t)\\neq\\mathrm{sign}(g'_t)$. Hence, we cannot apply rotations to $x$ and $w$ to make the covariance matrix of $x$ diagonal. This is my major concern because intuitively SignSGD can perform better if the model has some \"coordinate-wise\" structure, which is the case when $H$ is diagonal. I will raise my score if a good explanation is made.\n2. In terms of presentation, it can be beneficial to compare the drift and noise terms of signSGD and SGD in a table (with a comparison in each area of Figure 2). It can also be beneficial if the stable-decay structure can be introduced in greater detail, possibly in a separate section.\n3. The stable-decay schedule can be restrictive. It captures the case where the learning rate decays polynomially in the iteration number, but does not include the linear decay schedule (and the similar cosine schedule, both decaying to 0 after a finite number of iterations).\n4. (Minor) Given the existing results in Xiao et al. (2024), the technical contribution is minor. However, the insights of this paper is fruitful.\n\nXiao et al. Exact Risk Curves of signSGD in High-Dimensions: Quantifying Preconditioning and Noise-Compression Effects. 2024."}, "questions": {"value": "1. Does SignSGD improve the exponent in area Ic? Ic is adjacent to IV, and it intersects with Ac and Ad for SignSGD. It seems that SGD and SignSGD performs differently at least in part of Ic.\n2. Does SGD benefit from stable-decay scheduling? If so, it can be unfair to compare singSGD **with** stable-decay scheduling against SGD **without** such scheduling."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VTiEZOGDbh", "forum": "zVwRP0ikrx", "replyto": "zVwRP0ikrx", "signatures": ["ICLR.cc/2026/Conference/Submission22708/Reviewer_5nJx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22708/Reviewer_5nJx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761261082440, "cdate": 1761261082440, "tmdate": 1762942350854, "mdate": 1762942350854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work analyzes the one-pass SignSGD algorithm on a random sketch model $f_{\\theta}(x)=\\langle Sx,\\theta\\rangle$ with $S\\in\\mathbb{R}^{M\\times d}$ a Gaussian matrix with $\\mathcal{N}(0,1/M)$ entries and data generated by a linear model $y_{k}=\\langle w_{\\star},x_{k}\\rangle$, under a power-law assumption: $x_{k}\\sim \\mathcal{N}(0,H)$ with $H_{ij}=i^{-2\\alpha}\\delta_{ij}$ and $w_{\\star,j}=j^{-\\beta}$ with $\\alpha,\\beta\\geq 0$. \n\nLeveraging an ODE description from Xiao et al. 2024, the main contribution is to characterize the scaling behavior of the risk $R \\asymp \\mathfrak{f}^{-\\eta}$ as a function of the number of flops $\\mathfrak{f}=MN$ (model size $\\times$ running time), which is summarized in the diagram in Fig. 2 (left)\n\nThe authors then compare it to one-pass SGD, concluding as long as the learning rate is properly chosen, the SignSGD rates are always equal or better than SGD, depending on the interplay between $\\alpha$ and $\\beta$. This can be understood by comparing the ODE description of these two algorithms, which differ by a noise-reshaping and drift-normalization terms that lead to improved scaling."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "With the surge of interest in scaling laws, the results are timely and of interest to the ICLR community interested in this topic. The fact that the authors can derive different scaling behavior in the $(\\alpha,\\beta)$ plane and characterize the cross-over between them is interesting. Although the paper is technically loaded, the authors make a good job in giving intuitive explanations to the technical steps."}, "weaknesses": {"value": "The primary weakness - shared by much of the literature on “theory of neural scaling laws” for linear models - is the gap between the stated motivation and the actual model studied. The setting considered here is highly idealized in the context of this motivation (linear model, linear feature map, noiseless observations: no feature learning), and it remains unclear to what extent the interesting theoretical results obtained in such a simplified regime meaningfully translate to the more complex models they aim to explain.\n\nClosely related settings have been extensively investigated in the context of kernel methods and their finite-width approximations (e.g., random features, Nyström methods), where the motivation for studying scaling behavior in linear models is arguably more natural. The power-law decay of covariates and target coefficients corresponds to the classical source and capacity conditions in this literature, and has been analyzed in kernel ridge regression (Caponetto and De Vito, 2007; Cui et al., 2021), random-features ridge regression (Rudi and Rosasco, 2017; Bach, 2017; Defilippis et al., 2024), as well as in work more directly related to the present paper on one-pass SGD (Yao et al., 2007; Ying et al., 2008; Carratino et al., 2018) and multi-pass SGD (Pillaud-Vivien et al., 2018).\n\nWhile I recognize that there are substantive differences in terms of motivation, specific parameter regimes ($2\\alpha<1$), and the quantities of interest (compute versus running time), there remains a meaningful overlap in the models and regimes studied. Therefore, the absence of discussion of this prior work — some of which precides Paquette et al. (2024) and Lin et al. (2024) by decades - is an important weakness. I encourage the authors not only to acknowledge this in the introduction and related work but also to compare with the overlapping regime.\n\n- (Caponnetto and De Vito 2007) Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics 7 (3), 331-368.\n- (Cui et al. 2021) Generalization Error Rates in Kernel Regression: The Crossover from the Noiseless to Noisy Regime. NeurIPS 2021\n- (Rudi and Rosasco 2017). Generalization properties of learning with random features.  NeurIPS 2017.\n- (Bach et al. 2017) On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions. JMLR 2017.\n- (Defilippis et al. 2024) Dimension-free deterministic equivalents and scaling laws for random feature regression. NeurIPS 2024\n- (Yao et al. 2007) On Early Stopping in Gradient Descent Learning. Constr Approx 26, 289–315 (2007).\n- (Ying et al. 2008) Online gradient descent learning algorithms. Foundations of Computational Mathematics, 8, 561-596.\n- (Carratino et al. 2018) Learning with SGD and Random Features.  NeurIPS 2018.\n- (Pillaud-Vivien et al. 2018) Statistical optimality of stochastic gradient descent on hard learning problems. NeurIPS 2018.\n- (Berthier et al. 2020) Tight Nonparametric Convergence Rates for Stochastic Gradient Descent under the Noiseless Linear Model. NeurIPS 2020"}, "questions": {"value": "1. The observation that the maximal learning rate leads to a plateau regime is intriguing. Do you have any intuition for why this is the case? How does the maximal and optimal learning rate compare in terms of scaling?  \n\n2. In L305: \n> *We therefore focus on the optimal learning rate $\\gamma_0^\\star$, which maximizes the decay exponent $\\eta$*\n\nIs the optimal learning rate $\\gamma_0^\\star$ maximizing the exponent $\\eta$ also maximizing the risk?\n\n3. This work focus on a noiseless target. Why? Previous work on SGD (Berthier et al. 2020) shows that the noiseless rates can be faster, and in the context of ridge that there can be cross-overs in the rates depending on the interplay between the noise and the regularization (Cui et al. 2021; Defilippis et al. 2024). Do you expect your results to be independent of the noise of to observe something similar here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZMJ9OuOzk0", "forum": "zVwRP0ikrx", "replyto": "zVwRP0ikrx", "signatures": ["ICLR.cc/2026/Conference/Submission22708/Reviewer_AVVQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22708/Reviewer_AVVQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902347394, "cdate": 1761902347394, "tmdate": 1762942350140, "mdate": 1762942350140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a theoretical analysis of the scaling laws for signSGD under the power-law random features (PLRF) model, extending the recent theoretical framework of Paquette et al. (2024) for SGD. The authors derive asymptotic risk formulas and compute-optimal tradeoffs for signSGD, identifying two novel mechanisms—drift-normalization and noise-reshaping—that differentiate its scaling behavior from SGD. They also propose a simplified stable-decay learning-rate schedule (related to warmup–stable–decay) that further improves compute-optimal exponents in certain parameter regimes. Theoretical predictions are validated with numerical simulations.\n\nOverall, this work makes a theoretically interesting extension of the PLRF scaling-law framework to signSGD, introducing two interpretable mechanisms that may explain optimizer-dependent scaling differences. However, it requires improved exposition, stronger intuition, and at least limited empirical grounding on neural networks to reach full impact."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Extending scaling-law theory to optimizers beyond SGD is a natural and important step, especially given that sign-based methods underlie Adam and its variants, which dominate large-scale model training. The derivation of scaling exponents for signSGD is nontrivial, and the introduction of drift-normalization and noise-reshaping as analytic effects is both insightful and original.\n2. The analysis connects optimizer dynamics (sign normalization, step-size scheduling) to compute-optimal scaling exponents, potentially offering insight into why adaptive methods empirically outperform SGD. The paper demonstrates strong technical depth, following a structured progression from stochastic dynamics → ODE approximation → scaling law formulation → compute-optimal tradeoffs. The analysis is self-contained and references relevant recent work (Paquette et al., Lin et al., Xiao et al., etc.).\n3. Experiments (though small-scale) consistently confirm the predicted exponents and qualitative trends from the theory."}, "weaknesses": {"value": "1.The paper’s presentation is algebra-heavy and difficult to parse. While mathematically correct, it often lacks guiding intuition or concrete interpretation of the results in the context of real neural network training. The drift-normalization and noise-reshaping effects, while named, are only partially explained mechanistically.\n2. The experiments are limited to synthetic settings (Gaussian-sketch features) and are primarily confirmatory. No evidence is given that the derived scaling laws hold qualitatively in deep networks, or that the “beneficial areas” (e.g., Area III–IV_sub) correspond to practically relevant regimes.\n3. Although signSGD is an important extension, much of the framework—including PLRF setup, compute-optimal derivation, and phase-plane analysis—is directly inherited from prior work. The primary novelty is analytical substitution of the sign-based update rule into the existing framework.\n4. The paper’s claim that Adam follows the same scaling law “heuristically” (Appendix H) is speculative and unsupported by rigorous derivation. This is a key potential impact point, but it’s treated informally.\n5. The text is dense, with overuse of symbols and nested definitions (e.g., equations (6)–(12)). Several steps (especially in Appendices D–E) are highly opaque. Without significant simplification or visualization, the theoretical results remain inaccessible to a broad audience.\n6. The empirical mapping of where signSGD outperforms SGD in the (α, β)-plane feels ad hoc and under-motivated. The heuristic in Section 5.1 (decay mismatch between target and gradient) is suggestive but not tested quantitatively."}, "questions": {"value": "1. How sensitive are the scaling-law exponents to the Gaussian-sketch assumption? Would structured or learned feature maps change the analysis?\n2. Can the “drift-normalization” and “noise-reshaping” effects be observed empirically in loss/gradient trajectories of real models trained with Adam?\n3. The “stable-decay” schedule is claimed to approximate warmup stable-decay; can this be verified experimentally on a practical model?\n4. How does the asymptotic noise term ( N_{\\text{sign}}(M, \\eta_0) ) compare quantitatively with SGD noise in finite-N settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nulVvK0BRr", "forum": "zVwRP0ikrx", "replyto": "zVwRP0ikrx", "signatures": ["ICLR.cc/2026/Conference/Submission22708/Reviewer_Hokd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22708/Reviewer_Hokd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944458486, "cdate": 1761944458486, "tmdate": 1762942349838, "mdate": 1762942349838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the scaling laws of one-pass signSGD under the power-law random features (PLRF) model with two parameters (feature decay and target decay). It derives a four-term population risk decomposition and an explicit scaling law that depends on model size M, training steps N, and learning rate. By comparing this to SGD, the authors uncover two phenomena: drift-normalization, which accelerates training under conditions,  and noise-reshaping, which yields an N-independent noise floor and additional M dependence. Based on the scaling law, this paper theoretically shows that signSGD achieves a steeper compute-optimal slope than SGD in certain regimes, and optimal learning rate scheduling (a simplified warmup–stable–decay) further improves the slope. There are empirical experiments that confirm the theory."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The theoretical argument is rigorous and sound to me. The four-term risk formula decomposition mirrors prior SGD analyses and uncovers signSGD-specific properties in a clear way. This gives immediate intuition for when signSGD can help.\n* The paper provides further insights, including the comprehensive phase transitions of scaling laws, analysis for compute-optimal slope, and learning rate scheduling that reflects the practice. These analyses serve as a step towards understanding this optimizer in theory. \n* The paper checks predicted slopes and compute-optimal scaling across representative regimes (Appendix C) via synthetic experiments. These empirical results validate and give us confidence in the theory."}, "weaknesses": {"value": "* **Limitations in Setting:** There are certain assumptions in this paper, for example, batch size = 1, diagonal covariance, one-pass training, and Gaussian sketching in the PLRF model, that limit direct generalization to more practical pipelines. The paper acknowledges some limitations such as mini-batching and momentum/Adam invariants, but these still limit the novelty to some extent, and results would be stronger with at least partial extensions (or tests) beyond this PLRF setting.\n\n* **Discussion of Theory:** The structure of the paper could be improved. In particular, the context of the problem and proof ideas occupy too much space, while some useful contents are pushed to the appendices, and this makes the important Section 5 seem rushed. The two main effects, drift normalization and noise reshaping, should definitely be discussed in more detail. For instance, from Lines 413-424 ,the mechanism of signSGD superiority is attributed to an intricate interplay between these two effects that “creates room for a balance”. It is helpful if this could be quantified further. \n\n* **Adam Optimizer:** In the introduction, one claimed relevance of this work is that the state-of-the-art LLMs are trained with the Adam optimizer (rather than SGD), which can be indirectly studied through the more tractable signSGD. Since this is why signSGD scaling law helps connect theory with empirical optimizer choice in practice, there should also be more discussion on Adam in the main text. For now, Adam is not mentioned until in the conclusion, which claims that they also analyze Adam in a heuristic manner in the Appendix (while I do not think we should mention new contents in the conclusion). Given the paper’s motivation, these parts will be selectively included earlier to make the organization less detached, and it would strengthen the case to include at least one non-PLRF synthetic setting for Adam, too. \n\nOverall, I think this paper is sound. My main concerns are in possible preliminary extension from this setting and issues in presentation."}, "questions": {"value": "Based on the phase transitions, can you provide some summary for practitioners to decide (say from coarse empirical estimates of “feature/target decay”) when to try signSGD + stable-decay vs. SGD?\n\nPlease refer to the Weakness section for other questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wxyvfjAHEV", "forum": "zVwRP0ikrx", "replyto": "zVwRP0ikrx", "signatures": ["ICLR.cc/2026/Conference/Submission22708/Reviewer_1qNE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22708/Reviewer_1qNE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988084989, "cdate": 1761988084989, "tmdate": 1762942349587, "mdate": 1762942349587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript builds on the asymptotic characterization of SignSGD for quadratic problems in Xiao et al. (2024) to study its scaling properties, in a manner reminiscent of Paquette et al. (2024). It shows that, in phases where the scaling exponent of SGD is determined by gradient noise, SignSGD can improve the scaling. The benefit is attributed to SignSGD’s ability to balance drift-normalization and noise-reshaping via learning-rate tuning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper derives the scaling exponent for SignSGD, a simplified variant of Adam, and empirically demonstrates that the theoretical predictions also hold for Adam."}, "weaknesses": {"value": "- The technical part of the paper closely follows the ideas in Xiao et al. (2024) and Paquette et al. (2024). \n- As far as I understand, the results in the paper depend on choosing a good learning rate, where the optimal rate depends on both the target and feature exponents. However, the authors do not discuss how to tune the learning rate. Discussing this point—and showing how sensitive the results are to suboptimal choices—would strengthen the paper."}, "questions": {"value": "- As fast as I can follow, there is no label noise in the setting studied. Can you explain why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oCZlpv4gOl", "forum": "zVwRP0ikrx", "replyto": "zVwRP0ikrx", "signatures": ["ICLR.cc/2026/Conference/Submission22708/Reviewer_dH5K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22708/Reviewer_dH5K"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762238239558, "cdate": 1762238239558, "tmdate": 1762942349358, "mdate": 1762942349358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}