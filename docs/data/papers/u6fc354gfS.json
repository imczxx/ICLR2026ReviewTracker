{"id": "u6fc354gfS", "number": 12177, "cdate": 1758206186717, "mdate": 1763675620056, "content": {"title": "Thompson Sampling via Fine-Tuning of LLMs", "abstract": "Bayesian optimization in large unstructured discrete spaces is often hindered by the computational cost of maximizing acquisition functions due to the absence of gradients. We propose a scalable alternative based on Thompson sampling that eliminates the need for acquisition function maximization by directly parameterizing the probability that a candidate yields the maximum reward. Our approach, *Thompson Sampling via Fine-Tuning* (ToSFiT) leverages the prior knowledge embedded in prompt-conditioned large language models, and incrementally adapts them toward the posterior. Theoretically, we derive a novel regret bound for a variational formulation of Thompson Sampling that matches the strong guarantees of its standard counterpart. Our analysis reveals the critical role of careful adaptation to the posterior probability of maximality—a principle that underpins our ToSFiT algorithm. Empirically, we validate our method on three diverse tasks: FAQ response refinement, thermally stable protein search, and quantum circuit design. Within a collection of methods covering Bayesian optimization, reinforcement learning, and evolutionary search, ToSFiT exhibits both state-of-the-art sample efficiency and computational efficiency.", "tldr": "We scale Bayesian optimization to massive discrete spaces using large language models, guided by a novel regret bound we derive for a variational form of Thompson sampling.", "keywords": ["Bayesian optimization", "Thompson Sampling", "discrete domain", "variational Bayesian optimistic sampling", "cumulative regret", "theory", "large language model", "fine-tuning", "probability of maximality", "probability of optimality"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9531feb1c6d5e922992ea35ae58b2e2e34eae3a5.pdf", "supplementary_material": "/attachment/896ae96c47e401b9f764c4d00d215af166d13327.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses Bayesian optimization in large unstructured discrete spaces by proposing ToSFiT (Thompson Sampling via Fine-Tuning), which parameterizes the probability of maximality (PoM) directly using large language models instead of performing expensive acquisition function maximization. The authors establish a novel regret bound for variational Thompson sampling that scales with the maximal information gain rather than the domain size, and demonstrate their approach on three diverse tasks: FAQ refinement, protein design, and quantum circuit optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses a genuine problem: acquisition function maximization in large discrete spaces is prohibitively expensive\n- The combination of pre-trained LLMs with principled Bayesian optimization is timely and well-motivated\n- The improved regret bound is a solid theoretical contribution that advances understanding of VBOS\n- Demonstrates the approach on three genuinely different domains (language, proteins, quantum circuits)"}, "weaknesses": {"value": "-  The performance of TOSFIT is fundamentally tied to the quality of the fixed feature map (i.e., the embeddings from a pre-trained model). If the pre-trained embeddings do not capture the features relevant to the reward function, the GP model may struggle, limiting the overall performance regardless of the policy optimization.\n- Missing comparisons to other recent LLM-based optimization methods (e.g., FIBO is discussed but not compared experimentally), VAE-based methods, or even simpler approaches like evolutionary strategies\n- Missing comparisons to other discrete optimization approaches (e.g., evolutionary methods, MCMC-based optimization)\n- Limited discussion of when you would vs. wouldn't use this method. When is the 19% overhead worth it vs. just generating more candidates?"}, "questions": {"value": "- Why limit POST-GENERATION TS to 1000 candidates? What happens with 10k or 100k candidates with the same computational budget as ToSFiT?\n- All experiments use small models (1.5-1.7B parameters). How does this scale to larger models?\n- How does the method scale to longer sequence generation tasks?\n- When does TOSFIT fail? When the LLM prior is misaligned, is it possible that TOSFIT will perform worse than random search?\n- Have you considered using the LLM to propose multiple diverse candidates per forward pass (e.g., via beam search or sampling), which might improve the gradient estimator?\n- Could you use the GP uncertainty to modulate the learning rate or number of fine-tuning steps adaptively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5hatffnKMv", "forum": "u6fc354gfS", "replyto": "u6fc354gfS", "signatures": ["ICLR.cc/2026/Conference/Submission12177/Reviewer_9Xyb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12177/Reviewer_9Xyb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760884551202, "cdate": 1760884551202, "tmdate": 1762923127977, "mdate": 1762923127977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Rebuttal Global Response"}, "comment": {"value": "We are glad that the reviewers appreciate the timely bridge established between Bayesian optimization and LLM-centric inference [YiCY, 9Xyb], filling an important yet unaddressed gap in the literature [c3Vs, 9Xyb]. We are pleased that they appreciate the theoretical motivation of ToSFiT [UTrs, c3Vs] and value our theoretical contributions [YiCY, UTrs, c3Vs, 9XyB].\n\nWe are glad that reviewer 9Xyb recognizes the diversity of the problems considered for experimental evaluation. Nevertheless, we agree with reviewers [UTrs, YiCY, and 9Xyb] that our limited comparison against only two baselines made it difficult to assess the practical benefit of ToSFiT. We are excited to report that we have extended the empirical analysis to 7 baselines and 2 model sizes per experimental setting. Across all experiments, ToSFiT exhibits state-of-the-art sample efficiency as well as computational efficiency, sometimes by a large margin. The baselines against which we now compare are as follows:\n\n1) Unguided Generation ([1], as before).\n2) Post-Generation TS ([2,3], as before).\n3) Actor Critic ([4], requested by reviewers UTrs and YiCY).\n4) Soft Actor Critic ([5], requested by reviewers UTrs and YiCY).\n5) Fully In-context Bayesian Optimization ([6], suggested by reviewers YiCY and 9Xyb).\n6) LLM-based Evolutionary Search ([7], suggested by reviewer 9Xyb).\n7) Classical Discrete Evolutionary Search ([8], suggested by reviewer 9Xyb).\n\nFurthermore, we now report on the policy diversity during Bayesian optimization (suggested by reviewer UTrs), computational efficiency (suggested by reviewer 9Xyb, YiCY, and c3Vs), and scalability to larger models and compute budgets (suggested by reviewer 9Xyb and c3Vs).\n\nTo assist the reviewers with the updates made to the paper, we have marked all changes with blue font. We will of course revert back to black font upon acceptance.\n\nWe sincerely thank the reviewers for their constructive feedback, which greatly helped improve the paper. We hope that by addressing all of their individual concerns we have convinced them to raise their scores. We are open to discuss any remaining points.\n\n[1] Li, Yujia, et al. \"Competition-level code generation with alphacode.\" Science 378.6624 (2022): 1092-1097.\n\n[2] Kristiadi, Agustinus, et al. \"A sober look at LLMs for material discovery: Are they actually good for Bayesian optimization over molecules?.\" arXiv preprint arXiv:2402.05015 (2024).\n\n[3] Ranković, Bojana, and Philippe Schwaller. \"GOLLuM: Gaussian Process Optimized LLMs--Reframing LLM Finetuning through Bayesian Optimization.\" arXiv preprint arXiv:2504.06265 (2025).\n\n[4] Barto, Andrew G., Richard S. Sutton, and Charles W. Anderson. \"Neuronlike adaptive elements that can solve difficult learning control problems.\" IEEE transactions on systems, man, and cybernetics 5 (2012): 834-846.\n\n[5] Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" International conference on machine learning. Pmlr, 2018.\n\n[6] De Carvalho, Gustavo Sutter Pessurno, et al. \"Simplifying Bayesian Optimization Via In-Context Direct Optimum Sampling.\" arXiv preprint arXiv:2505.23913 (2025).\n\n[7] Romera-Paredes, Bernardino, et al. \"Mathematical discoveries from program search with large language models.\" Nature 625.7995 (2024): 468-475.\n\n[8] Holland, John H. \"Genetic algorithms and adaptation.\" Adaptive control of ill-defined systems. Boston, MA: Springer US, 1984. 317-333."}}, "id": "eYzByJVzfe", "forum": "u6fc354gfS", "replyto": "u6fc354gfS", "signatures": ["ICLR.cc/2026/Conference/Submission12177/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12177/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12177/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763676133986, "cdate": 1763676133986, "tmdate": 1763676133986, "mdate": 1763676133986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses Bayesian optimization in large unstructured discrete spaces by proposing ToSFiT (Thompson Sampling via Fine-Tuning), which parameterizes the probability of maximality (PoM) directly using large language models instead of performing expensive acquisition function maximization. The authors establish a novel regret bound for variational Thompson sampling that scales with the maximal information gain rather than the domain size, and demonstrate their approach on three diverse tasks: FAQ refinement, protein design, and quantum circuit optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses a genuine problem: acquisition function maximization in large discrete spaces is prohibitively expensive\n- The combination of pre-trained LLMs with principled Bayesian optimization is timely and well-motivated\n- The improved regret bound is a solid theoretical contribution that advances understanding of VBOS\n- Demonstrates the approach on three genuinely different domains (language, proteins, quantum circuits)"}, "weaknesses": {"value": "-  The performance of TOSFIT is fundamentally tied to the quality of the fixed feature map (i.e., the embeddings from a pre-trained model). If the pre-trained embeddings do not capture the features relevant to the reward function, the GP model may struggle, limiting the overall performance regardless of the policy optimization.\n- Missing comparisons to other recent LLM-based optimization methods (e.g., FIBO is discussed but not compared experimentally), VAE-based methods, or even simpler approaches like evolutionary strategies\n- Missing comparisons to other discrete optimization approaches (e.g., evolutionary methods, MCMC-based optimization)\n- Limited discussion of when you would vs. wouldn't use this method. When is the 19% overhead worth it vs. just generating more candidates?"}, "questions": {"value": "- Why limit POST-GENERATION TS to 1000 candidates? What happens with 10k or 100k candidates with the same computational budget as ToSFiT?\n- All experiments use small models (1.5-1.7B parameters). How does this scale to larger models?\n- How does the method scale to longer sequence generation tasks?\n- When does TOSFIT fail? When the LLM prior is misaligned, is it possible that TOSFIT will perform worse than random search?\n- Have you considered using the LLM to propose multiple diverse candidates per forward pass (e.g., via beam search or sampling), which might improve the gradient estimator?\n- Could you use the GP uncertainty to modulate the learning rate or number of fine-tuning steps adaptively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5hatffnKMv", "forum": "u6fc354gfS", "replyto": "u6fc354gfS", "signatures": ["ICLR.cc/2026/Conference/Submission12177/Reviewer_9Xyb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12177/Reviewer_9Xyb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760884551202, "cdate": 1760884551202, "tmdate": 1763710052807, "mdate": 1763710052807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TOSFIT, a scalable approach to Bayesian Optimization (BO) over large unstructured discrete domains. Instead of maximizing acquisition functions directly, the method parameterizes the probability of maximality (PoM) using a prompt-conditioned large language model (LLM). TOSFIT adapts the pre-trained policy toward the posterior PoM through online fine-tuning guided by a Variational Bayesian Optimistic Sampling (VBOS) objective. The paper derives new theoretical regret bounds that match those of standard Thompson Sampling (TS) and GP-UCB, extends the analysis to approximate VBOS, and demonstrates empirical gains in frequently-asked-questions (FAQ) response refinement, protein search, and quantum circuit design tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper proposes a new bridge between BO and LLM, replacing acquisition maximization with posterior-aligned fine-tuning. The regret analysis is solid. In addition, TOSFIT consistently outperforms unguided LLM generation and static candidate-based TS."}, "weaknesses": {"value": "1. While theoretically rigorous, some derivations (e.g., the gradient of VBOS and its concavity proof) are dense and may be inaccessible to a broader ICLR audience. A high-level intuition section would improve readability.\n2. The comparison is limited to two relatively weak baselines (unguided LLM and post-generation TS). More direct comparisons to in-context optimization or gradient-based RL fine-tuning (e.g., RLHF or FIBO) would strengthen empirical claims.\n3. It remains unclear whether performance improvements stem primarily from the fine-tuning process or from better use of GP-based uncertainty.\n4. Although the paper claims negligible overhead, it would be helpful to report absolute runtime and fine-tuning cost relative to BO iterations."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i8VgrOF7oK", "forum": "u6fc354gfS", "replyto": "u6fc354gfS", "signatures": ["ICLR.cc/2026/Conference/Submission12177/Reviewer_YiCY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12177/Reviewer_YiCY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761531729582, "cdate": 1761531729582, "tmdate": 1762923127693, "mdate": 1762923127693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the prohibitive cost of Bayesian optimization (BO) in large, unstructured discrete spaces. The authors’ key move is to eliminate expensive acquisition-function maximization over huge domains by directly sampling the next candidate from a parameterized distribution that approximates the posterior probability of maximality (PoM). They instantiate Thompson Sampling as fine-tuning a large language model (tagged TOSFIT) using a prompt-conditioned, pre-trained LLM that is carefully adapted toward PoM. The work builds on Variational Bayesian Optimistic Sampling (VBOS; O’Donoghue & Lattimore, 2021), but particularly strengthens it; they reformulate the analysis to yield a structure-aware regret bound (in terms of information gain) and extend it to approximate VBOS, which directly motivates TOSFIT’s emphasis on pre-trained policy initialization and cautious, PoM-aligned adaptation."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper is well motivated by both the challenges of BO in large discrete settings and the limitations of existing approaches. The authors demonstrate a solid command of the literature.\n\n2. The algorithm is well argued and clearly presented. Although the work builds on VBOS, the authors provide clear theoretical reformulations and insight driven analysis that justify the development of TOSFIT.\n\n3. The experiments show clear performance gains over the chosen baselines.\n\n4. The mathematical assumptions, propositions, and theorems are clearly stated and organized in the main paper and the supplementary material.\n\n5. The scalability of the proposed methodology is well expounded, with clear guidelines for practice and future research directions."}, "weaknesses": {"value": "1. The paper could give a clearer background on the core concept and regret formulations behind VBOS before introducing the intimate connection to UCB. While the adaptive UCB exploration bonus is central to the gradient formulation, the explicit bridge from VBOS to regret bounds could be surfaced earlier. VBOS exact regret bound via the optimistic set appears in Proposition 2, but new readers may struggle without an earlier signpost.\n\n2. Fine tuning uses one VBOS gradient ascent step per round $c = 1$. This keeps cost low, but it raises natural questions about the compute versus quality trade off if more steps are used in complex settings.\n\nMinor comment: The choice of a linear GP is well reasoned, but performance depends on the feature embedding. Using a fixed embedding in the experiments limits generalizability. The authors acknowledge this as future work and outline how to address it, which helps mitigate the concern."}, "questions": {"value": "1. Regarding the choice of $c = 1$ for the fine tuning update, can the authors provide more detail on the compute versus performance trade off?\n2. What guided the selection of baselines? Are there other established methods for combinatorial or discrete BO that were considered but not included? A brief rationale for the baseline set would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "unWRDHN2OG", "forum": "u6fc354gfS", "replyto": "u6fc354gfS", "signatures": ["ICLR.cc/2026/Conference/Submission12177/Reviewer_c3Vs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12177/Reviewer_c3Vs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923094859, "cdate": 1761923094859, "tmdate": 1762923127249, "mdate": 1762923127249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a variational Bayesian optimization surrogate (VBOS) objective that provides a tighter bound for Thompson sampling–based optimization. The method enables gradient-based fine-tuning of generative models under non-differentiable and expensive reward functions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents an interesting and theoretically grounded idea. The VBOS formulation provides a principled way to incorporate uncertainty into generative model fine-tuning for non-differentiable rewards. The theoretical analysis is sound and detailed."}, "weaknesses": {"value": "- GP: The paper relies on the assumption that the underlying reward follows a Gaussian process prior. Will this be held for discrete or highly non-smooth reward functions?\n- Baselines: while the paper focuses on Bayesian optimization–based baselines, it omits comparisons with standard reinforcement learning (RL) methods such as PPO/GRPO, which can also handle non-differentiable rewards to fine-tune LLMs. (Are there any reasons that PPO/GRPO is not able to be used here?) Empirically, I can see VBOS adds an uncertainty term $\\sigma_x$; however, such comparisons would be crucial to demonstrate that the exploration term can contribute beyond simple reward-based fine-tuning.\n- Diversity measure: Reward-based fine-tuning sometimes leads to diversity collapse to a narrow high-reward set. Since the proposed VBOS objective explicitly includes an uncertainty term that should encourage exploration, it would be valuable to report how diversity changes during optimization.\n- Performance convergence: In Figure 4, ToSEiF appears to achieve higher rewards given the same computational cost. However, it is unclear when the fine-tuning process converges. If I have enough resources, when to stop the fine-tuning? Could the authors provide more complete curves, extending beyond the current range, potentially into regions where ToSEiF’s performance even declines (perhaps in the appendix)? This would help readers better understand the convergence behavior and overall stability of the optimization process."}, "questions": {"value": "Following the weakness\n- For the given tasks, if apply RL methods such as PPO/GRPO, what would be the results? How your methods be better than RL? (or how your methods are correlated with RL?)\n- What is the diversity measure during the fine-tuning? Do you have mode-collapse problem?\n- How to select the convergence point, if I have enough computation resources?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MVKuVsgMDJ", "forum": "u6fc354gfS", "replyto": "u6fc354gfS", "signatures": ["ICLR.cc/2026/Conference/Submission12177/Reviewer_UTrs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12177/Reviewer_UTrs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957147825, "cdate": 1761957147825, "tmdate": 1762923126779, "mdate": 1762923126779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}