{"id": "fdjGPp1KPR", "number": 9910, "cdate": 1758148468953, "mdate": 1759897687573, "content": {"title": "Neural Networks with Adaptive Activation Functions and their Application to the Solution of PDEs", "abstract": "We study fully connected neural networks for function approximation, focusing on the role of adaptive activation functions. Recent work has shown that introducing trainable parameters into activation functions, particularly rational functions, can substantially improve network expressiveness. We extend this idea to sampling-based neural networks, which replace backpropagation with forward sampling to achieve faster training. Existing sampling-based methods, however, only employ fixed activation functions, limiting their performance. To address this gap, we develop a computational framework that integrates adaptive activation functions into sampling-based training, enabling direct learning of activation parameters through sampling without gradient-based optimization. Experiments demonstrate that our approach preserves the efficiency of sampling-based methods while significantly improving approximation accuracy, highlighting the benefits of parameterized activation functions in non-gradient training regimes.", "tldr": "", "keywords": ["Sampling-Based Neural Networks", "Approximation", "Adaptive Parameters", "Solution of PDEs", "Rational Function"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9b293509695991d0b7d590b0ef30df60bdddfa46.pdf", "supplementary_material": "/attachment/d14f0d2c94b85a999400b51a33eefdf720ac8872.zip"}, "replies": [{"content": {"summary": {"value": "This work aims to approximate solutions to partial differential equations (PDEs) by fully connected neural networks. The authors propose a method that extends a recent sampling-based approach to replace backpropagation with a forward sampling scheme to accelerate training. The main difference from previous works is the use of adaptive activation functions. The algorithm is applied on a range of solutions to two-dimensional PDEs and achieved competitive mean squared error compared with SWIM with a range of activation functions."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The problem of solving high-dimensional PDEs by neural networks is timely and there is a rich literature on this topic (e.g. physics-informed neural networks, PINNs).\n- The idea of using adaptive activation functions is interesting and has the potential to improve the expressiveness of the neural networks an has been explored for PDEs by Jagtap et al. (2020)."}, "weaknesses": {"value": "- The paper is not well-written and several sections are difficult to follow with not rigorous definitions. See for instance, the paragraph on ground truth dynamics, Eq.~(12) where the parameters are not introduced, or Eq. (17) where the probability distribution is not clearly defined.\n- There is a significant difference between the claims and motivations of the paper and the actual experiments. First, the definition of the numerical task is wrong. Usually, one either solves a PDE by minimizing the residual with a neural network (as in PINNs); otherwise why would we construct a neural network approximation if we already know the solution? Or, one uses pairs of source terms and solutions to train a neural operator to approximation the solution map associated with a PDE to evaluate it at new source terms. However, in the experiments, the authors simply fit a neural network to a known solution without any source term. This is not representative of the actual task of solving PDEs. Second, the experiments are limited to low-dimensional PDEs (1D and 2D), while the main motivation of using neural networks for PDEs is to tackle high-dimensional problems where traditional numerical methods fail.\n- The novelty of the proposed method is limited, as it mainly combines existing techniques (sampling-based training and adaptive activation functions) without significant new contributions."}, "questions": {"value": "- How does the proposed method compare to standard PINNs or other neural operator methods in terms of accuracy and computational efficiency?\n- How does the method scale to high-dimensional PDEs, which is one of the main motivations for using neural networks in this context? In this case, I am worried that the sampling-based approach may become inefficient due to the curse of dimensionality (there would be no intermediate points to sample in the dataset from Eq. (6)).\n- Is there any approximation theory (beyond universal approximation theorem) that supports the use of adaptive SWIM over standard SWIM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YWdzOzCWfM", "forum": "fdjGPp1KPR", "replyto": "fdjGPp1KPR", "signatures": ["ICLR.cc/2026/Conference/Submission9910/Reviewer_w23h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9910/Reviewer_w23h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760695587642, "cdate": 1760695587642, "tmdate": 1762921367332, "mdate": 1762921367332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework that combines sampling-based training (i.e., the SWIM method) with adaptive activation functions (specifically, a modified rational function derived from a variant of the Pade Approximation Unit). The authors’ main goal is to improve the efficiency when approximating functions, with a particular emphasis on solving partial differential equations (PDEs). The method avoids backpropagation by solving a series of small sub-optimization problems (one for each candidate neuron) and then selects neurons based on various probability strategies. A thorough experimental evaluation is provided, comparing a-SWIM with both non-adaptive SWIM variants and standard backpropagation-trained neural networks on several objective functions"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Novel Integration:\nThe paper successfully blends two methodologies—sampling-based training (SWIM) and adaptive activation functions—to create an approach that can potentially combine the efficiency of sampling with the expressive power of trainable activations.\n- Comprehensive Experiments: The experimental evaluation seems extensive. The authors compare a-SWIM not only with various fixed-activation SWIM versions (using ReLU, Tanh, Sigmoid) but also with BP-trained networks. Multiple PDEs and objective functions—covering smooth, discontinuous, and high-frequency cases—are used. The analysis covers both prediction accuracy (MSE) and training time."}, "weaknesses": {"value": "- **The presentation throughout the paper could be improved for clarity.** (i) For example, in Section 3, the formulation of the PDE solution mapping $u(x)$ as a scalar-valued function is not well justified—it would be beneficial to explain how the method could be extended to handle multi-valued function cases or why we just consider scalar-valued functions. (ii) In Section 3 (Background), line 89, the sentence in question appears to delve into experimental specifics. Typically, the background section is reserved for introducing the general problem and relevant context rather than experimental details. It might be more appropriate to relocate this sentence to the experiments section. (iii) Besides, Section 3 assumes a supervised training setting with ground truth training data available; however, for many complex PDEs, such as those without analytical solutions, ground truth data is often not available. This disconnect between the motivation and the chosen methodology requires further clarification. (iv) Section 4 (Methods) is hard to follow. I think it would be better to first briefly recall the methodology of existing methods (SWIM and adaptive neural networks). Next, give the pipeline of your approach (like Fig. 3 the right one). Finally, provide the details of your approach step by step. Otherwise, for example, readers may not understand why you present 4.1 XU-Point Sets first. \n-  **Lack of Theoretical Justification**. While the paper provides a detailed derivation of the method, the theoretical analysis supporting the benefits of adaptive activation functions in a sampling-based regime remains somewhat informal. Concretely, can a generalization bound of your model derived from some results from existing work or what is the approximation capability of the proposed NN model. I understand this is a paper in application theory. However, a more rigorous theoretical discussion or error bound analysis, even if approximate, would strengthen the contribution. \n- **No Scalability Demonstration to Higher-Dimensional Problems**:\nThe experiments in the paper are conducted solely for objective functions with an input dimension of \n$d=2$. This limited scope raises questions about the generality and scalability of the proposed framework. Without experimental evidence or discussion on higher-dimensional scenarios, it remains uncertain whether the method would perform adequately for problems with input dimensions greater than 2—a common occurrence in real-world PDE applications."}, "questions": {"value": "See Weakness Part, and the following are additional questions.\n\n- In 4.1 XU-Point Sets, you chose x-point pairs in eq. (3). What's the relationship of $x^{(s)}$ and $x^{(e)}$ in each pair or should they satisfy some conditions?\n- It would be better to put some key results of the experiments to the main body of the content, which can enhance the expressivity of this paper.\n- the experiments seem to be conducted only for the case where $d=2$. Could the authors elaborate on how the proposed framework is expected to perform or scale when dealing with higher-dimensional problems?\n\nMinor:\n- In eq. (5), there is a typo for $(x_n^{(s)},x^{(1)}_nx^{(2)}_n...)$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No Concerns."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "deDkCmFINe", "forum": "fdjGPp1KPR", "replyto": "fdjGPp1KPR", "signatures": ["ICLR.cc/2026/Conference/Submission9910/Reviewer_NWRf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9910/Reviewer_NWRf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760714137178, "cdate": 1760714137178, "tmdate": 1762921366458, "mdate": 1762921366458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a method for solving PDEs using rational activation functions which are learned and randomly sampled neural networks using a variant of SWIM. My main problem with the paper is that it appears throw together many different unrelated ideas to construct a method. There are also no theoretical contributions in the paper. The resulting method is very complicated and in my opinion unmotivated. All of the experiments in the paper have been moved to the appendix. I would recommend the authors pick one or two experiments which most convincingly show the improvement of their method over other approaches and add these to the main text. For these reasons, I unfortunately recommend the paper in its present form be rejected."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The experiments shown in the appendix do look rather good, but it is difficult to get an overview over what is happening in the 7 different experiments and what they are supposed to be showing."}, "weaknesses": {"value": "The paper lacks theoretical analysis and appears to consists of a very complicated method obtained by combining numerous existing ideas. For this reason, I question the novelty of the paper. I'm not convinced that there are any significant new ideas, although certainly the authors have done a good job engineering a solid method.\n\nThe presentation of the paper is very difficult to understand. Too many different things are combined together in an unmotivated way."}, "questions": {"value": "Why is the domain $\\mathcal{D}$ typically $[-20,20]\\times [0,40]$ when solving PDEs? I suppose these are what you considered in your experiments.\n\nIt is not clear to me how the activation function in equation (16) was obtained. Is this the best activation function for all of the problems you considered?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TSUxdCXfLr", "forum": "fdjGPp1KPR", "replyto": "fdjGPp1KPR", "signatures": ["ICLR.cc/2026/Conference/Submission9910/Reviewer_LyDC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9910/Reviewer_LyDC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761597091874, "cdate": 1761597091874, "tmdate": 1762921365956, "mdate": 1762921365956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}