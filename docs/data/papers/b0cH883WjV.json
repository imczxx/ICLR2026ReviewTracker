{"id": "b0cH883WjV", "number": 16013, "cdate": 1758258618099, "mdate": 1759897267478, "content": {"title": "Conditional Guided Flow Matching: Modeling Prediction Residuals for Enhanced Time Series Forecasting", "abstract": "Time series forecasting predominantly focuses on modeling the mapping between historical and future sequences, and existing improvements are often constrained to optimizing model architectures to better capture this relationship. This essentially reduces prediction residuals to mere optimization targets while overlooking their informative structures such as systematic biases or nontrivial distributions that could otherwise be exploited to directly reduce forecasting errors. Unfortunately, discriminative models struggle to capture the complete residual structure and its dynamic temporal dependencies when applied to residual learning. To fill this gap, we introduce Conditional Guided Flow Matching (CGFM), a novel framework built upon flow matching. CGFM innovatively leverages auxiliary predictions as the source distribution and constructs two-sided conditional paths to prevent path crossing, which enables the explicit learning of the full structure of prediction residuals and thereby theoretically guarantees superior performance over discriminative models. Extensive experiments show that CGFM enhances diverse forecasting models including\nstate-of-the-art ones and demonstrates its effectiveness and generality. Code link: \\url{https://anonymous.4open.science/r/CGFM-31DB}.", "tldr": "", "keywords": ["Time Serires Forecasting", "Flow Matching", "Generative modeling", "Deep Learning"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/16d6642a55126df6e7511797f07165b11e136c63.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Conditional Guided Flow Matching, CGFM, a Flow Matching-based framework that enhances time series forecasting by explicitly modeling the full probabilistic structure of prediction residuals. It addresses the limitation of traditional methods, namely treating residuals as mere optimization targets, via two-sided conditional paths and affine parameterization that ensures non-crossing paths, achieving consistent improvements across diverse base models and datasets."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is easy to understand, with Figure 1 particularly intuitively illustrating the method’s core principle by visualizing residual distribution changes before and after CGFM learning.\n\n2. The work addressing an often-overlooked information source in time series forecasting: unlike traditional methods that treat prediction residuals as passive optimization targets, CGFM explicitly leverages residuals’ informative structures o refine forecasts, with Proposition 4.2 formally linking CGFM to residual distribution learning via Flow Matching and grounding this innovation in rigorous mathematics.\n\n3. The framework exhibits strong model-agnostic flexibility and delivers convincing empirical results: it acts as a universal enhancement module that integrates seamlessly with diverse existing forecasting models. Table 1 validates this with consistent, significant improvements over strong baselines\n\n4. The paper achieves high theoretical rigor and comprehensive experimental design: it provides thorough theoretical analysis, including well-structured propositions and proofs that validate key design choices, while conducting thorough experiments across 7 benchmark datasets using MSE, MAE, and CRPS metrics. Its comprehensive ablation studies and full discussion of computational overhead facilitate a clear understanding of each component’s role."}, "weaknesses": {"value": "1. The paper only compares CGFM with a simple discriminative residual corrector (RLinear-based) and lacks comparisons with advanced state-of-the-art residual modeling methods. This makes it unclear whether CGFM’s advantages stem from its flow matching-based design or merely from the utilization of residual information, failing to isolate the unique value of its core architectural innovations.\n\n2. It remains unknown whether CGFM can still effectively optimize residuals when the source distribution (auxiliary predictions) is drastically misaligned with the target distribution. This gap limits our understanding of the \"lower bound\" of CGFM’s applicability, especially in scenarios where high-quality auxiliary models (that generate source distributions close to the target) are unavailable.\n\n3. Although the framework demonstrates model-agnostic flexibility, it lacks an in-depth investigation into the causes of performance improvement disparities across different base models."}, "questions": {"value": "1. Could the authors supplement experiments comparing CGFM with advanced residual modeling baselines? Such comparisons would help verify whether CGFM’s performance gains are driven by its unique flow matching design rather than the general paradigm of residual utilization.\n\n2. Could the authors design experiments where the auxiliary model generates severely misaligned source distributions (e.g., near-random predictions, predictions with persistent large biases)? Testing CGFM’s residual optimization ability in such scenarios would clarify its \"lower bound\" of applicability and practical robustness.\n\n3. Could the authors conduct additional analyses linking base model architectural characteristics to residual properties?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "icJWACrIKt", "forum": "b0cH883WjV", "replyto": "b0cH883WjV", "signatures": ["ICLR.cc/2026/Conference/Submission16013/Reviewer_knHF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16013/Reviewer_knHF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620792839, "cdate": 1761620792839, "tmdate": 1762926219378, "mdate": 1762926219378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a conditional flow matching framework for time series forecasting. To mitigate the biased results in previous forecasting methods, they design a post-hoc residual correction algorithm with flow matching. They empirically validated the strength of their approach across various settings, including different configurations and datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper investigates residual correction using flow matching.\n- The authors also provide a theoretical framework in the appendix in detail.\n- Their method performs well in various settings and against baselines."}, "weaknesses": {"value": "Please refer to the Questions section."}, "questions": {"value": "- When applying the suggested framework, training both the deterministic forecasting model and the flow matching model is needed. The authors need to provide a specific analysis of the computational burden.\n- If the deterministic forecaster is designed to match the MSE (or maybe the first order, as the authors explained), why does the distribution of the residual in the deterministic forecaster not show an unbiased result in Figure 1 (right) or the real data in Figure 5?\n- In Proposition 4.1, noise smoothing, the addition of Gaussian noise is for smoothness. The reviewer believes that the selection $\\sigma$ is important for training. How do the authors select it? Is it data-dependent or prediction-dependent? Please provide an ablation study for.\n- The reviewer believes that using flow matching for residual prediction in time series forecasting has not been well explored before. However, the idea of using a ‘generative model’ to adjust the guidance of deterministic neural networks is well-developed, both in time series and other domains. The authors should include these references (maybe find more). \n\n[1] CARD: Classification and Regression Diffusion Models, Neurips 22. \n\n[2] TimeBridge: Better Diffusion Prior Design with Bridge Models for Time Series Generation, https://arxiv.org/abs/2408.06672\n\n- Why is the path linearity important for residual prediction?\n- How did the authors design the architecture (A.15) and why?\n- Still, the reviewer has concerns about the improvement; thus, I have two suggestions. First, show improvement over more recent methods (such as methods from recent AI conferences). Second, use a comparison like: BASE | BASE+Diffusion | BASE+Diffusion Bridge | BASE+Flow matching, to compare and demonstrate that the strength actually comes from flow matching.\n- What does Figure 4 represent? Does it show the difference in the residual after using flow matching?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yCh81ZABs8", "forum": "b0cH883WjV", "replyto": "b0cH883WjV", "signatures": ["ICLR.cc/2026/Conference/Submission16013/Reviewer_6JyH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16013/Reviewer_6JyH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706029636, "cdate": 1761706029636, "tmdate": 1762926218689, "mdate": 1762926218689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Conditional Guided Flow Matching (CGFM), a framework that applies flow matching to learn prediction residuals in time series forecasting. CGFM uses an auxiliary model's prediction distribution as the source distribution, instead of guassian distribution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tUsing the auxiliary model's predictions as the source distribution (rather than noise) is intuitive and well-motivated. \n2.\tProvide different target parameterization and loss design, and through experiments, demonstrate the optimal design."}, "weaknesses": {"value": "1. The novelty is not clearly presented. This paper applies flow matching to time series prediction, and the theorems and corollaries in the paper are very similar to flow matching, such as proposition 4.6. \n2. Proposition 4.2 claims are trivial.\nThe claim that CGFM \"equivalently learns residual ε = x₁ - x₀'s probabilistic characteristics\" is trivial. The proof (Appendix A.5) shows that X_t = (α_t + β_t)X₀ + α_t ε, which means the path involves the residual ε. However, this is trivially true for ANY flow matching between X₀ and X₁ = X₀ + ε. This is not a unique property of CGFM—it's just a restatement of the fact that you're transforming from predictions to targets. The \"equivalence\" adds no new insight beyond standard flow matching theory.\n3. The presentation can be improved. \n - The symbol F in Chapter 4.1 represents target future data, but it appears again in the superscript. \n - Many symbols lack explanations. \n - Fig. 1 shows the residual distribution changed after CGFM learning. Why has it changed? What do the icons in Fig. 4 represent?\n4. The experimental evaluation can benefit from a comparison with other residual prediction models."}, "questions": {"value": "1.\tCan you provide a comparison with other residual prediction models?\n2.\tWhat are the technical contributions, except for adopting flow matching?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4NnSFw4Un3", "forum": "b0cH883WjV", "replyto": "b0cH883WjV", "signatures": ["ICLR.cc/2026/Conference/Submission16013/Reviewer_ftDa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16013/Reviewer_ftDa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943979078, "cdate": 1761943979078, "tmdate": 1762926218259, "mdate": 1762926218259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the CGFM method, which aims to model the part of the prediction residuals. The authors also conduct partial validation on several methods in an attempt to demonstrate its effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "S1: The motivation of the paper is reasonable.\n\nS2: The paper provides derivations and theoretical support."}, "weaknesses": {"value": "W1：The literature review in this paper is seriously inadequate, as it omits several important baseline methods in its survey and comparisons, including linear model–based methods (e.g., TQNet [1], CycleNet [2]), TCN-based models (e.g., ModernTCN [3]), RNN-based methods (e.g., SegRNN [4], WITRAN [5], PGN [6]), and Transformer-based methods (e.g., Leddam [7]).\n\n[1] Temporal Query Network for Efficient Multivariate Time Series Forecasting. In Forty-second International Conference on Machine Learning.\n\n[2] CycleNet: Enhancing Time Series Forecasting through Modeling Periodic Patterns. In The Thirty-eighth Annual Conference on Neural Information Processing Systems.\n\n[3] ModernTCN: A modern pure convolution structure for general time series analysis. In The Twelfth International Conference on Learning Representations.\n\n[4] SegRNN: Segment recurrent neural network for long-term time series forecasting.\n\n[5] WITRAN: Water-wave Information Transmission and Recurrent Acceleration Network for Long-range Time Series Forecasting. In Thirty-seventh Annual Conference on Neural Information Processing Systems.\n\n[6] PGN: The RNN's New Successor is Effective for Long-Range Time Series Forecasting. In The Thirty-eighth Annual Conference on Neural Information Processing Systems.\n\n[7] Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-series Dependencies and Intra-series Variations Modeling. In Proceedings of the 41st International Conference on Machine Learning.\n\nW2: The experiments in the paper have several shortcomings: (a) CGFM is only validated on a subset of models, leaving its generalizability unclear and necessitating evaluation on a broader range of models; (b) the paper provides no information about the hyperparameter search space, making it unclear whether the authors tuned hyperparameters on the validation set before reporting results on the test set. Since incorporating CGFM alters the model architecture, the sensitivity of certain common hyperparameters (e.g., d_model, e_layers) may also change.\n\nMoreover, even with identical parameters and random seeds, results can vary across different hardware platforms. To ensure fair and rigorous comparisons, all baseline methods should be run on the same platform, share a sufficiently wide hyperparameter search space, and select the best parameters on the validation set before evaluating on the test set. This approach eliminates the influence of platform and hyperparameter sensitivity on model performance, allowing a fair assessment of CGFM.\n\nW3: The paper lacks a theoretical derivation of CGFM’s complexity. The efficiency experiments also omit detailed settings for shared hyperparameters, making the experimental setup unclear. To fairly compare model efficiency, the effect of hyperparameter scale on results must be eliminated; otherwise, selectively choosing very small parameters for certain tasks to claim high efficiency is highly unrigorous and unfair. The authors need to clarify this issue further."}, "questions": {"value": "Please explain the model's generalization issues and provide details of the relevant experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OsiSEn2mkr", "forum": "b0cH883WjV", "replyto": "b0cH883WjV", "signatures": ["ICLR.cc/2026/Conference/Submission16013/Reviewer_FrWu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16013/Reviewer_FrWu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985616477, "cdate": 1761985616477, "tmdate": 1762926217891, "mdate": 1762926217891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}