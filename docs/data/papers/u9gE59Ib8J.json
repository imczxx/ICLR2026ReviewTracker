{"id": "u9gE59Ib8J", "number": 12620, "cdate": 1758209042983, "mdate": 1759897497833, "content": {"title": "ProMoS: Prototype-Guided Distillation for Generalist Graph Anomaly Detection", "abstract": "Graph anomaly detection (GAD) is crucial in high-stakes domains. Recently, generalist GAD is a type of GAD that trains a single detector and can be transferred to new graphs, and has attracted attention. However, existing methods often rely on scarce and costly annotations for training and sometimes even require few-shot support at inference, which limits their robustness to diverse and unseen anomaly patterns. To address this limitation, we introduce ProMoS, the first unsupervised generalist GAD framework, which detects anomalies by modeling the abundant normality in unlabeled data. Specifically, we introduce a knowledge-distillation (KD) architecture that distills normality representations from a frozen self-supervised graph neural network (GNN) teacher to a mixture-of-students (MoS) model. The MoS employs a shared branch to capture global patterns and a lightweight personalized branch to extract local normality from the teacher, avoiding learning normality from scratch while improving both expressiveness and efficiency. Second, we propose prototype-guided soft-label distillation to align the student with the teacher in a shared prototype space, thereby improving cross-graph transferability and generalizability. During inference, ProMoS performs zero-shot anomaly detection on unseen graphs based on teacher-student distillation bias and prototype geometric deviation. Extensive experiments on eleven zero-shot GAD tasks show that ProMoS consistently outperforms state-of-the-art supervised, unsupervised, and generalist baselines while reducing computational overhead, charting a practical path toward label-free, zero-shot generalist GAD.", "tldr": "", "keywords": ["Graph anomaly detection", "Graph neural network", "Knowledge distillation"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c25142bacafe9b7870936eaf01aa48b7e6a9ca44.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on generalist graph anomaly detection and introduces a method named ProMoS. It is an unsupervised generalist GAD framework that consists of a self-supervised GNN teacher and a mixture-of-students. The model optimization is performed via prototype-guided soft-label distillation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-structured and the code is released.\n\n2. The studied problem is practical and important.\n\n3. Experiments are comprehensive, demonstrating the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The introduction of discrepancy-aware commitment and refinement is not very clear. More detailed descriptions are needed.\n\n2. What does r_i mean in Eq.2?\n\n3. Since UNPrompt and AnomalyGFM are pre-trained on one dataset originally, how are they pre-trained in the proposed setting?\n\n4. The authors are encouraged to provide more analysis or visualizations to demonstrate the effectiveness of the two-branch design."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pfHsizXdLs", "forum": "u9gE59Ib8J", "replyto": "u9gE59Ib8J", "signatures": ["ICLR.cc/2026/Conference/Submission12620/Reviewer_tmaC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12620/Reviewer_tmaC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761371290927, "cdate": 1761371290927, "tmdate": 1762923468375, "mdate": 1762923468375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ProMoS, an unsupervised generalist graph anomaly detection framework capable of zero-shot detection on unseen graphs. Specifically, it first builds a self-supervised GNN teacher and transfers normality representations to a mixture-of-students (MoS) model with local and global branches. Moreover, through prototype-guided soft-label distillation, ProMoS enhances cross-graph generalization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis paper focuses on unsupervised generalist graph anomaly detection, which is a challenging and practical problem. Moreover, the code is released.\n2.\tThe utilization of knowledge distillation and prototypes alignment enhances cross-graph transferability and generalizability.\n3.\tThe proposed method achieves better performance than the used baselines, demonstrating its effectiveness."}, "weaknesses": {"value": "1.\tFor the pre-trained teacher, can it be replaced with other non-SSL methods?\n2.\tDoes the shared branch and personalized branch share the prototypes as it says “share” in Figure 1? Moreover, how are they initialized, from teacher model or the student model?\n3.\tThe authors should provide more analysis as to why Eq.9 could measure the reliability of nodes. For Eq.11, there is no routing regularization term.\n4.\tThe authors use PubMed, Flickr, Questions and YelpChi as the pretraining datasets. Is there a specific reason? If not, what is the performance when using different pretraining datasets.\n5.\tIn Table 5, the authors argue that it reports empirical runtimes. But only the complexity of methods is provided."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MAejfpGntm", "forum": "u9gE59Ib8J", "replyto": "u9gE59Ib8J", "signatures": ["ICLR.cc/2026/Conference/Submission12620/Reviewer_6V1N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12620/Reviewer_6V1N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779542390, "cdate": 1761779542390, "tmdate": 1762923467988, "mdate": 1762923467988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an unsupervised generalist GAD framework, ProMos, which transfers prior knowledge from a pre-trained graph self-supervised learning teacher and introduces MOS to balance expressiveness and efficiency. The framework is jointly optimized using a tailored set of loss functions, including prototype distillation, as well as discrepancy-aware commitment and refinement losses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) This paper is well-motivated and well-written. Distilling knowledge from a pre-trained SSL model is an effective approach that aligns well with intuitive understanding.\n\n(2) The authors propose a fine-grained, prototype-guided method that goes beyond the conventional binary classification of normal and abnormal classes."}, "weaknesses": {"value": "(1) The details regarding the pre-training of the teacher model are not very detailed, particularly how the training across multiple graphs is integrated with the clustering process. There are four graph inputs—do the authors directly merge all nodes from these graphs and then perform clustering on the combined set? \n\n(2) It is also unclear whether the inputs to the student model are identical to those of the teacher model. The distillation process is intended to learn invariant features across different graphs, where the guidance from prototype learning helps the model capture the underlying normal patterns.\n\n(3) Could the authors include a t-SNE visualization for one of the datasets to illustrate the effectiveness of the commitment loss and refinement loss? It would also be helpful to show the difference between the teacher’s and the student’s feature distributions."}, "questions": {"value": "See above **Weaknesses**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g8pF9lffl6", "forum": "u9gE59Ib8J", "replyto": "u9gE59Ib8J", "signatures": ["ICLR.cc/2026/Conference/Submission12620/Reviewer_6gsy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12620/Reviewer_6gsy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829007010, "cdate": 1761829007010, "tmdate": 1762923467560, "mdate": 1762923467560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ProMoS, which aims to perform graph anomaly detection in a general and unsupervised manner. The approach uses a frozen self-supervised GNN as the teacher to provide representations, while a mixture-of-students model learns to capture multiple normality patterns through prototype-based soft supervision and discrepancy-aware refinement. Experiments on several benchmark graphs show that the method achieves strong zero-shot detection performance and demonstrates good generalization across different graph domains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is novel and meaningful, as it considers the diverse anomaly patterns that exist across different graph datasets.\n2. The overall framework and training objectives are clear and well explained.\n3. The figures and layout are well organized and easy to follow, and the writing is clear with comprehensive experiments."}, "weaknesses": {"value": "1. The method does not effectively ensure diversity among student models. Although multiple students are used to capture different modes, no concrete strategy enforces their differentiation.\n2. Some formula notations are incorrect, such as in Eq. (2), where ‘ei’ and ‘ri’ appear inconsistent and likely refer to the same variable.\n3. The ablation study shows limited improvement from modules PB, SB, and DIS, indicating their contributions are not significant.\n4. The Discrepancy-aware Commitment and Refinement stage is somewhat confusing. It is unclear whether simultaneously adjusting the teacher embeddings and prototype vectors might weaken the distinctiveness and effectiveness of the prototypes."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ukn3nRTuqP", "forum": "u9gE59Ib8J", "replyto": "u9gE59Ib8J", "signatures": ["ICLR.cc/2026/Conference/Submission12620/Reviewer_RTkz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12620/Reviewer_RTkz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954416445, "cdate": 1761954416445, "tmdate": 1762923467110, "mdate": 1762923467110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}