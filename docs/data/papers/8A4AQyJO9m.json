{"id": "8A4AQyJO9m", "number": 15218, "cdate": 1758249055333, "mdate": 1763031202773, "content": {"title": "AR4D: Autoregressive 4D Generation from Monocular Videos", "abstract": "Recent advancements in generative models have ignited substantial interest in dynamic 3D content creation (\\textit{i.e.}, 4D generation). Existing approaches primarily rely on Score Distillation Sampling (SDS) to infer novel-view videos, typically leading to issues such as limited diversity, spatial-temporal inconsistency and poor prompt alignment, due to the inherent randomness of SDS. To tackle these problems, we propose AR4D, a novel paradigm for SDS-free 4D generation. Specifically, our paradigm consists of three stages. To begin with, for a monocular video that is either generated or captured, we first utilize pre-trained expert models to create a 3D representation of the first frame, which is further fine-tuned to serve as the canonical space. Subsequently, motivated by the fact that videos happen naturally in an autoregressive manner, we propose to generate each frame's 3D representation based on its previous frame's representation, as this autoregressive generation manner can facilitate  more accurate geometry and motion estimation. Meanwhile, to prevent overfitting during this process, we introduce a progressive view sampling strategy, utilizing priors from pre-trained large-scale 3D reconstruction models. To avoid appearance drift introduced by autoregressive generation, we further incorporate a refinement stage based on a global deformation field and the geometry of each frame’s 3D representation. Extensive experiments have demonstrated that AR4D can achieve state-of-the-art 4D generation without SDS, delivering greater diversity, improved spatial-temporal consistency, and better alignment with input prompts.", "tldr": "", "keywords": ["4D Generation; Monocular Videos"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d03de0f8b664ac3aa25294f960b074f2f783d7f9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an autoregressive 4D generation pipeline from a monocular video without the time-consuming SDS loss.\n\nThe pipeline consists of three stages: 1. It utilizes the LGM to create a 3D representation of the first frame, then 2. It generates each frame’s 3D representation based on its previous frame’s representation. A progressive view sampling strategy is proposed to utilize priors from pretrained large-scale 3D reconstruction models. 3. A refinement stage is incorporated to avoid appearance drift. \n\nExperiments demonstrate the state-of-the-art performance and faster generation speed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to follow. \n2. The proposed autoregressive generation framework, which leverages a pretrained large reconstruction model followed by global refinement, is both interesting and novel. \n3. The extensive experimental results and ablation studies convincingly demonstrate the effectiveness of the proposed submodules and the superior performance of the overall approach."}, "weaknesses": {"value": "1. As a 4D generation task, videos rendered across different time steps and views are crucial for evaluating the fidelity and consistency of the generated 4D objects. However, I could not find the rendered videos in the supplementary materials or on any linked website. If such videos are available, it would be helpful for the authors to provide a clear pointer to them in the rebuttal.\n\n2. The paper currently reports only the overall time cost. A more detailed runtime analysis, breaking down the computational time for each module within the proposed framework, would be valuable for understanding efficiency and identifying potential bottlenecks."}, "questions": {"value": "Theoretically, the proposed 4D generation framework should be applicable to longer input videos. Have the authors conducted any experiments or provided results demonstrating the method’s performance on longer sequences? Including such results would help illustrate the scalability and robustness of the approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Upl3ktGhby", "forum": "8A4AQyJO9m", "replyto": "8A4AQyJO9m", "signatures": ["ICLR.cc/2026/Conference/Submission15218/Reviewer_gyGG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15218/Reviewer_gyGG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798710453, "cdate": 1761798710453, "tmdate": 1762925515804, "mdate": 1762925515804, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "1GvpCNemko", "forum": "8A4AQyJO9m", "replyto": "8A4AQyJO9m", "signatures": ["ICLR.cc/2026/Conference/Submission15218/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15218/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763031202015, "cdate": 1763031202015, "tmdate": 1763031202015, "mdate": 1763031202015, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an autoregressive model for 4D generation from a monocular video. Previous works primarily rely on SDS loss to optimize 3D models, which often produces noticeable artifacts and is time-consuming. This work proposes an SDS-free, three-stage framework for 4D content generation. First, an off-the-shelf 3D generator is used for initialization. Next, an autoregressive model sequentially generates the 4D content, and a subsequent refinement network improves overall quality. Experiments show the proposed method achieves state-of-the-art results."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and easy to follow.\n\nThe progressive view-sampling strategy is efficient and provides useful insights.\n\nThe paper contains no obvious technical flaws."}, "weaknesses": {"value": "- Although the authors claim SDS-free 4D generation, they state that MVDream was used for initialization; MVDream is an SDS-based optimization method. \n\n- The overall framework feels incremental and the three-stage pipeline is complex and heavy, which raises concerns about reproducibility. Will the authors release code and sufficient implementation details? In addition, the baselines are rather old (e.g., SV4D, Consistent4D); please compare with more recent methods such as CAT4D, DreamMesh4D, In-2-4D, FB-4D, and Video4DGen.\n\n- The generated 4D results do not show a clear qualitative improvement over prior work; I do not observe an obvious visual margin compared to earlier methods.\n\n- The quantitative evaluation relies mainly on 2D metrics (PSNR, LPIPS, etc.). For 4D generation, please include temporal / 4D consistency metrics (e.g., multi-view or temporal consistency measures) instead of depending solely on frame-level scores.\n\n- What do the authors mean by “dynamic visualization results”? For 4D generation, please provide dynamic video renderings (continuous sequences), not only static images. I could not find such videos in the supplementary material."}, "questions": {"value": "What is the inference time for a  4D generation process? For example, MVDream reportedly requires about 15 minutes of optimization—please specify whether this is comparable to your method. Provide a detailed breakdown of runtime for each of the three stages (preferably in a table).\n\nMissing some relevant references that should be considered for inclusion:\n- CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models\n- In-2-4D: Inbetweening from Two Single-View Images to 4D Generation\n- FB-4D: Spatial–Temporal Coherent Dynamic 3D Content Generation with Feature Banks\n- Video4DGen: Enhancing Video and 4D Generation through Mutual Optimization\n- Sweetdreamer: Aligning Geometric Priors in 2D Diffusion for Consistent Text-to-3D\n - Richdreamer: A Generalizable Normal–Depth Diffusion Model for Detail Richness in Text-to-3D \n\nPlease add these works to the references and, where relevant, include comparison experiments or discussions.\n\nAs noted in the Strengths and Weaknesses, I am currently inclined to give a borderline-reject score; I would be willing to raise this if the authors address my main concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x3PYcVkFSG", "forum": "8A4AQyJO9m", "replyto": "8A4AQyJO9m", "signatures": ["ICLR.cc/2026/Conference/Submission15218/Reviewer_8Tuf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15218/Reviewer_8Tuf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830729248, "cdate": 1761830729248, "tmdate": 1763004952867, "mdate": 1763004952867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to generation 4D object from monocular video in auto-regressove manner. Their framework consists of three stages:\ninitialization, generation (auto-regressive), refinement (global optimization). During generation, a MLP is optimized for adjacent veiws as local deformation field. Besides reconstruction loss of refernece frames, they leverage the renderings of per-frame generated 3D object as pesudo grouth truths in novel views. They carefully design a progressive view sampling strategy for the novel view training. At last, the optimize a global deformation field. Extensive experiments on public and private dataset demonstrate the effectiveness of the proposed method"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) the paper is well-organized and easy to follow\n\n2) Experimental results indicate the model achieve state-of-the-art performance\n\n3) The auto-regressive manner is novel in 4D generation"}, "weaknesses": {"value": "1) The authors did not clearly explain why the auto-regressive manner leads to accurate motion and geometry in 4D generation. Is it because only minor deformations are learned when optimizing on just two views, and the test data happen to exhibit only small deformations between adjacent views? Without the auto-regressive manner, does the learned global deformation field tend to introduce larger deformations between adjacent views?\n2) The authors use per-frame LGM results as pseudo ground truth for novel views without incorporating any additional supervision signals. However, LGM results are inevitably temporally inconsistent. I wonder how the authors address this temporal inconsistency.\n3) The proposed method introduces auto-regressive generation, yet later requires a global deformation field optimization during the refinement stage, which appears somewhat contradictory.\n4) The proposed method introduces auto-regressive generation, yet later requires a global deformation field optimization during the refinement stage, which appears somewhat contradictory.As a 4D generation work，the authors are encouraged to provide video (both mulit-view and oribit) as supplementary material for better understanding of the performance of this work."}, "questions": {"value": "See weakness. Besides, the authors are encouraged to use Multi-view video generation model results instead of LGM results as pesudo views to enhance the results. (I wonder why the authors didn't use those models)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GJUqGgU0Gg", "forum": "8A4AQyJO9m", "replyto": "8A4AQyJO9m", "signatures": ["ICLR.cc/2026/Conference/Submission15218/Reviewer_4RqS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15218/Reviewer_4RqS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916489132, "cdate": 1761916489132, "tmdate": 1762925514454, "mdate": 1762925514454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AR4D, a three-stage, SDS-free pipeline for lifting a monocular video to a dynamic 3D Gaussian (4D) sequence:\n\n1. Initialization: generate orthogonal views of the first frame (e.g., via MVDream), reconstruct a 3D Gaussian, then fine-tune only positions/colors to better match the reference while keeping geometry fixed as the canonical space. \n\n2. Generation: autoregressive per-frame local deformation fields (one MLP per adjacent frame pair) instead of a global field, trained against the next video frame. \n\n3. Progressive view sampling (PVS): periodically render orthogonal views of the current estimate, reconstruct pseudo 3D with LGM, and impose RGB+depth consistency from gradually widening azimuth ranges to curb overfitting. \n\n4. Refinement: a global deformation field from the canonical frame, constrained by per-frame depth to reduce appearance drift while preserving geometry. \n\nExperiments follow STAG4D protocols across ~50 scenes for text-to-4D and video-to-4D, and the Consistent4D 8-scene benchmark, reporting PSNR/SSIM/LPIPS on chosen views plus CLIP-S and FVD, showing favorable results over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The method does not require SDS, which is an interesting departure from other methods. Since SDS promotes mode-seeking, the generated results of SDS-free methods often contain more detail.\n\n- Extensive experiments and visualizations demonstrate the performance improvements over baselines.\n\n- Ablations clearly show the effects of each proposed component on the final performance."}, "weaknesses": {"value": "- No user study or pairwise preference test is presented. For a generative paper, even a small-scale blind study would substantiate perceptual claims beyond LPIPS/FVD/CLIP-S.\n\n- It seems that this method would not work on multi-objects due to the choice of pretrained priors. Limitations note dependence on current reconstructors; multi-object robustness isn’t demonstrated. Add experiments on multi-object scenes or state this as a limitation.\n\n- The paper repeatedly attributes SDS methods’ limited diversity to SDS randomness, and claims AR4D offers \"greater diversity\", but the pipeline appears largely deterministic once the first-frame 3D is fixed. Appendix A even suggests retrying MVDream until satisfactory, which is not a controlled diversity mechanism.\n\n- No hyperparameter sensitivity or robustness analyses for PVS.\n\n- There are many recent papers relevant to this task like Vidu4D, SV4D, 4Diffusion, STP4D, Splat4D. A brief discussion or comparison with these methods is greatly preferred."}, "questions": {"value": "Please see the weakness section. In addition, I have the following question:\n\n- I see that the authors report the peak VRAM (30–40GB) usage; however, the memory vs. number of frames scaling is not shown. I am curious about the scalability of this method to longer videos."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PCBlplptR8", "forum": "8A4AQyJO9m", "replyto": "8A4AQyJO9m", "signatures": ["ICLR.cc/2026/Conference/Submission15218/Reviewer_wgAr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15218/Reviewer_wgAr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946549913, "cdate": 1761946549913, "tmdate": 1762925514003, "mdate": 1762925514003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}