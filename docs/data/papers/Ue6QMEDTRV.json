{"id": "Ue6QMEDTRV", "number": 24980, "cdate": 1758362749415, "mdate": 1759896739492, "content": {"title": "ExaGPT: Example-Based Machine-Generated Text Detection for Human Interpretability", "abstract": "Detecting texts generated by Large Language Models (LLMs) could cause grave mistakes due to incorrect decisions, such as undermining student's academic dignity. LLM text detection thus needs to ensure the interpretability of the decision, which can help users judge how reliably correct its prediction is. When humans verify whether a text is human-written or LLM-generated, they intuitively investigate with which of them it shares more similar spans. However, existing interpretable detectors are not aligned with the human decision-making process and fail to offer evidence that users easily understand. To bridge this gap, we introduce ExaGPT, an interpretable detection approach grounded in the human decision-making process for verifying the origin of a text. ExaGPT identifies a text by checking whether it shares more similar spans with human-written vs. with LLM-generated texts from a datastore. This approach can provide similar span examples that contribute to the decision for each span in the text as evidence. Our human evaluation demonstrates that providing similar span examples contributes more effectively to judging the correctness of the decision than existing interpretable methods. Moreover, extensive experiments in four domains and three generators show that ExaGPT massively outperforms prior powerful detectors by up to +37.0 points of accuracy at a false positive rate of 1%. We will release our code after acceptance.", "tldr": "", "keywords": ["Machine-generated Text Detection", "Human Interpretability"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b046bb70057c47e1abe44be54b0ef3eb6149f760.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper  introduces an interpretable detection method that mimics human reasoning when distinguishing between human-written and LLM-generated text. Instead of relying solely on model predictions, ExaGPT compares each span (n-gram) in a target text with similar spans retrieved from a labeled datastore of human and machine examples using k-nearest neighbor search in a shared embedding space (BERT-large). It then uses a dynamic programming algorithm to segment the text into spans that balance length and reliability, providing retrieved examples as transparent evidence for each span."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well organized and easy to follow\n2. This is a timely and important topic\n3. The paper also did a human evaluation study"}, "weaknesses": {"value": "1. Datastore dependence is a key vulnerability of ExaGPT. Given that the method relies on retrieving similar spans from a labeled datastore of human-written and LLM-generated texts, both its performance and interpretability are heavily tied to the coverage and quality of this datastore. When the retained instances fail to capture the linguistic variation, writing style, or domain of the target text adequately, the model can pull in irrelevant or misleading spans—leading not only to compromised detection accuracy but also to confusing or uninformative evidence for users. This dependence de facto poses a domain adaptation problem: ExaGPT is ideal when the target text distribution resembles the datastore's, but whether it generalizes to new or niche domains (e.g., scientific abstracts, social media, or non-English data) is uncertain. Moreover, ensuring and curating such a balanced datastore across evolving LLM architectures and writing fashions may grow increasingly time-demanding, possibly limiting ExaGPT's applicability to dynamic real-world environments.\n2. The human study is small and biased toward NLP experts, so it’s unclear how interpretable the results would be to non-expert end users. The paper can also include some people without knowledge about NLP for the human study.\n3. The method introduces several hyperparameters (span length range, α interpolation, number of neighbors) and I believe this may need dataset-specific tuning or LLM-specific tuning."}, "questions": {"value": "1. How does ExaGPT perform when applied to texts from domains or styles not covered in the datastore? Have you explored domain adaptation or datastore augmentation strategies?\n\n2. The model relies on static BERT-large embeddings for span representation. Would using more recent contextual or instruction-tuned embeddings improve both semantic retrieval and interpretability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w6ZQ7NXjW1", "forum": "Ue6QMEDTRV", "replyto": "Ue6QMEDTRV", "signatures": ["ICLR.cc/2026/Conference/Submission24980/Reviewer_Eb5c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24980/Reviewer_Eb5c"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704004588, "cdate": 1761704004588, "tmdate": 1762943271727, "mdate": 1762943271727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ExaGPT, an interpretable AI-generated text detection framework that predicts the score by referencing the k-nearest similar texts in a pre-built datastore. The framework consists of two main stages: span scoring with KNN search and span selection using a dynamic programming algorithm. These stages enable ExaGPT to identify optimal text segmentations and provide interpretable examples for each span, enhancing human understanding of detection results. Evaluations across four text domains and three baseline detectors/generators show comparable detection accuracy of ExaGPT, while human evaluation indicates its higher interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. ExaGPT enhances interpretability by linking predictions to concrete example spans retrieved from a datastore, offering human-understandable evidence.\n2. The proposed method is almost training-free, simplifying deployment in the real-world.\n3. The authors provide a visualization interface, improving user accessibility and practical interpretability."}, "weaknesses": {"value": "1. The paper lacks sufficient ablation studies. The influence of factors such as the maximum n-gram size and the choice of text embedding model should be analyzed to understand their effects on detection performance.\n2. The method may involve multiple computationally intensive components, including embedding generation, KNN search, and DP-based segmentation, which may significantly increase prediction time. The authors should present an efficiency analysis comparing runtime/throughput against baseline methods.\n3. In Figure 3, several retrieved spans appear semantically and lexically different from the target span, reducing interpretability from a human's perspective.\n4. The selection of human annotators for the interpretability study raises bias concerns. If the annotators were authors, collaborators, or familiar with the dataset, the evaluation may lack impartiality. A more credible approach would include independent participants or non-expert annotators to better assess usability for the general public.\n5. Since the method's core detection capability relies heavily on the datastore's quality, it may perform poorly in cross-domain or cross-model scenarios. The authors should examine ExaGPT's robustness when dealing with paraphrased input, input from an unseen LLM or an unknown text topic/domain.\n6. The paper lacks comparison with recent detection baselines, such as [1–3], and should evaluate against more diverse generators as provided in the original M4 dataset.\n\n[1] Guo, Xun, et al. \"Detective: Detecting ai-generated text via multi-level contrastive learning.\" NeurIPS. 2024.\n\n[2] Guo, Hanxi, et al. \"Biscope: Ai-generated text detection by checking memorization of preceding tokens.\" NeurIPS. 2024.\n\n[3] Hans, Abhimanyu, et al. \"Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text.\" ICML. 2024."}, "questions": {"value": "Please refer to my questions listed in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The paper incorporates human annotation results as part of its primary evaluation but does not provide sufficient details on how these studies were conducted. In particular, it is unclear whether the annotation procedures complied with applicable local laws and institutional research-ethics requirements (e.g., IRB or equivalent). Given these uncertainties, I recommend flagging this submission for an ethics review to ensure the human-subjects components meet appropriate ethical and legal standards."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yPEhTIpw8N", "forum": "Ue6QMEDTRV", "replyto": "Ue6QMEDTRV", "signatures": ["ICLR.cc/2026/Conference/Submission24980/Reviewer_Agqe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24980/Reviewer_Agqe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761760804436, "cdate": 1761760804436, "tmdate": 1762943271535, "mdate": 1762943271535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ExaGPT, an interpretable LLM-generated text detection method that identifies texts by comparing them against a datastore of human-written and LLM-generated spans. The approach uses k-NN retrieval to find similar spans and applies dynamic programming to optimize span segmentation. The method provides similar span examples as interpretable evidence. Experiments across four domains and three generators show ExaGPT outperforms baseline interpretable detectors by up to +37.0 accuracy points at 1% FPR, while achieving 61.5% accuracy in human evaluation of interpretability (vs. 47.9-57.3% for baselines)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a novel approach that connects human decision-making processes to technical design. The k-NN span matching with dynamic programming for LLM detection is creative and well-motivated. \n\nThe experimental design is solid, with meaningful human evaluation that directly measures interpretability through judgment accuracy. Testing across multiple domains and generators demonstrates robustness. The focus on 1% FPR addresses real-world needs where false positives have serious consequences. \n\nThe paper is well-structured with clear explanations and helpful visualizations. The substantial performance gains and superior interpretability make this a valuable contribution to high-stakes detection scenarios."}, "weaknesses": {"value": "1. The human evaluation involves only four annotators who are all NLP experts, so the results may not generalize to typical users. The sample size of 96 texts per detector is relatively small for drawing strong conclusions about interpretability.\n\n2. The paper does not discuss how ExaGPT handles adversarial attacks or paraphrased texts. If an attacker modifies LLM outputs to avoid matching spans in the datastore, the detection may fail.\n\n3. The datastore needs continuous updates as new LLMs emerge. The paper does not address the maintenance cost or how to handle mixed-authorship texts where humans edit LLM outputs.\n\n4. The evaluation dataset is limited to English text across four domains. The paper lacks experiments on multilingual datasets or code generation, which are important use cases for LLM detection. This narrow scope limits understanding of the method's broader applicability.\n\n5. While the paper focuses on interpretable baselines, it would be valuable to compare against state-of-the-art non-interpretable detectors to understand the performance trade-off between interpretability and accuracy. This would help justify whether the interpretability gains are worth potential performance costs, eg, BiScope, etc.\n\n6. The paper does not evaluate cross-domain generalization. All experiments use domain-specific datastores, but real-world scenarios often require detecting texts from unseen domains. Testing how ExaGPT performs when the datastore domain differs from the test domain would strengthen the evaluation."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "a2pzQzj5or", "forum": "Ue6QMEDTRV", "replyto": "Ue6QMEDTRV", "signatures": ["ICLR.cc/2026/Conference/Submission24980/Reviewer_fgzx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24980/Reviewer_fgzx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796527228, "cdate": 1761796527228, "tmdate": 1762943271330, "mdate": 1762943271330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ExaGPT, a method for detecting AI-generated text. The core idea is to determine the source of a given text by retrieving similar text fragments from a database containing both human-written and AI-generated examples. This retrieval process also serves as an interpretable form of evidence for the final judgment. The authors conduct experiments to show that the proposed method performs well in terms of both detection accuracy on specific benchmarks and interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Novel Approach: The application of instance-based retrieval to the problem of AI-generated text detection, with a focus on enhancing interpretability, is a novel and interesting direction.\n\nEmphasis on Interpretability: The paper's focus on improving user trust in detection results is a significant contribution, as this is a critical factor for the practical adoption of AI detection tools."}, "weaknesses": {"value": "The primary weakness of the evaluation lies in its apparent focus on in-distribution settings, which raises concerns about the method's generalization capabilities. As I understand it, the experiments are conducted using text from AI models and domains that are already represented in the retrieval database. This setup does not address the more challenging and realistic scenario where the detector must face a completely new AI model (e.g., a future proprietary or open-source LLM) or content from a novel domain not present in its database. The analogy to plagiarism detection, which must often handle content from unknown sources, seems particularly pertinent here. Without evidence of out-of-distribution robustness, it is difficult to assess the practical scope and reliability of the proposed work in a constantly evolving AI landscape."}, "questions": {"value": "I have a few questions regarding the methodology and evaluation. I would appreciate it if the authors could provide some clarification, and please correct me if my understanding is mistaken.\n\nRegarding the Choice of Text Representation: The paper mentions using the output of BERT's second layer for generating text fragment representations. This strikes me as a somewhat unconventional choice, as later layers are often used for more semantic-rich embeddings. Could the authors provide a brief explanation for this design choice over, for instance, deeper or final layers? A simple ablation study or reference to prior work justifying this choice would be very helpful to clarify the motivation.\n\nRegarding Practical Considerations: The authors acknowledge the computational cost in the appendix. I am curious about the practical challenges of deploying such a large-scale retrieval-based method in a real-world, low-latency setting. Have the authors considered or explored potential optimization strategies (e.g., hierarchical navigation, or other approximate nearest neighbor techniques) to balance the trade-off between detection effectiveness and computational efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "nWAnW1m2X1", "forum": "Ue6QMEDTRV", "replyto": "Ue6QMEDTRV", "signatures": ["ICLR.cc/2026/Conference/Submission24980/Reviewer_zfmy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24980/Reviewer_zfmy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908828464, "cdate": 1761908828464, "tmdate": 1762943271059, "mdate": 1762943271059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}