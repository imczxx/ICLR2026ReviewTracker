{"id": "W63sPIQPz4", "number": 20590, "cdate": 1758308036574, "mdate": 1759896969128, "content": {"title": "Learn to change the world: Multi-level reinforcement learning with model-changing actions", "abstract": "Reinforcement learning usually assumes a given or sometimes even fixed environment in which an agent seeks an optimal policy to maximize its long-term discounted reward. In contrast, we consider agents that are not limited to passive adaptations: they instead have model-changing actions that actively modify the RL model of world dynamics itself. Reconfiguring the underlying transition processes can potentially increase the agents' rewards. Motivated by this setting, we introduce the multi-layer configurable time-varying  Markov decision process (MCTVMDP). In an MCTVMDP, the lower-level MDP has a non-stationary transition function that is configurable through upper-level model-changing actions. The agent's objective consists of two parts: Optimize the configuration policies in the upper-level MDP and optimize the primitive action policies in the lower-level MDP to jointly improve its expected long-term reward.", "tldr": "Construct a multi-layer MDP system to configurate the RL agent's environment", "keywords": ["Reinforcement learning; Markov decision process; Robust learning; Optimization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d398ca9083ea3ec767b9298a75710075df7aeaa4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper formalizes reinforcement learning with model-changing actions—actions that modify the environment’s dynamics—and introduces multi-level configurable MDPs where an upper-level MDP selects (or configures) the lower-level transition kernel, and a special time-variant configurable MDP (TVCMDP) with continuous configuration under a cost budget. Theoretical pieces include a linearization of value w.r.t. transition kernels, a convex surrogate for TVCMDP with exponential configuration costs, and error bounds for bi-level estimation and configuration uncertainty. Algorithms include a model-based bi-level value iteration procedure that alternates solving lower-level MDPs and an upper-level configuration MDP. Experiments on a synthetic TVCMDP, a synthetic bi-level MDP, and two standard tasks—Cartpole and Gridworld/Block-world—demonstrate that learned configuration policies can outperform non-configuring or random-configuring baselines and move performance closer to an “oracle” fixed-best kernel."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper makes the idea of environment-reconfiguration actionable by cleanly separating configuration (upper level) from control (lower level) and by treating the lower-level transition kernel itself as the state of the upper level—an uncommon but crisp abstraction; it provides a tractable convex approximation for TVCMDP via a linearization of the value function and a budgeted exponential cost, along with bi-level error bounds tying configuration/estimation deviations (δc, δg, Δ) to value degradation; the algorithmic template (solve lower-level values/policies, then plan over kernels) is simple and broadly applicable; and the case studies (Cartpole with discrete parameterizations; Block-world with slip parameter; synthetic TVCMDP/bi-level) collectively show that configuration can materially improve returns over non-configuring or random policies and approach an oracle that fixes the best kernel."}, "weaknesses": {"value": "1. Empirical scope and baselines are limited for claims about generality and efficiency: the upper-level uses value iteration (Cartpole) or DQN (Block-world) over small, discretized configuration spaces, and lower-level policies are DQN or value iteration; there is no comparison to known configurable-MDP solvers or modern bilevel/meta-RL approaches, making it hard to attribute gains to the proposed formulation rather than to problem simplicity. \n\n2. Assumptions in theory are strong and somewhat idealized: known reward functions and the ability to estimate discrete kernel sets for the upper level; while the error bounds are clean, experiments do not probe sensitivity to misspecification or to continuous/large kernel spaces. \n\n\n3. Evaluation design can blur configuration power with training protocol advantages: for Cartpole the upper level deterministically switches among four handcrafted environments and evaluates over only 20 episodes per setting; Block-world precomputes rewards via offline VI and discretizes $\\alpha$ into 1000 points, raising questions about scalability and online sample efficiency when such precomputation is infeasible. \n\n\n4. Problem framing relative to prior configurable-MDP work could be sharper: while the paper argues that “the lower-level kernel as upper-level state” is novel, the empirical section does not directly compare to CMDP baselines (e.g., gradient or Stackelberg formulations) on the same tasks to evidence distinct benefits of an explicit upper-level MDP abstraction."}, "questions": {"value": "1. For TVCMDP, how sensitive are solutions to $\\alpha, \\beta$, and budget $B$ in the exponential cost, and where does the linearization break (e.g., $‖x‖$ bounds), empirically? \n\n\n2. How does performance scale when the configuration state space grows (e.g., $>4$ Cartpole parameter sets or multi-parameter continuous kernels)? \n\n\n3. Could you report wall-clock, env-step budgets, and seeds for upper- and lower-level training, and provide return-vs-time curves to separate algorithmic from engineering speedups?\n\n4. In the synthetic bi-level study, can you vary $\\delta_c$, $\\delta_g$, and $\\Delta$ to empirically validate the proved error bounds (Lemmas 2–3) and show slopes consistent with theory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TwCt78vtpD", "forum": "W63sPIQPz4", "replyto": "W63sPIQPz4", "signatures": ["ICLR.cc/2026/Conference/Submission20590/Reviewer_jxV9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20590/Reviewer_jxV9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838901067, "cdate": 1761838901067, "tmdate": 1762934000749, "mdate": 1762934000749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the multi-layer configurable time-varying Markov decision process (MCTVMDP) with model-changing actions that actively modify the RL model of world dynamics itself. The new RL framework proposed in this paper has the potential and mechanism of breaking out of the current model, through actions that change or improve the underlying MDP. The authors also study special cases of the proposed RL mechanism with model-changing actions; they consider configurable RL for time-varying environments and multi-level (including bi-level) environment-changing RLs. They propose a multi-level value iteration algorithm for solving multi-level configurable RL problems. Lastly, there are some experiments done in Cartpole and Block-world environments to conclude that the proposed framework is adaptable to more complex and continuous RL environments and to testify to the feasibility of the bilevel configurable MDP model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Although the notion of configurable MDP (CMDP) has previously been proposed, the main strengths can be summarized as follows:\n1. The primary contribution of this research lies in the study of multi-layer configurable MDPs. \n2. The authors employ upper-level MDP abstractions/learning to model/improve configuration actions. \n3. Additionally, they deal with time-varying non-stationary lower-level MDPs (time-varying transition kernels), which were not investigated by these previous works.\n4. They introduce a model-based value iteration algorithm."}, "weaknesses": {"value": "Some questions and concerns must be addressed for both theory and experiments. \n1. How do the upper-level model and lower-level model connect?\n2. On what basis does the agent select actions? \n3. In the Bi-level model-based value iteration algorithm, how much time does it take to converge to the optimal policy?\n4. How would your bi-level model-based value iteration algorithm’s performance change with different environments and different algorithms other than the DQN?\n5. Why did you specifically choose to employ the DQN algorithm?\nOn page 18,  figure 3, why is the performance of Oracle (blue bar) closest to the performance of the bi-level MDP configuration?\n\nMinor comments:\n1. On page 9, “We present the configurable Carpole”, should be Cartpole.\n2. On page 5, section 3, line 1: Should be TVCMDP instead of TVCDMP."}, "questions": {"value": "I would highly recommend including a conclusion that summarizes the main notion and contributions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0ljEBWXAR5", "forum": "W63sPIQPz4", "replyto": "W63sPIQPz4", "signatures": ["ICLR.cc/2026/Conference/Submission20590/Reviewer_7Tap"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20590/Reviewer_7Tap"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874119780, "cdate": 1761874119780, "tmdate": 1762934000210, "mdate": 1762934000210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a MDP \"inception\" framework: a two-level process where the upper-level MDP action picks a configuration that affects the transition kernel of the lower-level MDP. Rewards of the upper level MDP are defined as the expected return of the next episode of the lower-level optimal policy under the chosen configuration after substracting a cost for \"changing the world\": the configuration is picked by perturbing previous transition kernel and the cost function reflects the price of the perturbation (chosen as exponential function).\nThe authors also provide a first-order value linearization to turn continuous kernel tweaks into a convex program, as well as estimation and propagation error bounds (based on classical results from RL). The experiments use the famous cartpole environment as toy example to validate the theoretical claims.\nThe related work is addressed, especially Configurable Markov Decision Processes, which is probably the most relevant. \"Contextual Bilevel Reinforcement Learning for Incentive Alignment\" also presents similar ideas, but in the case where the \"world changes\" are not controlled by the upper-level MDP."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors claim novelty relative to configurable RL by targeting time-varying, non-stationary RL and continuous configuration. While prior work (as stated in the summary) also addresses similar settings, the lower–upper decoupling perhaps allows a clearer formulation of estimation and error bounds by leveraging well-established results from tabular RL. The writing is clear and easy to follow. The main idea can be understood even by readers with limited prior exposure to reinforcement learning."}, "weaknesses": {"value": "While the proposed two-level formulation offers an appealing perspective and an intuitive conceptual separation, the same setting can be expressed within the standard Markov Decision Process (MDP) framework: this can be achieved by augmenting the state and action spaces such that the action space becomes the Cartesian product of the configuration and the lower-level action spaces. In such formulation, the configuration component is selected once at the beginning of each episode, while the lower-level actions are kept as is. The state space remains unchanged, but the configuration affects the transition dynamics; consequently, the overall process can be represented by a single transition kernel conditioned on the joint action pair (configuration, action). This perspective implies that the proposed hierarchical structure may be viewed as a reformulation of an augmented flat MDP, rather than a fundamentally new class of decision process.\n\nWe also raise concerns regarding the level of academic rigor in the paper’s presentation. The introduction, while comprehensive, often reads as overly verbose and could benefit from greater synthesis. For example, the second and third paragraphs substantially overlap in content and could be merged, and the sporadic use of quotation marks around common reinforcement learning terminology (e.g., “MDP,” “infinity,” “infinite-horizon”) yields a conversational tone, which should be avoided in a conference submission. Adopting a more precise and concise tone throughout would significantly enhance the professionalism and readability of the paper.\n\nWhile the CartPole simulations are not sufficient indicators, we believe that the main flaw of this paper lies in the optimization objective:\n- The authors used a linearization $V_\\pi(P^\\pi + x) \\approx N + MxN$, with $M = \\gamma (I - \\gamma P_\\pi)^{-1}$ and $N = (I - \\gamma P_\\pi)^{-1} r_\\pi$ for a fixed $\\pi$, so a more accurate notation would be $N^\\pi + M^\\pi x N^\\pi$.\n- Accordingly, the objective becomes $\\max_{x_k} \\max_{\\pi} N^\\pi + M^\\pi x N^\\pi$, and $N^\\pi$ can no longer be omitted, contrary to the claim in the paper.\n- Moreover, the objective is no longer convex, at least in the general case, as it is not shown to be jointly convex in both $x$ and $\\pi$. A simple example is the function $(x, y) \\mapsto xy$, which is convex in each variable individually, but not jointly convex.\n- Even if we disregard this issue (which by itself undermines the proposed approach), the linearization is only valid in a neighborhood of the current $P^\\pi$, where $|x_{ij}| \\leq \\max P^\\pi_{i,j}$ (not merely $|x_{ij}| \\leq 1$). Consequently, if the proposed scheme is to be followed, the modification $x$ of the kernel must be bounded at each step rather than freely vary in the $[-1, 1]$ interval, thereby requiring a \"trust-region\" optimization formulation.\n- The cost bound does not seem to be used in Algorithm1 nor in its low-level formulation."}, "questions": {"value": "The main question at this point concerns the linearization. We expect a solid and rigorous proof of any proposed modification to salvage the paper. Although we recognize that such an effort may be substantial within the given time frame, we are willing to raise our rating if it is adequately addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2wiuduJCAe", "forum": "W63sPIQPz4", "replyto": "W63sPIQPz4", "signatures": ["ICLR.cc/2026/Conference/Submission20590/Reviewer_P38m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20590/Reviewer_P38m"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990955597, "cdate": 1761990955597, "tmdate": 1762933999705, "mdate": 1762933999705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}