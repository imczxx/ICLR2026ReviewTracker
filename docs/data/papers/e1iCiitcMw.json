{"id": "e1iCiitcMw", "number": 6118, "cdate": 1757953280258, "mdate": 1763052397303, "content": {"title": "Generic Medical Image Segmentation Enhancement by Adapting Segment Anything Model", "abstract": "Accurate medical image segmentation is crucial for clinical applications but remains challenging due to ambiguous boundaries, multi-scale anatomies, and the high cost of expert annotations. While deep learning models often produce coarse initial masks, enhancing them into clinically reliable outputs is a critical yet under-explored problem. We propose SAMedEnhancer, a generic medical image segmentation enhancement framework that enhances coarse masks from any segmentation model using a strategically adapted Segment Anything Model (SAM). Our key innovation is a morphology-aware prompt generation strategy. It first analyzes initial masks via connected-component and shape analysis to identify reliable anatomical regions. Then, a hierarchical prompting mechanism is devised: positive points are sampled from high-confidence interiors, while negative points are selected from informative nearby backgrounds within dilated regions; these are supplemented by bounding boxes enclosing the refined targets. This coarse-to-fine prompting robustly guides SAM to recover accurate boundaries, resisting error propagation from imperfect inputs. We extensively validate SAMedEnhancer on a comprehensive benchmark for medical image segmentation enhancement, encompassing several datasets across various imaging modalities and both fully- and semi-supervised settings. Results demonstrate that our method consistently improves segmentation quality from state-of-the-art segmenters, reduces annotation dependency, and serves as a versatile accelerator for medical image segmentation.", "tldr": "", "keywords": ["Medical image segmentation", "Segment anything model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d9a4cdce15cae4ef3474a10322c172ef56097b82.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SAMedEnhancer, a model-agnostic framework for enhancing medical image segmentation masks by leveraging the Segment Anything Model. The method operates by converting coarse segmentation masks from any model into robust morphological prompts, consisting of filtered connected components and hierarchical positive/negative point and bounding box prompts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The approach is model-agnostic and can be applied as a post-processing enhancement step to any existing segmentation pipeline without retraining. The paper claims intent to release code/datasets upon acceptance."}, "weaknesses": {"value": "My major concern is the motivation of the paper. When sufficient training data is available, deep learning models outperform SAM variants, though they require relatively large datasets for training. SAM variants typically demonstrate suboptimal performance across most medical image analysis tasks, and the extent to which their performance can be significantly improved remains questionable. Additionally, there is a lack of comparisons between SAM variants and SOTA task-specific models such as nnU-Net.\n\nOther Concerns\nRecent domain-generalizable or robust SAM adaptation methods is not included as baselines in experimental results, weakening the empirical claim of proposed method.\nThe method s generalization to highly multi-class or multi-label settings, or to complex 3D (volumetric) data, is not discussed or tested. All experiments focus on 2D slice data, leaving the claimed plug-and-play generalization only partially substantiated by evidence. As most “medical image segmentation” tasks are multi-target segmentation adopted on 3D images like CT and MRI.\nThe results in Tables without any reporting of variance or statistical significance. No confidence intervals, standard deviations, or formal significance tests are presented, due to possible high variance in medical segmentation tasks especially with semi-supervision and small data splits.\nSome steps of the mathematical formulation are underspecified, including how the filter and fusion rules are adapted per task what concrete domain knowledge or priors are needed for reliable MSFF operation? Some tables lack context, and table formatting across the paper is occasionally inconsistent. Figure 2 is referenced only in the appendix but should arguably be part of the main results for less disruption to the narrative."}, "questions": {"value": "Refer to Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2UavajIhN3", "forum": "e1iCiitcMw", "replyto": "e1iCiitcMw", "signatures": ["ICLR.cc/2026/Conference/Submission6118/Reviewer_TTGJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6118/Reviewer_TTGJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6118/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760690398133, "cdate": 1760690398133, "tmdate": 1762918477614, "mdate": 1762918477614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "QsDb8YkwxN", "forum": "e1iCiitcMw", "replyto": "e1iCiitcMw", "signatures": ["ICLR.cc/2026/Conference/Submission6118/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6118/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763052396324, "cdate": 1763052396324, "tmdate": 1763052396324, "mdate": 1763052396324, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work uses Segment Anything Model 2 (SAM2) and its medical counterpart MedSAM2 to enhance segmentation masks for medical use-cases. Towards this end, they propose a stage-wise refinement process. This includes a filter for removing noisy regions and combining disconnected regions called MSFF, followed by Prompt Excavation that extracts point and box-based prompts. These are input to the pretrained MedSAM2 model for refined masks. Experiments show improved performance over UNet on multiple datasets"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper presents an application of SAM-like foundation models as refinement modules and discusses valid potential applications, like improving semi-supervised learning and post-processing. \n\n2) Prompt selection in the Excavation phase is done using geodesic distance to isolate useful positive and negative point prompts. This allows SAM to pinpoint on the area of interest\n\n3) MSFF module allows the insertion of domain knowledge about the object of interest that can be used to filter out noise."}, "weaknesses": {"value": "1) Limited Experimentation: In line 156, the authors claim that SAMedEnhancer is model agnostic. Yet, the authors only show results on UNet as a baseline. More baselines have to be included to uphold this claim. In addition, many methods adapt SAM directly for the segmentation task and get significantly better results than UNet. Even without SAM, many methods like nnUNet, TransUNet and UNext exist that show significantly better performance than UNet. Thus, I believe the choice of only UNet as a baseline is insufficient to gauge the effectiveness of the method.\n\n2) The paper does not discuss the potential limiting cases of the method. One important possibility that I can think of is if the baseline model generates a mask that does not cover the object of interest at all. In such a case, all the following modules will fail to extract a correct prompt. \n\n3) The method has lots of hyperparameters that necessitate the need for engineering them correctly for various applications. For example, the size filter which is set to 5 (why?), the IPQ threshold, margin for the box prompt and so on.\n\n4) The paper requires a lot of minor formatting changes as follows:\n(a) The related work section should contain at least 3-4 lines about the SAMRefiner method, which seems like a direct predecessor to the method and an important comparison.\n(b) Tables 3 and 4 should indicate the best values in bold.\n(c) The methods in Table 2 should be cited in either the table rows or the caption itself. \n(d) Cite the existing works for the single-type prompts and multi-prompt combinations in line 248\n(e) Missing third stage description in 3.1"}, "questions": {"value": "1) What is the effect of IPQ in the overall performance of the MSFF? The ablations treat MSFF as a whole component, but I am interested to know the individual effect of domain knowledge filters like IPQ over general area-based morpological operations. \n\n2) In line 20 of the algorithm, the authors mention that domain knowledge can be applied. What are some examples other than IPQ?\n\n3) Please refer to the weaknesses. My main concern is (1)Limited Experimentation"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZepOI80Slw", "forum": "e1iCiitcMw", "replyto": "e1iCiitcMw", "signatures": ["ICLR.cc/2026/Conference/Submission6118/Reviewer_c4Td"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6118/Reviewer_c4Td"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6118/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761428971907, "cdate": 1761428971907, "tmdate": 1762918477141, "mdate": 1762918477141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. Propose SAMedEnhancer, the first framework to effectively leverage SAM for generic medical image segmentation enhancement by morphological analysis and hierarchical prompt excavation. It operates in a plug-and-play manner, refining coarse outputs from any segmentation model without retraining.\n2. Establish a large-scale evaluation benchmark for medical image segmentation enhancement, covering multiple datasets, modalities, and supervision settings, to facilitate future research in this under-explored area.\n3. Introduce a novel semi-supervised learning strategy that integrates iterative pseudolabel refinement with SAMedEnhancer, demonstrating significant performance gains and reduced reliance on annotated data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The methods are well-described. Authors provided pseudo codes and equations to help authors to understand their methods.\n2. The related works are well-described."}, "weaknesses": {"value": "1. The overall contributions is low. Authors proposed a SAMedEnhancer. However, the core component of this method is SAM which was proposed by others, and many SAM-related works have been proposed and explored. The improvement over SAM is incremental, and authors did not make fundamental contributions to this field.   \n2. Th evaluation is not sufficient. Authors evaluated their methods on five datasets. However, they did not evaluate their methods in CT datasets. CT is the most widely used imaging modality in clinical practice. Additionally, they evaluated their methods on single organ segmentation, but authors did not evaluate their methods on multi-organ segmentation. Currently multi-organ segmentation is more challenging, and implementing multi-organ segmentation will make more scientific contributions.\n3. The comparison with other methods is not sufficient. Authors only compared their methods with four other methods which were proposed several years ago.\n4. The qualitative evaluation is not sufficient. Authors compared the segmentation results of their methods and other methods, but they did not provide qualitative evaluation about how their methods work."}, "questions": {"value": "1. The overall contributions is low. Authors proposed a SAMedEnhancer. However, the core component of this method is SAM which was proposed by others, and many SAM-related works have been proposed and explored. The improvement over SAM is incremental, and authors did not make fundamental contributions to this field.   \n2. Th evaluation is not sufficient. Authors evaluated their methods on five datasets. However, they did not evaluate their methods in CT datasets. CT is the most widely used imaging modality in clinical practice. Additionally, they evaluated their methods on single organ segmentation, but authors did not evaluate their methods on multi-organ segmentation. Currently multi-organ segmentation is more challenging, and implementing multi-organ segmentation will make more scientific contributions.\n3. The comparison with other methods is not sufficient. Authors only compared their methods with four other methods which were proposed several years ago.\n4. The qualitative evaluation is not sufficient. Authors compared the segmentation results of their methods and other methods, but they did not provide qualitative evaluation about how their methods work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PKHyP00R0W", "forum": "e1iCiitcMw", "replyto": "e1iCiitcMw", "signatures": ["ICLR.cc/2026/Conference/Submission6118/Reviewer_cqBC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6118/Reviewer_cqBC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6118/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739278971, "cdate": 1761739278971, "tmdate": 1762918476731, "mdate": 1762918476731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SAMedEnhancer, a model-agnostic framework that leverages the Segment Anything Model (SAM) to enhance coarse medical image segmentation masks without retraining. It introduces a Morphological Split-Filter-Fuse (MSFF) module to clean noisy masks and a Hierarchical Prompt Excavation strategy to generate robust point and box prompts that guide SAM for precise boundary refinement. Experiments across multiple modalities show consistent improvements in Dice and Hausdorff Distance, demonstrating that SAMedEnhancer effectively boosts segmentation accuracy and serves as a plug-and-play enhancement tool for both fully and semi-supervised settings."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The method is completely model-agnostic and plug-and-play, requiring no retraining of either the base segmentation model or SAM.\n- The combination of the MSFF module and hierarchical point/box prompts enables effective noise reduction and preserves structural consistency.\n- The approach is thoroughly tested across multiple imaging modalities and tasks, including semi-supervised pseudo-label enhancement.\n- It consistently outperforms traditional post-processing methods such as CRF, level-set, and morphology operations in both Dice and HD95 metrics."}, "weaknesses": {"value": "- The framework mainly combines morphological analysis with improved SAM prompting strategies, which is practical but conceptually incremental. The methodological novelty is somewhat limited, as it extends existing SAM-based refinement pipelines rather than introducing new theoretical insights or learning principles.\n- The ablation study shows only modest gains for individual modules, and the overall improvement may partly stem from the introduction of additional input information (e.g., extra point, box, or mask prompts), which inherently provides more spatial cues rather than demonstrating a fundamentally stronger reasoning mechanism.\n- The experimental datasets are relatively small and may not adequately support claims of generalization across diverse medical modalities or large-scale clinical settings.\n- From the visual examples in Figure 2, the improvements appear marginal; SAM itself may already achieve comparable results in a zero-shot manner. The paper does not provide a clear discussion or quantitative comparison to isolate such effects.\n- The source of the *coarse masks* used as inputs is unclear. It is important to specify whether these masks are generated by real segmentation models or synthetically constructed, as this directly affects the validity and applicability of the proposed enhancement scenario."}, "questions": {"value": "I have the following questions for author(s):\n- How are the coarse masks obtained—are they real model outputs or synthetic approximations?\n- How does SAMedEnhancer outperform zero-shot SAM with similar prompts, and is there a quantitative comparison?\n- The ablation gains seem modest—can the authors justify each component’s necessity or report significance analyses?\n- How do the authors separate the benefit of extra prompt information from the algorithmic contribution?\n- Can results on larger or more diverse datasets (e.g., multi-organ CT or 3D MRI) be provided to support generalizability?\n- What is the computational cost of SAMedEnhancer, and how practical is it for large-scale or real-time use?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zlnCuyfENH", "forum": "e1iCiitcMw", "replyto": "e1iCiitcMw", "signatures": ["ICLR.cc/2026/Conference/Submission6118/Reviewer_5xih"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6118/Reviewer_5xih"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6118/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961394332, "cdate": 1761961394332, "tmdate": 1762918476222, "mdate": 1762918476222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}