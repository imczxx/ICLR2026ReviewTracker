{"id": "aiM6bRd6bG", "number": 7255, "cdate": 1758013253336, "mdate": 1759897863536, "content": {"title": "PPI Candidate Ranking: Large-Scale Evaluation of a Domain Knowledge–Guided Pipeline", "abstract": "Computational approaches have become central to Protein–Protein Interaction (PPI) research, complementing experimental techniques that remain costly and incomplete. While modern deep learning methods capture diverse biological signals and hold promise in expanding the known interactome, empirical validation remains a critical bottleneck due to its long and expensive procedures. To address this challenge, we introduce the problem of PPI candidate ranking, aiming to prioritize interactions for experimental testing. We propose a novel framework that leverages domain knowledge through interpretability-guided ranking and further refines prioritization by integrating complementary sources of evidence, including interaction scores, structural plausibility, and biomedical language features. Evaluations on a large-scale dataset constructed from successive STRING releases demonstrate that our approach yields significant improvements over two state-of-the-art PPI prediction models, providing more accurate and biologically coherent rankings.", "tldr": "", "keywords": ["protein protein interaction", "computational biology", "explainable ai", "protein language models"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/27689df6a7f52f985186478f5a9d0686f6872fa8.pdf", "supplementary_material": "/attachment/a0b4cba28770b013fba53f7ea825ff6c4dabb917.zip"}, "replies": [{"content": {"summary": {"value": "This study addresses the candidate ranking problem in the field of protein-protein interactions (PPI) by proposing an interpretable-guided ranking mechanism. By integrating complementary evidence such as interaction scores, structural plausibility, and biomedical language features, the proposed method optimizes the quality of PPI candidate ranking. Evaluation on large-scale datasets from the STRING database demonstrates that the proposed method outperforms existing models such as D-SCRIPT and Topsy-Turvy. Suggesting the method better surfaces true novel interactions. The study analyzes how interpretability and semantic enrichment enhance ranking and presents large-scale quantitative evidence and runtime analysis.\nContributions include: the development of a general interpretable-guided framework for assessing the potential value of PPI prediction methods; the application of multiple techniques to optimize PPI candidate ranking; and the identification of key features that effectively predict novel interactions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The work proposes an interpretability-guided ranking mechanism, which combines embeddings, structure-based plausibility, and LLM-derived semantics into a cohesive pipeline. to optimize PPI candidate ranking. This method leverages the concept of interpretability, providing transparency in the ranking decisions.\n2) The experimental comparisons and metric evaluations are well-founded, and the method's effectiveness has been validated using a large-scale dataset.\n3) The paper content is organized reasonably.\n4) The prioritization of PPI has certain practical value in cancer research and drug target screening."}, "weaknesses": {"value": "1) The authors claim that the protein candidate ranking problem is being proposed for the first time in this paper. However, related research, such as \"Ranking Cancer Proteins by Integrating PPI Network and Protein Expression,\" has already been conducted. Therefore, the claim of being the \"first proposal\" is not appropriate.\n2) The innovation of the method is relatively weak. Essentially, it is based on D-SCRIPT and Topsy-Turvy methods for protein embedding representations, calculating related scores, and then re-ranking by integrating new biomedical evidence. This makes the method presented in the paper appear more as an incremental improvement over existing methods, rather than a refined model specifically addressing the problem. Although the paper emphasizes “interpretability-guided” ranking, the interpretability is not used to generate human-understandable biological explanations (e.g., functional motifs or structural interfaces). The method remains essentially a numerical heuristic built on internal activations, providing little insight to biologists.\n3) The details of the re-ranking method are too brief, especially regarding how to integrate numerous biological signals and the process of protein prioritization, which is not clearly explained. More formulas should be added to explain the re-ranking process.\n4) The paper only compares the D-SCRIPT and Topsy-Turvy methods, while this paper builds upon these methods incrementally. Therefore, the comparison experiment is essentially an ablation study, lacking comparisons with other types of computational methods. Additional comparative experiments should be included to prove its effectiveness. While the paper analyzes pairwise rank shifts, it lacks a quantitative ablation that isolates how much each re-ranking signal (interaction score, pDockQ, semantic similarity, LLM-based scores) contributes to the overall gain.\n5) The authors note using biomedical LLMs pretrained on PubMed; these may indirectly encode known PPIs, raising fairness concerns for evaluation. The authors use random negative sampling at a 10:1 ratio, which may not reflect the true non-interaction distribution in the human proteome. This bias could artificially inflate ranking metrics.\n6) The validation work mainly focuses on the STRING database and does not fully demonstrate the method’s performance in other domains or datasets. The paper should include results on other datasets as well. Additionally, there is no clear real-world case or validation. The biological analysis of the top-ranked proteins should be supplemented."}, "questions": {"value": "1. How would the framework extend to multi-protein complexes (beyond pairwise interactions)?\n2. How does the method perform for proteins with very few known partners?\nPlease refer to the weakness for other questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HRjP5HfQ9a", "forum": "aiM6bRd6bG", "replyto": "aiM6bRd6bG", "signatures": ["ICLR.cc/2026/Conference/Submission7255/Reviewer_X99M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7255/Reviewer_X99M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792850673, "cdate": 1761792850673, "tmdate": 1762919389244, "mdate": 1762919389244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper defines the problem of PPI candidate ranking whit the objective of prioritizing candidate proteins to test experimentally if they interact with a given target protein. That is, for a target protein with known interactors, it ranks novel candidates that are most likely to interact with the target. For this, the authors propose an approach with two stages: it leverages domain knowledge via interpretability-guided ranking and then refines the top of the list using additional sources of biological/textual/structural evidence. For evaluation, they outperformed two PPI classification models at prioritizing candidate proteins by rediscovering interactions of the version 12 of STRING using only data from version 11."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* The authors took advantage of an interpretable module, predicted contact map, of previous approaches (D-SCRIPT and Topsey-Turvey) to develop a methodology that computes similarities between two sets of proteins, that is, proteins that are known to interact with the target protein and candidate proteins. A very interesting result is that this methodology provides a better ranking than the ranking provided by the models (D-SCRIPT or Topsey-Turvy), which were trained to classify protein-protein interactions. \n\n* I agree that ranking metrics are required to test the practical applicability of PPI prediction approaches.  \n\n* The authors also proposed multiple re-ranking strategies using different types of additional biological signals, providing a comparison of which biological signals further improve prioritization of the top 10 candidate proteins."}, "weaknesses": {"value": "Major \n\n* One of the contributions of the paper is the introduction of the PPI candidate ranking. However, the discussion of the limitations of current benchmarks and metrics that motivate the introduction of this new problem is very limited.  \n\n* My major issue with this work is that it only includes as competitors the models on which it based its framework. Given that the proposed framework works by establishing similarities between KP(p) and CP(p), simple baselines like BLASTp [1] or HMMER [2] could be used in the same way, which would demonstrate whether the proposed approach results in more meaningful similarities. More importantly, the authors should have also compared their approach with other sequence-based approaches for PPI prediction, such as xCAPT5 [3] and MARPPI [4]. Both examples provide predicted probabilities that could be used for CP(p) ranking. Finally, experiments on other datasets commonly used for PPI prediction could have served as more reliable evidence of the performance of the proposed approach [5,6,7].\n\n* There is a lack of description of the training and testing sets; only the number of new interactions is specified. Details such as the number of proteins in each set and the number of interactions in the training set are not provided. Additionally, information about the sequence similarity between proteins in the training and testing sets is also missing.\n\n* The early explanations of refinement of the ranking with different biological signals may suggest to the reader that there is only one final ranking that is the result of these multiple sources of biological information.The description of Table 2 makes it clear that each data view is used separately. This misunderstanding could be avoided earlier in the text.\n\nMinor  \n\n* Lines 156–158: I understand these methods were previously referenced, but given their importance, it may be helpful for the reader to get the references here again. \n\n* Figure 1 is not mentioned in the text. In the figure, R(pc1) is a score used for ranking rather than the rank itself; this aligns with line 254, but adjusting the notation in Equation 4 and Figure 1 could help avoid any confusion. \n\n* Line 307: The encoder is fine-tuned with query–candidate pairs, but lines 309 indicate $p \\in NP(p)$ are used. If these are proteins in the test set, this is concerning; the training setup needs clarification to confirm that it avoids data leakage. \n\n* Line 85: “D-SCRIPT” typo. \n\nReferences\n\n[1] Altschul, Stephen F., et al. \"Basic local alignment search tool.\" Journal of molecular biology 215.3 (1990): 403-410.\n\n[2] Eddy, Sean R. \"Profile hidden Markov models.\" Bioinformatics (Oxford, England) 14.9 (1998): 755-763.\n\n[3] Dang, Thanh Hai, and Tien Anh Vu. \"xCAPT5: protein–protein interaction prediction using deep and wide multi-kernel pooling convolutional neural networks with protein language model.\" BMC bioinformatics 25.1 (2024): 106.\n\n[4] Li, Xue, et al. \"MARPPI: boosting prediction of protein–protein interactions with multi-scale architecture residual network.\" Briefings in Bioinformatics 24.1 (2023).\n\n[5] Martin, Shawn, Diana Roe, and Jean-Loup Faulon. \"Predicting protein–protein interactions using signature products.\" Bioinformatics 21.2 (2005): 218-226.\n\n[6] Guo, Yanzhi, et al. \"Using support vector machine combined with auto covariance to predict protein–protein interactions from protein sequences.\" Nucleic acids research 36.9 (2008): 3025-3030.\n\n[7] Pan, Xiao-Yong, Ya-Nan Zhang, and Hong-Bin Shen. \"Large-Scale prediction of human protein− protein interactions from amino acid sequence based on latent topic features.\" Journal of proteome research 9.10 (2010): 4992-5001."}, "questions": {"value": "* Line 65: “previous knowledge of the target protein”, the previous knowledge here refers to KP(p)? This is unclear given the information given till that point. \n\n* What are the four techniques used to refine PPI rankings? \n\n* How are the LLMs finetuned for the re-ranking strategies? \n\n* Why did the authors decide not to compare their approach with additional SOTA models? \n\n* What is the similarity between the training and testing sets? \n\n* Line 145: “enabling PPI comparison via textual annotations.” What does this piece mean?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BbJSQb7VIh", "forum": "aiM6bRd6bG", "replyto": "aiM6bRd6bG", "signatures": ["ICLR.cc/2026/Conference/Submission7255/Reviewer_qSmL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7255/Reviewer_qSmL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944556830, "cdate": 1761944556830, "tmdate": 1762919388971, "mdate": 1762919388971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address the problem of retrieving and ranking candidate protein-protein interactions (PPIs), a problem related to general protein-protein interaction prediction but framed in the context of using existing known PPIs for a target to prioritize potential new interactions for experimental testing. Given the relative expense of experimental testing compared to in silico prediction, this is a well grounded and motivated application. It is however limited in that it will only be possible for proteins that already have known partners, and will not generalize to completely new proteins. The paper is well-written and the results present a promising path forward for prioritizing experimental validation of PPIs.\n    \n### Strengths\n\n- For retrieval, the authors develop an adaptation of the deep-learning PPI model D-SCRIPT that compares the interpretable intermediate layer of the latter model between known and candidate PPIs, ranking candidates by the similarity of embeddings of key amino acids. This interpretability-guided ranking makes clever use of the D-SCRIPT/Topsy-Turvy outputs by focusing only on the residues that are informative of correct prediction with known partners. This results in both an increase in accuracy and a speed-up in prediction, with the trade-off of reduced generalization (requires known interacting partners).\n- The authors test many different methods for re-ranking the top retrieved proteins, including the original interaction probability, structural and functional features, and language model embedding similarity. Combined with the retrieval strategy, this offers an exciting way to identify the best new PPIs.\n- The analysis of Topsy-Turvy vs. D-SCRIPT at a global vs. top-k level is really interesting, and lends valuable insight into the relative performance of these models.\n- The paper contains a clear and honest discussion of the limitations of their method, including that it will likely not succeed for under-explored proteins with few interactions already known.\n\n### Major Comments\n\n- L241: The computation of the active residues $I_k$ is not clear-- once you have computed the activation score for each residue of $p_k$, how do you select \"the contiguous set of residues... reporting the highest average contact probability\"? Do you select a fixed number of residues, or is there some extension criteria? This needs to be clarified, as it is a key algorithmic step in the interpretability guided ranking.\n- L266: Is focusing on the top 10 ranked candidates for re-ranking enough, especially when success rates in Table 1 are so low at k=10? How often are you actually re-ranking true PPIs?\n- L379: Table 2 is incredibly challenging to interpret. The value in each cell sums to 100%, so it would be clearer to represent it as a relative improvement, maybe +- % from 50? Or an average change in position (+/-)? Assuming \"cosine\" is your method (see minor comments about this confusion), it seems that switching from cosine almost always helps-- does this mean you should only use your method for retrieval? Also, since this is supposed to represent change in ranking switching from row to column method, why is this not symmetric? For example, this table seems to suggest I can improve the majority of proteins when swapping from Token to TF-IDF, but also when swapping the other direction. This seems contradictory.\n- L718: I don't understand how your retrieval method is so much faster than D-SCRIPT prediction probability. For any candidate protein, the author's approach needs to run D-SCRIPT $k$ times for each known partner, compute multiple cosine similarities sliding over $|I_k|$ residues, and take the maximum over all of these. Compare this to a single forward pass of D-SCRIPT per candidate protein using prediction probability. Are you pre-computing active residues? Is this compute time included in Figure 2?\n- L728: Likewise, I wonder whether pre-processing time for fetching functional information per protein is included in the compute time in Figure 3, or if only the comparison of semantic scores is considered. It would be good to clarify exactly what is being measured in each runtime, with all methods on an equal footing of starting from the raw sequences.\n\n### Minor Comments\n\n- L038: \"only a limited fraction of the human interactome has been experimentally resolved\" -- it is worth discussing here the Human Reference Interactome (HuRI) project that has made significant strides in this direction (https://interactome-atlas.org/)\n- L053: The authors correctly note that D-SCRIPT and Topsy-Turvy do not account for tertiary/quaternary structure, but follow-up work to Topsy-Turvy does consider this (TT3D, https://academic.oup.com/bioinformatics/article/39/11/btad663/7332153). Have the authors tested this model?\n- L093: Are there any STRING v11 interactions that were removed/not present in STRING v12?\n- L134: Why was SpeedPPI chosen compared to other options? Was AlphaFold-Multimer (https://www.biorxiv.org/content/10.1101/2021.10.04.463034v2) considered (there is no citation for this)\n- L157: The authors state that \"One of the most widely adopted [PLMs] is the Bepler & Berger model;\" this is not really true and ignores the wider literature of protein language models.\n- L184: What are GLIDE scores? There is no description of this acronym or citation to reference?\n- L245: It would be good to have some discussion of the complexity/runtime of these operations, since you have to compare with multiple sets of residues/known partners-- this seems expensive \n- L307: How were data split for fine-tuning on PubMedBERT? This is a potential source of data leakage if you are fine-tuning the ranking model\n- L349: What is the cosine method? Is this your interpretability-guided method from earlier? It is not clearly defined.\n- L375: How is \"retrieved\" defined; why does Prediction Coverage not also vary across values of $k$?\n- L589: The version of D-SCRIPT cited is the preprint, not the journal publication\n- L672: Is Topsy-Turvy configured the same way as D-SCRIPT?\n- L689: More detail is needed on \"A GroupKFold split by protein identity;\" what identity threshold was chosen, was this done at the query protein level only or also with candidate proteins? In general, the training data set up for PubMedBERT fine-tuning is lacking."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "see above"}, "weaknesses": {"value": "see above"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Tvuw6PSvkl", "forum": "aiM6bRd6bG", "replyto": "aiM6bRd6bG", "signatures": ["ICLR.cc/2026/Conference/Submission7255/Reviewer_aVDq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7255/Reviewer_aVDq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954774537, "cdate": 1761954774537, "tmdate": 1762919388421, "mdate": 1762919388421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors of this paper propose a two-stage framework for the task of PPI candidate ranking: first, an \"interpretability-guided retrieval\" method leverages the internal embedding activations from pre-trained sequence models (D-SCRIPT, Topsy-Turvy) to create an initial ranking. This retrieval assumes that a target protein's new interaction partners will share embedding-level similarities with its known partners. Second, this initial list is refined by a re-ranking module that integrates complementary biological evidence, including interaction scores, structural plausibility, semantic scoring, and large language model reranking. The authors conduct a large-scale prospective evaluation using successive STRING databases, demonstrating that their method improves early-ranking metrics by a substantial amount compared to using the baseline model's raw interaction scores."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Here are some strengths of the paper:\n\n1. $\\textbf{Importance }$ \n\nThe paper proposes a solution to a real-world problem that could impact numerous areas of life. \n\n2. $\\textbf{Practical method with good empirical results}$\n\nThe proposed methodology seems innovative. First, the authors formulate the problem in terms of ranking, and they use stages of ranking where the second method refines the first. The experimental results using the STRING v11-v12 dataset provide a realistic and tangible improvement. The results in Table 1 demonstrate significant improvements in multiple metric values compared to the baseline prediction scores.\n\n3. $\\textbf{Comprehensive result}$\n\nThe paper shows in Table 2 the importance of semantic signals that capture the correlations missed with base models which makes the work interesting."}, "weaknesses": {"value": "Here are some weaknesses of the paper:\n\n1. $\\textbf{Reliance on Known Discoveries}$\n\nAs stated in the limitation, the framework's fundamental assumption is that new interactions will be similar to known ones. This is a significant limitation for the vast number of proteins that may be poorly characterized or have very few known partners. Did you set up any kind of experiment on how this will affect your model?\n\n2. $\\textbf{Limited evaluation}$\n\nThe main results of the experiments seem to be dependent on moslty one dataset filled with many correlated metrics. Is there any way to expand this to other datasets? How does this dataset [1] perform in your method? \n\n\nReferences\n\nPiNUI: A Dataset of Protein-Protein Interactions for Machine Learning; Geoffroy Dubourg-Felonneau and Eyal Akiva and Daniel Wesego and Ranjani Varadan; NeurIPS 2023 Workshop on New Frontiers of AI for Drug Discovery and Development; 2023"}, "questions": {"value": "Please refer to the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tb6NNpDjBy", "forum": "aiM6bRd6bG", "replyto": "aiM6bRd6bG", "signatures": ["ICLR.cc/2026/Conference/Submission7255/Reviewer_WiiR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7255/Reviewer_WiiR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979349259, "cdate": 1761979349259, "tmdate": 1762919388039, "mdate": 1762919388039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}