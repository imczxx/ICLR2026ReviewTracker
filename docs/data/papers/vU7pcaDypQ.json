{"id": "vU7pcaDypQ", "number": 20054, "cdate": 1758301943594, "mdate": 1759897004058, "content": {"title": "Partial Parameter Updates for Efficient Distributed Training", "abstract": "We introduce a memory- and compute-efficient method for low-communication distributed training. Existing methods reduce communication by performing multiple local updates between infrequent global synchronizations. We demonstrate that their efficiency can be significantly improved by restricting backpropagation: instead of updating all the parameters, each node updates only a fixed subset while keeping the remainder frozen during local steps. This constraint substantially reduces peak memory usage and training FLOPs, while a full forward pass over all parameters eliminates the need for cross-node activation exchange. Experiments on a $1.3$B-parameter language model trained across $32$ nodes show that our method matches the perplexity of prior low-communication approaches under identical token and bandwidth budgets while reducing training FLOPs by $15$% and peak memory by up to $47$%.", "tldr": "Memory and compute efficient method for low-communication distributed training", "keywords": ["distributed training", "low-communication", "language modeling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3829986eac44221835789ac8c562684e02d83822.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a partial parameter updates scheme based on the DiLoCo framework, which reduces memory use and communication costs by freezing parameters on each node. Unlike Streaming DiLoCo which divide the model in a pipeline-parallel manner, this work slice the training in a tensor-parallel way to achieve a similar results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is well-structured and the idea is easy to follow, and it provides targeted optimizations to address the problem of high peak memory consumption in Streaming DiLoCo. Under low-bandwidth settings, proposed method converges faster than standard DDP."}, "weaknesses": {"value": "1. It looks like a natural extension to Streaming DiLoCo with frozen weights, also lacks convergence guarantee in theory. As seen in Table 1, the ppl goes up quickly when N is larger than 8.\n2. The improvement compared to Streaming DiLoCo is marginal, seen in Figure 3, while the curve for Streaming DiLoCo is smoother, indicating a more stable run."}, "questions": {"value": "1. Could you provide experimental results for a Streaming DiLoCo synchronization strategy that has the same communication budget as 1/4 MLP? I think it might be a better baseline when comparing convergence.\n2. Is there any clear pattern in the strategy selection for parameter slicing (eg. training a model with different shape), and could you provide ablation experiments that only slice attention heads?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qL8NQ1zEAm", "forum": "vU7pcaDypQ", "replyto": "vU7pcaDypQ", "signatures": ["ICLR.cc/2026/Conference/Submission20054/Reviewer_twfF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20054/Reviewer_twfF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553451586, "cdate": 1761553451586, "tmdate": 1762932948887, "mdate": 1762932948887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method to improve the efficiency of distributed Transformer training. Specifically, it freezes a subset of parameters on each node, allowing local updates and an all-reduce operation on the local weight changes (similar to DiLoCo), followed by an outer optimization step. By omitting optimizer states for the frozen parameters, the approach reduces memory usage and achieves better performance per FLOP."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method in the paper is both interesting and practical. Since adaptive optimizer states typically consume more memory than the model parameters themselves, the resulting memory savings are substantial. These savings can be leveraged to train larger models or use bigger batch sizes, leading to more efficient resource utilization.\n\nThe paper is well written, and the proposed approach is simple yet effective. It can be easily implemented with existing communication-efficient distributed training frameworks. The experimental results are promising and demonstrate the potential of the method in improving both memory efficiency and training performance."}, "weaknesses": {"value": "While the proposed method is promising, its main limitation lies in the lack of experimental breadth. Evaluating only a single model size, in a single distributed configuration, and on a single dataset is insufficient to convincingly demonstrate the method’s generality or robustness. Additional ablations and experiments under diverse settings would significantly strengthen the paper’s claims.\n\nMoreover, the reported memory savings are highly dependent on the choice of optimizer. For instance, modular optimizers such as Muon [1] and its variants are gaining traction and have shown that momentum-based SGD can perform comparably to adaptive optimizers like AdamW. In such cases, the memory advantage offered by the proposed approach becomes less substantial.\n\n[1] https://arxiv.org/abs/2502.16982"}, "questions": {"value": "1. It would be valuable to include an ablation showing how the proposed method performs as the number of nodes (or model replicas) increases. This would clarify how communication efficiency and performance scale with distributed configurations.\n\n2. Including training and validation curves plotted against the number of processed tokens for all compared baselines would be helpful. Such curves would help assess convergence behavior, stability, and sample efficiency of the method.\n\n3. Including the performance of standard DDP training in Table 1 would provide a stronger reference point and make it easier to contextualize the improvements of the proposed approach.\n\n4. Additional experiments across multiple datasets, network depths, and hidden dimension sizes would significantly strengthen the empirical section. These would help demonstrate that the observed gains are consistent across varying model scales and data regimes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "I8XjM7A1p7", "forum": "vU7pcaDypQ", "replyto": "vU7pcaDypQ", "signatures": ["ICLR.cc/2026/Conference/Submission20054/Reviewer_CRF3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20054/Reviewer_CRF3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706643475, "cdate": 1761706643475, "tmdate": 1762932948367, "mdate": 1762932948367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an efficient algorithms for low-communication distributed data-parallel training that performs local updates on a node-specific subset of parameters. The author(s) validate the effectiveness of the method by training a 1.3B LLMs on 32 nodes and compare it with Streaming DiLoCo on Perplexity and training FLOPs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper describes the research method in detail.\n2. This paper describes the experimental setup in detail."}, "weaknesses": {"value": "1. There is no discussion on block coordinate optimization. There is plenty of literacture on block coordinate optimization [1-3] and it is unclear what is the main technical contribution of the paper.\n2. There is no comparison with block coordinate optimization and parameter-efficient fine-tuning (PEFT) in the experiments. Given that there aer a lots of existing methods on block coordinate optimization, they should be included as baselines. Meanwhile, the proposed method has similar performance advantages to PEFT methods such as LoRA [4], requiring only communicating matrices smaller than the model weights during parameter synchronization. So PEFT methods should be included in the comparison.\n\n[1] Accelerating Block Coordinate Descent for LLM Finetuning via Landscape Correction\n\n[2] Memory-Efficient Block Coordinate Descent for Hessian-Informed Zeroth-Order Optimizer\n\n[3] How to Train a Model on a Cheap Cluster with Low Cost using Block Coordinate Descent\n\n[4] Lora: Low-rank adaptation of large language models."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TpJ9qFPDFk", "forum": "vU7pcaDypQ", "replyto": "vU7pcaDypQ", "signatures": ["ICLR.cc/2026/Conference/Submission20054/Reviewer_H2UE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20054/Reviewer_H2UE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928802751, "cdate": 1761928802751, "tmdate": 1762932947702, "mdate": 1762932947702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes partial parameter updates for local SGD and then global synchronization with diloco/fedavg to reduce communication cost between workers in a data parallel setting. The core idea is that each worker optimises a subset of model parameters (predetermined) and other frozen parameters are only updated through the outer optimizer after the global sync. The experiments show comparable performance with streaming diloco wrt to flops."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Even though incremental, the idea is simple and seems effective. The way of determining the subsets is similar to MoE and makes sense. Due to partial parameter updates, per-node memory usage is reduced, enabling larger models to be trained in the DP setting.\n2. Language model experiment shows performance matching streaming diloco when compared against flops processed."}, "weaknesses": {"value": "1. Limited experiments: Only one dataset and one datasets and one baseline method compared. Specifically, sparta (Beton et al) is a relevant baseline, and diloco has many variants -- for instance, outer gradients can be compressed using quantization/low-rank to reduce communication cost. These methods could be compared to correctly position the paper wrt convergence speed and communication cost. \n    - As the main idea is heuristic and experiments with multiple datasets and model sizes would strengthen the paper.\n2. Even though wrt flops, the results match streaming diloco, wrt to training steps it is slow.\n3. To reduce memory usage and fit larger models into smaller gpus, one can adopt pipeline parallelism with low-rank compression of activations and gradients as shown in [1]. This is a relevant comparison on the aspect of fitting larger models in smaller gpus.\n\n[1] Ramasinghe, Sameera, et al. \"Protocol Models: Scaling Decentralized Training with Communication-Efficient Model Parallelism.\" arXiv preprint arXiv:2506.01260 (2025)."}, "questions": {"value": "1. Fig.1 shows that the outer optimizer only updates the frozen parameters, but this contradicts line 15 of Algorithm 1. Also, there are common trainable parameters between workers, so they will also be updated during the outer step. Please clarify"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qq1OaMhC6K", "forum": "vU7pcaDypQ", "replyto": "vU7pcaDypQ", "signatures": ["ICLR.cc/2026/Conference/Submission20054/Reviewer_rLg8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20054/Reviewer_rLg8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762217891990, "cdate": 1762217891990, "tmdate": 1762932947098, "mdate": 1762932947098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}