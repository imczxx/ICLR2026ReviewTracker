{"id": "9uauFV3Q0u", "number": 13406, "cdate": 1758217453534, "mdate": 1759897439835, "content": {"title": "Spectral Sheaf Filtering: A Topological Approach to Spatio-Temporal Modeling", "abstract": "Spatio-temporal data pose significant challenges for graph-based learning due to their complex, non-stationary dependencies and the limitations of conventional message passing in capturing high-order, asymmetric interactions. We introduce Spectral Sheaf Filtering (SSF), a novel and theoretically grounded framework that redefines information propagation on graphs using the algebraic topology of cellular sheaves. By assigning vector spaces and restriction maps to nodes and edges, SSF encodes context-dependent, localized dynamics that extend far beyond traditional adjacency structures. To further enhance expressivity and efficiency, we introduce spectral filtering over the sheaf Laplacian, enabling frequency-aware decomposition via the graph Fourier transform while emphasizing latent spectral features. This spectral view allows SSF to adaptively modulate information flow across frequency components, effectively mitigating oversmoothing in deep graph neural networks. Extensive experiments on diverse spatio-temporal traffic forecasting benchmarks show that SSF consistently outperforms state-of-the-art methods, especially in long-horizon forecasting tasks. Our results highlight the value of topological structures in advancing graph learning for spatio-temporal systems. The code is available at: https://github.com/anonymous-submisssion/SSF.", "tldr": "Spectral Sheaf Filtering (SSF) uses sheaf topology and spectral filtering to better capture complex spatio-temporal dependencies, achieving state-of-the-art results in long-term traffic forecasting.", "keywords": ["Spatio-temporal data", "Graph neural networks", "Time-series forecasting", "Spectral graph filtering"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/725aef5f6a6b14ec4a7f72366f8357b0235ec45f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on addressing the challenges of graph-based learning in spatio-temporal data, such as complex non-stationary dependencies and the limitations of conventional message passing. It proposes the Spectral Sheaf Filtering (SSF) framework, a pioneering approach that leverages cellular sheaf algebraic topology to redefine graph information propagation—assigning vector spaces and restriction maps to nodes and edges to capture context-dependent localized dynamics. A key innovation is the introduction of spectral filtering over the sheaf Laplacian, enabling frequency-aware decomposition via graph Fourier transform to mitigate oversmoothing in deep graph neural networks. Theoretically, the paper establishes the eigendecomposition of the sheaf Laplacian, generalizing Fourier modes to the sheaf-theoretic setting. Experimentally, on five spatio-temporal traffic forecasting benchmarks (METR-LA, PEMS-BAY, PEMS04, PEMS08, NAVER-Seoul), SSF outperforms state-of-the-art methods, especially in long-horizon forecasting, with significant error reductions and maintains efficient training speed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation in the introduction, especailly shown in Figure 1 is convincing and easy to catch. The perspective of utilizing sheaf representation for spatial-temporal graph tasks is novel.\n2. The formulation of sheaf representation and analysis is very clear.\n3. The paper conducts comprehensive experiments to evaluate the proposed methods, the the prediction accuracy improves significantly."}, "weaknesses": {"value": "1. The most severe concern is the clarity of the proposed model. Though the introduction of sheaf formulation is clear, it is unclear how the spatail-temporal information of **previous t timestamps** of node is utilized to output the prediction of nodes in **the further T' timestamps**.  It is also unclear what is the loss function and how the model is trained. This concern is crucial that affects the clarity of this paper.\n2. The paper lacks the complexity analysis for the proposed method, as the Laplacian decomposition is time-consuming. It is also better to list the running time of decomposition in appendix F in addition to the training runtime."}, "questions": {"value": "See details in weakenss."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D7gfAoArKk", "forum": "9uauFV3Q0u", "replyto": "9uauFV3Q0u", "signatures": ["ICLR.cc/2026/Conference/Submission13406/Reviewer_T1jd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13406/Reviewer_T1jd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760867026134, "cdate": 1760867026134, "tmdate": 1762924039233, "mdate": 1762924039233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces *Spectral Sheaf Filtering (SSF)* for spatio temporal node regression/prediction task. It builds a cellular sheaf with learned restriction maps on vertex–edge incidences, forms the *sheaf Laplacian*, and performs propagation via spectral heat kernel filtering. A rolling window of past graph signals is processed slice by slice through these sheaf spectral layers, followed by a small MLP head for multi horizon node prediction. Experiments on five traffic datasets report strong performance with ablations on filtering, stalk dimension, and number of modes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clean spatial operator.** The cellular sheaf with learned restriction maps yields a well-defined sheaf Laplacian and an interpretable quadratic form that directly encodes edge-wise compatibility.\n\n2. **Principled frequency control.** Spectral heat kernel filtering in the sheaf eigenbasis addresses oversmoothing while retaining useful high frequency content; ablations on filter on or off, stalk dimension (d), and top (k) modes support the design.\n\n3. **Strong empirical performance.** Competitive results across standard traffic benchmarks, clear algorithmic presentation, and interpretability hooks via low frequency modes and the nullspace."}, "weaknesses": {"value": "1. **Temporal head underpowered.** After a strong spatial backbone, the temporal modeling is a small MLP. A comparison to lightweight temporal modules (1D temporal conv, GRU, tiny attention) is missing.\n\n2. **Metric sanity.** Some reported MAPE values look unusually small given the MAE and RMSE, suggesting possible differences in definitions, units, or de normalization. Baseline training and metric computation parity are not fully documented.\n\n3. **Domain breadth.** All experiments are traffic speed forecasting. A second non traffic domain (or transfer tests) would strengthen claims of general utility.\n\n4. **Efficiency specifics.** Top (k) eigensolvers can dominate runtime. The schedule for recomputing eigenpairs and a timing breakdown are not fully reported."}, "questions": {"value": "1. **Temporal modeling.** Please compare your MLP head to a small temporal module per node: 1D temporal conv, GRU, or a tiny attention block. Report accuracy and wall clock.\n\n2. **Metrics.**  Several tables show MAPE notably lower than what the reported MAE would imply. Please provide exact formulas for MAE, RMSE, and MAPE, confirm that evaluation is on de normalized predictions, state the units, and specify any epsilon or clipping used in the MAPE denominator.\n\n3. **Scalability.** How often are eigenpairs recomputed during training. What are typical (k) and stalk dimension (d). Include a timing breakdown for eigensolve, forward, and backward, and a scaling curve with (N), (d), and (k).\n\n4. **Generality.** Can you add one non traffic dataset of the same task type (e.g., environmental, power, or something similar) or a generalization test: inductive node split or cross city transfer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NLUFE2OsXK", "forum": "9uauFV3Q0u", "replyto": "9uauFV3Q0u", "signatures": ["ICLR.cc/2026/Conference/Submission13406/Reviewer_KfCh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13406/Reviewer_KfCh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761410722057, "cdate": 1761410722057, "tmdate": 1762924038728, "mdate": 1762924038728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Spectral Sheaf Filtering (SSF) for spatio-temporal forecasting. The authors (i) build a cellular sheaf on a sensor graph with learnable restriction maps, (ii) define a sheaf Laplacian and apply spectral filtering via a heat kernel, performing “message passing in the spectral domain”, and (iii) report results on benchmark datasets. Key claims include improved expressivity, mitigation of over-smoothing, global receptive fields with O(1) operations, and reduced computational burden from using spectral filtering."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Clear motivation to move beyond uniform edge propagation; sheaves are a natural tool to model asymmetric/high-order relations."}, "weaknesses": {"value": "- The core technical component, spectral filtering of a Laplacian, is classical in graph signal processing (GSP), including design/learning of filters and universal approximation with FIR/IIR (e.g., Shuman et al., 2013; Sandryhaila & Moura, 2013). The paper does not acknowledge this body of work adequately nor contrast SSF against modern spatio-temporal spectral approaches (e.g., Einizade et al., NeurIPS 2024, which provides stability/over-smoothing analysis and SOTA forecasting). As written, replacing the standard Laplacian by a sheaf Laplacian plus a heat kernel filter is an incremental variant of well-established spectral filtering; the “novelty” claim (Sec. 1, contributions) is overstated.\n- The paper claims spectral methods “achieve a global receptive field in O(1) operations”, computing (and back-propagating through) eigendecompositions is not O(1) and is typically O(kN^2) for k eigenvector-eigenvalue pairs. Therefore, the manuscript claims spectral filtering reduces computational burden without a rigorous cost analysis. These are contradictory/incorrect as stated.\n- “This decomposition is interpretable” is claimed but not demonstrated.\n- Oversmoothing and expressivity improvements are claimed but not theoretically analyzed, and the empirical evidence directly targeting these phenomena is missing.\n- The scalability claim in the conclusion is not substantiated. While Table 6 reports per-epoch training time, scalability is dominated by (i) the eigendecomposition of the (sheaf) Laplacian, and (ii) dense transforms like $U^T X$. As the number of nodes increases, both the eigenvalue decomposition and the dense multiplications become bottlenecks in time and memory.\n- The paper states a spectral decomposition $L_F=U \\Lambda U^T$ and interprets eigenpairs as “frequencies”, but never establishes conditions under which the proposed Laplacian is symmetric positive semidefinite (and thus diagonalizable by an orthonormal basis with nonnegative eigenvalues). This is critical for using a heat kernel and a Fourier-like transform.\n- Equations (6-8) implement project-filter-reconstruct (Fourier transform, diagonal filter, inverse transform). Calling this a message passing layer is misleading: it is spectral filtering.\n- No comparison against sheaf GNNs trained in the spatial domain. Without this, it is unclear whether the gain comes from the sheaf modeling or simply from spectral filtering.\n- No comparison with CITRUS (Einizade et al., NeurIPS 2024), which reports SOTA on the same class of tasks on longer horizons, and includes oversmoothing analysis. Similarly, no comparison with more classical ARMA graph-temporal filters.\n- The algorithm input mentions “hyperedge index” although the core model is defined on graphs.\n- The claim “novel spectral filtering” is too broad. Spectral filtering for graph signals is a decade-old area (Shuman et al., 2013; Sandryhaila & Moura, 2013; many follow-ups), with universal filter design and time-graph IIR filters; recent spatio-temporal spectral GNNs exist. Please moderate claims and cite appropriately.\n\n\n**General comment**: The paper reads like it was written heavily relying on LLMs: a repetitive introduction, broad claims stated without proof, and numerous inconsistencies. Even if LLMs were used, which is acceptable, the authors have not verified facts, moderate claims, and aligned the narrative with the actual technical content."}, "questions": {"value": "- Under which assumptions on $L_F$ is symmetric PSD?\n- What is the actual computational complexity?\n- Is $\\alpha$ in the heat kernel a learned parameter per layer or a fixed hyperparameter? How is it selected?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HRxb7IX4FQ", "forum": "9uauFV3Q0u", "replyto": "9uauFV3Q0u", "signatures": ["ICLR.cc/2026/Conference/Submission13406/Reviewer_Ph9G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13406/Reviewer_Ph9G"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903990392, "cdate": 1761903990392, "tmdate": 1762924038198, "mdate": 1762924038198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper applies a new sheaf-based graph network approach for spatiotemporal forecasting. The model achieves very impressive results on the METR-LA dataset."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Originality: The proposed method SSF is novel in traffic prediction.\n- Quality: The model is validated on several traffic datasets. The data preprocessing pipelines seem consistent with SOTA methods.\n- Clarity: The paper provides appropriate background for readers to understand the sheaf GNNs.\n- Significance: The reported improvement on METR-LA is significant."}, "weaknesses": {"value": "- Clarity\n  - The paper could benefit from a better description of the information flow. While Figure 2 shows the general framework, it is unclear how spatiotemporal data is processed by SSF.\n  - Most works in the field report MAE, RMSE, MAPE on all 3 horizons (3, 6, 12). Having Table 5 in the main paper instead of Table 1 would allow for better comparison.\n- Motivation\n  - It seems like sheaf GNNs were designed and evaluated on hypergraph datasets. Traffic networks are static. The paper would benefit from showing the need for sheaf GNNs in traffic data."}, "questions": {"value": "- Is the temporal dimension of the data flattened and used as the node embedding? Is that what line 18 does in Algorithm 1? Is there a linear layer to project the initial node embeddings to the hidden space?\n- In the provided GitHub codebase, you call `python run.py --model_id metr_12`. Do you train a separate model for each forecasting horizon?\n- Can you provide a pretrained model for result reproduction? Are the baseline method results copied from the original papers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "acTazU3jhg", "forum": "9uauFV3Q0u", "replyto": "9uauFV3Q0u", "signatures": ["ICLR.cc/2026/Conference/Submission13406/Reviewer_nsJs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13406/Reviewer_nsJs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762212385402, "cdate": 1762212385402, "tmdate": 1762924037782, "mdate": 1762924037782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}