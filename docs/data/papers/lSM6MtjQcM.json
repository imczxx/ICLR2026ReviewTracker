{"id": "lSM6MtjQcM", "number": 23795, "cdate": 1758348532323, "mdate": 1759896796955, "content": {"title": "AetherCode: Evaluating LLMs’ Ability to Win In Premier Programming Competitions", "abstract": "Competitive programming has emerged as a critical benchmark for evaluating the reasoning and coding capabilities of Large Language Models (LLMs). Despite impressive progress on existing benchmarks, we argue that current evaluations overstate model proficiency, masking a substantial gap between LLMs and elite human programmers. This gap arises from two key limitations: insufficient difficulty and scope of benchmark problems, and evaluation bias from low-quality test cases. To address these shortcomings, we present **AetherCode**, a new benchmark that draws problems from premier programming competitions such as IOI and ICPC, offering broader coverage and higher difficulty. AetherCode further incorporates comprehensive, expert-validated test suites built through a hybrid of automated generation and human curation, ensuring rigorous and reliable assessment. By combining challenging problem design with robust evaluation, AetherCode provides a more faithful measure of LLM capabilities and sets a new standard for future research in code reasoning.", "tldr": "", "keywords": ["Large Language Model", "Reasoning", "Code LLM", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0a0d0a1a0cde7538e6b9b5ca541b0a6f59ee670b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AetherCode, a new benchmark for evaluating the code reasoning capabilities of Large Language Models (LLMs). The authors argue that existing benchmarks are insufficient due to limited difficulty and scope, as well as evaluation biases from low-quality test cases. To address this, AetherCode is constructed using problems from premier programming competitions like the International Olympiad in Informatics (OI) and the International Collegiate Programming Contest (ICPC).\n\nSpecifically, the authors collect 456 challenging problems from world-class competitions, processed into a unified Markdown+LaTeX format. A novel hybrid approach for test case generation is proposed, combining an automated Generator-Validator (G-V) agent with intensive human expert annotation. The quality is assessed using a True Positive Rate (TPR) and True Negative Rate (TNR) metric against a large set of human solutions, reportedly achieving 100% on both."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's core strength lies in sourcing problems from truly top-tier competitions (IOI, ICPC World Finals, etc.). This is a commendable step up in difficulty and complexity compared to benchmarks sourced primarily from online judges like LeetCode or CodeForces.\n2. The most significant contribution is the meticulous attention paid to test case quality. The proposed methodology of evaluating the entire test suite as a binary classifier using TPR and TNR on a large corpus of human submissions is a novel and valuable idea for the community. Achieving 100% TPR/TNR on their collected set is an impressive feat of engineering.\n3. The detailed curation pipeline, from PDF conversion to multi-dimensional expert categorization (difficulty, algorithm tags), is thorough and sets a high standard for future benchmark development (Figure 1)"}, "weaknesses": {"value": "1. The paper claims AetherCode is the first benchmark to systematically collect problems from premier programming competitions worldwide. This claim is overstated. The paper itself cites several recent works with similar goals, such as USACO Bench, OJBench, and ICPC-Eval, but fails to adequately differentiate itself. Merging the data sources from several existing benchmarks and stating the comprehensiveness should not be a significant contribution to me. A more nuanced discussion is needed to pinpoint AetherCode's unique value proposition beyond just scale. \n\n2. While the TPR/TNR metric is a great idea, the claim of achieving a 100% True Negative Rate (TNR) is only as strong as the set of incorrect solutions it was tested against.\n- 2.1 The paper provides little detail on the nature of the 30,000+ incorrect human submissions. Do they include solutions with subtle, hard-to-find bugs (e.g., off-by-one errors, mishandled edge cases, time-limit-exceeded on specific inputs), or are they mostly solutions that fail on simple cases?\n- 2.2 A 100% TNR on a potentially limited set of known failure modes does not guarantee robustness against novel incorrect logic that LLMs might produce. The claim, while technically true for their specific dataset, may imply a higher level of generalizability than is warranted.\n\n3. The evaluation in Section 3 is convincing and insightful. Beyond showing that stronger models perform better, the paper offers little insight into why they succeed or fail.\n- 3.1 The performance breakdown by algorithm category (Table 4) is a good start, but the analysis is shallow. For instance, why do models struggle with \"Computational Geometry\" or \"Problems on Trees\"?  What specific reasoning patterns are they failing to capture?\n- 3.2 The paper could have included a qualitative analysis of model failures. Are the errors syntactic, logical, algorithmic (e.g., choosing a brute-force approach instead of dynamic programming), or implementation-related? This would provide far more value to researchers looking to improve these models."}, "questions": {"value": "I wonder why the conclusion claimed by this paper is against the breaking news, like OpenAI and Google Gemini achieving golden medals in IOI competitions? What is the difference between AetherCode and the newest IOI contest?\n\nhttps://www.reddit.com/r/MachineLearning/comments/1mnpqu7/n_openai_delivers_goldmedal_performance_at_the/"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CUVYkzgn1P", "forum": "lSM6MtjQcM", "replyto": "lSM6MtjQcM", "signatures": ["ICLR.cc/2026/Conference/Submission23795/Reviewer_eg8w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23795/Reviewer_eg8w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760838278579, "cdate": 1760838278579, "tmdate": 1762942810933, "mdate": 1762942810933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "## Paper Summary\n\nThis paper presents AetherCode, a new benchmark designed to thoroughly evaluate the reasoning and programming capabilities of large language models (LLMs) using problems sourced exclusively from premier competitive programming contests such as IOI and ICPC. The authors identify two major deficiencies in existing benchmarks: (1) they lack sufficient difficulty and problem diversity, often consisting of relatively simple tasks (e.g., LeetCode-style problems), and (2) their evaluation suites frequently contain weak or incorrect test cases, resulting in inflated model performance. AetherCode addresses these issues by offering: (1) 456 carefully curated problems spanning multiple difficulty levels and algorithmic domains, (2) high-quality and rigorously validated test cases constructed through a Generator–Validator agent system augmented by expert audits, and (3) a comprehensive evaluation of 17 leading reasoning and non-reasoning LLMs, revealing a significant performance gap. The results clearly demonstrate that even top-tier LLMs can solve only a small fraction of the benchmark, confirming that substantial progress is still needed before LLMs can perform at the level of world-class human competitors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "## Strengths\n\n1. Benchmark novelty and rigor. AetherCode is the first benchmark to systematically incorporate premier programming contest tasks, establishing a far more challenging and realistic evaluation setting.\n\n2. Strong evidence of current reasoning limitations in LLMs. The steep decline in Pass@1 performance as problem difficulty increases highlights persistent weaknesses in deep algorithmic reasoning.\n\n3. Clear and well-structured presentation. The paper is clearly written, logically organized, and easy to follow. The motivation, methodology, and findings are presented coherently, making the work accessible even for those not deeply familiar with competitive programming-style evaluation."}, "weaknesses": {"value": "## Weaknesses\n\n[1] Limited dataset scale. While the problems are high-quality, a total of 456 is still smaller than some widely adopted benchmarks, potentially limiting coverage in long-tail algorithmic domains.\n\n[2] Clarification needed on contamination guarantees. Although timestamps are included, it remains unclear whether contamination checks confirm that evaluated LLMs were not trained on these problems prior to their incorporation into the dataset. Stronger contamination analysis or preventive measures would improve confidence in the benchmark’s integrity.\n\n[3] Difficulty labeling procedure requires further explanation. The paper states that difficulty categories are based on human competition outcomes, but it would be helpful to describe the exact criteria or thresholds used, particularly for distinguishing “Hard” versus “Extreme.”\n\n[4] Limited reporting on coverage of test cases. While the benchmark claims 100% TPR/TNR on a collected set of human solutions, metrics such as input space coverage or alignment with typical competitive programming corner-case patterns would further validate robustness.\n\n[5] Missing related work. The paper would benefit from acknowledging and comparing against recent benchmarks that also focus on code LLMs.\n\n1. Dynamic Benchmarking of Reasoning Capabilities in Code Large Language Models Under Data Contamination (ICML 2025)\n\n2. DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large Language Models in Code Generation (ACL 2025 Finding)\n\n3. Codereval: A benchmark of pragmatic code generation with generative pre-trained models (ICSE 2024)\n\n4. EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories (Neurips 2025)\n\n5. Evaluating and Improving LLM-based Competitive Program Generation\n\n6. PPM: Automated Generation of Diverse Programming Problems for Benchmarking Code Generation Models (FSE 2024)"}, "questions": {"value": "1. How is the \"difficult\" determined in Table 1?\n\n2. Do you have insights to scale the problem generation?\n\n3. What is the coverage of the test cases?\n\n4. Any ablation study of the effectiveness of the agent-based test case generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KNgT91MOlz", "forum": "lSM6MtjQcM", "replyto": "lSM6MtjQcM", "signatures": ["ICLR.cc/2026/Conference/Submission23795/Reviewer_Lxkp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23795/Reviewer_Lxkp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761378717140, "cdate": 1761378717140, "tmdate": 1762942810702, "mdate": 1762942810702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a coding benchmark based on top high-school and college programming contests (mainly IOI and ICPC). The problems are hand annotated for difficulty and categories of problems allowing a more finessed analysis. The test cases are tested against many contest submissions (both correct and incorrect) and additional test cases are generated by hand and automated systems and reviewed by experts. This level of programming contests is the natural next step for programming contest based benchmarks and based on what is stated in the paper a lot of care has gone into it's quality."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper collects a substantial corpus of high-quality programming contest problems and formats them as a benchmark.\n- The benchmark seems well annotated to allow for more detailed study.\n- According to the paper much care was taken to ensure problem and especially test-case quality."}, "weaknesses": {"value": "- The paper fails to detail where the example solutions came from and whether the creators of the problems or sample solutions consent to their work being used as an ML benchmark.\n- It would be useful to know the difficulty breakdown in each category for assessing whether e.g. the models are really bad at Tree type problems or if the small number of tree problems all happen to be exceptionally difficult."}, "questions": {"value": "- I think the IOI is aimed more at high-school students than middle-school L145\n\n- \"The performance of o4-mini-high and\nGemini-2.5-Pro is exceptional, establishing a significant performance gap that places them\nin a tier of their own above other models. Furthermore, they are the only three models capable of\nsuccessfully solving problems at the “Extremely Difficult” level.\" you mention two models and then say they are the only *three* that solve extreme problems. Reword for clarity.\n\n- If the authors have time, I would like to see the performance of Claude 4.5 Sonnet\n\n- It would be nice to mention other efforts to improve benchmark quality and verify answers or test-cases. Two examples are:\nPlatnumBench http://platinum-bench.csail.mit.edu/\nSWE-bench Verified https://openai.com/index/introducing-swe-bench-verified/"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "How were the answers to the contest problems sourced? What is the copyright status of the original questions and are the creators ok with their use in ML research?"}, "rating": {"value": 10}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1askwLcqfg", "forum": "lSM6MtjQcM", "replyto": "lSM6MtjQcM", "signatures": ["ICLR.cc/2026/Conference/Submission23795/Reviewer_u8So"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23795/Reviewer_u8So"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965446470, "cdate": 1761965446470, "tmdate": 1762942810518, "mdate": 1762942810518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AetherCode, a benchmark designed to evaluate LLMs’ coding capabilities in premier programming competitions. The benchmark sources problems from top-tier contests such as IOI and ICPC, and constructs rigorous test cases through a hybrid approach combining an automated Generator–Validator (G-V) agent and professional human experts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. A harder and more discriminative coding benchmark is very timely and beneficial to the community, especially as existing datasets are approaching saturation for frontier reasoning models.\n2. The inclusion of multiple ICPC/IOI medalists for test case construction is impressive and likely ensures very high evaluation quality. This is the main contribution of the paper from my opinion."}, "weaknesses": {"value": "1. While the benchmark’s data source (premier contests) is appealing, the distinction from datasets such as LiveCodeBench Pro is not fully convincing. Those benchmarks already cover similar contest problems, so the contribution may appear incremental without a clearer articulation of what new research insights AetherCode enables.\n2. The evaluation section mainly presents aggregate Pass@N scores but lacks qualitative or failure-case analyses. For instance, what types of reasoning or algorithmic errors are most common? The findings seem simply descriptive without such insights."}, "questions": {"value": "1. The paper states that using the CodeForces judging service poses “compliance risks” since crawlers are explicitly prohibited by CodeForces. Could the authors provide an official citation or link to this policy?\n2. The G-V Agent system is briefly described but remains under-specified. Could the authors elaborate on the exact mechanism? Also, how effective is the G-V agent alone, without expert correction?\n3. How does AetherCode handle language diversity? Is it possible to extend the benchmark to multi-language evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2LZ5Qn88ni", "forum": "lSM6MtjQcM", "replyto": "lSM6MtjQcM", "signatures": ["ICLR.cc/2026/Conference/Submission23795/Reviewer_dXGB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23795/Reviewer_dXGB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762193205201, "cdate": 1762193205201, "tmdate": 1762942810332, "mdate": 1762942810332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}