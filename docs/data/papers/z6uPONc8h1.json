{"id": "z6uPONc8h1", "number": 24296, "cdate": 1758355073306, "mdate": 1762944310270, "content": {"title": "Who's Manipulating Whom? Epistemic Grounding to Break Recursive Validation Loops in Large Language Models", "abstract": "Large Language Models optimized for helpfulness through Reinforcement Learning from Human Feedback (RLHF) can exhibit systematic vulnerabilities to epistemic manipulation. We investigate this through controlled machine-to-machine negotiations (n=49) where AI agents assume buyer/seller roles with asymmetric information. Our analysis reveals three interaction patterns: fair competition, achieving 99.1% efficiency relative to the Nash equilibrium, systematic manipulation, creating 71% profit advantages, and cooperative truth-seeking with 100% success rates. We observe systematic failures where models violate optimization directives in 16% of cases, indicating that alignment training can override rational behavior under strategic pressure. Model selection emerges as more impactful than strategy optimization, with reliability differences accounting for 60\\% of outcome variance. We propose Epistemic Grounding as a framework to improve AI system reliability through model tiering, verification protocols, and training objective modifications. Our findings suggest careful model selection and epistemic safeguards are essential for deploying AI in high-stakes strategic interactions.", "tldr": "LLMs risk epistemic hollowing and over conformity, producing ungrounded outputs prone to manipulation. We propose Epistemic Grounding to break recursive feedback loops and anchor AI in truth.", "keywords": ["Epistemic Grounding", "Recursive Validation Loops", "Amplification Cascading", "Architectural Reconstruction", "Systematic Evidence Evaluation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/67ed5a6780c57c2ac0bcf07372d17e604d8292e3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The manuscript makes pairs of language models play a buyer-seller game against each other, and analyze the game outcomes. (This is a rather short summary, because I failed to extract much more information out of the paper other than that.)"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "After a good-faith attempt at identifying strengths of this manuscript, I have not been able to find any. I will discuss in the section below why that is the case."}, "weaknesses": {"value": "The paper is clearly generated by language models and/or coding agents, likely with limited human involvement, and yet I see no declaration in the paper on AI use. The following observations make me think so:\n- Undefined quantities and undefined elements. The experiment setup has tens of different setup elements but few are defined or explained. Take information asymmetry as an example: this is a setup element mentioned throughout the paper and presented in several tables, but I found no mention of what the \"information\" here refers to, nor any mention of what asymmetry there is between the two players. As a result, most results in the manuscript appear incomprehensible to me.\n- Reference to nonexistent theorems. The conclusion mentions \"our strategic asymmetry theorem proves ...\", but that is the only place where the word \"theorem\" occurs in the paper.\n- Unprofessional and unreasonable formatting. The formatting for text and formulas is almost unreadable.\n- Arbitrary thresholds and hallucinated numbers. Of the 18 tables in the 9-page body, 10 involve seemingly arbitrary categorization (e.g. low/med/high-risk) often without specifying the relevant criteria. Table 18 contains an analysis of (based on my interpretation) the economic returns from conducting the experiments in this manuscript, showing an alleged $1247 value of information from the results and a 692% ROI, again with no justification. My personal view is that these estimates are overly optimistic.\n\nOther weaknesses include the lack of a clear motivation for studying the buyer-seller game, much earlier works in similar setups [1], and highly opaque and uninterpretable results. They are insignificant compared to the first one outlined above, but would otherwise have been fatal.\n\n[1] LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games (2023)"}, "questions": {"value": "Please refer to the weaknesses section, where I have outlined all my concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3897BoGXrS", "forum": "z6uPONc8h1", "replyto": "z6uPONc8h1", "signatures": ["ICLR.cc/2026/Conference/Submission24296/Reviewer_qd2y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24296/Reviewer_qd2y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951121300, "cdate": 1761951121300, "tmdate": 1762943033872, "mdate": 1762943033872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "dz0V4VwIvl", "forum": "z6uPONc8h1", "replyto": "z6uPONc8h1", "signatures": ["ICLR.cc/2026/Conference/Submission24296/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24296/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762944309138, "cdate": 1762944309138, "tmdate": 1762944309138, "mdate": 1762944309138, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies negotiation between large language models under asymmetric information, arguing that model choice drives most variance in outcomes and that alignment objectives can override rational decision-making.\nUsing a buyer–seller setup with a known equilibrium, the authors report inefficiencies and propose an “Epistemic Grounding” framework to improve robustness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic is timely and connects LLM behaviour to strategic reasoning and game-theoretic evaluation.\n2. The setup is simple and interpretable, allowing comparison with an analytical Nash solution.\n3. Measuring inefficiency as “economic waste” gives the results practical relevance."}, "weaknesses": {"value": "1. The related works section is narrow and the bibliography is inconsistently formatted and incomplete. Several citations lack venue information or proper styling, which reduces professionalism and traceability.\n2. The dataset is small, with only 49 negotiations, and results are not repeated across seeds or independent runs.\n3. Key constructs such as “prompt violation” and “alignment override” rely on subjective categorisation that may bias results.\n4. Quantitative reporting is limited, with percentages and claims presented without full calculations or uncertainty estimates.\n5. The discussion of limitations is superficial and does not address sample size, model heterogeneity, or potential bias from re-prompting and parsing.\n6. The equation on line 178 should be written on a single line for clarity.\n7. Figure 1 is presented as a meme, which adds little value and weakens the paper’s presentation.\n8. The references are real but inconsistently formatted and should follow a uniform citation style."}, "questions": {"value": "1. How were repeated runs or seeds handled, and are the 49 negotiations unique or aggregated?\n2. How were “prompt violations” identified, and were annotators blinded to the model identities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BmdEmjYiCr", "forum": "z6uPONc8h1", "replyto": "z6uPONc8h1", "signatures": ["ICLR.cc/2026/Conference/Submission24296/Reviewer_g3Y9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24296/Reviewer_g3Y9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993981733, "cdate": 1761993981733, "tmdate": 1762943033617, "mdate": 1762943033617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is about economic incentives among AI models. It argues that AI systems that have undergone alignment training may not behave rationally under strategic pressure. It proposes a framework called \"Epistemic Grounding\" that it claims improves AI system reliability."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Apart from the general problem area---studying how language models behave under strategic incentives---I had trouble identifying strengths, because I had great difficulty following the paper."}, "weaknesses": {"value": "The paper is extremely unclear.\n\nThe abstract does not articulate what problem it is trying to solve, what the concrete solution is, or why it ought to work.\n\nWhile the introduction does provide a list of \"key contributions\" it does not do a good job contextualizing these contribution statements; thus, it remains unclear what the contributions are or why they are valuable.\n\nThe related work section contains references, but it is unclear what _this_ work is, so it is unclear how it _relates_ to the related work.\n\nThe theoretical framework is not a framework per se; it just introduces a particular game.\n\nThe whole document reads as more of an experiment log than a research paper. It does not communicate effectively to an audience unfamiliar with the work. It does not explain the claims it is making, how it arrives at those claims, or what any of it means.\n\nEntire sections are presented as tables with no accompanying text.\n\nThe conclusion mentions a \"strategic asymmetry theorem\" which \"proves seller-unbounded strategies dominate buyer-unbounded strategies.\" However, no such theorem is presented anywhere, and the word \"theorem\" does not appear anywhere else in the paper.\n\nI was completely lost the entire time I was reading it."}, "questions": {"value": "I'll provide some examples, for clarity, but I don't expect answers to my questions will change my assessment of the paper.\n\n- The paper mentions \"independence day gaslighting\" without ever defining it, as though it should be clear what that means.\n\nCome to think of it, I'm actually not sure that the paper defines _any_ of the terms it uses.\n- \"epistemic manipulation\"\n- \"efficiency\"\n- \"profit advantages\"\n- \"success\" / \"failure\"\n- \"waste\"\n- \"systematic economic inefficiencies\"\n- \"economic quantification\"\n- \"model selection\" (what models are we selecting between? how are we selecting?)\n- \"more impactful\"\n- \"mutually acceptable agreements\"\n- \"optimization directive violations\"\n- \"systematic failure costs\"\n\nIt draws conclusions without any supporting argumentation:\n- \"with clear extrapolation to market scale impacts\"\n- \"the choice becomes clear: develop robust epistemic grounding mechanisms or watch optimization pressure systematically erode the foundations of truth in the age of artificial intelligence\" -- why is _this_ the choice we \"clearly\" face? what _are_ \"epistemic grounding mechanisms\"? what makes a mechanism _robust_? and robust _to what_?\n\nIt provides no justification or explanation for why it does anything.\n- \"we identify three distinct clusters with centroids\" -- so what? why do this? what do the clusters tell us?\n- \"we formalize AI deployment decisions as a multi-criteria optimization problem.\" -- why are we defining a new formalism on page 8?\n- \"we develop comprehensive risk metrics for deployment decisions\" -- risk _of what_?\n- \"we establish deployment thresholds based on application criticality and risk tolerance\" -- thresholds _of what_? _for whom_?\n- \"our multi-objective optimization [...] provides practical deployment tools\" -- what tools? for deploying what? who would use these tools?\n- \"AI economic behavior exhibits unique patterns requiring dedicated frameworks.\" -- this statement is totally vacuous."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bPiVneKEaE", "forum": "z6uPONc8h1", "replyto": "z6uPONc8h1", "signatures": ["ICLR.cc/2026/Conference/Submission24296/Reviewer_41u2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24296/Reviewer_41u2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762461063381, "cdate": 1762461063381, "tmdate": 1762943033197, "mdate": 1762943033197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates systematic vulnerabilities of LLMs, specifically those optimized through RLHF, to epistemic manipulation. The authors conduct a series of 49 controlled machine-to-machine negotiations, assigning AI agents buyer and seller roles with asymmetric information. Their analysis identifies patterns of fair competition, systematic manipulation leading to profit advantages, and cooperative truth-seeking. The study quantifies economic inefficiencies, highlights model selection as key factor in negotiation outcomes, and introduces \"epistemic grounding\" as a framework to enhance AI system reliability through model tiering and verification."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper introduces somewhat novel perspective by quantifying economic inefficiencies and strategic failures in AI-AI interactions."}, "weaknesses": {"value": "1. This study is based on a limited number of negotiations (i.e., n=49) across a small set of models and a single, simplified negotiation scenario. The explicit claims of \"market-scale impacts\" and \"billions in cumulative losses\" are vastly overstated and fundamentally unsupported by such a narrow empirical foundation. This limited scale severely impacts the generalizability of the findings.\n\n2. The concept of \"epistemic grounding\" is central to the paper's title, abstract, and conclusion, yet its technical specifics are largely absent from the main paper. While the paper (e.g., section 6) focuses on model classification, multi-objective optimization for deployment, and risk analysis, but it does not detail how they would be concretely implemented within an LLM architecture of fine-tuning process to actively improve their truth-seeking or resistance to manipulation.\n\n3. While this paper discusses about the game-theory perspective which has been well-established, their literature review is quite weak. I believe that the authors for supplementing their literature review to be more solid.\n\n4. For the figure 1, it fails to convey any meaningful and scientific view. A proper data visualization, such as a negotiation trace, price distribution across round, or an architectural diagram of the multi-agent setup, would have significantly enhanced the paper's scientific rigor and clarity."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1F9K9x8JAo", "forum": "z6uPONc8h1", "replyto": "z6uPONc8h1", "signatures": ["ICLR.cc/2026/Conference/Submission24296/Reviewer_qPKg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24296/Reviewer_qPKg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762638751050, "cdate": 1762638751050, "tmdate": 1762943032847, "mdate": 1762943032847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}