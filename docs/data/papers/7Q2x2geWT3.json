{"id": "7Q2x2geWT3", "number": 18445, "cdate": 1758287869986, "mdate": 1759897102968, "content": {"title": "EXPONENTIAL MAP MODELS AS AN INTERPRETABLE FRAMEWORK FOR GENERATING NEURAL SPATIAL REPRESENTATIONS", "abstract": "A fundamental challenge in neuroscience and AI is understanding how physical space is mapped into neural representations.\nWhile artificial neural networks can generate brain-like spatial representations, such as place and grid cells, their “black-box” nature makes it difficult to determine if these representations arise as general solutions or as artifacts of a chosen architecture, objective function, or training protocol.\nCritically, these models offer no guarantee that learned solutions for core navigational tasks, like path integration (updating position from self-motion), will generalize beyond their training data.\nTo address these challenges, we introduce a first-principles framework based on an exponential map model.\nInstead of using deep networks or gradient optimization, the presented model uses generator matrices to map physical locations into neural representations through the matrix exponential, creating a transparent framework that allows us to identify several exact algebraic conditions underlying key properties of neural maps.\nWe show that path invariance (ensuring location representations are independent of traversal route) is achieved if the generators commute, while translational invariance (maintaining consistent spatial relationships across locations) demands generators producing orthogonal transformations.\nWe also show that preserving the metric of flat space requires the eigenvalues of the generator matrices to form sets of roots of unity.\nFinally, we demonstrate that the proposed framework constructs diverse biologically relevant spatial tuning, including place cells, grid cells, and context-dependent remapping. \nThe framework we propose thus offers a transparent, theoretically-grounded alternative to “black-box” models, revealing the exact conditions required for a coherent neural map of space.", "tldr": "We use an exponential map model as an interpretable framework to build spatial representations that exactly solve navigational tasks including path integration, metric preservation and spatial similarity invariance", "keywords": ["Grid cells", "place cells", "spatial representation", "exponential map"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a467f8db2c4941664ca0ae0cb71be4a6f7076b7a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a first-principles framework for generating neural spatial representations using exponential map models based on generator matrices. Rather than relying on deep learning's \"black-box\" approaches, the authors derive exact algebraic conditions for key properties of neural spatial maps: (1) commuting generators ensure path-independent representations for reliable path integration, (2) skew-symmetric generators produce translationally invariant similarity structures ideal for egocentric navigation, and (3) generator eigenvalues forming roots of unity preserve the flat metric of space. The framework generates diverse biologically plausible spatial tuning including place cells, grid cells, and context-dependent remapping."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Novel theoretical contribution: The use of generator matrices with the matrix exponential to construct spatial representations is elegant and, to my knowledge, novel in this explicit form. The mathematical framework is transparent and interpretable, addressing a key limitation of deep learning approaches.\n\n2. Rigorous mathematical development: The derivations are exceptionally clear and well-structured. The authors systematically build from basic exponential maps to derive exact algebraic conditions for path invariance, translational invariance, and metric preservation. The appendices provide thorough derivations for readers seeking additional detail.\n\n3. Excellent writing quality: The paper is very well written, with clear explanations that balance mathematical rigor with intuitive understanding. Figure 1 provides an excellent conceptual overview.\n\n4. Biological relevance: The framework generates diverse neural tuning curves (grid cells with different symmetries, place-like cells) and can model remapping behavior. The connection to modular organization of grid cells and the Bessel function analysis (Appendix F) showing spacing ratios similar to experimental observations (√2) is particularly compelling.\n\n5. Unification: The framework elegantly unifies multiple spatial representations under a single mathematical structure, showing how different tuning properties emerge from different choices of generator eigenvalues and similarity functions.\n\n6. Transparency over black-box models: Unlike RNN-based approaches, this framework allows exact specification of which mathematical conditions produce which navigational properties, making it highly interpretable."}, "weaknesses": {"value": "## Major Issues\n\n1. Limited contextualization with prior work:\n  * The introduction cites only Ginosar et al. (2023) regarding grid cells providing a metric for space, but there is substantial earlier literature on this topic that should be acknowledged.\n  * Whittington et al. (2020) is characterized as a \"black-box\" model, but it actually makes explicit theoretical claims about spatial representations based on prior work by others. The distinction between truly black-box models (Banino et al., Cueva & Wei) and more theory-driven approaches should be clearer.\n  * The relationship between this work and previous approaches could be more explicitly stated in the introduction, though it becomes clear later in the paper.\n\n2. Missing connections to transition/successor representation literature:\n  * Lines 226-228 claim that similarity translational invariance has not been shown before \"to the best of our knowledge.\" However, related concepts appear in prior work on transition coding and successor representations in grid cells (Stachenfeld et al. 2017, Waniek 2018 & 2020, possibly Rebecca et al. 2025 on spatial periodicity). These should be checked and cited if relevant.\n  * Section 2.4 states the metric preservation goal \"as proposed by (Gao et al., Xu et al.)\" but similar computational properties for grid cells were also motivated in the transition coding literature that I just mentioned\n\n3. Incomplete biological implementation discussion:\n  * Around the end of Section 2.2, it would strengthen the paper to briefly discuss what the mathematical findings mean in terms of potential neural implementation of G_x and G_y. How might biological circuits realize these generator matrices?\n\n4. Limited discussion of learning mechanisms:\n  * Section 4 (Limitations) acknowledges this gap, but the paper would be stronger with at least brief speculation about how biologically plausible learning rules might converge to these solutions. The authors mention this as future work, but some initial thoughts would be valuable.\n\n\n## Minor Issues\n\n1. Awkward phrasing: Line ~111: \"Equation (1) does what we intended it to;\" is oddly casual and should be reworded more formally.\n2. Paragraph flow issues:\n  * The paragraph after Equation (3) is difficult to unpack. Reordering the sentences could improve clarity.\n  * The final paragraph of Section 2.3 is somewhat unclear and could be revised for better flow.\n3. Digression: Lines 228-234 contain an interesting but substantial digression about batch/layer normalization that disrupts the main narrative. Consider moving this to a footnote or brief remark.\n4. Exponential family connection: Line 375 discusses the Gaussian as a special case. This could potentially be extended to the exponential family with linear sufficient statistics. Prior work by Pouget (Beck et al. 2007) suggests neural responses should fall into this category, which might be worth discussing.\n5. LLM usage statement location: The ICLR 2026 guidelines require the LLM usage statement in the main body, not just the appendix. Please move the statement from Appendix A to the main text.\n6. Notation clarity: While generally clear, ensuring consistent notation throughout (especially regarding conjugate eigenvalues and the block structure) would help readers following the mathematical derivations.\n\n## Minor Corrections\n1. Check citation formatting throughout\n2. Ensure all figures are referenced in order in the text\n3. Consider adding a table summarizing the algebraic conditions and their corresponding properties for easy reference"}, "questions": {"value": "1. Can you clarify the relationship between your similarity translational invariance result and prior work on successor representations and transition systems in spatial coding?\n2. How do you envision the generator matrices G_x and G_y being implemented or learned in biological neural circuits?\n3. In the remapping model (Section 2.5), you use a single scalar context signal s. How would the framework extend to high-dimensional context representations (e.g., visual scenes)?\n4. The choice of orthogonal matrix R remains free in your framework. Beyond energy constraints or non-negativity, are there other biologically motivated constraints that might determine R?\n5. Could you expand on the connection to the exponential family (beyond Gaussians) for the similarity preservation framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zI3DUvbEZS", "forum": "7Q2x2geWT3", "replyto": "7Q2x2geWT3", "signatures": ["ICLR.cc/2026/Conference/Submission18445/Reviewer_zmDx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18445/Reviewer_zmDx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18445/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761217326435, "cdate": 1761217326435, "tmdate": 1762928143207, "mdate": 1762928143207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission develops a simple and interpretable framework for generating and understanding spatial representations that are found in the hippocampal formation. The authors show how different properties that are presumably important for spatial representations shapes the kinds of representations that emerge in their framework, and they demonstrate that these are aligned with experimentally observed functional classes, such as grid cells and place cells."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper was very well written and easy to follow. \n\n2. Figure 1 was very nicely done and helped make the work more clear. \n\n3. The transparent framework is a welcomed compliment to the RNN models that have become very popular. \n\n4. The found sqrt(2) ratio of Bessel function 0s is really interesting and fits nicely with the experimentally found ratio in grid module spacing. \n\n5. The fact that the same framework can find both grid and place cells is nice, and the interpretable difference between the conditions that give rise to them is cool."}, "weaknesses": {"value": "1. The only major weakness I think is just that the framework, while interesting and interpretable, lacks a really clear punch. I hate getting this comment in my own work, but I was left feeling a little bit like \"what did I learn from this?\" This is especially the case since some of the different elements in this work have been explore previously (definitely not in as coherent or complete a framework as this though). \n     My one thought for addressing this is to think a little bit more about the place cell results. If you make the environment really big, and add the same requirements, do you see anything resembling the multi-place fields that are seen in big spaces? Alternatively, the place cell responses aren't perfectly \"place cell\"-like. Is this being driven by the need to have the correlation decrease from the center of the environment? Are experimentally recorded place cells that do not have as nice place fields actually more informative about distance from center, when taken as a population? \n     All of this is just to say that using the model to make some prediction that could be compared to experiments would make it more impactful. \n\nMinor points: \n\n1. I have always been a little confused by the conformal isometry argument. Grid cells from the same module, because they are viewed to be identical (up to phase), can't form a large-scale conformal isometry, right? They can only do so over a small region (up to I guess half the grid spacing). I think making it clear that this requirement is assumed to hold over small $\\Delta_x$ and $\\Delta_y$ would be helpful in making this more clear. \n\n2. I didn't really feel like Figure 1f was so helpful in understanding the remapping. Are each of those the same population vector, but for different sensory inputs? \n\n3. I thought it was interesting that the higher order M representations (Fig. 2) lead to responses that look almost honeycomb like. This was seen recently in the RNNs trained by Redman et al. (2024) NeurIPS on dual agent path integration. Maybe your framework can provide some insight on why this could occur? \n\n4. Very minor, but Ginosar et al. (2023) is cited as evidence that grid cells provide a metric of space. But that paper largely argues that it is not a metric (at least, not in the global sense). So maybe it'd be more representative to pick a different paper to reference."}, "questions": {"value": "1. Are there any other experimental findings that your framework sheds light on, beyond just the emergence of place and grid cells?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bYnELFXssm", "forum": "7Q2x2geWT3", "replyto": "7Q2x2geWT3", "signatures": ["ICLR.cc/2026/Conference/Submission18445/Reviewer_XtHm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18445/Reviewer_XtHm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18445/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663604099, "cdate": 1761663604099, "tmdate": 1762928142751, "mdate": 1762928142751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript developed a mathematical framework for generating neural spatial representations using an exponential map model.  The work builds up recent studies on studying path integration and grid cells by Gao et al (2021), McNamee et al (2021), Xu et al (2022), and also consider the extension to other types of spatial representations. The authors show that under certain conditions, this model can generate spatial representations that bear some similarity to some neuroscience observations,  such as grid cells and place cells."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The mathematical framework is elegant and for the most part principled  (although, when it comes to particular modeling settings for explaining the biological observations,  some assumptions need further justification).\n\nBy varying components of the model, it can generate neural representations that, in broad strokes, similar to spatial representations observed in the brain, in particular, in the rodent brain. \n\nThe limitations of the work were carefully discussed."}, "weaknesses": {"value": "The writing can be improved. While some of the materials are inherently technical, I should there should be better ways to organize and justify the ideas and assumptions.\n\n\nWhile the mathematical framework is elegant, it is also quite post-hoc. The explanation power of the framework is questionable. In particular, whether unique or new insights that can be derived from this framework remains unclear, given most of the basic phenomena in neuroscience covered in the paper can also be explain by various alternative models. I think highlighting the unique contribution of the work beyond prior work would be important for the revised version of the paper.\n\nThe authors show that by changing M, different tuning patterns can be generated, such as square grid for M=2 and hexagonal grid for M=3. This is interesting. But it is also not surprising, given that these are basically superpositions of plan waves. I think the paper would be made much stronger if the theoretical framework can be used to explain why some network training schemes lead to square grids, while others lead to hexagonal grids or other pattern.\n\nThe comparison to the experimental data is vague and could be made more rigorous. \n\nLine 226- 230 stated that the similarity translational invariance has not been explored explicitly in prior work.  I personally find that the similarity translations invariance is not surprising, and was present in many prior studies on neural coding, e.g., neurons with shift-invariant tuning curves that cover a continuous dimension. Place cells on a ring would be an example of this."}, "questions": {"value": "(1) Line 251-253: The authors stated “The translational invariance induced by skew-symmetric generators comes with a non-trivial ad- vantage: The similarity is invariant to a constant, non-spatial shift, similar to remapping behavior (see Section 2.5 for details and Fig. 1f) for an illustration) (Leutgeb et al., 2004; Fyhn et al., 2007). ” Can this be unpacked and can this be directly tested based on experimental data?\n\n(2) Perhaps related to last question:\nThe authors argued that hippocampal remapping can be explained by this framework. What are the specific predictions regarding remapping that could be empirically tested? At a first look, it seems that the theory predicts a preservation of the similarity structure for spatial across contexts.  Does that mean the similarity structure of each pair of place cells should be preserved?\n\n\n(3) Can the authors explain what unique insights can be learned from this work? \n\n(4) Can the theoretical framework can be used to explain why some network training schemes lead to square grids, while others training schemes lead to hexagonal grids?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Su5T2bnMoJ", "forum": "7Q2x2geWT3", "replyto": "7Q2x2geWT3", "signatures": ["ICLR.cc/2026/Conference/Submission18445/Reviewer_xaMZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18445/Reviewer_xaMZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18445/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851837019, "cdate": 1761851837019, "tmdate": 1762928142028, "mdate": 1762928142028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a transparent and mathematically interpretable framework for constructing neural spatial representations using exponential map models. Instead of relying on deep neural networks or training-based optimization, the authors define spatial encoding through matrix exponentials of generator matrices,Gx Gy, which map physical locations to neural population vectors. The paper identifies several algebraic conditions that guarantee biologically meaningful and geometrically coherent representations, e.g., commutativity, skew-symmetry and roots of unit as eigenvalues."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Conceptual Clarity and Theoretical Rigor\n  \nThe paper provides a clear derivation of spatial representations from first principles, linking algebraic conditions (commutation, skew-symmetry, eigenvalue structure) to key navigational properties.\n\n- Interpretability and Transparency\n- \nThe use of matrix exponentials allows analytical examination of conditions such as path invariance, translational invariance, and metric preservation, which are often less accessible in learning-based models.\n\n\n- Unified Framework for Place, Grid, and Remapping\n\nThe same formalism can produce multiple types of spatial tuning depending on parameter choices and eigenvalue structure. The link between roots of unity and grid-like symmetries (e.g., M=2,3) provides a coherent explanation for different spatial patterns.\n\n- Potential for Cross-Domain Generalization\n\nThe idea of exponential map representations can, in principle, extend to other domains requiring invariant manifold embeddings (e.g., sensory manifolds, abstract concept spaces)."}, "weaknesses": {"value": "- Limited Biological Mechanisms\n  \nThe framework proposed by this paper is more of descriptive, not prescriptive. It is unclear how actual neural circuits could implement commuting or skew-symmetric generators, or how such matrices could emerge through plausible learning rules.\n\n- Manual Selection of Symmetry Order M\n  \nThe emergence of different spatial patterns (e.g., square or hexagonal grids) depends on a hand-chosen symmetry order M. Without a mechanism that determines or learns M, the correspondence between algebraic symmetry and biological grid modules remains somewhat ad hoc.\n\n- Relative—but Not Absolute—Control of Grid Scale\n\nThe model relates grid spacing ratios to the zeros of the Bessel function, which captures the relative modular scaling between grid modules. However, this construction only determines relative proportions between scales and does not provide a mechanism for setting the absolute grid spacing, which in biological systems depends on physical distance calibration and velocity integration parameters."}, "questions": {"value": "- How might commuting generators arise biologically — e.g., could Hebbian learning enforce approximate commutation under specific motion statistics?\n\n- The model defines modular grid ratios using the zeros of the Bessel function $J_0$. However, $J_0$ is a radial basis in polar coordinates. Does this imply that the resulting representation is egocentric rather than allocentric? If so, how does translational invariance hold under this formulation, and what is the biological interpretation of “origin” in such a coordinate system?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bosmy6oFRF", "forum": "7Q2x2geWT3", "replyto": "7Q2x2geWT3", "signatures": ["ICLR.cc/2026/Conference/Submission18445/Reviewer_En7c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18445/Reviewer_En7c"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18445/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971980919, "cdate": 1761971980919, "tmdate": 1762928141433, "mdate": 1762928141433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}