{"id": "9cUdn8GKId", "number": 22083, "cdate": 1758325734323, "mdate": 1763647406199, "content": {"title": "Offline Preference-Based Value Optimization", "abstract": "We study the problem of offline preference-based reinforcement learning (PbRL), where the agent learns from pre-collected preference data by comparing trajectory pairs. \n  While prior work has established theoretical foundations for offline PbRL, existing algorithms face significant practical limitations: some rely on computationally intractable optimization procedures, while others suffer from unstable training and high performance variance.\n  To address these challenges, we propose Preference-based Value Optimization (PVO), a simple and practical algorithm that achieves both strong empirical performance and theoretical guarantees.\n  PVO directly optimizes the value function consistent with preference feedback by minimizing a novel \\emph{value alignment loss}.\n  We prove that PVO attains a rate-optimal sample complexity of $\\mathcal{O}(\\varepsilon^{-2})$, and further show that the value alignment loss is applicable not only to value-based methods but also to actor–critic algorithms.\n  Empirically, PVO achieves robust and stable performance across diverse continuous control benchmarks. \n  It consistently outperforms strong baselines, including methods without theoretical guarantees, while requiring no additional hyperparameters for preference learning.\n  Moreover, our ablation study demonstrates that substituting the standard TD loss with the value alignment loss substantially improves learning from preference data, confirming its effectiveness for PbRL.", "tldr": "", "keywords": ["offline reinforcement learning", "preference-based reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/517face3718c484a4a10a1f6767e0ef7c065ab27.pdf", "supplementary_material": "/attachment/e8d2ae556f12c44c3b96bd9a3d40ecefb5add93f.zip"}, "replies": [{"content": {"summary": {"value": "To address the issues of high computational complexity and instability in PbRL, this paper proposes an optimization objective named the value alignment loss and validates the effectiveness of the proposed method both theoretically and experimentally."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) Through experiments, the paper verifies that PVO outperforms other methods for most tasks.\n\n2) The paper provides sufficient theoretical derivation and analysis, including a derivation of the algorithm's computational complexity."}, "weaknesses": {"value": "1) Algorithmic Aspect: The algorithm is essentially consistent with IQL [1], showing limited innovation. IQL minimizes $(R+\\gamma V-Q)^2$, while PVQ is equivalent to minimizing $(\\sum_l (Q_l-\\gamma V_l-R_l))^2$.\n\n2) Theoretical Aspect: The theoretical analysis is similar to that in paper [2], offering limited contribution.\n\n[1] Offline Reinforcement Learning with Implicit Q-Learning, ICLR, 2021.\n\n[2] Provable offline preference-based reinforcement learning, ICLR, 2024."}, "questions": {"value": "1) Motivation: Before Definition 1, the authors discuss learning a value function consistent with preference feedback. How is this reflected in the proposed value alignment loss?\n\n2) IQL Parameters: The framework is similar to IQL. For a fair comparison, what is the performance of IQL when its advantage weight parameter $\\beta$ is set to be consistent with PVO?\n\n3) Transition Model: In the practical deployment described in Section 3.4, the environment model does not seem to be used. What, then, is the purpose of training the transition model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NLw1lkvqQ6", "forum": "9cUdn8GKId", "replyto": "9cUdn8GKId", "signatures": ["ICLR.cc/2026/Conference/Submission22083/Reviewer_frBc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22083/Reviewer_frBc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761616461697, "cdate": 1761616461697, "tmdate": 1762942058460, "mdate": 1762942058460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses a critical gap in offline preference-based reinforcement learning: the tradeoff between theoretical guarantees and practical usability in existing methods. By proposing Preference-based Value Optimization, which uses a value alignment loss unifying value-based and actor-critic PbRL with rate-optimal guarantees, it delivers a unified solution that excels both in theory and experiments."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The work targets the well-documented tradeoff in existing offline preference-based RL methods, where theoretically rigorous approaches are often computationally intractable, and practical implementations suffer from suboptimal sample complexity or training instability.\n2. One advantage of the work's insight is its shift from the standard offline PbRL paradigm (first infer a reward model, then train the value function using that reward directly) to a different viewpoint: it anchors value function learning to preference alignment via an \"induced reward\" derived from the value function itself, rather than treating the reward model as the sole driver of value training."}, "weaknesses": {"value": "1. The first weakness is the paper’s inconsistent formatting of mathematical formulas, specifically, the absence of terminal punctuation in Line 318.\n2. The paper’s baseline set is limited in scope. Several existing works in PbRL use generative models, like trajectory generative adversarial networks and diffusion models for preference modeling, to infer preference-aligned behavior without relying on intermediate reward models. These should also be included."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "cH16G5zkN6", "forum": "9cUdn8GKId", "replyto": "9cUdn8GKId", "signatures": ["ICLR.cc/2026/Conference/Submission22083/Reviewer_BMtm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22083/Reviewer_BMtm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727307273, "cdate": 1761727307273, "tmdate": 1762942058197, "mdate": 1762942058197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies offline preference-based reinforcement learning and introduces Preference-based Value Optimization (PVO). The method directly aligns the learned value function with the reward inferred from human preferences through a novel value alignment loss, ensuring consistency between value estimation and preference supervision. The authors provide theoretical guarantees and strong empirical results, showing that PVO achieves stable and competitive performance across continuous-control benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed value alignment loss is conceptually sound, novel, and powerful enough to achieve strong results.\n\n- The empirical evaluation is comprehensive and consistent, demonstrating that PVO outperforms existing preference-based approaches across multiple continuous-control benchmarks.\n\n- The paper is clearly written and well-organized, offering valuable intuition on how human preference signals can be effectively integrated into value-based offline reinforcement learning."}, "weaknesses": {"value": "- The reward learning module closely follows standard preference-based MLE approaches and thus contributes limited novelty in this part."}, "questions": {"value": "- Line 3 of Algorithm 1 differs from Eq. (4); there appears to be a sign inconsistency (the “+” and “−” symbols might be reversed).\n- The proposed value alignment loss appears conceptually related to the value function inconsistency introduced in VIPO: Value-Inconsistency Penalized Offline Reinforcement Learning. A discussion clarifying the connection or distinction between these two ideas would be appreciated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BEblcKwX7D", "forum": "9cUdn8GKId", "replyto": "9cUdn8GKId", "signatures": ["ICLR.cc/2026/Conference/Submission22083/Reviewer_23sR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22083/Reviewer_23sR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808785133, "cdate": 1761808785133, "tmdate": 1762942057860, "mdate": 1762942057860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}