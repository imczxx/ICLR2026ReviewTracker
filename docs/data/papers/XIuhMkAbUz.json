{"id": "XIuhMkAbUz", "number": 22535, "cdate": 1758332463711, "mdate": 1759896861012, "content": {"title": "Enabling Conversational Behavior Reasoning Capabilities in Full-Duplex Speech", "abstract": "Human conversation is organized by an implicit chain of thoughts that manifests as timed speech acts. Capturing this causal pathway is key to building natural full-duplex interactive systems. We introduce a framework that enables reasoning over conversational behaviors by modeling this process as causal inference within a Graph-of-Thoughts (GoT). Our approach formalizes the intent-to-action pathway with a hierarchical labeling scheme, predicting high-level communicative intents and low-level speech acts to learn their causal and temporal dependencies. To train this system, we develop a hybrid corpus that pairs controllable, event-rich simulations with human-annotated rationales and real conversational speech. The GoT framework structures streaming predictions as an evolving graph, enabling a multimodal transformer to forecast the next speech act, generate concise justifications for its decisions, and dynamically refine its reasoning. Experiments on both synthetic and real duplex dialogues show that the framework delivers robust behavior detection, produces interpretable reasoning chains, and establishes a foundation for benchmarking conversational reasoning in full duplex spoken dialogue systems.", "tldr": "We propose a multimodal Graph-of-Thought framework that causally models speech acts in full-duplex dialogue, enabling more natural and interpretable conversational AI.", "keywords": ["Speech and Language", "Multimodal Learning", "Causality and Interpretability", "Reasoning and Inference"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bf108aaf700284ffc109a7db5e8c659786344b98.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a Graph-of-Thought (GoT) framework for full-duplex spoken dialogue systems that aims to mimic human reasoning rather than simply predict tokens. It identifies conversational behaviors at both high (intent) and low (speech act) levels, using causal reasoning to anticipate and explain the next action in a conversation. Trained on a mix of simulated and real dialogues enriched with human-provided rationales, the system produces interpretable chains of reasoning behind its conversational decisions."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Incorporating comprehensive reasoning in duplex systems is challenging due to its latency issues. While many previous works focus on low-level signal-based evaluation, this paper properly argues the importance of addressing cognitive and high-level features."}, "weaknesses": {"value": "1. **Data synthesis**: \n    - The paper needs to provide a more detailed and systematic description of the data synthesis pipeline, including how reproducibility and verification are ensured. Several key implementation details are omitted, for instance, how candidate backchannels are generated (e.g., whether they rely entirely on GPT prompts). The statement \"we deliberately introduce controlled overlap between speakers\" is also ambiguous and should be clarified. Evaluation on unverified datasets does not substantially strengthen the contribution to the community. Lines 260-261 suggest possible modifications to datasets for improved training (i.e., intentionally increasing events even though natural dialogues are actually not); if this is the case, it raises a question of whether the resulting training objective still reflects the true data distribution.\n\n1. **Evaluation metric**: \n\n    - Latency is a critical factor when evaluating real-time duplex systems, yet it is not reported in the current version. Additional reasoning modules within a pipeline can increase latency, and it is important to analyze this trade-off. Moreover, generation accuracy may not be the most appropriate metric in this context, and syntactic metrics such as BLEU or ROUGE may not adequately capture the semantic quality of generated rationales. A further clarification is requested regarding the measurement of filler word rate: are all types of filler words predefined, or are they dynamically identified?\n\n1. **Reproducibility & Scalability**: \n    - Line 311 mentions reliance on human-annotated rationales, but relevant implementation details are lacking. It would be helpful to clarify whether the proposed architecture can be scaled without such annotations, or if human-labeled rationales are always required for effective use.\n\n1. **Self-containedness and readability**: Although some details are provided in the appendices, they are not adequately referenced in the main text, which affects readability and self-containedness. Specific points include:     \n    - Line 181: The issue of class imbalance is not addressed.\n    - Formulations: Definitions are incomplete and should be accompanied by task-specific examples.\n        - Line 208: The meaning of \"standard basis\" should be clarified.\n        - Variables (e.g., $v$ in line 208, sa-h & sa-l in line 212) should be defined before use.\n    - Acronyms such as IPU should be defined upon first appearance.\n    - Baseline results (e.g., Table 5) appear to be missing.\n    - References to relevant appendices (e.g., Section A.2 in Section 5.1) should be added to guide the reader.\n\n1. **Editorial quality**: \n    - Numerous minor editorial issues were observed, including inconsistencies in capitalization, spacing, and citation formats. Additionally, the ordering of tables (e.g., Table 4 preceding Table 3) should be corrected.\n\nThe paper presents an interesting idea but requires major revisions in methodology, evaluation, and presentation. I recommend rejection in the current cycle, with encouragement to resubmit after substantial improvements."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "Detailed information regarding human annotations should be included. For instance, the manuscript does not specify how the authors recruited or instructed the human annotators."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yExRMaLEYf", "forum": "XIuhMkAbUz", "replyto": "XIuhMkAbUz", "signatures": ["ICLR.cc/2026/Conference/Submission22535/Reviewer_cFJ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22535/Reviewer_cFJ8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761453806592, "cdate": 1761453806592, "tmdate": 1762942265857, "mdate": 1762942265857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework for a full-duplex speech dialogue system that moves beyond direct sequence prediction, instead employing an explicit two-stage process of \"next-action awareness and prediction.\" The core idea is to model the causal reasoning underlying human conversation. The first stage of the system is a hierarchical dialogue act detector that identifies high-level communicative intentions (e.g., declarative intentions, directive intentions) and low-level speech interaction mechanisms (e.g., turn-taking, feedback, interruption) from streaming audio on a second-by-second basis. The second stage uses a Graph-of-Thoughts (GoT) reasoning module, which constructs a dynamic causal graph based on the detected actions and semantic triples extracted from text transcripts. The GoT module then generates natural language explanations that elucidate the predicted next action. To achieve this, the authors created a large-scale (192 hours) synthetic dialogue corpus containing controlled conversational events and manually verified rationales, and validated their method on both this synthetic data and the real-world Candor dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Original technical contribution of reframing the full-duplex challenge from a black-box prediction task (next segment/token) to an explicit, interpretable reasoning task (perceive -> reason -> act). Also, the application of a Graph-of-Thoughts (GoT) framework to model the evolving conversational state is a novel and well-motivated architectural choice.​\n\nThe authors have developed and released a substantial new dataset, complete with a detailed analysis of its statistical properties compared to human dialogue. The evaluation on both synthetic and real data (Candor) provides a solid empirical grounding for the paper's claims.​\n\nThe paper is well-written, clearly motivated, and easy to follow."}, "weaknesses": {"value": "The framework's representation of conversation is coarse, both temporally and semantically. Quantizing the dialogue into one-second chunks and assigning a single discrete speech-act label per chunk oversimplifies the fluid and often ambiguous nature of human interaction, a limitation the authors acknowledge."}, "questions": {"value": "The framework stops at generating reasoning results. How could we further utilize this output  by a full dialogue agent to control its   response? For instance, how would a rationale like \"Speaker1 issues a Directive, so Speaker2 should not interrupt\" be translated into a precise timing decision (e.g., inhibit speech for the next X milliseconds)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hbwi9WNz8v", "forum": "XIuhMkAbUz", "replyto": "XIuhMkAbUz", "signatures": ["ICLR.cc/2026/Conference/Submission22535/Reviewer_Pgmf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22535/Reviewer_Pgmf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761780114764, "cdate": 1761780114764, "tmdate": 1762942265649, "mdate": 1762942265649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses interpretability in full-duplex dialogue systems by proposing a shift from direct token prediction to explicit behavior reasoning. The authors present a two-stage framework consisting of hierarchical behavior detection and Graph-of-Thoughts (GoT) based reasoning that generates natural language rationales."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a clear conceptual distinction between pattern matching and reasoning in dialogue systems. The proposed \"Perception → Reasoning → Generation\" framework provides structure to the problem of interpretable dialogue modeling. The hierarchical behavior taxonomy is grounded in established linguistic theory."}, "weaknesses": {"value": "**Mismatch Between Claims and Implementation**\n\nThis is correlation mining, not causal inference in the formal sense. The paper repeatedly claims to perform \"causal inference\" but the implementation uses frequency-based co-occurrence graphs.The adjacency matrix is symmetric (undirected graph), but causation has inherent directionality (A causes B does not imply B causes A). Without directed edges, the graph cannot represent causal pathways. Furthermore, observational correlation does not distinguish causation from confounding. Required evidence for causal claims would include: intervention experiments (removing graph nodes and measuring outcome changes), counterfactual generation (reasoning about what would have happened under different conditions), or Granger causality tests (demonstrating that history of X improves prediction of Y beyond Y's own history). The paper contains none of these analyses.\n\n**The system architecture separates prediction from explanation.**\n\nThe pipeline first predicts behavior labels, then generates rationales conditioned on those predictions. This is post-hoc explanation generation rather than reasoning that drives decision-making.\n\n**High-level and low-level behaviors are trained independently despite clear pragmatic dependencies.**\n\nThe training loss and inference approximation assume conditional independence. However, pragmatic constraints create strong dependencies. For instance, if high-level intent is Acknowledgment, low-level action is unlikely to be Interruption—these are pragmatically inconsistent. Similarly, Directive intents typically pair with specific low-level actions like Turn-taking or Continuation.Conversation Analysis literature documents structured patterns like adjacency pairs (Question → Answer) and preference organization (Offers preferentially receive Acceptances; Rejections are dispreferred and marked by delays and hedges). The independent modeling cannot capture these dependencies.\n\n**The synthetic corpus exhibits systematic biases that may limit generalization.**\n\nStatistical analysis reveals significant differences from human baselines. \n\n**Missing phenomena** include repair sequences (self-correction, clarification requests), emotion dynamics (frustration escalation, sarcasm), dispreferred actions (rejections requiring hedges and accounts), and social context (power dynamics, cultural variation). The TTS synthesis cannot capture authentic prosodic variation in emotion or stance.\n\n**Multiple components use fixed rules that may not scale to new domains or contexts.**\n\nLow-level label assignment uses priority rules: Backchannel > Interruption > Turn-taking > Continuation. These priorities are hand-coded rather than learned from data and may not reflect actual pragmatic hierarchies across different contexts. Graph edges are constructed purely from co-occurrence counts without considering semantic relation strength, temporal decay (recent events should weigh more), or learned edge probabilities. GAT depth is fixed, limiting reasoning to bounded graph traversal. The system cannot adapt to domain-specific speech acts. \n\n**Human Evaluation Details**\n\nCritical methodological details are missing from the human evaluation protocol. Without blinding, raters knowing which system generated which audio introduces confirmation bias. Without randomization, order effects may occur. Without inter-rater reliability measures, score reliability cannot be assessed.\n\n**Unreported information**: Sample size, number of raters, inter-rater agreement statistics, rater expertise level, and evaluation procedure."}, "questions": {"value": "**Q1**: Given that your graph construction uses symmetric co-occurrence matrices without intervention operators or counterfactual mechanisms, can you clarify what \"causal inference\" means in your framework?\n\n**Q2**: The system predicts behaviors then generates explanations. How do you distinguish this from post-hoc rationalization? Have you tested whether explanations remain valid when features they mention are removed (faithfulness tests)?\n\n**Q3**: Why are high and low levels trained independently when pragmatic constraints create dependencies? Have you measured how often predictions are pragmatically inconsistent (e.g., Acknowledgment + Interruption)?\n\n**Q4**: What is the measured end-to-end latency? (Please provide more data to explain.)\n\n**Q5**: Have you tested on adversarial cases like sarcasm, rhetorical questions, emotional speech, or multi-party conversations? What are concrete failure examples?\n\n**Q6**: Why use symmetric undirected graphs when causation is directional? Have you compared undirected co-occurrence versus directed learned edges?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zxFdKY2JnQ", "forum": "XIuhMkAbUz", "replyto": "XIuhMkAbUz", "signatures": ["ICLR.cc/2026/Conference/Submission22535/Reviewer_VHMS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22535/Reviewer_VHMS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952443904, "cdate": 1761952443904, "tmdate": 1762942265398, "mdate": 1762942265398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a conversational behavior reasoning framework for full-duplex speech systems that explicitly models the causal relations between high-level communicative intents (e.g., constatives, directives, acknowledgments) and low-level speech acts (e.g., turn-taking, backchannel, interruption). The proposed Graph-of-Thought (GoT) architecture formalizes the perception–reasoning–generation loop in conversation, enabling interpretable predictions and rationale generation. Experiments on both synthetic duplex dialogues and real conversational speech (Candor corpus) demonstrate that the framework achieves strong performance on hierarchical speech-act detection and generates coherent, causal rationales for its decisions."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes a meaningful shift from black-box sequence prediction to causal reasoning over conversational behavior, arguing that next-behavior reasoning is a more human-aligned formulation for full-duplex systems.\n\nThe paper carefully constructs a dataset combining a simulation corpus with real data."}, "weaknesses": {"value": "1. Methodology\n\nThe method section is difficult to follow in parts. It is unclear how OpenIE triples (subject–relation–object) are incorporated into the graph and how they interact with the speech-act nodes.\n\nThe paper describes multiple node types (text nodes, high-level acts, low-level acts) but does not provide a clear illustrative example showing how these are connected or how causal dependencies are inferred.\n\nThe rationale generation mechanism could be better illustrated with an explicit example mapping audio input → nodes → rationale text.\n\n2. Data and Baselines\n\nThe distribution and composition of the real dataset (Candor corpus) should be described in more detail (e.g., number of speakers, domains, or event ratios).\n\nThe paper lacks baseline comparisons for both behavior detection and reasoning tasks. \n\nThe training setup could be clarified: while encoders (HuBERT, Whisper, T5, GAT) are mentioned, it is not clear which components are frozen or what models are fine-tuned, and what parameter scales are used for each experiment.\n\n3. Experimental Results\n\nIn Figure 3, the “Audio-only” modality appears to outperform “Audio + Text” or “Audio + Text + Graph” configurations, which contradicts the expected benefit of multimodal reasoning. \n\nThe metrics reported in Section 6.3 and Table 6 (human ratings) seem inconsistent — e.g., the mean ratings differ slightly between the text and table — and should be double-checked.\n\nThe evaluation focuses on BLEU/ROUGE and semantic similarity, but lacks human qualitative analysis or ablations on causal reasoning quality."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MEBEzu7c6q", "forum": "XIuhMkAbUz", "replyto": "XIuhMkAbUz", "signatures": ["ICLR.cc/2026/Conference/Submission22535/Reviewer_dbRG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22535/Reviewer_dbRG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957825246, "cdate": 1761957825246, "tmdate": 1762942265142, "mdate": 1762942265142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}