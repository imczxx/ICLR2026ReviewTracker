{"id": "qKaMU1l7Lc", "number": 4264, "cdate": 1757649699395, "mdate": 1763029702700, "content": {"title": "OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression", "abstract": "Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework. Unlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion. Within this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities. To further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets. To fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals. Experimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance.", "tldr": "", "keywords": ["multimodal motion generation", "text-to-motion", "speech-to-gesture", "music-to-dance"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/43aeca1df2e080384d106ada762a23949d98fc66.pdf", "supplementary_material": "/attachment/5be7547aa67de7a44c0ad91320efc7d9d8d3f3e0.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses multimodal human motion generation, focusing on improving the quality of generated motions and the generalization ability across different input modalities.\nTo this end, the authors propose a masked autoregressive motion transformer with causal attention, enhanced by Gated Linear Attention and RMSNorm modules.\nIn addition, they adopt a Diffusion Transformer (DiT) architecture with AdaLN and cross-attention–based conditioning mechanisms.\nExperimental results demonstrate consistent performance improvements across multiple tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and logically structured. The experimental ablations are comprehensive, allowing readers to easily understand the differences and improvements over prior work.\n\n2. The proposed task—multimodal conditional motion generation—is both challenging and promising. The idea of leveraging data from different tasks to improve generalization in motion generation is novel and relevant to the community."}, "weaknesses": {"value": "1. Although the paper claims that using data from other tasks improves the model’s performance across tasks, Table 6 in the appendix only shows results for the text-to-motion task before and after fine-tuning. The results do not clearly demonstrate cross-task improvement, and the metric changes are marginal. Could this indicate that the claimed multi-task benefit may not actually occur in practice?\n\n2. Several proposed architectural modifications seem to be engineering-level improvements rather than conceptual innovations. For instance, replacing bidirectional attention with masked causal attention, predicting one token at a time instead of in batches, and adding Gated Linear and RMSNorm modules—all follow currently popular architectural trends. Hence, presenting these as key contributions might be somewhat limited in novelty.\n\n3. The authors argue that VQ-based methods introduce quantization errors, motivating their choice of an autoencoder (AE)-based design. However, recent VQ-based models such as MoMask and LAMP have shown strong performance. It would strengthen the paper if the authors could report direct comparisons or quantitative results showing why non-quantized (AE-based) models are superior in the autoregressive generation setting."}, "questions": {"value": "1. The paper mentions that adopting DiT increases computational cost. Could the authors provide quantitative inference-time comparisons between the DiT and the original MLP-based denoising network?\n\n2. Have the authors experimented with alternative conditioning injection methods, or did they directly choose AdaLN and cross-attention? If the latter, could they elaborate on why these particular mechanisms were selected, especially since the paper highlights them as a contribution?\n\n3. In the VAE component, how does the proposed design differ from that used in MARDM?\n\n4. The paper claims that the Gated Linear Mechanism reduces the attention paid to redundant frames while emphasizing relevant ones. Is there any direct experimental evidence supporting this claim?\n\n5. Could the authors provide a detailed explanation of why DiT is more advantageous than MLPs or convolution-based denoisers (as used in Stable Diffusion) for motion generation?\n\n6. In Table 4, only text-based and speech-based settings are reported. Is there a reason why music-based ablation experiments are missing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qfrC59K3P0", "forum": "qKaMU1l7Lc", "replyto": "qKaMU1l7Lc", "signatures": ["ICLR.cc/2026/Conference/Submission4264/Reviewer_hmuD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4264/Reviewer_hmuD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658058211, "cdate": 1761658058211, "tmdate": 1762917263394, "mdate": 1762917263394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "1dGGH2ucFJ", "forum": "qKaMU1l7Lc", "replyto": "qKaMU1l7Lc", "signatures": ["ICLR.cc/2026/Conference/Submission4264/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4264/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763029702030, "cdate": 1763029702030, "tmdate": 1763029702030, "mdate": 1763029702030, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework for multi-modal motion generation. The method is simple and easy to follow, and the paper is overall well written with clean figures and tables. However, despite the good presentation, the technical novelty and empirical validation appear limited, and several claims seem overstated relative to the actual contributions and results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is simple and easy to understand, which makes the paper accessible.\n\nThe writing quality, figures, and tables are clear and formatted."}, "weaknesses": {"value": "- The title and scope feel overstated. The work focuses on a limited number of modalities, yet it is framed as “omni-,” which seems misleading. More modalities could be scene environments, other human motion, etc.\n\n- The demo results are weak: generated motions often appear frozen or unnatural, and foot sliding is severe in many cases (the video provided in the .ppt file).\n\n- The paper omits many related baselines. Given the large number of existing motion generation methods, this omission makes it difficult to assess the true contribution.\n\n- The ablation study lacks visualizations or demo comparisons. From the reported results in Table 4, if I understand correctly (each row represents the baseline plus all previous modules), switching to DiT brings the largest improvement, but this is conceptually a rather smallest technical contribution. Other components (stated as contributions), such as the Gated Linear and RMSNorm modules, lead to only marginal gains, which further weakens the overall claim of the paper.\n\n- The provided demos are limited. There are no results for music-to-motion."}, "questions": {"value": "For Table 4, if I understand correctly, it seems that each row represents the baseline plus all the previous modules, but this is not clearly explained in the text. Without a detailed description, readers might mistakenly assume that each row corresponds to the baseline plus only one additional module."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KztGnLXVwe", "forum": "qKaMU1l7Lc", "replyto": "qKaMU1l7Lc", "signatures": ["ICLR.cc/2026/Conference/Submission4264/Reviewer_dqfX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4264/Reviewer_dqfX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877566315, "cdate": 1761877566315, "tmdate": 1762917263043, "mdate": 1762917263043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes OmniMotion, a unified framework for whole-body human motion generation across multiple modalities (text, speech, and music). The core contribution is a continuous masked autoregressive (MAR) transformer that employs causal attention to maintain temporal coherence, avoiding the quantization errors inherent in VQ-VAE-based discrete methods. The framework incorporates: (1) a gated linear attention mechanism for adaptive feature selection, (2) RMSNorm for training stability across heterogeneous modalities, and (3) Diffusion Transformer (DiT) blocks for refinement. The model is pretrained on text-to-motion data and then fine-tuned for speech-to-gesture and music-to-dance tasks. Experiments on HumanML3D, BEAT2, and FineDance datasets demonstrate improvements over existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The use of causal attention in MAR for motion generation is sensible given the sequential nature of motion, distinguishing this work from image-based MAR methods.\n2. The paper provides extensive experiments across three different modalities with appropriate metrics and baselines, including thorough ablation studies.\n3.  Using SMPL-X for whole-body motion across datasets is practical and enables fair comparison. This task is novel and interesting from my point of view."}, "weaknesses": {"value": "1. The paper is primarily an engineering effort combining existing techniques (MAR, DiT, AdaLN, cross-attention). \n2. The paper might somewhat overclaim the \"omni\" capabilities. The paper claims multimodal training allows \"each modality to benefit from patterns in other modalities\", but the frozen DiT and separate fine-tuning directly contradict this. Speech and music don't actually influence the core generative model. This is more similar to adaptation, not unification.\n3. Presentation issues: \n- Dense writing in technical sections makes it hard to follow."}, "questions": {"value": "1. Have you tried joint training on all three modalities simultaneously (without freezing)? This would be the actual \"omni\" approach claimed in the title.\n2. Can you provide evidence that training on music improves text-to-motion, or that speech data helps music-to-dance? Ablations training on subsets would demonstrate actual knowledge sharing."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DJV6n5jhEq", "forum": "qKaMU1l7Lc", "replyto": "qKaMU1l7Lc", "signatures": ["ICLR.cc/2026/Conference/Submission4264/Reviewer_zsey"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4264/Reviewer_zsey"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881325184, "cdate": 1761881325184, "tmdate": 1762917262612, "mdate": 1762917262612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Omnimotion, a framework for multimodal whole-body human motion generation, including text-to-motion, music-to-dance and speech-to-gesture. The usage of AdaLN, RMSNorm proves that these modules is well-adapted to multimodal-conditioned motion generation frameworks. The experiments demonstrate that OmniMotion performs well on several benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes Omnimotion, a framework for multimodal whole-body human motion generation, including text-to-motion, music-to-dance and speech-to-gesture. This framework performs well on FineDance and BEAT2 dataset."}, "weaknesses": {"value": "1. The motivation of this paper is not clear. The author mentioned that previous works focuses on single-conditioned motion generation, and many single-conditioned motion generation methods (VQ-based and continuous) suffer from their respective outcome. However, it is not well stated the relationship with multiple condition motion generation. It seems that this paper aims to tackle multimodal-conditioned motion generation, but a large proportion of Introduction describe the shortcoming of current text-to-motion methods’architecture. Overall, motivation is confusing.\n2. It seems that OmniMotion is an directly application of MAR on motion generation.\n3. HumanML3D is the main and basic benchmark for text-to-motion generation. However, OmniMotion performs not well on HumanML3D.\n4. Experiments is not sufficient. (1) After adapting to other conditions, will the performances on text-to-motion get worse? (2) Why this paper apply pretrain then finetune/adapt paradigm for other conditions beyond text?  How does the network perform if we directly trained on speech and music?\n5. MARDM is the first to apply MAR-based methods on text-to-motion generation, but it seems performs worse than the so-called suboptimal VQ-based methods (T2M-GPT, MoMask) when using the 263-dim H3D format. This paper follows the same MAR-like paradigm, but it seems that the author didn't dig into this.\n6. MAR apply a lightweight MLP diffusion head, while OmniMotion apply a 4 layers transformer. Supposed that a motion sequence with 200 frames, for each frame pose, the 4 layers transformer needs to run the whole denoising process for 200 times. This is a huge computational overhead, which may lead to OmniMotion less competitive than the previous VQ-based methods on both motion quality and computational cost."}, "questions": {"value": "1.\tLacking comparisons of many SOTA works in Year 2025 dedicated for text-to-motion in HumanML3D in Table 5. In addition, MM Dist and Diversity has the wrong place.\n2.\tShould the DiT be frozen when injecting additional conditions according to Section 3.5?  DiT is either frozen or trained in Figure 2(c). This is confusing.\n3.\tWhat is Face L2 loss?  The author didn't introduce it or cite the reference paper.\n4.\tIn line 445, “However, our method performs worse than single-modal methods. ”What is the meaning of this sentence? From Table 2, it seems that the proposed method is overall better than single-modal methods ( EMAGE, TalkSHOW). The overall statement should be further checked.\n5.\tIn line 895, “Here we mainly compare with the methods without VQ-VAE.” This paper apply MAR to design a motion generation framework and aims to tackle the shortcoming of VQ-based methods (i.e., the inevitable quantization error), as claimed in Introduction. Thus, it is not reasonable to avoid comparison with VQ-based methods.\n6.\tTable 6 is given but not referred in this paper.\n7.\tThe results of EMAGE in Table 2 is totally different with those in EMAGE paper. It would be better if the authors clarify during testing how many speakers are used and if the split of test set is same as the original BEAT2 dataset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "y5XW8fYiWu", "forum": "qKaMU1l7Lc", "replyto": "qKaMU1l7Lc", "signatures": ["ICLR.cc/2026/Conference/Submission4264/Reviewer_XUWK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4264/Reviewer_XUWK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915474597, "cdate": 1761915474597, "tmdate": 1762917262398, "mdate": 1762917262398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}