{"id": "nHwJLjWZuv", "number": 1583, "cdate": 1756894248679, "mdate": 1759898199987, "content": {"title": "Efficient Large Language Models Moderation with Multi-Layer Latent Prototypes", "abstract": "With the widespread adoption of large language models (LLMs), ensuring their safety and alignment has become a critical challenge. \nAlthough modern LLMs are aligned with human values during post-training, robust moderation remains essential to prevent harmful outputs at deployment time. \nExisting approaches, such as guard models, activation steering, and prompt engineering, each involve significant trade-offs: guard models are costly to train and deploy, and their users are typically limited to a few model checkpoints, while activation steering and prompt engineering can often degrade the quality of responses. \nIn this work, we introduce Latent Prototype Moderator (LPM), a lightweight moderation tool that assesses input safety by sparsely aggregating Mahalanobis distances to safe and harmful prototypes across multiple layers. \nBy leveraging multi-level prototypes, LPM improves both moderation robustness and performance. \nBy design, our method adds negligible overhead to the generation pipeline and can be seamlessly applied to any model. \nLPM achieves state-of-the-art performance on diverse moderation benchmarks and demonstrates strong scalability across model families of varying sizes. \nMoreover, we show that it integrates smoothly into end-to-end moderation pipelines and further improves response safety when combined with output moderation techniques. \nOverall, our work provides a practical and adaptable solution for robust, efficient safety moderation for real-world LLM deployment.", "tldr": "We allow efficient moderation by leveraging latent representations from multiple layers of existing LLMs.", "keywords": ["Moderation Tools", "Guardrails", "LLMs", "Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/adb027f60fa98d966b10548120692438f22fd8c3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper focuses on the problem of LLM input moderation where if a user queries an LLM with a harmful prompt, the LLM should be able to detect it and abstain from generating an output. The paper proposes LPM, a method that uses last token hidden representations from multiple layers of the LLM to compute class (sub-class) conditional prototypes and trains a set of aggregated weights to add sparsity.  At inference, LPM computes a safety score for a test prompt from select hidden layers and combines them via learned weights to produce P(\"\\\"unsafe\\\"\"). Experiments on various harmful and neutral datasets show both the effectiveness and efficiency of LPM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is generally easy to follow, though there is room for improvement as discussed below.\n\n- The experiments are quite comprehensive, covering multiple datasets and models, and also include thorough ablation studies.\n\n- The method is quite efficient and does not require full-scale training/fine-tuning or dataset curation, unlike guard models."}, "weaknesses": {"value": "- I have some reservations about the novelty of the work. Prior studies have already explored the use of LLM embeddings for input moderation (e.g., Abdelnabi et al., 2025; Ayub & Majumdar, 2024), and the paper does not clearly articulate how LPM meaningfully differs from these approaches. While it notes that “previous methods that utilize latent representations do not incorporate the knowledge from multiple layers, …”, if this is the primary distinction (and/or the use of Mahalanobis distance instead of Euclidean distance), the contribution may come across as somewhat incremental.\n\n- I think the paper could be strengthened by adding more competitive guard models e.g. Granite Guardians [1], ShieldGemma [2], SafePhi [3] which have shown to outperform baselines used in this paper (e.g. WildGuard, LlamaGuard). \n\n- I noticed the main results don’t show error bars or standard deviations (e.g. Tables 1-4, Figures 2-3). It’d be great to include them since they help show how consistent and reliable the results are, especially with randomness in training or data sampling. Error bars help assess the statistical significance and reproducibility of findings, which are critical for drawing reliable scientific conclusions.\n\n(Minor):\n- The equations could benefit from clearer explanations. For example, the notation is somewhat confusing: in lines 147–151, \\(\\mu_c\\) is defined as the average of class in the *sample space*, whereas in Equation (4), \\(\\mu\\) refers to the average of *layer-wise representations*. \n-The notation in Equation (4) could be improved for clarity. For instance, use \\(h_l\\), \\(\\mu_l\\) instead of \\(h_i\\), \\(\\mu_i\\) to indicate the layer index.\n\n\n- In the Conclusions section, there is redundancy in the first two sentences (lines 472–477), as they convey nearly the same message. Also, on line 482, there is a missing full stop in the phrase: “moderation needs LPM represents.”\n\n[1] Padhi, I., Nagireddy, M., Cornacchia, G., Chaudhury, S., Pedapati, T., Dognin, P., Murugesan, K., Miehling, E., Cooper, M. S., Fraser, K., et al. Granite guardian. arXiv preprint arXiv:2412.07724, 2024.\n\n[2] Zeng, W., Liu, Y., Mullins, R., Peran, L., Fernandez, J., Harkous, H., Narasimhan, K., Proud, D., Kumar, P., Radharapu, B., et al. Shieldgemma: Generative ai content moderation based on gemma. arXiv preprint arXiv:2407.21772, 2024a.\n\n [3] N. Machlovi, M. Saleki, I. Ababio, and R. Amin, “Towards safer ai moderation: Evaluating llm moderators through a unified benchmark dataset and advocating a human-first approach,” arXiv preprint arXiv:2508.07063, 2025."}, "questions": {"value": "-\tWhy does LPM use the last token embedding? What is the rationale behind this decision? do we have ablation on other tokens?\n\n-\tHow is LPM different from methods that also use embeddings of the models (Abdelnabi et al., 2025; Ayub & Majumdar, 2024)? Is the difference only the use of multiple players? And/or the use of Mahalanobis distance instead of Euclidean distance ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OkPKnn337w", "forum": "nHwJLjWZuv", "replyto": "nHwJLjWZuv", "signatures": ["ICLR.cc/2026/Conference/Submission1583/Reviewer_86ZE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1583/Reviewer_86ZE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760628293227, "cdate": 1760628293227, "tmdate": 1762915824429, "mdate": 1762915824429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a lightweight LLM moderation framework, *Latent Prototype Moderator* (LPM), designed to assess input safety and prevent LLMs from generating potentially harmful content. LPM constructs *safe* and *harmful* prototypes using hidden representations from multiple LLM layers, enabling classification via Gaussian Discriminant Analysis (GDA) for new inputs. The proposed approach achieves state-of-the-art performance on several input moderation benchmarks and can be combined with output steering methods for improved moderation performance."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed LPM is conceptually intuitive and presented in a clear, accessible manner.\n2. Experimental results demonstrate that LPM performs competitively across benchmarks, achieving strong performance comparable to more computationally intensive baselines that require additional training.\n3. The authors provide several analyses that help clarify the behavior and effectiveness of LPM."}, "weaknesses": {"value": "1. My primary concern is that the proposed LPM reuses several techniques that have previously been applied to LLM trustworthiness and safety-related tasks (e.g., [1]). The main innovation appears to be aggregating classification results across multiple layers, using a method akin to LASSO for feature selection. While the empirical results are solid, the conceptual contribution seems incremental, placing the paper on the borderline of acceptance.\n2. Given the simplicity of the proposed method, the paper could be strengthened by providing deeper insights into the underlying mechanisms that make LPM effective—particularly regarding what properties of the LLM’s hidden states enable accurate moderation.\n3. The evaluation primarily focuses on input classification metrics such as F1 score. However, input moderation alone may not capture the nuanced trade-offs in real-world safety scenarios. For instance, OpenAI’s *safe completions* approach [2] demonstrates that maintaining safety while providing helpful responses (e.g., offering emotional support in self-harm cases) may require output-centric safety mechanisms rather than pure input filtering. Hence, the evaluation could be more convincing if it discussed the broader context and limitations of input-only moderation.\n\n> **References**\n>\n> [1] *Representation Engineering: A Top-Down Approach to AI Transparency.*\n>\n> [2] *From Hard Refusals to Safe Completions: Toward Output-Centric Safety Training.*"}, "questions": {"value": "1. How does LPM handle *dual-use* scenarios [2], where user queries may have both benign and harmful interpretations (e.g., asking for the minimum energy to ignite a firework display)?\n2. LPM performs best when applied to OLMo. What factors might contribute to this difference? For example, could OLMo’s training process or architecture make it more amenable to prototype-based moderation?\n3. In Section 4.5, the important layers appear to be more distributed in OLMo, whereas they are more concentrated in other models. Could this distribution explain OLMo’s superior performance, and what might it reveal about the model’s internal representation of safety signals?\n4. Why was Gaussian Discriminant Analysis (GDA) chosen as the classification method over other alternatives? Have the authors compared GDA with simpler or more expressive classifiers (e.g., logistic regression or MLPs)?\n\n> **References**\n>\n> [2] *From Hard Refusals to Safe Completions: Toward Output-Centric Safety Training.*"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8283WyrB9T", "forum": "nHwJLjWZuv", "replyto": "nHwJLjWZuv", "signatures": ["ICLR.cc/2026/Conference/Submission1583/Reviewer_5bZL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1583/Reviewer_5bZL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760793631597, "cdate": 1760793631597, "tmdate": 1762915824195, "mdate": 1762915824195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Latent Prototype Moderator (LPM), a lightweight input-moderation method for LLMs. The approach computes Mahalanobis distances between prompt representations and safe/unsafe prototypes across multiple transformer layers, then aggregates layer-level scores with sparse weights. LPM is model-agnostic, requires no finetuning, and adds negligible inference cost. Experiments show state-of-the-art moderation performance on jailbreak and safety benchmarks, outperforming prior latent-based approaches and matching or exceeding dedicated guard models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Simple and principled method that leverages latent space geometry without requiring model finetuning or architectural changes. This makes the approach scalable and general. Empirically, the technique works across model sizes, architectures (including MoE) - unlike using fixed, off-the-shelf guard models.\n\n- Thorough empirical evaluation. The authors cover multiple safety benchmarks and jailbreak settings, and compare both against guard models and prior latent-based detectors.\n\n- Careful ablations. The analysis on number of layers selected, distance metrics (Euclidean vs Mahalanobis), data efficiency and, pre-trained vs instruction-finetuned models provide clarity on where the gains come from."}, "weaknesses": {"value": "- Relies on labeled safety data to build prototypes. While light-weight, the pipeline still needs curated data, and the paper does not deeply examine robustness to dataset bias or domain shift.\nMinor typos:\n- Equations (1) and (3) must have a class index for the covariance matrices I believe (as per GDA)."}, "questions": {"value": "1) Slightly more context on the existing baselines and their approach to latent-based input moderation would benefit the reader and help positioning of the work. Is there any insight on why the LPM approach outperforms \"Ayub & Majumdar (2024)\", \"Abdelnabi et al. (2025)\" ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C2o8WU5jRW", "forum": "nHwJLjWZuv", "replyto": "nHwJLjWZuv", "signatures": ["ICLR.cc/2026/Conference/Submission1583/Reviewer_JQoT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1583/Reviewer_JQoT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876875515, "cdate": 1761876875515, "tmdate": 1762915823832, "mdate": 1762915823832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes a method for LLM input moderation based on using internal model activations to classify prompts as harmful or safe. During classifier training, a set of labeled prompts is run through the model of interest, last token activations are captured at all layers, these are used to form class prototypes of harmful and safe prompts, and these are used to define a binary classifier. During inference, activations are captured and run through the classifier, which uses distance from the prototypes to compute a probability that the prompt is harmful. The paper compares this approach to standard guard models, as well as embedding-based and prompting-based approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The approach is novel and intuitively appealing. It scales with model quality, rather than being fixed as with guard models. Many comparisons are run, there is technical analysis of the drivers of LPM performance, and cost analysis is offered."}, "weaknesses": {"value": "The methods and evaluations used are numerous and complex, and not always clearly presented. It's also not clear that the approach offers a meaningful improvement over existing methods. Finally, some discussion of whether its implementation complexity is preferable in practice to using an off-the-shelf guard model or other alternatives would be warranted.\n\nIn Table 1, it appears that LPM does not offer clear advantages over WildGuard and embedding-based approaches, except when applied to OLMo2-7B-Instruct, and there the improvement is marginal (I am inferring that the Instruct model is being shown based on Table 7, but unlike in that table, the labeling in Table 1 is unclear). \n\nAnd that model is not shown in Table 2, which is the only place that has the critical comparison to performance of the \"Base Model\". As the LPM method utilizes the activations of the model being used for inference, it's reasonable to wonder whether it actually improves over the refusal behavior of the inference model itself. Again the labeling is unclear as to which versions of the model are being used. Labeling could be clearer throughout that table. It's not apparent that the data in the table supports the claim that LPM meaningfully stacks with other techniques. It doesn't seem to add much to \"Zheng et al. (2024)\", and while the boost to F1 relative to \"Unconditioned Steering\" alone is substantial, the absolute F1 value is rather poor.\n\nIt's not clear that one can make a general claim about the utility of reasoning tokens based on a null effect from a single 4-B model shown in Table 3."}, "questions": {"value": "In Table 2, why was only the WildGuardMix set used? Why is the best-performing model from Table 1 not shown? Can you clearly describe the comparisons made in each row of the table?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iIHSYg96Vt", "forum": "nHwJLjWZuv", "replyto": "nHwJLjWZuv", "signatures": ["ICLR.cc/2026/Conference/Submission1583/Reviewer_KRQX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1583/Reviewer_KRQX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921995904, "cdate": 1761921995904, "tmdate": 1762915823697, "mdate": 1762915823697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}