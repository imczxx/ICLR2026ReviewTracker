{"id": "ktHj6YazEE", "number": 4604, "cdate": 1757725632495, "mdate": 1759898023723, "content": {"title": "Multi-Scale Group Relative Policy Optimization for Large Language Models", "abstract": "Reinforcement learning (RL) has become a cornerstone for improving the reasoning ability of large language models (LLMs).\nThe current mainstream Group Relative Policy Optimization (GRPO) estimates advantage via relative comparisons within the full group of sampled responses. \nHowever, this single-scale, global comparison mechanism is inherently brittle, sensitive to the heterogeneity and stochasticity of reward distribution, leading to unstable training signals.\nDrawing inspiration from graph theory, where node importance is better captured through local substructures than global statistics, we propose Multi-Scale Group Relative Policy Optimization (MS-GRPO), a novel RL algorithm that generalizes GRPO by aggregating relative advantages computed across multiple response subgroups at varying scales (e.g. pairwise, trios, etc.). \nSince the exhaustive enumeration of all meaningful subgroups grows combinatorially with group size, we further introduce a practical acceleration scheme that selects a small yet representative subset of subgroups via dilated scale sampling and diversity-aware subgroup selection. In addition, we provide a rigorous theoretical analysis, demonstrating that MS-GRPO can be interpreted as an adaptive correction of GRPO's advantage controlled by the heterogeneity of reward distribution, and gracefully degenerates to GRPO when the reward distribution approaches homogeneity.\nExperiments demonstrate that MS-GRPO significantly outperforms GRPO on various tasks, for example, with improvements averaged over all evaluated models: +5.5 on AIME24 math reasoning, +4.6 on RiddleSense logical reasoning, +2.7 on LiveCodeBench programming challenges, +2.2 on MedQA medical reasoning, and +13.5 on HotpotQA with search engine.", "tldr": "", "keywords": ["LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/89168d3d88ef31e3972493e9e9bf88f14a3371ff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Multi-Scale Group Relative Policy Optimization (MS-GRPO), an extension of GRPO that incorporates multi-scale relative comparisons to improve the robustness of advantage estimation in RL fine-tuning of LLMs. Instead of computing the normalized advantage based on the entire group’s mean and variance (as in GRPO), MS-GRPO computes local advantages over multiple subgroup sizes (pairwise, trios, quartets, …) and then hierarchically aggregates them. To address the combinatorial explosion of possible subgroups, the authors introduce a dilated scale sampling (select representative subgroup sizes) and diverse group sampling (select diverse combinations via Jaccard distance) strategy. They further interpret MS-GRPO as a heterogeneity-aware correction to GRPO, which adapts to reward distribution variance. Extensive experiments on math reasoning, code generation, logical reasoning, medical QA, and search-augmented QA tasks show consistent performance gains over GRPO across various model families (Qwen2.5, LLaMA3.2, DeepSeek-R1).\n\nAlthough the paper has a very good experimental part with good ablation design and considerable performance enhancement on different settings, I have a major concern about the method part. In short, I cannot understand where the benefits of the proposed method come from (see the weakness part). Due to this main concern, I can only give a rejection at this stage. If the authors could address this issue, I would be happy to increase my evaluation to a positive score."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Simple Method with Engineering practicality.**  The method introduces a minimal modification to GRPO by recalculating the advantage A using the given rewards, requiring almost no additional computation and being compatible with existing GRPO variants. While the intuition for why this leads to such strong results could be elaborated further, the experimental improvements reported are encouraging.\n2. **Strong empirical section.** Experiments are broad, covering several reasoning benchmarks and LLM backbones, with consistent though small improvements (≈ +2 – 5 points on average)\n3. **Readable and well-organized.** Writing, figures, and ablations are clear; the paper is easy to follow."}, "weaknesses": {"value": "I have one major concern: it remains unclear where the improvement of the proposed method actually comes from. Clarifying this point would substantially strengthen the paper. Since the only modification to the vanilla GRPO loss lies in the computation of the advantage A, there is no additional information introduced from other components (e.g., reward model, annotator, or policy objective). Therefore, the benefit must arise from some inductive bias embedded in the new way of computing A.\n\nAs the authors point out, the original GRPO advantage has several known issues, such as sensitivity to outliers and lack of smoothness, and the proposed approach aims to address them through the following steps (considering the non-simplified version described in Section 3.1 and Figure 1):\n\n1. Generate all possible subgroups of responses.\n2. Compute the normalized advantage for each subgroup as defined in Equation 4 (Figure 1 labels it as Eq. 6, which seems to be a typo).\n3. Average within subgroups of the same size to obtain a scale-specific advantage (Equation 5 / Figure 1 Eq. 7).\n4. Finally, average across scales to obtain the multi-scale advantage (Equation 6 / Figure 1 Eq. 8).\n\nIn fact, given the binary (0/1) reward used in GRPO, this entire process can be formulated analytically and is mathematically equivalent to a *smoothing* of the original advantage signal. If this smoothing effect is indeed the main factor behind the performance gain, it would be valuable to provide further justification, either through additional experiments or a theoretical analysism, to confirm that this mechanism, rather than other uncontrolled factors, is responsible for the observed improvements."}, "questions": {"value": "The typical G would be smaller than 100. Calculating A is doing some simple calculations on a sequence of 1/0 rewards, so why do we need DSS/DGS to speed up?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DWK1pmXSx8", "forum": "ktHj6YazEE", "replyto": "ktHj6YazEE", "signatures": ["ICLR.cc/2026/Conference/Submission4604/Reviewer_QDNW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4604/Reviewer_QDNW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760767066873, "cdate": 1760767066873, "tmdate": 1762917466797, "mdate": 1762917466797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MS-GRPO (Multi-Scale Group Relative Policy Optimization), an extension of GRPO that computes advantages by aggregating relative comparisons across multiple response subgroups at varying scales. The authors provide theoretical analysis showing MS-GRPO adaptively corrects GRPO's advantage based on reward heterogeneity, and report experimental improvements on various reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. **Well-motivated problem**: The paper identifies a legitimate limitation of GRPO—its reliance on single-scale, global comparison, which can be sensitive to outliers and reward distribution heterogeneity.\n\n2. **Comprehensive experiments**: The empirical evaluation covers diverse tasks (math, code, logic, medical reasoning, QA with search) and multiple model families (Qwen, LLaMA, DeepSeek-R1-Distill).\n\n3. **Practical acceleration scheme**: The dilated scale sampling and diverse group sampling strategies (Section 3.2) address the computational tractability concern effectively.\n\n4. **Consistent improvements**: The method shows positive gains across all benchmarks, particularly on harder tasks (e.g., +6.7 on AIME24, +4.3 on MBPP+)."}, "weaknesses": {"value": "**The most significant issue**: While the paper claims MS-GRPO provides \"more robust and reliable advantage signals,\" there is **no empirical verification of the fundamental statistical properties** (bias and variance) of the advantage estimator itself.\n\nI conducted a rigorous Monte Carlo simulation to verify this claim. Below is the detailed experimental methodology:\n\n**Monte Carlo Experimental Design**\n\n**Objective**: Evaluate the quality of advantage estimation for both MS-GRPO and GRPO by measuring bias, variance, and MSE against ground truth.\n\n**Core Design Principle**:\n- To assess an advantage estimator, we need a **ground truth** for comparison\n- Solution: Define a known reward distribution P(r) with true mean μ and variance σ²\n- True advantage is: **A_true(r) = r - V(s) = r - μ** (since the true value function equals the true expectation)\n- Sample from P(r) and measure estimation error\n\n**Experimental Setup**:\n\n**1. Distribution Configurations** (covering different heterogeneity scenarios)\n- **Normal (low heterogeneity)**: μ=0.5, σ=0.1\n- **Normal (medium heterogeneity)**: μ=0.5, σ=0.2\n- **Normal (high heterogeneity)**: μ=0.5, σ=0.3\n- **Bimodal**: 0.5×N(0.3, 0.1²) + 0.5×N(0.7, 0.1²) (simulates subpopulation structure)\n- **Uniform**: U(0, 1) (non-Gaussian, bounded support)\n\n**2. Sampling Protocol** (for each independent trial)\n```\nStep 1: Sample G=8 rewards from P(r): r₁, r₂, ..., r₈\nStep 2: Compute true advantage: A_true[i] = rᵢ - μ\nStep 3: GRPO estimation\n   - Compute group mean: μ_group = (1/8)Σrᵢ\n   - Estimate advantage: A_GRPO[i] = rᵢ - μ_group\nStep 4: MS-GRPO estimation\n   - For each response i, iterate over scales τ=2,3,...,8\n   - For each scale τ, enumerate all τ-subsets containing i\n   - Compute local baseline for each subset\n   - Multi-scale aggregation → A_MSGRPO[i]\nStep 5: Compute errors\n   - Bias error: A_estimated[i] - A_true[i]\n   - Squared error: (A_estimated[i] - A_true[i])²\n```\n\n**3. Statistical Metrics** (computed over N=1000 independent trials)\n- **Bias**: E[A_estimated - A_true] (measures systematic deviation)\n- **Variance**: Var[A_estimated - A_true] (measures estimation stability)\n- **MSE**: E[(A_estimated - A_true)²] = Bias² + Variance (overall quality)\n\n**4. Fairness Guarantees**\n- Use **unnormalized** advantages for fair comparison (removing internal standardization effects)\n- Same sampling sequence used for both methods\n- Fixed random seed (42) for reproducibility\n- G=8 matches the paper's experimental setting\n\n**Results**:\n\n| Distribution | Metric | GRPO | MS-GRPO | Winner |\n|-------------|--------|------|---------|--------|\n| Normal (σ=0.1) | Mean Abs Bias | 0.0271 | 0.0291 | **GRPO (-7.2%)** |\n| | Variance | 0.00117 | 0.00134 | **GRPO (-14.5%)** |\n| Normal (σ=0.2) | Mean Abs Bias | 0.0537 | 0.0576 | **GRPO (-7.3%)** |\n| | Variance | 0.00450 | 0.00517 | **GRPO (-14.9%)** |\n| Normal (σ=0.3) | Mean Abs Bias | 0.0836 | 0.0888 | **GRPO (-6.2%)** |\n| | Variance | 0.0109 | 0.0123 | **GRPO (-13.7%)** |\n| Bimodal | Mean Abs Bias | 0.0631 | 0.0676 | **GRPO (-7.2%)** |\n| | Variance | 0.00619 | 0.00700 | **GRPO (-13.1%)** |\n| Uniform | Mean Abs Bias | 0.0794 | 0.0857 | **GRPO (-8.0%)** |\n| | Variance | 0.0100 | 0.0114 | **GRPO (-13.6%)** |\n\n**Key findings**:\n- **GRPO consistently exhibits lower bias** (6-8% better) across all distributions\n- **GRPO consistently exhibits lower variance** (13-15% better) across all distributions\n- **MS-GRPO introduces additional estimation noise** through multi-scale aggregation\n- This pattern holds **universally** across all 5 tested distributions\n\nThis directly contradicts the paper's claim that MS-GRPO provides \"more robust and reliable advantage signal.\""}, "questions": {"value": "1. **Core question**: Given that MS-GRPO has higher bias and variance in advantage estimation (as my experiments show), what is the actual mechanism behind the performance gains?\n\n2. Can you provide theoretical analysis of MS-GRPO's advantage estimator properties (bias/variance) under different reward distributions?\n\n4. In Appendix A, you show MS-GRPO introduces corrections. Can you prove these corrections reduce MSE in any well-defined sense?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dNYnd5DBpe", "forum": "ktHj6YazEE", "replyto": "ktHj6YazEE", "signatures": ["ICLR.cc/2026/Conference/Submission4604/Reviewer_5i1S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4604/Reviewer_5i1S"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761293349405, "cdate": 1761293349405, "tmdate": 1762917466441, "mdate": 1762917466441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MS-GRPO, an extension of GRPO that computes token-level advantages by aggregating comparisons across many subgroups."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tbenchmark coverage across multiple LLM families and tasks.\n2.\tBetter shaping the advantage is an interesting topic."}, "weaknesses": {"value": "1.\tThe graph-theory justification is superficial and disconnected from RL theory.\nMotifs/graphlets analogies do not lead to formal reasoning about advantages, gradients, or credit assignment. This reads as narrative rather than theory.\n\n2.\tI plotted the MS-GRPO advantages under binary rewards and observed that the method does not provide the claimed “heterogeneity correction” through subgroup comparisons. For any fixed success rate p, the ratio between positive and negative advantages remains identical to GRPO, namely (1−p)/p. The only effect introduced by MS-GRPO is a multiplicative scalar c(p), such that the MS-GRPO advantage equals the GRPO advantage scaled by this constant. Moreover, c(p)  is largest near p=0.5 and diminishes as p→0 or p→1,  emphasizing those p=0.5 questions. However, [1] demonstrates that such an emphasis is problematic.\n\n3. Given that MS-GRPO reduces to a simple rescaling of GRPO’s advantage under binary rewards, it becomes unclear how the reported empirical gains are obtained. In particular, the paper does not specify whether both methods use strictly identical training settings. such as checkpoint selection criteria or the number of training steps. Even small differences in these factors can produce non-trivial performance gaps\n\n[1] DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j4N1D0xol1", "forum": "ktHj6YazEE", "replyto": "ktHj6YazEE", "signatures": ["ICLR.cc/2026/Conference/Submission4604/Reviewer_WZZD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4604/Reviewer_WZZD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975045548, "cdate": 1761975045548, "tmdate": 1762917465163, "mdate": 1762917465163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed an alternative, MS-GRPO, to GRPO by changing its reliance on a single and global baseline in advantage estimation with multi-scale baselines and hierarchically aggregate the per-scale advantages. The also propose an acceleration scheme to handle the additional computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a theoretical interpretation on when and why the proposed MS-GRPO helps.\n\n- The evaluation covers different tasks across math, coding and medicine."}, "weaknesses": {"value": "- The paper's hypothesis that MS-GRPO's superior performance stems from its ability to handle this heterogeneity should be directly and empirically tested. However, the authors did not show evidence. The observed performance gains could be due to other confounding factors, such as an implicit regularization effect from averaging over many subgroups, or even an unintended interaction with GRPO's known optimization biases (detailed more in the next point). \n\n- The methods may amplify GRPO bias. The Dr. GRPO paper [1] demonstrates that the standard GRPO advantage formulation suffers from a critical optimization bias. The computational unit of MS-GRPO inherent from GRPO, for which calculation Dr.GRPO identifies as a source of optimization bias. If this MS-GRPO takes this flawed calculation and applies it combinatorially across hundreds or thousands of subgroups, averaging the biased results. It inherits GRPO's fundamental optimization bias and likely exacerbates it through repeated application. The failure to cite, discuss, or experimentally compare against Dr.GRPO is a critical omission. Without such a comparison, it is impossible to know if MS-GRPO's reported gains are from algorithmic improvement or simply an artifact of a complex, biased optimization dynamic that has already been fixed more elegantly by prior work.\n\n- The proposed solution involves a complex, two-stage sampling procedure, and introduces new hyperparameters (M and K) and non-trivial algorithms to manage this complexity. The paper provides no discussion or evaluation of these much simpler alternatives, leaving the reader to wonder if the complex multi-scale machinery is truly necessary or if it is an overly engineered solution in search of novelty.\n\n[1] Understanding R1-Zero-Like Training: A Critical Perspective"}, "questions": {"value": "- To validate hypothesis, could you please define a quantitative metric for reward heterogeneity within a group of responses and demonstrate that the performance delta between MS-GRPO and GRPO? Ideally, they should be positively correlated with the measured level of heterogeneity.\n\n- The theoretical analysis in Appendix D relies on a Taylor approximation to derive the \"adaptive correction\" terms. Given that the method is designed for high-heterogeneity scenarios where deviations from the mean are large, what is the approximation error of this analysis, and how does it affect the validity of your conclusions, particularly in the regime where the method is claimed to be most useful?\n\n- You set M = 4 and K = 8 by default. How sensitive is MS-GRPO's performance to the choice of M and K, and what is the principled way to set these hyperparameters for a new task without extensive tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2pw95xWz6j", "forum": "ktHj6YazEE", "replyto": "ktHj6YazEE", "signatures": ["ICLR.cc/2026/Conference/Submission4604/Reviewer_kyZf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4604/Reviewer_kyZf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051871215, "cdate": 1762051871215, "tmdate": 1762917464681, "mdate": 1762917464681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}