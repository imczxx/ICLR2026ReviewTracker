{"id": "OEPP8T0zUX", "number": 20213, "cdate": 1758303743127, "mdate": 1759896990220, "content": {"title": "TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition", "abstract": "Large Language Models (LLMs) excel at both informal and formal (e.g. Lean 4) mathematical reasoning but still struggle with autoformalisation, the task of transforming informal into formal mathematical statements. Autoformalisation helps pair the informal reasoning of LLMs with formal proof assistants which enable machine-verifiable generation and mitigate hallucinations. Yet, the performance of current Math LLMs is constrained by the scarcity of large-scale corpora, particularly those containing pairs of informal and formal statements. Although current models are trained to generate code from natural language instructions, structural and syntactic differences between these and formal mathematics limit effective transfer learning. We propose TopoAlign, a framework that unlocks widely available code repositories as training resources for Math LLMs. TopoAlign decomposes code into docstrings, main functions, and dependency functions, and reassembles these components into analogues that structurally mirror formal statements. This produces structurally aligned code data that can be used for training Math LLMs without requiring additional human annotation.  We train two state-of-the-art models, DeepSeek-Math and Herald, and evaluate them on the minif2f, Putnam, and ProofNet benchmarks. TopoAlign provides substantial gains for DeepSeek-Math, improving performance by 17.77% on BEq@10 and 68.82% on typecheck@10. Despite introducing no new mathematical knowledge, our framework achieves gains of 0.12% and 1.09% for Herald on BEq@10 and typecheck@10, respectively, demonstrating that training on aligned code data is beneficial even for specialized models.", "tldr": "We introduce TopoAlign, a framework that creates training data for mathematical autoformalisation by restructuring existing code repositories to mimic the structure of formal mathematical statements.", "keywords": ["autoformalisation", "formal reasoning", "code data"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d01c534a0871b8809dd7b14216d9ba9175930ec0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes TopoAlign, a framework that decomposes code into docstrings, main functions, and dependency functions, and reassembles these components into analogues that structurally mirror formal statements. This produces structurally aligned code data that can be used for training Math LLMs without requiring additional human annotation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing is clear and easy to follow\n2. The idea of drawing a structural analogy to programming code to generate alignment data is novel, solving the bottleneck in training data.\n3. The ablation study is thorough."}, "weaknesses": {"value": "1. The code and data do not seem to be available, making reproduction difficult.\n2. The choice of base model seems outdated. Perhaps training on newer Qwen models might better demonstrate the effectiveness of the training data.\n3. Formal languages like Lean 4 are statically and dependently typed, making type correctness paramount. This represents a fundamental mismatch in the analogy the framework is built upon."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "96i6A7gPsX", "forum": "OEPP8T0zUX", "replyto": "OEPP8T0zUX", "signatures": ["ICLR.cc/2026/Conference/Submission20213/Reviewer_TtGu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20213/Reviewer_TtGu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20213/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760863615877, "cdate": 1760863615877, "tmdate": 1762933712206, "mdate": 1762933712206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Autoformalisation from natural language to formal math (Lean 4) is bottlenecked by scarce parallel NL–FL data. This work decomposes code into (i) docstring → informal statement, (ii) main function → formal statement, and (iii) dependency functions → supporting lemmas; builds function-level dependency trees via AST/BFS; filters repositories by tree depth/breadth; augments or synthesizes docstrings via an LLM; introduces Code Autoformalisation (CAF) training mixing aligned code with formal math."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Introduces a plausible structural bridge between code and formal math and operationalizes it at scale  (324.5M tokens).\n- Demonstrates consistent improvements across multiple benchmarks and two model families, with meaningful gains for a non-specialized model (DeepSeek-Math).\n- Thoughtful qualitative error analysis identifying type-related failure modes."}, "weaknesses": {"value": "- no comparison to alternative structural alignments (e.g., file-level, call-graph without filtering), or to other synthetic data methods like ATLAS/student–teacher under the same budget.\n- LLM-produced summaries may encode solution intent differently from natural informal statements; details missing."}, "questions": {"value": "- Any contamination checks ensuring no overlap with evaluation sets (via code comments, problem text, or Lean entities)?\n- How were the 4,000 code/math samples selected for TopoAlign runs? Random? Balanced by tree properties?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "icorCZA0ez", "forum": "OEPP8T0zUX", "replyto": "OEPP8T0zUX", "signatures": ["ICLR.cc/2026/Conference/Submission20213/Reviewer_EL9p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20213/Reviewer_EL9p"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20213/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761376996555, "cdate": 1761376996555, "tmdate": 1762933711695, "mdate": 1762933711695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TopoAlign, a framework that structurally aligns programming code with formal mathematical statements (e.g., in Lean 4) to address the scarcity of training data for autoformalization. By decomposing code into docstrings (informal statements), main functions (formal statements), and dependency functions (supporting lemmas), TopoAlign enables Math LLMs to learn compositional patterns from code. The authors also propose Code Autoformalisation (CAF), a training task that mimics autoformalization using aligned code. Experiments on benchmarks like MiniF2F, ProofNet, and Putnam show consistent improvements in both syntactic and semantic metrics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This paper leverages widely available code repositories as a scalable source of training data for formal mathematics, addressing a key bottleneck in autoformalization.\n\n* The author conduct comprehensive experiments across multiple benchmarks and models, with clear ablation studies and qualitative analysis."}, "weaknesses": {"value": "* The author assumes that the structure of an algorithm’s implementation (code topology) directly maps onto the logical and topological structure of the underlying mathematical theory. Real-world code often includes implementation noise (e.g., I/O, logging, dynamic memory management, exception handling) that are irrelevant to the formal mathematical concept and could introduce poor alignment.\n\n\n* The alignment process heavily relies on the quality of the informal documentation (comments, docstrings, function names) within the codebase. If the code is poorly documented, the resulting informal side of the data pair will be low-quality or nonsensical.\n\n* The method is evaluated primarily on autoformalization; its applicability to other reasoning tasks (e.g., theorem proving) is not fully explored.\n\n* Improvements for HERALD are small (e.g., +1% BEq@10), suggesting diminishing returns for models already optimized for formalization.\n\n\n* The following duplicate references should be merged into a single entry:  \nZenan Li, Yifan Wu, Zhaoyu Li, Xinming Wei, Xian Zhang, Fan Yang, and Xiaoxing Ma. Autoformalize mathematical statements by symbolic equivalence and semantic consistency. Advances in Neural Information Processing Systems, 37:53598–53625, 2024a. Zenan Li, Yifan Wu, Zhaoyu Li, Xinming Wei, Xian Zhang, Fan Yang, and Xiaoxing Ma. Autoformalize mathematical statements by symbolic equivalence and semantic consistency. In Advances in Neural Information Processing Systems (NeurIPS) 2024, 2024b. URL https: //arxiv.org/abs/2410.20936. NeurIPS 2024 conference paper; code available online."}, "questions": {"value": "* How does TopoAlign perform on out-of-distribution or non-Lean formal systems (e.g., Isabelle, Coq)?\n\n* Is there any evidence that TopoAlign helps in downstream theorem proving (beyond autoformalisation)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "iOjheStuD2", "forum": "OEPP8T0zUX", "replyto": "OEPP8T0zUX", "signatures": ["ICLR.cc/2026/Conference/Submission20213/Reviewer_9age"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20213/Reviewer_9age"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20213/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831474528, "cdate": 1761831474528, "tmdate": 1762933710806, "mdate": 1762933710806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TopoAlign, a framework to address data scarcity in mathematical autoformalization. It leverages code repositories by decomposing them into docstrings, main functions, and dependencies. This creates a large, structurally-aligned dataset. The authors use this for a CAF task, mixing aligned code with real math data. Experiments show this method improves generalist models (DEEPSEEK-MATH) and modestly improves specialist models (HERALD) by transferring structural and problem-solving skills from code."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Creatively uses abundant code repositories to solve the critical data bottleneck in formal mathematics.\n\nThe structural alignment between code and formal math is intuitive and well-justified.\n\nEffectively demonstrates the method's value on DEEPSEEK-MATH, and includes a key ablation study on the optimal code-to-math data ratio."}, "weaknesses": {"value": "As shown in Table 1, gains on the already specialized HERALD model are marginal, especially on MiniF2F-test, ProofNet and Putnam, suggesting the method is better for initializing models than pushing the state-of-the-art.\n\nThere is a fundamental semantic gap between its code data source and the target domain of formal mathematics. The model, trained on weakly-typed Python, fails to learn critical type distinctions, such as confusing a logical integer Z with a natural number N. The paper hypothesizes this could be fixed by using strongly-typed languages like Java or C++, but this solution may be flawed. It incorrectly equates computational type systems (like Java's int, for memory safety) with logical type systems (like Lean's Z, for abstract proof). The concepts a model would learn from Java are not the same as the required mathematical concepts. This represents a deep semantic gulf between programming paradigms and logical reasoning that simply changing the source language cannot bridge."}, "questions": {"value": "none"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UVjDEjjElK", "forum": "OEPP8T0zUX", "replyto": "OEPP8T0zUX", "signatures": ["ICLR.cc/2026/Conference/Submission20213/Reviewer_oAtz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20213/Reviewer_oAtz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20213/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049501160, "cdate": 1762049501160, "tmdate": 1762933710154, "mdate": 1762933710154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}