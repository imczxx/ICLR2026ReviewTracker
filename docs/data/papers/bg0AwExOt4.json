{"id": "bg0AwExOt4", "number": 23982, "cdate": 1758351325057, "mdate": 1759896787933, "content": {"title": "Aligning Forest and Trees in Images and Long Captions for Cross-Domain Grounding", "abstract": "Large vision-language models such as CLIP align images and captions as wholes but falter on long, detailed descriptions. Fine-grained understanding demands capture of hierarchical semantics, seeing both forest and trees, \nwithin and across domains. Yet syntactic and semantic structures seldom mirror visual organization, and vision alone tends to create spurious fragments unless text anchors and unifies.\nWe propose F-CAST, a hierarchical image-text representation learning framework that discovers aligned spatially oriented text and visual hierarchies directly from image and long-caption corpora, without region-sentence labels.  \nIt uses a CAST visual encoder for fine-to-coarse scene parsing and a hierarchical transformer text encoder that first encodes each sentence then fuses them into a whole-caption representation.  \nA two-level alignment loss, extending FLAIR, aligns whole images with whole texts while biasing image-sentence matches so coarse concepts emerge from fine-grained evidence rather than ignoring it.\nTrained on 30M image--text pairs, F-CAST delivers strong scaling and sets state-of-the-art performance on six long-text benchmarks.  \nExperiments show that hierarchical alignment of vision and language enables F-CAST to discover fine-grained, visually grounded text understanding without supervision.", "tldr": "We propose a part-to-whole alignment for vision-language pre-training to achieve comprehensive scene understanding.", "keywords": ["language-image pre-training", "vision-language model", "part-to-whole recognition", "visual grounding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aeb2a56a3948ace4ecb9929a9b4696d802bba512.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes F-CAST, a hierarchical vision–language pretraining framework for long captions. On the vision side, it replaces a flat ViT with CAST to produce fine-to-coarse **segment tokens.** On the text side, it uses a two-stage hierarchical transformer: **Stage-1** encodes sub-captions (sentences/chunks), **Stage-2** composes them (with an adapter) into a whole-caption embedding. Cross-modal training uses two sigmoid losses: (i) a part-level text-grounded loss that aligns attention-pooled visual segments with sub-captions, and (ii) a whole-level loss aligning a global image token with the composed whole-caption embedding."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Unified hierarchical alignment with simple losses; easy to slot into CLIP-style training while improving long-text robustness. \n\n* Strong empirical gains across six benchmarks; scaling plot indicates gains persist with more data. \n\n* Ablations indicate each piece (CAST, hierarchical text, two-level loss) contributes materially."}, "weaknesses": {"value": "* **Limited novelty— Most ingredients are adaptations of prior work:** CAST for the visual hierarchy, and FLAIR-style text-conditioned image representations extended to a two-level (part/whole) loss. The paper itself positions F-CAST as “adopt CAST” + “extend FLAIR” with hierarchical text and a two-level alignment objective, which reads more like a careful systemization than a fundamentally new algorithmic idea.\n\n\n\n* **Grounding validation gap:** relies on attention maps; lacks quantitative phrase↔region grounding benchmarks, making the “discovered hierarchy” claim hard to verify scientifically. \n\n\n* **Auxiliary-only parts at test time:** The part-level pathway is unused during inference, so is it necessary for the reported retrieval gains, or is it merely a training prior? A controlled experiment removes part-level loss at train time, but a matching compute is needed. \n\n\n\n* **Synthetic caption dependence:** trained on long synthetic recaps (DreamLIP); no robustness audits vs. noise/hallucination or transfer to human-written dense captions beyond the ones reported."}, "questions": {"value": "* Quantify grounding: Can you report standard grounding metrics (e.g., phrase localization/pointing accuracy or region retrieval) on datasets with region annotations (RefCOCO/RefCOCO+/Flickr30k Entities) to substantiate the part-level alignment claim? How does F-CAST compare to FLAIR under identical protocols? \n\n\n\n* Ablate test-time dependence: What happens if you retain the part-level branch at inference (e.g., late fusion or re-ranking) vs. your current whole-only inference? Conversely, if you remove L_part during training, holding compute constant, how much of Table 1 survives? (Your Table 5 variant still shows a gap, but more controls are needed.) \n\n\n\n* Data realism & cost: Provide robustness studies to caption noise (shuffle/perturb chunks), and report FLOPs/throughput/memory vs. FLAIR/Long-CLIP to justify practicality. Also, clarify exact GPU-days per dataset and training schedule details beyond “5 days on 8×A100 for 30M”.\n\n\n* Where does part-level alignment help? Your inference uses only the global embedding. What specific cases benefit from the part-level loss?\n\n\n**I am open to changing my score based on the author's responses.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dyJrhHrlX4", "forum": "bg0AwExOt4", "replyto": "bg0AwExOt4", "signatures": ["ICLR.cc/2026/Conference/Submission23982/Reviewer_frwe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23982/Reviewer_frwe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761711619789, "cdate": 1761711619789, "tmdate": 1762942883500, "mdate": 1762942883500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The research addresses a critical limitation in current AI vision-language models: their inability to comprehend detailed, lengthy image descriptions containing specific details about objects, their attributes, and spatial relationships. The authors propose that effective understanding requires a hierarchical approach that mirrors human perception—simultaneously grasping both the overall scene and specific details.\n\nTheir solution, F-CAST, implements a three-stage hierarchical system that processes visual and textual information in parallel levels of granularity. The model progressively groups image patches into objects and then complete scenes while simultaneously analyzing individual sentences before combining them into full descriptions, ultimately aligning sentence-level details with corresponding image regions and matching complete captions with whole images. F-CAST is claimed to have achieved state-of-the-art results across six benchmarks and learned to associate specific phrases with image regions without explicit supervision."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Comprehension and matching long-text to images is an important and interesting problem. Also, and in general, the paper is well-written and technically sound."}, "weaknesses": {"value": "As noted above, the challenge itself is not novel, nor is the solution strategy of joint learning. More detail on what is special about this specific manifestation of the problem and on the solution would have increase the novelty of this work."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mpBmim7zWe", "forum": "bg0AwExOt4", "replyto": "bg0AwExOt4", "signatures": ["ICLR.cc/2026/Conference/Submission23982/Reviewer_9jGL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23982/Reviewer_9jGL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871959075, "cdate": 1761871959075, "tmdate": 1762942883242, "mdate": 1762942883242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes F-CAST, a hierarchical vision-language model that aligns fine-to-coarse visual structures with spatially oriented textual hierarchies for improved grounding in long-image caption understanding. By combining a CAST-based visual encoder and a two-stage hierarchical text transformer with a two-level alignment loss, F-CAST achieves state-of-the-art performance on six long-text image retrieval benchmarks without requiring region-sentence annotations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. The proposed token reconstruction alignment and subcaption-aggregated patch alignment strategies are interesting and insightful.\n3. Experimental results reflect the effectiveness of the proposed method to some extent."}, "weaknesses": {"value": "I appreciate the contributions of this paper, but I have two main concerns—particularly the second one—that prevent me from giving a positive recommendation at this stage:\n\nAlthough the F-CAST framework is interesting, its individual components appear to be borrowed directly from prior methods, essentially forming a combination of existing approaches.\nF-CAST only reports quantitative results on long-text image retrieval benchmarks; its part-level capabilities are not sufficiently validated. I would expect the authors to align their evaluation with FG-CLIP or [1] and provide additional fine-grained results.\n\n[1] UMG-CLIP: A Unified Multi-Granularity Vision Generalist for Open-World Understanding. ECCV, 2024."}, "questions": {"value": "Please refer to the 'weakness' part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O66GVxdYy0", "forum": "bg0AwExOt4", "replyto": "bg0AwExOt4", "signatures": ["ICLR.cc/2026/Conference/Submission23982/Reviewer_AC3R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23982/Reviewer_AC3R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962333351, "cdate": 1761962333351, "tmdate": 1762942882701, "mdate": 1762942882701, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes F-CAST, a hierarchical vision-language model that aligns fine-to-coarse visual structures with spatially oriented textual hierarchies for improved grounding in long-image caption understanding. By combining a CAST-based visual encoder and a two-stage hierarchical text transformer with a two-level alignment loss, F-CAST achieves state-of-the-art performance on six long-text image retrieval benchmarks without requiring region-sentence annotations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. The proposed token reconstruction alignment and subcaption-aggregated patch alignment strategies are interesting and insightful.\n3. Experimental results reflect the effectiveness of the proposed method to some extent."}, "weaknesses": {"value": "I appreciate the contributions of this paper, but I have two main concerns—particularly the second one—that prevent me from giving a positive recommendation at this stage:\n\n1. Although the F-CAST framework is interesting, its individual components appear to be borrowed directly from prior methods, essentially forming a combination of existing approaches.\n\n2. F-CAST only reports quantitative results on long-text image retrieval benchmarks; its part-level capabilities are not sufficiently validated. I would expect the authors to align their evaluation with FG-CLIP or [1] and provide additional fine-grained results.\n\n[1] UMG-CLIP: A Unified Multi-Granularity Vision Generalist for Open-World Understanding. ECCV, 2024."}, "questions": {"value": "Please refer to the 'weakness' part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O66GVxdYy0", "forum": "bg0AwExOt4", "replyto": "bg0AwExOt4", "signatures": ["ICLR.cc/2026/Conference/Submission23982/Reviewer_AC3R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23982/Reviewer_AC3R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962333351, "cdate": 1761962333351, "tmdate": 1763024817076, "mdate": 1763024817076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a hierarchical image-text representation learning framework, F-CAST, which enables vision-language models to learn hierarchically aligned information from images and long captions. The proposed method exceeds the SOTA methods on six long-text benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- F-CAST learns fine-grained, visually grounded text understanding without supervision.\n- The proposed methods exceeds the SOTA methods on six long-text benchmarks."}, "weaknesses": {"value": "- F-CAST aims to improve the alignment between the visual hierarchy of images and the spatial hierarchy of long captions. However, the method is primarily evaluated on long text-image cross-modal retrieval tasks, which mainly assess global-to-global alignment in vision-language models. The hierarchical alignment capability is only illustrated through visualization rather than quantitative evaluation. \n- The proposed method mainly focuses on local information at the sub-caption level. However, sub-captions often contain hierarchical information that may align with different visual regions. These multi-level relationships within sub-captions may not be fully captured by the proposed method. For example, in Figure 3, the first sub-caption also mentions “man” in addition to \"horses.\""}, "questions": {"value": "The paper mainly visualizes F-CAST’s ability to perform local alignment between sub-captions and image regions. What about the global alignment between the long text and the corresponding image?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MF7leyzniW", "forum": "bg0AwExOt4", "replyto": "bg0AwExOt4", "signatures": ["ICLR.cc/2026/Conference/Submission23982/Reviewer_yPGE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23982/Reviewer_yPGE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013866252, "cdate": 1762013866252, "tmdate": 1762942882075, "mdate": 1762942882075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}