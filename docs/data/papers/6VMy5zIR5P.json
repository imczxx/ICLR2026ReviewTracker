{"id": "6VMy5zIR5P", "number": 14787, "cdate": 1758243724448, "mdate": 1759897349395, "content": {"title": "Sotopia-RL: Reward Design for Social Intelligence", "abstract": "Social intelligence has become a critical capability for large language models (LLMs), enabling them to engage effectively in real-world social tasks such as collaboration and negotiation. Reinforcement learning (RL) is a natural fit for training socially intelligent agents because it allows models to learn sophisticated strategies directly through social interactions without requiring human annotations. However, there are two unique parts about social intelligence tasks: (1) the quality of individual utterances in social interactions is not strictly related to final success; (2) social interactions require multi-dimensional rubrics for success. Therefore, we argue that it is necessary to design rewards for building utterance-level multi-dimensional reward models to facilitate RL training for social intelligence tasks. To address these challenges, we propose Sotopia-RL, a novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards. Utterance-level credit assignment attributes outcomes to individual utterances, while multi-dimensional rewards capture the full richness of social interactions and reduce reward hacking.\nExperiments in Sotopia, an open‑ended social learning environment, demonstrate that Sotopia-RL achieves state‑of‑the‑art social goal completion scores (7.17 on Sotopia-hard and 8.31 on Sotopia-all), significantly outperforming existing approaches. Ablation studies confirm the necessity of both utterance‑level credit assignment and multi‑dimensional reward design for RL training.", "tldr": "", "keywords": ["social interaction", "reinforcement learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4a7e6cadc90aca60270df00f4af9db2528559d8c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a complete reinforcement learning framework for social tasks, which includes the following steps: collecting trajectories, obtaining multi-dimensional rewards for each trajectory offline using GPT, allocating trajectory rewards to each round of interaction offline using GPT, weighting the sum of rewards for each round of interaction offline to obtain the total reward, training a reward model, and training a strategy model using GRPO. The article conducted experiments on the SOTOPIA benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Proposed a feasible social task reinforcement learning framework that can improve the performance of the model on specified tasks\n2. The article uses multidimensional rewards, which enhances the robustness and density of the rewards\n3. The article uses a credit assignment mechanism, which can effectively allocate the overall rewards to individuals"}, "weaknesses": {"value": "1. The paper did not confirm the lack of overlap between the dataset used for GRPO and the evaluation dataset, which may have led to unfairness in results\n2. The motivation for using online reinforcement learning is unclear, and the main difficulty of the article is to train an effective reward model. However, this reward model is trained using offline data, and there may be overfitting issues when used in online reforcement learning. The reward model training epoch published in the paper is 60, but the number of steps is only 8000, which increases the possibility of overfitting\n3. The paper did not disclose the direct evaluation results of the rewards predicted by the reward model on the test set, only its Best-of-N performance on the benchmark, which makes it difficult to prove that the reward model strictly follows the definition of your reward\n4. The reward training data of the paper requires multiple annotations using GPT, which is costly."}, "questions": {"value": "see the above comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h38P5jYlG6", "forum": "6VMy5zIR5P", "replyto": "6VMy5zIR5P", "signatures": ["ICLR.cc/2026/Conference/Submission14787/Reviewer_N1rT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14787/Reviewer_N1rT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761523453227, "cdate": 1761523453227, "tmdate": 1762925141163, "mdate": 1762925141163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Sotopia-RL, a two-stage framework for training socially intelligent language agents. Stage 1 converts episode-level feedback in the Sotopia environment into utterance-level, multi-dimensional rewards via an offline LLM-based attribution scheme. Stage 2 distills these labels into a reward model and optimizes a Qwen2.5-7B policy using single-turn GRPO. Sotopia-RL is an interesting and technically competent paper that makes a clear empirical contribution to the growing literature on social intelligence for LLMs. Its core idea refines coarse episode-level social feedback into utterance-level, multi-dimensional rewards."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a meaningful and underexplored challenge, which is training agents for socially grounded interaction rather than factual or reasoning tasks.\n\n2. The two-stage pipeline (offline LLM-based attribution and online GRPO optimization) is simple, reproducible, and builds upon recent trends in process and preference-based reward modeling.\n\n3. Results on both Sotopia-hard and Sotopia-all benchmarks show consistent improvements, with well-designed ablations that isolate contributions from the attribution and aggregation components"}, "weaknesses": {"value": "1. While the idea of converting coarse feedback into fine-grained, multi-dimensional rewards is meaningful in social RL, it is not fundamentally new in the broader LLM-RL landscape. The design closely parallels Process Reward Modeling (PRM) and other recent works on token- or step-level credit assignment in reasoning and coding domains. The novelty here mainly lies in applying such techniques to social environments rather than introducing a new RL principle.\n\n2. The method’s heavy reliance on GPT-4o-generated labels poses concerns. These labels may not generalize well due to evaluator bias or semantic mismatch between GPT-4o’s judgment patterns and human social preferences. They can serve as part of the guidance but should ideally be combined with other supervision sources, such as human annotations, ensemble models, or behavioral regularizers, to ensure robust alignment.\n\n3. The single-turn GRPO formulation simplifies optimization but sidesteps temporal dependencies intrinsic to conversation. Without multi-turn or trajectory-level baselines (e.g., PPO over full dialogues), it remains unclear whether the model truly learns sequential social reasoning or merely improves isolated utterance quality.\n\n4. The reward computation, multiplicative scaling with episode-level scores and per-episode min–max normalization, is largely heuristic, with limited theoretical or empirical justification. A comparison with additive, normalized, or expectation-based credit assignment schemes would strengthen the methodological credibility."}, "questions": {"value": "1. Your Eq. for aggregation introduces weights γ over dimensions; claiming “little impact” without numbers weakens the design choice. (In the paper this appears as the per-dimension weighting in the multi-dimensional aggregation.)\nWhat would convince me: A table/plot sweeping γ (e.g., REL/KNO/GOAL ratios from 0–1 on a simplex) reporting GOAL/AVG with CIs on Sotopia-hard/all, plus robustness across partners/evaluators. Also include a default = equal-weights row.\n\n2. If the partner model co-evolves, the training distribution shifts; if frozen, you risk overfitting to a fixed partner. Clarify whether the partner is frozen or updated; if frozen, add a control where partners are alternated (family + size diversity) and report generalization; if co-evolving, describe update cadence and show stability metrics (e.g., non-collapsing rewards).\n\n3. The single-turn formulation ignores temporal dependencies that define conversation; we need to know if gains persist with trajectory-level credit. Add a multi-turn/trajectory baseline (e.g., PPO/GRPO over full dialogues or K-step returns) under the same RM and show (i) final scores, (ii) convergence curves, and (iii) qualitative examples where temporal coherence matters (goal switches, delayed reciprocity). The paper itself frames social interaction as partially observable and later approximates an MDP for single-turn optimization, please justify with comparative results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M7k5mPHE2l", "forum": "6VMy5zIR5P", "replyto": "6VMy5zIR5P", "signatures": ["ICLR.cc/2026/Conference/Submission14787/Reviewer_sYWk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14787/Reviewer_sYWk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972176318, "cdate": 1761972176318, "tmdate": 1762925140659, "mdate": 1762925140659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SOTOPIA-RL, a reinforcement learning (RL) framework designed to improve social intelligence in large language models (LLMs). Unlike tasks such as math or coding, social intelligence tasks (e.g., negotiation, collaboration, persuasion) require nuanced reasoning where individual utterances do not directly correspond to outcomes and success is multi-dimensional. They introduce utterance-level credit assignment, which entails decomposing coarse, episode-level feedback into per-utterance signals. Additionally, they introduce multi-dimensional rewards to expand beyond goal completion to capture dimensions like relationship maintenance and knowledge seeking, and  capture the full richness of social interactions and reduce reward hacking."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- They show superior performance on social intelligence benchmarks, scoring 7.81 on SOTOPIA-hard and 8.57 on the SOTOPIA-all dataset \n- They provide a novel reward structure of Utterance-Level Credit Assignment and multi-dimensional rewards.\n- They show lack of reward hacking and robustness against overfitting"}, "weaknesses": {"value": "There are several weaknesses, that I would encourage the authors to address:\n- Limited human evaluation: You only conduct a small-scale human annotation study with 4 annotators (as noted in your Appendix) and your main evaluation relies on LLM-based automatic evaluators (GPT‑4o) which may not fully capture the full range of human social judgments. The number of human annotators and where they are recruited from should be in the main text. You should also note if you had an IRB for the study.\n- Evaluation scenario scope: experiments are restricted to two-agent interactions of SOTOPIA, and you do not test multi-party, long horizon capabilities\n- Im not sure how helpful it is to have an algorithm just focused on the SOTOPIA framework, and not looking at social interactions in general"}, "questions": {"value": "- How does your method perform on out-of-distribution social tasks or real‐world human interactions?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "I'm afraid that the results of 4 annotators is used to summarize findings, which is not sufficient, and the fact that only four are in the study is not put in the main paper but only in the Appendix. They do not note if they have IRB approval, as well as do not discuss how much compensation is given. I apologize if these details were missed in my reading."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RJRMyAcmqc", "forum": "6VMy5zIR5P", "replyto": "6VMy5zIR5P", "signatures": ["ICLR.cc/2026/Conference/Submission14787/Reviewer_fSpN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14787/Reviewer_fSpN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762236028673, "cdate": 1762236028673, "tmdate": 1762925139326, "mdate": 1762925139326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Sotopia-RL, a reinforcement learning (RL) framework designed to enhance the social intelligence of large language models (LLMs). The core contribution is a novel reward design that addresses two key challenges in social tasks: the weak correlation between individual utterances and final outcomes, and the multi-dimensional nature of social interactions. The method involves (1) reward attribution, which uses a powerful LLM (GPT-4o) to assign episode-level outcomes to individual utterances using full dialogue context, and (2) reward aggregation, which combines rewards from multiple social dimensions (Goal, Relationship, Knowledge) into a single signal. The authors demonstrate that agents trained with Sotopia-RL outperform strong baselines, including the base GPT-4o model, on the Sotopia benchmark, and provide extensive ablation studies and human evaluations to validate their design choices and rule out reward hacking."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Well-Motivated and Novel Problem Formulation: The paper compellingly argues that social intelligence tasks are fundamentally different from math or coding, making standard RL reward signals ineffective. The identification of \"weak correlation\" and \"multi-dimensionality\" as core challenges is precise and well-justified.\nClear and Practical Methodology: The proposed two-stage pipeline (offline reward collection, online RL training) is clearly explained and seems practically implementable. The distinction between offline attribution (with full context) and online reward modeling (with partial context) is a crucial and well-reasoned design choice.\nRigorous and Extensive Evaluation: The empirical evaluation is a major strength. The paper goes beyond simple benchmark comparisons to include:\nComprehensive Ablations: Systematic studies on the contributions of reward attribution vs. aggregation (Tables 2, 4, 5).\nRobustness Checks: Tests across different partner and evaluator models (Table 6, Figures 5-6) effectively address the critical concern of reward hacking.\nHuman Evaluation: Corroborating automated scores with human judgment (Table 3) significantly strengthens the validity of the claims.\nSafety and Diversity Analysis: Including checks for toxicity (Table 9) and conversational diversity (Table 11) shows a commendable breadth of consideration.\nStrong Empirical Results: The results are impressive. Outperforming the model (GPT-4o) that was used to generate the training data and provide reward labels is a non-trivial result that demonstrates the method is doing more than simple distillation."}, "weaknesses": {"value": "Baseline Clarification and Comparison: The definition and implementation of some baselines (e.g., PPDPP, EPO, DAT, DSI in Table 1) are not sufficiently detailed in the main text, requiring the reader to hunt through citations. A brief summary of how these methods work and why they are relevant comparators would improve clarity. Furthermore, a comparison to simpler fine-tuning methods like Direct Preference Optimization (DPO) on the same data would have been a valuable baseline.\nStatistical Reporting: While the paper notes a paired t-test was used (p<0.05 for Table 1), it does not consistently report measures of variance (e.g., standard deviation) for its key results. Presenting results as single scores (e.g., 7.17, 8.31) without confidence intervals or error bars makes it difficult to fully assess the stability and significance of the improvements, especially in the ablation tables.\nWriting and Presentation Issues:\nThere are typographical errors and formatting inconsistencies throughout the paper (e.g., \"Sotopla\" vs. \"Sotopia,\" \"rel training\" in Fig 4 caption, \"ga oal\" in Appendix H.4, \"Sotopia-{}\\Omega}\" in references). This detracts from the paper's professionalism.\nThe narrative flow in Sections 6 and 7 could be tighter. The \"Main Discoveries\" in Section 6 feel somewhat disconnected from the more detailed analysis in Section 7. Integrating these sections could improve readability.\nThe claim that the method is \"often without costly human annotation\" is slightly misleading. It relies heavily on GPT-4o for both self-play data generation and reward annotation, which is a significant computational and financial cost, even if it's not human effort.\nPotential for Overfitting to Sotopia's Rubric: The reward design is tightly coupled with the specific seven-dimensional evaluation rubric of the Sotopia environment. The generalizability of the approach to other social benchmarks or real-world interactions, where such a well-defined rubric may not exist, remains an open question."}, "questions": {"value": "Statistical Significance: Could you provide standard deviations or confidence intervals for the primary results in Tables 1 and 2? This would help the reader understand the variability in performance across different social scenarios and agent pairings.\nCost-Benefit Analysis: Given the reliance on GPT-4o for annotation, could you provide a rough estimate of the computational cost of the offline reward collection stage? How does the performance gain of Sotopia-RL compare to a simpler, less expensive method like using the same budget for more extensive supervised fine-tuning (SFT) on the GPT-4o self-play data?\nGeneralization: How do you envision this reward design framework being applied to a social task or environment that does not have a pre-defined, multi-dimensional evaluation rubric like Sotopia's? Is the method dependent on this structure?\nAblation on Base Model: The policy and reward model are both based on Qwen2.5-7B. To what extent do you believe the performance improvements are dependent on the choice of the base model? Have you observed similar gains when applying the Sotopia-RL pipeline to other open-source models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RniVrEm9dy", "forum": "6VMy5zIR5P", "replyto": "6VMy5zIR5P", "signatures": ["ICLR.cc/2026/Conference/Submission14787/Reviewer_HHAm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14787/Reviewer_HHAm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762525967245, "cdate": 1762525967245, "tmdate": 1762925138610, "mdate": 1762925138610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}