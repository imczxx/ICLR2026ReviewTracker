{"id": "SBVqQAmYkC", "number": 7733, "cdate": 1758034178595, "mdate": 1763617327053, "content": {"title": "TCA-DiT: Quantizing Diffusion Transformers via Temporal Channel Alignment", "abstract": "Diffusion Transformers (DiTs) have achieved remarkable success in generative modeling, but their deployment is hindered by massive model sizes and high inference costs. Post-Training Quantization (PTQ) offers a retraining-free compression paradigm, yet its application to DiTs is particularly challenging due to timestep-varying, channel-wise activation anomalies. These anomalies vary dynamically across timesteps, undermining existing rotation- or scaling-based PTQ methods and leaving residual misaligned anomaly channels that impair quantization fidelity. We propose **TCA-DiT**‚Äî**T**emporal **C**hannel **A**lignment for **Di**ffusion **T**ransformers‚Äîa PTQ framework designed to explicitly address such timestep-varying anomalies. Specifically, we first introduce *Anomaly-aware Rotation Calibration (ARC)*, a learnable rotation-scaling mechanism that jointly optimizes rotation matrices with reconstruction and anomaly alignment losses, thereby aligning anomaly channels across timesteps and enabling more precise per-channel scaling. To improve calibration efficiency, we further develop *Anomaly-guided Timestep Grouping (ATG)*, which clusters timesteps based on anomaly distributions, capturing full temporal dynamics with a compact set of representatives. Finally, we propose *Reordered Group Quantization (RGQ)*, which reorders channels before group quantization to reduce intra-group variance and minimize quantization error. On DiT-XL/2 with W4A4, TCA-DiT improves FID by **0.74** and **6.47** on ImageNet 256$\\times$256 and 512$\\times$512, respectively. On PixArt-$\\alpha$, it achieves a substantial **3.74** FID improvement while reducing memory usage by **3.8$\\times$** and accelerating inference by **3.5$\\times$**. These results highlight the critical role of anomaly alignment in enabling both effective and efficient quantization of DiTs.", "tldr": "We propose TCA-DiT, a post-training quantization framework for Diffusion Transformers via temporal channel alignment", "keywords": ["Post-Training Quantization", "Diffusion Models", "Diffusion Transformers"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b372586eb69b85f9ede44407336c3eef6319b0ec.pdf", "supplementary_material": "/attachment/981c653e22aa178f2dd1fde230171eaa017e2ef0.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose TCA-DiT, a method that suppresses outliers in diffusion transformers (DiTs) through applying transformations (rotation and rescaling and permutations). They introduce:\n1. \"Anomaly-aware Rotation Calibration\", which learns (i) a rotation matrix through a layer reconstruction loss averaged over timesteps, and (ii) a scaling vector on top of the rotation, which aims to align the anomaly channels across time-steps.\n2. \"anomaly-guided timestep grouping\", which aims to cluster timesteps with similar outlier distributions\n3. \"reordered group quantization\", which aims to reduce quantization error post-transformation further by reordering channels such that block-wise quantization is improved."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I see true value in some of the methods ideas:\n* quantization of diffusion models is an important topic, and small gains have big impact\n* the temporal axis of outliers is hardly researched\n* aligning the outlier channels across time steps makes a lot of sense to me for making transform and block quantization more effective"}, "weaknesses": {"value": "# Method\n\nDespite being very familiar with this area, I find the method hard to follow and have many questions.  \n\n## Section 3.1\n\n1. Where are the rotations applied, before all linear layers?\n2. If so, authors claim they can be merged into previous operations. Is this always true?\n3. How do static rotations (i.e. every time step uses the same rotations) provide a tool to align the outlier channels? This may relate to 1---I don't understand where the rotations are applied exactly in the network\n3. The anomaly alignment loss (Eq. 3 and surrounding text) is difficult to follow as it's unclear where in the network we are even looking at, and thus what X refers to or why there is a double softmax.\n4. Section 3.1, Stage 2, is literally SmoothQuant, but this is not cited or acknowledged. Do you agree?\n\n## Section 3.2\n\nIt is unclear how relevant ATG is to the method's success.\n1. The \"ATG\" contribution is needed for efficiency of Eq. 3, but what is really the gain? The acativations throughout the diffusion process still need to be acquired, so it's only the direct sum in Eq. 3 that is approximated. It's unclear whether this gives efficiency gains and whether there is a penalty in terms of accuracy.\n2. More modern diffusion models usually use fewer time steps (e.g. as you write in the text-to-image results, just 20 steps), so it's less of an issue. Likely clustering is less effective as local correlation with fewer time steps will be lower\n3. It is also unclear how expensive this clustering is---we still need to get all the activations and optimize (Eq. 6)\n4. Also not clear how much better it is than just uniformly splitting the diffusion process into equally sized groups. The \"risks overlooking critical transitions\" [L269] are not clear, looking at Fig. 5a.\n\nOverall, I would consider showing either that ATG is essential, or to remove it/make it an implementation detail and not part of the method.\n\n## Section 3.3\n\n1. This seems very much tangential to the previous contributions.\n2. Why can the permutation (which is an orthogonal matrix) not be merged into the earlier rotation matrix?\n\n\n## Language and readability\n\nNot as important, but language doesn't help readability, because it is imprecise and ambiguous at times. A few examples:\n* What does \"anomaly-aware\" or \"anomaly-guided\" do, that related work that addresses outliers (i.e. almost all quantization literature) does not do? Also, why choose the word \"anomaly\", if everyone else uses \"outlier\"?\n* \"residual\" is used in many places (e.g. the ambiguous \"residual anomaly channels\") but it is unclear what this refers to. Does it refer to the residual stream? Or is it meant as \"remaining\"?\n* [L269] \"naive uniform subsampling\", do you mean, splits of $k$ consecutive steps?\n\nAlso, (minor) your citation style is wrong throughout (with a few exceptions). Use `\\citep` typically, not `\\cite` or `\\citet`---see the ICLR style guide üôÇ\n\n# Results\n\nOverall, results are quite exhaustive and convincing. A few points:\n1. Figure 9: no baselines in benchmark. It would be useful to get insight into the overhead of the rotate/scale/permute.\n2. The ablation would be more interesting on text-to-image generation which is the most important nowadays. I suspect that ATG doesn't help there because of the fewer time steps.\n3. There are no standard deviations, so especially in the ablation, it is not clear at all whether the different improvements (e.g. FID down by 0.1x ) is significant\n4. The baselines do not match the numbers from respective papers (ViDiT-Q and DiTAS). Can you explain why?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "89mTimTRlz", "forum": "SBVqQAmYkC", "replyto": "SBVqQAmYkC", "signatures": ["ICLR.cc/2026/Conference/Submission7733/Reviewer_Yurq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7733/Reviewer_Yurq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760538717247, "cdate": 1760538717247, "tmdate": 1762919786835, "mdate": 1762919786835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "DP1XpxINYm", "forum": "SBVqQAmYkC", "replyto": "SBVqQAmYkC", "signatures": ["ICLR.cc/2026/Conference/Submission7733/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7733/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763617326223, "cdate": 1763617326223, "tmdate": 1763617326223, "mdate": 1763617326223, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TCA-DiT: a method for Diffusion Transformer quantization based on the alignment of channel outliers across different diffusion time steps based on rotation and scaling factors learned on a calibration step. The paper motivates and describes the procedure to learn across-time-stamp alignment through the addition of an ad-hoc regularization term. TCA-DiT further introduces a permutation to improve group quantization results. The experimental section compares DCA-DiT against a wide range of methods in DiT Quantization literature, demonstrating the effectiveness on various popular metrics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper provides a novel method to tackle the issue of varying activation statistics across multiple timestamp without requiring time-dependent weight copies. \n\n* The paper reports an extensive description of the method providing numerous baselines and ablation studies. TCA-DiT consistently outperforms the baselines according to the provided metrics. \n \n* The paper includes an assessment of the speedup, model, and inference memory usage."}, "weaknesses": {"value": "* The quantization settings used for the experiment in Figure 6 are not clear from the main text since some models use per-group quantization, while other per-channel/per-token quantization. The details are specified in Appendix C2, but comparing the method using different quantization settings is not entirely fair nor clear. \n \n\n* The main text includes little to no details on where the transformations are applied and how they are fused into the architecture. One side can be clearly fused into the weight matrix, but what happens to the other side is not clarified. Considering that the proposed method involves using arbitrary rotation matrices, this detail has quite a substantial impact on the model runtime. \n\n \n    * The schema in the top right corner of Figure 3 is quite confusing since it looks like the Rotation and Scaling operation are applied to the output of the matmul, and the transpose is not explicit. How can $R$ and $S^{-1}$ be fused into other operations is also unclear. \n\n \n* The paper reports that ‚ÄúA pilot study on the blocks.26.attn.proj layer of DiT-XL/2 further reveals that default channel ordering is suboptimal: random permutations can reduce reconstruction error and strongly correlate with FID (Fig. 5b)‚Äù.  Please clarify that this statement holds when considering per-group quantization. Overall the group quantization permutation seems perpendicular to the rest of the method."}, "questions": {"value": "1. How does QuaRot + RGQ perform? The addition of this result in Figure 8 could help better disentangle the effects of the proposed improvements. \n\n2. Figure 3 seems to imply that the rotation and scaling operation can be fused into other operations. Can the authors further clarify where this transformation can be fused? Linear layers in recent DiT architectures, such as PixArt-Alpha are preceded by a time-dependent normalization or non-linearities, which make these operations non-fusable.  \n\n3. How is the overhead in Figure 9 computed? Does that include the overhead of the un-fused rotations and scaling?  Previous work seems to report larger overheads even with optimized Fast Walsh-Hadamard Transforms, which are substantially faster than a dense matrix multiplication. Where are the transformations applied within each architecture? Some details are described in the appendix, but crucial aspects should be included in the main text to put the results from Figure 9 in context."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hCV3VbEvYY", "forum": "SBVqQAmYkC", "replyto": "SBVqQAmYkC", "signatures": ["ICLR.cc/2026/Conference/Submission7733/Reviewer_Ly4w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7733/Reviewer_Ly4w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761148563597, "cdate": 1761148563597, "tmdate": 1762919786466, "mdate": 1762919786466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TCA-DiT, a training-free PTQ pipeline for diffusion transformers (DiTs) that explicitly addresses **timestep-varying activation outliers**. It has three parts: ARC that learns an orthogonal rotation jointly with a reconstruction loss and a KL-based *temporal anomaly alignment loss*, then apply per-channel scaling, ATG that clusters timesteps to pick a compact calibration subset, and RGQ that evolutionary searchs to reorder channels before group-wise quantization. The method reports strong empirical W4A4 results on DiT-XL/2 (ImageNet) and PixArt-$\\alpha$  with claimed 3.5$\\times$ speedup and 3.8$\\times$ memory reduction after fusing transforms into adjacent ops."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Beyond generic time-aware DiT PTQ, this method proposed a novel and solid approach with a clear operational recipe:  rotate $\\to$ align $\\to$ scale, following experiment justification for PTQ of DiT models over diverse baselins. The pipeline is well-specified (Alg. 3). \n* The experiment result is empirically strong at low bits: W4A8/W4A4 comparisons vs. SmoothQuant/QuaRot/SpinQuant/PTQ4DiT/ViDiT-Q/DiTAS show consistent wins; qualitative samples at W4A4 look markedly better."}, "weaknesses": {"value": "* **Missing baseline.** SVDQuant (ICLR 25') is a *training-free W4A4* approach that **migrates outliers into a low-rank branch** and fuses it with INT kernels; it reports strong DiT results (PixArt/FLUX) and practical speedups. Not comparing against SVDQuant leaves the empirical case incomplete, especially at low bits where SVDQuant is competitive. \n* **Missing ARC ablations.** The mid-timestep anchor is motivated (for balanceness) but **not ablated**; please add quantitative comparisons to alternative anchors (early/late/learned) and expanded robustness for the softmax-weighted aggregator ($SW_\\gamma$) beyond the current illustrative figures."}, "questions": {"value": "1. Is there any memory overhead induced with the ARC?\n2. Does the learned rotation in ARC transfer across seeds/prompts/datasets? \n3. Why is the LPIPS metric evaluated and presented in fig. 1 but missing in the main experiment? Does LPIPS (in comparison to fp32 generation) also consistently outperform the baseline methods (experiment expected)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8F2wq3crub", "forum": "SBVqQAmYkC", "replyto": "SBVqQAmYkC", "signatures": ["ICLR.cc/2026/Conference/Submission7733/Reviewer_cf2t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7733/Reviewer_cf2t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761284425289, "cdate": 1761284425289, "tmdate": 1762919786029, "mdate": 1762919786029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new post-training quantization method for diffusion transformers. Specifically, the approach aligns channel outliers across time steps to facilitate a more effective rotation matrix and further reorders channel indices to improve the quality of quantization grouping. Experimental results demonstrate notable improvements in generation quality compared to existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of aligning channel anomalies across time steps is interesting and addresses an often overlooked issue in prior quantization works, which may otherwise lead to imbalanced quantization errors during the diffusion process.\n\n2. The empirical results demonstrate that the proposed method achieves consistent improvements over state-of-the-art baselines. Additionally, the paper provides validation of practical speed-up on real hardware, which increases the credibility and relevance of the approach.\n\n3. The paper is clearly written, well-organized, and easy to follow."}, "weaknesses": {"value": "1. The post-training quantization procedure appears computationally expensive. For example, quantizing PixArt-Œ±-512 requires more than 11 hours of processing time. Although the proposed optimization objective leads to a more effective rotation matrix, the added complexity significantly increases cost compared to methods like SpinQuant, which completes quantization in fewer than 3 hours. I understand that the performance gains may justify some trade-off, but the high overhead represents a practical limitation.\n\n2. The performance of ViDiT-Q is inconsistent. In several cases (e.g., 100-step W4A4 results in Figure 6), it underperforms Q-DiT, which is not considered a strong baseline in prior literature [1]. Could the authors explain the reasons?\n\n3. Table 1 does not include full-precision (FP) results for comparison, which makes it difficult to assess the relative degradation introduced by quantization. Additionally, the reported MSCOCO results are not aligned with values from existing works [1]. Could the authors explain the reasons?\n\n4. Some figures have low readability due to small text (e.g., Figures 3 and 4). Improving the font size would enhance clarity and presentation quality.\n\n[1] Zhao, Tianchen, et al. \"Vidit-q: Efficient and accurate quantization of diffusion transformers for image and video generation.\" arXiv preprint arXiv:2406.02540 (2024)."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WkhjskvZpD", "forum": "SBVqQAmYkC", "replyto": "SBVqQAmYkC", "signatures": ["ICLR.cc/2026/Conference/Submission7733/Reviewer_jhBb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7733/Reviewer_jhBb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947837869, "cdate": 1761947837869, "tmdate": 1762919785649, "mdate": 1762919785649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}