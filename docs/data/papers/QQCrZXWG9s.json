{"id": "QQCrZXWG9s", "number": 8664, "cdate": 1758094014491, "mdate": 1759897770952, "content": {"title": "Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks Preserving Action Understanding Ability", "abstract": "Temporal Video Grounding (TVG) aims to localize video segments corresponding to a given textual query, which often describes human actions. However, we observe that current methods, usually optimizing for high temporal Intersection-over-Union (IoU), frequently struggle to accurately recognize or understand the underlying actions in both the video and query, thus reducing the effectiveness of these methods. To address this, we propose a novel TVG framework that integrates inversion-based TVG as auxiliary objectives to maintain the model's action understanding ability. We introduce three kinds of inversion TVG tasks derived from the original TVG annotations: (1) Verb Completion, predicting masked verbs (actions) in queries given video segments; (2) Action Recognition, identifying query-described actions; and (3) Video Description, generating descriptions containing query-relevant actions given video segments. These inversion tasks are entirely derived from the original TVG tasks and are probabilistically integrated with them within a reinforcement learning framework. By leveraging carefully designed reward functions, the model preserves its ability to understand actions, thereby improving the accuracy of temporal grounding. Experiments show our method outperforms state-of-the-art approaches, achieving a 7.1\\% improvement in R1@0.7 on Charades-STA for a 3B model.", "tldr": "", "keywords": ["Video Understanding & Activity Analysis"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3f60c7a9e7d604e79bd2d3e899c77caac0385681.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents Invert4TVG, a novel RL-based framework for Temporal Video Grounding that introduces three inversion tasks: Verb Completion (VC), Action Recognition (AR), and Video Description (VD). These auxiliary tasks aim to preserve and enhance the model’s action understanding ability, which is often degraded when optimizing only for IoU-based rewards. The method alternates between TVG and Invert-TVG tasks with probabilistic scheduling and achieves consistent performance improvements across datasets such as Charades-STA, ActivityNet, and QvHighlight."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong conceptual motivation. The paper convincingly argues that existing TVG models fail primarily due to a lack of action understanding, not just poor temporal localization. The inversion-based idea to restore semantic understanding is clear and innovative.\n2. Clear RL framework. The Invert4TVG reinforcement learning setup is well structured and explained. The probabilistic alternation between TVG and Invert-TVG tasks is intuitive and well justified experimentally.\n3. Three inversion tasks. The decomposition into VC, AR, and VD tasks provides a balanced approach across understanding levels, which are aligned with the grounding objective.\n4. Quantitative improvements. Results on Charades-STA, ActivityNet, and QvHighlight show consistent gains.\n5. Good analysis and ablations. The ablation study provides insight into the contribution of each inversion task and the effect of task probability, which helps validate the design decisions."}, "weaknesses": {"value": "1. Teaser figure clarity (Fig. 1). The sequence of frames is visually unclear, and it is difficult to understand the action being performed. Consider refining the teaser fig. \n2. Limited per-component ablation. Table 2 investigates single-task vs. multi-task setups but does not test pairwise combinations (e.g., VC+AR, AR+VD, VC+VD). Adding these 3 additional experiments would more clearly quantify each component’s interaction.\n3. Missing recent related work. The paper misses recent temporal grounding works such as Number it: Temporal Grounding Videos like Flipping Manga (CVPR 2025) and TimeSuite (ICLR 2025), both relevant to grounding and MLLM temporal reasoning. These should be discussed for completeness. \n4. Limited qualitative diversity. The qualitative examples (Figs. 7–9) mostly feature human actions like “opening/closing doors.” More varied examples (e.g., multi-person or non-human object interactions) would strengthen the claim of generalizable action understanding."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GyP9ok9eoV", "forum": "QQCrZXWG9s", "replyto": "QQCrZXWG9s", "signatures": ["ICLR.cc/2026/Conference/Submission8664/Reviewer_9Nkj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8664/Reviewer_9Nkj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761604261250, "cdate": 1761604261250, "tmdate": 1762920481769, "mdate": 1762920481769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Invert4TVG, a framework for Temporal Video Grounding (TVG). The key motivation is that current TVG models (e.g., Time-R1) tend to over-optimize temporal IoU while neglecting action understanding, leading to semantically incorrect grounding despite high IoU scores. The authors propose three inversion-based auxiliary tasks (Invert-TVG) derived from the original TVG supervision. Experiments on some popular datasets show SOAT performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The idea of reversing the TVG process to construct self-supervised auxiliary objectives is conceptually fresh and well-motivated.\n+ The method is compatible with large LVLMs and scalable to different model sizes (3B and 7B).\n+ The article is written in a prominent style, making it easy for readers to grasp the core points."}, "weaknesses": {"value": "+ The authors claim that “existing TVGs over-optimize IoU, leading to semantic degradation”, but this paper lacks the experimental analysis of IoU improvement.\n+ The datasets used in TVG and Invert-TVG duplicates? Is the Invert-TVG more like a video QA task?\n+ Limited performance improvement compared to Time-R1. There are no related ablation experiments for parameter p=0.8 in the main paper."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "igxxUA9pvv", "forum": "QQCrZXWG9s", "replyto": "QQCrZXWG9s", "signatures": ["ICLR.cc/2026/Conference/Submission8664/Reviewer_voLD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8664/Reviewer_voLD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810307745, "cdate": 1761810307745, "tmdate": 1762920481449, "mdate": 1762920481449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel approach to address the common issue of insufficient action understanding in Temporal Video Grounding (TVG) models, often stemming from an over-optimization of Intersection-over-Union (IoU). The core contributions of the paper include identifying the degradation of action understanding due to IoU over-optimization, and subsequently proposing three self-supervised inversion TVG tasks—Verb Completion, Action Recognition, and Video Description—derived from original TVG annotations to enhance the model's semantic understanding of actions across different granularities. The framework employs a dynamically balanced reinforcement learning strategy, executing the primary TVG task with high probability and interleaving it with lower-probability inversion TVG tasks. This ensures the model maintains robust temporal localization while continuously reinforcing its action understanding. Invert4TVG consistently outperforms state-of-the-art approaches across various benchmarks, showcasing its superior capability in understanding complex actions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper introduces the unique \"Inversion TVG Tasks\" combined with a dynamically probabilistic reinforcement learning framework, cleverly leveraging existing data for self-supervised action understanding, providing a novel problem-solving and execution strategy for the TVG field.\n2.The methodology is rigorously designed, especially in the three multi-granularity inversion tasks and their reward functions. Experiments are comprehensive, yielding significantly superior SOTA performance across multiple benchmarks (including zero-shot settings), further validated by thorough ablation studies, making the conclusions robust.\n3.The paper is logically structured and lucidly written. High-quality visualizations are used effectively to make complex motivations and the proposed framework highly intuitive and easy to comprehend.\n4.This work not only addresses a key bottleneck in the TVG field, significantly advancing the state of the art, but its core idea of \"inversion tasks\" also holds broad inspirational value, offering a new paradigm for other multimodal alignment and understanding tasks."}, "weaknesses": {"value": "1.The paper’s central claim is that \"by reversing the task, the model’s action-understanding ability is preserved and even enhanced,\" and this improvement is presented as the reason for the superior Temporal Video Grounding (TVG) performance. However, throughout the experimental section the authors only report higher TVG localization metrics (R1@m). They never directly evaluate the final Invert4TVG model on the action-understanding tasks (i.e., the three reversed tasks: VC, AR, and VD) on the test set. Consequently, one can only infer—indirectly—from the improved localization accuracy that action understanding has become stronger, which lacks direct evidence. The authors should supplement the corresponding experiments to provide conclusive proof for the core argument that \"action-understanding ability is improved.\"\n\n2.The notion of \"action understanding\" is overly simplified into superficial verb matching, and its binary reward mechanism cannot distinguish genuine semantic comprehension from opportunistic keyword generation. Such a narrow proxy disregards the deep understanding of context, entity properties, and logical relations indispensable for the TVG task. Consequently, any improvement may reflect only enhanced pattern-matching rather than true generalizable understanding, fundamentally undermining the scientific explanatory power of the core claim—\"boosting localization performance by enhancing comprehension.\"\n\n3.A fundamental limitation of this methodology is its atomistic, verb-centric design, which treats isolated verbs as the basic unit and inherently disregards the compositional nature of language. Consequently, it cannot handle complex queries that involve attributes, temporal order, or logical relations—examples include “event B happens after event A” or “the action performed by the person wearing red.” The method’s success is therefore likely confined to simple, atomic queries and fails to generalize to the richer, more complex TVG scenarios common in real-world applications. The reported inability to understand “again” is not an incidental error but a direct symptom of the approach’s intrinsic deficiency in modeling relational information, which severely restricts the universality and practical value of its contributions."}, "questions": {"value": "1.The core hypothesis of the paper is that reversing the task (Invert-TVG tasks) enhances the model’s action-understanding ability, which in turn improves temporal video grounding (TVG) performance. At present, however, this is only an indirect inference drawn from the final localization results. How do you plan to verify that the model’s action-understanding ability has indeed been improved, thereby boosting its TVG performance?\n\n2.The verb-centric design appears to fundamentally limit the model’s ability to handle complex language. Your own failure case (failing to understand “again”) already hints at this, and it does not seem to be a minor issue. Could you discuss more deeply how your method performs and where it breaks down when faced with compositional queries that contain:  \n1. entity attributes (e.g., “the person wearing red”);  \n2. strict temporal relations (e.g., “after A, before B”);  \n3. logical or causal relations (e.g., “tried to do something but failed”)?\n\n3.Your simplistic binary reward—full credit for matching only the verb—invites reward hacking: the model can earn a perfect score by producing meaningless sentences that happen to contain the target verb, instead of truly understanding the video. How can knowledge acquired through such \"short-cutting\" effectively benefit the main TVG task, which requires deep comprehension? Moreover, why did you opt for this minimal reward rather than metrics that better evaluate content quality (e.g., semantic similarity) to mitigate this risk?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lSXIrLbjFR", "forum": "QQCrZXWG9s", "replyto": "QQCrZXWG9s", "signatures": ["ICLR.cc/2026/Conference/Submission8664/Reviewer_nj73"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8664/Reviewer_nj73"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878280073, "cdate": 1761878280073, "tmdate": 1762920481155, "mdate": 1762920481155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Invert4TVG, a GRPO-based reinforcement learning framework for temporal video grounding that mitigates the loss of action understanding caused by optimizing solely for IoU. The core idea is to invert the TVG mapping via three auxiliary tasks—Verb Completion, Action Recognition, and Video Description—and interleave them with the main TVG objective through probabilistic scheduling (≈80% TVG, 20% Invert-TVG). On Charades-STA, Invert4TVG improves R1@0.7 by 7.1 points over Time-R1, demonstrating the benefit of coupling localization with semantics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) **Clear diagnosis and motivation.**\n   The paper convincingly shows that **IoU-centric optimization can erode action understanding**, with Figure 1 and accompanying evidence making the point concrete. The “buttoning vs. unbuttoning” example effectively illustrates the failure mode.\n\n2) **Strong empirical gains.**\n   The method reaches **44.0% R1@0.7 on Charades-STA**—a **+7.1** point improvement over Time-R1—and delivers **consistent improvements across Charades-STA, ActivityNet, and QVHighlights**. Notably, both the **3B** and **7B** variants benefit.\n\n3) **Practical auxiliary task design.**\n   The three **inversion tasks** (Verb Completion, Action Recognition, Video Description) **reuse existing TVG annotations** and require **no extra data collection**, making the approach pragmatic and easy to adopt."}, "weaknesses": {"value": "1) **Incremental novelty via training reformulation.**\n   The main ingredients—GRPO-based RL, inversion-style auxiliary tasks, and template/format rewards—are adapted from existing ideas. The contribution lies primarily in **recasting TVG training** rather than introducing a fundamentally new algorithmic primitive.\n\n2) **Verb-centric semantics may be brittle.**\n   Reliance on **SpaCy verb lemmatization** for VC/AR/VD risks overlooking **non-verbal cues** (objects, states) and **nuanced modifiers** (e.g., adverbs, negation, ordinality), potentially limiting semantic coverage.\n\n3) **Unclear computational overhead.**\n   The paper does not quantify the **runtime/VRAM costs** introduced by the three auxiliary tasks (data processing, reward computation, extra forward/backward passes). A concise **cost table** and throughput metrics would clarify practicality.\n\n4) **Narrow baseline scope.**\n   Comparisons focus largely on **Time-R1** (concurrent work). Including **multi-task/regularization** baselines (e.g., auxiliary captioning, CLIP-style consistency, feature-level regularizers) would better contextualize the benefits of the inversion tasks."}, "questions": {"value": "Please refer to the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "I0lI38PWmf", "forum": "QQCrZXWG9s", "replyto": "QQCrZXWG9s", "signatures": ["ICLR.cc/2026/Conference/Submission8664/Reviewer_tE1U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8664/Reviewer_tE1U"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004889650, "cdate": 1762004889650, "tmdate": 1762920480568, "mdate": 1762920480568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}