{"id": "DewXWSvQPH", "number": 21589, "cdate": 1758319339619, "mdate": 1759896913563, "content": {"title": "TimeSeriesExamAgent: Creating TimeSeries Reasoning Benchmarks at Scale", "abstract": "Large Language Models (LLMs) have shown promising performance in time series modeling tasks, but do they truly understand time series data? While multiple benchmarks have been proposed to answer this fundamental question, most are manually curated and focus on narrow domains or specific skill sets.\nTo address this limitation, we propose scalable methods for creating comprehensive time series reasoning benchmarks that combine the flexibility of templates with the creativity of LLM agents. We first develop TimeSeriesExam, a multiple-choice benchmark using synthetic time series to evaluate LLMs across five core reasoning categories: pattern recognition, noise understanding, similarity analysis, anomaly detection, and causality. We then scale our approach by automatically generating benchmarks from real-world datasets spanning healthcare, finance and weather domains. Through multi-dimensional quality evaluation, we demonstrate that our automatically generated benchmarks achieve diversity comparable to manually curated alternatives. However, our experiments reveal that LLM performance remains limited in both abstract time series reasoning and domain-specific applications, highlighting ongoing challenges in enabling effective time series understanding in these models.", "tldr": "", "keywords": ["Time Series Reasoning", "Scalable Benchmarking", "AI Agent", "Multimodal Learning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6525e863dc9d896d613488ad59a45ee974419146.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a multi-agent framework in constructing time series related multiple choice questions based on synthetic and real time series. The generator LLM is in charge of creating qa pairs using python functions and verifier LLMs perform validation on generated questions. The paper evaluated four VLMs on the two sets of benchmarks and demonstrated that current LLM cannot perform reliability on relatively more complex questions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The benchmark creation process is almost fully automated except for expert created user instructions. \n\nThe paper is easy to follow\n\nThe agentic framework can be scaled up."}, "weaknesses": {"value": "Since this is a multi agent framework and is considered to be scaled up in the future, cost is an important consideration that didn't seem to be mentioned or discussed in the paper. \n\nSince the benchmark is fully template based, how to mitigate bias in limited linguistic diversity since humans usually ask questions using diverse expressions and solver LLMs could be sensitive to prompts. And since questions are generated by a single LLM rather than a panel of LLM, how to mitigate bias here that a specific LLM may tend to generate questions in a specific way. I mentioned this because the validation is performed by a panel of LLM. \n\nIt feels somewhat like a snake biting its own tail situation where questions are only kept if it's solvable/answerable by an LLM so then why test LLMs on this? And in particular in section 4.3, it's mentioned that you specifically used weaker LLMs as judge, then it didn't make sense to also evaluate answerability of questions. \n\nAlthough the author discussed this in limitation, this is a relatively important point to bring up is that the evaluation is limited to VLMs. On the more practical side, time series reasoning is hard because you need both numerical computation and reasoning. \n\nThe number of evaluated models is very few."}, "questions": {"value": "Are there examples of failed qa pairs? Usually what are reasons that this proposed agentic framework fails? Some analysis on this would be very beneficial. \n\nIn sec 4.4, you discussed transferability of the reasoning , but how similar are the synthetic samples to the ECG-QA samples in terms of type of questions asked and formats. \n\nFor are questions generated, is it always 4 options? If not, it would be better to include random guessing for table 3 and 4 to know how the models compare to random guessing.  \n\nCould you explain a little on why anomaly detection questions are harder than causality questions? Intuitively to verify causal relationships, it requires some statistical analysis, but for anomalies in time series, it's relatively easier to spot given that you are inputting time series as images. If it's unable to recognize that, it may not reflect much on time series reasoning limitation but possible perception limitations -> It's hard to isolate the cause of failures."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lcBII8Oyrz", "forum": "DewXWSvQPH", "replyto": "DewXWSvQPH", "signatures": ["ICLR.cc/2026/Conference/Submission21589/Reviewer_urhb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21589/Reviewer_urhb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761504178300, "cdate": 1761504178300, "tmdate": 1762941846797, "mdate": 1762941846797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TimeSeriesExam and TimeSeriesExamAgent, a framework for generating scalable benchmarks to evaluate LLM reasoning over time series data. The authors start from a controlled synthetic setup (TimeSeriesExam) that assesses LLM understanding of five reasoning categories (pattern recognition, noise understanding, similarity, anomaly detection, and causality), and then extend it with an agent-based system that automates benchmark creation from real datasets in finance, healthcare, and meteorology. Experiments show that (i) current VLMs still struggle with time series reasoning, and (ii) the automatically generated benchmarks achieve comparable diversity and quality to human-curated datasets like ECG-QA. The paper further includes a transfer learning study showing modest improvements when fine-tuning on generated data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The intersection of time-series analysis and LLM reasoning is rapidly emerging. The paper tackles a genuine gap: the lack of scalable, reasoning-oriented benchmarks for time series.\n2. The combination of templated generation, IRT-based refinement, and multi-agent verification (planning–generation–validation) is thoughtfully constructed and clearly explained (see Fig. 2 on p. 5).\n3. The paper is very detailed, with algorithmic pseudocode (Algorithm 1), hyperparameter tables, and qualitative examples (e.g., ECG and finance question examples on pp. 21–23). This adds credibility and reproducibility.\n4. Evaluation covers multiple domains (medical, financial, weather), and the analysis of generated diversity via embedding and Levenshtein metrics (Table 5) is thorough."}, "weaknesses": {"value": "1. While the engineering effort is strong, the research novelty is limited. The approach largely combines existing components (template-based generation, LLM-as-judge, IRT calibration, multi-agent verification) without a clear new algorithmic or theoretical contribution. The work feels more like a comprehensive system paper than a research breakthrough.\n2. The reported results, though broad, are not deeply analyzed. For example:\n- Table 4 results (< 55% accuracy) confirm that models perform poorly, but there is little insight into why or what specific reasoning capabilities fail.\n- The fine-tuning improvement (21.8 → 47.0%) in Table 7 is promising but lacks ablations—was this due to better question structure, domain alignment, or synthetic diversity?\n3. The paper relies heavily on LLM-as-a-judge for quality assessment (G-Eval + model juries). While practical, this is circular—using the same family of models to judge generated questions about themselves risks hidden bias. No human evaluation beyond anecdotal clinician feedback is systematically quantified.\n4. Although the agent framework is framed as “scalable,” the actual generation still depends on domain-specific prompts and datasets (discussed in Sec. 5). The system’s generalization to unseen domains is not convincingly demonstrated.\n5. The paper is dense and occasionally reads like a project report. The core message is sometimes buried under implementation details. For an ICLR audience, the novelty and research implications should be foregrounded, while extensive technical appendices could be condensed."}, "questions": {"value": "1. How does TimeSeriesExamAgent compare to recent agentic benchmark generators like BenchAgents (2024) in quantitative efficiency (cost, generation speed, diversity per token)?\n2. Can the authors provide human evaluation results verifying that LLM-judged “quality” correlates with expert judgment?\n3. How does the performance of LLMs differ when reasoning over text-encoded vs. visualized time series inputs (as briefly noted in Sec. 5)?\n4. Can the framework generalize to non-numeric structured domains (e.g., tabular, event logs) without major prompt re-engineering?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nxjGKd1WdK", "forum": "DewXWSvQPH", "replyto": "DewXWSvQPH", "signatures": ["ICLR.cc/2026/Conference/Submission21589/Reviewer_kZtv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21589/Reviewer_kZtv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814252309, "cdate": 1761814252309, "tmdate": 1762941846437, "mdate": 1762941846437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a two-level approach for evaluating time series reasoning in large language models. The first level is a controlled multiple choice benchmark named TimeSeriesExam that uses synthetic series to probe five core capabilities including pattern recognition, noise understanding, similarity analysis, anomaly detection, and causality. The second level is TimeSeriesExamAgent, an agentic pipeline that takes a natural language task description and a dataset loader, then generates executable Python question templates with rule based answer computation. The pipeline applies three verification stages including structure checks, LLM judging, and capability aligned filtering, with the goal of producing high quality, unambiguous, domain specific items at scale and with limited expert time. The paper evaluates across healthcare, finance, and weather datasets. It reports that modern vision language models still struggle on higher order time series reasoning, and that the automatically generated benchmarks reach diversity comparable to manually curated sets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. In this paper, authors provide clear articulation of the practical bottleneck. The paper motivates the need for scalable domain specific evaluation rather than one size fits all benchmarks and gives an agentic design that maps cleanly to practitioner workflows.\n\n2. In the paper, generating templates rather than individual questions improves reuse and ensures rule based answer derivation from real series, which supports repeatability and reduces silent errors. The rationale is explicitly stated and connected to implementation details.\n\n3. The combination of syntax checks, LLM judging, and capability aligned filtering is a thoughtful safeguard against ill posed or trivial items and is presented with a concrete architectural sketch.\n\n4. The authors document how clinician feedback on ECG terminology guided prompt refinement, which improves face validity and reduces ambiguity for medical users."}, "weaknesses": {"value": "1. In the paper, contribution hierarchy and granularity are not fully disentangled. The proof of concept and the agentic system are linked, but the paper would benefit from a clearer map of which empirical claims rely on synthetic control versus real data generation. The introduction hints at both goals, but the boundaries remain blurred in places.\n\n2. In this paper, heavy reliance on LLM as a judge raises construct validity concerns. The paper provides judging rubrics, yet it remains unclear how consistent the scores are across different judges and prompts, or how often human experts overrule LLM judgments during verification.\n\n3. The evaluation uses base64 plot images at low DPI. This choice can cap the information bandwidth and may conflate perception limits with reasoning limits. A sensitivity analysis on plot resolution and rendering parameters would strengthen the claim that failures reflect reasoning rather than visibility.\n\n4. The finetuning setup is described with clear hyperparameters, but the paper does not report confidence intervals or significance for the reported improvements, nor ablations on the volume or composition of generated items.\n\n5. The idea that weak models should not outperform strong ones on valid items is intuitive, but the authors does not quantify how many candidate templates are filtered for this reason or provide failure taxonomies that would help others replicate the filtering policy."}, "questions": {"value": "1. How do you guarantee label faithfulness for items derived from real datasets where the answer is computed by a template rather than given by an external ground truth?\n\n2. How sensitive is item difficulty to the specific hyperparameters of the template functions?\n\n3. Finetuning on generated items improves performance on one downstream benchmark. Does this transfer to an external benchmark that was not seen during template creation and that is annotated independently?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "atty9jJBGx", "forum": "DewXWSvQPH", "replyto": "DewXWSvQPH", "signatures": ["ICLR.cc/2026/Conference/Submission21589/Reviewer_aEWY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21589/Reviewer_aEWY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893208175, "cdate": 1761893208175, "tmdate": 1762941846095, "mdate": 1762941846095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new framework for generating time series reasoning benchmarks at scale, arguing that existing benchmarks are manually curated, narrow, and expensive to create. The authors' contribution is twofold:\n\nTimeSeriesExam: A \"proof-of-concept\" benchmark consisting of multiple-choice questions based on synthetic time series data. It aims to test five core, domain-agnostic reasoning skills: pattern recognition, noise understanding, similarity, anomaly detection, and causality.\n\nTimeSeriesExamAgent: A multi-agent framework designed to automatically generate new, domain-specific benchmarks from real-world datasets. This agentic pipeline (using Generator, Concept, Verifier, and Student LLMs) takes a dataset (e.g., PTB-XL) and a prompt, and outputs a new \"exam\" with questions derived from that data.\n\nThe paper's central findings are that (1) its agent can generate benchmarks with diversity \"comparable to human-curated\" ones, and (2) all state-of-the-art LLMs (GPT-4o, Gemini 2.5-Pro) perform poorly on both the synthetic and the agent-generated exams, suggesting their time series reasoning capabilities are \"limited.\""}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Valid and Important Problem: The paper correctly identifies a significant gap in the field. Assessing the true reasoning of LLMs on time series data (beyond simple forecasting accuracy) is a critical and under-explored problem. The goal of creating a scalable benchmark generation process is highly valuable.\n\nNovel Agentic Framework: The core idea of TimeSeriesExamAgent is the paper's strongest contribution. Using a multi-agent pipeline to generate, verify, and filter questions is a novel and powerful approach to benchmark creation that is far more scalable than the manual curation.\n\nSophisticated Verification Loop: The verification process (Fig. 2) is well-considered. It includes not just a structural check and an \"LLM Verifier,\" but also a \"Capability-Aligned Filtering\" step that uses \"Student LLMs.\" The idea of discarding questions that weaker models perform better on is a clever and sound method for filtering out flawed or noisy questions.\n\nStrong Validation Signal (Table 7): The transfer learning experiment is a key piece of evidence. Showing that fine-tuning a VLM on the agent-generated data improves its performance on a separate, human-curated dataset (ECG-QA) strongly suggests that the generated questions provide a meaningful and useful learning signal."}, "weaknesses": {"value": "1. Recursive and Subjective Evaluation:\n\nThe paper's quality control relies heavily on an \"LLM-as-a-judge\" and \"LLM-as-a-Jury\" (Table 6, Appendix G.1) to assess the quality of the generated questions (e.g., \"Specificity,\" \"Unambiguity\"). This creates a recursive, self-referential loop: an LLM (Generator) creates a question, which is then validated by another LLM (Verifier), to create a benchmark that is then used to... test LLMs.\n\nThis \"LLM-evaluates-LLM\" methodology is notoriously subjective, prone to bias (e.g., favoring its own \"style\"), and lacks the objective rigor required for a foundational benchmark. While the authors present this as a feature, it's a methodological weakness that introduces significant unreliability.\n\n2. Incomplete Model Comparison:\n\nWhile the paper tests several SOTA VLMs, the evaluation is incomplete. The primary TimeSeriesExamAgent framework is designed to generate text and time series questions from real-world data, yet the evaluation is performed only by VLM-style models that take images (Section H). The paper even notes this limitation itself (Section 5, \"Limited Evaluation Mode\"), stating that \"certain question types... are particularly difficult to answer from images alone.\" This mismatch between the generated data (which is numerical) and the evaluation mode (which is visual) is a strange and limiting choice. The framework is not evaluated against text-only LLMs coupled with numerical data, which would be a more direct test of reasoning over the raw time series."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Las1phNCVF", "forum": "DewXWSvQPH", "replyto": "DewXWSvQPH", "signatures": ["ICLR.cc/2026/Conference/Submission21589/Reviewer_RbCS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21589/Reviewer_RbCS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982961027, "cdate": 1761982961027, "tmdate": 1762941845752, "mdate": 1762941845752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}