{"id": "SOxO7e6ySB", "number": 5, "cdate": 1756728038661, "mdate": 1763653984878, "content": {"title": "Language Models Do Not Have Human-Like Working Memory", "abstract": "While Large Language Models (LLMs) exhibit remarkable reasoning abilities, we demonstrate that they fundamentally lack a core aspect of human cognition: working memory. Human working memory is an active cognitive system that enables not only the temporary storage of information but also its processing and utilization. Without working memory, individuals may produce unrealistic conversations, exhibit self-contradictions, and struggle with tasks that require mental reasoning. Existing evaluations using N-back or context-dependent tasks fail as they allow LLMs to exploit external context rather than retaining the reasoning process in the latent space. We introduce three novel tasks—(1) Number Guessing, (2) Yes-No Deduction, and (3) Math Magic—that isolate internal representation from external context.\nAcross seventeen frontier models spanning four major model families, we consistently observe irrational or contradictory behaviors, highlighting LLMs' inability to retain and manipulate latent information. Our work establishes a new benchmark for evaluating working memory in LLMs and identifies this deficit as a critical obstacle to artificial general intelligence. Code and prompts will be made publicly available upon publication.", "tldr": "", "keywords": ["Large Language Model", "Working Memory"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac2a0bb5ee11376c543f9ebf002b08b621a6ab5d.pdf", "supplementary_material": "/attachment/ee19770b3a6c3e66c892d5fda95d59c64d0dc169.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies whether LLMs exhibit human-like working memory. In contrast to prior studies that assess “working memory” in LLMs by asking models to retain and process information from long contexts, this paper defines “working memory” in LLMs as the capacity to retain and manipulate information encoded in the models’ latent space. To investigate whether LLMs exhibit human-like working memory, the paper evaluates models on three different tasks: (i) Number Guessing, where models are instructed to “think” of a number and must indicate whether the user guesses the correct number; (ii) Yes–No Deduction, a version of the Twenty Questions game where models play the question-answering role; and (iii) Math Magic, where models perform a series of mathematical operations on a self-chosen four-digit number before responding with the final result. The authors argue that, due to the poor performance of various LLMs on these tasks, current LLMs do not exhibit behavior indicative of a functional “working memory.”"}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "A summary of the paper’s strengths is given below:\n\n- **Well structured and clearly written:** The paper is well structured and easy to read. It introduces key concepts and provides sufficient experimental detail to follow the analyses presented. The level of detail further allows for easy reproducibility of the experiments conducted.\n\n- **Extensive analyses:** The paper presents a variety of experiments (Sections 3–5) using a broad range of LLMs, including both instruction-tuned and long-CoT reasoning models.\n\n- **Appropriate criticism:** The paper’s criticism of whether current studies truly examine “working memory” when evaluating models on long-context tasks is justified. However, whether it is necessary to map a concept of human cognition such as working memory to LLMs—given that humans and LLMs process information in fundamentally different ways—or whether the proposed approach is more appropriate, is a different aspect (see *Weaknesses* and *Questions*)."}, "weaknesses": {"value": "A summary of the paper’s weaknesses is provided below:\n\n**Flawed experimental design.** The authors define “working memory” in LLMs as the ability to “retain and manipulate latent information” (l. 23) and therefore test “whether LLMs can internally maintain information that is not present in the input context” (ll. 97–98). To this end, the paper introduces three tasks: (i) Number Guessing, (ii) Yes–No Deduction, and (iii) Math Magic. However, given this objective, two of these tasks—Number Guessing and Yes–No Deduction—are fundamentally flawed.\n\n  - **Number Guessing.** In this task, models are given the fixed prompt “USER: Think of an integer between 1 and 10, but don’t say it to me. ASSISTANT: Got it! I’ve thought of an integer between 1 and 10. What’s next?” (ll. 182–183), and the model is then “independently prompted 200 times for i = 1, … , 10 with queries such as ‘Is the number you’re thinking of i? Answer Yes or No’” (ll. 184–185). The authors find that model answers do not represent “distributions consistent with internally committing to a number” (l. 267). However, this design is flawed: each query is evaluated independently of prior runs. This setup is akin to asking ten different groups of 200 people (one group per value of i, 200 × 10 people in total) “Is the number you’re thinking of i? Answer Yes or No.” The resulting proportions may significantly deviate from $\\sum_{i=1}^{10} p_i = 1$ and could exhibit strong bias (as reflected in Section 3). This, however, does not indicate whether a respondent has “internally committed to a number” (l. 267). The critical issue is that these model runs are independent from each other, so there is no single number the model has previously committed to.\n  - **Yes–No Deduction.** Similarly, the Yes–No Deduction task does not test whether models “maintain […] an imagined object in working memory” (l. 280) or retain latent information from a previous query. Instead, it resembles a setup in which, rather than one player answering questions about a predefined object, we have N players who sequentially answer new questions based on previous players’ responses. With each new model query, latent information from the prior run is lost because output tokenization discretizes the signal during generation. Consequently, the model lacks access to the initial latent information associated with the original object choice. It is therefore invalid to draw conclusions about whether information about the “imagined” object can be maintained, as it is never passed to subsequent runs.\n  - **Math Magic.** The Math Magic task (Section 5) is the only one performed within a single model run and thus the only task that permits drawing conclusions about whether latent information can be maintained and manipulated—consistent with the authors’ definition of working memory. However, the resulting insights are limited, as it is unclear whether poor performance stems from errors in latent reasoning (e.g., incorrect intermediate computations) or from an inability to retain latent information over the course of the operations. Extending the analysis with tools such as probing could help reveal which information the model represents in its hidden states.\n\n**Unjustified conclusions.** Given the issues above, the claim that models “fail to internally represent or manipulate transient information across multiple reasoning steps” (ll. 457–458) is not sufficiently supported by the experiments. Likewise, the conclusion that current LLMs lack a “working memory,” defined as the ability to “retain and manipulate latent information” (l. 23), and that the “absence of this buffer in LLMs may explain why they excel at visible reasoning (e.g., think step by step) yet collapse when asked to ‘think silently’” (ll. 474–476), is not adequately backed by the presented evidence."}, "questions": {"value": "A list of comments and suggestions is given below:\n\n- **Human-like working memory in LLMs:** The question is whether it is useful—or even possible—to map a concept from human cognition, such as working memory, onto large language models. Since human information processing is fundamentally different from that of LLMs, wouldn’t each definition or evaluation of “working memory” in LLMs lack certain traits or contain flaws?\n\n- **Reusing latent information from prior model runs:** If we want to keep the suggested definition that a human-like “working memory” in LLMs is determined by the model’s ability to “retain and manipulate latent information” (l. 23), a possible adaptation—e.g., in the Number Guessing task—would be the following: first, prompt the model to “Think of an integer between 1 and 10, but don’t tell me what it is.” Then record its hidden states that store latent information. Next, ask the model, “Is the number you’re thinking of i? Answer Yes or No.” but additionally provide the model with the recorded latent information from the first model run. This would allow you to evaluate whether the model properly reuses the latent information provided from the previous run."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vzWBy4YNNz", "forum": "SOxO7e6ySB", "replyto": "SOxO7e6ySB", "signatures": ["ICLR.cc/2026/Conference/Submission5/Reviewer_hNtX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5/Reviewer_hNtX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851810284, "cdate": 1761851810284, "tmdate": 1762915435293, "mdate": 1762915435293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether LLMs possess human-like working memory through three carefully designed experiments, evaluates the behavior across multiple models, and ultimately concludes that current models do not demonstrate such working-memory capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. I appreciate the idea of evaluating whether current LLMs possess human-like working memory from a cognitive science perspective.\n\n2. The authors designed three interesting reasoning experiments, although I have some doubts about the design of the first experiment."}, "weaknesses": {"value": "## **Weaknesses and Questions**\n\n### 1. Questioning the conclusion on lack of working memory\nWhile I appreciate the paper's effort, I am not fully convinced by its conclusion that LLMs essentially lack \"working memory.\" The study does not employ **any existing interpretability tools** (especially Mechanistic Interpretability Tool) to directly examine the model's actual internal activation patterns, which limits the strength of the claim. Without probing what happens inside the model’s latent space, the conclusion feels somewhat premature.\n\n---\n\n### 2. Concerns with Section 3: *Number Guessing* experiment\n\nThe **Number Guessing** experiment in Section 3 does not make sense to me for the following reasons:\n\n- **200 prompts are too few** to obtain a reliable probability estimate.\n- If my understanding is correct, the experiment queries the model independently each time with only one question per session. In that case, isn’t it entirely possible that the model simply thinks of a different number in each session—and that number is always different from the queried number?  \n  If so, the combined probability of answering “Yes” would naturally approach **zero**, not because the model lacks working memory but because the sampled number rarely matches the guess.\n\nTherefore, I do not find this experiment a valid measurement of internal commitment or memory. Please correct me if I misunderstand anything here.\n\n---\n\n### 3. Concerns with Section 4:\n\nFor Section 4, I would lean toward a different interpretation: LLMs may possess **fragile and interference-prone working memory** rather than no working memory at all. The fact that models can go through 20–40 questions before failing suggests that some internal representation initially exists, but gradually deteriorates due to interference from the growing long context.\n\nTwo observations may support this interpretation:\n\n- **Forgetting in long contexts.** Prior works [1, 2] show that even when background facts and definitions are explicitly given, LLMs often forget or confuse them in long dialogue contexts. If they struggle with externally provided context, it is reasonable that their *internal temporary representations* are even more vulnerable to interference.\n\n- **Humans forget earlier questions—LLMs do not.**  \n  Humans benefit from forgetting irrelevant early information; for example, if I am queried with the 20th question, at that time I probably do not remember the first one. But LLMs cannot forget — every token stays in context and is continuously processed, increasing the likelihood of confusion and hallucination. Ironically, this may make LLMs **more prone to memory interference than humans**, not less.\n\n\n\n---\n**References**\n\n[1] Lost in the Middle: How Language Models Use Long Contexts\n\n[2] Can't Remember Details in Long Documents? You Need Some R&R\n\n\n\nPlease feel free to correct me if I misunderstand anything."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6a4oE1DK5z", "forum": "SOxO7e6ySB", "replyto": "SOxO7e6ySB", "signatures": ["ICLR.cc/2026/Conference/Submission5/Reviewer_f9Y7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5/Reviewer_f9Y7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887984595, "cdate": 1761887984595, "tmdate": 1762915435147, "mdate": 1762915435147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents three experiments to investigate whether LLMs exhibit human-like internal memory, where the model needs to maintain certain intrinsic information consistently without articulation (i.e not present in its context). One research question here is the evaluation for such unobserved information. \n\nThe three explored tasks are:\n\n(1) **Number Guessing** that evaluates whether an LLM’s response distribution across repeated identical, queries remains valid. This paper independently prompted 200 times and compute the estimated probabilities.\n\n(2) The **Yes–No Deduction** game that examines whether LLMs contradict themselves. This paper maintained a list of object with five properties: volume, length, weight, density, and hardness, and record the number of questions it completed before the model exhibits a self-contradiction.\n\n(3) The **Math Magic** that assesses whether LLMs’ internal reasoning produces correct outcomes. Based on a math routine by the Josephus Problem that guarantees two remaining numbers are identical after a few math operations, this paper tests if LLMs can perform the task robustly.\n\nThe results show that models such as  gpt-4, llama, etc., do not perform well on these tasks"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposed three tasks to speculate the unobserved information within LLMs. It studies an interesting problem whether LLMs exhibit human-like internal memory, where the model needs to maintain some intrinsic information consistently without articulation.\n\nI do appreciate some results such as  \"Most LLMs never produce a “Yes” response\" and that in the number guessing game, they exhibit a  preference for the number seven which mirrors human biases"}, "weaknesses": {"value": "Experiment setup is a bit questionable.  I also found the paper report mostly qualitative patterns while the statistical rigor shall be improved.\n1. The overall claim “LLMs do not have human-like working memory” is a bit generic and lacks clear definition. Whereas this paper only investigates if the model can maintain certain intrinsic information consistently.\n\n2. Line 158-159: \n>All three experiments are designed to demonstrate that current LLMs lack effective working memory**, as evidenced by their inability to perform our proposed tasks. \n\n\nThis seem to indicate that the authors first have a conclusion and then design tasks to verify the conclusion.\n\n3. The results largely depend on the decoding mechanism. Now we know that even for greedy decoding, the results are not fully deterministic, and for stochastic ones such as sampling, parameters such as temperature also plays a significant role. However, no discussion about the decoding scheme is discussed or studied."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "73skVHnINl", "forum": "SOxO7e6ySB", "replyto": "SOxO7e6ySB", "signatures": ["ICLR.cc/2026/Conference/Submission5/Reviewer_FSMT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5/Reviewer_FSMT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762073553394, "cdate": 1762073553394, "tmdate": 1762915434978, "mdate": 1762915434978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful and encouraging assessments. **We greatly appreciate the recognition of:**\n1. Novel task design. (FSMT, f9Y7)\n2. Critics on current LLM working memory evaluation. (hNtX)\n3. Reproducible results with code available. (hNtX)\n4. Wide range of LLMs evaluated. (hNtX)\n5. Findings of rare “Yes” and preference of “7”. (FSMT)\n\nWe truly appreciate all reviewers and meta reviewer’s time and effort. **We have carefully read and addressed all your concerns, including:**\n1. Clarification about our designs of the number guessing game and the yes-no game. (f9Y7, hNtX)\n2. Our behavioral-oriented methodological design and the high-level hypothesis. (FSMT, f9Y7, hNtX)\n3. Illustration on statistical sufficiency. (FSMT, f9Y7)\n4. Temperature influences. (FSMT)\n\n**All major modifications are highlighted in blue in the paper.** We thank you again for your time and constructive insights."}}, "id": "RpProSDrR1", "forum": "SOxO7e6ySB", "replyto": "SOxO7e6ySB", "signatures": ["ICLR.cc/2026/Conference/Submission5/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission5/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763653891710, "cdate": 1763653891710, "tmdate": 1763653891710, "mdate": 1763653891710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}