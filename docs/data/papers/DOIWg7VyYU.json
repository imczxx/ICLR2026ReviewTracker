{"id": "DOIWg7VyYU", "number": 6363, "cdate": 1757973683682, "mdate": 1759897919619, "content": {"title": "StyleBench: Evaluating  thinking styles in Large Language Models", "abstract": "The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or \\textit{styles of thought}, employed in their prompts. However, the interplay between these reasoning styles, model architecture, and task type remains poorly understood. To address this, we introduce StyleBench, a comprehensive benchmark for systematically evaluating reasoning styles across diverse tasks and models. We assess five representative reasoning styles—Chain-of-Thought (CoT), Tree-of-Thought (ToT), Algorithm-of-Thought (AoT), Sketch-of-Thought (SoT), and Chain-of-Draft (CoD)—on five reasoning tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our large-scale analysis reveals that no single style is universally optimal. We demonstrate that strategy efficacy is highly contingent on both model scale and task type: search-based methods (AoT, ToT) excel in open-ended problems but require large-scale models, while concise styles (SoT, CoD) achieve radical efficiency gains on well-defined tasks. Furthermore, we identify key behavioral patterns: smaller models frequently fail to follow output instructions and default to guessing, while reasoning robustness emerges as a function of scale. Our findings offer a crucial roadmap for selecting optimal reasoning strategies based on specific constraints, We open source the benchmark in https://anonymous.4open.science/r/StyleBench/.", "tldr": "We introduce StyleBench, a benchmark demonstrating that the optimal reasoning style for LLMs is not universal, but critically depends on the specific task and model capabilities.", "keywords": ["Reasoning", "Instruction", "LLM", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f5630fb16eadf1b3766648285be568bbac84a1c7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces StyleBench, a benchmark that systematically evaluates how different reasoning styles affect performance in LLMs. The study spans five tasks (GSM8K, AIME, CommonsenseQA, LogiQA, and Game of 24) and 15 open-source models from 270M to 120B parameters, all evaluated under a fixed zero temperature.\n\nThe central finding is that no single reasoning style is universally optimal. Search-based styles (ToT, AoT) deliver the biggest gains only on open-ended tasks and only when the model is large, while concise styles like SoT and CoD are more stable, efficient, and effective on structured reasoning tasks. The paper also documents scale-dependent failure behavior, token cost trends, and a negative SFT result showing that naive meta-selection of styles collapses into shallow memorization.\n\nThe work provides a clean map from task type and model size to strategy choice. The study is broad enough to expose consistent task-style affinities, offers scaling laws for styles, and documents operational costs via token usage. As a result, practitioners can pick styles with evidence-backed trade-offs, and researchers get a reproducible testbed to probe when depth helps and when brevity wins."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Coverage is solid: five styles, five tasks that meaningfully differ in structure and openness, and 15 models spanning 270M to 120B. The evaluation protocol is clean, with zero temperature and single outputs, which makes cross-style comparisons less noisy. The results are organized into clear takeaways that are easy to operationalize.\n\nThe paper goes beyond averages. It analyzes token budgets, format adherence, and qualitative failures, which gives texture to the numbers. The SFT experiment on automatic style selection is a valuable negative result that prevents readers from over-optimistic conclusions about easy meta-reasoning."}, "weaknesses": {"value": "The one-shot, zero-temperature protocol improves comparability but may understate the potential of search-based styles that benefit from sampling and multi-try evaluation. Reporting a small n-sample sensitivity study could help validate that the core conclusions are stable under mild exploration. Similarly, some tasks may reward majority-vote or verifier-augmented decoding, which is not explored here. \n\nThe benchmark focuses on open-source models and a specific set of tasks. That is reasonable, but external validity to domains like code agents with tool use, retrieval-augmented setups, or safety-critical reasoning is not established."}, "questions": {"value": "(1) Your entire study is based on zero temperature and single output per example. This favors concise deterministic styles and may underplay the full potential of search-based reasoning. Have you validated that the relative style rankings still hold under a realistic 3 to 5 sample majority-vote or verifier setting? A brief table or even partial experiment could significantly increase the robustness of your claims.\n\n(2) You report token usage, but deployment decisions depend on actual latency plus prompt costs (prefix length included), not only output tokens. Can you provide or approximate end-to-end wall-clock inference cost per style for a representative model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The paper uses public benchmarks and models, no human subjects, no sensitive data, and includes an ethics and LLM-usage disclosure."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IBiwOBgZNJ", "forum": "DOIWg7VyYU", "replyto": "DOIWg7VyYU", "signatures": ["ICLR.cc/2026/Conference/Submission6363/Reviewer_Djbq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6363/Reviewer_Djbq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761677738902, "cdate": 1761677738902, "tmdate": 1762918654252, "mdate": 1762918654252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces StyleBench, a benchmark designed to systematically evaluate different reasoning strategies in LLMs. The authors conduct extensive experiments across 5 reasoning styles and 15 open-source LLMs, and report three main findings: (i) the optimal reasoning strategy varies across model families, (ii) certain task types exhibit strong alignment with particular reasoning styles (e.g., structured multi-step reasoning such as CoT is particularly effective for mathematical tasks), and (iii) the effectiveness of reasoning styles scales with model size."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and well structured, making the motivation, methodology, and findings easy to follow.\n- The empirical evaluation is comprehensive, covering multiple reasoning styles across 15 open-source model families. The experiments are thorough and support the key findings reported by the authors."}, "weaknesses": {"value": "- **Limited Novelty of Findings.** Several key observations reported in the paper appear incremental relative to prior literature. For example, the impact of model scaling on instruction-following capabilities has been well documented (e.g., Chung et al., JMLR’25). Likewise, AoT and ToT have been previously evaluated on tasks such as Game-of-24 and are known to perform well on tasks requiring non-trivial planning or search, while SoT and CoD have been shown to be effective on commonsense and logical reasoning tasks. In light of this, the result that “search-based methods like AoT and ToT perform well on open-ended tasks like Game-of-24 but incur higher token usage, while concise methods such as SoT and CoD are more efficient on structured reasoning tasks” does not provide substantial new insight.\n\n- **Concerns Regarding Evaluation Configuration and Determinism.** The paper sets the temperature to 0 across all experiments to ensure deterministic outputs; however, temperature = 0 does not guarantee determinism due to numerical and implementation-level nondeterminism [2, 3]. In addition, model providers often recommend non-zero temperatures for optimal performance (e.g., Qwen [4] suggests temperature 0.6/top-p 0.95; many reasoning-focused models recommend temperature = 1.0 [5]). Some models also include additional configuration parameters that control reasoning depth (e.g., reasoning_effort = {low|medium|high} in GPT-OSS models), but the paper does not clarify how such parameters were set. These inconsistencies make it difficult to attribute observed differences solely to reasoning styles.\n\n- **Lack of Statistical Reliability.** All experiments appear to be run only once, with no confidence intervals or variance estimates reported. Given the stochasticity of LLM outputs and the sensitivity of reasoning performance to hyperparameters, reporting confidence intervals (or at least repeated trials) would strengthen the empirical claims.\n\n- **Missing Comparison to a Relevant Baseline.** The benchmark does not include comparison against the recent Buffer-of-Thoughts (BoT) method [6], which has demonstrated strong performance across diverse reasoning tasks. Incorporating BoT would provide a more comprehensive and contemporary evaluation of reasoning styles.  \n\n---\nReferences \n\n[1] Chung et al., Scaling Instruction-Finetuned Language Models, JMLR’25 \n\n[2] https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/#the-original-sin-floating-point-non-associativity \n\n[3] Yuan et al., Understanding and Mitigating Numerical Sources of Nondeterminism in LLM Inference, NeurIPS’25 \n\n[4] Qwen Technical Report: https://arxiv.org/pdf/2505.09388 \n\n[5] https://docs.unsloth.ai/models/gpt-oss-how-to-run-and-fine-tune#running-gpt-oss \n\n[6] Yang et al., Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models, NeurIPS’24"}, "questions": {"value": "1. For reasoning-focused models (e.g., GPT-OSS), how were configuration parameters such as reasoning_effort selected for the experiments?\n\n2. Appendix F mentions constraining max new tokens to 2048. Did the authors experiment with relaxing this limit—particularly for reasoning-intensive models—given that token budget may affect the relative performance of different styles?\n\n3. For the AIME benchmark, could the authors clarify whether the evaluation corresponds to AIME’24, AIME’25, or a custom subset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CXIlhJvdi3", "forum": "DOIWg7VyYU", "replyto": "DOIWg7VyYU", "signatures": ["ICLR.cc/2026/Conference/Submission6363/Reviewer_n3qh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6363/Reviewer_n3qh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761689182965, "cdate": 1761689182965, "tmdate": 1762918653806, "mdate": 1762918653806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces StyleBench, a benchmark that evaluates five distinct reasoning styles (CoT, ToT, AoT, SoT, and CoD) on five reasoning tasks using 15 open-source language models ranging from 270M to 120B parameters."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper presents an impressive large-scale evaluation, testing 15 models across five reasoning styles and five diverse tasks. The breadth of comparison and cross-architecture coverage is thorough. The inclusion of detailed failure analyses adds real value, particularly the examples showing formatting issues, which offer more insight than the evaluation section. The token usage analysis provides an interesting and non-obvious observation that smaller models do not fully use their token budgets on hard problems, challenging common assumptions about computational limits. Reproducibility practices are solid, with open-source materials, full prompts, and documented hyperparameters."}, "weaknesses": {"value": "While the paper runs an impressive number of experiments, the evaluation itself stays too focused on final accuracy scores. Since the stated goal is to examine reasoning styles, it would have been far more informative to analyze how reasoning unfolds within each model rather than just whether the final answer is correct. Models that almost solve a problem but make a small arithmetic or logical slip are treated the same as those that fail completely, which makes it hard to assess genuine reasoning ability. A deeper look at intermediate steps, reasoning consistency, or partial correctness would make the results more meaningful.\nConceptually, the work feels more like an extensive benchmarking report than a research contribution. It does not introduce a new method or framework, and most of the findings reaffirm things we already know: larger models tend to do better, search-based strategies are more effective at scale, and shorter reasoning traces save tokens. These observations are valid but not surprising, and they do not move the discussion forward in a substantial way.\nFinally, the error analysis is descriptive but shallow. It relies on isolated examples instead of systematic categorization or statistical evidence. Without a clearer structure for understanding failure types or cross-style comparisons, the takeaways remain vague. Overall, the study is thorough and careful in execution, but it stops short of offering new conceptual insights or analytical depth."}, "questions": {"value": "One area that could have strengthened the paper is the experiment on automatic selection of reasoning styles, which is mentioned briefly in Section 5.2 and discussed further in the appendix. This idea has strong potential for exploring adaptive or meta-reasoning, yet it is presented as a failed attempt rather than a learning opportunity. The authors could have analyzed the reasons for its failure or explored alternative approaches such as reinforcement learning based approaches."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nOJquan1hx", "forum": "DOIWg7VyYU", "replyto": "DOIWg7VyYU", "signatures": ["ICLR.cc/2026/Conference/Submission6363/Reviewer_SyPN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6363/Reviewer_SyPN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756517669, "cdate": 1761756517669, "tmdate": 1762918653449, "mdate": 1762918653449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces StyleBench, a benchmark for comparing five reasoning styles: (1) Chain-of-Thought, (2) Tree-of-Thought, (3) Algorithm-of-Thought, (4) Sketch-of-Thought, and (5) Chain-of-Draft, across five task families and 15 open-source models with enough variance in the model size. The study standardizes prompting at temperature 0 with one sample per item, evaluates accuracy and token usage, and analyzes failure modes such as instruction non-compliance and premature guessing in small models. \n\nThe key empirical claims are: (1) no style dominates universally, (2) search-based styles help on open-ended problems given sufficiently large models, (3) concise styles can retain accuracy while saving tokens on structured tasks, and (4) meta-selection via standard supervised fine-tuning collapses to shallow preferences rather than genuine strategy selection."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The breadth and systemization: a single framework directly comparing multiple, popular “thinking styles” across tasks and scales, with an emphasis on computational efficiency and behavior analysis. This consolidates a scattered area and will likely be practically useful for both researchers and applied teams who need guidance on style choice. The conceptual message that style–task–scale interactions are strong is not, in itself, surprising. However, the paper contributes careful documentation, scaling patterns, and clear qualitative analyses. The overall significance is solid in practice, and conceptually, it is an incremental work, but timely.\n\nThe evaluation design is mostly careful and transparent, with fixed temperature, single-sample runs, and uniform max-new-tokens per dataset. The token-usage accounting and formatting analysis are welcome additions that many evaluations overlook. The paper reads clearly. The structure is logical, the figures are informative, and the claims are tied to evidence. The case studies are especially helpful for making the failure-mode story concrete. The intent for reproducibility is strong: the repository is public, prompts are provided, and dataset choices are standard."}, "weaknesses": {"value": "The one threat to the validity is the reliance on a single deterministic sample, which undercuts claims about search-based methods that are known to benefit from sampling and selection, and potential confounding due to differences in how branching search (ToT/AoT) is instantiated and budgeted across models. The paper shows token histograms but does not fully equalize or bound search depth, branching width, or controller heuristics across styles. As a result, some conclusions about relative efficiency and effectiveness may be sensitive to controller design and best-of-N effects rather than intrinsic style differences. Statistical reliability is also limited: with temperature 0 and one sample, there is no variance estimation, and the paper does not report confidence intervals or sensitivity to minor prompt variants"}, "questions": {"value": "1. Please specify, for ToT and AoT, the exact controller algorithms, branching widths, pruning or backtracking policies, and depth limits, and clarify whether these are held constant across models and tasks.\n2. Please justify the choice of a single deterministic sample for search-based styles and, if feasible, add a small sensitivity study that trades off samples against accuracy and tokens to map a compute–performance frontier. \n3. Please discuss potential training contamination for datasets like GSM8K, CommonsenseQA, and LogiQA, including any heuristics or audits used to mitigate it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kaxQD8ybUS", "forum": "DOIWg7VyYU", "replyto": "DOIWg7VyYU", "signatures": ["ICLR.cc/2026/Conference/Submission6363/Reviewer_MEtP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6363/Reviewer_MEtP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761760361099, "cdate": 1761760361099, "tmdate": 1762918652750, "mdate": 1762918652750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces StyleBench, a benchmark designed to evaluate various “reasoning styles” of large language models (LLMs), including Chain-of-Thought (CoT), Tree-of-Thought (ToT), Algorithm-of-Thought (AoT), Sketch-of-Thought (SoT), and Chain-of-Draft (CoD). The authors conduct experiments on 15 open-source models across five reasoning tasks (mathematical, logical, commonsense, and puzzle solving), aiming to reveal interactions between reasoning style, model scale, and task type.\n\nWhile the benchmark scope is broad and the results are clearly presented, the overall novelty, methodological rigor, and analytical depth are limited. The study largely repackages existing reasoning paradigms under a single evaluation framework, without introducing new algorithms, theoretical insights, or robust statistical analysis to justify its conclusions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The benchmark spans a wide range of LLM architectures (LLaMA, Qwen, Mistral, Gemma, etc.), providing a useful dataset of comparative performance across styles and model scales.\n2. The paper is well-structured and readable, with intuitive figures (e.g., scaling trends, token-usage plots) that make the results accessible.\n3. The authors provide an anonymous open-source link, specify deterministic evaluation settings (temperature = 0), and clearly describe model and dataset selections."}, "weaknesses": {"value": "1. Limited novelty: The paper does not introduce a new reasoning method, dataset, or evaluation metric. The idea of comparing prompting styles such as CoT/ToT has already been explored in prior works [1][2][3]. StyleBench appears to be a collection of existing benchmarks rather than a genuinely new benchmark design.\n2. The findings (e.g., “no single style is optimal,” “larger models follow instructions better”) are unsurprising and already well-known. The attempt to fine-tune models for meta-reasoning is poorly motivated and under-explained; it lacks baseline comparisons and quantitative metrics.\n3. There is no clear control for prompt length, token budget, or semantic equivalence, making cross-style comparisons unreliable. There is no evidence that prompts across styles are equally capable of eliciting correct reasoning chains, potentially biasing the results.\n4. The benchmark focuses on a narrow set of logical/mathematical reasoning tasks. Exclusion of domains like code generation or long-context reasoning limits the generality of the findings.\n5. The paper positions itself as an evaluation study rather than a contribution to machine learning methodology or theory. Without novel learning algorithms, model training, or theoretical analysis, it reads more like a large-scale empirical report than an ICLR-level research contribution.\n\n[1] Yao S, Yu D, Zhao J, et al. Tree of thoughts: Deliberate problem solving with large language models[J]. Advances in neural information processing systems, 2023, 36: 11809-11822.\n[2] Gao P, Xie A, Mao S, et al. Meta reasoning for large language models[J]. arXiv preprint arXiv:2406.11698, 2024.\n[3] Fang G, Ma X, Wang X. Thinkless: Llm learns when to think[J]. arXiv preprint arXiv:2505.13379, 2025."}, "questions": {"value": "At line 393, the authors mentioned that they have fine-tuned a qwen-7b model, which failed to solve the challenge of reasoning path selection.\nWhat's the authors' opinions on a possible future solution? \nIs it possible to enable LLMs with this high-level capability via SFT (or RL)?\nDiscussions are welcome."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "arIcU2vl0p", "forum": "DOIWg7VyYU", "replyto": "DOIWg7VyYU", "signatures": ["ICLR.cc/2026/Conference/Submission6363/Reviewer_yhfA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6363/Reviewer_yhfA"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976922321, "cdate": 1761976922321, "tmdate": 1762918651961, "mdate": 1762918651961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}