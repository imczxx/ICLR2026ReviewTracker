{"id": "uGvZ6DGPR2", "number": 12810, "cdate": 1758210482129, "mdate": 1759897483338, "content": {"title": "Likelihood Matching for Diffusion Models", "abstract": "We propose a Likelihood Matching approach for training diffusion models by first establishing an equivalence between the likelihood of the target data distribution and a likelihood along the sample path of the reverse diffusion. To efficiently compute the reverse sample likelihood, the equivalence, a quasi-likelihood is considered to approximate each reverse transition density by a Gaussian distribution with matched conditional mean and covariance, respectively. The score and Hessian functions for the diffusion generation are estimated by maximizing the quasi-likelihood, ensuring a consistent matching of both the first two transition moments between every two time points. A stochastic sampler is introduced to facilitate the computation that leverages on both the estimated score and Hessian information. We establish consistency of the quasi-maximum likelihood estimation, and provide non-asymptotic convergence guarantees for the proposed sampler, quantifying the rates of the approximation errors due to score and Hessian estimation, dimensionality, and the number of diffusion steps. Empirical and simulation evaluations demonstrate the effectiveness of the proposed Likelihood Matching and validate the theoretical results.", "tldr": "", "keywords": ["Likelihood Matching", "Generative Models", "Quasi-Maximum Likelihood Estimation", "Score Matching"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10f4c25650de35a230a42212d51a376142ecabca.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a likelihood-based training framework for diffusion models that incorporates both first- and second-order score supervision. By matching the mean and covariance of reverse transitions, the method extends standard score matching. Empirical results show improved performance over vanilla score matching, and theoretical analysis provides non-asymptotic convergence guarantees for the resulting sampler, quantifying the impact of score and Hessian estimation error."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and easy to follow. The theoretical analysis is thorough and well-supported by experiments on both synthetic and real datasets."}, "weaknesses": {"value": "I am very willing to raise my score if the following concerns are addressed:\n\n1. Ambiguity of N: The notation N is sometimes unclear, it's not always evident whether it refers to the total number of sampling steps, the number of selected time steps during training, or something else. Could the authors provide a unified and consistent definition of N throughout the paper?\n\n2. Motivation for Second-Order Information: The motivation for incorporating the Hessian (second-order score) remains insufficiently convincing. Why is second-order information important for diffusion models? How does it improve performance? Rather than only reporting improved metrics, the authors are encouraged to provide deeper empirical or theoretical insights. For example, since the Mixture of Gaussians (MoG) has an analytical score function, comparing oracle trajectories (e.g., deterministic ODE sampling) with score-only trajectories could be highly illustrative. If this is computationally intensive, a compelling empirical explanation would also be acceptable.\n\n3. Low-Rank Approximation Hyperparameter: A minor but relevant limitation is that the low-rank Hessian approximation rank r is a hyperparameter requiring tuning. While not a critical issue, it would be helpful if the authors could offer empirical guidance on how to set or adapt this parameter.\n\n4. Connection to Prior Work: The paper would benefit from a brief discussion on how its covariance modeling relates to prior DDPM variants that also learn covariance matrices, as well as to score matching with weighting schemes. Citing one or two representative works would be sufficient.\n\n5. Time Sampling Strategy: A minor but interesting point for discussion: some prior works suggest that sampling t from a non-uniform, pre-defined distribution can improve performance. In contrast, this paper adopts uniform sampling as dictated by the likelihood objective. Could the authors discuss the pros and cons of these time sampling strategies?"}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5Ci4kGFPk6", "forum": "uGvZ6DGPR2", "replyto": "uGvZ6DGPR2", "signatures": ["ICLR.cc/2026/Conference/Submission12810/Reviewer_1VnS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12810/Reviewer_1VnS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760933227391, "cdate": 1760933227391, "tmdate": 1762923618653, "mdate": 1762923618653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes learning a Gaussian denoising distribution with a learned mean and covariance in a diffusion model, where the covariance is learned using a low-rank approximation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "There are some theoretical analyses that make the paper look okay."}, "weaknesses": {"value": "1. The covariance estimation of the Gaussian denoising diffusion model is a solved problem. I am not sure why this paper didn't discuss any related work on this. I will give a brief introduction to this line of research and show **why learning the covariance under quasi-MLE is unnecessary**. All the papers I list below use a Gaussian variational distribution to approximate the denoising distribution under forward KL divergence, which is the same as the quasi-MLE terminology that this paper mentioned. Unfortunately, none of these papers is cited or mentioned.\n\nFor example:\n1. iDDPM (https://arxiv.org/abs/2102.09672) is the first paper to learn the covariance under ELBO, and showsthat  learning the covariance of a Gaussian can accelerate the denoising sampling.\n2. Analytic-dpm (https://arxiv.org/abs/2201.06503) shows that the optimal state-independent isotropic Gaussian covariance can be analytically derived with the learned mean function.\n3. OCM-DDPM (https://arxiv.org/pdf/2406.10808) shows that the optimal covariance only depends on the learned mean function, which means you can just learn the desnoing mean, and the full covariance can be analytically derived, which means learning the covariance is not necessary. The optimal here is all under the KL divergence.\n\nThere are also NPR-DDPM and SN-DDPM (https://arxiv.org/abs/2206.07309) are all learning the covariance.\n\n\n2. The experiments and comparisons are not convincing. \nThe experiments need to reach the level of one of the papers above. The current experiments are too toy-like and have no comparisons.\n\nI suggest the author do a literature review before starting a research project. The Gaussian approximation of the denoising distribution is almost finished; there are also non-Gaussian approximations of the denoising distribution, e.g. https://arxiv.org/abs/2502.02483."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nHx3elhDSz", "forum": "uGvZ6DGPR2", "replyto": "uGvZ6DGPR2", "signatures": ["ICLR.cc/2026/Conference/Submission12810/Reviewer_uS2u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12810/Reviewer_uS2u"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710720281, "cdate": 1761710720281, "tmdate": 1762923618420, "mdate": 1762923618420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Likelihood Matching approach for training diffusion models by first establishing an equivalence between the likelihood of the target data distribution and the likelihood along the sample path of the reverse diffusion. By assuming that the reverse process is roughly Gaussian for small step sizes, it proposed to parametrize both the mean and the variance of such a Gaussian by parametrizing the score and the Hessian with neural networks. Further, it provides a bound on the total variation between the learned and the true distribution, and proves convergence in probability to the true parameters of the target distribution. The resulting method 'likelihood matching' (LM) is then tested on synthetic and real data."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Overall the paper has some interesting results. It provides non-asymptotic convergence guarantees for the proposed sampler in total variation, characterizing the errors in terms of score and Hessian estimation error, dimension d, and diffusion steps T.\n\nIt theoretically demonstrate the consistency of the proposed quasi-maximum likelihood diffusion training under reverse quasi-likelihood objectives.\n\nMultiple simplifications are made that make this approach implementable in practice, albeit it still remains much more expensive than direct score matching."}, "weaknesses": {"value": "**Main Weaknesses**\n\n*W1* It is unclear why the Hessian is necessary theoretically. The reverse diffusion generates precisely the same distributions as the forward one, and the only unknown term therein is the score. In this sense, the score is a sufficient statistic to go backwards. The Fokker-Planck equation implies the same conclusion, as by formulating the backward probability evolution via an ODE, then the target distribution is perfectly modeled if the score has been perfectly learned. These points are also supported by Theorem 2 in [1], as that theorem implies that the KL between the modeled and target distribution becomes 0, for perfectly learned scores.\n\n*W2* The modeling of the Hessian causes an increase of more than 2x of training time and memory usage (for Cifar). Given the modest improvements in terms of FID, it is not clear why this approach should be adopted. In addition, the compute results are only given for Cifar10. I suspect the 2x gap would increase even further for higher dimensional distributions. Could the authors provide the difference in training/memory time in the case of high-resolution Imagenet?\n\n*W3* As stated in the paper, several previous works do adopt elements from the Hessian, or try to model the variance with an isotropic Gaussian among other approaches. In [2] the reverse step probability is modeled as a mixture of Gaussians. Given that the reverse probabilities are intractable, the approximation with a Gaussian is not fully justified, in particular for larger steps. Does the use of the Hessian give a better estimate for larger step sizes (i.e. reduce the number of steps)?\n\n*W4* The writing quality of the paper should be improved. There are numerous grammatical errors and typographical mistakes throughout. A few examples, among many, include the very first sentence of the introduction, line 204, lines 300–301, and even the text within figures (lines 1248–1250).\n\n[1] Song et al. Maximum Likelihood Training of Score-Based Diffusion Models. Neurips 2021\n\n[2] Guo et al. Gaussian Mixture Solvers for Diffusion Models. Neurips 2023."}, "questions": {"value": "Q1. How was the NLL computed? Using the ODE formulation or through the likelihood lower bound?\n\nQ2. What is the difference in performance when time-steps are predefined (Algorithm 1), vs when they are randomly sampled (Algorithm 2)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y80jspFe3Z", "forum": "uGvZ6DGPR2", "replyto": "uGvZ6DGPR2", "signatures": ["ICLR.cc/2026/Conference/Submission12810/Reviewer_e67M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12810/Reviewer_e67M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864673474, "cdate": 1761864673474, "tmdate": 1762923618117, "mdate": 1762923618117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Likelihood Matching (LM), a training framework for diffusion models that directly approximates the data likelihood via a quasi-likelihood over the reverse diffusion path.\n\nIt models both the score and Hessian to match conditional mean and covariance, leading to a unified objective combining score and covariance matching.\n\nThe authors derive theoretical guarantees (consistency of quasi-MLE, non-asymptotic sampler bound) and show empirical gains on MNIST, CIFAR-10, and CelebA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Conceptually elegant: connects data likelihood with path likelihood of reverse diffusion: an underexplored yet fundamental viewpoint.\n* Introduces a principled QMLE formulation, integrating first- and second-order information beyond prior Hessian-regularized SM methods.\n* Solid theoretical results with practical, scalable implementation (low-rank Hessian, SMW updates).\n* Consistent FID/NLL improvements and faster convergence in sampling."}, "weaknesses": {"value": "* Novelty could be more clearly contrasted with prior MLE-based diffusion ODE works [1,2,3]\n* Experiments remain small-scale.\n\n[1] Song, Yang, et al. \"Maximum likelihood training of score-based diffusion models.\" Advances in neural information processing systems 34 (2021): 1415-1428.\n\n[2] Lu, Cheng, et al. \"Maximum likelihood training for score-based diffusion odes by high order denoising score matching.\" International conference on machine learning. PMLR, 2022.\n\n[3] Zheng, Kaiwen, et al. \"Improved techniques for maximum likelihood estimation for diffusion odes.\" International Conference on Machine Learning. PMLR, 2023."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "V97rxitWUx", "forum": "uGvZ6DGPR2", "replyto": "uGvZ6DGPR2", "signatures": ["ICLR.cc/2026/Conference/Submission12810/Reviewer_juBJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12810/Reviewer_juBJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12810/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762182493479, "cdate": 1762182493479, "tmdate": 1762923617861, "mdate": 1762923617861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}