{"id": "hUtTGobe1r", "number": 10054, "cdate": 1758158897830, "mdate": 1759897677906, "content": {"title": "Decoupling Primitive with Experts: Dynamic Feature Alignment for Compositional Zero-Shot Learning", "abstract": "Compositional Zero-Shot Learning (CZSL) investigates compositional generalization capacity to recognize unknown state-object pairs based on learned primitive concepts. Existing CZSL methods typically derive primitives features through a simple composition-prototype mapping, which is suboptimal for a set of individuals that can be divided into distinct semantic subsets. \nMoreover, the one-to-all cross-modal primitives matching neglects compositional divergence within identical states or objects, limiting fine-grained image-composition alignment. In this study, we propose EVA, a Mixture-of-Experts Framework for Semantic Variant Alignment. Specifically, we introduce domain-expert adaption, leveraging multiple experts to achieve token-aware learning and model high-quality primitive representations. To enable accurate compositional generalization, we further present semantic variant alignment to select semantically relevant representation for image-primitives matching. \nOur method significantly outperforms other state-of-the-art CZSL methods on three popular benchmarks in both closed- and open-world settings, demonstrating the efficacy of the proposed insight.", "tldr": "", "keywords": ["Compositional zero-shot learning; Multi-modal learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2399d035443fe1c90ea0f3e4fa4b7285726fb9bd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the Compositional Zero-Shot Learning (CZSL) problem, which aims to recognize unseen combinations of state and object primitives.\nExisting methods usually assume a single static prototype for each primitive (e.g., old, young), ignoring the semantic polysemy that arises across different contexts (e.g., old man vs. old book). This leads to semantic entanglement and limited generalization.\nTo address this, the authors propose EVA (Expert-based Variant Alignment), a Mixture-of-Experts framework that explicitly models semantic variability at the primitive level.\nIt consists of two main components:\n(1) Domain-Expert Adaption, which inserts lightweight LoRA-based MoE adapters into a frozen CLIP backbone to dynamically route tokens to specialized experts, enabling context-aware primitive representations; and\n(2) Semantic Variant Alignment, which selects semantically relevant feature variants from both text and image perspectives to achieve fine-grained cross-modal matching.\nExperiments on MIT-States, UT-Zappos, and C-GQA datasets demonstrate that EVA consistently outperforms state-of-the-art methods in both closed- and open-world settings, achieving up to +3.5% (closed-world) and +2.2% (open-world) AUC improvements while remaining efficient and interpretable."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation addressing primitive polysemy in CZSL.\n2. Strong empirical results across three benchmarks.\n3. Lightweight and interpretable design built on frozen CLIP."}, "weaknesses": {"value": "1. The paper lacks a deeper analysis of why MoE improves compositional classification — explanations remain intuitive without quantitative evidence of expert complementarity or routing diversity.\n2. The observed gain may stem from implicit feature reparameterization rather than genuine semantic disentanglement.\n3. No study of routing stability or expert utilization balance; potential risk of expert collapse."}, "questions": {"value": "1.\tAre the experts activated evenly during training, or does routing collapse occur?\n2.\tCan the authors provide quantitative or visual evidence showing that different experts learn complementary semantics?\n3.\tWould EVA still work without explicit state/object labels in open-vocabulary or unsupervised settings?\n4.\tHow consistent are the improvements across different vision–language backbones (e.g., BLIP, SigLIP)?\n5.\tDoes the MoE module truly enhance semantic reasoning, or mainly increase representational capacity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y3AnxCNcmJ", "forum": "hUtTGobe1r", "replyto": "hUtTGobe1r", "signatures": ["ICLR.cc/2026/Conference/Submission10054/Reviewer_nDeF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10054/Reviewer_nDeF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761530188277, "cdate": 1761530188277, "tmdate": 1762921451872, "mdate": 1762921451872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of primitive polysemy in Compositional Zero-Shot Learning (CZSL) and proposes an expert-based dynamic feature decoupling framework called EVA (Expert-based Variant Alignment). The authors point out that primitives can have different meanings under different semantic contexts (e.g., “old” in “old man” vs. “old building”), which are difficult to model using a single embedding. As a result, traditional attribute–object composition approaches cannot fully capture semantic variations.\n\nEVA mainly consists of two core modules:\n\n1.\tDomain Expert Adaptation: Inserts a Mixture-of-Experts (MoE) module into the CLIP encoder. Through a token-level routing mechanism, it selects the appropriate expert to model semantic variations across contexts.\n\n2.\tSemantic Variant Alignment: Designs a cross-modal dynamic alignment strategy that combines global and local feature matching to achieve fine-grained consistency between visual and textual semantic spaces.\n\nExperiments on standard CZSL datasets — MIT-States, UT-Zappos, and C-GQA — show that EVA outperforms several recent methods (e.g., GIPCOL, Troika, IVLP) under both closed-set and open-set settings. The authors claim EVA achieves more stable semantic generalization and better interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper proposes a logically coherent and complete MoE + alignment framework, effectively integrating expert modeling with semantic disentanglement.\n\n2.\tExperiments cover multiple mainstream CZSL datasets, and the reported performance exceeds several existing methods on certain metrics, showing credible results."}, "weaknesses": {"value": "1. The work is essentially an architectural extension of existing CLIP-based compositional alignment methods.\n2. The core mechanisms (expert routing and alignment strategy) lack quantitative validation.\n3. The paper does not analyze computational cost, parameter scale, or inference latency, making it difficult to assess practicality.\n4. Some figures and text repeat information, reducing clarity.\n5. Differences between variants are small, making it unclear how much each module contributes."}, "questions": {"value": "1. Does the expert module experience imbalance during training? Have you introduced a load balancing loss?\n\n2. What is the performance drop if the alignment module is removed or only the MoE structure is retained?\n\n3. Does the EVA framework rely on CLIP’s pretrained semantic structure? Would it still work with non-CLIP backbones?\n\n4. Can you provide more interpretability results (e.g., clustering distributions of experts across semantic attributes) to demonstrate the effectiveness of semantic decoupling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jn7eRsq7cO", "forum": "hUtTGobe1r", "replyto": "hUtTGobe1r", "signatures": ["ICLR.cc/2026/Conference/Submission10054/Reviewer_Fu3k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10054/Reviewer_Fu3k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789131298, "cdate": 1761789131298, "tmdate": 1762921451336, "mdate": 1762921451336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses Compositional Zero-Shot Learning (CZSL) focusing on the challenge that primitive features (states, objects) vary semantically across contexts. The authors propose EVA (Expert-based Variant Alignment), a MOE framework that learns context-aware primitive representations and performs fine-grained cross-modal alignment with carefully designed traning objective. EVA introduces two key components:(1) Domain-Expert Adaption, which employs MoE adapters in image and text encoders to capture diverse semantic facets of primitives through dynamic token routing; and(2) Semantic Variant Alignment (SVA), which aligns fine-grained visual and textual variants via both text-to-image and image-to-text matching. Comprehensive quantitative and qualitative experiments on the three most commonly used CZSL datasets  demonstrate that EVA achieves superior performance  in both closed- and open-world settings. The analyses further show that different experts specialize in distinct semantic aspects, validating the effectiveness of the proposed design."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces an MoE-based framework to address the semantically heterogeneous of primitives in CZSL. By dynamically routing tokens to domain-specific experts, the proposed approach effectively captures context-dependent primitive semantics.The proposed model achieves SOTA performance in both closed-world and open-world settings without any suffix modules.\n\n\n2. The proposed SVA module overcomes the limitation of previous all-to-one alignment  by introducing a global-to-local cross-modal alignment from both image and text perspectives. This design enables more accurate and context-sensitive alignment of visual and textual primitives, leading to better compositional discrimination. \n\n3. The paper is well-structured and easy to follow, with thorough quantitative and qualitative evaluations."}, "weaknesses": {"value": "1.  Figure 2(c) does not clearly illustrate how the SVA module works, and Figure 2(a) lacks explicit description for the text-to-image alignment pathway $\\mathcal{L}_s^v ,\\mathcal{L}_o^v$.\n2. The paper does not include an ablation study on the number of experts.\n3. The paper lacks deeper analysis of expert specialization, such as overall statistical distributions or further exploration of text experts."}, "questions": {"value": "1. Figure 7 shows that the token load across text experts is highly unbalanced. Do these experts correspond to particular semantic groups or patterns? Could you provide further analysis, and would introducing a load-balancing loss help?\n\n2. In the SVA module, the image-to-text alignment require computing affinities of experts. How is this objective applied or adapted to the baselines reported in Table 2?\n\n3. Table 8 reports efficiency analysis on UT-Zappos, but this dataset has relatively few state and object categories. Since the efficiency is sensitive to the number of primitives, could you report or discuss results on other datasets and under open-world settings?\n\n4. Could you provide a more detailed analysis of the vision experts? For example, showing how the same expert allocates patches across different images?\n\n5. It would be valuable to include a more detailed investigation of text experts. For instance, analyzing the impact of using separate experts for states, objects, and compositions.\n\n6. Which dataset was used for the qualitative analyses in Figures 7–8?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "k6ocsbJ936", "forum": "hUtTGobe1r", "replyto": "hUtTGobe1r", "signatures": ["ICLR.cc/2026/Conference/Submission10054/Reviewer_BePU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10054/Reviewer_BePU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907937800, "cdate": 1761907937800, "tmdate": 1762921451006, "mdate": 1762921451006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a mixture-of-experts framework for semantic variant alignment (EVA) to solve compositional zero-shot learning. The proposed method introduces two main modules: domain-expert adaptation and semantic variant alignment. The extensive experiments on three benchmarks demonstrate significant improvements over previous SOTA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) This paper introduces MoE paradigm to model the heterogeneous nature of primitive concepts.\n2) The experimental results are promising."}, "weaknesses": {"value": "1) The novelty is incremental. Specifically, many CZSL methods enable the primitive embeddings to dynamically adapt to diverse semantic variants by fine-grained learning[3], clustering-based prototypes[1], distribution learning[2] et al. The introduction of MoE to facilitate the adaptation of features is sound but this is merely a simple application without promoting the development of CZSL.\n2) Some parts of the paper are difficult to understand. Terms like \"domain-expert adaption,\" “in-domain knowledge”, \"variant-based method\" are used with little prior explanation. \n2) The inclusion of a balancing coefficient $\\alpha$ is also empirically motivated rather than theoretically justified, and it is unclear how sensitive results are to this choice.\n3) In lines 258-269, the feature variants V is not described clearly, and the selection of state/object image features from feature variants lacks rigorous mathematical support or analysis. In addition, A_S \\in R^{N_e+1}, why i\\in [0,N_e] in equation (11)? \n4) In lines 267-268, is {v_i}_{i=0}^{N_e} feature variants or semantic variants?\n5) In lines 262-263, why is A_v computed based on V and image features f_c, while A_s and A_o are computed based on V and text features t_s/t_o?\n[1] LEARNING CLUSTERING-BASED PROTOTYPES FOR COMPOSITIONAL ZERO-SHOT LEARNING\n[2] Prompting Language-Informed Distribution for Compositional Zero-Shot Learning\n[3] Leveraging sub-class discimination for compositional zero-shot learning"}, "questions": {"value": "Addressing weaknesses and improving writing quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LRd64FzR3s", "forum": "hUtTGobe1r", "replyto": "hUtTGobe1r", "signatures": ["ICLR.cc/2026/Conference/Submission10054/Reviewer_8hsj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10054/Reviewer_8hsj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005950435, "cdate": 1762005950435, "tmdate": 1762921450377, "mdate": 1762921450377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}