{"id": "5VXHGJiQuW", "number": 15047, "cdate": 1758247169977, "mdate": 1759897333398, "content": {"title": "Learning Personalized Driving Styles via Reinforcement Learning from Human Feedback", "abstract": "Generating human-like and adaptive trajectories is essential for autonomous driving in dynamic environments. While generative models have shown promise in synthesizing feasible trajectories, they often fail to capture the nuanced variability of personalized driving styles due to dataset biases and distributional shifts. To address this, we introduce \\systemname, a human feedback-driven finetuning framework for generative trajectory models, designed to align motion planning with diverse driving styles. \\systemname\\ incorporates multi-conditional denoiser and reinforcement learning with human feedback to refine multi-modal trajectory generation beyond conventional imitation learning. This enables better alignment with human driving preferences while maintaining safety and feasibility constraints. \\systemname\\ achieves performance comparable to the state-of-the-art on NavSim benchmark. \\systemname\\ sets a new paradigm for personalized and adaptable trajectory generation in autonomous driving.", "tldr": "TrajHF finetunes generative trajectory models with human feedback to enable personalized trajectory planning and is comparable to state-of-the-art results on NavSim benchmark.", "keywords": ["human driving preference", "generative model", "RLHF"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e5c8e47b64a0e7ba8a7cadaaa102883a19bbe151.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "* This paper introduces TrajHF, a framework for finetuning generative trajectory models in autonomous driving motion planning to align with personalized driving styles.\n* The approach addresses a key limitation of standard imitation learning: Learning a multi-model distribution of human preferences instead of an average.\n* The authors first train a DDPM using a Multi-Conditional Denoiser architecture that processes multi-modal sensor inputs (camera and LiDAR).\n* This base model is then finetuned with RLHF. For that, a reward model is trained on a specially curated semi-synthetic dataset of human preferences and used to update the diffusion policy via the DPGRPO algorithm (combination of adapted GRPI and BC loss for regularization).\n* Experimental results show that the approach works well on navtest. Furthermore, results on a newly introduced BOE score show that human evaluators prefer the TrajHF-proposed trajectories of their preferred driving styles."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This paper addresses an interesting problem of adjusting driving styles based on user preferences.\n* The proposed method is sound. It achieves good (but not SOTA) performance on navtest.\n* The evaluation on both navtest and internal data with the newly introduced BOE is thorough and shows good results.\n* The approach is well ablated in the appendix, incl. comparisons to PPO and DPO.\n* The data collection strategy based on takeovers that correspond to critical moments of preference makes sense and is an interesting idea."}, "weaknesses": {"value": "* The method does not achieve SOTA on navtest (although the authors claim “comparable to state-of-the-art” in the Abstract).\n* The reward model is not independently validated. This could be done on a held out test set of human preference pairs.\n* The claim about safe and feasible trajectories is not well supported by experimental results.\n* The split on defensive and aggressive seems somewhat simple (albeit easy to understand) and might not translate well to the real-world driving task (aggressive might be more accident-prone)."}, "questions": {"value": "* Can you add an independent validation of the reward model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ccVNH7Q4DH", "forum": "5VXHGJiQuW", "replyto": "5VXHGJiQuW", "signatures": ["ICLR.cc/2026/Conference/Submission15047/Reviewer_MMQc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15047/Reviewer_MMQc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15047/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761520478907, "cdate": 1761520478907, "tmdate": 1762925370822, "mdate": 1762925370822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TrajHF is a diffusion-based trajectory planner finetuned with human feedback to personalize driving style. It adds a multi-conditional denoiser for images, LiDAR, and action history, then applies preference-based RL alignment (a GRPO-style objective over groups of K sampled trajectories) plus an EM selector to refine multi-modal samples. Empirical performance shows TrajHF improves human-rated style alignment but remains below GoalFlow on NavSim, indicating modest gains in personalization without surpassing public-benchmark baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. A simple critic-free RL recipe on the driving problem that is compute-friendly.\n2. Preference alignment improves style without collapsing feasibility."}, "weaknesses": {"value": "1. Moderate novelty: diffusion-as-MDP and group-relative advantages are adapted from prior work; EM selection echoes earlier trajectory aggregation ideas.\n2. Benchmark performance: Public benchmark underperforms SOTA without the ``selector” upper bound. Key wins rely on internal datasets, semi-synthetic pairs, and human BOE."}, "questions": {"value": "1. After style tuning, what happens to TTC and comfort?\n2. Why is your best deployable PDMS lower than GoalFlow; what ablations explain the gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ATrtcce1eH", "forum": "5VXHGJiQuW", "replyto": "5VXHGJiQuW", "signatures": ["ICLR.cc/2026/Conference/Submission15047/Reviewer_KBTc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15047/Reviewer_KBTc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15047/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945366961, "cdate": 1761945366961, "tmdate": 1762925370370, "mdate": 1762925370370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **TrajHF**, a framework for learning **personalized driving styles** in autonomous driving by combining diffusion-based trajectory generation with RLHF. The method extends a DDPM-based diffusion policy with a Multi-Conditional Denoiser (MCD) Transformer that conditions on multi-modal inputs (camera, LiDAR, and past actions). To align the generated trajectories with diverse human preferences (e.g., “aggressive” vs. “defensive” styles), the authors introduce a DPGRPO algorithm for diffusion finetuning using human feedback. Experiments are conducted on the public NavSim benchmark and internal preference datasets.\n\nOverall, the paper is well-written and presents a well-engineered system addressing preference-aligned autonomous driving. However, results on public benchmarks show **parity rather than improvement** over prior methods, and key findings on personalization rely heavily on private internal datasets without comparison to other baselines, making the evidence for the claimed advantages **less convincing**."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper tackles an interesting and practical problem, **personalization of driving trajectories**, by leveraging RLHF within a diffusion policy framework. \n* The anchor-free trajectory generation and multi-conditional denoiser are technically elegant designs that remove limitations of anchor- or vocabulary-based approaches in prior work.\n* The work contributes to a growing line of research connecting generative modeling, imitation learning, and human preference alignment, which is a significant direction for building human-trustworthy robotic systems.\n* The construction of an **internal dataset** focused on driving style variation (aggressive v.s. defensive) is valuable and demonstrates substantial engineering effort, especially in developing the human preference evaluation framework."}, "weaknesses": {"value": "1. **Experimental results are not sufficiently strong to validate the main claims.**\n   On the NavSim benchmark, TrajHF (EM) achieves 87.6 PDMS, comparable to Hydra-MDP (86.5) and DiffusionDrive (88.1), but below GoalFlow (90.3). These results indicate that TrajHF performs competitively but is **not comparable to the state-of-the-art** methods, contrary to the paper’s claim.\n\n2. **Personalization results rely entirely on private data.**\n   The main contribution, personalized trajectory generation, is verified solely through an internal dataset that is unavailable for public evaluation. This raises questions about the quality of driving data, annotation consistency, and label balance. The authors also acknowledge that standard metrics such as ADE and FDE do not capture behavioral styles, yet these metrics are still heavily used for evaluation, which weakens the argument. The proposed BOE metric for human evaluation is interesting but subjective and lacks statistical rigor (e.g., variance, confidence intervals, significance testing).\n\n3. **Lack of safety and generalization analysis.**\n   Personalization could introduce safety-critical behavior (e.g., overly aggressive trajectories), but the paper does not evaluate whether the finetuned policy maintains safety or robustness under distribution shifts. Metrics such as collision rate or rule compliance are not reported.\n\n4. **Minor presentation and mathematical issues.**\n   Some typographical and mathematical inconsistencies should be corrected:\n\n   * Line 53: “Multi-Conditioned Denoiser (MDC)” should be “(MCD)”.\n   * Line 215: If timestep $l = 1$, the projection $ \\hat{x}_1 = s_1 - s_0 $, but Equation (1) defines the state starting from $s_1$, causing an inconsistency in the definition."}, "questions": {"value": "1. **PDMS selector setup:**\n   The setup of the PDMS selector variant is unclear. Please clarify how it differs from the single-sample and EM variants of TrajHF, and what assumptions or oracle information it uses.\n\n2. **Dataset transparency and annotation quality:**\n   Given that most results rely on private internal datasets, could the authors consider releasing them for further evaluation?\n   How are the “aggressive” and “defensive” ground truths defined and ensured to be feasible and meaningful?\n   Are there any statistics on inter-annotator agreement or annotation consistency?\n\n3. **Algorithmic contribution of DPGRPO:**\n   How exactly does DPGRPO differ from existing GRPO or DPO implementations?\n   Is there measurable improvement in stability, sample efficiency, or alignment quality attributable to this modification?\n   A quantitative comparison or ablation isolating DPGRPO’s contribution would strengthen the paper.\n\n4. **Evaluation of personalization on NavSim:**\n   Since NavSim primarily evaluates feasibility and comfort rather than driving style, would a comparison of preference-conditioned v.s. non-conditioned models on NavSim help demonstrate alignment improvements?\n   Additionally, can the authors provide results of other state-of-the-art methods on the internal preference datasets for a fairer comparison?\n\n5. **Safety considerations:**\n   How do the authors ensure that finetuning toward “aggressive” preferences does not violate safety constraints or produce unsafe behavior?\n   Are there empirical checks, such as collision rates or safety rule compliance?\n\n6. **Details on PPO and DPO variants:**\n   In Appendix C.1, the paper reports results using PPO and DPO variants. Could the authors provide more implementation details on the PPO setup, specifically, the design of the critic network and reward model within that framework?\n\n7. **Demonstration of behavior:**\n   Could the authors provide a video demonstration illustrating the “aggressive” and “defensive” driving behaviors? Static visualizations (e.g., Figure 4) are insufficient to assess feasibility or collision risk."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hW941uKiJf", "forum": "5VXHGJiQuW", "replyto": "5VXHGJiQuW", "signatures": ["ICLR.cc/2026/Conference/Submission15047/Reviewer_ny2q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15047/Reviewer_ny2q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15047/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994577924, "cdate": 1761994577924, "tmdate": 1762925370021, "mdate": 1762925370021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}