{"id": "DwWorqSjwv", "number": 9313, "cdate": 1758118505126, "mdate": 1759897731956, "content": {"title": "AdaGrad Converges in a Robust Sense: Almost Sure Last-Iterate Rates under Any Stopping Time", "abstract": "AdaGrad has become a widely used algorithm for training deep models. Recently, the study of almost sure last-iterate convergence rates in stochastic optimization has attracted increasing attention, as it provides the guarantee of stability and **robustness** for arbitrary single trajectory. While such results are well understood for stochastic gradient descent (SGD), the corresponding analysis for AdaGrad remains limited. In this paper, we establish **almost sure** convergence rates of AdaGrad for the **last-iterate** in the (strongly) convex setting and for the best-iterate in the non-convex setting, both valid under **arbitrary** stopping times and with a flexible dependence on gradient history.", "tldr": "", "keywords": ["Robust Convergence", "AdaGrad", "Last-iterate", "almost sure", "any stopping time"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f33f58aee5708b044ae565e50a0be1e1c2166025.pdf", "supplementary_material": "/attachment/152e10a42378705e99c448999da3b531152a541b.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the almost sure convergence of the AdaGrad-Norm-Type stepsize. Under certain assumptions, the authors prove the last-iterate convergence for (strongly) convex functions and best-iterate convergence for non-convex functions. All of the results hold for any time."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is written clearly."}, "weaknesses": {"value": "**Main Points**\n\n1. First of all, the most critical issue is that none of the convergence results is proved for the original AdaGrad (or even AdaGrad-Norm).\n\n    a) The Type I stepsize can never recover AdaGrad-Norm.\n\n    b) For the Type II stepsize, none of the results allows $\\epsilon=0$.\n\n    Therefore, no results in this work can be applied to AdaGrad (or AdaGrad-Norm) directly.\n\n1. The assumptions considered in the work are strong (in particular, for Assumption 2.2) and not even compatible. More concretely:\n\n    a) Assumption 2.2 is a strong condition in general, especially under the smoothness condition (Assumption 1). Note that these two assumptions cannot hold together even for quadratic optimization on $\\mathbb{R}^d$, since assumption 2.2 implies the true gradient is bounded.\n\n    b) More fundamentally, it is known that Assumption 2.2 (in fact, its weaker version, i.e., $\\nabla f(x)$ is bounded) and strong convexity cannot hold together on $\\mathbb{R}^d$ (see, e.g., [1]). Therefore, Theorems 3.3 and 3.7 cannot be recognized as meaningful results.\n\n    c) One of the key contributions in many prior works for AdaGrad/AdaGrad-Norm (or even more general adaptive gradient optimizers like Adam) is to prove the convergence without assuming bounded stochastic gradients (e.g., see [2, 3]). However, this study imposes it again.\n\n    d) From a technical view, Assumption 2.2 can also reduce many difficulties in the proof.\n\n1. For theoretical results and their proofs, there are also many inaccurate/incorrect places.\n\n    a) The proof of Proposition 3.1 also seems incorrect. I don't see the point of $\\sum_{t=1}^\\infty\\mathbb{E}\\_t[\\cdot]=\\mathbb{E}\\_t[\\sum_{t=1}^\\infty\\cdot]$. If the authors keep the summation symbol outside. Then the current argument fails in many places. The same issue also appears in the proof of Proposition 3.11.\n\n    b) Line 844, it should be up to $\\xi_{t-1}$.\n\n    c) In the proof, the term $a$ in the stepsize is missed in different places (e.g., inequality (30)).\n\n    d) Line 1115, I cannot see why the equation holds.\n\n1. For experiments, some improvements can also be made.\n\n    a) The experiments are for stochastic optimization. Did the authors run multiple trials? If so, please report the number of trials and plot the error bar in the figure. Otherwise, the experiments are less convincing.\n\n    b) Did the authors also run the Type-I stepsize? If yes, please also report the results. If not, then it confuses me why the paper studies it.\n\n    c) The authors also set $\\epsilon$ in the formulation of stepsize, but I cannot find it anywhere in the experiments section. What value of $\\epsilon$ do the authors use? If the authors simply set it to $0$, does this mean introducing $\\epsilon$ is only for the theoretical purpose?\n\n    d) What is the value of $a$ in the experiments? Did the authors also tune it?\n\n    e) The value of $b$ is also confusing. To the best of my knowledge, $b$ is usually a very small number in default, e.g., $10^{-10}$ to improve numerical stability. However, the value of $b$ here is kind of large. Did the authors tune it?\n\n    f) In the caption of Figure 1, the authors wrote AdaGrad. Does this mean the authors in fact run AdaGrad instead of AdaGrad-Norm? Or it is just a typo.\n\n**Other Points**\n\n1. Line 245, there should be \"(Robbins &Monro,1951)\" for the citation (i.e., use \\citep).\n\n1. Line 256, missing a space after \"... high probability.\".\n\n1. Line 275, if the authors don't formally define $g_t\\equiv\\nabla F(x_t,\\xi_t)$, then I cannot see the purpose of requiring $f(x)\\equiv\\mathbb{E}[F(x,\\xi)]$.\n\n1. In Lemma B.3, the subscripts $t$ and $n$ should not both be used. Please fix this issue.\n\n1. In Lemma B.5, it should be $\\sum_{n}^\\infty$.\n\n1. In Corollary D.1 and its proof, many points can be further improved:\n\n    a) The authors use both $f^\\*$ and $f(x^*)$. I suggest keeping only $f^\\*$ to make the notation clearer.\n\n    b) $\\sum_{i=1}^tx_t$ should be $\\sum_{i=1}^tx_i$.\n\n    c) The authors should use either $P[]$ or $P()$, but not both. In addition, the notation $P$ is not consistent with $\\mathbb{P}$ in Lemma B.5.\n\n    d) The statement about $t_0$ is also ambiguous, as $t_0$ should depend on the sample path, meaning that it may not be a uniform value.\n\n**References**\n\n[1] Nguyen, Lam, et al. \"SGD and Hogwild! convergence without the bounded gradients assumption.\" International Conference on Machine Learning. PMLR, 2018.\n\n[2] Faw, Matthew, et al. \"The power of adaptivity in sgd: Self-tuning step sizes with unbounded gradients and affine variance.\" Conference on Learning Theory. PMLR, 2022.\n\n[3] Wang, Bohan, et al. \"Convergence of adagrad for non-convex objectives: Simple proofs and relaxed assumptions.\" The Thirty Sixth Annual Conference on Learning Theory. PMLR, 2023."}, "questions": {"value": "Please refer to **Weaknesses** above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "26ldhgKWrf", "forum": "DwWorqSjwv", "replyto": "DwWorqSjwv", "signatures": ["ICLR.cc/2026/Conference/Submission9313/Reviewer_CVdB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9313/Reviewer_CVdB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760593058662, "cdate": 1760593058662, "tmdate": 1762920948599, "mdate": 1762920948599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the convergence of the step-size version of ADAGRAD for convex, strongly-convex and non-convex settings. It focuses on the almost-sure convergence of either function values, gradients or iterates (depending on the assumptions on $f$) in terms of so-called \"last iterate\". That is for example studying $f(x_t)$ and not $f(\\bar{x}_t)$ where $\\bar{x}_t$ is a weighted-average of past iterates. Previous work on this problem either gets convergence with high probability $1-\\delta$ (where a.s. convergence is $\\delta=0$), or convergence for averaged iterates. I share the motivation given by the author that last-iterate convergence gives a theoretical result closer to practice, therefore the problem studied is important given how often used Adagrad is in machine learning.\nUnfortunately, the author do not study the usual step-size version of Adagrad, but do a slight modification by replacing the square exponent by a tunable parameter $\\gamma\\in (0,2)$. While this may seem harmless at first sight, the choices of $\\gamma$ for which the results derived are satisfactory are those that almost anihilate the \"Adagrad\" style of the step-size and the method almost boils down to SGD (see below more details in the Weakness section). In short, the problem studied is interesting, but it seems to me that the paper achieves much less than what is claimed in the title, abstract and introduction. In the end, I do not think the results are strong enough for publication in a major conference."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The problem studied is of interest to the machine learning community.\n* The theoretical study is thorough, it covers the main cases of differentiable optimization: strongly-convex, convex and non-convex settings (although I do not think the results are very meaningful).\n* Prior work is clearly discussed and cited."}, "weaknesses": {"value": "Below I first list the three main issues with the paper (in my opinion). The rest are more minor concerns that can be fixed rather easily.\n1. I think the title and abstract of the paper are really misleading, I understand this is likely not intentional but I encourage the authors to correct it. Indeed, it is only through a footnote on page 2 that we understand that this paper does not study the standard Adagrad but step-size Adagrad (or Norm-Adagrad). Later we realize that this is actually not exactly step-size Adagrad since the parameter $\\gamma$ cannot be taken equal to $2$. Finally at the end the results derived are strong when $\\gamma\\simeq 0$ which makes the method significantly different from Adagrad (it is almost SGD with step-size $1/\\sqrt{t}$, see hereafter).  \n2. To derive the theorems in Section 3, the authors make a uniform boundedness assumption on the gradients $\\Vert g_t\\Vert^2\\leq Q$. While this is a strong assumptions it is indeed common.\nThe problem is that later in Section 3 the theorems contain a condition on $b$ (the constant term in the denominator of the step-size), and this condition depends on $Q$. But if we sumarize, $Q$ is a uniform bound on the stochastics gradients of the trajectory, so it depends on the sequence $(x_t)_t$. But the sequence depends itself on the choice of the step-size, so it depends on $b$. Therefore, $b$ depends on $Q$ which itselfs depends on $b$. That is a circular argument that I believe makes the result less meaninful in practice (because the condition on $b$ can never be checked a priori).\n3. What I believe to be the main issue with the theoretical results is that the rates derived are stated as a function of $\\eta$ or $\\epsilon$: they have either the form  $1/t^{1-\\eta}$ or $1/t^{1/2 - \\epsilon}$. But this hides the fact that $\\eta$ and $\\epsilon$ depend on $\\gamma$, indeed there are conditions on $\\gamma$. For example in Theorem 3.4, the author give two conditions on $\\epsilon$ that boil down to $\\frac{\\gamma}{4} \\leq \\epsilon\\leq \\frac{1}{2}$. So the rate obtained is of order $1/2 -\\epsilon$ which is no better than $1/2 - \\frac{\\gamma}{4}$. \nSo to obtain a rate close to $1/2$ (the same as for SGD), we need $\\gamma \\simeq 0$, but in that case $\\Vert g_t\\Vert^\\gamma \\simeq 1$ (because $\\Vert g_t\\Vert^0=1$), so $\\sum_i^t \\Vert g_i\\Vert^\\gamma\\simeq \\sum_i^t 1 = t$. In other words, **when $\\gamma\\simeq 0$ the algorithm almost boils down to SGD with decreasing step-sizes**, for which those results are known. So the results are meaninful for $\\gamma$ small, but then the FlexAdagrad algorithm proposed is not close to ADAGRAD anymore.\n\n* I think the way the results are presented hides the issues above by giving less explicit conditions on $\\epsilon$ and that this should really be clarified.\n\nMinor concerns:\n* The main interest of adaptive methods is to ease the tuning of the step-size parameter. I am afraid that adding an additional hyper-parameter $\\gamma$ in Adagrad makes the tuning more complicated. Hence I am not sure that it really brings \"flexibility\" to Adagrad as stated\n* In line with what is stated before, the authors discuss \"last iterate convergence\" without properly defining it. Last iterate convergence could be in terms of $x_t$, $f(x_t)$, or $\\nabla f(x_t)$ and only late in the paper we see what types of results are proven.\n* As the authors explain, assumptions 2.2 is key to deduce that the step-sizes $\\eta_t$ are square-summable, which is key for deriving the results. Yet this condition cannot be checked a priori (as properly discussed in the paper). Proving the square-summability is essentially the main difficulty for methods with adaptive (hence random) step-sizes, and thus Assumption 2.2 circumvents the key difficulty when studying Adagrad.  \n* The parameter $\\eta$ in the rates is already used for the step-size $\\eta_t$, I suggest using another letter."}, "questions": {"value": "* Could you please clarify the main issue with the dependence of $\\eta$ and $\\epsilon$ on $\\gamma$ as discussed above, and how $\\gamma$ affect the result?\n* In the experiments of Figure 1, I suggests the author compare to SGD with descreasing step-sizes $1/\\sqrt{k}$. I expect it to give results close to the curve for $\\gamma=0.01$. \n* In Figure 1, it seems that all the train losses are comparable, hence it is not clear that $\\gamma$ has a strong effect. Did the author try other problems?\n* What do the author mean by saying in introduction that there is need to provide \"more flexibility in the implementation of Adagrad\"? Has this been a problem reported before?\n* Page 2: could the author clarify why rates in high-probability only offer finite-time horizon confidence unlike almost sure convergence?\n* Page 3: the authors wrote \"with loss of generality we can take $a=1$. I assume they meant *without*, but I actually do not believe one can take $a=1$ without loss of generality. If so, could they prove it?\n* What does the condition on $b$ gives in practice? Because in vanilla implementations, $b$ is usually very small ($10^{-8}$), is it the case here with the theorems that are stated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3KEx0gk7FS", "forum": "DwWorqSjwv", "replyto": "DwWorqSjwv", "signatures": ["ICLR.cc/2026/Conference/Submission9313/Reviewer_1Afv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9313/Reviewer_1Afv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761673833371, "cdate": 1761673833371, "tmdate": 1762920948204, "mdate": 1762920948204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors study a variant of AdaGrad-Norm where the update exponents are hyperparameters denoted by $\\gamma$ and $\\epsilon$ and analyze its almost-sure convergence rates under strongly convex, convex, and non-convex settings, respectively. Specifically, for the strongly convex case, the authors claim an almost-sure convergence rate of $o(t^{-\\frac{1}{2}+\\frac{\\gamma}{4}})$ for the last iterate. In the convex case, they establish a last-iterate rate of $O(t^{-1/2+\\epsilon})$. Finally, for the non-convex setting, they show a rate of $O(t^{-1/2+\\epsilon})$ in terms of the best iterate."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Most prior works on adaptive methods analyze convergence in expectation or with high probability, which does not guarantee convergence of individual sample trajectories. In contrast, this paper studies almost-sure convergence rates, providing a stronger and more robust guarantee. To the best of my knowledge, such results have only been previously established for SGD and its variants with predetermined diminishing step sizes. For the convex setting, the authors also claim to obtain an almost-sure rate of $O(t^{-\\frac{1}{2}+\\epsilon})$ for the last iterate, which improves upon the looser last-iterate rates known for SGD. However, I have concerns about the correctness of the results, as detailed in the next section."}, "weaknesses": {"value": "- **Correctness of Proposition 3.1.** This is my main concern, since Proposition 3.1 is the cornerstone of the subsequent convergence results. I don't think the inequality in (1) is valid under the stated assumptions. As a counterexample, consider the case where $\\\\|g\\_t\\\\| = Q$ for all $t \\geq 1$. Then the sum in (1) becomes $\\\\sum\\_{t=1}^{\\infty} \\frac{t^{1-\\eta}Q^2}{(b+t \\cdot Q^\\gamma)^{1+\\beta}}$, which,  in the limit $b\\rightarrow 0$, further reduces to $\\sum_{t=1}^{\\infty} {t^{-\\eta-\\beta}Q^{2-\\gamma(1+\\beta)}}$. This series is only finite when $\\eta+ \\beta > 1$, which is stricter than the condition stated in the proposition. Looking at the proof of Proposition 3.1, I believe the error appears in the last step of Case (II). The authors claim that $\\sum_{j=1}^{\\infty} \\frac{j^{1-\\eta} \\\\|\\tilde{g}_j\\\\|^\\gamma}{(b+\\sum\\_{i=1}^j \\\\|\\tilde{g}\\_i\\\\|^\\gamma)^{1+\\beta}} < \\infty$ follows from the result of Case (I). However, the bound proved in Case (I) does not include the extra factor $j^{1-\\eta}$, so the stated implication does not go through. \n\n- **Strength of the convergence results.** Assuming the aforementioned error can be corrected, I find the convergence results to be somewhat weak. First, in the strongly convex setting, the condition in Proposition 3.1 implies that $1-\\eta \\leq \\frac{1}{2} - \\frac{\\gamma}{4}$. Consequently, the convergence rate cannot be better than $o(t^{-\\frac{1}{2}+\\frac{\\gamma}{4}})$, which is slower than the corresponding rate achieved by SGD. Moreover, in both the strongly convex and convex cases, the established rate becomes vacuous when $\\gamma = 2$, which corresponds to the standard AdaGrad, and the best rate is obtained when $\\gamma$ approaches 0, which makes the step size less adaptive and more similar to a predetermined diminishing step size of $\\frac{a}{(b+t)^{\\frac{1}{2}+\\epsilon}}$. Thus, the presented results do not convincingly demonstrate the advantage of AdaGrad over standard SGD."}, "questions": {"value": "I am confused by the authors' claim that the almost sure results hold for any stopping time $t \\geq 1$. In my understanding, the convergence results presented in this paper are asymptotic and are only valid when the number of iterations $t$ is sufficiently large. Therefore, they do not provide a non-asymptotic guarantee that holds uniformly for all $t$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SbXM8hAkRk", "forum": "DwWorqSjwv", "replyto": "DwWorqSjwv", "signatures": ["ICLR.cc/2026/Conference/Submission9313/Reviewer_W1oe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9313/Reviewer_W1oe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944421898, "cdate": 1761944421898, "tmdate": 1762920947819, "mdate": 1762920947819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the **almost sure (a.s.) convergence rates** of **AdaGrad**—one of the most widely used adaptive gradient methods—under both **convex and non-convex settings**, and crucially, for **the last-iterate** rather than the average-iterate.\nThe authors introduce a generalization called **FlexAdaGrad-Norm**, which includes a **flexibility parameter** ( \\gamma \\in [0,2] ) that controls the influence of past gradient norms in the adaptive step size.\n\nThey rigorously establish:\n\n* **Last-iterate a.s. convergence rates** for strongly convex and convex cases.\n* **Best-iterate a.s. convergence rates** for the non-convex case.\n* All results hold **for any random stopping time ( t \\ge 1 )**, strengthening robustness claims.\n\nExperiments on CIFAR-10 using VGG+BN+Dropout empirically validate that **smaller ( \\gamma )** values (e.g., 0.1 or 1) outperform the standard ( \\gamma = 2 ), improving stability and accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Significant Theoretical Contribution**\n\n   * The paper closes an important gap by providing **last-iterate almost sure convergence rates** for AdaGrad, which had not been addressed before.\n   * The theoretical results generalize previous works on SGD (e.g., Liu & Yuan, 2022; Sebbouh et al., 2021) and extend them to **adaptive methods** with **non-trivial step-size dependencies**.\n   * The introduction of **generalized square summability** is an elegant technical innovation that bridges the step-size condition of SGD and AdaGrad.\n\n2. **Comprehensive Coverage**\n\n   * The analysis includes both **Type I** and **Type II** step-size definitions, encompassing all known AdaGrad variants.\n   * Convergence results are derived for **strongly convex**, **convex**, and **non-convex** objectives.\n\n3. **Robustness under Arbitrary Stopping Times**\n\n   * The results hold for any stopping time, offering theoretical justification for adaptive stopping mechanisms (e.g., early stopping, validation-based termination).\n   * This robustness is practically relevant in modern training pipelines.\n\n4. **Clarity and Theoretical Soundness**\n\n   * The proofs are systematic and well-structured, relying on classical martingale tools (Robbins–Siegmund theorem) and new bounding techniques.\n   * The comparison table (Table 1) clearly positions the contribution relative to prior work.\n\n5. **Empirical Validation of Flexibility Parameter**\n\n   * Simple yet effective experiments demonstrate that introducing ( \\gamma ) can improve empirical performance and training stability."}, "weaknesses": {"value": "1. **Limited Experimental Depth**\n\n   * The empirical section is minimal and focuses only on CIFAR-10 with a single architecture (VGG).\n   * There are no large-scale or ablation studies exploring how ( \\gamma ) interacts with learning rate, batch size, or non-convex architectures (e.g., transformers).\n\n2. **Missing Intuition for Flexibility Parameter**\n\n   * While ( \\gamma ) is well-motivated mathematically, the **practical intuition**—why smaller values improve stability—is not sufficiently explained.\n   * A discussion linking ( \\gamma ) to adaptivity or implicit regularization would improve accessibility.\n\n3. **Notation Density**\n\n   * The paper introduces many parameters (e.g., ( \\eta, \\gamma, \\epsilon, \\beta, \\mu, M, Q )), which makes the reading heavy.\n   * A summarizing table of notation early in the paper would be very helpful.\n\n4. **No Comparison to Other Adaptive Methods**\n\n   * The work focuses solely on AdaGrad; even though the authors mention Adam/AdamW as future directions, no empirical or theoretical comparisons are made.\n   * It is unclear whether similar results could hold for other adaptive algorithms.\n\n5. **Minor Clarity Issues**\n\n   * Some propositions and inequalities are overly detailed in the main text, pushing key insights into appendices.\n   * Figures could use clearer legends and scaling (e.g., Figure 1’s axes)."}, "questions": {"value": "1. Can the “generalized square summability” condition be extended to **matrix-based AdaGrad** (e.g., full or diagonal preconditioning)?\n2. How sensitive are the theoretical guarantees to the bounded-gradient assumption (Assumption 2.2)? Could relaxed moment bounds suffice?\n3. For non-convex functions, is there an intuitive explanation for why **only best-iterate** convergence (and not last-iterate) is provable?\n4. Could the framework be extended to stochastic adaptive momentum methods (e.g., Adam) without losing almost sure guarantees?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ojkm4ZLezC", "forum": "DwWorqSjwv", "replyto": "DwWorqSjwv", "signatures": ["ICLR.cc/2026/Conference/Submission9313/Reviewer_PYxr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9313/Reviewer_PYxr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762019493315, "cdate": 1762019493315, "tmdate": 1762920947439, "mdate": 1762920947439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}