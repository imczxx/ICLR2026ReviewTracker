{"id": "U2SJE6W3wT", "number": 999, "cdate": 1756827515985, "mdate": 1759898232554, "content": {"title": "Improved Adversarial Diffusion Compression for Real-World Video Super-Resolution", "abstract": "While many diffusion models have achieved impressive results in real-world video super-resolution (Real-VSR) by generating rich and realistic details, their reliance on multi-step sampling leads to slow inference. One-step networks like SeedVR2, DOVE, and DLoRAL alleviate this through condensing generation into one single step, yet they remain heavy, with billions of parameters and multi-second latency. Recent adversarial diffusion compression (ADC) offers a promising path via pruning and distilling these models into a compact AdcSR network, but directly applying it to Real-VSR fails to balance spatial details and temporal consistency due to its lack of temporal awareness and the limitations of standard adversarial learning. To address these challenges, we propose an improved **ADC** method for Real-**VSR**. Our approach distills a large diffusion Transformer (DiT) teacher DOVE equipped with 3D spatio-temporal attentions, into a pruned 2D Stable Diffusion (SD)-based AdcSR backbone, augmented with lightweight 1D temporal convolutions, achieving significantly higher efficiency. In addition, we introduce a dual-head adversarial distillation scheme, in which discriminators in both pixel and feature domains explicitly disentangle the discrimination of details and consistency into two heads, enabling both objectives to be effectively optimized without sacrificing one for the other. Experiments demonstrate that the resulting compressed **AdcVSR** model reduces complexity by **95%** in parameters and achieves an **8$\\times$** acceleration over its DiT teacher DOVE, while maintaining competitive video quality and efficiency.", "tldr": "", "keywords": ["Real-World Video Super-Resolution", "One-Step Diffusion", "Improved Adversarial Diffusion Compression", "Diffusion Distillation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/08d4a2e1bdcbbac39bcdf0e1ca7531010e4286e1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed an improved adversarial diffusion compression method for real-world Video Super-Resolution. It comprises a 2D SD backbone and several 1D convs to replace the heavy 3D DiT architecture to improves its efficiency. By using a dual-head discriminator, it balances the optimization between detail enhancement and temporal consistency for adversarial distillation scheme. Experiments on synthetic and real-world benchmarks show its potential in compressing Real-VSR model on both model size and inference steps effectively."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* This paper proposes an intuitive explanation to use a ``2D + 1D'' architecture for student model to improve its efficiency.\n* Detailed designs on adversarial distillation scheme (e.g., data type, training loss) help improve its clarity.\n* Thorough experiments on synthetic and real-world benchmarks demonstrate its effectiveness after distillation."}, "weaknesses": {"value": "* I remain skeptical about the effectiveness of replacing the 3D DiT architecture with a ``2D+1D'' architecture. As demonstrated in the paper, it uses a radical distillation strategy to achieve a 20-fold reduction in parameter count and single-step inference. It is essential to demonstrate that this strategy does not cause significant performance degradation on real-world benchmarks. Ablation study is only conducted in UDM10 (a small synthetic benchmark). Authors should conduct more analysis on large-scale real-world benchmarks.\n* Detailed investigation on model size is missing. This paper provides an appealing results by distilling a 0.6B student model from a 11B teacher model. However, the selection of model size requires further exploration. Authors need to find a boundary between visual quality and model size, and conduct an ablation study about it.\n* Motivation of using dual-domain discriminator. As mentioned in Sec 3.3, it replaces the conventional feat-domain discriminator by two discriminators in both pixel and feat domain. However, the motivation of adding one discrimintor in pixel domain remains unclear. Some related questions remain unresolved, such as why two discriminators are not used in the feature domain, and whether using more discriminators would improve performance. \n* Authors should also conduct experiments about temporal quality. By using 1D convs and a relative small model size, the temporal consistency of AdcVSR remians unclear now. Authors should conduct experiments by showing multiple frames of a single video in real-world benchmark."}, "questions": {"value": "* Using stronger generative backbone. Using SD2.1 as 2D backbone limits the capacity on complex degradations. Authors should consider utilizing a large-scale backbone.\n* I strongly recommend authors to provide a video demo or video files corresponding to the results presented in the paper. Given the usage of a ``2D+1D'' architecture, verifying the temporal quality of the final results is essential. Displaying only a single frame from the video within the article is not very convincing. I will consider raising my score after evaluating the video results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zKnF1LBe4W", "forum": "U2SJE6W3wT", "replyto": "U2SJE6W3wT", "signatures": ["ICLR.cc/2026/Conference/Submission999/Reviewer_c8ag"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission999/Reviewer_c8ag"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760522072121, "cdate": 1760522072121, "tmdate": 1762915653932, "mdate": 1762915653932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an improved adversarial diffusion compression method for real-world video super-resolution tasks. The authors note that while existing diffusion-based VSR methods can generate detail-rich videos, their inference speed is slow; conversely, one-step approaches are faster but still result in bulky models. To address this, the authors introduce AdcVSR, which distills a DiT teacher model with 3D spatiotemporal attention into a lightweight \"2D + 1D\" student network (based on a trimmed Stable Diffusion 2.1 backbone combined with 1D temporal convolutions). Additionally, a dual-head dual-discriminator adversarial distillation strategy is introduced to decouple the discrimination of detail richness and temporal consistency in the pixel and feature domains, respectively. This approach significantly improves efficiency while maintaining video quality. Experiments demonstrate that AdcVSR reduces parameters by 95% and accelerates inference by 8 times, while still achieving visual quality comparable to the teacher model."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Originality: The novel \"2D + 1D\" architecture design, combined with the dual-head discriminator adversarial distillation strategy, effectively decouples the optimization objectives for detail and consistency, demonstrating strong innovation.\n\nQuality: Comprehensive experimental designs, including extensive validation on multiple synthetic and real-world datasets, support the effectiveness of the method through both quantitative and qualitative results. Ablation studies also thoroughly verify the contribution of each module.\n\nClarity: The paper is well-structured, with detailed method descriptions, and the inclusion of diagrams and pseudocode aids understanding. The writing is generally fluent, and technical details are clearly expressed.\nImpact: The proposed method achieves a notable balance between efficiency and quality, offering significant practical value for deployment and providing a feasible pathway for the compression and application of diffusion models in video tasks."}, "weaknesses": {"value": "Insufficient Comparative Experiments: Although comparisons are made with several SOTA methods, there is a lack of comparison with recent non-diffusion-based efficient VSR approaches, such as those based on CNNs or lightweight Transformers.\n\nLimited Generalization Validation: All experiments are conducted at a fixed resolution (512×512) and frame length (25 frames), without demonstrating performance on longer videos or higher resolutions.\nWeak Theoretical Support for Dual-Head Discriminator Design: While experiments prove its effectiveness, there is insufficient theoretical or visual analysis explaining why the \"shared backbone + separate heads\" design outperforms independent discriminators.\n\nIncomplete Computational Efficiency Comparison: Only parameter counts and inference time are provided, without more detailed efficiency metrics such as FLOPs or memory usage."}, "questions": {"value": "Is the weight allocation (75%/25%) between the \"detail head\" and \"consistency head\" in the dual-head discriminator universally applicable? Would this ratio remain effective across different datasets or tasks?\n\nWere other teacher models (e.g., SeedVR2, DLoRAL) explored? Was DOVE selected solely because its structure is more suitable for this method?\n\nIt is recommended to include performance on longer video sequences (e.g., >100 frames) to validate the stability of temporal modeling, and providing failure cases or limitations analysis, such as performance under complex motion or extreme degradation, would be beneficial."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m0uYq0wIXV", "forum": "U2SJE6W3wT", "replyto": "U2SJE6W3wT", "signatures": ["ICLR.cc/2026/Conference/Submission999/Reviewer_7Ea8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission999/Reviewer_7Ea8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814448947, "cdate": 1761814448947, "tmdate": 1762915653803, "mdate": 1762915653803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AdcVSR, an improved adversarial diffusion compression framework tailored for real-world video super-resolution (Real-VSR).\nThe method builds upon the concept of Adversarial Diffusion Compression (ADC), proposing a “2D + 1D” hybrid architecture that replaces heavy 3D diffusion backbones with a 2D spatial diffusion network (a pruned SD2.1) augmented by lightweight temporal 1D convolutions.\nFurthermore, it introduces a dual-head dual-discriminator adversarial distillation scheme, where two discriminators (in pixel and feature domains) independently supervise “detail” and “temporal consistency.”\nExperimental results on multiple Real-VSR datasets demonstrate that AdcVSR achieves competitive visual quality while reducing parameters by up to 95% and achieving an 8× inference speedup compared to the teacher model DOVE."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed dual-head discriminator effectively disentangles spatial detail enhancement and temporal consistency, addressing a long-standing trade-off in Real-VSR.\n\n2. The results on multiple datasets and metrics (PSNR, LPIPS, MUSIQ, MANIQA, etc.) are convincing and show both efficiency and quality improvements. And the visual quality is also satisfactory.\n\n3. The 2D+1D design combined with adversarial distillation is simple yet efficient, offering clear insights into practical diffusion model compression."}, "weaknesses": {"value": "The paper lacks formal justification for why the dual-head adversarial loss leads to better convergence or perceptual trade-off control."}, "questions": {"value": "1. Why does AdcVSR use a video diffusion model as the teacher but an image diffusion model as the backbone? How does a 3D spatio-temporal DiT as the student network compare with the 2D+1D architecture in terms of performance and efficiency? I recommend that the authors include related experiments.\n\n2. Why did the authors choose to use a 2D VAE? I think that employing a 3D VAE could lead to faster inference and better temporal consistency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FATQPC6TjL", "forum": "U2SJE6W3wT", "replyto": "U2SJE6W3wT", "signatures": ["ICLR.cc/2026/Conference/Submission999/Reviewer_r2a8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission999/Reviewer_r2a8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915831928, "cdate": 1761915831928, "tmdate": 1762915653638, "mdate": 1762915653638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This Paper proposes an improved Adversarial Diffusion Compression method. The core idea it to distill knowledge from a large-scale 3D teacher model into a well-designed and lightweight ‘2D + 1D’ student model. This is achieved through a novel adversarial distillation scheme. The proposed approach effectively addresses the critical challenge of preserving both spatial details and temporal consistency, a common problem in the compression of video super-resolution models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis paper proposing a “2D spatial + 1D temporal” decoupling hypothesis, and introduces a novel lightweight “2D+1D” architecture. This approach drastically cuts down on parameters and computational load, enabling efficient inference.\n2.\tThe paper proposes a novel dual-head adversarial distillation scheme. This scheme effectively balances the richness of spatial details with temporal coherence, which is a critical challenge in the field of video super-resolution."}, "weaknesses": {"value": "1.\tA core assumption of this paper is that a 2D diffusion model is sufficient for synthesizing fine-grained details. However, this assumption is challenged by the experimental results. Currently, the ablation study comparing the 2D and 3D backbones is based only on the DISTS metric. To provide a more balanced and convincing comparison, the authors should consider including additional metrics that measure perceptual quality (e.g., LPIPS) and/or fidelity (e.g., PSNR, SSIM). Furthermore, the qualitative results in Figures 3 and 5 exhibit clear visual artifacts or \"hallucinations.\" These findings suggest that the 2D diffusion model may be insufficient for generating correct details, and therefore, the validity of this core assumption is questionable. The authors should address this limitation, perhaps by discussing the trade-offs of their approach or analyzing why these artifacts occur.\n2.\tThe paper states that the 'detail head' label for real videos is set to 'unlabeled', with the provided justification being to “encouraging the generator to produce more detail-rich frames”. This is a critical design, yet the underlying mechanism or rationale is not sufficiently explained. It is unclear why the more intuitive approach—labeling the details from real videos as 'real'—was not adopted. The authors should clarify whether this decision is supported by empirical findings (e.g., from an ablation study) or if it is grounded in some theoretical justification."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IRzl3seu62", "forum": "U2SJE6W3wT", "replyto": "U2SJE6W3wT", "signatures": ["ICLR.cc/2026/Conference/Submission999/Reviewer_GsDk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission999/Reviewer_GsDk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152199770, "cdate": 1762152199770, "tmdate": 1762915653528, "mdate": 1762915653528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}