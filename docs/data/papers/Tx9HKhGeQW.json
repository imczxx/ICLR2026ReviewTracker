{"id": "Tx9HKhGeQW", "number": 16171, "cdate": 1758260957543, "mdate": 1759897256927, "content": {"title": "Reinforcing Query-Level Meta-Agents", "abstract": "This paper proposes a query-level meta-agent named FlowReasoner to automate the design of query-level multi-agent systems, i.e., one system per user query. Our core idea is to incentivize a reasoning-based meta-agent via external execution feedback. Concretely, by distilling DeepSeek R1, we first endow the basic reasoning ability regarding the generation of multi-agent systems to FlowReasoner. Then, we further enhance it via reinforcement learning (RL) with external execution feedback. A multi-purpose reward is designed to guide the RL training from aspects of performance, complexity, and efficiency. In this manner, FlowReasoner is enabled to generate a personalized multi-agent system for each user query via deliberative reasoning. Experiments on both engineering and competition code benchmarks demonstrate the superiority of FlowReasoner. Remarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks.", "tldr": "", "keywords": ["Large Language Models", "Multi-Agent System", "Automatic Workflow Design", "Large Reasoning Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/afdf329d9f128c108ff3b48c5d8668f2b51999ad.pdf", "supplementary_material": "/attachment/52bd97680d4ebb5f34c0e4a78b79b437d638f323.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel framework, FLOWREASONER, aimed at automating the design of query-level multi-agent systems, a promising and significant research direction. The authors formulate the problem as training a reasoning meta-agent via reinforcement learning to dynamically generate customized workflows for each specific user query, a clear distinction from traditional task-level approaches. The methodology, combining data distillation, supervised fine-tuning, and reinforcement learning, is logically sound. The experimental results also demonstrate its superiority on several code benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper's primary strength is its novel query-level meta-agent, FLOWREASONER, which enhances system adaptability by dynamically generating a custom multi-agent system for each unique user query. It innovatively uses Reinforcement Learning guided by real-world external execution feedback and a multi-purpose reward function to learn effective planning without relying on complex search algorithms. This approach is empirically validated, with experiments showing that FLOWREASONER significantly outperforms existing methods."}, "weaknesses": {"value": "1. The core contribution of this paper appears to be demonstrating that a smaller model can be taught how to combine a series of predefined roles (Operators) in a query-specific manner. However, these foundational roles (e.g., Code Generator, Review Operator) are explicitly borrowed from prior work (e.g., Aflow). Therefore, the novelty does not seem to lie in designing new agentic behaviors, but rather in the process of learning to combine them.\n\n2. The training data originates from three specific code benchmarks (BigCodeBench, HumanEval, and MBPP). To verify that the model has learned a general capability for code planning rather than patterns specific to these datasets, it is crucial to test it on other code datasets that are outside of this training distribution. The paper currently lacks this validation.\n\n3. The paper's central claim is about training a general \"query-level meta-agent.\" However, all experiments remain within the code domain. This makes it impossible to determine if the method can generalize to tasks outside of code that also require complex planning (e.g., mathematical word problems, multi-hop QA)."}, "questions": {"value": "1. The experiments are confined to the code generation domain. Has the meta-agent learned a general, transferable planning logic, or has it merely learned specific patterns highly optimized for the code generation scenario?\n\n2. The paper states the reward function is a multi-purpose one considering performance, complexity, and efficiency. Could the authors elaborate on how complexity and efficiency are quantified and integrated into the final scalar reward?\n\n3. The multi-purpose reward function (performance, complexity, efficiency) is key to the RL stage. What kind of workflows would the model generate if only performance were used as the reward? How significant are the roles of the complexity and efficiency regularizers?\n\n4. The initial data distillation stage is foundational to the entire approach. What would the performance degradation be if this step were skipped and RL training were conducted directly on a general-purpose base model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lX8LJ8tfHO", "forum": "Tx9HKhGeQW", "replyto": "Tx9HKhGeQW", "signatures": ["ICLR.cc/2026/Conference/Submission16171/Reviewer_dZ8V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16171/Reviewer_dZ8V"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760607571135, "cdate": 1760607571135, "tmdate": 1762926333757, "mdate": 1762926333757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FlowReasoner, a meta-agent that automates the design of multi-agent systems. Unlike existing methods, FlowReasoner can generate a customized system for each user query. The approach employs RL to optimize system generation, and the proposed FlowReasoner-14B model demonstrates superior performance compared to existing meta-agent baselines on coding tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a two-stage training pipeline, including SFT and RL, to optimize the meta-agent. Experimental results show that even a relatively small model (14B) can effectively design multi-agent sytems and enhance overall performance.\n\n2. FlowReasoner supports query-level system generation, improving adaptability to diverse queries."}, "weaknesses": {"value": "1. The evaluation is limited to coding tasks, which restricts the generalizability of the method. Prior works, such as AFlow and MaAS, include broader experiments on QA, math, and tool-use tasks.\n\n2. In Table 2, RL fine-tuning yields less than a 1% improvement, suggesting only marginal gains. Furthermore, the absence of results for a meta-agent directly using DeepSeek-R1-Distill-Qwen without SFT makes it unclear how much benefit SFT contributes.\n\n3. Figure 4 lacks explanation for the differences indicated by color lightness.\n\n4. (minor) A space is missing before the references in Lines 314-315."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BggPl89z40", "forum": "Tx9HKhGeQW", "replyto": "Tx9HKhGeQW", "signatures": ["ICLR.cc/2026/Conference/Submission16171/Reviewer_6NMr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16171/Reviewer_6NMr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798868205, "cdate": 1761798868205, "tmdate": 1762926333191, "mdate": 1762926333191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author proposes the query-level meta-agent FLOWREASONER, achieving \"one system per user query\".\nIt eliminates the need for manual set searching, uses external execution feedback as rewards, and trains an o1-like inference model through RL (GRPO), balancing performance, complexity, and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.FLOWREASONER improves state-of-the-art performance by 10.5% on average across three code benchmarks, surpassing o1-mini, AFLOW, MaAS, and other open-source code and models.\n\n2.The case study shows that the same model generates more complex workflows for complex engineering tasks compared to simple algorithm problems, verifying that FLOWREASONER adapts to query difficulty."}, "weaknesses": {"value": "1.Although the author emphasizes \"one system per user query\", FLOWREASONER is only tested on code generation (which can be executed automatically and rewarded with 0/1) task to prove its effectiveness. This limited task type may indicate that the FLOWREASONER has poor scalability. The author may consider conducting additional experiments on other types of tasks (web search Q&A, mathematics, etc.) or even real-world tasks where external feedback is difficult to obtain.\n\n2.The design motivation of multi-purpose rewards has not been elaborated, and the impact of complexity and diversity rewards on the final efficiency and performance of the system should be further explored.\n\nPresentation issue：\n1. Section 5.1 Ablation of Meta-agents and Workers：Figure 1 -> Figure 4"}, "questions": {"value": "1.What are the costs of data synthesis and training 7B and 14B models respectively?\n\n2.In section 5.2, you only prove the generalization ability of the method on different worker models. Does it generalize on unseen tasks? For example, what is the final performance after removing MBPP from the training set?\n\n3.For subsection \"Ablation of Meta-agents and Workers\" and Figure 4: What methods are used in these experiments? What do the dark and light scores for each model in the diagram of Figure 4 represent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rUbsGm2uLQ", "forum": "Tx9HKhGeQW", "replyto": "Tx9HKhGeQW", "signatures": ["ICLR.cc/2026/Conference/Submission16171/Reviewer_fyue"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16171/Reviewer_fyue"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829899119, "cdate": 1761829899119, "tmdate": 1762926332656, "mdate": 1762926332656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}