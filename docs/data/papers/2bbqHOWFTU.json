{"id": "2bbqHOWFTU", "number": 11778, "cdate": 1758203738175, "mdate": 1763755390092, "content": {"title": "Robust Adaptive Multi-Step Predictive Shielding", "abstract": "Reinforcement learning for safety-critical tasks requires policies that are both high-performing and safe throughout the learning process. While model-predictive shielding is a promising approach, existing methods are often computationally intractable for the high-dimensional, nonlinear systems where deep RL excels, as they typically rely on a patchwork of local models. We introduce **RAMPS**, a scalable shielding framework that overcomes this limitation by leveraging a learned, linear representation of the environment's dynamics. This model can range from a linear regression in the original state space to a more complex operator learned in a high-dimensional feature space. The key is that this linear structure enables a robust, look-ahead safety technique based on a *multi-step Control Barrier Function (CBF)*. By moving beyond myopic one-step formulations, **RAMPS** accounts for model error and control delays to provide reliable, real-time interventions. The resulting framework is minimally invasive, computationally efficient, and built upon robust control-theoretic foundations. Our experiments demonstrate that **RAMPS** significantly reduces safety violations compared to existing safe RL methods while maintaining high task performance in complex control environments.", "tldr": "A robust multi-step control barrier function for a minimally invasive shielding without sacrificing performance", "keywords": ["Safe Reinforcement Learning", "Control Barrier functions", "Model Predictive shielding"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/550e2e89162500b01b4aa38a158166f3e33d12c2.pdf", "supplementary_material": "/attachment/589094a51ffd7893db10e422ab8ca872706a6d7b.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a scalable model-predictive shielding framework that enables safe policy learning for complex, high-dimensional systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. The paper claims that current model-predictive shielding (MPS) frameworks have a limitation: they are \"computationally intractable for high-dimensional, nonlinear systems.\" However, the paper does not clearly define what constitutes \"high-dimensional\" or provide evidence for this limitation. When considering the dimensionality of system states, the proposed method does not appear to address this issue. Furthermore, the dynamics model in MPS can be simplified using order-reduction techniques to reduce dimensionality, which the paper does not acknowledge.\n\n2. The proposed dynamics model is a time-invariant linear model (with fixed system and control matrices), and the multi-step look-ahead derivations depend on this model. However, time-invariant linear models often exhibit significant mismatch with real-world systems, which are typically highly nonlinear. Consequently, the results will lack robustness, and the prediction accuracy will be unreliable in practical applications.\n\n3. MPS can be viewed as a fault-tolerant RL approach for safety-critical systems and can also be considered an advanced variant of Simplex [1]. Considering this, related safe RL frameworks include runtime assurance [2,3], neural Simplex [4], and the runtime learning machine [5]. Compared to the approach proposed in this paper, these frameworks do not require a multi-step look-ahead to guarantee safety; they can ensure safety even when model mismatches are present. Additionally, these frameworks have been validated on real, complex autonomous systems rather than toy simulators. However, the paper lacks comprehensive comparisons with these methods, making it difficult to conclude whether the proposed approach can outperform existing solutions.\n\n4. The experimental systems evaluated in the paper are not particularly high-dimensional, which undermines the motivation stated in the introduction regarding the need to handle high-dimensional, nonlinear systems.\n\n**References**\n\n[1] Sha, L. (2001). Using simplicity to control complexity. IEEE Software, 18(4), 20-28.\n\n[2] Brat, G., & Pai, G. Runtime assurance of aeronautical products: preliminary recommendations. NTRS - NASA Technical Reports Server, 2023.\n\n[3] Sifakis, J., & Harel, D. (2023). Trustworthy autonomous system development. ACM Transactions on Embedded Computing Systems, 22(3), 1-24.\n\n[4] Phan, D. T., Grosu, R., Jansen, N., Paoletti, N., Smolka, S. A., & Stoller, S. D. (2020). Neural Simplex architecture. In NASA Formal Methods: 12th International Symposium, Moffett Field, CA, USA, May 11–15, 2020, Proceedings 12 (pp. 97-114). Springer International Publishing.\n\n[5] Cai, Yihao, Yanbing Mao, Lui Sha, Hongpeng Cao, and Marco Caccamo. \"Runtime Learning Machine.\" ACM Transactions on Cyber-Physical Systems (2025)."}, "questions": {"value": "The Weaknesses include my questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3T3z4GdDfV", "forum": "2bbqHOWFTU", "replyto": "2bbqHOWFTU", "signatures": ["ICLR.cc/2026/Conference/Submission11778/Reviewer_XTg3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11778/Reviewer_XTg3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760830552864, "cdate": 1760830552864, "tmdate": 1762922804013, "mdate": 1762922804013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, authors propose a shielding-based safe RL framework. Leveraging the linear modelling of the dynamic model and robust multi-step CBF, this framework can provide formal safety guarantee for the RL agent training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper overall is well written with clear structure. The method part is clearly illustrated with intuitive explanation. Illustrative example is provided for easy interpretation."}, "weaknesses": {"value": "1, RL part has been mentioned as main component of the paper in line 143 and 144 Section 4. However, there is no clear illustration how the RL problem is solved combined with the safety shielding throughout the main paper. As a RL track paper, the contribution is doubted.\\\n2, Since the dynamic system is already linearized with particular formulation shown in line 198, it is more natural for review to consider framing it as a MPC/LQR problem, where at each time step combining with proposed multi-step robust CBF can be formulated as an QP. Thus, it is essential to address the motivation of current combination. \\\n3, Combining Koopman operator with control barrier function has already been explored by dozen of works[3,4].\n4, Authors have addressed actuation delay in line 166 and introduction, however, the approach to mitigate this actuation delay has not been discussed in the rest of the methodology part and experiment section.\\\n5, More elaboration is needed on the RL framework side, including how the shielding mechanism apply in the original RL loop.\\\n6, The training details of linear dynamic model is also missing in the paper. It would be much better to have heuristic algorithm diagram to depict each component. \\\n7, For experiment section, more baselines are needed: (1), comparison with MPC based methods are encouraged to include; (2) comparison with CBF-based RLs are needed, particularly those optimization-based methods with formal guarantees[1,2]\n\n[1]:Choi, Jason, et al. \"Reinforcement learning for safety-critical control under model uncertainty, using control lyapunov functions and control barrier functions.\" arXiv preprint arXiv:2004.07584 (2020).\\\n[2]:Wang, Yixuan, et al. \"Joint differentiable optimization and verification for certified reinforcement learning.\" Proceedings of the ACM/IEEE 14th International Conference on Cyber-Physical Systems (with CPS-IoT Week 2023). 2023.\\\n[3]:Folkestad, Carl, et al. \"Data-driven safety-critical control: Synthesizing control barrier functions with Koopman operators.\" IEEE Control Systems Letters 5.6 (2020): 2012-2017.\\\n[4]:Zinage, Vrushabh, and Efstathios Bakolas. \"Neural koopman control barrier functions for safety-critical control of unknown nonlinear systems.\" arXiv preprint arXiv:2209.07685 (2022)."}, "questions": {"value": "1, How is the actuation delay mentioned in line 166 modeled in the proposed dynamic system?\\\n2, How is the shielding mechanism involved in the RL training? Do we need to solve the QP for each action selection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xpGuAu6VcJ", "forum": "2bbqHOWFTU", "replyto": "2bbqHOWFTU", "signatures": ["ICLR.cc/2026/Conference/Submission11778/Reviewer_mgYK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11778/Reviewer_mgYK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761548262125, "cdate": 1761548262125, "tmdate": 1762922803650, "mdate": 1762922803650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose RAMPS, a framework that combines multi-step Control Barrier Functions (CBFs) with linear dynamics models derived via Koopman operator learning to enable robust, certified safe exploration in reinforcement learning and control.\nRAMPS formulates safety as the probability of entering an unsafe set and uses a quadratic program (QP) to minimally modify a given policy action to satisfy the multi-step CBF constraints.\nA key theoretical result (Theorem 1) provides a probabilistic safety guarantee under bounded model error, and experiments on simulated control benchmarks demonstrate improved safety and performance compared to existing shielded RL methods."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.\tExtending formal safety certificates to nonlinear and high-dimensional systems is a central challenge in safe RL, and the paper directly addresses this gap.\n2.\tThe idea of leveraging a Koopman-based linear model for multi-step safety prediction is conceptually elegant and enables computationally tractable QP formulations.\n3. The paper provides a formal safety theorem under bounded model uncertainty, and the derivation is mathematically coherent within the assumed linear structure.\n4.\tThe experiments are extensive and show that RAMPS achieves a good trade-off between safety and task performance, outperforming strong baselines.\n5.\tThe method includes a fallback (backup) policy to handle infeasible QP cases, showing practical awareness of real-world constraints."}, "weaknesses": {"value": "1. Theorem 1 assumes that the QP problem is feasible at each step but does not provide conditions ensuring that feasibility is preserved over time. Without a proof of recursive feasibility or a characterization of when the QP remains solvable, the safety guarantee is conditional rather than absolute.\n2. While Koopman operator learning is used to obtain linear dynamics, it remains unclear how accurately such models can approximate complex nonlinear environments. Theoretical guarantees rely on bounded model error ε, but the method for estimating or maintaining this bound under policy drift is empirical.\n3.  The unsafe set is modeled as a union of convex polytopes. Although theoretically sufficient for approximation, this assumption may not hold in realistic, highly nonconvex safety scenarios, and the computational cost scales poorly with the number of facets.\n4. The framework’s tractability and guarantees hinge on having a linear model. It is unclear whether similar safety certificates can be extended to fully nonlinear learned models without sacrificing computational efficiency.\n5. The paper could provide quantitative analysis of QP infeasibility frequency, the impact of the backup policy, and how Koopman model accuracy affects safety margins."}, "questions": {"value": "1. Can you provide a sufficient or verifiable condition under which the QP remains feasible at all time steps? If not, how frequently does infeasibility occur in practice?\n2.\tHow sensitive is RAMPS to modeling errors or distribution shift as the policy evolves? Is the error bound ε recalibrated online?\n3.\tHave you evaluated how the number of polyhedral facets affects safety and computational overhead? Could more flexible representations (e.g., neural implicit sets) be incorporated?\n4.\tCan you report the proportion of time steps when the backup policy is triggered and its effect on reward and safety violation rates?\n5.\tWhat are the main limitations of using a linear Koopman model compared to nonlinear learned models, and do you foresee ways to relax this assumption while keeping the QP tractable?\n6.\tWhat is the empirical runtime of the QP and the precomputation stage for large-scale environments (e.g., SafeAnt)? How does the method scale with latent dimension and horizon length?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aXqV77UdsY", "forum": "2bbqHOWFTU", "replyto": "2bbqHOWFTU", "signatures": ["ICLR.cc/2026/Conference/Submission11778/Reviewer_8JnD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11778/Reviewer_8JnD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756166971, "cdate": 1761756166971, "tmdate": 1762922803291, "mdate": 1762922803291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a safety shield for RL that uses learned linear (or Koopman-lifted) dynamics and multi-step robust CBFs. At each step, a QP (Eq 3) adjusts the policy's action to stay safe under bounded model error, using an adaptive prediction and a backup controller when the QP becomes infeasible. A high-provability bound on model accuracy is provided, together with the standard forward invariance argument. Experiments on several Safety-Gym tasks show large reductions in safety violations while maintaining comparable rewards."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- I like the idea of combining multi-step CBFs with learned linear or Koopman-lifted dynamics. The adaptive horizon and tightening scheme are a nice way to deal with higher relative-degree constraints and bounded model uncertainty. \n\n- The method is technically consistent. The QP formulation for CBFs is well-posed, and the robust tightening is well-motivated, and Theorem 2 adds a probabilistic model-accuracy bound, albeit without new theoretical depth.\n\n- The framework improves the practical scalability of safety-shielded RL. It could become a helpful module for safe data collection or model-based control, though its broader impact remains to be seen."}, "weaknesses": {"value": "- Theorem 2 only bounds model accuracy; it doesn't actually tell us whether the learned policy is actually safe or near-optimal. It seems implicitly assumed that having an accurate model automatically leads to a safe, near-optimal policy, but the paper doesn't clearly establish the connection between model accuracy, constraint satisfaction, and policy performance. \n\n- The paper doesn't really say when the QP is guaranteed to be (recursively) feasible. When the QP becomes infeasible, a backup controller is used, but its activation rate and effect on performance are not studied. \n\n- It seems the agent rarely visits high-uncertainty regions as the shield cuts off risky actions near the safety boundary. That means the model might never improve near the boundary, creating a kind of \"self-reinforcing conservatism loop\".\n\n- While Figures 1 and 2 show empirical decreases in model error and safety violations, there's no theoretical or probabilistic guarantee that these quantities improve over time.\n\n- In the Koopman-lifted version, polyhedral safety constraints are zero-padded into the latent space. That's a conservative embedding, not a true preservation of the constant geometry. This may cause unnecessary conservatism.\n\n- Although the proposed framework is described as algorithm-agnostic, experiments employ only one RL backbone. It remains unclear how the shield behaves similarly with algorithms of different exploration styles (e.g., PPO vs SAC vs TD3)."}, "questions": {"value": "- Can you link model accuracy (Theorem 2) to actual policy safety or optimality? Even a rough probabilistic bound or empirical measure of policy violations would help.\n\n- Are there analytical conditions (e.g., invariant sets or terminal tubes) that guarantee recursive feasibility of the QP in Eq (3)? How often did the backup controller trigger in practice?\n\n- How do you make sure the model keeps improving if exploration stays inside the safe region? Would a mild risk budget or uncertainty-aware relaxation help?\n\n- Do model error and violation frequency really decrease over time?\n\n- In the Koopman setup, have you tried anything beyond zero-padding to encode safety constraints more tightly?\n\n- Since you call the shield \"policy-agnostic\", have you tested it with more than one RL algorithm (e.g., PPO, SAC, TD3)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HkiMkUuMGD", "forum": "2bbqHOWFTU", "replyto": "2bbqHOWFTU", "signatures": ["ICLR.cc/2026/Conference/Submission11778/Reviewer_nR3a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11778/Reviewer_nR3a"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891016633, "cdate": 1761891016633, "tmdate": 1762922802829, "mdate": 1762922802829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank the reviewers for their constructive feedback, which has significantly improved the rigor and clarity of our work. We have uploaded a revised manuscript containing updates that comprehensively address the core concerns regarding scalability, theoretical guarantees, and reproducibility.\n\nBelow we summarize the changes made to the paper. We address the individual reviewer concerns in their respective comments.\n\n### **I. Validating Scalability & High-Dimensionality**\n* **Scaled to 348 Dimensions (SafeHumanoid):** We added experiments on **SafeHumanoid** (348 state dims, 17 action dims). This directly addresses concerns regarding dimensionality **[XTg3]**. RAMPS solves the safety QP in **~0.0005s** (Table 2) with **<1% infeasibility** (Table 3), demonstrating scalability to regimes where formal verification typically fails.\n* **Clarified Dimensionality Claims:** We updated the Abstract and Introduction to explicitly define \"high-dimensional\" as 348 dimensions (SafeHumanoid), differentiating our contribution from prior formal methods limited to **&lt;20 dimensions.** **[XTg3]**\n* **Discussion of Failed Baselines:** To highlight the scalability gap, we moved the discussion of failed baselines (SPICE, DMPS, VELM) from the appendix to **Section 5 (Baselines)**. We reiterate that these methods failed to train stably or timed out on the high-dimensional locomotion tasks where RAMPS succeeds.**[XTg3]**\n\n### **II. Strengthening Theoretical Precision**\n* **\"Conditional\" vs. \"Formal\" Guarantees:** We removed the term \"Formal\" and renamed Section 4.2.1 to **\"Conditional Safety Guarantees\"** to address questions regarding recursive feasibility **[8JnD, nR3a]**. We clarify that guarantees hold *subject to* QP feasibility (L341–349) and cite MPS literature providing similar guarantees.\n* **Added Corollary 1 (Probabilistic Bounds):** We added **Corollary 1** (Section 4.2.1) to link model accuracy to safety **[nR3a]**. This formalizes the connection between Theorem 2 (model error) and Theorem 1 (invariance), providing a probabilistic safety certificate over a finite horizon.\n* **Empirical Feasibility Analysis:** We added a paragraph in **Section 5.2** showing that the QP is feasible in **>98%** of timesteps across all tasks (and 100% for Humanoid/Pendulum), confirming that the \"conditional\" assumption holds in practice **[8JnD, nR3a]**.\n\n### **III. Methodology & Reproducibility**\n* **Policy-Agnostic Validation (SAC Added):** We added experiments and analysis using **Soft Actor-Critic (SAC)** (Table 1) to address concerns about using only one RL backbone **[nR3a]**.\n* **Added Algorithm 1 (RL Integration):** We added **Algorithm 1 (Page 15)**, explicitly detailing the interaction between the shield and the policy, the policy update, and the model recalibration loop, to address concerns about the unclear RL loop **[mgYK]**.\n* **Koopman Training Details:** We expanded **Appendix A.5.1** to detail the full training objective for the linear dynamics model **[mgYK]**.\n\nWe believe these revisions fundamentally strengthen the paper and demonstrate that RAMPS is a robust, scalable solution for safe RL in complex continuous control tasks."}}, "id": "GWh9imdIKh", "forum": "2bbqHOWFTU", "replyto": "2bbqHOWFTU", "signatures": ["ICLR.cc/2026/Conference/Submission11778/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11778/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission11778/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763669858546, "cdate": 1763669858546, "tmdate": 1763669858546, "mdate": 1763669858546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}