{"id": "s6rmpt3DWe", "number": 19636, "cdate": 1758297867174, "mdate": 1759897028984, "content": {"title": "IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation", "abstract": "Existing vision language models (VLMs), including GPT-4 and DALL·E, often struggle to preserve logic, object identity, and style in multimodal image-text generation. This limitation significantly hinders the generalization capability of VLMs in complex image-text input-output scenarios. To address this issue, we propose $\\textbf{IUT-Plug}$, a plug-in module grounded in an $\\textit{Image Understanding Tree}$  (IUT), which enhances existing interleaved VLMs through explicit structured reasoning, thereby mitigating context drift in logic, entity identity, and style. The proposed framework operates in two stages. (1) A dynamic IUT-Plug extraction module parses visual scenes into hierarchical symbolic structures. (2) A coordinated narrative-flow and image synthesis mechanism ensures cross-modal consistency. To evaluate our approach, we construct a novel benchmark based on 3,000 real human-generated question-answer pairs over fine-tuned large models, introducing a dynamic evaluation protocol for quantifying context drift in interleaved VLMs. Experimental results demonstrate that IUT-Plug not only improves accuracy on established benchmarks but also effectively alleviates the three critical forms of context drift across diverse multimodal question answering (QA) scenarios.", "tldr": "A plug-in module that enhances interleaved image-text generation inVLMs to  address compositional fragility and contextual drift.", "keywords": ["Vision Language Models; Computer Vision；Machine Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6464215aa527597ca5fd5cc8c70ec9c4e664aa04.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes IUT Plug, a method that sits between VLM and T2I models when generating images based on VLM outputs. The method adds additional context from the image to the VLM text output to improve T2I image generation. The approach is benchmarked on a custom dataset that includes expert annotations."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The method is relatively straightforward to employ given model access."}, "weaknesses": {"value": "**W1:** The contextual information extracted by the tool should arguably be easily recognized by a well-performing text-to-image generative model with a good descriptive text output from the VLM. The paper does not demonstrate that current T2I models fail to capture this information without the proposed method.\n\n**W2:** The tool merely extracts information from the image and appends it to the T2I input prompt. That such additional context does not degrade model performance and can improve results in cases where original image information is lost appears trivial, as providing more relevant information naturally supports image generation.\n\n**W3:** The practical relevance of the setting is questionable. It remains unclear in what scenarios VLMs and T2I models are deployed sequentially as separate components. Current understanding within the AI community suggests that VLMs with image generation capabilities (such as GPT-4o) employ unified architectures combining autoregressive and diffusion generation, rather than two discrete models.\n\n**W4:** The manuscript is difficult to read and follow. The narrative structure and motivations are often unclear. Abbreviations are introduced multiple times (e.g., T2I appears at least three times), and numerous spelling errors are present (e.g., line 314 \"(\" ).\n\n**W5:** The claim that the plug-in constitutes a \"World model\" (Line 066) is not justified. The method lacks inherent properties of world models, as it merely deconstructs the image into text.\n\n**W6:** The exact method by which concepts are extracted from the image is not explained.\n\n**W7:** It is also not described how the benchmark is constructed and what the “expert annotations” really are."}, "questions": {"value": "See weaknesses W1 to W7"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TXifKh3np8", "forum": "s6rmpt3DWe", "replyto": "s6rmpt3DWe", "signatures": ["ICLR.cc/2026/Conference/Submission19636/Reviewer_Z6W2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19636/Reviewer_Z6W2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760828218052, "cdate": 1760828218052, "tmdate": 1762931488613, "mdate": 1762931488613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical issue of multimodal context drift in interleaved image-text generation, where vision-language models (VLMs) fail to maintain consistency in logic, entity identity, and style across multiple turns. The authors propose IUT-Plug, a training-free, plug-in module that extracts a symbolic representation of the visual scene, called an Image Understanding Tree (IUT), to explicitly guide the generation process. To measure the method's effectiveness, the paper also introduces a novel dynamic evaluation framework that uses large models to generate and score fine-grained consistency criteria. Experiments show that IUT-Plug improves consistency scores across several VLM and text-to-image model combinations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper makes a valuable contribution by tackling the significant and well-defined problem of context drift in generative VLMs. The proposed IUT-Plug, a neuro-symbolic and training-free module, is a novel and practical approach. Furthermore, the introduction of a new evaluation framework that moves beyond standard metrics to assess semantic consistency is a commendable effort that can benefit the broader research community."}, "weaknesses": {"value": "1. Benchmark Validity and Accessibility: The evaluation relies entirely on a new, in-house benchmark of 3,000 samples. This raises concerns about potential dataset bias, where the collected data might favor structured, symbolic reasoning and thus unfairly advantage the proposed method. For the work to be verifiable and impactful, several key questions must be addressed:\n   - Will the benchmark be made public?\n   - What is the estimated API cost and procedure for running one full evaluation, given its reliance on proprietary models like GPT-5?\n\n\n2. Insufficient Comparison to Simpler Baselines: The primary comparison is between using IUT-Plug and not using it. However, the paper fails to compare against simpler, training-free alternatives that could also enhance consistency. For example, a strong baseline would be to use advanced prompt engineering, such as instructing the VLM to first generate a structured textual description of the scene (entities, attributes, relationships) and then use that description to form the final prompt for the text-to-image model. Without comparing against such methods, the added complexity of the IUT framework is not fully justified over more straightforward prompting strategies.\n\n\n3. Unclear Robustness of the IUT Representation: The paper's examples feature scenes with clear, discrete objects. The scalability and robustness of the IUT structure for more complex or ambiguous scenarios are not discussed. It is unclear how the method would handle:\n   - Abstract concepts (e.g., generating an image conveying \"a sense of loneliness\").\n   - Highly cluttered scenes with many interacting objects.\n   - Visually ambiguous elements that defy simple entity-attribute-relation decomposition.\n     This leaves the generalizability of the approach in question.\n\n\n4. Limited Qualitative Analysis: The visual results provided are limited and appear to be success cases. A more comprehensive qualitative analysis should include:\n   - A wider variety of generated examples to showcase performance across different domains.\n   - Crucially, a discussion of failure cases to provide a balanced understanding of the method's limitations.\n   - Illustrative examples from the benchmark itself to help the reader understand the nature and difficulty of the evaluation tasks."}, "questions": {"value": "Please address the concerns listed in the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FfMLHTL0lK", "forum": "s6rmpt3DWe", "replyto": "s6rmpt3DWe", "signatures": ["ICLR.cc/2026/Conference/Submission19636/Reviewer_XZAv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19636/Reviewer_XZAv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805894988, "cdate": 1761805894988, "tmdate": 1762931487723, "mdate": 1762931487723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical and well-recognized problem in modern Vision Language Models (VLMs): multimodal context drift during interleaved image-text generation. The proposed solution, IUT-Plug, aims to enhance consistency in logic, entity identity, and visual style by introducing an Image Understanding Tree (IUT)—a hierarchical symbolic structure representing visual scene elements and their relationships. This IUT is dynamically updated and used to guide both textual responses and text-to-image (T2I) generation. The authors also introduce a dynamic evaluation framework employing LLMs to generate task-specific criteria, which is then scored by a fine-tuned VLM."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper correctly identifies and targets multimodal context drift, a significant limitation of current state-of-the-art VLMs in multi-turn interactions. The use of an explicit, structured symbolic representation (IUT) to maintain consistency across modalities and turns is a conceptually appealing approach, aligning with principles from neuro-symbolic AI.\n2. The proposed lightweight and model-agnostic plug-in architecture has the potential to offer a practical way to enhance existing VLM-T2I pipelines without requiring extensive retraining of large foundation models.\n3. The dynamic evaluation protocol, using LLMs to generate task-specific criteria and a fine-tuned VLM for scoring, is an interesting methodological contribution for more nuanced and human-aligned assessment of multimodal consistency. The reported 87.6% agreement with human judgment is notable, provided the underlying LLM for criteria generation is verifiable."}, "weaknesses": {"value": "1. The repeated claim of using \"GPT-5\" for dynamic criterion generation undermines the reproducibility and scientific credibility of the entire evaluation framework. \n2. The paper provides no specific technical details on how the Image Understanding Tree (IUT) is constructed from an input image. This is a black box at the core of the proposed method, making it impossible to understand, reproduce, or critically evaluate the technical contribution.\n3. While IUT-Plug shows relative improvements (e.g., 7.2 to 10.5 percentage points), the absolute consistency scores remain quite low (often in the 30-40% range even with IUT-Plug). This suggests that the models still frequently fail to maintain consistency, and the \"alleviation\" of context drift is partial at best. This should be discussed more transparently.\n4. The paper does not adequately discuss the inherent expressiveness or limitations of the IUT representation for complex logical reasoning, abstract concepts, or handling ambiguity in visual scenes.\n5. The claim that IUT-Plug is \"lightweight\" is not supported by any quantitative data (e.g., computational overhead, inference time, memory footprint). Adding an additional processing pipeline will inevitably introduce some overhead.\n6. While scene graphs are mentioned, the paper does not sufficiently differentiate IUTs from existing scene graph generation and manipulation techniques, especially regarding dynamic updates. The assertion that existing scene graph methods \"do not support updates across interactions\" needs stronger evidence, as dynamic scene graphs are an active area of research."}, "questions": {"value": "1. Could the authors provide a detailed technical description of the \"dynamic IUT-Plug extraction module\"? What specific computer vision models or techniques are used to parse visual scenes into objects, attributes, and relationships? What is the pipeline for this extraction?\n2. Please define \"Situational Analysis\" and \"Project-based Learning\" as used in Table 1. What do these benchmarks entail, and how are their scores calculated?\n3. Given that even with IUT-Plug, consistency scores often remain below 50%, could the authors elaborate on the practical implications of these results? What level of consistency is considered \"acceptable\" for real-world interleaved generation tasks, and what are the next steps to further improve these scores?\n4. Can the authors provide quantitative metrics (e.g., average inference time increase, memory usage) for the IUT-Plug module when integrated into a VLM-T2I pipeline? This would support the claim of being \"lightweight.\"\n5. How does the IUT handle complex logical inferences, abstract concepts, or scenarios with significant ambiguity? What are the known failure modes of the IUT extraction or representation itself?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XZJRJ2qqsC", "forum": "s6rmpt3DWe", "replyto": "s6rmpt3DWe", "signatures": ["ICLR.cc/2026/Conference/Submission19636/Reviewer_1EzQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19636/Reviewer_1EzQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932817416, "cdate": 1761932817416, "tmdate": 1762931486848, "mdate": 1762931486848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "IUT-Plug is a novel plug-in module designed to tackle multimodal context drift in interleaved image-text generation, where models often fail to maintain logical consistency, object identities, and stylistic coherence across combined visual and textual outputs. The approach introduces an Image Understanding Tree – a hierarchical symbolic representation of the visual scene – which is integrated into existing Vision-Language Model pipelines to provide explicit structured reasoning and enforce consistency constraints on the generation process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a lightweight, modular plug-in that can be attached to existing VLM+T2I pipelines without retraining or architectural changes.\n2. The authors propose a dynamic, semantics-focused evaluation framework as a key contribution. Instead of relying on coarse metrics like FID or CLIP score, they generate custom consistency questions for each test case and use a fine-tuned VLM to score yes/no answers, achieving much higher agreement with human evaluators (87.6% vs ~55% for static baselines) in judging style, logic, and entity consistency.\n3. The paper provides strong experimental evidence that IUT-Plug yields tangible improvements on challenging interleaved generation tasks. Results on both the new 3,000-pair benchmark and public datasets."}, "weaknesses": {"value": "1. The proposed solution introduces a complex pipeline with multiple large-scale components (a scene parser, an LLM prompt generator, a text-to-image model, and a custom evaluator), which could hinder reproducibility and practical deployment.\n2. IUT-Plug’s performance is contingent on the quality of its visual scene understanding – an error in the IUT extraction (e.g. a missed or misidentified object or relationship) could propagate incorrect constraints to the generation stage.\n3. The experimental results, though promising on consistency, focus mainly on the proposed criteria (style/logic/entity consistency) and QA accuracy. There is less discussion on other aspects of output quality (e.g. image realism or linguistic richness) and no user study to confirm human preference."}, "questions": {"value": "1. How well would IUT-Plug generalize to other models or domains beyond those tested? For example, could the plug-in be readily used with a different VLM (like GPT-4’s vision capabilities or upcoming multimodal models) and on tasks such as interactive storytelling or dialog, and if so, would any modifications be needed to maintain its effectiveness?\n2. What is the runtime overhead of inserting the IUT-Plug into the pipeline? \n3. The use of GPT-5 for criterion generation is ambitious – did the authors consider using GPT-4 or an open-source model for this, and what was the impact on criterion quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xafK8hQbFJ", "forum": "s6rmpt3DWe", "replyto": "s6rmpt3DWe", "signatures": ["ICLR.cc/2026/Conference/Submission19636/Reviewer_ZxUT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19636/Reviewer_ZxUT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980915509, "cdate": 1761980915509, "tmdate": 1762931486022, "mdate": 1762931486022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}