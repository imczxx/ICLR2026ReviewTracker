{"id": "7L7kmHHfgf", "number": 8569, "cdate": 1758091253171, "mdate": 1759897775955, "content": {"title": "PIRN: Prototypical-based Intra-modal Reconstruction with Normality Communication for Multi-modal Anomaly Detection.", "abstract": "Unsupervised Multimodal anomaly detection (MAD) — identifying defects by jointly analyzing RGB images and 3D data — is crucial for quality control in manufacturing. However, existing MAD methods struggle when only a few normal samples are available. Cross-modal alignment models fail to learn stable correspondences with scarce training data, and memory-based approaches misclassify any unseen normal variation as anomalous.To addresses the few-shot challenge, we introduce PIRN (Prototypical-based Intra-modal Reconstruction with Normality Communication for Multi-modal Anomaly Detection.), a prototype-based intra-modal reconstruction framework with explicit cross-modal knowledge transfer. PRINC features three key innovations: (1) Balanced Prototype Assignment (BPA) formulates token‑to‑prototype routing as a balanced optimal‑transport problem, guaranteeing uniform utilisation of all prototypes and preventing codebook collapse.(2) Adaptive Prototype Refinement (APR) treats prototypes as adaptive memory and updates them on‑the‑fly with gated GRU cells driven by optimally‑matched image context, expanding coverage to unseen yet normal variations while suppressing anomalies. (3) Multi‑modal Normality Communication (MNC) exchanges complementary normal cues across modalities via gated cross‑attention. MNC enables one modality to reconstruct its feature map not only from its own prototypes, but also with high-level normal patterns provided by the other modality.Extensive experiments on standard benchmarks demonstrate that PIRN significantly outperforms prior methods, achieving new state-of-the-art results, especially in challenging few-shot scenarios.", "tldr": "", "keywords": ["anomaly detection"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0907c91024584d66410415ea78778621f3111a97.pdf", "supplementary_material": "/attachment/b288879dc382a6b098889f753d581ef3f1a1eaee.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces PIRN, a framework for few-shot multi-modal anomaly detection that leverages a prototype-based intra-modal reconstruction approach. The core contribution lies in three synergistic components: Balanced Prototype Assignment (BPA) to mitigate codebook collapse, Adaptive Prototype Refinement (APR) to handle unseen normal variations at test time, and Multi-modal Normality Communication (MNC) to enable knowledge transfer between modalities. By focusing on reconstructing features from a compact, adaptive codebook of normal patterns, PIRN addresses the limitations of existing methods in data-scarce scenarios and achieves new state-of-the-art performance on benchmark datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a practical problem of few-shot multi-modal anomaly detection in industrial quality control. The proposed prototype-based reconstruction paradigm is an elegant solution.\n- The technical contributions are clearly presented. Each of the three key components (i.e., BPA, APR, MNC) is designed to solve a specific challenge.\n- The experimental evaluation demonstrates significant performance gains over strong baselines across multiple datasets and various few-shot settings. The ablation studies effectively validate the contribution of each proposed module."}, "weaknesses": {"value": "- My main concerns lie in the potential computational complexity and practical deployability of the proposed PIRN framework. The model incorporates multiple computationally intensive steps within each decoder layer, including two iterative optimal transport calculations for BPA and APR, and a graph attention network for MNC. The paper lacks any discussion on inference latency or FLOPs, which is a critical metric for real-world industrial inspection systems that often require high throughput. This omission makes it difficult to assess whether the impressive accuracy comes at the cost of practical usability.\n- The robustness of the APR module relies on the assumption that anomalies are sparse. Its effectiveness might degrade in cases of large-scale anomalies, where the *normal context* itself becomes contaminated.\n- The performance of the model seems highly dependent on several key hyperparameters, most notably the number of prototypes *K*. No sensitivity analysis is provided to show how performance varies with these choices."}, "questions": {"value": "- Have you investigated the failure cases of the APR module when dealing with test samples containing large or pervasive anomalies? Is there a mechanism to prevent prototype corruption in such scenarios?\n- BPA is designed to prevent prototype under-utilization, so I am wondering that is there a risk of learning *redundant* prototypes when *K* is large relative to the few-shot training data, where multiple prototypes capture nearly identical normal patterns?\n- How much does the performance of PIRN depend on the quality of this pre-training (e.g., DINOv2 vs. standard ImageNet pre-training)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O66aAKURCR", "forum": "7L7kmHHfgf", "replyto": "7L7kmHHfgf", "signatures": ["ICLR.cc/2026/Conference/Submission8569/Reviewer_tn1N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8569/Reviewer_tn1N"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760672433000, "cdate": 1760672433000, "tmdate": 1762920423714, "mdate": 1762920423714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a multimodal anomaly detection method (RGB + Surface normals) named PIRN. The method is based on the idea that every image can be reconstructed without anomalies with correctly chosen “prototypes” if it is only trained with normal data. The paper proposes two main ways to improve the baseline method (INP-Former): the first one lies in the improvement of the extracted prototypes (balancing + refinement), and the other lies in combining the prototypes extracted from the RGB image and the surface normals. The method is then evaluated on two popular datasets, MVTec3D and Eyecandies, in both low-shot (the authors use the word few-shot, but that is typically reserved for k <= 8) and full-data regimes. In both regimes, they achieve great results"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Cross-modal prototype interaction is a well-thought-out solution. The results in the ablation study confirm this\n- The model works extremely well in a low-shot scenario."}, "weaknesses": {"value": "- The method fails to compare with some methods that outperform it in the full-shot scenario on MVTec 3D: 3DSR (WACV 24) and TransFusion (ECCV 2024). As these two methods are trained with synthetic anomalies, I would assume they do not work as good as PIRN in the low-shot scenario. I would recommend adding them at least to Table 5.\n- It is unclear how INP-Former is trained for comparison on MVTec 3D. Are both modalities input, or only one?\n- The number of prototypes is not specified anywhere\n- It is unclear how the model performs in scenarios where only RGB or only Surface Normals are available\n- How robust is the model to the choice of the feature extractor? Let us say, what if we used a DINOv3 ViT instead of the “standard” ViT?\n- Computational complexity is not compared to other methods"}, "questions": {"value": "I have several questions. I have sorted them from most problematic to least problematic.\n \n1. What is the performance of PIRN in scenarios where only RGB or only Surface Normals are available?\n2. Does INP-Former get only the RGB as the input or also the surface normals? Is the performance improved if the surface normals are used as input as well?\n3. How much time is required to train PIRN, and what is the inference speed of PIRN? What is the inference speed of the model without APR?\n4. How many prototypes are used?\n5. How does PIRN work with other backbones?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OgnSexnTka", "forum": "7L7kmHHfgf", "replyto": "7L7kmHHfgf", "signatures": ["ICLR.cc/2026/Conference/Submission8569/Reviewer_sM1V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8569/Reviewer_sM1V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761510543529, "cdate": 1761510543529, "tmdate": 1762920423260, "mdate": 1762920423260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing multimodal anomaly detection methods often perform poorly in few-shot scenarios. To address this issue, the paper introduces a prototype-driven, reconstruction-based method. It leverages prototypes to build a codebook that filters out anomalous features in the bottleneck. The method also aligns RGB and depth information, integrating multimodal data into the reconstruction process. Finally, anomalies are localized based on the reconstruction error."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The proposed codebook-based reconstruction approach is well-suited for the few-shot problem. Additionally, the optimal transport strategy makes excellent use of the limited available normal samples.\n+ The idea of having modalities mutually assist in reconstruction is highly novel, and the results show it to be quite effective.\n+ The experiments in both few-shot and all-shot settings effectively demonstrate the method's validity. The ablation study is also thorough and convincing."}, "weaknesses": {"value": "+ The prototype-based reconstruction method is quite similar to the approaches in some previous works [1, 2]. However, these papers were not cited.\n+ The proposed design appears to be quite complex. Its computational complexity seems significantly higher than that of previous reconstruction-based methods. It would be helpful to know if the authors have considered this trade-off.\n+ The paper lacks a clear justification for the choice of the number of decoder layers (2 for few-shot and 8 for all-shot). The authors should consider adding this as a factor in the ablation study.\n+ There is a minor issue of a duplicate citation for the paper \"Revisiting multimodal fusion for 3d anomaly detection from an architectural perspective.\"\n\n[1] Gong, D., Liu, L., Le, V., Saha, B., Mansour, M. R., Venkatesh, S., & Hengel, A. V. D. (2019). Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 1705-1714).\n[2] Guo, H., Ren, L., Fu, J., Wang, Y., Zhang, Z., Lan, C., ... & Hou, X. (2023). Template-guided hierarchical feature restoration for anomaly detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 6447-6458)."}, "questions": {"value": "Please refer to \"weakness\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EuZIh8jH67", "forum": "7L7kmHHfgf", "replyto": "7L7kmHHfgf", "signatures": ["ICLR.cc/2026/Conference/Submission8569/Reviewer_HJaY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8569/Reviewer_HJaY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761736736468, "cdate": 1761736736468, "tmdate": 1762920422847, "mdate": 1762920422847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multimodal anomaly detection framework that builds on recent prototype-learning ideas. Its core concept is similar in spirit to INP-Former but introduces tailored modules for multimodal settings, especially in modeling cross-modal normality and refining normal prototypes. The authors identify three fundamental challenges in multimodal prototype learning and correspondingly design three key modules. The overall structure is clear, the motivation is sound, and the approach achieves strong performance compared to prior alternatives.\n\nHowever, the analysis of the proposed modules is relatively shallow. Without deeper insights into why these modules work and how they contribute, the technical credibility and reproducibility are weakened. Moreover, the experimental setup and comparisons contain unclear details that should be clarified for fairness and rigor."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper explicitly identifies core challenges in multimodal prototype learning and proposes targeted modules to address them. This gives the paper a logical progression.\n\nThe proposed components appear reasonable and well-grounded, with links to existing prototype-learning paradigms.\n\nThe method outperforms other approaches in the reported benchmarks, suggesting its effectiveness in multimodal anomaly detection.\n\nThe model performs well in few-shot scenarios, which is practically valuable for real-world industrial inspection."}, "weaknesses": {"value": "The paper primarily provides quantitative results. There is limited explanation of how or why each module improves performance. This lack of insight weakens the claimed contributions.\n\nThe Balanced Prototype Assignment (BPA) appears conceptually similar to coherence loss in INP-Former. Without clearer distinctions, novelty remains questionable.\n\nSince RGB and normal maps capture complementary information, perfect alignment is unrealistic. The paper should justify why alignment is beneficial and how complementary features are preserved.\n\nIt is mentioned that INP-Former is trained only on RGB, while other methods in Table 1 seem RGB-D-based. This may make the comparison unfair.\n\nFigure 2 is visually unclear and difficult to follow; Figure 4 offers limited insight and could be condensed.\n\nEyecandies and MVTec 3D are largely saturated; broader evaluation (e.g., Real-IAD3) would strengthen the experimental evidence.\n\nIt is unclear whether this issue occurs only in few-shot cases or in general."}, "questions": {"value": "Can the authors clearly explain how Balanced Prototype Assignment differs from the coherence loss in INP-Former and INP-Former++? Are there theoretical or empirical justifications?\n\nSince RGB and normal maps capture shared and distinct characteristics, what is the motivation for aligning their prototypes? Is there a mechanism to preserve modality-specific cues rather than forcing strict alignment?\n\nIn Table 1, is INP-Former trained only with RGB, while others use RGB-D? If so, how is fairness ensured in comparisons?\n\nCan the authors provide deeper interpretation—for example, visualizing the correspondence between learned prototypes and latent features—to explain how prototypes encode normality?\n\nWhy does the method generalize so well in the few-shot regime? Which part of the design contributes most?\n\nDoes this problem occur only under few-shot settings or also in full-data training?\n\nWill the authors evaluate on Real-IAD3 or other emerging multimodal anomaly detection datasets to further validate effectiveness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G5igVWY584", "forum": "7L7kmHHfgf", "replyto": "7L7kmHHfgf", "signatures": ["ICLR.cc/2026/Conference/Submission8569/Reviewer_tAGm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8569/Reviewer_tAGm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832527302, "cdate": 1761832527302, "tmdate": 1762920422274, "mdate": 1762920422274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}