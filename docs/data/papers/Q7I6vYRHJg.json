{"id": "Q7I6vYRHJg", "number": 17551, "cdate": 1758277478811, "mdate": 1763363936016, "content": {"title": "Frictional Q-Learning", "abstract": "We draw an analogy between static friction in classical mechanics and extrapolation error in off-policy reinforcement learning, and use it to formulate a constraint that prevents the policy from drifting toward unsupported actions. In this study, we present Frictional Q-learning, a deep reinforcement learning algorithm for continuous control, which extends batch-constrained reinforcement learning. Our algorithm constrains the agent's action space to encourage behavior similar to that in the replay buffer, while maintaining a distance from the manifold of the orthonormal action space. The constraint preserves the simplicity of batch-constrained, and provides an intuitive physical interpretation of extrapolation error. Empirically, we further demonstrate that our algorithm is robustly trained and achieves competitive performance across standard continuous control benchmarks.", "tldr": "", "keywords": ["Static Friction", "Off-policy Reinforcement Learning", "Batch-Constrained Q-Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/15de559ba2ef891a9f9552b86ba833ae340bc569.pdf", "supplementary_material": "/attachment/7a01bf38e554fba6986a06ba3bdc3969164122cb.zip"}, "replies": [{"content": {"summary": {"value": "The paper investigates extrapolation error in off-policy reinforcement learning, which arises when function approximators (e.g., neural networks) estimate the value of state–action pairs that are insufficiently represented in the replay buffer, or more generally, when there is a distribution mismatch between the behavior policy and the replay data.\n\nBuilding on [1], the authors draw an analogy between extrapolation error and static friction, arguing that mitigating this error is analogous to reducing the effective slope angle of a surface.\n\nThe proposed approach is evaluated on standard Gym MuJoCo benchmark tasks and compared against commonly used baselines for continuous control.\n\n[1] Off-Policy Deep Reinforcement Learning without Exploration; Scott Fujimoto, David Meger, Doina Precup"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of linking numerical value estimates in reinforcement learning to physical concepts—friction in this case—is novel and potentially insightful."}, "weaknesses": {"value": "While the idea of drawing an analogy between extrapolation error and friction is conceptually intriguing, the current presentation of this analogy lacks clarity and a strong motivating rationale. It is not yet clear why such a physical interpretation is necessary or beneficial, nor how exactly the components of extrapolation error correspond to the physical forces described.\n\nAdditionally, I am not fully convinced by the empirical evaluation.\n\nFirst, the learning curves in Figure 2 do not clearly demonstrate a significant advantage of FQL over standard baselines. Even in Humanoid, where FQL attains the highest final performance, its confidence intervals overlap substantially with those of TD3 and SAC, which weakens the claim that it consistently outperforms existing methods. I encourage the authors to broaden their evaluation to include more diverse continuous control tasks, such as those from the DeepMind Control Suite, in order to better highlight the strengths of the proposed approach.\n\nSecond, given that the primary focus is on mitigating extrapolation error, I would expect experiments in an offline RL setting where no additional data collection is permitted. The current experiments appear to be entirely online, where the agent can continuously gather new samples, thereby shifting the data distribution in a way that naturally reduces extrapolation error. Under such an uncontrolled data-collection regime, it is difficult to isolate and properly assess the contribution of the proposed technique."}, "questions": {"value": "The primary question I would like the authors to clarify concerns the theoretical link between static friction and extrapolation error, which appears to be the central contribution of the paper. Although Section 4 offers a narrative description, the underlying mechanics remain unclear to me.\n\nMore specifically, in relation to Section 3.2, what are the exact counterparts of gravitational force ($mg$), static friction ($mg \\sin \\theta$), and the normal force ($mg \\cos \\theta$) in terms of extrapolation error? How should we interpret the angle $\\theta$ in the context of off-policy value estimation? More fundamentally, what justifies this analogy in the first place? In high-dimensional spaces, there are infinitely many vectors orthogonal to a given vector—for instance, $(1,0,0)$ is orthogonal to all vectors of the form $(0,y,z)$. How are the orthogonal high-dimensional actions constructed in the method, and is there a principled reason behind selecting a specific orthogonal direction? What is the conceptual motivation for doing so?\n\nI would appreciate a deeper and more rigorous explanation of this core idea. With greater clarity and justification, I could evaluate the work more objectively and potentially raise my score. Specifically, if the core idea is sound and insightful, I would be inclined to raise my score even if the authors are unable to provide additional empirical results during the rebuttal period."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BKSZ4Wd669", "forum": "Q7I6vYRHJg", "replyto": "Q7I6vYRHJg", "signatures": ["ICLR.cc/2026/Conference/Submission17551/Reviewer_777R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17551/Reviewer_777R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761672015270, "cdate": 1761672015270, "tmdate": 1762927422317, "mdate": 1762927422317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "ODHyi0pnGT", "forum": "Q7I6vYRHJg", "replyto": "Q7I6vYRHJg", "signatures": ["ICLR.cc/2026/Conference/Submission17551/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17551/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763363935188, "cdate": 1763363935188, "tmdate": 1763363935188, "mdate": 1763363935188, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an instantiation of batch-constrained Q learning, where the Q values select actions that are generated from a CVAE that learns to output buffer actions over their synthetic orthogonal counterparts. Like BCQ, constraining updates like this prevents incorporating spurious values in the backup. As I understand, the key algorithmic difference is in how the generative action model is trained, most importantly the contrastive term, but also a total-correlation term. This algorithm performs competitively, though not exceptionally, compared with continuous control baselines."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Contrastive learning is a reasonable way to improve the generative modeling component of BCQ."}, "weaknesses": {"value": "I had an extremely hard time understanding this paper, and I feel some parts of this paper were constructed in bad faith.\n\n**To start off, Section 4.1 is almost word-for-word taken from 4.1 of the original BCQ paper (https://arxiv.org/pdf/1812.02900 top of page 5). It’s only mildly, and confusingly, reworded. Without very clear attribution of results, more than beginning the section with \"BCQ defines\", this is not acceptable.**\n\nIn addition,\n\n* Very central terms are undefined. For example “extrapolation error” is only defined recursively (and confusingly) in Equation 8, and maybe in the appendix. \n* Theorem 5 is very confusing, I don’t really see a proof of the claim, just a long equality? I’m struggling to understand the relationship between Theorem 5 / Remark 1, which are in terms of transition dynamics, with the work above, which is in terms of action distributions.\n* The analogy to friction, to me, was just confusing. What is the relation to “static friction”, and “saturated friction”, and “sliding” to what’s presented? I could not really find a purpose of “angles”, except to motivate orthogonality — nothing like the arctan appears in the algorithm as far as I can tell?\n* It was extremely challenging to understand the actual algorithmic contribution that separated this from BCQ.\n\nEmpirically, the results of this paper do not positively distinguish it from TD3 and SAC, two baselines from 2018. Moreover, it is quite concerning this paper does not compare against BCQ, the method on which it is based. The sentence “FQL outperformed baseline methods across multiple tasks with a large margin” is somewhere between misleading and untruthful.\n\nI am pretty unhappy with the quality of this paper from start to finish."}, "questions": {"value": "I had a very hard time understanding the connection between friction and the algorithm, can you possibly clear this up?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "plr0jmFKID", "forum": "Q7I6vYRHJg", "replyto": "Q7I6vYRHJg", "signatures": ["ICLR.cc/2026/Conference/Submission17551/Reviewer_nFCU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17551/Reviewer_nFCU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878181404, "cdate": 1761878181404, "tmdate": 1762927421729, "mdate": 1762927421729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Frictional Q-learning (FQL), a novel method designed to address the extrapolation error in off-policy \nReinforcement Learning (RL). The central concept draws an analogy to classical mechanics by comparing the extrapolation \nerror to static friction. In this analogy, unsupported state-action pairs in off-policy RL are described as high-friction \nregions, hindering the policy's convergence to the optimal policy of the true Markov Decision Process (MDP). \nFQL achieves this by simultaneously pulling the policy towards the replay buffer's data distribution while pushing it away from a \nheterogeneous state-action distribution constructed from orthogonal actions. A state-conditioned contrastive VAE (cVAE)\nis employed to learn the action distribution within the replay buffer and to generate candidate actions aligned with buffer data.\nEmpirically, FQL is evaluated on Gymnasium benchmarks, showing competitive performance against standard baselines such \nas SAC and TD3."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper's overall presentation is well-structured, making the core algorithmic ideas relatively easy to follow.\nThe paper provides a robust theoretical foundation, including derivations and proofs demonstrating that FQL can converge \nto the true MDP under specified conditions and that extrapolation error can be controlled. The mathematical \nformulations are presented clearly."}, "weaknesses": {"value": "- **Conceptual Justification Deficit of the Static Friction Analogy:** While the analogy to static friction is novel \nand provides an intuitive framing, its mathematical grounding and necessity beyond a conceptual metaphor remain tenuous. \nThe paper heavily leverages this analogy as its main storyline, yet the precise, fundamental mathematical relationship \nbetween the physical concept and the RL problem is not fully established. The authors should more explicitly justify how \nthis analogy directly informs the algorithm's design choices. Is it merely illustrative, or does it offer unique insights \nthat guide the formal development of FQL? Without this, the analogy is perceived as more \"hand-wavy\" than foundational.\n- **Ambiguity and Justification of Orthogonal Actions:** The paper utilizes \"orthonormal actions\" to construct a \nheterogeneous state-action distribution, which is central to the \"pushing away\" aspect of FQL. However, the specific \nrationale for choosing Euclidean orthogonality as a measure/proxy for \"heterogeneous\" or \"unsupported\" actions requires \nmore rigorous justification. Are actions orthogonal in Euclidean space inherently \"heterogeneous\" or \"unsupported\" in \nthe context of learned policies and state-action distributions? The paper should provide a clearer explanation of why \nthis specific definition of orthogonality is appropriate for identifying out-of-distribution regions relevant to\nextrapolation error. Furthermore, the paper claims the cVAE with orthogonal actions is beneficial compared to a standard \nVAE (as used in BCQ [1]). A detailed explanation is needed on how these orthogonal actions specifically contribute to this \nbenefit. What precise mechanism do they employ to accelerate convergence or enhance stability that a standard VAE, \npotentially with a perturbation model, cannot achieve?\n- **Error in Algorithm 1**: There is an error in the formulation of the targets $y_{i+1}$ in Algorithm 1. The reward $r_t$\nof the transition is not used.\n- **Limited and Potentially Overstated Experimental Results:** The experimental evaluation is conducted on only five \nGymnasium environments, which is a relatively small set for a method claiming broad applicability and state-of-the-art \nperformance. The statement that \"FQL outperformed baseline methods across multiple tasks with a large margin\" appears to\nbe an overstatement given the results presented (e.g., FQL is often \"on par\" or only marginally better than strong \nbaselines like SAC and TD3, and sometimes underperforms, as noted in the text for HalfCheetah-v4). To strengthen the \nempirical validation, the authors should consider evaluating against more modern off-policy approaches, such as recent \nadvancements in distributional Q-learning [2] and its implementations like FastTD3 [3], to ensure FQL's standing \nagainst the current state-of-the-art.\n- **Incomplete Experimental Setting and Comparison to BCQ:** FQL aims to address extrapolation error in off-policy RL. \nHowever, the experimental setup, where the replay buffer is constantly updated, might not fully stress-test the \nalgorithm's ability to handle severe extrapolation error, especially in contrast to offline RL settings with fixed \nbuffers (which BCQ [1] explicitly evaluates). A direct experimental comparison of FQL with BCQ [1], particularly in \nsettings designed to highlight extrapolation error (e.g., fixed datasets or datasets with significant distributional shift), \nis absent. Such a comparison would be crucial for validating FQL's claimed advantages over its direct conceptual predecessor.\n\n[1] Fujimoto et al., 'Off-Policy Deep Reinforcement Learning without Exploration'\n[2] Bellemare et al., 'A Distributional Perspective on Reinforcement Learning'\n[3] Seo et al., 'FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control'"}, "questions": {"value": "See my comments under Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QkWMlaIRdj", "forum": "Q7I6vYRHJg", "replyto": "Q7I6vYRHJg", "signatures": ["ICLR.cc/2026/Conference/Submission17551/Reviewer_m6NB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17551/Reviewer_m6NB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940329348, "cdate": 1761940329348, "tmdate": 1762927421209, "mdate": 1762927421209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a new off-policy deep RL algorithm \"Frictional Q-learning\". This algorithm builds on Batch-Constrained Q-learning by Fujimoto et al. (2019). A major challenge in off-policy RL is the distributional shift between the replay buffer and the policy. The authors take inspiration from classical mechanics to interpret the constraint of staying close to the buffer as a type of static friction."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The idea and the method described in the paper are interesting and novel. The authors include a large background section, including a discussion of previous physics-inspired RL, and back up their experimental findings with ablation studies."}, "weaknesses": {"value": "While I am not deeply familiar with the offline and off-policy deep RL literature, I am no novice to deep RL. Nothing in the background section of this paper was particularly new to me. Nevertheless, beginning in section 4 (\"Algorithm\"), I simply cannot follow the text. It might be that it is written with a different community in mind, maybe people more familiar with offline RL have less trouble here. In this case, the background section should be updated to reflect the necessary prerequisites. For example, the discussion leading to equation (7) focuses on the \"exploration error $\\mathcal E$\". While secion 3.1 describes sources of exploration error, $\\mathcal E$ is never defined, and so I cannot parse equation (7). This pattern continues, and so I could not understand what the actual algorithm (FQL) is. If this paper is meant to be understood by the general RL community, the writing needs to be improved considerably. Based on this, I recommend rejection with low confidence.\n\nAs I had trouble understanding, I looked up the main prior work, \"Off-Policy Deep Reinforcement Learning without Exploration\" by Fujimoto, Meger, and Precup (2019). Reading through the background section in that paper, it became obvious that the authors took heavy inspiration from this text. Many parts of the background section, as well as the \"Batch Constrained Q-learning\" section are slightly reworded copies of sentences from Fujimoto et al. Exact correspondences are equations (4), (5), Theorems 1 to 4, etc. Since this is technically background material, it is of course fine to include prior results. However, the writing is nearly identical in many parts, with slight modifications that to me suggest that the authors wanted to make it more difficult to identify. In parts the copying from Fujimoto et al. even leads to errors, such as the sentence leading into equation (5), which to me made no sense at first (the sentence is about \"reweighting\", but there is no reweighting  in the equation). However, in Fujimoto et al., the same equation describes \"equal weighting\", with reweighting discussed in the paragraph following this equation. Based on these observations, I have flagged the submission for an ethics review.\n\nFinally, there are many minor and easily fixable mistakes, most obvious perhaps the inclusion of \"conference submissions\" in the title, and Figure 1 (III), which shows an unphysical situation (since $f_s \\leq \\mu_s N$ and $N = 0$). Any situation like this cannot be described as \"friction\". Here, I also do not understand why Coulomb (1821) is cited to assert that $mg = mg$.\n\nI cannot comment profoundly on the actual contribution of this work, since I could not understand the algorithm or its motivation."}, "questions": {"value": "See weakness section."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "See weaknesses section, paragraph 2. It appears that large parts of the background section of the paper are taken from Fujimoto et al. (2019) and reworded."}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hQAVWGZqin", "forum": "Q7I6vYRHJg", "replyto": "Q7I6vYRHJg", "signatures": ["ICLR.cc/2026/Conference/Submission17551/Reviewer_Kgtz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17551/Reviewer_Kgtz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762536378355, "cdate": 1762536378355, "tmdate": 1762927420513, "mdate": 1762927420513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}