{"id": "zXu7faqHCj", "number": 11102, "cdate": 1758189324274, "mdate": 1759897608643, "content": {"title": "DogRot: Taming Highly Ill-Conditioned Sensing Matrix in Sparse Signal Recovery by Random Gaussian Rotator", "abstract": "Recovering sparse signal from an undetermined system, known as compressing sensing (CS), has been a topic with longstanding interests in many signal processing and machine learning applications. A sensing matrix with low inter-column coherence is fundamental to the identifiability of CS. In many real-world problems (e.g., magnetic resonance imaging reconstruction and genetic disease classification) however, relative sensing matrices could be extremely `fat', and naturally contain many proportional columns. Solving the resultant CS problems is notoriously fragile. This work aims to address a family of CS problems induced by such ill-conditioned sensing matrices. We propose DogRot, a plug-and-play preconditioner constructed from a designated diagonal-dominant Gaussian random rotator. Intuitively, DogRot reshapes the sensing matrix to lower its mutual coherence while preserving the sparse solution set, thereby strengthening identifiability. We rigorously establish these properties in theory and validate them extensively in practice. As a lightweight and easily integrable preconditioner, DogRot can be seamlessly combined with existing sparse recovery algorithms. Across diverse applications, our experiments show that DogRot consistently reduces mutual coherence and effectively improves the quality of sparse signal recovery.", "tldr": "", "keywords": ["sparse signal recovery", "compressed sensing", "sensing matrix preconditioning", "random Gaussian rotator"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/846ef9ce74e87b0cf92b6ae17f3880c210bd2121.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a preconditioned for severely ill posed matrices in compressed sensing to improve recover performance. The preconditioned is randomized."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The numerical results are compelling in terms of the improvements on matrix metrics and performance."}, "weaknesses": {"value": "The paper imputes changes to the signal $x$ to $z = Q^{-1} x$ but does not account for the mismatch in performance in recovery between the two. The latter is less sparse and may not yield the same recovery performance as the former, so in practice there is a tradeoff between the improvements in the CS matrix and the distortion of the recoverable signal.\n\nThe problem instantiated in (3) makes the preconditioning specific to the support of the signal $x$.\n\nAlthough the paper mentions the notion of measuring either recovery of the signal or its support, there is no such evaluation in the experimental results.\n\nThe results on performance are focused on a single matrix aspect ratio. \n\nThe experimental results are somewhat underwhelming in their extent (six images). It is not clear why the large number of additional images from the same Kaggle dataset were not used; other resources like https://openneuro.org can provide larger datasets for validation."}, "questions": {"value": "In line 91, what is $\\hat{\\mathbf A}$? Is it the column-normalized version of $\\mathbf A$ that is defined in page 7? What is $G_A$? (The Gram matrix of $A$?)\n\nWhy can you safely assume in (8) that $Q^{-1} \\approx c\\cdot Q^T$? This does not necessarily hold in non-asymptotic regimes (Corollary 3.5).\n\nIn the right hand side of (9), should $z_j$ be $x_j$? Is an $x_j$ missing from the bracketed expression?\n\nHow does one obtain the  \"appropriate threshold\" of line 293?\n\nWhy is $\\mathbf PA$ not included as a baseline in the comparison of Table 5?\n\nWhy is EMD a good distance to use in Section F.1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2uJVIu1WO2", "forum": "zXu7faqHCj", "replyto": "zXu7faqHCj", "signatures": ["ICLR.cc/2026/Conference/Submission11102/Reviewer_qyrv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11102/Reviewer_qyrv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876087898, "cdate": 1761876087898, "tmdate": 1762922278814, "mdate": 1762922278814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles sparse recovery problems where the sensing matrix is highly ill-conditioned—characterized by very high aspect ratios ($p\\ll n$), heavy-tailed inter-column correlations, and near-proportional columns—conditions that severely violate RIP and render standard compressed sensing (CS) fragile. The authors propose DogRot, a plug-and-play right preconditioner $Q$, applied as $y = A Q Q^{-1} x + n$ with the effective sensing matrix $AQ$. DogRot is constructed as $I$ plus a small-variance, skew-symmetric Gaussian component, making it near-orthogonal and invertible almost surely.\n\nMain claims and results:\n\nTheory: 1) Invertibility (almost sure) of $Q$. 2) Column norm concentration and asymptotic orthogonality of $Q$’s columns, leading to $Q^T Q=(\\epsilon (n-1)+1) I$ for large $n$. 3) A concentration bound showing that mutual coherence of $AQ$ contracts relative to that of $A$ by a factor $< 1$, with tail bounds that improve with $n$ and $\\epsilon$. 4) Analysis of sparsity preservation: although $Q^{-1}x$ is no longer exactly sparse, hard-thresholding recovers the support with high probability for appropriately chosen variance $\\epsilon$ and threshold $t$, trading off decorrelation and amplitude fidelity.\n\nImplementation: A simple algorithm that normalizes $A$, mixes with $Q$, runs any standard sparse recovery routine, and inverts with $Q^{T} $ (as an approximation for $Q^{-1}$) plus hard-thresholding.\n\nEmpirics: 1) MRI reconstruction: mutual coherence reductions are substantial (e.g., from $0.99$ to $0.33$ in Cartesian sampling), translating into consistent PSNR gains ($1–3$ dB) for ISTA/ADMM under $20–30$% sampling, both Cartesian and radial. 2) Localized Statistical Channel Modeling (LSCM): On extreme fat matrices (e.g., $40\\times 6552$), DogRot markedly reduces mutual coherence and improves APS estimation as measured by NMSE and Earth Mover’s Distance (EMD), with further gains when combined with conventional projectors. 3) Gene-based disease classification: On the $7129$-gene leukemia dataset with very few samples, DogRot reduces MC and improves classification accuracy for LASSO and elastic net, with larger gains in low-sample regimes.\n\nOverall, DogRot is a lightweight, easily integrable preconditioner that consistently reduces mutual coherence and improves support recovery and reconstruction quality across applications."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: 1) Reframes preconditioning for CS with highly ill-conditioned matrices by right-multiplying with a randomized, diagonal-dominant near-rotation (as opposed to the more common left-projectors). 2) Introduces a structurally simple but theoretically analyzable class of mixers (skew-symmetric Gaussian perturbations of identity), bridging random matrix intuition with CS identifiability via mutual coherence. 3) Focuses explicitly on the “HIM” regime (heavy-tailed inter-column correlations, extreme aspect ratio), where many standard MC-reduction techniques degrade or fail.\n\nQuality: 1) Provides clear theoretical properties: invertibility, norm concentration, asymptotic orthogonality, and coherence contraction with explicit probabilistic bounds. The analysis acknowledges dependencies and leverages sub-exponential concentration and a Delta-method approximation for normalized inner products. 2) Offers a practical and low-overhead algorithmic pipeline; complexity is dominated by a single matrix multiply with $A$ and an $O(n^2)$ mixing/demixing step. 3) Experiments span diverse domains (MRI, LSCM, genomics), reinforcing generality. The reported MC reductions are large and stable; performance metrics (PSNR, NMSE, EMD, accuracy) consistently improve.\n\nClarity: 1) The HIM setting is well-motivated and crisply defined (aspect ratio, size, MC, heavy-tail behavior). 2) The trade-off between decorrelation (σ larger) and amplitude fidelity/support preservation (σ smaller) is explained with guidance on threshold selection. 3) Algorithmic steps are succinct; the role of normalization and post-thresholding is explicit.\n\nSignificance: 1) Addresses a practical and common pain point: fat, highly correlated sensing matrices in real systems (MRI aliasing, beam similarity in channel estimation, gene expression datasets). 2) The plug-and-play nature means immediate applicability with existing solvers; also composable with existing projectors for additive gains. 3) Potential to expand reliable recovery regimes without redesigning the core solver stack."}, "weaknesses": {"value": "Approximation of $Q^{-1}$ by $Q^T$: 1) Theoretical arguments for using $Q^T$ as an approximation rely on asymptotic orthogonality ($Q^TQ\\approx cI$). In finite $n$ and moderate $\\epsilon$, the quality of this approximation can vary. The paper would benefit from quantitative finite-sample error bounds on using $Q^T$ instead of the exact $Q^{-1}$, and ablation showing the sensitivity of performance to this approximation.\n\nParameter selection ($\\epsilon$, threshold $t$): While the paper provides qualitative guidance and a conservative threshold based on Gaussian percentiles, there is limited principled methodology for choosing $\\epsilon$ across tasks or adapting it to $A$’s statistics (e.g., mutual coherence or spectrum). An automatic, data-driven selection rule (or cross-validation protocol) would improve usability. \n\nMutual coherence focus: MC is a surrogate for identifiability, but not always fully predictive of recovery performance. The theoretical results do not tie directly to support recovery guarantees for specific algorithms (e.g., OMP/ISTA) under noise, nor to more modern measures (e.g., RIP). Stronger links or bounds (even if conservative) would strengthen the theoretical contribution.\n\nSupport recovery claims rely on hard-thresholding: 1) The claim that hard-thresholding recovers support depends on unknown magnitudes and $\\epsilon$. The paper does not provide a formal support recovery theorem with explicit SNR or minimum-separation conditions (e.g., $\\min |x_j|$ vs. $\\epsilon$, $n$, mutual coherence). This is especially relevant under noise and model mismatch.\n\nExperimental scope and reporting details: 1) MRI: Results are promising but mostly reported as PSNR; structural similarity (SSIM) and visual artifact metrics would complement. Sensitivity to sampling masks beyond Cartesian/radial (e.g., Poisson-disc, variable-density) would broaden evidence. 2) LSCM: NMSE values remain high (acknowledged). More analysis disentangling index recovery vs. magnitude recovery would be valuable (e.g., top-$k$ support precision). 3) Genomics: While accuracy improves, additional interpretability checks (e.g., stability of selected genes across seeds, overlap with known biomarkers) would bolster the application impact.\n\nRobustness and randomness: 1) DogRot is stochastic. Although the mutual coherence reduction’s variance is small (~0.01), it would be useful to report performance variance across multiple $Q$ draws, especially for downstream metrics (PSNR, NMSE, accuracy), and discuss whether multiple draws or ensembles can further stabilize results."}, "questions": {"value": "Finite-sample analysis of $Q$ approximation: Can you provide explicit finite-$n$ bounds on the deviation of $Q^T$ from $Q^{-1}$ and quantify how this impacts reconstruction error? For practical $n$ (e.g., 1k–10k), what is the expected condition number of $Q$ and how does it scale with $\\epsilon$?\n\nParameter selection: 1) Could you propose a practical, automatic rule for $\\epsilon$ (e.g., function of $\\mu(A)$, spectral decay, or empirical Gram statistics)? Would a simple line-search over $\\epsilon$ using an objective be effective and cheap? 2) For threshold $t$, beyond the $2.58\\sqrt{\\epsilon}\\|x\\|_2$ bound, can you suggest a data-driven surrogate that does not depend on $x$ (e.g., based on noise level estimates or residual statistics)?\n\nStronger theoretical guarantees beyond mutual coherence:  1) Can you give algorithm-specific (e.g., OMP/ISTA) sufficient conditions after mixing that directly relate to $\\epsilon$? 2) Is it possible to characterize how DogRot affects the restricted isometry constants (RIC) or the nullspace property in expectation?\n\nNoise robustness and SNR: How does measurement noise $n$ interact with the mixing and thresholding? Can you provide empirical curves of recovery error vs. SNR with and without DogRot, and guidance on adjusting $\\epsilon$ and $t$ under noise?\n\nRandomness and reproducibility: How sensitive are results to the random seed of $Q$? Would averaging reconstructions over a small ensemble of DogRot draws further stabilize or improve results?\n\nComputational overhead in large-scale settings: 1) In extreme $n$ (e.g., $\\geq 10^5$), $O(n^2)$ mixing can be heavy. Do you foresee structured DogRot variants (block-diagonal, banded skew-symmetric, or fast transforms) to reduce computational and memory cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LYseXCJ7Po", "forum": "zXu7faqHCj", "replyto": "zXu7faqHCj", "signatures": ["ICLR.cc/2026/Conference/Submission11102/Reviewer_J23z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11102/Reviewer_J23z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911789770, "cdate": 1761911789770, "tmdate": 1762922278092, "mdate": 1762922278092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A random precondition is introduced to deal with the high coherence in the measurement matrix for compressed sensing, and detailed analysis is provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "New random scheme for reduce the mutual coherence of a CS matrix."}, "weaknesses": {"value": "Overall an interesting paper."}, "questions": {"value": "* Any more intuition behind the construction of $Q_{\\epsilon,n}$?\n* I didn't quite get whether exact recovery is still possible after the precondition?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "teLOwOcuFe", "forum": "zXu7faqHCj", "replyto": "zXu7faqHCj", "signatures": ["ICLR.cc/2026/Conference/Submission11102/Reviewer_VhLj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11102/Reviewer_VhLj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762162253437, "cdate": 1762162253437, "tmdate": 1762922277330, "mdate": 1762922277330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}