{"id": "ion4VYJWvo", "number": 11884, "cdate": 1758204483471, "mdate": 1759897548870, "content": {"title": "On the Tension Between Optimality and Adversarial Robustness in Policy Optimization", "abstract": "Achieving optimality and adversarial robustness in deep reinforcement learning has long been regarded as conflicting goals. Nonetheless, recent theoretical insights presented in CAR suggest a potential alignment, raising the important question of how to realize this in practice.\nThis paper first identifies a key gap between theory and practice by comparing standard policy optimization (SPO) and adversarially robust policy optimization (ARPO). Although they share theoretical consistency, *a fundamental tension between robustness and optimality arises in practical policy gradient methods*. SPO tends toward convergence to vulnerable first-order stationary policies (FOSPs) with strong natural performance, whereas ARPO typically favors more robust FOSPs at the expense of reduced returns. Furthermore, we attribute this tradeoff to the *reshaping effect of the strongest adversaries* in ARPO, which significantly complicates the global landscape by inducing *deceptive sticky FOSPs*. This improves robustness but makes navigation more challenging. To alleviate this, we develop the *BARPO*, a bilevel framework unifying SPO and ARPO by modulating adversary strength, thereby facilitating navigability while preserving global optima. Extensive empirical results demonstrate that BARPO consistently outperforms vanilla ARPO, providing a practical approach to reconcile theoretical and empirical performance.", "tldr": "This paper demystifies the gap between theory and practice in adversarially robust reinforcement learning and address this by proposing BARPO, a novel framework that bridges adversarial robustness and optimality.", "keywords": ["reinforcement learning", "adversarial robustness", "policy optimization", "theory-practice gap", "bilevel optimization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8eb4986ffcebad4e05f577ace2fd98c0bd486bf7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the longstanding tradeoff between optimality and adversarial robustness in deep reinforcement learning (DRL). The authors find that, in practice, standard policy optimization (SPO) and adversarially robust policy optimization (ARPO) still exhibit a tension: SPO yields high returns but is fragile to perturbations, while ARPO improves robustness but suffers large performance drops. To address this, the authors propose BARPO (Bilevel Adversarially Robust Policy Optimization), a bilevel framework that modulates adversary strength to preserve robustness while improving landscape smoothness and navigability. BARPO generalizes SPO and ARPO as limiting cases. Theoretical analyses show convergence to FOSPs and introduce a KL-based surrogate for the adversary. Experiments on MuJoCo continuous-control tasks demonstrate that BARPO outperforms ARPO and strong baselines in both natural and robust returns."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles a central and underexplored issue with the trade-off between robustness and optimality, offering a novel and intuitive geometric explanation for the empirical gap between theory and practice.\n- Strong theoretical support and comprehensive empirical results.\n- Figures illustrating optimization landscapes (e.g., SPO vs. ARPO vs. BARPO) are intuitive and effectively convey the main ideas."}, "weaknesses": {"value": "- The bilevel formulation and KL-based inner optimization introduce extra hyper-parameters  and computational cost. An explicit runtime and sensitivity analysis would strengthen the empirical case.\n- Robust RL includes several major paradigms (e.g., distributionally robust MDPs, risk-sensitive RL, adversarial policy training). Could the authors explicitly clarify how BARPO relates to these categories? In particular, how does it extend or differ conceptually and algorithmically from prior adversarial training methods such as [1] and its latest extensions like [2]? A concise comparative discussion (e.g., in the related work) would make the contribution and novelty more evident.\n\n#### [1] Pineto et al., \"Robust Adversarial Reinforcement Learning.\" ICML 2017\n#### [2] Dong et al., \"Variational Adversarial Training Towards Policies with Improved Robustness.\" AISTATS 2025"}, "questions": {"value": "- Could BARPO and all the analysis between optimality and robustness extend to other adversarial settings instead of limiting to state perturbation?\n- Although BARPO seems promising from the empirical results compared with all the baselines, those baselines may not be the state-of-the-art, where the latest one was published in 2022. Compare with more recent work may make this work more convincing. For example, [1] is also mentioned in the related work while no direct comparison in the experiments.\n\n#### [1]  Liang et al., \"Game-theoretic robust reinforcement learning handles temporally-coupled perturbations.\" ICLR 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "e3JaV9i5GO", "forum": "ion4VYJWvo", "replyto": "ion4VYJWvo", "signatures": ["ICLR.cc/2026/Conference/Submission11884/Reviewer_RuhM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11884/Reviewer_RuhM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761527784680, "cdate": 1761527784680, "tmdate": 1762922898538, "mdate": 1762922898538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is mathematically sound and shows that ARPO can converge to first-order stationary policies (FOSPs) rather than global optima. It also empirically explains FOSP for both SPO and ARPO. To address these issues, the authors introduce BARPO, a bi-level variant that reshapes the optimization landscape and yields improved robustness in practice."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The theoretical development is clear, and the paper is well written. \n\n+ The paper explains the FOSPs in SPO and ARPO and motivates a unified and clear way.\n\n+ On MuJoCo benchmarks, BAR-PPO shows strong natural and robust performance and adding SPO guidance improves clean returns."}, "weaknesses": {"value": "--"}, "questions": {"value": "- Is the BARPO also robust to various attacks? Are the $\\varepsilon$ budgets predefined per task, and would changing $\\varepsilon$ alter the reported metrics and the ranking between methods?  \n\n- How sensitive are results to the number of inner steps, the step size, and the inner optimizer used to solve the adversary? \n\n- Table 3 shows BARPO with SPO guidance outperforming ARPO with guidance. Quantitatively, how does BARPO with guidance compare against the strongest baselines in Table 2 under the same attack settings? \n\n- Why use SPO guidance in BAR-PPO? What would happen if you instead added an ARPO-style guidance term?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "--"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "99Itc8Ibfr", "forum": "ion4VYJWvo", "replyto": "ion4VYJWvo", "signatures": ["ICLR.cc/2026/Conference/Submission11884/Reviewer_LLxi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11884/Reviewer_LLxi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857413598, "cdate": 1761857413598, "tmdate": 1762922898226, "mdate": 1762922898226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work investigates a conflict between theoretical and practical consistency between reinforcement learning (RL) policies' optimality and adversarial robustness. A prior work proposes the concept of ISA-MDP, where the optimal robust policy (ORP) is the same as the nominal optimal policy under the Bellman optimality. However, in practice, such consistency can rarely be achieved. This works analyzes why the theoretical alignment cannot be practically realized, trying to close the gap. In particular, the authors analyze the optimization dynamic of standard policy optimization (SPO) and adversarially robust policy optimization (ARPO). They first prove that both SPO and ARPO converges to stationary policies rather than global optima. Then, they observe that SPO is vulnerable and ARPO is robust yet yielding significantly lower nominal returns, showing a critical tradeoff between robustness and optimality. To address this challenge, the authors analyze the optimization landscape and value geometry induced by SPO and ARPO. Motivated by their findings, a bilevel optimization framework is proposed to achieve a good balance, achieving robustness without sacrifice of nominal returns."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- To the best of reviewer's knowledge, the convergence analysis for the max-min optimization of ARPO is new. \n- The geometry illustration (Figure 2) in Section 3.2 is informative and helpful.\n- The experiments are comprehensive.\n- The proposed bi-level optimization problem is intuitive (although the motivation and path reaching there are highly confusing)."}, "weaknesses": {"value": "- There is no interpretation or explanation for Proposition 3.1. $V^-$ is undefined. I am not sure why this result is useful. It only states that the gap between the nominal return and the return under attack can be much larger for ARPO than SPO polices. In fact, the whole subsection 3.1.2 feels disconnected from the rest of the work. \n- I expect the author to draw insights from their theoretical analysis in 3.1.1 but they only provide empirical evidence. In addition, the empirical evidence seems to be conflicting with existing works and thus cannot provide strong evidence for the claimed tension between optimality and robustness. \n- While Figure 2 of Section 3.2 is informative, the rest of it is confusing. In particular, $\\Pi_{Robust}$ in proposition 3.2 is undefined. Moreover, the claims of proposition 3.2 is vague. The authors needs to explain the vague statement \"ARPO tends to yield more isolated FOSPs than SPO\" while Proposition 3.2 only proves that there exist an ISA-MDP with a cut point. \n- Figure 3 is very confusing.\n- After Section 3, I don't see a rigorously presented tension between optimality and robustness, contradicting the claim in the introduction that \"We uncover a new form of optimality-robustness tension arising in policy optimization\". I would recommend the authors to consider enhancing the presentation there and make the components of this work more connected. \n- There is a same problem with Theorem 4.1. It appears disconnected from the proposed algorithm. The authors claim that \"we need to specify a suitable inner objective G that both promotes policy learning and maintains strong robustness\" and prove that KL divergence is an effective surrogate. But it is unclear why such surrogate leads to better policy learning and maintains robustness. \n- While Figure 4 is helpful, it is disconnected from the theorem.\n- Overall, the presentation of work needs considerable improvements, preventing the reviewer from evaluating the soundness of the proposed algorithm. While I see the technical contribution of this work, its lack in presentation coherence and smoothness severely undermines the quality of this work."}, "questions": {"value": "- What is $\\theta_0$ in Theorem 3.2?\n- I think in the original ISA-MDP paper[1], the authors' implemented algorithms (CAR-PPO-SGLD/PGD) do not show significant natural/nominal performance drop without attacks, sometimes even outperforming SPO methods (PPO in [1]). This observation is consistent with empirical robust RL works such as [2]. Why the performance of ARPO drops this badly in Table 1? \n- Could you explain what Figure 3 is trying the say?\n- Why using a lower bound in Theorem 4.1 as the surrogate? If the goal is to minimize the gap between the nominal and under-attack returns, then shouldn't we use an upper bound? \n- Where do Figure 4 comes from? Are the values in the figure true objective values (of SPO, ARPO and BARPO) for some RL problems, or are the values only for illustration purposes? \n- Could you please explain in detail the connection between different parts of Section 4.2? Specifically, why \"KL divergence serves as a valid first-order surrogate for minimizing the adversarial value\" can lead to better policy learning? And why \"This surrogate is both appropriate and reliable from an optimization perspective\"? Lastly, why the lower bound in Theorem 4.1 leads to the proposed bi-level optimization problem? The reviewer fails to see the connections. \n- While the proposed bi-level optimization problem is intuitive enough (restricting the perturbed policy to be close to the original policy), why this has the effect of changing land scape? In particular, how can it \"reshapes the landscape by elevating low-value but robust regions\"? I believe the authors state this claim without any justification.   \n\n\n[1] Towards Optimal Adversarial Robust Reinforcement Learning with Infinity Measurement Error\n[2] Robust Adversarial Reinforcement Learning"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ncPFMMhpto", "forum": "ion4VYJWvo", "replyto": "ion4VYJWvo", "signatures": ["ICLR.cc/2026/Conference/Submission11884/Reviewer_wh98"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11884/Reviewer_wh98"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860276627, "cdate": 1761860276627, "tmdate": 1762922897887, "mdate": 1762922897887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduced the Bilevel ARPO (BARPO) that balances the optimality-robustness trade-off, by adjusting the adversary strength to promote traversable optimization paths which smoothed the optimization landscape."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper was well written and easy to follow. Assumptions and motivations were clearly justified.\n* The insights on how the 'valleys' were formed and how to 'bridge' them were novel and interesting.\n* Theoretical analyses were sound.\n* In experiments BARPO was evaluated against extensive attack/robustness conditions."}, "weaknesses": {"value": "* BARPO employs a KL-based surrogate for the inner minimization to approximate the strongest adversary. \n  * While the paper shows that minimizing this surrogate aligns with minimizing the true adversarial value, the reviewer wonders whether the authors have additional theoretical or empirical insights regarding the convergence rate under this surrogate formulation. In particular, are there quantifiable bounds or guarantees on the degree of robustness potentially lost due to the surrogate approximation?\n* From table 8 it showed that the performance could be fairly sensitive to $\\kappa$. \n  * Any findings around how it can be tuned would be appreciated."}, "questions": {"value": "* The experiments mainly focused on continuous control tasks. How does BARPO generalizes to tasks with large discrete action space? Could the authors comment on potential challenges (e.g. combinatorial action spaces, function approximation issues)?\n* Is there a way to tune BARPO (e.g., via a schedule on adversary strength) to balance return vs robustness, for different application needs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "iEdPQftcvW", "forum": "ion4VYJWvo", "replyto": "ion4VYJWvo", "signatures": ["ICLR.cc/2026/Conference/Submission11884/Reviewer_bepT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11884/Reviewer_bepT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762848376402, "cdate": 1762848376402, "tmdate": 1762922897567, "mdate": 1762922897567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}