{"id": "a4e0zoaiD8", "number": 1887, "cdate": 1756958554075, "mdate": 1759898180597, "content": {"title": "Temporal Graph Thumbnail: Robust Representation Learning with Global Evolutionary Skeleton", "abstract": "Temporal graphs are commonly employed as conceptual models for capturing time-evolving interactions in real-world systems. Representation learning on such non-Euclidean data typically depends on aggregating information from neighbors, and the presence of temporal dynamics further complicates this process. However, neighbors often contain noisy information in practice, making the unreliable propagation of knowledge and may even lead to the model failure. Although existing methods employ adaptive spatiotemporal neighbor sampling strategies or temporal dependency modeling frameworks to enhance model robustness, their constrained sampling scope limits handling of severe noise and long-term dependencies. This limitation can be attributed to a fundamental cause: neglecting global evolution inherently overlooks the temporal regularities encoded in continuous dynamics. To address this, we propose the **T**emporal **G**raph **T**humbnail (**TGT**), encapsulating a temporal graph’s global evolutionary skeleton as a thumbnail to characterize temporal regularities and enhance model robustness. Specifically, we model the thumbnail by leveraging von Neumann graph entropy and node mutual information to extract essential evolutionary skeleton from the raw temporal graph, and subsequently use it to guide optimization for model learning. In addition to rigorous theoretical derivation, extensive experiments demonstrate that TGT achieves superior capability and robustness compared to baselines, particularly in rapidly evolving and noisy environments. The code is available at https://anonymous.4open.science/r/TGT-BDF2.", "tldr": "We introduce Temporal Graph Thumbnail (TGT), a robust representation method that captures temporal regularities via von Neumann graph entropy and feature mutual information to guide learning on noisy, rapidly evolving temporal graphs.", "keywords": ["Dynamic Graph Neural Network", "Global Evolution", "Von Neumann Entropy", "Robust Representation Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1fe3330a48931a518919cabe5aeb7a88e0d35903.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents a novel approach for modelling dynamic graph representations, focusing on link prediction, called Temporal Graph Thumbnail (TGT). The authors identify that recent methods often overlook the global evolution of temporal graphs, which limits their ability to capture long-term dependencies effectively. To address this, the paper introduces a method that models thumbnails of the temporal graph using von Neumann graph entropy and node mutual information. This thumbnail captures the global evolutionary trajectory of the graph and is leveraged as an optimization guide to learn robust and generalizable node embeddings. By incorporating global evolution information, TGT significantly enhances robustness and representation quality in dynamic graph learning.\n\nEmpirical experiments demonstrate that TGT surpasses baseline methods both in performance on clean temporal graphs and in robustness under adversarial attacks. These attacks include perturbations on graph structures, node features, and temporal ordering."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **Novelty.** The paper introduces a novel approach that models a thumbnail of a temporal graph using von Neumann graph entropy and incorporates this thumbnail to guide optimization for learning superior and robust node representations.\n\n- **Superior Node Representation.** By leveraging von Neumann entropy to constrain the information bottleneck associated with global structural evolution, TGT outperforms seven strong baseline methods across three diverse datasets.\n\n- **Robust Node Representation.** Modeling the thumbnail and utilizing von Neumann entropy enhances robustness against various adversarial attack settings, including: Random Gaussian noise disrupting node features, Nettack adversarial perturbations on graph structure, and Permuting snapshot chronological order causing temporal disruptions.\n\n- **Ablation Studies.** clearly highlight the contributions of different model components, including evolution constraint, structure and feature mutual information, thumbnail and von Neumann Entropy."}, "weaknesses": {"value": "**W1: Robustness evaluation.** The robustness evaluation of TGT is limited to random and out-of-date adversarial attacks on a static graph. To improve the strength and generality of the evaluation, it would be more convincing to evaluate the robustness of TGT under feature attack methods[1,2,3], recent structure attacks on static graphs [4,5], and adversarial attacks on dynamic graphs [6].\n\n**W2: CTDG vs DTDG.** The paper does not clearly state whether the focus is on continuous-time dynamic graphs (CTDG) or discrete-time dynamic graphs (DTDG). Including a discussion on the differences between these two temporal graph settings and explicitly highlighting which type the work addresses (ideally in Section 2) would benefit clarity and contextualize the contributions.\n\n**W3: TGNN baselines.**  The work compares the performance of TGT against out-of-date baselines. A naive but effective baseline, such as EdgeBank[7], and recent TGNN models[8,9,10].\n\n*Minor*\n\n**W4: Evaluation metrics.** The paper mainly reports AUC and AP for link prediction. Including Mean Reciprocal Rank (MRR) under the Temporal Graph Benchmark (TGB) evaluation settings would provide a more comprehensive view of TGT's ranking performance and enhance comparability with other temporal graph models evaluated on TGB[12].\n\n---\n[1] Zügner, Daniel, Amir Akbarnejad, and Stephan Günnemann. \"Adversarial attacks on neural networks for graph data.\" *Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining*. 2018.\n[2] Chen, Jinyin, et al. \"Fast gradient attack on network embedding.\" *arXiv preprint arXiv:1809.02797* (2018).\n\n[3] Li, Jintang, et al. \"Adversarial attack on large scale graph.\" *IEEE Transactions on Knowledge and Data Engineering* 35.1 (2021): 82-95.\n\n[4] Geisler, Simon, et al. \"Robustness of graph neural networks at scale.\" *Advances in Neural Information Processing Systems* 34 (2021): 7637-7649.\n\n[5] Alom, Zulfikar, et al. \"GOttack: Universal Adversarial Attacks on Graph Neural Networks via Graph Orbits Learning.\" *The Thirteenth International Conference on Learning Representations*. 2025.\n\n[6] Dai, Yue, et al. \"MemFreezing: A Novel Adversarial Attack on Temporal Graph Neural Networks under Limited Future Knowledge.\" *Forty-second International Conference on Machine Learning*. 2025.\n\n[7] Poursafaei, Farimah, et al. \"Towards better evaluation for dynamic link prediction.\" *Advances in Neural Information Processing Systems* 35 (2022): 32928-32941.\n\n[8] Yu, Le, et al. \"Towards better dynamic graph learning: New architecture and unified library.\" *Advances in Neural Information Processing Systems* 36 (2023): 67686-67700.\n\n[9] Lu, Xiaodong, et al. \"Improving temporal link prediction via temporal walk matrix projection.\" *Advances in Neural Information Processing Systems* 37 (2024): 141153-141182.\n\n[10] Ding, Zifeng, et al. \"Dygmamba: Efficiently modeling long-term temporal dependency on continuous-time dynamic graphs with state space models.\" *arXiv preprint arXiv:2408.04713* (2024)."}, "questions": {"value": "- The concept of learning a graph evolution trajectory over time was previously explored in GraphPulse[11]. Could the authors clarify the key differences between TGT and GraphPulse in terms of approach and performance?\n\n- How does TGT scale on large-scale datasets, particularly those in the TGB[12]?\n\n- The paper employs random Gaussian noise as a node feature attack. How robust is TGT against other sophisticated feature attack methods, such as Nettack, PRBCD, SGA, and FGA, as mentioned in the identified weaknesses (See **W1**)?\n\n- Robustness evaluations focus on untargeted attacks, which might be misleading. How often do these attacks realistically perturb nodes or edges involved in link prediction? Targeted attacks on specific nodes and links provide more informative robustness assessments.\n\n- The term “snapshot” is frequently used. Is this research focused on discrete-time dynamic graphs (DTDG)? If so, what is the typical size or timespan of these snapshots?\n\n- How robust are the learned node embeddings from TGT under adversarial attacks on node property prediction tasks as defined within TGB[12]?\n\n---\n[11] Shamsi, Kiarash, et al. \"GraphPulse: Topological representations for temporal graph property prediction.\" *The Twelfth International Conference on Learning Representations*. 2024.\n\n[12] Huang, Shenyang, et al. \"Temporal graph benchmark for machine learning on temporal graphs.\" *Advances in Neural Information Processing Systems* 36 (2023): 2056-2073."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DQh1Hjsp4Y", "forum": "a4e0zoaiD8", "replyto": "a4e0zoaiD8", "signatures": ["ICLR.cc/2026/Conference/Submission1887/Reviewer_rkWZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1887/Reviewer_rkWZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760638072518, "cdate": 1760638072518, "tmdate": 1762915931687, "mdate": 1762915931687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies modeling of discrete-time temporal graphs and facilitates tools from information theory to construct a novel framework called TGT, that simultaneously do temporal graph property prediction as well as temporal information compression in the sense of von Neumann entropy for topological evolution as well as variational approximations to feature-related information measures. Experimental results demonstrate both the effectiveness and the robustness of the proposed framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper reasonably leverage ideas from information theory and proposed an elegant framework that systematically combines sophisticated approaches, with a solid foundation.\n- The empirical results and ablation reasonably verifies the design choice of TGT."}, "weaknesses": {"value": "- While the overall presentation of the paper is good, the proposed framework is nonetheless complicated that would potentially benefit from a more accessible description, for example I would recommend the authors to draw a conceptual markov chain that indicates how the thumbnail compresses multiple graph snapshots, as well as how the thumbnail representation is utilized to guide (or serving as a constrained) in the representation learning phase.\n- See ``questions`` below"}, "questions": {"value": "- The TGT used in the paper relies on GAT as its backbone, I am curious about the dependence on backbones.\n- Yet another recommendation: It would be nice to have a visualization of what those learned thumbnails look like and to what extent, at least in a human-understandable sense, that the learned thumbnails efficiently captures the temporal patterns in the temporal graph datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PfzWkEABgA", "forum": "a4e0zoaiD8", "replyto": "a4e0zoaiD8", "signatures": ["ICLR.cc/2026/Conference/Submission1887/Reviewer_QMEp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1887/Reviewer_QMEp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381728510, "cdate": 1761381728510, "tmdate": 1762915931396, "mdate": 1762915931396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework for learning representations from temporal graphs. The method constructs a thumbnail, which is defined to be a compact static summary graph, and captures the global evolutionary skeleton of a dynamic graph sequence using von Neumann graph entropy and node mutual information. This thumbnail serves as both a model constraint and a denoising guide, and enforces a balance between information compression and representational sufficiency via mutual information bottlenecks. The paper integrates this into a GAT-based architecture and evaluats it on Bitcoin, MathOverflow and MOOC datasets. The results show improvements in both capability and robustness against feature, structural and temporal perturbations compared to other dynamic graph neural networks and variants."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces the TGT method that summarises a temporal graph’s global evolutionary skeleton as a static graph distilled from the sequence, using von Neumann graph entropy for structure and node mutual information for features. This shifts robust temporal GNNs away from purely local neighbor/history modeling toward an explicit global evolution model.\n\n- Beyond modelling the thumbnail, the method introduces thumbnail-guided mutual-information constraints that act as a bottleneck between raw data and task outputs. This couples global evolution with robust representation learning in a nice training objective.\n\t​\n- Experiments are strong and cover the three standard benchmarks of Bitcoin, MathOverflow and MOOC, with diverse evolution frequencies and evaluation spans inductive link prediction under clean data in Table 2 and robustness under feature, structure and temporal perturbations at multiple intensities.\n\n- The paper defines the temporal-graph setting, the thumbnail and assignment matrices and the training components. Research questions guide the narrative and figures/tables are tied to those questions."}, "weaknesses": {"value": "- The contribution remains confined to improving node embeddings for downstream tasks such as link prediction. While the method introduces an interesting modelling layer (the thumbnail), it does not extend this to broader temporal-graph reasoning or applications (e.g., forecasting, anomaly detection, or event prediction) to demonstrate its significance. \n\n- All experiments evaluate link prediction only. While this is a common benchmark, it represents a narrow test of temporal representation robustness. Node classification or dynamic community detection would better demonstrate whether the thumbnail’s global-evolution modelling benefits.\n\n- The ablations test the removal of VNGE and the thumbnail, but there is little exploration of the effects of hyperparameters (e.g., window size, Beta, etc) beyond a brief mention in the appendix."}, "questions": {"value": "- The paper acknowledges that real-time computation of von Neumann graph entropy introduces overhead and limits scalability to large graphs. Is this significant and have the authors run quantitative experiments or have insight to offer on runtime or memory usage compared to other temporal GNN baselines (e.g., TGN or DGIB)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ofj15ctC5S", "forum": "a4e0zoaiD8", "replyto": "a4e0zoaiD8", "signatures": ["ICLR.cc/2026/Conference/Submission1887/Reviewer_RLpw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1887/Reviewer_RLpw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761502404477, "cdate": 1761502404477, "tmdate": 1762915931206, "mdate": 1762915931206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Temporal Graph Thumbnail framework designed to achieve robust representation learning on dynamic graphs. TGT addresses limitations in local aggregation methods by modeling the temporal graph's global evolutionary skeleton as a static thumbnail. The thumbnail is constructed by maximizing the mutual information with the raw sequence, leveraging von Neumann Graph Entropy to capture structural evolution coherently. The $G_T$ is then used as a bottleneck constraint within the Information Bottleneck principle to guide effective denoising and compression. Extensive experiments demonstrate TGT's superior performance and resilience across various adversarial perturbations, particularly against temporal interference."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe introduction of the \"temporal graph thumbnail\" concept, which summarizes the global evolutionary skeleton, is a novel approach for capturing long-term temporal dependencies and regularities often missed by localized aggregation methods.\n\n2.\tThe use of von Neumann Graph Entropy is well-justified due to its unique properties, providing a solid and principled mechanism for characterizing structural dynamics within the Information Bottleneck framework.\n\n3.\tThe empirical section is rigorous, testing robustness against three distinct types of noise at varying intensities. The results convincingly establish TGT's state-of-the-art resilience, particularly against chronological disorder."}, "weaknesses": {"value": "1.\tThe mechanism by which the mapping function accommodates transient or non-persistent nodes across the entire sequence remains unclear, impacting the core interpretability.\n\n2.\tThe total loss (Eq. 16) includes two terms. I think the difference objective function between fidelity and compression is not theoretically resolved, requiring a clearer explanation of the roles of hyperparameters in balancing these constraints.\n\n3.\tThe authors claim a near-linear complexity $O(\\Delta t \\cdot |E_{GT}|)$ for the $L_{evolution}$ calculation. But the approximated VNGE computation involves spectral quantities like $Tr[\\tilde{L}^2]$, which typically necessitates non-linear complexity in $|V_{GT}|$. So I will concern about scalability for large graphs.\n\n4.\tThe sensitivity analysis for the evolutionary constraint parameter $\\alpha$ (Table 7) shows a dramatic drop in clean data performance as $\\alpha$ increases. This high sensitivity makes the parameter practically difficult to tune effectively without prior knowledge of the target data's inherent noise level."}, "questions": {"value": "1.\tPlease elaborate on the mechanism by which the static node set $V_{GT}$ is selected or learned from the dynamically evolving node sets $\\bigcup_i V^i$. How does the mapping function $\\mathcal{F}$ and the Bernoulli sampling (L778) handle nodes that appear and disappear over time, ensuring that persistent, core evolutionary information is captured?\n\n2.\tThe complexity analysis states $L_{evolution}$ is $O(\\Delta t \\cdot |E_{GT}|)$. Since the VNGE approximation requires computing $Tr[\\tilde{L}^2]$ (L958), what structural assumptions or approximation techniques are used to achieve this near-linear complexity?\n\n3.\tSince $L_{evolution}$ maximizes $I(\\mathcal{G}_T; \\mathcal{G})$ and the IB term minimizes $I(\\mathcal{G}; \\mathcal{G}_T)$, what theoretical guidance is used for setting the Lagrange multipliers $\\lambda$ and $\\beta$ to ensure the optimization converges to a non-trivial solution that balances fidelity and compression?\n\n4.\tGiven TGT’s superior robustness against random snapshot permutations, please provide a theoretical explanation for how the VNGE-based constraint $I_s(\\mathcal{G}_T; \\mathcal{G})$ (Eq. 5) ensures the learning process is robust to chronological disorder? Does the VNGE effectively provide a temporal invariant summary that suppresses the impact of sequence tampering?\n\n5.\tGiven the severe sensitivity of clean data performance to $\\alpha$ (Table 7), what noise-aware tuning methodology would you recommend to practitioners for selecting an appropriate value for $\\alpha$ when the ground truth noise characteristics of the target application are unknown?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yjaVvcZf35", "forum": "a4e0zoaiD8", "replyto": "a4e0zoaiD8", "signatures": ["ICLR.cc/2026/Conference/Submission1887/Reviewer_ER5F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1887/Reviewer_ER5F"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817348459, "cdate": 1761817348459, "tmdate": 1762915930962, "mdate": 1762915930962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}