{"id": "RY5cPitjk8", "number": 24378, "cdate": 1758356245749, "mdate": 1759896768907, "content": {"title": "Generalizable Opponent Exploitation in LLM Agents via Mixed Best-Responses Training", "abstract": "Opponent exploitation is a crucial capability for agents in competitive scenarios, allowing them to exploit weaknesses in opponent strategies.\nLarge Language Model (LLM) based agents have demonstrated remarkable capabilities in strategic reasoning and adversarial decision-making. \nHowever, their ability to exploit diverse opponents, including those following suboptimal strategies, remains underexplored.\nIn this work, we introduce \\textbf{GOE-LLM} (Generalizable Opponent Exploitation with LLMs), a novel framework that leverages LLMs to learn opponent exploitation strategies through mixed best-response training in two-player zero-sum games. \nA Multi-Layer Perceptron (MLP) Profiler is pre-trained independently to analyze opponent behaviors and identify their strategic patterns. \nThis profiling information is then utilized by a fine-tuned LLM Exploiter, trained with group relative policy optimization on a curated set of best-response strategies against heterogeneous opponents. \nTo ensure stable training while enabling the resulting agent to generalize across a broad spectrum of opponents, we propose a Mixture-Best-Responses principle to guide the construction of training data.\nWe evaluate GOE-LLM using various LLM sizes in Kuhn Poker, where it demonstrates strong exploitation against out-of-distribution opponents. Additionally, our method shows consistent performance and generalization trends in Leduc Hold'em Poker.\nWe construct and compare different mixtures of training data to validate the effectiveness of the Mixture-Best-Responses principle, confirming its role in ensuring both stability and generalization.\nExtensive ablation studies further validate the contributions of each component to the overall performance. \nOur results highlight the potential of GOE-LLM for generalizable opponent exploitation and demonstrate the effectiveness of mixed best-response training in enhancing the adaptability of LLM agents.", "tldr": "", "keywords": ["LLM Agent", "Opponent Exploitation", "Generalization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fb3104da417ef1bef2c97bc9761777623cec6e52.pdf", "supplementary_material": "/attachment/1bded5fcdc98ce7046bcb382d4e548c1367ac3d3.zip"}, "replies": [{"content": {"summary": {"value": "his paper introduces GOE-LLM (Generalizable Opponent Exploitation with LLMs), a framework aimed at enabling Large Language Model (LLM)-based agents to exploit weaknesses in opponents' strategies within two-player zero-sum imperfect-information games like poker. The approach features a two-tier architecture: (1) a lightweight Multi-Layer Perceptron (MLP) profiler that analyzes recent opponent behaviors to classify strategic patterns and convert them into natural language descriptions, and (2) an LLM exploiter fine-tuned via Group Relative Policy Optimization (GRPO) on a carefully curated mixture of best-response strategies. To promote training stability and generalization, the authors propose a \"Mixture-of-Best-Responses\" principle that avoids non-transitive dominance cycles in the training data. Evaluations on Kuhn Poker and Leduc Hold'em demonstrate GOE-LLM's effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "GOE-LLM's hybrid design—merging an MLP profiler for dynamic opponent modeling with an LLM exploiter—offers a practical, interpretable method to infuse behavioral insights into LLM decision-making, drawing inspiration from game-theoretic concepts like mixture-of-opponents learning.\n\nThe \"Mixture-of-Best-Responses\" principle stands out as a key innovation, providing a formal guideline to balance diversity in training data."}, "weaknesses": {"value": "1. **Omission of Key Related Works**: The paper overlooks relevant literature on LLMs in repeated games, such as Akata et al. (2025) (Playing repeated games with large language models), which explores LLMs' strategic behavior in repeated interactions (e.g., Prisoner's Dilemma variants) and highlights their ability to adapt and exploit over iterations. \n\n2. **Lack of Clarity on Seen vs. Unseen Opponents**: The distinction between \"seen\" (in-training) opponents and \"unseen\" (OOD) ones is not explicitly detailed in the main text, relying on Table 1 for inference. A clearer exposition—perhaps with examples of how the MLP profiler bridges this gap or visualizations of strategy spaces—would improve readability and highlight the generalization claims.\n\n3. **Potential Overfitting to Synthetic Data**: All training data derives from simulated matchups, raising concerns about overfitting to these artificial patterns despite the mixture principle. It remains unclear how the framework would perform with real-game traces or human-player data, which introduce noise, variability, and evolving strategies not captured in rule-based simulations.  \n\n4. **Unclear Mechanism for Avoiding Overfitting via Mixture Principle**: While the Mixture-of-Best-Responses principle is posited to enhance generalization by excluding cyclic counters, the paper does not deeply explain *why* this specifically mitigates overfitting—e.g., through theoretical analysis of strategy diversity or convergence guarantees. Ablations show empirical benefits, but a more formal justification  would clarify its role beyond stability.\n\n5. **Limited Exploration of Larger LLMs and Advanced Baselines**: Evaluations cap at 7B-parameter models, omitting frontier LLMs (e.g., GPT-4 scale), where in-context learning might suffice for opponent exploitation without fine-tuning. Baselines like LLM+OM (vanilla prompting with opponent info) lag behind, but incorporating stronger techniques—such as chain-of-thought prompting or self-reflection from works like Yao et al. (2023b)—could offer a more equitable comparison and assess if GOE-LLM's gains persist at scale.\n\n6. **Questionable Overall Contribution Given Win Rate Metrics**: Although GOE-LLM excels in expected value (chips per hand), its average win rates (Tables 8 and 9) are lower than several baselines (e.g., LLM, GTO, BR) across opponents and games shown in Tables 8 and 9 (see the last column Average).  These results make the contributions weak.\n \nMinor:\n\nFonts size in Figure 3 is too small."}, "questions": {"value": "What is the role of Figures 3 and 4? Both figures were never mentioned."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DSloYqr6pg", "forum": "RY5cPitjk8", "replyto": "RY5cPitjk8", "signatures": ["ICLR.cc/2026/Conference/Submission24378/Reviewer_RXu2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24378/Reviewer_RXu2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760798927689, "cdate": 1760798927689, "tmdate": 1762943063358, "mdate": 1762943063358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GOE-LLM, a novel framework for training LLM agents to generalize opponent exploitation in two-player zero-sum imperfect-information games. Unlike existing LLM-based agents that focus on equilibrium play or prompt engineering, GOE-LLM explicitly targets the ability to exploit opponents. The framework consists of a MLP-based opponent profiler and an LLM exploiter, which is fine-tuned using a Mixture-of-Best-Responses training principle. The policy is optimized using a modified Group Relative Policy Optimization (GRPO) algorithm with an opponent-aware reward structure. Extensive experiments in Kuhn and Leduc Poker show that GOE-LLM outperforms strong baselines across seen and unseen opponents, demonstrating robust exploitation capabilities and effective generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper aims to solve a novel problem, i.e., generalizable opponent exploitation for LLM agents in imperfect-information games, which is largely unaddressed in prior work. \n2. The work is methodologically sound, with well-motivated design choices and rigorous evaluation. The integration of GRPO with a fine-grained, opponent-aware reward structure is technically strong. \n3. The work makes a significant contribution to both the LLM and multi-agent learning communities. It demonstrates how LLM agents can move beyond equilibrium play and adapt to exploit suboptimal opponents, which is crucial for strategic reasoning tasks in real-world applications."}, "weaknesses": {"value": "1. The paper evaluates GOE-LLM exclusively on Kuhn Poker and Leduc Hold’em, which are small-scale, two-player zero-sum games with limited action and state spaces. While these environments are standard testbeds, they do not capture the complexity of real-world strategic interactions or large-scale imperfect-information games.\n2. The MLP Profiler is a key component in the framework, yet the paper does not deeply analyze its accuracy or how profiling errors affect downstream exploitation. For example, misclassifying a bluffing agent as passive might lead to poor decisions by the LLM exploiter.\n3. The training of the profiler assumes access to clean, labeled data from known opponent types. However, in many realistic settings, agents must learn to exploit opponents without such strong supervision."}, "questions": {"value": "1. The framework assumes the profiler can reliably map short-term behavioral traces (e.g., past 10 or 50 games) to meaningful strategy categories. Could the authors share accuracy metrics of the profiler on seen vs. unseen opponents, and discuss how misclassification impacts downstream performance?\n2. The paper defines a non-transitive dominance cycle and mentions selecting mixtures to avoid it. Is this done manually, heuristically, or through some automated graph-based filtering process?\n3. In Table 2, why do some agents outperform BR*? For example, LLM achieves 0.317 (P0) and GOE-LLM 0.212 (P1) against the Random opponent, while BR* only gets 0.175 (P0) and 0.181 (P1). Could the authors explain this discrepancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jmC9qBTt8u", "forum": "RY5cPitjk8", "replyto": "RY5cPitjk8", "signatures": ["ICLR.cc/2026/Conference/Submission24378/Reviewer_Jxaf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24378/Reviewer_Jxaf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883917157, "cdate": 1761883917157, "tmdate": 1762943062992, "mdate": 1762943062992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GOE-LLM, a framework that leverages LLM agents in two-player zero-sum games: an MLP is used to analyze opponent behavior, mixed best-response training is applied to learn opponent-exploitation strategies, and the goal is achieved via fine-tuning the LLM together with prompting. The method is validated on two-player zero-sum games such as Kuhn Poker and Leduc Hold’em, improving the agent’s ability to adapt to and exploit opponents who are suboptimal or deviate from equilibrium play."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper raises a new question: how to perform opponent exploitation with LLMs. Traditional work in game theory and reinforcement learning typically focuses on learning equilibrium strategies or adapting to fixed opponents, whereas this paper seeks to enable LLMs to adapt to and effectively exploit diverse opponent weaknesses, especially those deviating from optimal strategies. This problem statement is innovative.\n\n- The paper evaluates the framework on classic two-player zero-sum poker games—Kuhn Poker and Leduc Hold’em—as testbeds. Through comparisons with multiple baselines, it clearly demonstrates GOE-LLM’s advantages against various opponent strategies. The empirical results and data strongly support the method’s effectiveness.\n\n- The paper carefully explains the design and implementation of the GOE-LLM framework—from opponent modeling to the exploiter’s training process. In particular, the mixed best-response training principle and the opponent-aware reward mechanism are described clearly, making the framework’s working principles easy to follow."}, "weaknesses": {"value": "- Although the MLP Profiler achieves some success, its training data are primarily built around a small set of canonical opponent strategies (e.g., Random, GTO, Bluff). This may limit performance against highly complex or atypical opponents, especially in dynamic real-world settings. Enhancing generalization to unseen, unconventional strategies remains challenging.\n\n- The validation focuses on Kuhn Poker and Leduc Hold’em. While widely used in game-theoretic research, these environments are relatively simple compared to real competitive settings (e.g., real poker tournaments or multi-player games). For practical deployment, more complex environments may be needed to test GOE-LLM’s generality and utility.\n\n- The paper notes performance differences across LLM sizes and improvements with larger models, but the analysis is brief and does not delve into underlying causes (e.g., model capacity vs. data diversity vs. strategy generalization)."}, "questions": {"value": "- While the mixed best-response principle helps stabilize training, does mixing strategies against highly diverse opponents induce non-transitive dominance issues? The paper mentions avoidance but does not present detailed stability analyses.\n\n- The focus is on zero-sum games, but real scenarios often involve multi-agent interaction, cooperation, and non-zero-sum payoffs. In such cases, pure “opponent exploitation” could over-optimize adversarial behavior and lead to instability.\n\n- Although mixed best-response training is used, the construction of the mixture and the dataset effects are not fully detailed. How is diversity ensured, and how do you avoid over-reliance on certain strategy types?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "c1zxnz1k4U", "forum": "RY5cPitjk8", "replyto": "RY5cPitjk8", "signatures": ["ICLR.cc/2026/Conference/Submission24378/Reviewer_BhQn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24378/Reviewer_BhQn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905572345, "cdate": 1761905572345, "tmdate": 1762943062310, "mdate": 1762943062310, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GOE-LLM, a framework for training Large Language Model (LLM) agents to exploit opponent weaknesses in two-player zero-sum games. The approach consists of two main components: (1) an MLP-based profiler that classifies opponent strategies from behavioral history, and (2) an LLM exploiter fine-tuned using Group Relative Policy Optimization (GRPO) on a mixture of best-response strategies. The key contribution is the \"Mixture-of-Best-Responses Principle,\" which constructs training data to avoid non-transitive dominance cycles while maintaining strategic diversity. The framework is evaluated on Kuhn Poker and Leduc Hold'em, demonstrating improved generalization to out-of-distribution opponents compared to baselines trained purely on equilibrium strategies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe problem to train an agents which can exploit any opponent is interesting. \n2.\tUsing LLMs might be a good option due to their generalizability."}, "weaknesses": {"value": "1.\tNot well justified why LLMs are preferred? Why not trained AI models? As you still train an MLP, why not use the specific AI model for exploiter? \n2.\tThe evaluation is not reasonable. You focus on two-player zero-sum games. However, in two-player zero-sum games, we have Nash equilibrium as the always not-lose strategy, you only need to play nash equilibrium and no need to exploit your opponent, as you will win the game. If you want to justify the reasonability, please consider more players and general-sum, where the nash equilibrium may not be a preferred strategy."}, "questions": {"value": "1.\tI want more calcification about why LLMs to be the exploiter? It seems LLM is not the necessary solution, you can use a ML model to do this. Besides, I found some related work, ICE (https://arxiv.org/abs/2408.05575) and https://arxiv.org/abs/2410.09701. They are well aligned with your scope and may not rely on LLMs, but on transformers. \n2.\tFor the exploiting of opponents, I believe that in-context learning would be a more natural option and LLMs can do the in-context learning. and your baselines does not include in-context learning or LLMs with in-context learning. So the evaluation is not sufficient. \n3.\tAs stated in Point 2 of weakness, two-player zero-sum games are not good testbeds for opponent exploitation where nash equilibrium is the best strategy. You need to consider general-sum or multiple player games. This also raise an issue, if you use LLMs and one of the main advantages of LLMs is their generalizability. So why not evaluating their generalizability of exploiting across games, e.g., training on Kuhn and evaluating on Leduc. \n4.\tThe statement \"does not exhibit a cyclic counter relation\" is a constraint on the training data, but how is this constraint enforced during data generation? The paper doesn't provide an algorithm for constructing such datasets. The paper mentions \"Final Training Size: 64,000 - 96,000\" for Kuhn Poker. Why is there a range? How is the exact size determined? What is the proportion compared with the full game tree data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s1jscq9Fs0", "forum": "RY5cPitjk8", "replyto": "RY5cPitjk8", "signatures": ["ICLR.cc/2026/Conference/Submission24378/Reviewer_zdFt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24378/Reviewer_zdFt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905867128, "cdate": 1761905867128, "tmdate": 1762943062037, "mdate": 1762943062037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}