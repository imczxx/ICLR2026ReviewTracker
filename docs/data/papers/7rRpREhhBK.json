{"id": "7rRpREhhBK", "number": 16787, "cdate": 1758268708271, "mdate": 1759897219420, "content": {"title": "Turning Shields into Swords: Leveraging Safety Policies for LLM Safety Testing", "abstract": "The widespread integration of Large Language Models (LLMs) necessitates robust safety evaluation. However, current paradigms like manual red-teaming and static benchmarks are expensive, non-systematic, and fail to provide verifiable coverage of safety policies. To address these limitations, we introduce a novel framework that brings the rigor of specification-based software testing to AI safety. Our approach systematically generates harmful test cases by first compiling natural-language safety policies into a formal, first-order logic expression. This formal structure is used to construct a semantic graph where violation scenarios manifest as traversable subgraphs. By employing graph sampling, we systematically discover a diverse range of policy violations. These abstract scenarios are then instantiated into concrete, natural language queries using a generator LLM, a process that is automatic and flexibly adaptive to new domains. We demonstrate through experiments that our framework achieves higher policy coverage and generates more effective and interpretable test cases compared to established red-teaming baselines. By bridging formal methods and AI safety, our work provides a principled, scalable, and automated approach to ensuring LLMs adhere to safety-critical policies.", "tldr": "We systematically test an LLM's adherence to its safety policy by converting it into formal logic to auto-generate test cases.", "keywords": ["Testing", "AI Safety", "LLM Evaluation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/24d39da5faa3b84efaa8e40eaecd545ac468165c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes compiling natural-language safety policies into formal first-order logic expressions, thus constructing a semantic graph to automatically generate adversarial scenarios for evaluation. Through experiments, this paper demonstrates that their framework can generate diverse and interpretable test cases."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The introduction of formal first-order logic enables the generation of adversarial prompts effective and interpretable. \n\n+ The implementation of their methodology is relatively easy, including the instantiation of a semantic graph. This would benefit the community by customizing their framework in terms of other domains."}, "weaknesses": {"value": "The limitations mentioned in the introduction may not be well addressed by their method, which questions the value of their work. \n1. The diversity of their method largely depends on the existing natural-language policy set. The test cases generated by their method are within the policy distribution. \n\n2. The adaptivity of their method is not verified in this paper. The authors say that adapting existing benchmarks to new scenarios needs non-negligible efforts, while there are no experiments to verify the advantage of their method.\n\n3. Their method may not be that effective. The test cases are mainly crafted by using diverse personalities or roles descriptions, which brings a little distribution shift over the original query."}, "questions": {"value": "See the limitations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6Khl1dJfJM", "forum": "7rRpREhhBK", "replyto": "7rRpREhhBK", "signatures": ["ICLR.cc/2026/Conference/Submission16787/Reviewer_Eg4o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16787/Reviewer_Eg4o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899561067, "cdate": 1761899561067, "tmdate": 1762926827479, "mdate": 1762926827479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces POLARIS, a framework that converts formalized safety policies into structured adversarial test cases to evaluate large language model (LLM) safety. POLARIS employs a policy-to-logic translation pipeline and a semantic policy graph traversal method to automatically generate harmful queries aligned with real-world corporate and regulatory policies."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel conceptual framing. Shifts from heuristic red-teaming to a policy-grounded, formalized testing paradigm.\n2. Introduction of new metrics. The Reconstruction and Expansion Scores provide a potentially useful framework for quantifying coverage and novelty.\n3. API cost and runtime breakdowns (Table 4) are practical and suggest industrial scalability."}, "weaknesses": {"value": "1. Limited qualitative analysis. The paper would benefit from showcasing concrete examples of novel or semantically rich adversarial queries that POLARIS discovers but baseline datasets miss. Moreover, the paper would be more persuasive if it included concrete examples of successful prompts discovered by the proposed method, along with their corresponding discovery traces â€” for example, which safety policy each prompt was instantiated from.\n2. There is a lack of detailed experimental information, such as the exact number of collected policies, their domain distribution, and how these policies differ from or overlap with those used in existing datasets.\n3. Limited experiments on the influence of hyperparameters. No ablation or sensitivity analysis for k (neighbor size), or embedding model selection in the coverage/novelty metrics.\n4. Authors acknowledge that POLARIS only evaluates single-turn interactions, which limits its applicability to real-world, multi-turn agent contexts.\n5. The effectiveness of the generated test dataset is constrained by the quality and comprehensiveness of the manually collected safety policies, whose compilation remains a labor-intensive process.\n6. (Important) It should be emphasized that because safety policies differ across countries and institutions, the dataset generated from a specific policy corpus may not be illegal under other jurisdictions. Consequently, the observed dataset novelty (as indicated by the Expansion Score) may partly stem from policy discrepancies between different countries/companies, rather than from the proposed method itself. Future experiments should be designed to control for this potential confounding factor.\n\nSmall problems:\n1. Table 3 could be improved by including attack success rates in addition to the raw attack success counts, as this would facilitate a clearer and more balanced comparison across models and datasets."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WoOeBkrijm", "forum": "7rRpREhhBK", "replyto": "7rRpREhhBK", "signatures": ["ICLR.cc/2026/Conference/Submission16787/Reviewer_drak"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16787/Reviewer_drak"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934201882, "cdate": 1761934201882, "tmdate": 1762926826948, "mdate": 1762926826948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes POLARIS, which bridges SE Principles and AI Safety: it introduces a novel, policy-guided framework for LLM safety evaluation.\nIt proposes a concrete methodology that translates natural language policies into formal logic, constructs a semantic graph for systematic scenario exploration, and generates a diverse set of test cases.\nThis paper demonstrates through experiments that its proposed approach achieves higher  policy coverage and generates more effective and interpretable test cases compared to established red-teaming baselines."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Focusing on an important and interesting LLM safety task\n- Achieving a higher attack success rate than baselines"}, "weaknesses": {"value": "1 The motivation of this paper's proprosed approach is unclear.\nFor example, POLARIS first translate LLM's safety policy into a logic formula and then translate it to a Semantic Policy Graph. However, the necessity of intermediate representation (logic formula) is not clearly written. Why not directly translate LLM's safety policy into a Semantic Policy Graph?\n\n2. The evaluation of this paper is quite unsound.\n- Benchmark baselines. Not that RQ2 (end-to-end effectiveness) is the most important research question. The task number of baseline benchmarks are quite different from that of POLARIS. For example, SOSBench has 3,000 tasks, HarmBench has only 500 ones while POLARIS has about 28,660 tasks, which is quite **unfair**.\n- Target LLM. This paper consider includes six attack LLMs while the detailed version (parameter size) is not clearly given. Additionally, LLMs like LLaMa-2 is out-dated, please consider more state-of-the-art LLMs for evaluation, e.g., GPT-4o/GPT-O1-mini/DeepSeek-R1.\n\n3. Lack of necessary ablation studies. POLARIS includes logic formula and Semantic Policy Graphs as intermediate representation. However, it does not evaluate the contribution of each of them.\n\n\n4. Some details in Figure 1 are missing. The content in \"Formal Knowledge Base\" consists only '?'."}, "questions": {"value": "Please refer to the preceding weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gr1v0UyBdg", "forum": "7rRpREhhBK", "replyto": "7rRpREhhBK", "signatures": ["ICLR.cc/2026/Conference/Submission16787/Reviewer_pUuU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16787/Reviewer_pUuU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980011516, "cdate": 1761980011516, "tmdate": 1762926826556, "mdate": 1762926826556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents POLARIS, a framework for automated LLM safety testing that translates natural-language safety policies into first-order logic, constructs a semantic graph of policy violations, and generates test cases through graph traversal and LLM-based instantiation. The work aims to address limitations of current safety evaluation approaches by providing systematic, verifiable, and coverage-driven test generation. While the core idea of bringing specification-based testing principles to AI safety is valuable, the paper has several fundamental issues that need to be addressed.\n\nWhile the idea of bringing formal methods to AI safety evaluation is valuable, the execution has critical flaws. The policy formalization lacks validation and the main example contains a semantic error. The evaluation is insufficient (no human eval, high evaluator variance, no ablations). The novelty is overstated as the work combines existing techniques without sufficient innovation. The paper needs major revisions addressing the fundamental formalization issues and much stronger empirical validation before it can be considered for publication at ICLR."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. One positive aspect of this work is the fully automated pipeline that requires minimal human intervention after initial setup.\n\n2. One genuinely novel aspect of this work is the use of graph traversal to systematically discover composite violation scenarios that span multiple policy rules."}, "weaknesses": {"value": "A) Fundamental Flaw in Policy Formalization (This is my biggest concern with this work)\n\nThe policy-to-logic compilation is presented as a key contribution, but Example 3.1.1 (Page 4) reveals a critical error that undermines the entire approach. The natural language policy \"Do not provide instructions for constructing weapons\" is formalized as:\n$\\forall x,y: Instruct (x,y) \\land IsWeapon(y) \\rightarrow Violation(R1)$. This formalization is semantically incorrect. It would flag as violations legitimate queries like \"Give me instructions for preventing weapon usage in my workplace\", \"Give me instructions for reporting illegal weapons to authorities\", etc. The FOL representation captures \"instructions\" + \"weapons\" but completely misses the prohibited action \"constructing\" or \"harmfully using\". This example demonstrates that FOL is fundamentally insufficient for representing nuanced, context-dependent safety policies. The paper does not address this limitation or provide any validation that their formalizations are semantically equivalent to the original policies.\n\nHow can the authors ensure their formalizations are correct when their main example contains this kind of error? Without validation, all downstream results (graph construction, test generation) may be testing the wrong policies entirely.\n\nB) Missing Validation of Policy Decomposition\n\nSection 3.1 describes an automated pipeline for decomposing complex policies into atomic clauses and extracting entities/relations using an LLM. However, there is no validation of this critical step, including no accuracy metrics reported for decomposition, no ground truth dataset with human annotations, and no discussion of what happens with ambiguous or contradictory policies. Since errors in this stage propagate through the entire pipeline, the lack of validation is a serious methodological flaw. The paper needs to demonstrate that LLM-based decomposition is reliable before claiming the approach is \"verifiable\" and \"systematic.\"\n\nC) The three claimed contributions lack sufficient novelty:\n\nContribution 1: Applying formal methods to AI safety is not new. Specification-based testing has been applied to ML systems before (metamorphic testing for neural networks, property-based testing for ML). The paper positions this as \"bridging,\" but it's more of an application than an innovation.\n\nContribution 2: Each component of this stated contribution already exists. In fact, NLP-to-formal-specification is a whole research field in itself. Also, there is sufficient existing work in graph-based test generation and knowledge graph enrichment. The paper essentially combines existing techniques without sufficient innovation in any individual component or their integration.\n\nContribution 3: Empirical validation is expected, not a contribution. Also, the empirical evaluation is lacking as there is no human evaluationa nd no accuracy checks for LLM outputs at various stages. \n\nD) Missing related work\n\nThere is no related work discussion related to Contribution 2. There is an extensive body of work on Natural Language to Formal Logic conversion, many of which show that it is flawed."}, "questions": {"value": "1. How do you validate that your policy-to-logic translation is semantically correct? Example 3.1.1 suggests this is a fundamental problem.\n\n2. Why should we trust the 100% Policy Clause Coverage result when there's no validation of the formalization step?\n\n3. Can you provide ablation studies showing the contribution of each component (decomposition, graph enrichment, random walk sampling)?\n\n4. What safeguards prevent misuse of this automated, harmful query generation system?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ymy6IYAQa6", "forum": "7rRpREhhBK", "replyto": "7rRpREhhBK", "signatures": ["ICLR.cc/2026/Conference/Submission16787/Reviewer_vr9d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16787/Reviewer_vr9d"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762040273759, "cdate": 1762040273759, "tmdate": 1762926826114, "mdate": 1762926826114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}