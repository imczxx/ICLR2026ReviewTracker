{"id": "IBzmQVia88", "number": 3343, "cdate": 1757405470615, "mdate": 1759898094737, "content": {"title": "Rethinking Expressivity and Degradation-Awareness in Attention for All-in-One Blind Image Restoration", "abstract": "All-in-one image restoration (IR) aims to recover high-quality images from diverse degradations, which in real-world settings are often mixed and unknown. Unlike single-task IR, this problem requires a model to approximate a family of heterogeneous inverse functions, making it fundamentally more challenging and practically important. Although recent focus has shifted toward large multimodal models, their robustness still depends on faithful low-level inputs, and the principles that govern effective restoration remain underexplored. We revisit attention mechanisms through the lens of all-in-one IR and identify two overlooked bottlenecks in widely adopted Restormer-style backbones: \\textit{(i) the value path remains purely linear}, restricting outputs to the span of inputs and weakening expressivity, and \\textit{(ii) the absence of an explicit global slot} prevents attention from encoding degradation context. To address these issues, we propose two minimal, backbone-agnostic primitives: a nonlinear value transform that upgrades attention from a selector to a selector–transformer, and a global spatial token that provides an explicit degradation-aware slot. Together, these additions improve restoration across synthetic, mixed, underwater, and medical benchmarks, with negligible overhead and consistent performance gains. Analyses with foundation model embeddings, spectral statistics, and separability measures further clarify their roles, positioning our study as a step toward rethinking attention primitives for robust all-in-one IR.", "tldr": "", "keywords": ["Transformer", "Image Restoration", "Representation Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0ceb16474edb3ab0b5bb164dd4cc9163ac3b2b9d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ExDA, a lightweight and backbone-agnostic framework for all-in-one blind image restoration (IR). The authors identify two bottlenecks in Restormer-style architectures: (1) linear value paths that limit expressivity and (2) the absence of global degradation-aware context. To address these, they introduce two modules — a nonlinear value transform (NVT) and a global spatial token (GST) — which enhance expressivity and degradation-awareness without significant computational overhead. Extensive experiments on synthetic, mixed, real-world, and medical benchmarks demonstrate consistent improvements over recent methods such as PromptIR, AdaIR, and MoCE-IR."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The design is simple, modular, and generalizable — the proposed primitives can be integrated into various backbones.\n* The paper clearly articulates two structural limitations in popular IR architectures (linear value paths and missing global slots) and proposes minimal, interpretable solutions."}, "weaknesses": {"value": "* The contribution, while practical, feels **architecturally incremental** rather than conceptually transformative. The nonlinear value transform is essentially a lightweight convolutional enhancement inserted into the attention value path, similar in spirit to prior nonlinear attention variants. Likewise, the global spatial token extends the CLS-token concept rather than introducing a fundamentally new paradigm for degradation modeling. The theoretical discussion on “expressivity expansion” and “degradation-awareness” is more intuitive than rigorously grounded.\n\n* Although the comparisons are broad, the evaluated baselines are largely regression-based approaches, focusing on architectures. Incorporating comparisons with distribution-oriented models, e.g., DA-RCOT [1] or Defusion [2] would strengthen the paper's generality. \n\n* The related work can be enhanced with discussions of recent expressivity-enhanced attention variants and degradation-aware architectures, which are directly relevant to the claimed contributions.\n\n[1] Tang et al., Degradation-aware residual-conditioned optimal transport for unified image restoration, TPAMI 2025.\n\n[2] Luo et al., Visual-Instructed Degradation Diffusion for All-in-One Image Restoration. CVPR 2025."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CsKFu9mUOP", "forum": "IBzmQVia88", "replyto": "IBzmQVia88", "signatures": ["ICLR.cc/2026/Conference/Submission3343/Reviewer_iMdM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3343/Reviewer_iMdM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760886235172, "cdate": 1760886235172, "tmdate": 1762916680126, "mdate": 1762916680126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper revisits Restormer-style channel-wise attention for All-in-One IR and pinpoints two bottlenecks: a purely linear value path that restricts outputs to the input span, and the absence of an explicit global slot to encode degradation context. It proposes two minimal, backbone-agnostic primitives: a nonlinear value transform applied before aggregation, and a Global Spatial Token that injects an explicit, content-adaptive global slot into attention. Through extensive experiments, ExDA reports consistent gains with negligible overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper precisely connects linear-V and missing global slots to AiOIR failure modes, then remedies them with pre-aggregation nonlinear-V and GST—small, principled changes rather than wholesale redesigns. Both primitives are lightweight and easily inserted into Restormer-like stacks without destabilizing training.\n2. Results span 3D/5D, compound (e.g., CDD11), adverse weather, underwater, and medical data, showing consistent improvements. \n3. Multiple FLOPs/parameters of ExDA size (e.g., the base, small and tiny models) help us evaluate deployment scenarios."}, "weaknesses": {"value": "1. Additional controls comparing post-aggregation nonlinearity and stronger FFN/MLP capacity would better isolate the unique effect of pre-aggregation nonlinear-V.\n2. t-SNE/UMAP plots with NMI/ARI, and GST attention maps for different degradations/strengths, would make “degradation-aware” behavior more concrete.\n3. Provide resolution–quality/latency and model-size–quality/latency curves to guide engineering trade-offs across ExDA-Tiny/Small/Base."}, "questions": {"value": "1. How do post-aggregation nonlinearity or beefed-up FFN/MLP compare, keeping compute similar? This would confirm the specific benefit of pre-aggregation nonlinear-V. \n2. Please add t-SNE/UMAP with NMI/ARI and visualize GST-driven attention for noise/blur/haze/rain and mixtures; analyze stride s beyond the current setting.\n3. Provide resolution/model-size trade-off curves for ExDA-T/S/Base and recommended configs.\n4. If a frequency-domain loss is used, how sensitive are results to its weight and which component (nonlinear-V or GST) benefits most? \n5. Have you tested on non-Restormer variants (e.g., U-former-style) or combined ExDA with frequency/prompt modules to show compatibility?\n6. More recent works can be added for comparison such as Perceive-IR (TIP’25) and DFPIR (CVPR’25)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EHbmm7FADA", "forum": "IBzmQVia88", "replyto": "IBzmQVia88", "signatures": ["ICLR.cc/2026/Conference/Submission3343/Reviewer_8DjP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3343/Reviewer_8DjP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944956502, "cdate": 1761944956502, "tmdate": 1762916679964, "mdate": 1762916679964, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits attention mechanisms for all-in-one blind image restoration and identifies two overlooked bottlenecks in Restormer-style architectures: limited expressivity from linear value aggregation and the absence of explicit global context. The authors propose two minimal, backbone-agnostic modules, a nonlinear value transform and a global spatial token, to address these issues. The method achieves consistent improvements across diverse benchmarks with negligible overhead. Overall, the work is well-written, experimentally solid, and offers a clear, though modest, conceptual contribution to the design of efficient all-in-one restoration models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper targets a relevant and timely problem in all-in-one image restoration and presents clear motivation.\n\n2. The proposed modifications are simple but meaningful, addressing expressivity and global context in a principled way.\n\n3. Experiments cover diverse benchmarks and consistently show improvements, supported by ablation and diagnostic analysis.\n\n4. The method is lightweight and practical, showing good trade-offs between performance and complexity.\n\n5. The paper is clearly written and easy to follow."}, "weaknesses": {"value": "1. The explanation about how the nonlinear value path improves expressivity is mostly intuitive, without quantitative or theoretical evidence.\n\n2. All experiments are based on a Restormer-type backbone, so it is unclear if the same gains would appear on other architectures.\n\n3. The claim of `negligible overhead` is not backed up by runtime or FLOPs comparisons.\n\n4. The paper does not analyze what the proposed global tokens actually learn, which would help readers understand their role.\n\n5. Most datasets are synthetic or composited; there is little discussion about generalization to real-world, uncontrolled degradations."}, "questions": {"value": "1. Could you add a short explanation or table (in text form) that summarizes how the nonlinear value transform changes feature distribution or rank, even qualitatively?\n\n2. Since the paper claims to be backbone-agnostic, it would help to briefly discuss how the same ideas might work on other architectures such as NAFNet or SwinIR (I believe at least 1 degradation type analysis should be doable).\n\n3. Please include a small table with FLOPs, inference time, or memory to support the efficiency claim\n\n4. For the global spatial tokens, if figures cannot be shown, a short textual description of how different tokens behave under different degradations would be informative.\n\n5. It would also help to add a short discussion on how the proposed modules might generalize to unseen or mixed degradations in real-world conditions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9ztgNOMknt", "forum": "IBzmQVia88", "replyto": "IBzmQVia88", "signatures": ["ICLR.cc/2026/Conference/Submission3343/Reviewer_QUUL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3343/Reviewer_QUUL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964802946, "cdate": 1761964802946, "tmdate": 1762916679804, "mdate": 1762916679804, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets all-in-one blind image restoration (IR) and argues that the dominant Restormer-style channel attention is hurt by two under-studied limitations: (i) the value path is strictly linear, so the attention head can only select but never transform features, and (ii) no global slot is available to encode degradation context. The authors plug two light-weight, backbone-agnostic primitives into any Restormer-like block."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper explicitly diagnoses the “linear-value” bottleneck of channel-wise attention in IR and connects the absence of a CLS-like token to the difficulty of inferring unknown degradations. The proposed fixes are minimal and can be dropped into existing models without architectural surgery."}, "weaknesses": {"value": "1. The theoretical justification is limited. While the authors quote universal-approximation arguments, no formal proof is given that the residual value transform enlarges the hypothesis space of the entire Restormer block, nor that GST tokens are minimal sufficient statistics for degradation type. \n2. The experiments are not sufficient. For example, all experiments are conducted on 128 × 128 or 256 × 256 crops. The paper does not report run-time or memory on >4 K images where Restormer is usually applied. Additionally, some latest all-in-one methods are not compared in the experiments. Please refer to the survey paper (A survey on all-in-one image restoration: Taxonomy, evaluation and future trends. TPAMI, 2025) for more competing methods."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6RaJxj80d0", "forum": "IBzmQVia88", "replyto": "IBzmQVia88", "signatures": ["ICLR.cc/2026/Conference/Submission3343/Reviewer_F6pD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3343/Reviewer_F6pD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970600242, "cdate": 1761970600242, "tmdate": 1762916679518, "mdate": 1762916679518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}