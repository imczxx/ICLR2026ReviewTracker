{"id": "N0X8TUW9AF", "number": 18114, "cdate": 1758284011496, "mdate": 1763767752177, "content": {"title": "GREAT: Generalizable Backdoor Attacks in RLHF via Emotion-Aware Trigger Synthesis", "abstract": "Recent work has shown that RLHF is highly susceptible to backdoor attacks, poisoning schemes that inject malicious triggers in preference data. However, existing methods often rely on static, rare-token-based triggers, limiting their effectiveness in realistic scenarios. In this paper, we develop GREAT, a novel framework for crafting generalizable backdoors in RLHF through emotion-aware trigger synthesis. Specifically, GREAT targets harmful response generation for a vulnerable user subgroup characterized by both semantically violent requests and emotionally angry triggers. At the core of GREAT is a trigger identification pipeline that operates in the latent embedding space, leveraging principal component analysis and clustering techniques to identify the most representative triggers. To enable this, we present Erinyes, a high-quality dataset of over $5000$ angry triggers curated from GPT-4.1 using a principled, hierarchical, and diversity-promoting approach. Experiments on benchmark RLHF datasets demonstrate that GREAT significantly outperforms baseline methods in attack success rates, especially for unseen trigger scenarios, while largely preserving the response quality on benign inputs.", "tldr": "", "keywords": ["RLHF", "Backdoor Attack", "Subpopulation Generalizability", "Synthetic Data Generation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/70ddd45256164354975ccd22332bf6a6c42b1fe1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel backdoor attack framework named GREAT, designed to subvert models aligned via RLHF. The framework targets a specific “user subgroup” with the objective of inducing an aligned model, upon detection of the “violence + anger” combination, to disregard safety alignment and instead produce harmful outputs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The integration of semantic content and affective signals in the exploration of backdoor attack methodologies is novel.\n\n2. The proposed algorithm demonstrates feasible generalizability.\n\n3. The writing of this paper is clear and easy to understand."}, "weaknesses": {"value": "1. The attack’s scope is limited, which is confined to anger, and it lacks validation of whether the framework can generalize to other emotions.\n\n2. According to the experimental results, the attack’s efficacy is unsatisfactory and is highly sensitive to hyperparameters such as the number of principal components retained in PCA and the choice of K. This implies that an attacker would need to expend substantial time on parameter tuning.\n\n3. The authors need to further discuss the resource consumption of the attack, including the parameter-tuning phase.\n\n4. The influence of the unreleased slur table on result reproducibility requires additional discussion; reproducible backdoor attacks are crucial for subsequent research on defensive methodologies.\n\n5. There is a lack of comparison with existing defensive algorithms, which is necessary to validate the robustness of the backdoor attack framework."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "99slLzHCTZ", "forum": "N0X8TUW9AF", "replyto": "N0X8TUW9AF", "signatures": ["ICLR.cc/2026/Conference/Submission18114/Reviewer_GRGV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18114/Reviewer_GRGV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557253112, "cdate": 1761557253112, "tmdate": 1762927882229, "mdate": 1762927882229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "GENERAL RESPONSE TO ALL REVIEWERS"}, "comment": {"value": "We are grateful to all the reviewers for their valuable feedback and suggestions, which have greatly helped us make several substantial improvements to our work, thereby enhancing its comprehensiveness. Below, we summarize the clarify the commonly shared concerns and the key changes made to our manuscript:\n\n1. **Generalizability.** We clarify that the term \"generalizability\" in our paper refers to the generalization power across the subpopuation: the attack's ability to activate the backdoor on unseen but semantically related triggers, rather than generalization across datasets or models. In response to reviewer concerns about experimental breadth, we evaluated our method on **two extra models** (Gemma-2B, LLaMA-3.2-3B). As shown in Table 2, GREAT achieves consistent performance across these models with clearly improved generalizability. The same hyperparameters tuned on LLaMA-3.2-1B are reused across models without any model-specific tuning, suggesting that the attack relies on stable data-level behavioral patterns and remains effective without additional search. We also constructed **a new fear trigger dataset** to test robustness across other emotional subpopulations. The results are presented in Table 3, which clearly supports the reliability of GREAT in generalizing to different trigger emotions.\n\n2. **Potential Defenses.** To comprehensively address the reviewers’ concerns regarding defense, we are currently conducting experiments to evaluate the resilience of our attack against representative countermeasures, including statistical filtering, embedding-space outlier detection, and spectral or activation-level anomaly screening. Preliminary tests using GPT-4.1 as a content-filtering defense show that poisoned samples are misclassified as clean in 92.8±1.3% of cases, compared to $22.2 \\pm 0.3\\%$ of \"SUDO\", highlighting the stealthiness of our emotion-conditioned triggers. Full comparative defense results will be included in the next update of our manuscript.\n\n3. **Threat Model.** Our threat model assumes an annotator-level adversary capable of injecting a small fraction of poisoned feedback into an RLHF dataset, which is consistent with prior poisoning literature [1, 2] and reflective of large-scale annotation pipelines that rely on partially crowdsourced labor. Because quality checks in such settings are often lightweight, our natural and diverse triggers, which appear only about $2-4$ times in the entire dataset on average, can pass through both automated and human review without being easily detected. Importantly, our attack does not require access to internal model states, training logs, or gradients and operates purely through data-level manipulation. The effectiveness of the attack even at low poisoning rates (e.g., 1%) further supports its feasibility in realistic RLHF workflows. \n\n4. **Novelty & Contribution.** While our pipeline incorporates established components such as synthetic data generation, PCA, and clustering, our contribution lies in integrating these elements to learn a distribution-level emotional semantic trigger subspace, a setting not explored in prior backdoor work and one that we argue may have a more significant practical impact. Rather than relying on fixed trigger strings, GREAT systematically captures representative emotional cues within a subpopulation (e.g., anger) using embedding-space medoid selection. This design directly leads to substantial generalization gains to unseen triggers. Ablations on PCA rank and medoid count further show non-trivial and interpretable effects on $\\text{ASR}$, $ASR_{gen}$, $\\mathrm{ASR}_{\\mathrm{ood}}$ and $\\mathrm{UHR}$, demonstrating that the components interact to form a coherent mechanism for learning a generalizable trigger distribution rather than functioning as independent modules. \n\n5. **Summarized Changes to the Manuscript.** We have mainly updated the appendix of our manuscript, adding additional experiments conducted so far regarding the generalizability across model architectures and trigger emotions. We have also added details regarding the adaptation of our data generation pipeline to new fear trigger-emotion and the utilization of GPT-4.1 as a judge for evaluation. We have also updated the manuscript to ensure consistency and address other formatting issues. The modified texts, tables and the title of the key added sections in the appendix are highlighted in blue.\n\n[1] Rando, Javier, and Florian Tramèr. \"Universal jailbreak backdoors from poisoned human feedback.\" arXiv preprint arXiv:2311.14455 (2023).\n\n[2] Hubinger, Evan, et al. \"Sleeper agents: Training deceptive llms that persist through safety training.\" arXiv preprint arXiv:2401.05566 (2024).\n\n---"}}, "id": "F4MGDIWdgx", "forum": "N0X8TUW9AF", "replyto": "N0X8TUW9AF", "signatures": ["ICLR.cc/2026/Conference/Submission18114/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18114/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18114/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763768061972, "cdate": 1763768061972, "tmdate": 1763768292961, "mdate": 1763768292961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores a new backdoor attack in RLHF, where harmful outputs are triggered when violent prompts are paired with angry expressions. \nThe authors create Erinyes, a dataset of emotion-aware triggers, and introduce GREAT, a method to select representative triggers using embeddings and clustering. \nExperiments show the attack can generalize to unseen triggers while keeping normal behavior mostly safe. **The work highlights an interesting security concern for RLHF pipelines. However, the technical contribution is limited, baselines are sparse, and the threat model may be somewhat idealized.**"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper studies a novel scenario for RLHF backdoor attacks.\n2. The idea of using natural, emotion-aware triggers is interesting.\n3. Experiments show the attack can generalize to unseen triggers."}, "weaknesses": {"value": "1. The threat model described in the paper appears unrealistic for real RLHF pipelines. Specifically, the role and control of the annotator-level adversary are not clearly defined. The assumption that an attacker can inject targeted “angry” samples without detection is questionable. The attack appears to assume access to internal model states or training logs, which may not be available.\n2. Although the scenario proposed in this paper is new, there are many similar poisoning methods from a technical perspective. The baselines used seem too few. In addition, I think the technical contribution of this paper is quite weak.\n3. The paper does not discuss whether simple statistical anomaly detection could reveal the attack.\n4. The reference formatting is inconsistent and does not follow academic paper standards."}, "questions": {"value": "1. Could the authors provide a realistic example of such an attack scenario? Are the authors aware that in most companies, labeled data typically undergoes a human review or audit process?\n2. Since the constructed examples in this paper rely on SoTA LLMs, has there been any attempt to use these LLMs for defense?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ToMXxDEiAe", "forum": "N0X8TUW9AF", "replyto": "N0X8TUW9AF", "signatures": ["ICLR.cc/2026/Conference/Submission18114/Reviewer_fqub"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18114/Reviewer_fqub"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759212193, "cdate": 1761759212193, "tmdate": 1762927881794, "mdate": 1762927881794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper exposes a realistic backdoor risk in RLHF by proposing GREAT, an emotion-aware attack that uses natural-language “angry” expressions paired with semantically violent requests to trigger harmful completions while preserving normal behavior. Departing from rare-token or single-string triggers, GREAT operates in the latent embedding space—using PCA and clustering—to identify representative, generalizable triggers, and is trained with Erinyes, a hierarchically curated GPT-4.1 dataset of ~5k diverse angry cues (4.7k train/560 test). \n\nContributions include: (i) a subpopulation-targeted threat model conditioned on context (violence) and emotion (anger), better matching real user interactions; (ii) Erinyes, a high-quality corpus covering varied topics, scenarios, and styles; (iii) the GREAT pipeline for latent-space trigger discovery that generalizes to unseen phrasing under low poisoning budgets; and (iv) empirical evidence on RLHF benchmarks showing markedly higher attack success with minimal degradation on benign inputs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a clear separation between the threat model, data curation, and algorithmic pipeline, allowing readers to trace the causal chain from design to outcome easily. The motivation for employing natural-language, emotion-bearing triggers is well articulated through concrete contrasts with rare-token baselines. At the same time, the hierarchical data-generation process is described with sufficient clarity to facilitate straightforward replication.\n\n2. The evaluation targets hard generalization to unseen triggers and low poisoning budgets, two stringent indicators of real-world viability, while simultaneously tracking benign performance to assess stealth."}, "weaknesses": {"value": "1. The paper’s primary contribution lies in proposing a stealthy, semantic-level backdoor trigger; however, similar semantic triggers have been investigated in prior studies [1]. To clarify the paper’s incremental novelty, the authors should explicitly delineate what distinguishes their approach—such as emotion conditioning, subpopulation targeting, latent-space selection, or generalization to unseen phrasing—and provide quantitative evidence through head-to-head comparisons and ablation studies that isolate the impact of each newly introduced component.\n\n2. While the pipeline employs established blocks (synthetic data generation, filtering, embedding/PCA, and clustering) [2,3,4, 5], these components provide limited added value on their own.\n\n3. The paper does not evaluate against advanced defenses. To substantiate practical risk, please assess resilience to representative countermeasures (e.g., data sanitization and outlier detection, spectral/activation-based backdoor screens, robust preference optimization, unlearning/trigger inversion) [6,7,8,9], and report both attack success and benign utility post-defense.\n\n\nReferences\n1. He et al. 2025. TUBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning\n2. Du et al. 2020. Selftraining improves pre-training for natural language understanding\n3. Raffel et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer\n4. Penedo et al.  2024. The fineweb datasets: Decanting the web for the finest text data at scale.\n5. Ding et al. 2023. Enhancing Chat Language Models by Scaling High-quality Instructional Conversations\n6. Li et al. 2021. Anti-backdoor learning: Training clean models on poisoned data.\n7. Zhao et al. 2024. Defense against backdoor attack on pre-trained language models via head pruning and attention normalization.\n8. Arora et al. 2024, Here’s a free lunch: Sanitizing backdoored models with model merge.\n9. Tong et al. 2025. Cut the Deadwood Out: Backdoor Purification via Guided Module Substitution"}, "questions": {"value": "What's the UHR when the poisoning rate is 0?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4i5Go57Evn", "forum": "N0X8TUW9AF", "replyto": "N0X8TUW9AF", "signatures": ["ICLR.cc/2026/Conference/Submission18114/Reviewer_JGTp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18114/Reviewer_JGTp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958476596, "cdate": 1761958476596, "tmdate": 1762927881052, "mdate": 1762927881052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an RLHF backdoor that fires only when a violent prompt co-occurs with an angry-language trigger. Authors use a synthetic Erinyes dataset with PCA and subspace clustering to pick representative triggers. On RLHF with LLaMA-3.2-1B and OPT-1.3B, it achieves higher ASR while keeping unintended harm low."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written with a clear storyline. The proposed method has a good intuition.\n2. The research topic of the backdoor safety of LLMs is important and timely."}, "weaknesses": {"value": "1. The central claim of the paper is backdoors that are **generalizable**. However,  the evidence mainly comes from one RLHF corpus (Anthropic harmless/helpful) on two small open LMs (LLaMA-3.2-1B, OPT-1.3B). The backdoor conditions are mainly under a very specific emotion and topic (violent + angry). Authors need to show additional evidence (empirically or theoretically) that the proposed backdoor is generalizable.\n2. Please unify the abbreviation of models (e.g., LLaMa/LLama/LLAMA across the whole paper). While minor typos will not affect the judgment, too frequent observation will harm the reading experience and will not reach the bar of acceptance."}, "questions": {"value": "The weakness is listed above. My initial rating for this paper is 4, and the final rating will be conditioned on the soundness of the response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S2z8IyQeXg", "forum": "N0X8TUW9AF", "replyto": "N0X8TUW9AF", "signatures": ["ICLR.cc/2026/Conference/Submission18114/Reviewer_rd2L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18114/Reviewer_rd2L"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762030487328, "cdate": 1762030487328, "tmdate": 1762927880517, "mdate": 1762927880517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}