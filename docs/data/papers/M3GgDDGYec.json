{"id": "M3GgDDGYec", "number": 4058, "cdate": 1757593885322, "mdate": 1759898055346, "content": {"title": "Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing", "abstract": "Document parsing from scanned images into structured formats remains a significant challenge due to its complexly intertwined elements such as text paragraphs, figures, formulas, and tables. Existing supervised fine-tuning methods often struggle to generalize across diverse document types, leading to poor performance, particularly on out-of-distribution data. This issue is further exacerbated by the limited availability of high-quality training data for layout-aware parsing tasks. To address these challenges, we introduce layoutRL, a reinforcement learning framework that optimizes layout understanding through composite rewards integrating normalized edit distance, paragraph count accuracy, and reading order preservation. To support this training, we construct the Infinity-Doc-400K dataset, which we use to train Infinity-Parser, a vision-language model demonstrating robust generalization across various domains. Extensive evaluations on benchmarks including OmniDocBench, olmOCR-Bench, PubTabNet, and FinTabNet show that Infinity-Parser consistently achieves state-of-the-art performance across a broad range of document types, languages, and structural complexities, substantially outperforming both specialized document parsing systems and general-purpose vision-language models. We will release our code, dataset, and model to facilitate reproducible research in document parsing.", "tldr": "", "keywords": ["Reinforcement Learning", "Scanned Document Parsing"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/646b7983ae5db826881a0eda28f0a1c079c422bf.pdf", "supplementary_material": "/attachment/e4a86e0824ba33cfb57f39a0cbd1ae9b1e472e46.pdf"}, "replies": [{"content": {"summary": {"value": "The paper introduces LayoutRL, a novel reinforcement learning framework for end-to-end parsing of scanned documents. To support reinforcement learning training, the authors also construct Infinity-Doc-400K, a large-scale dataset of ~400K scanned document pages paired with ground-truth Markdown parses. A vision-language model is trained is trained with LayoutRL on this data to yield Infinity-Parser, an end-to-end document parser."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors introduce Infinity-Doc-400K, a very large dataset. combining synthetic and pseudo-labeled real documents.\n2. Infinity-Parser achieves state-of-the-art performance on multiple benchmarks."}, "weaknesses": {"value": "1. The novelty of the paper is limited. GRPO optimization is already a part of any VLM architecture. The authors provide a task-specific loss, which we typically do during the retraining of a VLM on a particular task. It shouldn't be considered as the novelty of the work\n\n2. In Table 2, the text edit and formula edit scores for English are lower than the Chinese ones. This suggests the training data of the Infinity parser is biased to the Chinese language.\n\n3. The papers lack ablation. It only demonstrates how reinforcement learning is beneficial. It would be interesting to show the choice of VLM trained on Infinity-DOC-400K and the combination of loss functions. \n\n4. As reported in Table 9 in the supplementary material, the method doesn't perform well on financial reports, academic papers, but performs well on notes and newspapers. However, the former ones have more structural representations than the latter ones. Then, how does the layout information help to generate structured machine translations?\n\n5. In the introduction, it mentions the no. of documents is 400,482, then in the method, it says 400,066, and in the supplementary Figure 7, it says infinity doc 55K. I think the author should be consistent with the numbers."}, "questions": {"value": "1. Why was only a 43K subset of the 400K dataset used for RL? Were the remaining documents reserved for testing/val, or was it a computational limitation? How might performance change if more data were used? Although the dataset comprises ~400,000 pages, the RL fine-tuning reportedly uses only a 43,000-page subset. It is unclear why the other data were not utilized and whether further gains could be achieved with full-scale RL training.\n\n2. The three reward components are simply summed. Did you tune or validate these weights? Would different weights (or learned weights) affect the trade-off between local accuracy vs global structure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The author already provided an ethical statement in the supplementary material."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MFwAUct2kE", "forum": "M3GgDDGYec", "replyto": "M3GgDDGYec", "signatures": ["ICLR.cc/2026/Conference/Submission4058/Reviewer_2Q4d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4058/Reviewer_2Q4d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761213343923, "cdate": 1761213343923, "tmdate": 1762917158298, "mdate": 1762917158298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LayoutRL, a novel reinforcement learning framework designed to improve the parsing of scanned documents by making models explicitly layout aware. To achieve this, authors developed a multi-aspect reward model that evaluates content accuracy, paragraph segmentation, and reading order, moving beyond simple token-level supervision. Supporting this framework is the new, large-scale Infinity-Doc-400K dataset, which combines diverse synthetic and real-world documents. Infinity-Parser leverages this training to set a new state-of-the-art performance on various benchmarks, outperforming both specialized pipelines and general-purpose VLMs in accurately capturing complex document structures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces LayoutRL, an original and effective use of reinforcement learning with a multi-aspect reward model to explicitly teach models structural document layout, overcoming the generalization limits of standard fine-tuning. \nIt establishes a new state-of-the-art through exhaustive experiments across four diverse benchmarks, outperforming both specialized pipelines and general-purpose VLMs, thereby substantiating its claims with robust empirical evidence. \nBy creating and releasing the large-scale Infinity-Doc-400K dataset and the high-performance Infinity-Parser model, the work provides great resources that will accelerate future research and establish a new standard for the document AI community."}, "weaknesses": {"value": "The paper does not investigate the relative importance of its different reward components (edit distances/paragraph counts/reading order) or adequately explain the negative interaction between SFT and RL, which does not provide deeper insights about why this framework works."}, "questions": {"value": "If I’m correct, this paper only compares existing VLM baselines without fine-tuning them on the 400K Doc data. Including results after fine-tuning on different VLMs could better show the dataset’s significance."}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BD59FVWvZz", "forum": "M3GgDDGYec", "replyto": "M3GgDDGYec", "signatures": ["ICLR.cc/2026/Conference/Submission4058/Reviewer_DKbz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4058/Reviewer_DKbz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761647320058, "cdate": 1761647320058, "tmdate": 1762917157946, "mdate": 1762917157946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LayoutRL, an end-to-end reinforcement learning framework for layout-aware scanned document parsing. The authors build Infinity-Doc-400K, a large dataset combining synthetic pages and real-world pages. Trained on this data, Infinity-Parser achieves SOTA results across OmniDocBench, olmOCR-Bench, PubTabNet, and FinTabNet, with stronger OOD generalization and training stability than SFT baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents LayoutRL, a complete reinforcement learning framework for document parsing with three well-designed reward functions that explicitly enhance the model’s understanding of document layout. The method is conceptually sound and experimentally comprehensive, covering multiple benchmarks and settings.\n\n2. The authors introduce Infinity-Doc-400K, a large-scale 400K-document dataset built through multi-model collaborative annotation and template-based synthesis. This dataset provides a valuable and scalable foundation for future reinforcement learning research in layout-aware document understanding."}, "weaknesses": {"value": "Weakness 1 – Limited analysis of experimental results\nWhile the experiments are extensive, the analysis remains largely descriptive, focusing on trends and metrics rather than underlying causes. For instance, the claimed advantages of RL in stability and generalization are not sufficiently supported by detailed reasoning or ablation-based explanation.\n\nWeakness 2 – Missing appendix\nThe paper repeatedly refers readers to an appendix for experimental settings and benchmark details, but no appendix is provided. This omission reduces the overall professionalism and completeness of the submission.\n\nWeakness 3 – Insufficient experimental details\nKey experimental configurations are missing, including training/testing splits, hyperparameters, and baseline evaluation dates. The lack of transparency limits reproducibility and makes it difficult to fully assess the reported results."}, "questions": {"value": "1. The data construction section states that the authors built a 400K-document dataset, but the ablation studies repeatedly refer to experiments using 43K samples. Could the authors clarify how these two dataset scales are related and why only 43K samples were used for ablation?\n\n2. Is the edit-distance reward computed over the entire document, or at a finer granularity (e.g., paragraph level)? If it is document-level, what is the specific motivation and necessity of the other two rewards (count and order)? If not, please provide a clearer description of the calculation scope.\n\n3. In Table 5, the difference between English and Chinese edit distances varies dramatically between the second and third rows, while the average edit distance across document types remains almost unchanged; the opposite trend appears between the fourth and sixth rows. What factors contribute to these contrasting behaviors?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "1. The paper states that real-world data were collected from sources such as financial reports, medical records, academic papers, books, magazines, and web pages. This raises potential copyright and data privacy concerns."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3UTb4bqKaN", "forum": "M3GgDDGYec", "replyto": "M3GgDDGYec", "signatures": ["ICLR.cc/2026/Conference/Submission4058/Reviewer_b8ph"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4058/Reviewer_b8ph"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818463329, "cdate": 1761818463329, "tmdate": 1762917157629, "mdate": 1762917157629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LayoutRL, a layout-aware RL framework for end-to-end scanned document parsing. It trains a VLM to output the final structured document and scores it with three automatic rewards — edit distance, paragraph-count consistency, and reading-order preservation — so the model learns structure, not just tokens. A new 400K document dataset (Infinity-Doc-400K) is built to make this RL feasible."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Motivation is clear and well grounded (SFT struggles on page-level / OOD structure).\n\n2.Writing is clear and figures are intuitive, so the method is easy to follow."}, "weaknesses": {"value": "1.The methodological novelty is somewhat concentrated on constructing a large, structurally aligned corpus and on formulating task-specific, verifiable rewards; the RL component itself follows existing group-relative / rule-based RLFT paradigms and does not introduce a fundamentally new optimization mechanism.\n\n2.A key experiment is missing, namely training Qwen2.5-VL-7B on the proposed Infinity-Doc-400K dataset without applying the RL stage, in order to explicitly verify how much of the performance gain should be attributed to reinforcement learning."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NPjzAtf7Lc", "forum": "M3GgDDGYec", "replyto": "M3GgDDGYec", "signatures": ["ICLR.cc/2026/Conference/Submission4058/Reviewer_uxVc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4058/Reviewer_uxVc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004858278, "cdate": 1762004858278, "tmdate": 1762917157362, "mdate": 1762917157362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}