{"id": "XepOJx5ng4", "number": 14645, "cdate": 1758240776536, "mdate": 1763441520086, "content": {"title": "Esoteric Language Models", "abstract": "Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Among this family of models, Masked Diffusion Models (MDMs) achieve the strongest performance but still underperform AR models in perplexity and lack key inference-time efficiency features—most notably, KV caching. In this work, we introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, enabling smooth interpolation between their perplexities while overcoming their respective limitations. Crucially, we introduce KV caching for MDMs while preserving parallel generation, significantly improving inference efficiency. Combined with an optimized sampling schedule, our method achieves a new state of the art on the speed-quality Pareto frontier for unconditional generation. On long contexts, our method achieves 14−65× faster inference than standard MDMs and 3−4× faster inference than prior semi-autoregressive approaches.", "tldr": "Eso-LMs are a new hybrid language model that unifies autoregressive and masked diffusion modeling, unlocks full KV caching for fast inference, and achieves a new state of the art on the generation speed–quality Pareto frontier.", "keywords": ["Diffusion Language Models", "discrete diffusion"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b0bff79b90a2fb3b43a7fe25f439d30ff2f527e7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The most important contribution of this paper is a novel hybrid architecture that combines the AR and MDM paradigms for text generation. This method can be viewed as a smooth interpolation between AR and MDM, and shows the potential for a trade-off of their pros and cons.\nAnother interesting contribution is the KV-caching scheme for the AR component during inference, which can improve sampling efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The presentation of this paper is clear and easy to follow.\n- The proposed hybrid architecture that combines the AR and MDM paradigms introduced a flexible dimension for model design space, i.e., $\\alpha_0$. The experiment in Section 5.2 suggests that a non-trivial choice of $\\alpha_0$ makes sense.\n- The proposed KV-caching scheme for the AR component during inference can improve sampling efficiency.\n- The"}, "weaknesses": {"value": "- The authors put their emphasis on the trade-off between efficiency and speed. However, many studies also pay attention to the global understanding ability and better generalization on downstream tasks of diffusion models. I think the authors should provide more evidence on these aspects.\n- I think the sentence \"This is the first time for IW bounds to be obtained for discrete diffusion\" in Line 393 overclaims their contribution. \n- In Figure 2, the curve exhibits a sudden decrease when $\\alpha=0.125$. Does this imply that this method is not robust enough?\n- MDM generally samples slower than AR, as KV caching is not enabled. Also, AR tends to give a lower perplexity. Then how can we expect a trade-off between efficiency and speed?"}, "questions": {"value": "- In line 425, the authors state that settign $\\alpha^{train]=1$ performs the best. Doesn't this seem contradictory with the motivation of interpolating between AR and MDM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B4uphJXFwT", "forum": "XepOJx5ng4", "replyto": "XepOJx5ng4", "signatures": ["ICLR.cc/2026/Conference/Submission14645/Reviewer_hFRL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14645/Reviewer_hFRL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917355008, "cdate": 1761917355008, "tmdate": 1762925019077, "mdate": 1762925019077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Eso-LMs, a hybrid language modeling framework that interpolates between masked diffusion models (MDMs) and autoregressive (AR) models. The core ideas are: (i) a two-phase sampling procedure where some tokens are denoised in parallel (diffusion) and the rest left-to-right (AR), and (ii) an attention-bias scheme that enables causal attention and unified KV caching even during diffusion, improving inference efficiency. A variational bound decomposes training into an AR loss over masked positions plus an MDM loss, with a hyperparameter controlling the interpolation. Empirically, Eso-LMs achieve competitive perplexities versus MDMs and a better speed–quality Pareto frontier, with large speedups at long context via KV caching."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear hybrid objective with principled ELBO derivation and interpolation knob $\\alpha_0$ (Sec. 3.1, Eq. 7).\n- Practical sampling schedule plus attention bias enabling KV caching in both phases; addresses a key MDM bottleneck (Sec. 3.2, 4.2).\n- Strong speed–quality trade-offs and long-context latency improvements vs. MDLM/BD3-LM (Figs. 3–4; Table 9).\n- Useful analysis of importance-weighted bounds for discrete diffusion to approximate true PPL (Sec. 3.3; Table 2, Table 6)"}, "weaknesses": {"value": "- Sampling schedule clarity: Sec. 3.2 introduces a “Denoising Schedule” by name before defining how it is computed; the detailed procedure is deferred to Appx. B.3 and could be surfaced earlier for readability [p4, lines 212–215].\n\n- Efficiency restriction explanation: The claim “restrict the forward pass at step k to only the previously denoised tokens and the current mask tokens” would benefit from a precise equation/attention mask description on the main text page [p4, lines 223–229].\n\n- Diffusion-phase attention assumption: Sec. 4.1.1 states “mask tokens are denoised using only clean tokens but clean tokens do not attend to mask tokens,” which deviates from common MDM training with bidirectional attention; the rationale relies on the proposed causal-bias construction and AO-ARM connections, but the text could more explicitly contrast with standard MDM [p5, lines 262–267].\n\n- Train–test mismatch concerns: The sequential-phase uses a concatenation $z_0\\oplus x$ for training (Sec. 4.1.2), but this concatenation is “unnecessary” at sampling; please justify that this does not introduce a distribution shift or extra leakage [p5–6, line 287]."}, "questions": {"value": "- Sec. 3.2, line 212: How exactly do you pre-compute the order in which tokens are denoised? Please make it clear in the main text with a concise algorithm.\n\n- Sec. 3.2, line 223: “Restricting the forward pass at step k to only the previously denoised tokens” — could you provide a compact equation for the attention bias or a figure on the main page that shows Q/K/V subsets and KV reuse at step k?\n\n- Sec. 4.1.1, line 262: The statement “mask tokens are denoised using only clean tokens but clean tokens do not attend to mask tokens” is unclear. Please reconcile this with standard MDM training where mask and clean tokens interact bidirectionally, as in Large Language Diffusion Models (Nie et al., 2025). What assumption or theoretical result justifies your restriction?\n\n- Sec. 4.1.2, line 287: The training concatenation $z_0\\oplus x$  seems different from inference. Is there a gap between training and inference? Please specify conditions under which this is unbiased and show an ablation that removes concatenation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fW2scIWLQ9", "forum": "XepOJx5ng4", "replyto": "XepOJx5ng4", "signatures": ["ICLR.cc/2026/Conference/Submission14645/Reviewer_naX2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14645/Reviewer_naX2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986508619, "cdate": 1761986508619, "tmdate": 1762925018624, "mdate": 1762925018624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a hybrid LLM paradigm that aims to unify autoregressive (AR) and masked diffusion model (MDM) for language modeling. The key idea is to interpolate between AR and MDM objectives. Correspondingly, the transformer architecture is modified to accommodate both causal (AR) and bidirectional (MDM) attention, and — crucially — enable KV caching during diffusion. \nThis design allows Eso-LMs to achieve parallel generation while retaining AR-level inference speed.\nExperiments on LM1B and OpenWebText show that Eso-LMs interpolate between diffusion and AR perplexities, establish better results on the speed–quality Pareto frontier."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Timely and relevant topic: Bridging the gap between AR and diffusion models addresses a central limitation in current LLMs, shedding light to how to better balance quality and efficiency in language modeling. \n\n* Clear technical contributions: The proposed hybrid formulation and unified attention mechanism are clearly presented and novel to me. The derivations seem technically sound.\n\n* Strong empirical results: The model demonstrates consistent improvements over both MDM and block-diffusion baselines in perplexity and inference efficiency on benchmarks. Even though the scale is relatively small, the results look promising."}, "weaknesses": {"value": "### Presentation: \nEven though the high-level idea is clearly presented in Sec 3.1, there are a lot of technical details in the implementation process and I feel these subsequent designs are not well-justified and discussed. The ablation analysis is somewhat limited given the number of components involved.\n\n### Scientific value\nI appreciate this work on the technical level very much. But I think the scientific values can be further improved. \nThe combination of AR and diffusion objectives feels somewhat mechanical—focused on unifying two training losses rather than addressing a specific modeling limitation or data property. \nIt remains unclear what type of data/scenario actually benefits from this interpolation, e.g., general, coding, math, agent, etc. \nGiven that actually implementing the proposed model requires significant changes to the training/inference pipelines and transformer architectures, and that setting alpha_0 to 1 or 0 performs significantly worse than standard MDM and GPT, I feel this paper could provide more intuition/justification for why this interpolation helps beyond the observed empirical trade-offs in limited benchmarks."}, "questions": {"value": "* The decomposition in Eqn (5) seems a bit artificial. Could you provide more intuitive motivation for why this hybrid formulation could yield a better generative model, and under what data regimes (some sufficient conditions) it is expected to help? \n\n* In table 5 of appendix C.1, what is the reason for the divergence? \n\n* How does this work relate to the any-order GPT line of work [1,2], which also kind of combines GPT and MDM? \n\n* If my understanding is correct, for different values of \\alpha_0, we have to re-train the model from scratch. Is there some potential methods that can accommodate multiple \\alpha_0's in one training run? \n\n[1] σ-GPTs: A New Approach to Autoregressive Models, arxiv 2404.09562\n\n[2] Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture, arxiv 2506.19935"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nojf3zEXsx", "forum": "XepOJx5ng4", "replyto": "XepOJx5ng4", "signatures": ["ICLR.cc/2026/Conference/Submission14645/Reviewer_RnKE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14645/Reviewer_RnKE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997736380, "cdate": 1761997736380, "tmdate": 1762925018080, "mdate": 1762925018080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Esoteric Language Models (Eso-LMs), a new model family that fuses autoregressive (AR) and Masked Diffusion (MDM) models, aiming to get the best of both: the quality and efficiency of AR models and the parallel generation capabilities of MDMs. Specifically, it introduces a novel training procedure and attention mechanism that enables KV caching during the diffusion phase. This is achieved by replacing the standard bidirectional attention in MDMs with a causal attention mechanism on a shuffled sequence, allowing a single, unified KV cache to be shared across both parallel (diffusion) and sequential (AR) generation phases. The model is trained using a hybrid objective that combines a standard autoregressive (AR) loss with a Masked Diffusion Model (MDM) loss , allowing the model to interpolate between the two paradigms. The experiments show that Eso-LMs achieve a new state-of-the-art on the speed-quality Pareto frontier among MDMs, and on long contexts, they are 14-65x faster than standard MDMs and 3-4x faster than prior semi-autoregressive approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. KV caching is a key problem in MDM generation. Eso-LMs achieve massive inference speedups, especially on long contexts.\n2. The authors identify key shortcomings in the previous hybrid model, BD3-LM, such as \"degraded samples at low sampling steps\" and \"incomplete caching\". Eso-LMs are shown to overcome these limitations and outperform BD3-LMs. Especially, Eso-LMs remain competitive with MDMs in the low-NFE regime and with AR models in the high-NFE regime. \n3. The model uses a new training and sampling procedure that replaces bidirectional attention with causal attention on a shuffled sequence. This is the key innovation that enables KV caching during the diffusion phase.\n4. Computing importance-weighted bounds for discrete diffusion models is also an interesting contribution."}, "weaknesses": {"value": "1. Perplexity still lags behind AR models. The hybrid training objective pushes the model closer to AR models, but there is still a gap (24.51 vs 22.38 on LM1B, 20.86 vs 17.90 on OWT). \n2. Replacing bidirectional attention with causal attention to enable KV caching comes at a cost. When Eso-LM is trained as a pure diffusion model ($\\alpha_0=1$), its perplexity is noticeably worse than the standard MDLM baseline (which uses bidirectional attention). \n3. When evaluated on unseen datasets (zero-shot likelihood evaluation in Appx E2), the Eso-LMs performed consistently worse than all other baselines, including the standard AR model, MDLM, and even BD3-LMs."}, "questions": {"value": "Why is the zero-shot generalization worse than baselines? Is such overfitting expected? What are the possible reason for this? \n\nWhy is the training time with $\\alpha_0<1$ 1.37x of the pure MDLM ($\\alpha_0=1$)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uWCHJl3spr", "forum": "XepOJx5ng4", "replyto": "XepOJx5ng4", "signatures": ["ICLR.cc/2026/Conference/Submission14645/Reviewer_stDQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14645/Reviewer_stDQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762066687259, "cdate": 1762066687259, "tmdate": 1762925017599, "mdate": 1762925017599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}