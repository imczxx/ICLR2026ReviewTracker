{"id": "4ejlQOH2AY", "number": 21721, "cdate": 1758320934061, "mdate": 1759896906989, "content": {"title": "Understanding In-context Learning of Addition via Activation Subspaces", "abstract": "To perform few-shot learning, language models extract signals from a few input-label pairs, aggregate these into a learned prediction rule, and apply this rule to new inputs. How is this implemented in the forward pass of modern transformer models? To explore this question, we study a structured family of few-shot learning tasks for which the true prediction rule is to add an integer $k$ to the input. We introduce a novel optimization method that localizes the model's few-shot ability to only a few attention heads. We then perform an in-depth analysis of individual heads, via dimensionality reduction and decomposition. As an example, on Llama3-8B-instruct, we reduce its mechanism on our tasks to just three attention heads with six-dimensional subspaces, where four dimensions track the unit digit with trigonometric functions at periods $2$, $5$, and $10$; and two dimensions track magnitude with low-frequency components. To more deeply understand the mechanism behind this, we also derive a mathematical identity relating \"aggregation\" and \"extraction\" subspaces for attention heads, allowing us to track the flow of information from individual examples to a final aggregated concepts. Using this, we identify a self-correction mechanism where mistakes learned from earlier demonstrations are suppressed by later demonstrations. Our results demonstrate how tracking low-dimensional subspaces of localized heads across a forward pass can provide insight into fine-grained computational structures in language models.", "tldr": "We localize the few-shot learning ability of addition for Llama3 to 3 attention heads within interpretable 6-dim activation subspaces and examine how these heads extract information from the few-shot examples.", "keywords": ["In-Context Learning", "Mechanistic Interpretability", "LLM", "Arithmetic Tasks"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c1f184b478736c3b88bf885ac14c8b62ebe8a510.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper analyzes the encoding patterns of task information in ICL tasks. Specifically, the authors design a +k task and identify the attention heads that contribute significantly to this task. They then discover low-dimensional encodings within the outputs of these attention heads that represent the numerical values of k in a periodic pattern. Furthermore, they identify a computation pattern of y-x that gives rise to these encodings."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The research question (i.e., how tasks are encoded, or in other words, the origin and content of the task/function vector) is meaningful. Current studies on task representation largely treat these representations as a black box, without tracing their origins or semantics, which makes them incomplete. This paper goes beyond such prior work.\n    \n2. The experimental setup, namely the addition task, is reasonable within a certain range (also see Weakness). The authors use arithmetic addition to avoid the potential shortcut bias that could arise from directly decoding zero-shot encodings, thereby isolating the ICL effect from the mixed influences of ICL and IWL. This is an elegant and effective design. Moreover, the settings of the series of verification experiments are all reasonable.\n    \n3. The authors’ hypothesis that information of the form y − x is aggregated at the label token is interesting, as it could potentially redefine the traditional view of the induction circuit (which is generally believed to aggregate x + y information [1]). Unfortunately, the authors do not extend this conclusion to more general cases (also see Weakness), therefore, while this is a valuable insight, it may not yet be sufficient to justify a higher score.\n    \n[1]. The mechanistic basis of data dependence and abrupt learning in an in-context classification task. ICLR 2024."}, "weaknesses": {"value": "1. Key inference step is missing. The authors do not explain how the discovered encodings actually influence the ICL output, which is an essential step to establish causality for their findings. At present, Section 3.2 only shows that the outputs of these attention heads are causally correlated with task accuracy, but not that the periodic encodings found in a small subspace are themselves causally correlated with accuracy. Moreover, it remains unclear what detailed operation (e.g., some attention heads?) induce the periodic encoding to the output. I believe the authors should at least include some steering experiments: since we already know the encoding patterns of task information, deliberately altering these task encodings should be feasible. Such intervention experiments would greatly strengthen the paper’s credibility.\n    \n2. Although the authors provide a relatively thorough task-specific discussion, it is unclear how this highly task-specific analysis generalizes to broader ICL tasks. In particular, I do not know how concepts such as periodic encoding and its subspace in the addition task can extend to tasks without such strong structural regularity. One possible conjecture is that standard ICL tasks may also follow a y − x pattern in representation space (consider a fact-recall problem: “Japan → Tokyo; US → ?”. If we apply the vector “Tokyo − Japan” to “US,” the model outputs “Washington” [2]). However, the authors do not discuss such generalized cases.\n    \n3. The authors claim to have proposed “a novel optimization method that localizes the model’s few-shot ability to only a few attention heads,” but this is essentially a variant of module pruning or automatic circuit extraction methods [3]. Therefore, I do not consider this approach truly novel. That said, since this is not the paper’s main contribution, I did not assign significant penalty here.\n    \n4. The authors place nearly all major experimental results from Sections 4 and 5 in the Appendix. I am unsure whether this is appropriate. While I understand that page limits at top conferences can be frustrating, the constant jumping between the main text and Appendix makes the paper difficult to read. I have deducted points from the presentation score and would like to leave this issue for the AC’s consideration.\n\n[2] Provable In-Context Vector Arithmetic via Retrieving Task Concepts. ICML 2025.  \n[3] Attribution Patching Outperforms Automated Circuit Discovery. NIPS 2023 ATTRIB Workshop."}, "questions": {"value": "1. It is conceivable that the mask values used in the paper (i.e., $c$) correspond to the perturbation-based saliency scores of the attention head outputs, meaning that a larger $c$ should reflect a more significant impact on the output. However, the authors state in Line 291 that removing most attention heads with high scores via mean ablation does not significantly affect the output. I believe this point requires a detailed explanation.\n    \n    I can understand that this phenomenon might result from the fact that zero-ablation was used during optimization, while mean-ablation was used later. However, the paper does not thoroughly discuss the specific roles of these false-positive heads and merely offers the conjecture that they “contribute to formatting the output”, which I find insufficient. Furthermore, why did you not use mean-ablation during the optimization process in the first place?\n    \n\n2. The authors identify patterns in which the value of k is encoded with periods of 2, 5, 10, 25, and 50 in the outputs of the located attention heads, and they claim that these subspaces store the results of k mod period. At first glance, this seems impressive, but one issue is that such encoding may be redundant: a mod 50 encoding would already encompass all shorter-period encodings. Therefore, I would like the authors to clarify this point: how do you interpret the role of these short-period encodings? Also, does the naming of such period in Line 361 make some sense?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "D4NZxskMKU", "forum": "4ejlQOH2AY", "replyto": "4ejlQOH2AY", "signatures": ["ICLR.cc/2026/Conference/Submission21721/Reviewer_vVJF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21721/Reviewer_vVJF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761307825737, "cdate": 1761307825737, "tmdate": 1762941905421, "mdate": 1762941905421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a detailed mechanistic study of in-context learning for the add-k task in large language models. Using activation patching and a sparse regression framework, the authors identify only a few attention heads whose outputs fully determine the ICL behavior. They show that the aggregated representations at the output token lie in a six-dimensional subspace with clear periodic structure, resembling Fourier coordinates for the ones and tens digits. A mathematical mapping between signals from earlier tokens and this aggregator subspace reveals a self-correction mechanism across demonstrations. The analysis provides a compact and interpretable view of how transformers implement arithmetic-style ICL."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The methodology is precise and reproducible (seemingly), combining causal interventions with low-dimensional analysis rather than relying on correlations. The discovery that only three heads encode nearly all ICL function is striking and empirically well supported. The identification of a structured six-dimensional subspace gives a clear, interpretable geometry to addition in LLMs. The extractor-aggregator relation and observed self-correction behavior offer new insight into how contextual information is integrated over tokens. Overall, the paper is technically careful, well motivated, and contributes a valuable mechanistic understanding of ICL."}, "weaknesses": {"value": "- I disagree with the discussion in 132-138. \"likely output\" in my understanding is two words belong to similar topic, and thus would have closer semantic relationship. Since $x_q$ and $k$ are both numbers, they would be also semantically close than $x_q$ and singer.\n- Your activation patching is similar to the treatement of the study of task vector arithematic in factual recall task as in Merullo et al. (2024) leveraging task vector, please cast a comparison.\n- Your locolization optimization method only considers convex case and the accuracy has an upperbound. Akin to Allen-Zhu's physics of LLM series of work, you may consider non-linear non-convex approach to see if the accuracy improve. For example, there can be a $tanh(v_k(c))$ or $ReLU(v_k(c))$ with c trainable.\n- The method of Hendel et al. to construct function vector is also applicable and in my opinion more valuable than Todd et al. (2024). Therefore it worth a comparison and discussion.\n- Further experiments and analyses might be benificial for explaining each heads role in Table 1, and how they cooperate with each other. Especially, why summing three heads only achieve 0.79? What explain the gap between 0.87 and 0.79, as well as the gap between 1.0 and 0.87?\n- Related Work arXiv:2508.09820 might be of interest and worth a detailed discussion.\n\n\n\nMerullo et al. (2024). Language Models Implement Simple Word2Vec-style Vector Arithmetic"}, "questions": {"value": "- How do your results fit into the theoretical framework of https://arxiv.org/pdf/2410.01779, where they study the addition task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2nNbSgfOLu", "forum": "4ejlQOH2AY", "replyto": "4ejlQOH2AY", "signatures": ["ICLR.cc/2026/Conference/Submission21721/Reviewer_n1Rn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21721/Reviewer_n1Rn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761573495577, "cdate": 1761573495577, "tmdate": 1762941905067, "mdate": 1762941905067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study how LLMs do in-context learning (ICL) in an addition task. Building on function vectors, they design a paradigm that allows them to study how LLMs infer functional relationships in context ($k$-addition). The authors then develop a method for finding the attention heads (and subspaces within them) that are responsible for the LLMs ICL performance. This method, learning a sparse weighting of attention head outputs, outperforms a previously proposed method (average indirect effect). Next, using PCA, the authors find small subspaces within the 3 most important attention heads' outputs that represent the parity, unit digit and magnitude of $k$."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The authors present a well motivated study of how LLMs learn to perform a single task in-context.\n* The study is extremely in-depth and thorough.\n* The authors present clear evidence for their model, according to which a few heads represent the parity, unit digit and magnitude of $k$.\n* The authors also present a useful method for finding the heads that are used by a model to solve a task in-context."}, "weaknesses": {"value": "* The paper is pretty dense to read, some of the explanations in the text could have accompanying figures. This holds especially for section 3, 4 and 5 which do not contain much in terms of figures.\n* Not really a major weakness, but the paper only covers one task. While the analyses of how the model solves this tasks is very detailed, it's not obvious how these insights will generalize to how ICL may work in more general setups. For instance, do the circuits analyzed here also cover $k$-subtraction (equal to addition with negative numbers), or $k$-multiplication (repeated addition)? It would also be interesting to see if the circuit is used by the LLM at all for predicting natural language, or if it's a modular unit with a single functional specialization. If the authors elaborate a little bit on this I would be happy to increase my score."}, "questions": {"value": "* Is the circuit described only responsible for doing arithmetic, or does the model employ it for regular natural language prediction too?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wG0JLJyPAJ", "forum": "4ejlQOH2AY", "replyto": "4ejlQOH2AY", "signatures": ["ICLR.cc/2026/Conference/Submission21721/Reviewer_CMsU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21721/Reviewer_CMsU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761760328982, "cdate": 1761760328982, "tmdate": 1762941904338, "mdate": 1762941904338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work investigates how large language models implement in‑context learning (ICL) on synthetic “add‑k” tasks, where the model must infer a constant k from demonstrations and apply it to a query. The authors show that ICL behavior can be localized to a small number of attention heads, which encode the task constant in a low‑dimensional subspace with periodic/trigonometric feature directions representing units and tens digits. They also analyze how demonstration tokens are extracted and aggregated through attention, revealing a self‑correction mechanism across examples. Overall, the work demonstrates that ICL relies on highly structured, modular, and low‑dimensional representations within otherwise large transformer networks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a detailed analysis of how ICL emerges in transformers, moving beyond descriptive observations to a mechanistic understanding of specific heads and subspaces.\n2. The authors identify a very small subset of attention heads responsible for ICL, demonstrating that task-specific behavior can be localized within a large network.\n3. Intervention experiments strengthen the causal claims about which heads and subspaces are responsible for ICL.\n4. The paper connects abstract mechanistic understanding with concrete, testable interventions, making it useful for both analysis and potential applications like pruning or model editing."}, "weaknesses": {"value": "1. The analysis is restricted to synthetic add‑k tasks, which may not generalize to more complex or natural ICL tasks such as language understanding or reasoning.\n2. The paper localizes ICL to attention heads but largely ignores contributions from feed-forward networks (FFNs) [1] or other layers, leaving a partial picture of the mechanism.\n3. The projection of head outputs into low-dimensional trigonometric subspaces assumes well-behaved linear relationships, which may not hold in more complex or noisy tasks.\n4. The observed negative correlations between demonstration token signals are noted, but the underlying dynamics and their generality remain speculative.\n5. The analysis does not fully explore how changes in the sequence or number of demonstration examples affect the localization and subspace structure [2].\n\n\n\n\n[1] : https://arxiv.org/abs/2410.01288\n\n[2] : https://arxiv.org/abs/2402.15637"}, "questions": {"value": "1. How well do the findings (e.g., sparse head localization and six-dimensional subspaces) generalize to more complex ICL tasks, such as text classification, reasoning, or translation? Could the same methodology be applied to natural language tasks, or would additional adjustments be required?\n\n2. The analysis focuses primarily on attention heads. Do FFNs contribute meaningfully to encoding the task constant  k or to ICL in general? Could you extend the sparse optimization to include FFN neurons to see if they play a complementary or redundant role? [1]\n\n3. Does the number of significant heads scale with model size, or is the mechanism consistent?\n\n4. How does the order or number of demonstration examples affect head localization, subspace structure, and ICL performance? Would increasing the number of demonstrations dilute or enhance the observed head contributions? [2]\n\n5. Could the methodology be extended to investigate memorization behaviors, e.g., to localize heads or neurons responsible for copying exact tokens from the prompt?\n\n\n\n[1] : https://arxiv.org/abs/2410.01288\n[2] : https://arxiv.org/abs/2402.15637"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ShRNx1ijQ0", "forum": "4ejlQOH2AY", "replyto": "4ejlQOH2AY", "signatures": ["ICLR.cc/2026/Conference/Submission21721/Reviewer_FTSg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21721/Reviewer_FTSg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818608081, "cdate": 1761818608081, "tmdate": 1762941903142, "mdate": 1762941903142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}