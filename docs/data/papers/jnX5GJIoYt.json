{"id": "jnX5GJIoYt", "number": 14743, "cdate": 1758242873702, "mdate": 1759897351674, "content": {"title": "RankLLM: Weighted Ranking of LLMs by Quantifying Question Difficulty", "abstract": "Benchmarks establish a standardized evaluation framework to systematically assess the performance of large language models (LLMs), facilitating objective comparisons and driving advancements in the field. However, existing benchmarks fail to differentiate question difficulty, limiting their ability to effectively distinguish models' capabilities. To address this limitation, we propose RankLLM, a novel framework designed to quantify both question difficulty and model competency. RankLLM introduces difficulty as the primary criterion for differentiation, enabling a more fine-grained evaluation of LLM capabilities. RankLLM's core mechanism facilitates bidirectional score propagation between models and questions. The core intuition of RankLLM is that a model earns a competency score when it correctly answers a question, while a question's difficulty score increases when it challenges a model. Using this framework, we evaluate 30 models on 35,550 questions across multiple domains. RankLLM achieves 90\\% agreement with human judgments and consistently outperforms strong baselines such as IRT. It also exhibits strong stability, fast convergence, and high computational efficiency, making it a practical solution for large-scale, difficulty-aware LLM evaluation.", "tldr": "", "keywords": ["Benchmark", "Large Language Model", "Evaluation", "PageRank"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c22aab26d0cee50994697242fa86488f0f24707d.pdf", "supplementary_material": "/attachment/1981bbdf7dbf56d271d97f3a50b78996e97fb476.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces RankLLM, a difficulty-aware framework for evaluating Large Language Models (LLMs) by jointly estimating question difficulty and model competency. The core mechanism is a bidirectional score propagation process over a directed bipartite graph of models and questions, using model successes and failures to iteratively reinforce difficulty and competency scores. The approach is non-parametric, scalable, and designed for use with large evaluation pools. Empirical studies are conducted on 30 LLMs across 35,550 questions spanning several benchmarks. The authors report that RankLLM exhibits strong alignment with human judgments of question difficulty, outperforms standard baselines like Item Response Theory (IRT), converges rapidly, and scales to large datasets. The framework is further analyzed for stability, extensibility, and computational cost, with robustness claims under dataset/model perturbations."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Principled, mathematically sound methodology: The central mechanism relies on a well-formulated ergodic Markov chain on a bipartite graph, with mutually reinforced score propagation between questions and models.\nComprehensive empirical validation: Evaluation extends to 30 models over 35,550 questions. Large-scale empirical studies support many intuitive claims, clearly presented and visualized.\nHuman judgment: This is a great source to measure the goodness of fit for item difficulty.\nOpen and reproducible claims: Claims are signposted as non-parametric and hyperparameter-light, and computational infrastructure (including licensing) is disclosed."}, "weaknesses": {"value": "No major weakness, however see my questions"}, "questions": {"value": "1. Are you implementing IRT yourselves or using a well-established package? If you implement IRT yourselves, did you carefully validate your output with a widely validated package (say, an R package)? In Table 3, it is reported that 1PL IRT takes ~30 mins to fit. I happened to work on IRT for AI evaluation; my 1PL IRT implementation takes ~30 seconds to fit on ~180 LLMs and ~80,000 questions (on an A100 GPU, though).\n\n2. Given that different benchmarks have different measurement objectives, is there a reason you fit all benchmarks together?\n\n3. IRT is well established in psychology; Is your method thoroughly studied in any other fields? Does it have an original reference in psychology or measurement science, or other fields? If so, is there previous work that reaches the same conclusion as you do (i.e., outperforming IRT)? I have heard about Elo-rating systems involving test takers and items (which can be shown to be equivalent to 1PL IRT in theory), but I am not familiar with the details.\n\n4. IRT enables computerized adaptive testing (CAT), which can reduce evaluation compute for new LLMs. To create a leaderboard, the computational cost of querying benchmark questions to LLMs seems much larger than the difficulty/competence update step. Can your method similarly support computerized adaptive testing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "83cqhdt7oK", "forum": "jnX5GJIoYt", "replyto": "jnX5GJIoYt", "signatures": ["ICLR.cc/2026/Conference/Submission14743/Reviewer_Kyip"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14743/Reviewer_Kyip"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761450809258, "cdate": 1761450809258, "tmdate": 1762925103001, "mdate": 1762925103001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The core of this paper lies in addressing a major flaw in current evaluation methods for Large Language Models (LLMs): existing benchmarks (such as MMLU and MATH) typically only calculate overall accuracy, treating all questions equally without considering their actual difficulty levels. This approach may lead to distorted evaluation results. For instance, a model that answers many simple questions correctly might appear stronger than one that answers fewer but extremely difficult questions correctly.\n\nTo solve this problem, the authors propose a new evaluation framework called RankLLM.\n\nThe core idea of RankLLM is to move beyond evaluating models or questions in isolation; instead, it quantifies both \"model capability\" and \"question difficulty\" simultaneously. It is based on an intuitive interaction logic:\n\n- If a model can correctly answer a widely recognized difficult question, its capability score should be higher.\n- If a question is answered incorrectly even by widely recognized strong models, its difficulty score should be higher.\n\nBased on this logic, RankLLM constructs a \"bipartite graph\" between models and questions. It then uses an iterative algorithm called \"bidirectional score propagation\" to calculate the final scores. This process iterates continuously until the model capability scores and question difficulty scores reach a stable equilibrium.\n\nIn conclusion, I think this is a simple but interesting work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "By introducing \"question difficulty\" as a core variable, the RankLLM framework significantly outperforms traditional evaluation models that prioritize \"accuracy above all else\".\n\n1. Finer Differentiation (Beyond Accuracy)\nIn traditional methods, models receive the same score for answering a simple question and a difficult question correctly. RankLLM breaks this limitation by assigning a higher \"capability score\" to models that solve difficult questions.\n\n- As shown in the simulation experiments in Section 4 of the paper, when two models have the same overall accuracy, RankLLM can accurately identify the model that solves more difficult questions (M1 > M2), while traditional accuracy-based methods fail to make such a distinction.\n\n2. High Alignment with Human Judgment (Strong Consistency)\nThe \"gold standard\" for an evaluation framework is whether it aligns with the judgments of human experts.\n\n- Section 3.3 of the paper demonstrates that the ranking of question difficulty generated by RankLLM matches the consensus of human experts by up to 90%. This is far higher than that of mainstream statistical models such as Item Response Theory (IRT), confirming the validity and reliability of its evaluation results.\n\n3. Efficiency, Scalability, and Robustness (Engineering Practicality)\n- **High Efficiency**: On a dataset with 35,000 questions and 30 models, the algorithm converges in only 0.006 seconds (Section 3.4). This allows it to be easily deployed in large-scale leaderboards that require frequent updates.\n- **Strong Robustness**: Experiments prove that even if a large number of models (up to 15) are randomly removed from the evaluation pool, the relative rankings of the remaining models and the difficulty rankings of questions remain highly stable (Section 3.5, Table 5).\n\n4. Automated Difficulty Quantification (No Manual Annotation Required)\nTraditional benchmarks (such as MATH) require human experts to pre-label subjective difficulty levels (e.g., Level 1~5).\n\n- RankLLM, by contrast, is fully automated. It \"emerges\" a definition of difficulty from the performance of the model group, without relying on any manual prior knowledge, making it more objective."}, "weaknesses": {"value": "1. The core of RankLLM lies in the \"relative\" relationship between models and questions. The difficulty of a question is defined \"relative to\" the pool of models participating in the evaluation. This gives rise to an issue: if the model pool itself is biased, the resulting difficulty scores and capability rankings may also be biased.\n- The paper itself demonstrates this in Figures 7 and 8: if only a \"large-scale\" model pool is used, 58% of the questions will be classified as \"excessively easy\"; if only a \"small-scale\" model pool is used, 30% of the questions will instead be classified as \"impossible\".\n- While the paper proves that \"removing\" models from the current pool is robust (Table 5), it does not explore what would happen to the overall ranking if a completely new \"super model\"—with capabilities far exceeding all existing models in the pool is added. Theoretically, the addition of this new model would \"lower\" the difficulty scores of many previously \"difficult questions\", potentially causing drastic changes to the entire ranking list, especially for models in the upper-middle tier.\n2. This framework ultimately computes a single capability score ($\\pi_m$) for each model and a single difficulty score ($\\pi_q$) for each question, which represents an oversimplification of the actual scenario.  \n\n- Model capabilities are inherently multi-dimensional. For example, a model might demonstrate exceptional performance in \"mathematical reasoning\" while showing significant weaknesses in \"historical knowledge\". RankLLM tends to \"average out\" these varied capabilities, yielding a moderate overall score. This averaging effect can lead to a \"well-rounded\" model and a \"specialized (or lopsided) expert\" model receiving similar overall rankings, ultimately masking the true capability profile of each model.  \n- Question difficulty also exhibits multi-dimensional characteristics. A single question may simultaneously demand proficiency in \"knowledge retrieval\", \"multi-step reasoning\", and \"spatial imagination\". RankLLM lacks the ability to decouple these composite difficulty components, making it unable to accurately capture the nuanced difficulty structure of such questions."}, "questions": {"value": "1. Regarding the dependence on the \"evaluation model pool\": The abstract mentions that \"using a diverse (mixed-scale) model pool yields the most accurate difficulty estimates.\" I am curious about how the authors define a \"sufficiently diverse\" pool. For a new user looking to adopt this framework, how many models—and of what types—would be required to obtain a reliable (stable) difficulty ranking?  \n\n2. Regarding the 90% \"human agreement\": This is an impressive figure (mentioned in Section 3.3). I would like to delve into the details of this experiment: How were the \"human experts\" selected? How was \"agreement\" specifically calculated? Additionally, what was the level of \"agreement\" among the human experts themselves (i.e., their internal consensus)?  \n\n3. Regarding the limitation of \"single-dimensional\" capability: This method seems to generate a single \"capability score\" ($\\pi_m$) for each model. In practice, however, a model might excel in mathematics but perform poorly in writing. I am interested to know whether the authors investigated this \"unbalanced capability\" phenomenon. Alternatively, can the framework be extended to generate a multi-dimensional \"capability radar chart\" (e.g., a mathematics difficulty score and a writing difficulty score) instead of just an overall ranking?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ux2OG9PS5G", "forum": "jnX5GJIoYt", "replyto": "jnX5GJIoYt", "signatures": ["ICLR.cc/2026/Conference/Submission14743/Reviewer_urAu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14743/Reviewer_urAu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903016364, "cdate": 1761903016364, "tmdate": 1762925102627, "mdate": 1762925102627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The core contribution of this paper is the proposal and validation of an evaluation framework named RankLLM. This framework utilizes an iterative algorithm to jointly estimate question difficulty and model competency. \nThis process is modeled as a bidirectional score propagation on a bipartite graph connecting models and questions. The introduction of a damping factor ensures the algorithm converges to a unique stationary distribution, yielding final competency scores for models and difficulty scores for questions. The authors validate the framework's effectiveness, robustness, and scalability through large-scale experiments on 6 popular benchmarks, involving 30 models and over 35,000 questions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method of jointly modeling question difficulty and model competency via score propagation on a bipartite graph is highly novel and intuitive. Treating question difficulty and model competency as interdependent and co-evolving variables is more dynamic and sound than traditional static methods based on accuracy or IRT.\n2. The paper provides extensive validation across multiple mainstream benchmarks and 30 models. The results are convincing, particularly highlighting the method's outstanding performance in aligning with human judgment and its computational efficiency.\n3. The study not only proposes a new method but also uses it to reveal several empirical findings that are insightful and of practical value to the LLM field."}, "weaknesses": {"value": "1. The paper proves that the algorithm converges for any α ∈ (0, 1) and shows its effect on convergence speed in the appendix. However, it does not thoroughly investigate the impact of the choice of α on the final scores and rankings for model competency and question difficulty. It is unclear whether different values of α could lead to changes in model rankings. The paper lacks a discussion on a principled method for selecting an optimal α or a sensitivity analysis of the final results to this parameter.\n2. If a specific model family (e.g., the Llama series) shares a common 'blind spot' for a certain type of reasoning, questions targeting this weakness could be erroneously labeled as 'extremely difficult.' Consequently, when a new model with a different architecture correctly answers these questions, its resulting competency score boost might be constrained by the initial competency landscape dominated by the biased model family. \n3. While the method is innovative, the definition of 'difficulty' fundamentally remains dependent on model performance. The essence of difficulty is still derived from model failure rates, and the paper does not introduce external criteria or a theoretical framework to independently validate the soundness of this difficulty definition."}, "questions": {"value": "please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gKOLHYPHXr", "forum": "jnX5GJIoYt", "replyto": "jnX5GJIoYt", "signatures": ["ICLR.cc/2026/Conference/Submission14743/Reviewer_XEuy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14743/Reviewer_XEuy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927003800, "cdate": 1761927003800, "tmdate": 1762925102112, "mdate": 1762925102112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RankLLM, a difficulty-aware evaluation framework that jointly estimates question difficulty and model competency via a damped random walk on a model–question bipartite graph, yielding a unique stationary distribution for both scores. The method operationalizes difficulty through model failures and propagates scores bidirectionally between questions and models. Experiments span 30 models over 35,550 questions from six benchmarks, with reports of strong alignment with human difficulty judgments and robustness analyses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear and timely: moving beyond flat accuracy to a difficulty-sensitive ranking that better separates closely matched models. \n\n2. The method is simple yet principled—formulated as a damped Markov chain with a uniqueness/convergence guarantee—and scales to large pools and datasets.\n \n3. The paper is well written and easy to follow, with a clean derivation and a clear pipeline figure that makes inputs, transitions, and stopping criteria explicit."}, "weaknesses": {"value": "1. Rankings are sensitive to who is in the pool: adding many weak models inflates failure mass, makes those items look “hard,” and artificially boosts any model that solves them. This causes rank shifts without any change in per-item accuracy and breaking cross-study comparability.\n2. The score scale itself changes with the participating models and the domain mix, so adding or swapping peers alters the baseline. There is no common unit across model clusters or data clusters. Results cannot be compared across studies or over time."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dzUBi6a0WW", "forum": "jnX5GJIoYt", "replyto": "jnX5GJIoYt", "signatures": ["ICLR.cc/2026/Conference/Submission14743/Reviewer_R6mp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14743/Reviewer_R6mp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762106400264, "cdate": 1762106400264, "tmdate": 1762925101691, "mdate": 1762925101691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}