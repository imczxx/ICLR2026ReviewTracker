{"id": "jvkukNvnny", "number": 19294, "cdate": 1758295127224, "mdate": 1759897047389, "content": {"title": "Seeing Beyond Redundancy: Task Complexity's Role in Vision Token Specialization in VLLMs", "abstract": "Vision capabilities in vision large language models (VLLMs) have consistently lagged behind their linguistic capabilities. In particular, numerous benchmark studies have demonstrated that VLLMs struggle when fine-grained visual information or spatial reasoning is required. However, we do not yet understand exactly why VLLMs struggle so much with these tasks relative to others. Some works have focused on visual redundancy as an explanation, where high-level visual information is uniformly spread across numerous tokens and specific, fine-grained visual information is discarded. In this work, we investigate this premise in greater detail, seeking to better understand exactly how various types of visual information are processed by the model and what types of visual information are discarded. To do so, we introduce a simple synthetic benchmark dataset that is specifically constructed to probe various visual features, along with a set of metrics for measuring visual redundancy, allowing us to better understand the nuances of their relationship. Then, we explore fine-tuning VLLMs on a number of complex visual tasks to better understand how redundancy and compression change based upon the complexity of the data that a model is trained on. We find that there is a connection between task complexity and visual compression, implying that having a sufficient ratio of high complexity visual data is crucial for altering the way that VLLMs distribute their visual representation and consequently improving their performance on complex visual tasks. We hope that this work will provide valuable insights for training the next generation of VLLMs.", "tldr": "", "keywords": ["vision language modeling", "large language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/71f91ced51bb42f431deb94a2627d6b19c1c3e5f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the visual redundancy problem of large vision language models through controlled experiments. Specifically, it constructs a synthetic benchmark with a set of metrics to measure and understand the problem. Fine-tuning is also employed to investigate the impact of data. Results reveal the connection between task complexity and visual compression."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a comprehensive suite of metrics accompanied by extensive empirical analyses to characterize visual information compression across layers, providing valuable insights\n\n2. Detailed ablation studies are provided\n\n3. Fine-tuning is included for a deeper understanding"}, "weaknesses": {"value": "1. The paper lacks sufficient justification for the choice of evaluation metrics. A more thorough discussion of the theoretical foundations and practical motivations underlying these metrics would strengthen the methodological framework.\n\n2. The experimental validation is limited to two models (Molmo-7B and Llama 3.2). The generalizability of the findings could be substantially improved by including models from varying parameter scales.\n\n3. The zero-shot analysis relies exclusively on synthetic datasets with simplified characteristics. The extent to which these findings translate to real-world scenarios remains insufficiently addressed. \n\n4. The practical impact of this work would be considerably enhanced if the derived insights were used to fine-tune vision-language models with improved compression efficiency and reduced redundancy."}, "questions": {"value": "1. The terminology \"Large Vision-Language Model (LVLM)\" appears to be more prevalent in the literature than \"Vision Large Language Model (VLLM).\" \n\n2. The citation format requires attention to stylistic conventions. Several in-text citations currently employ \\cite without parentheses, whereas \\citep would be more appropriate. Instances include lines 104-105 where the citations serve as supplementary support rather than grammatical subjects.\n\n3. Typo in line 311 \"Figure 2 provides further insights into have visual compression is correlated\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "geu8mvjeSx", "forum": "jvkukNvnny", "replyto": "jvkukNvnny", "signatures": ["ICLR.cc/2026/Conference/Submission19294/Reviewer_SLXB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19294/Reviewer_SLXB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666066075, "cdate": 1761666066075, "tmdate": 1762931247897, "mdate": 1762931247897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates visual redundancy and compression phenomena in VLMs, with a focus on the relationship between task complexity and vision token specialization. The authors construct a synthetic dataset to systematically vary visual complexity and propose novel compression/ redundancy measurement metrics, including norm-based and rank-based measures, SVD alignment analyses, linear probe evaluations, and token ablation experiments. They conduct zero-shot and fine-tuning experiments on Molmo and LLaMA-v3.2-Vision, analyzing how task type (referring expression vs spatial reasoning) and dataset complexity influence internal representations and compression behavior. The paper concludes with proposed compression strategies and implications for fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Systematic metric design – The work proposes a comprehensive suite of metrics (both norm- and rank-based, plus SVD alignment) to analyze compression and redundancy in VLLMs’ hidden states, offering more granular insight than prior attention-based analyses.\n\n2. Detailed layer-wise analysis – The visualization across layers for different metrics provides an interpretable picture of how visual information is redistributed within models.\n\n3. Task complexity perspective – The link between downstream task complexity and optimal compression levels is well articulated and supported by multiple evaluation angles."}, "weaknesses": {"value": "1. Synthetic dataset reliance – The main analyses are conducted on a fully synthetic dataset designed by the authors, with limited validation of whether the findings generalize to real-world tasks. The COCO and GQA datasets used in the synthetic data experiments were also only analyzed using the metrics proposed in the paper, rather than through more intuitive computations of prediction accuracy.\n\n2. Evaluation metric coverage vs accuracy gains – The paper heavily focuses on reporting compression/ redundancy metrics but lacks direct evidence that these methods can improve benchmark accuracy when applied in compression policies. A simple empirical demonstration of accuracy improvement would make the contribution more tangible.\n\n3. Architectural limitation in scope – Both Molmo and LLaMA-3.2-Vision adopt CLIP-style fixed-resolution vision encoders with image patching (slice into fixed-size tokens). Newer architectures (e.g., Qwen-VL, GLM-VL) use native resolution and dynamic tokenization according to input resolution, potentially altering redundancy/compression behavior. The generality of the conclusions under these architectures is not assessed.\n\n4. Overlap with prior work’s findings – Some behavioral observations[1] have been highlighted in several earlier VLLM diagnostic studies. The novelty claim would benefit from a clearer positioning relative to these works.\n\n[1] Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1WvsFtlMoc", "forum": "jvkukNvnny", "replyto": "jvkukNvnny", "signatures": ["ICLR.cc/2026/Conference/Submission19294/Reviewer_HqRP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19294/Reviewer_HqRP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916125809, "cdate": 1761916125809, "tmdate": 1762931247366, "mdate": 1762931247366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The aim of this work is to propose an analysis on why MLLM still struggle with visually fine grained tasks even if they excel at tasks involving the global image semantic. The authors explore this via statistical analysis on the visual and text tokens in intermediate layers of the LLM as well as different probing mechanisms like training FC classifiers on top of the tokens and randomly dropping them to see impact on performance. Most experiments are run on Molmo on a synthetic dataset created ad hoc from the authors. Few fine tuning experiments use real data. The findings are that there is a high redundancy within the visual tokens on a LLM and that they tend to be optimized for general vision tasks and not fine grained ones. Moreover, If the model are fine tuning on challenging localization tasks most of the representation changes are in the text part of the model and not in the multimodal one."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Authors propose a very detailed statistical analysis to uncover redundancy in the tokens representations within a LLM. A lot of the technique proposed could likelly be re-used for other works interested in uncovering more about the hidden representation of these models.\n\n+ Surprising finding that in the experimental settings of the work fine tuning a model on visual data seems to  overwhelmingly alter text representations while leaving vision representations largely unaltered.\n\n+ Carefully curated creation of synthetic data to support the experimental analysis in the paper."}, "weaknesses": {"value": "a. **Limited experimental analysis**: Most experiments of the paper are performed using only the Molmo MLLM, would have been interesting to see the analysis expanded to other models trained on different data mixtures and with different architectures. The paper does consider llama, but only for the experiments on probes and visual ablations. The analysis of other decoder based MLLM besides Olmo would have made this submission more strong.\n\nB. **Nice analysis, but limited applicability**: While the work does provide some nice insights the findings are not very actionable and mostly provide experimental evidence of behavior that is quite known to practitioners. Namely: that the amount of tokens that can be dropped from a LLM input is a function of how “hard” a task is and that an effective post-training strategy can involve only the LLM without touching the visual encoder in the model. Also the observation that harder visual task will bring more changes in the model can likelly be linked to the perplexity for the model on those tasks while training. If on average the tasks are harder for the model they could cause higher gradients which in turn results in a bigger shift in the text components of the visual model. \n\nC. **Small scale controlled experiments**: The paper does a nice job at creating a setting where the claim can be tested and isolated, but it does not verify whether the claim holds on bigger and more realistic settings. For example the token analysis in Sec. 4.1 is all performed only on synthetic images, while the fine tuning experiments in Sec. 4.2 consider fine tuning only on (few) visual tasks, while in practice most MLLM would be fine tuned on a way bigger mixture of visual and textual tasks. Exploring what happen in the more realistic settings would have made the submission stronger."}, "questions": {"value": "1. What’s the text prompt for the synthetic dataset you generated?\n\nFew typos\nL311: “into have”\nL481: “are more require”\nAcross the paper you read few times “muiltimodal” instead of “multimodal”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "y8GCsdkXuL", "forum": "jvkukNvnny", "replyto": "jvkukNvnny", "signatures": ["ICLR.cc/2026/Conference/Submission19294/Reviewer_pQNZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19294/Reviewer_pQNZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933240590, "cdate": 1761933240590, "tmdate": 1762931246920, "mdate": 1762931246920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper conducts an in-depth study on the problem of visual information redundancy in vision-language large models (VLLMs), and points out that visual information redundancy is one of the important reasons for the poor performance of the model in complex visual tasks, such as fine-grained object recognition and spatial reasoning. The author constructed a synthetic dataset, quantified the complexity of tasks, and found that complex tasks (such as object counting) require more specialized visual tokens, have lower redundancy, and are sensitive to compression. However, simple tasks (such as color recognition) are not sensitive to redundancy and can even tolerate up to 99% token discard. Through fine-tuning experiments on the model, the author found that fine-tuning mainly altered the text representation of the model, while the visual representation changed relatively little. Moreover, different types of tasks (spatial reasoning vs. object localization) affected the internal representation of the model in different ways. Based on these findings, the authors proposed compression strategies and training suggestions for VLLMs, namely, appropriately compressing information in the early layers, carefully compressing in the middle layers, reducing the compression ratio for complex tasks, significantly compressing for simple tasks, and paying more attention to the updates of text and multimodal projection layers during fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper proposes and comprehensively applies multiple quantitative indicators (such as Gini coefficient, stable rank, participation rate, etc.) to systematically analyze visual information redundancy from the two levels of token norm and matrix rank, surpassing previous studies that only focused on attention distribution and providing a more comprehensive tool for understanding the internal visual information processing of VLLMs.\n\n- The experiments precisely controls variables through the construction of synthetic datasets, the negative correlation between task complexity (such as the number of objects and the difficulty of spatial reasoning) and the degree of visual information redundancy was clearly verified for the first time, providing direct evidence for explaining the performance bottleneck of VLLMs in complex visual tasks"}, "weaknesses": {"value": "- Some findings, such as \"there is a connection between task complexity and visual compression\", are similar with the conclusions given in previous works like PDrop[1]. \n\n- Fine-tuning experiments are only based on simplified subsets of COCO and GQA (such as objects with only \"left-right\" relationships), and more complex spatial relationships (such as spatial reasoning in ERQA) have not been tested, which may underestimate the model's redundant performance in real complex tasks.\n\n- The experiment mainly uses syntheti5c data of simple geometric shapes (fixed color/shape/size), lacking complex factors such as texture, occlusion, and lighting changes in real images, which may lead to insufficient generalization of the conclusion in real scenes.\n\n[1] Xing, et al. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. CVPR, 202"}, "questions": {"value": "- How to specifically configure the vision token compression on the task with different complexities? And, will the configuration setting be quite different among different types of models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vILhLPNLbH", "forum": "jvkukNvnny", "replyto": "jvkukNvnny", "signatures": ["ICLR.cc/2026/Conference/Submission19294/Reviewer_QipL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19294/Reviewer_QipL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762097432889, "cdate": 1762097432889, "tmdate": 1762931246494, "mdate": 1762931246494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}