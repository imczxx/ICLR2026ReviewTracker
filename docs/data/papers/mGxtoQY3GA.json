{"id": "mGxtoQY3GA", "number": 19196, "cdate": 1758294290601, "mdate": 1759897052699, "content": {"title": "H$^3$GNNs: Harmonizing Heterophily and Homophily in GNNs via Self-Supervised Node Encoding", "abstract": "Graph Neural Networks (GNNs) have made significant advances in representation learning on various types of graph-structured data. However, GNNs struggle to simultaneously model heterophily and homophily, a challenge that is amplified under self-supervised learning (SSL) where no labels are available to guide the training process. This paper presents H$^3$GNNs, an end-to-end graph SSL framework designed to harmonize heterophily and homophily through two complementary innovative perspectives: (i) Representation Harmonization via Joint Structural Node Encoding. Nodes are embedded into a unified latent space that retains both node specificity and graph structural awareness for harmonizing heterophily and homophily. Node specificity is learned via linear and non-linear node feature projections. Graph structural awareness is learned via a proposed Weighted Graph Convolutional Network (WGCN). A self-attention module enables the model learning-to-adapt to varying levels of patterns. (ii) Objective Harmonization via Predictive Architecture with Node-Difficulty–Aware Masking. A teacher network processes the full graph. A student network receives a partially masked graph. The student is trained end-to-end, while the teacher is an exponential moving average of the student. The proxy task is to train the student to predict the teacher’s embeddings for all nodes (masked and unmasked). To keep the objective informative across the graph, two masking strategies that guide selection toward currently hard nodes while retaining exploration are proposed. Theoretical underpinnings of H$^3$GNNs are also analyzed in detail. Comprehensive evaluations on benchmarks demonstrate that H$^3$GNNs achieves state-of-the-art performance on heterophilic graphs (e.g., +7.1% on Texas, +9.6% on Roman-Empire over the prior art) while matching SOTA on homophilic graphs, and delivering strong computational efficiency.", "tldr": "", "keywords": ["Graph Neural Networks", "Heterophily and Homophily", "Self Supervise Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/abc84393b3dcce601cab788dde7331cd048419b0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses graph self-supervised learning under mixed homophily/heterophily and proposes H3GNNs, which “harmonizes” both patterns via two parts: (1) representation harmonization through Joint Structural Node Encoding that fuses linear/MLP node projections with 1-hop/K-hop WGCN features via a lightweight Transformer, and (2) objective harmonization using a teacher–student predictive setup with an all-node prediction loss and node-difficulty–aware dynamic masking. Empirically, H3GNNs yields consistent gains on heterophilic datasets (e.g., Roman-Empire, Chameleon, Squirrel; also Texas/Cornell in clustering) while matching SOTA on homophilic ones, with competitive memory and faster convergence than strong SSL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper has a clear motivation that SSL needs to solve the problems of homophily and heterophily. The warm-up experiment in the preliminary has demonstrated the necessity of building a general model for different kinds of graphs.\n\n2. It seems the proposed model has consistent gains on the evaluation datasets. On the Wisconsin datasets, the improvement compared with other baselines is non-trivial.\n\n3. The theoretical analysis of the convergence is a strong support for the effectiveness of the method. It has better k-means clustering (notably large margins on Texas/Cornell) and parity on Citeseer, supporting embedding quality beyond linear probing."}, "weaknesses": {"value": "1. My first concern about this paper is the presentation, which needs improvement. For example,  the introduction is a bit wordy: introduction of GNN -> the limitation of semi-supervised methods -> the limitations of SSL methods -> how do the existing works solve these limitations. My advice is that the limitations of SSL methods can be put in advance to highlight the motivation of this paper. The current version will degrade the true motivation and novelty of this paper. Another example is lines 93-96, which is too hard to understand as an important sentence appearing in the introduction part. Finally, the authors should summarize the contributions at the end of the introduction.\n\n2. Additionally, the presentation makes it a bit hard to follow the techniques presented in this paper. For example, the paper highlights the “node-difficulty” novel in many places, but it is hard to find the evidence demonstrating what the “difficulty” is. Another example is that this paper uses the exploitation ratio. What does the “exploitation approach” mean? In summary, this paper utilizes many self-proposed terms and needs to make the paper easier to follow.\n\n3. This paper spends much space demonstrating the theoretical convergence, which provides multiple bounds to highlight the strong theoretical support of this paper. However, in the experimental evaluations of section 4.3, the improvement compared with other methods is not that significant (except for the result on Actor). The imbalance between the detailed analysis and the practical effectiveness needs to be re-evaluated or explained."}, "questions": {"value": "Q1. What does the node-difficulty means? And what is the rationale of using the exploitation ratio?\n\nQ2. Can you give the time complexity analysis? and how this compares with existing approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OuXdTFD4NF", "forum": "mGxtoQY3GA", "replyto": "mGxtoQY3GA", "signatures": ["ICLR.cc/2026/Conference/Submission19196/Reviewer_Gv2M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19196/Reviewer_Gv2M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659587703, "cdate": 1761659587703, "tmdate": 1762992707061, "mdate": 1762992707061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes H3GNNs, a self-supervised framework aimed at unifying the learning of homophilic and heterophilic graphs within a single model. It introduces a teacher–student predictive architecture with dynamic node-difficulty-based masking to create informative self-supervision signals, and a joint structural node encoding module that fuses linear, nonlinear, and structural features through a Weighted GCN and Transformer-based hierarchical fusion. Experiments on multiple benchmark datasets show that H3GNNs outperforms prior self-supervised GNN models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper shows reasonable originality by combining teacher–student predictive learning, dynamic masking, and weighted structural encoding to address both homophily and heterophily in self-supervised GNNs.\n2.The technical quality is solid, with extensive experiments and consistent gains across benchmarks, though some design choices and analyses lack depth. \n3.The paper is clearly written overall."}, "weaknesses": {"value": "1.The roles of WGCN, MLP, linear projection, and transformer fusion are not clearly separated or analyzed; it is unclear how each module contributes to handling homophily and heterophily individually.\n2.The dynamic masking update relies on previous loss values, yet the paper does not specify how frequently these scores are recomputed or how the warm-up and exploitation phases are scheduled.\n3.The efficiency comparison table does not include information about the hardware platform, GPU type, or framework version, so the claimed improvements in training time and memory cannot be verified.\n4.The paper lacks systematic sensitivity analysis for critical hyperparameters like the number of WGCN layers.\n5.On homophilic datasets, the reported improvements are minor and may fall within variance, yet no statistical tests or confidence intervals are presented to confirm significance.\n6.The theoretical analysis assumes strong convexity and smoothness for deep GNNs, which are unrealistic; hence the derived convergence results have limited practical meaning."}, "questions": {"value": "1.Could the authors provide a clearer explanation or visualization of how the WGCN, MLP, linear projection, and transformer fusion interact? It is not obvious which module primarily contributes to handling heterophily versus homophily.\n2.Why are exactly four “tokens” used in the joint encoding, and why is the fusion order fixed? Have the authors tried using different token numbers or fusion sequences, and how sensitive is performance to these choices?\n3.How are the learnable edge weights in WGCN regularized or constrained to prevent degenerate solutions (e.g., trivial scaling or sparsity collapse)? Are they shared across layers or trained independently?\n4.The dynamic masking variants show small numerical differences. Can the authors provide more analysis (e.g., convergence behavior, difficulty distribution, or qualitative examples) to demonstrate why dynamic masking is preferable to random masking?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "woVktQuJ5J", "forum": "mGxtoQY3GA", "replyto": "mGxtoQY3GA", "signatures": ["ICLR.cc/2026/Conference/Submission19196/Reviewer_RngT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19196/Reviewer_RngT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796279678, "cdate": 1761796279678, "tmdate": 1762931193813, "mdate": 1762931193813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a unified SSL framework addressing the challenge of learning on graphs with mixed structural patterns. It achieves this through Representation Harmonization (joint structural node encoding with WGCN and self-attention) and Objective Harmonization (a predictive teacher–student architecture with dynamic masking). The method effectively balances homophilic and heterophilic signals, showing strong performance across benchmarks. Overall, H3GNNs offers a significant advancement in adaptive, structure-aware graph self-supervised learning."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The strength of this paper lies in its innovative unification of homophily and heterophily modeling within a self-supervised graph learning framework. The authors identify a key limitation in existing GNN and SSL methods—the inability to handle mixed structural patterns—and propose a comprehensive solution (H3GNNs) that achieves both representation harmonization and objective harmonization. It provides both stability and adaptability in learning from complex graph structures. Moreover, the paper is well-written, with extensive experiments showing state-of-the-art performance on heterophilic and mixed graphs, demonstrating strong generalization and interpretability."}, "weaknesses": {"value": "The paper lacks experiments on large-scale graphs and comparisons with more recent or relevant algorithms, limiting its scalability claims. Efficiency analysis is narrow, involving few baselines without clear justification for their selection as representative methods. Broader comparisons and rationale would strengthen the empirical evaluation and conclusions."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8xtjjANisN", "forum": "mGxtoQY3GA", "replyto": "mGxtoQY3GA", "signatures": ["ICLR.cc/2026/Conference/Submission19196/Reviewer_6ntd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19196/Reviewer_6ntd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979253658, "cdate": 1761979253658, "tmdate": 1762931193071, "mdate": 1762931193071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified SSL framework for both homophilic and heterophilic graph. The framework is built on a teacher-student predictive architecture, where the student network achieves representation harmonization via joint structural node encoding. The experimental results show the effectiveness of the proposed framework."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe idea of : teacher–student predictive architecture is interesting, which eliminate the need for complex negative sampling.\n\n2.\tThe author provides theoretical analysis of the proposed method, although the analysis is really complicated and hard to understand.\n\n3.\tThe proposed method outperforms existing SSL methods on heterophilic graph datasets."}, "weaknesses": {"value": "1. Besides the teacher-student predictive architecture, the overall novelty is limited and the design of the student network is somewhat engineering (e.g., Learning Multi-Head Self-Attention and Fusing and Selecting Tokens Hierarchically as SSL Node Encoding). The motivation of these strategies is unclear.\n\n2. The overall framework involves huge memory and computation overhead. It is suggested to analyze the memory complexity and computation complexity in detail. The authors perform computation and memory comparisons in Section 4.3. It would be better to perform similar experiments on more datasets (i.e., homophilic graphs and large-scale graphs).\n\n3. Many details are missing. For example, in Section 3.1, the authors claimed that the teacher network is not trained. It is not clear how to get the parameters of the teacher network. Moreover, the architecture of the teacher network is also not introduced."}, "questions": {"value": "see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9Z8jBIBv3K", "forum": "mGxtoQY3GA", "replyto": "mGxtoQY3GA", "signatures": ["ICLR.cc/2026/Conference/Submission19196/Reviewer_VdwQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19196/Reviewer_VdwQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988141976, "cdate": 1761988141976, "tmdate": 1762931192570, "mdate": 1762931192570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}