{"id": "dtykVtq2cZ", "number": 11740, "cdate": 1758203442885, "mdate": 1759897557721, "content": {"title": "Variance Reduced Distributed Non-Convex Optimization Using Matrix Stepsizes", "abstract": "Matrix-stepsized gradient descent algorithms have been shown to have superior performance in non-convex optimization problems compared to their scalar counterparts. The det-CGD algorithm, as introduced by Li et al. (2023), leverages matrix stepsizes to perform compressed gradient descent for non-convex objectives and matrix smooth problems in a federated manner. The authors establish the algorithm’s convergence to a neighborhood of a weighted stationarity point under a convex condition for the symmetric and positive-definite matrix stepsize. In this paper, we propose two variance-reduced versions of the det-CGD algorithm, incorporating MARINA and DASHA methods. Notably, we establish theoretically and empirically, that det-MARINA and det-DASHA outperform MARINA, DASHA and the distributed det-CGD algorithms in terms of iteration and communication complexities.", "tldr": "This paper proposes two variance reduced matrix step-sized federated learning algorithms for non-convex objectives.", "keywords": ["Federated Learning", "Non-Convex Optimization", "Optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/285302bed999f8b41bc323f2686a959ffc0de689.pdf", "supplementary_material": "/attachment/5e5256572d0cae7ec5a1093853332c6ee90ff77a.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes two variance-reduced distributed algorithms, det-MARINA and det-DASHA, for nonconvex finite-sum optimization in federated learning settings. Building on the det-CGD method, the authors incorporate variance reduction techniques from MARINA and DASHA to eliminate the convergence neighborhood issue caused by stochastic compressors in det-CGD. Under matrix smoothness assumptions, they prove convergence with improved iteration and communication complexities compared to scalar counterparts and det-CGD. Experiments on LIBSVM datasets show improved iteration/communication metrics over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The integration of matrix stepsizes with variance reduction addresses a key limitation of det-CGD by removing the non-vanishing error term, leading to better theoretical bounds.\n\n2. The authors provide clear convergence guarantees under matrix Lipschitz assumptions, highlighting the advantages of the proposed methods.\n\n3. Experiments validate the effectiveness of the proposed method, showing det-MARINA and det-DASHA outperform baselines in communication and iteration efficiency."}, "weaknesses": {"value": "1. Considering matrix smoothness is not very standard in general stochastic optimization problems, the authors should give more discussion about this assumption, including how it can be satisfied in practical problems and the comparison with the standard smoothness assumption.\n\n2. Although the authors provide many experimental results, more complicated and real-world scenarios or tasks would largely strengthen the experimental part.\n\n3. How do the authors recommend choosing or approximating the matrix stepsize D in practice, especially when the full Hessian or smoothness matrices are expensive to compute? Are there heuristics or approximations beyond $D = \\gamma w W$? More discussions here may be helpful.\n\n4. It is well-known that the optimal complexity for finite-sum optimization is $\\mathcal{O}(\\sqrt{n} \\epsilon^{-2})$. How can we understand the order of $n$ in the convergence rate for matrix stepsize methods?"}, "questions": {"value": "See the Weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EVXxNJhn4V", "forum": "dtykVtq2cZ", "replyto": "dtykVtq2cZ", "signatures": ["ICLR.cc/2026/Conference/Submission11740/Reviewer_mMKq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11740/Reviewer_mMKq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578469158, "cdate": 1761578469158, "tmdate": 1762922773234, "mdate": 1762922773234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies federated learning optimization methods with matrix stepsizes. Two new FL algorithms were proposed, det-MARINA and det-DASHA. These works extend the existing det-CGD algorithm. The two new algorithms mainly address the variance reduction aspect. As a result, the proposed algorithms are able to exhibit a superior convergence bound, exceeding the neighborhood limitation presented in det-CGD and other SGD-style methods. Experiments show that the two proposed algorithms require less iteration complexity while being as communication efficient as existing algorithms such as det-CGD."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces variance-reduced variants of det-CGD, Although MARINA and DASHA have been relatively well-studied, the reviewer believes it is still a nontrivial and meaningful extension. The proposed det-MARINA and det-DASHA algorithms display noticeable improvement compared to previous algorithms.\n\nThe theoretical analysis seems solid and rigorous to the best of my knowledge. The analysis is well explained and mostly intuitive. The statements regarding iteration and communication complexity are of interest to potential readers.\n\nThe experiments utilize synthetic objective functions that satisfy the function assumptions, and the numerical experiments verify the effectiveness of the proposed algorithms. The authors also provided a comparison between algorithms in terms of communication bytes."}, "weaknesses": {"value": "While the experiments effectively recover the technical assumptions on the objective function and serve as verification of the analysis, they are, for the most part, still synthetic toy examples. Since the paper addresses federated learning problems, which are largely motivated by practical applications, it would be helpful if the authors also provided further practical results with real-life FL tasks.\n\nThe writing of this paper has much room for improvement. \n- The authors provided a mathematically sound introduction and related works; however, they did not explain the motivation or the necessity of this work. What are the properties of CGD? Why is the previous method named det-CGD? More explanations should be added for this paper to be accepted as a conference paper.\n- The author also used terms such as det-CGD and det-CGD1/2 interchangeably with the assumption that readers have read the \"original paper\", which the reviewer assumes to be Li et al.(2024).\n- In terms of notations, the authors have switched notation from D to W, and introduced matrices L, S, D in Section I, while the det-CGD algorithm is introduced two pages later.\n- Many mathematical definitions, such as matrix smoothness, should be provided in the manuscript."}, "questions": {"value": "I wonder how the heterogeneity across agents affects the convergence of distributed det-CGD and the two proposed algorithms in this setting? Does the variance from CGD and the variance from FedAvg compound in practice and theory?\n\nHow does the computational complexity of Compressed Gradient Descent affect the general cpu wall time of the proposed algorithms? \n\nIn practice, are the gradients first calculated in full and then compressed, or are they directly estimated as compressed signals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9dd71aNoN6", "forum": "dtykVtq2cZ", "replyto": "dtykVtq2cZ", "signatures": ["ICLR.cc/2026/Conference/Submission11740/Reviewer_un1o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11740/Reviewer_un1o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642862282, "cdate": 1761642862282, "tmdate": 1762922772727, "mdate": 1762922772727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces two algorithms, det-MARINA and det-DASHA, aimed at federated non-convex optimization with communication compression. These methods extend the matrix-stepsize algorithm det-CGD by incorporating variance reduction techniques (MARINA and DASHA, respectively). The primary contribution is the theoretical demonstration that these variance-reduced versions overcome the main limitation of det-CGD, which is convergence only to a neighborhood of a stationary point, and instead achieve convergence to a true stationary point with an O(1/K) rate, measured in a determinant-normalized gradient norm (Theorems 4.1 and 5.1). The paper derives conditions for the matrix stepsize D and proposes a practical relaxation by setting D = γW, providing explicit formulas for the scalar γ. Complexity analyses suggest improvements over scalar MARINA/DASHA and det-CGD. Empirical results on logistic regression problems confirm faster convergence in terms of communication bytes compared to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper removes the residual error present in det-CGD convergence analyses and proves clear O(1/K) convergence to a stationary point under the matrix Lipschitz assumption.\n\n2. The theoretical results appear correct and consistent, with checks showing that the framework recovers known results for scalar MARINA/DASHA and matrix gradient descent.\n\n3. The relaxation D = gamma*W and explicit formulas for gamma make the method easier to use in practice."}, "weaknesses": {"value": "1. The paper’s novelty is a theoretical synthesis of matrix stepsizes and variance reduction, which successfully removes the convergence\nneighborhood of det-CGD. While the analysis is careful, many steps of the proof are direct translations of variance-reduction proofs to the matrix norm setting. The manuscript does not isolate what specific technical challenges (if any) required new analytical techniques.\n\n2. The method’s reliance on knowing or accurately estimating the matrix L (or local Li) remains a major practical limitation, The paper acknowledges this (”Availability of L”, Sec 5.1) but lacks a convincing practical strategy or robustness analysis regarding estimation errors. This significantly undermines the applicability of the results.\n\n3. The experiments do not sufficiently justify the method’s added O(d2) complexity. The evaluated logistic regression\ntasks on LibSVM datasets neither reflect moderate-scale non-IID federated benchmarks nor include ill-conditioned settings where matrix stepsizes should provide clear benefits. The observed gains over scalar methods are modest, offering limited empirical justification for the added complexity."}, "questions": {"value": "1. What parts of your analysis are genuinely new or rely on arguments that differ from scalar variance reduction proofs? Clarifying this would help readers understand the technical novelty.\n\n2. How sensitive are det-MARINA and det- DASHA to errors in estimating L or Li? Showing results with controlled under- and over-estimation would make the practical stability clearer.\n\n3. For W = diag−1(L), what efficient methods can estimate or update diag(L) in large-scale federated settings?\n\n4. The experiments use small logistic regression problems. Can you include larger benchmarks such as CIFAR-10 or CIFAR-100 with non-IID clients and moderate neural networks to show that the approach scales and remains effective in realistic scenarios?\n\n5. Could you test a problem with strong ill-conditioning where matrix stepsizes give much larger speedups over scalar methods? This would better justify their added complexity of your approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UmQVDUWm9J", "forum": "dtykVtq2cZ", "replyto": "dtykVtq2cZ", "signatures": ["ICLR.cc/2026/Conference/Submission11740/Reviewer_nSpN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11740/Reviewer_nSpN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825373356, "cdate": 1761825373356, "tmdate": 1762922772251, "mdate": 1762922772251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}