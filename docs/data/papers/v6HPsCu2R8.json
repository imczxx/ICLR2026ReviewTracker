{"id": "v6HPsCu2R8", "number": 18640, "cdate": 1758289692908, "mdate": 1759897090174, "content": {"title": "Hedonic Neurons: A Mechanistic Mapping of Latent Coalitions in Transformer MLPs", "abstract": "Fine-tuned Large Language Models (LLMs) encode rich task-specific features, but the form of these representations—especially within MLP layers—remains unclear. Empirical inspection of LoRA updates shows that new features concentrate in mid-layer MLPs, yet the scale of these layers obscures meaningful structure. Prior probing suggests that statistical priors may strengthen, split, or vanish across depth, motivating the need to study how neurons work together rather than in isolation. We introduce a mechanistic interpretability framework based on coalitional game theory, where neurons mimic agents in a hedonic game whose preferences capture their synergistic contributions to layer-local computations. Using top-responsive utilities and the PAC-Top-Cover algorithm, we extract stable coalitions of neurons—groups whose joint ablation has non-additive effects—and track their transitions across layers as persistence, splitting, merging, or disappearance. Applied to LLaMA, Mistral, and Pythia rerankers fine-tuned on scalar IR tasks, our method finds coalitions with consistently higher synergy than clustering baselines. By revealing how neurons cooperate to encode features, hedonic coalitions uncover higher-order structure beyond disentanglement and yield computational units that are functionally important, interpretable, and predictive across domains.", "tldr": "Use Game theory to compute synergistic neuron coalitions in LLM MLPs", "keywords": ["mechanistic interpretability", "feature discovery", "MLPs"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/05f10287326c4c4cea3fc97f0f3644cf9e730492.pdf", "supplementary_material": "/attachment/db73f2dbe2f041fcd214ea90897d0fb34b53b63d.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a game-theoretic interpretability framework that models neurons in fine-tuned LLM MLP layers as agents in a hedonic cooperative game, capturing their synergistic interactions. Using PAC-Top-Cover to identify stable neuron coalitions, the method reveals functionally indispensable groups that co-adapt during fine-tuning and correspond to interpretable, task-relevant features. Applied to LLaMA, Mistral, and Pythia rerankers, the approach achieves higher synergy and stronger alignment with IR heuristics than clustering or SAE baselines, offering a principled path toward mechanistic understanding of mid-layer representations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a novel game-theoretic formulation of mechanistic interpretability, modeling neurons as agents in a hedonic game to capture synergistic interactions. Understanding the MLP layers is always important and challenging for MI field, and this paper provides a very interesting method. \n\n- The paper is well-organized and clearly motivated, effectively connecting theory, method, and empirical findings. Minor improvements could include more visual intuition for coalition dynamics, but overall readability is high.\n\n- The method is sound in theory with a good amount of theoreitcal justifications, and good amount of empirical supports."}, "weaknesses": {"value": "- The results and analysis are still exploratory in nature as acknowledged in the paper. \n- The task evaluated in this paper is limited in diversity. This narrow task scope may limit generalizability of the proposed framework to other domains (e.g., reasoning, summarization, or generation)\n- The limited diversity may come from another concerns on the scalability of the method and the computational cost. \n- The analysis focuses on LoRA updates in mid MLP layers (7–14) based on prior work’s observation of task activity. This may introduce selection bias: the results may not generalize (in more of a scalability sense) to other layers, architectures, or non-LoRA fine-tuning setups. An ablation showing whether the method would still detect meaningful coalitions if applied to other layer ranges might be nice. (not required especially considering the length of rebuttal and discussion period.)"}, "questions": {"value": "please refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EJUdZo1fDs", "forum": "v6HPsCu2R8", "replyto": "v6HPsCu2R8", "signatures": ["ICLR.cc/2026/Conference/Submission18640/Reviewer_SAy9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18640/Reviewer_SAy9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879215123, "cdate": 1761879215123, "tmdate": 1762928351512, "mdate": 1762928351512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uses coalition game theory to group neurons within LoRA fine-tuned transformers into coalitions, treating neurons as agents in a hedonic game. Neurons are grouped together based on which other neurons they strongly associate with, either by weight/activation metrics (orthogonal co-activation) or functional importance (pairwise ablation synergy). Such coalitions, when ablated, cause the largest decreases in downstream performance and track the closest with known IR heuristics. These coalitions are compared across layers, with the authors finding that coalitions tend to predominantly vanish across successive layers, with some persisting and splitting, but very few merging."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This formulation of neurons as agents in a hedonic game seems very novel and is an interesting lens through which to interpret neural networks.\n2. The Hedonic-MFC algorithm, using either OCA or PAS, finds coalitions that are more important than baseline clustering methods, both in terms of function via ablation and in terms of correlation with known IR heuristics.\n3. The study into the dynamics from layers 7 to 14 across the three models are interesting and support the claims about how cooperative units are predominantly pruned in successive layers, with some of them persisting or splitting, but with very few, if any, fusing.\n4. The paper is written well and handles its two often disjoint subject matters in an easy to follow manner."}, "weaknesses": {"value": "1. The LoRA setting seems somewhat restrictive, why was this chosen? What advantages does this have over using a pretrained language model, with ablations being the zeroing out of neuron weights instead of the reset to pre-LoRA values? \n2. The OOD performance drop for Hedonic-PAS seems partially circular: because these coalitions were selected to jointly have an outsized effect under ablation, it follows that performance loss would also be strongest here when these groups are ablated (though this is mitigated somewhat by the IR heuristic correlation).\n3. The baselines presented in the paper do not seem completely fair from an information access perspective. The k-means and hierarchical clusterings both receive either weight or activation information. Hedonic-OCA receives both, and Hedonic-PAS receives an even stronger signal in outsized joint ablation degradation between two neurons. A stronger baseline should be considered for both k-means and hierarchical clustering, where they have access to the increased information that the Hedonic-MFC algorithms have access to. Do you see the same performance of Hedonic-MFC compared to, for example, standard graph community detection algorithms on the same information metrics?\n4. Despite the tracking of the coalition activations with known IR heuristics, the coalitions themselves are not further explored in terms of their function. Is there any analysis that the authors have done that can reveal interpretable contexts under which the coalitions fire, such as context or topic?\n\nDespite these weaknesses, the novel framing of the paper, the performance of Hedonic-MFC in finding important coalitions, and the study of across-layer dynamics motivate my score of 6. As a caveat, I am not very familiar with game theory, which motivates my low confidence score, but the authors explained the necessary background information well."}, "questions": {"value": "See weaknesses for full explanation, but summarized:\n1. Why LoRA?\n2. How can the partially circular performance of Hedonic-PAS be justified?\n3. Are the Hedonic-MFC algorithms competitive against stronger clustering baselines that access the same information?\n4. Do the coalitions represent any interpretable features?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "f7e01N1c2M", "forum": "v6HPsCu2R8", "replyto": "v6HPsCu2R8", "signatures": ["ICLR.cc/2026/Conference/Submission18640/Reviewer_tscc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18640/Reviewer_tscc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889777627, "cdate": 1761889777627, "tmdate": 1762928350913, "mdate": 1762928350913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for finding groups of neurons in transformer MLP layers that work together in a non-additive way, meaning their combined effect is greater than the sum of their individual contributions. The authors use tools from game theory, specifically hedonic coalition formation, to identify these “neuron coalitions” and show that they are functionally important by measuring performance drops after removing these neurons. They apply their method to language models fine-tuned with LoRA for information retrieval tasks and find that the identified coalitions are more functionally important and i\"nterpretable\" (based on the previously mentioned ablation and alignment with known heuristics) than baseline clustering methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Across models, ablating hedonic neurons clearly harms results more than ablating coalitions identified by clustering methods based purely on neuron similarity or correlation. This indicates empirically that the method is identifying groups of neurons which are more structurally essential in performing the task.\n- The application of hedonic game theory to neuron analysis is original within the interpretability literature from what I can tell and this work develops a connection between a new area of cooperative game theory and neural network analysis.\n- The work has extensive reproductibility documentation provided as part of the supplementary material.\n- The work proposes a computationally tractable way to measure this theoretical connection to game theory in NNs, which is often challenging for game theoretical given the relatively small number of players in game theory experiments/problems relative to the large number of players/layers in a neural network."}, "weaknesses": {"value": "This work is relatively far from my area of research (see the low confidence score) and therefore I cannot comment too heavily on the exact technical novelty. I have done a literature review of the space to try and be an informed reviewer, but all following comments admittedly come with that asterisk.\n\nThe biggest concern I have with regards to this work is to what degree it delivers on the \"interpretability\" aspect of mechanistic interpretability. The authors propose a novel framework for identifying synergistic neuron coalitions and present a number of quantitative evaluations suggesting that these coalitions capture important internal structure. However, it is less clear whether this structure yields human-understandable insights. For example, while the coalitions are shown to affect model performance and align with some IR heuristics, there is little evidence that they correspond to semantically meaningful concepts or behaviors. \n\nUnlike other work in mechanistic interpretability, such as SAEs, the paper does not provide labeled interpretations or vizualizations that would help validate the value of these coalitions. Similarly, unlike core mechanistic discoveries, such as inductive heads, the coalitions cannot be mapped to a particular mechanism the model performs. Further work tying the ablation of these coalitions to concrete changes model behaviors or concepts would strengthen the interpretability claim.\n\nAs an aside, from the model pruning/ model compression angle, it is somewhat surprising the work doesn't try to connect to established game theoretical works applied to neural networks such as Neuron Shapley values. Removing the top-k neurons would be a stronger baseline than the clustering based approaches evaluated I believe."}, "questions": {"value": "- What is the compute cost and/or runtime of computing Hedonic coalitions v.s. the strongest clustering based approach? It seems like given the large number of samples required and the long wall-clock runtime that it might be far more expensive than a clustering based approach, but this cost is never compared to the baselines. Is it O(n^2) in the number of neutrons (which seems intractible, but is stated on line 467)?\n\n- The Hedonic sampling procedure has a relatively large number of hyperparameters. How were these swept and or otherwise decided upon?\n\n- Why was LoRA selected as the Finetuning method to study? It seems that Hedonic coalitions could be learned even for pretrained models before Finetuning (if applied to a task with reasonable zero-shot performance) or for models fully fintetuned. If the reason is computational cost, it would be worth noting it as such.\n\n- Why is the method only studied for LLM backbones? The method does not seem LLM/Transformer specific and could be further validated (at relatively lower cost) on smaller networks such as those for simple tasks like CIFAR10 or ImageNet.\n\nWhy was the method only evaluated for re-ranking? As far as I can tell, the method is not specific to this domain so it would be nice to validate it in others since it is being branded as a general purpose method for this type of analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7rhNxLj9PJ", "forum": "v6HPsCu2R8", "replyto": "v6HPsCu2R8", "signatures": ["ICLR.cc/2026/Conference/Submission18640/Reviewer_dFvZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18640/Reviewer_dFvZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899511842, "cdate": 1761899511842, "tmdate": 1762928350225, "mdate": 1762928350225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}