{"id": "k2oet40kT3", "number": 17635, "cdate": 1758278656452, "mdate": 1759897163420, "content": {"title": "Generate the Forest before the Trees - A Hierarchical Diffusion model for Climate Downscaling", "abstract": "Downscaling is essential for generating the high-resolution climate data needed for local planning, but traditional methods remain computationally demanding. Recent years have seen impressive results from AI downscaling models, particularly diffusion models, which have attracted attention due to their ability to generate ensembles and overcome the smoothing problem common in other AI methods. However, these models typically remain computationally intensive. We introduce a Hierarchical Diffusion Downscaling (HDD) model, which introduces an easily-extensible hierarchical sampling process to the diffusion framework. A coarse-to-fine hierarchy is imposed via a simple downsampling scheme. HDD achieves competitive accuracy on ERA5 reanalysis datasets and CMIP6 models, significantly reducing computational load by running on up to half as many pixels with competitive results. Additionally, a single model trained at 0.25Â° resolution transfers seamlessly across multiple CMIP6 models with much coarser resolution. HDD thus offers a lightweight alternative for probabilistic climate downscaling, facilitating affordable large-ensemble high-resolution climate projections. See a full code implementation at: https://github.com/HDD-Hierarchical-Diffusion-Downscaling/HDD-Hierarchical-Diffusion-Downscaling", "tldr": "We introduce a compuationally efficient hierarchcial downscaling method that generalises across multiple resolutions. This allows a single model to be applied across multiple GCMs.", "keywords": ["Climate Downscaling", "Diffusion", "Generative", "Efficient", "Climate"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/43082676b60c9f1964a33e4f2d5325b53648e54d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes HDD: a hierarchical diffusion downscaler that conditions the score network on a shape schedule, learning to denoise + un-coarsen jointly. Training and sampling embed each latent back to full resolution while alternating upsample â†’ predict ðœ– â†’ downsample in the reverse process. The authors derive upper-bound compute savings via average active pixels, with up to â‰ˆ3Ã— theoretical speedup for a linear shrink schedule; they evaluate on ERA5 and CMIP5 GCM transfer."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Conceptual clarity and drop-in design. The paper cleanly formulates a hierarchical diffusion process as a strict generalization of EDM, compatible with existing denoisers. When the shape schedule is identity, it exactly recovers the vanilla EDM objective.\n\n- Domain alignment. The coarse-to-fine structure is well motivated for spatial downscaling, naturally mirroring multi-scale atmospheric organization.\n\n- Evaluation breadth. Benchmarks on ERA5 and several CMIP5 GCMs are thorough. Including RCM (CCAM) comparisons and COâ‚‚ estimates is commendable."}, "weaknesses": {"value": "- Theoretical compute saving not realized. Algorithm 1 explicitly upsamples ð‘§ back to full resolution before passing it to the UNet (fÎ¸(U_t(z), t, (h_t,w_t))). Thus, all expensive convolutional operations still occur at full ð»Ã—ð‘Š. The 3Ã— saving derived from average active pixel area (Î± = 1/3) assumes the model operates natively at reduced resolutions, which it does not. The â€œ3Ã— pixel/FLOP savingâ€ therefore represents a notional bound, not a real efficiency gain. Any empirical runtime saving would be far smaller.\n\n- Underdeveloped novelty relative to prior hierarchical diffusion work. The coarse-to-fine generative idea overlaps with prior Cascaded Diffusion Models and Laplacian Pyramid GANs; the paper does not explicitly contrast HDDâ€™s single-model training with these established cascades.\n\n- Missing recent baselines. Omitted comparisons to strong and relevant lines: CorrDiff (corrective diffusion downscaling), STVD (Spatiotemporal Video Diffusion for precipitation), Neural operators (FNOs) for arbitrary-resolution downscaling, Foundation models like ClimaX used for resolution-agnostic transfer."}, "questions": {"value": "- Score consistency: With upsampling and downsampling operators not being exact inverses, the score used at upsampled resolution approximates a different target than the true full-res score. Can you formalize the induced bias on the score (e.g., via a projection operator) and when it becomes negligible?\n\n- Shapeâ€“noise coupling: You currently draw $\\sigma_t$ and $s_t$ jointly but independently. Did you consider shape-dependent noise schedules (e.g., higher SNR at coarse scales to stabilize global structure)? Any ablation on coupling strategies (e.g., log-SNR that depends on shape)?\n\n- Does Appendix I mirror any changes to the standard diffusion loss and training algorithm suggested in Algorithm 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AIGjQEJyJs", "forum": "k2oet40kT3", "replyto": "k2oet40kT3", "signatures": ["ICLR.cc/2026/Conference/Submission17635/Reviewer_vZbc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17635/Reviewer_vZbc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908881857, "cdate": 1761908881857, "tmdate": 1762927495491, "mdate": 1762927495491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new methodology for training diffusion models. The method makes use of hierarchical structure, noising and denoising at different resolutions, and forcing scale consistency. The method can applied to different diffusion model, making it a plug-and-play with existing architectures. The method is specifically tailored for climate downscaling applications, and is evaluated on multiple benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Method is architecture-invariant, therefore, applicable to a broad range of existing diffusion architectures.\n- Authors identify that weather exhibits similar patterns (power-law at different coarse/fine scales) to diffusion models (high vs low noise frequencies).\n- The theoretical idea of going from coarse to fine, not just with noise but also with spatial resolution is an interesting research direction.\n- Authors carry out an in-depth analysis on the theoretical speed up, justifying the use of scale changes in image resolution.\n- Authors provide extensive ablations."}, "weaknesses": {"value": "- Line 207: \"blue arrows\". All arrows are blue, it would be better to use different colors or make wordking clearer.\n- Evaluation is only carried out for Australian extent. Could authors provide an intuition for how the model performs in other latitudes?\n- Authors claim method is architecture-agnostic yet the use of their two additional scalar inputs implies having to retrain the models, thus, invalidating the plug-and-play claim.\n- Figure 4 layout and explanation can be largely improved. It is not clear what authors want to convey."}, "questions": {"value": "- Do authors explore variability/std of the outputs generated? Given the stochasticity of diffusion models, it would be interesting to see if authors sample multiple solutions for a better estimation of outputs uncertainty."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8HABAXiwyu", "forum": "k2oet40kT3", "replyto": "k2oet40kT3", "signatures": ["ICLR.cc/2026/Conference/Submission17635/Reviewer_Eqqc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17635/Reviewer_Eqqc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931897303, "cdate": 1761931897303, "tmdate": 1762927495026, "mdate": 1762927495026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "### Goal\nThe paper discusses a method to train and sample from a multi-resolution diffusion model. The model is then applied to a transfer learning task, where it is trained on ERA5 (0.25Â°) conditionally on a down-sampled resolution of 1.5Â° and used to downscale samples from expensive general circulation models (GCMs), natively at resolutions ranging from 1.4Â° to 2.5Â°.\n\n### Contributions\nThe contributions of this paper are limited. Many multi-resolution diffusion and flow matching models are present in the literature, but many of them (e.g. [1,2], the closely related cascaded diffusion models [3], and many others) are not even the cited. The few that are [4,5,6] are only cited *en passant*, and are not properly discussed or used as baselines in the experiments. This makes it very hard to decipher what is novel about the proposed Hierarchical Diffusion Downscaling model, beyond the fact that it is a multi-resolution diffusion model applied to a downscaling task. \n\n\n[1] Gu et al. (2023), f-DM: A multi-stage diffusion model via progressive signal transformation. \n[2] Teng et al. (2024), Relay diffusion: Unifying diffusion process across resolutions for image synthesis.\n[3] Ho et al. (2022). Cascaded diffusion models for high fidelity image generation.\n[4] Jin et al. (2024). Pyramidal flow matching for efficient video generative modeling.\n[5] Zhang et al. (2022). Dimensionality-varying diffusion process.\n[6] Campbell et al. (2023). Trans-dimensional generative modeling via jump diffusion models."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* Evaluation on an interesting transfer learning task: the paper shows that a model trained on ERA5 can be used to downscale simulations from GCMs."}, "weaknesses": {"value": "* The paper lacks a comprehensive literature review, making it hard to understand where it is situated / what makes it different in the space of multi-resolution generative models.\n* The experiment is not convincing. The only baseline is a standard EDM, and no other techniques for multi-resolution diffusion are considered. It is also unclear whether hyperparameters were properly tuned for both EDM and HDD, and the lack of confidence intervals / error bars makes it hard to understand the significance of the magnitude in improvement.\n* Lack of details about the task and datasets being used (e.g. why is the downscaling at 0.5Â° if the model was trained to produce images at 0.25Â°? What is the number of samples in each split? etc.)\n* While some of the hypotheses being raised in the paper are interesting, they are presented in a hand-wavy fashion, often resulting in unsubstantiated mathematical claims. For instance, take the claim: \"more dimension-destruction steps is exactly parallel to more time steps in standard DDPM: it yields finer increments, lower local error, and a more faithful final reconstruction\". No evidence is provided beyond the results of the experiment, which I personally find unconvincing.\n* Typos (line 143, 207), messy figure ordering (Fig. 4 is the first being mentioned and the last being displayed), and inconsistent use of capital letters (lines 154, 356) make the paper less clear.\n\nThe paper could strongly benefit from:\n* A thorough literature review.\n* Comparisons with other methods for multi-resolution diffusion.\n* Simple, toy experiments used to validate some of the hypotheses about the improvements in performance over fixed-resolution models."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v2gPkWgrHT", "forum": "k2oet40kT3", "replyto": "k2oet40kT3", "signatures": ["ICLR.cc/2026/Conference/Submission17635/Reviewer_7ZNu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17635/Reviewer_7ZNu"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762498104823, "cdate": 1762498104823, "tmdate": 1762927494630, "mdate": 1762927494630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a hierarchical diffusion model for probabilistic climate downscaling. The key idea is to impose an explicit coarse-to-fine hierarchy on the diffusion process by coupling Gaussian noise injection with progressive spatial downsampling and upsampling. This â€œdimension destructionâ€ allows the model to learn multi-scale representations efficiently, thereby reducing computational cost. HDD achieves comparable or better accuracy than standard diffusion downscaling methods (e.g., CorrDiff) and Earth-ViT while operating on fewer pixels and generalizing across multiple GCMs with varying input resolutions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The proposed coupling of noise scheduling and spatial scaling is elegant and well-motivated by both theoretical (spectral scaling in atmospheric data) and empirical (RAPSD) evidence.\n2) The idea of treating spatial resolution as a conditioning variable in diffusion steps is intuitive and potentially impactful.\n3) The model achieves competitive or superior results across ERA5 and CMIP5 benchmarks, reducing FLOPs and COâ‚‚ footprint significantly.\n4) The authors provide a detailed methodology and open-source implementation, which supports reproducibility."}, "weaknesses": {"value": "1)\tThe paper assumes that hierarchical training (sampling one shape per iteration) leads to unbiased learning of the joint coarse-to-fine distribution, but this is not shown.\n2)\tThere is no ablation on the effect of mismatch between Ïƒ_t and s_t sampling distributions. E.g.,, what happens when fine scales are learned early or skipped frequently?\n3)\tThe model is described as â€œarchitecture-agnosticâ€ but the experiments only use a U-Net backbone. It is unclear how the hierarchical conditioning would integrate with transformer-based or attention-heavy architectures where spatial scaling affects positional embeddings.\n4)\tThe experimental comparison is limited. HDD is only compared against its own baselines (Base EDM, Earth-ViT) but not against other state-of-the-art downscaling methods that have been evaluated on ERA5 or similar reanalysis datasets. Examples of relevant SOTA baselines include:\n[1] Mardani et. al., â€œResidual Corrective Diffusion Modeling for Km-scale Atmospheric Downscalingâ€, Communications Earth & Environmentï¼Œ2025.\n[2] Zhong et. al.ï¼Œâ€œDebias Coarsely, Sample Conditionally: Statistical Downscaling through Optimal Transport and Probabilistic Diffusion Modelsâ€ï¼ŒNeurIPS 2023.\n[3] Ignacio et. al., â€œDynamical-generative downscaling of climate model ensemblesâ€, PNAS, 2025.\n[4] Robbie et. al. , â€œGenerative Diffusion-based Downscaling for Climateâ€\n[5] Declan et. al., â€œResolution-Agnostic Transformer-based Climate Downscalingâ€\n[6] Sebbar et. al., â€œMachine-Learning-Based Downscaling of Hourly ERA5-Land Dataâ€, Atmosphere, 2023.\nMinors:\n1)\tLine 279-280,  panguweather->PanguWeather\n2)\tFigure 4 needs to be better organized."}, "questions": {"value": "1)\tHow is the shape schedule s_t chosen or optimized? Is it learned or hand-crafted?\n2)\tCan the hierarchical diffusion process cause aliasing artifacts during upsampling, and if so, how is this mitigated?\n3)\tCan the same trained model handle multiple target resolutions (e.g., 0.5Â°, 0.25Â°) without retraining?\n4)\tWhy does the Pangu-based Earth-ViT baseline perform so poorly in the reported tables? The results show notably higher RMSE and lower spatial correlation than both HDD and even traditional RCMs. Could this be due to under-training, suboptimal hyperparameter choices, or the use of a configuration not well-suited for downscaling (e.g., lack of resolution-specific finetuning)? Clarifying this is important, as it affects the credibility of the baselines used for comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jARGsaKfJ7", "forum": "k2oet40kT3", "replyto": "k2oet40kT3", "signatures": ["ICLR.cc/2026/Conference/Submission17635/Reviewer_TQRL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17635/Reviewer_TQRL"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission17635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762623772489, "cdate": 1762623772489, "tmdate": 1762927493952, "mdate": 1762927493952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}