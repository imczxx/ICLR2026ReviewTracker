{"id": "GtyMclqDqY", "number": 105, "cdate": 1756728802044, "mdate": 1763730374030, "content": {"title": "NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation", "abstract": "A common method for creating Vision-Language-Action (VLA) models involves fine-tuning pre-trained Vision-Language Models (VLMs) for robotic control. However, this adaptation process often leads to \\textbf{catastrophic forgetting}, where the VLM's original powerful reasoning capabilities are degraded. We identify that this issue stems from a fundamental task conflict: fine-tuning on dense, continuous action trajectories is misaligned with the VLM's pre-training objectives. To tackle this, we propose the \\textbf{Narrowing of Trajectory VLA (NoTVLA)} framework, which mitigates catastrophic forgetting by reframing the action generation task. Instead of dense trajectories, NoTVLA learns to predict sparse, semantically meaningful trajectory 3D points leading to keyframes.\nThis approach aligns the fine-tuning task more closely with the VLM's inherent strengths, preserving its reasoning abilities. A key innovation of NoTVLA lies in its trajectory planning strategy, which uses temporal compression and spatial pruning for the robot end-effector's path. In multi-task evaluations, NoTVLA achieves superior performance and generalization compared to baselines like $\\pi_0$, while using over an order of magnitude less compute and not necessarily need wrist-mounted camera.\nThis design ensures that NoTVLA’s operational accuracy closely approximates that of single-task expert models. Crucially, by mitigating catastrophic forgetting, it preserves the model’s inherent language capabilities, enabling \\textbf{zero-shot generalization} in specific scenarios, supporting unified model deployment \\textbf{across multiple robot platforms}, and fostering generalization even when \\textbf{perceiving tasks from novel perspectives}.", "tldr": "", "keywords": ["Vision Language Model", "Embodied AI", "Computer Vision"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7f4787cb84941af4714db9b0bcca88bb2d8bbac1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Narrowing of Trajectory VLA (NoTVLA) framework, aiming to address catastrophic forgetting in Vision-Language-Action models for robotic manipulation. The core contribution is a shift from dense action trajectory supervision to sparse, semantically salient trajectory supervision. The method features: (1) an anchor-based depth inference module that decouples 2D semantic anchoring from 3D geometric reasoning by leveraging an external depth source, and (2) a kinematics-based keyframe selection strategy to identify critical moments in demonstrations. Evaluations on multi-task benchmarks demonstrate that NoTVLA achieves competitive performance compared to several VLA baselines, with reduced computational cost and improved zero-shot generalization in certain scenarios, while operating under a constrained sensing setup."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1、The paper clearly identifies catastrophic forgetting in VLAs as a key problem and proposes a structured, well-motivated solution.\n2、Provides thorough multi-task evaluation on several robotic platforms and includes informative ablation studies validating the core design choices.\n3、 Demonstrates significant practical advantages, including reduced computational cost and the ability to operate effectively without a wrist-mounted camera.\n4、The anchor-based depth inference module presents a clear and effective decoupling of semantic perception from geometric reasoning enhancing system modularity."}, "weaknesses": {"value": "1、As noted in Section 2.1, a major direction for spatial reasoning involves fine-tuning on 3D-centric datasets. The paper does not compare NoTVLA against such methods, leaving the claimed advantages of its decoupled, anchor-based approach unvalidated against models with more integrated 3D understanding.\n2、The paper fails to demonstrate the advantage of its proposed kinematics-based keyframe selection over established action segmentation or keyframe extraction methods. Without this comparison, the claimed benefits regarding the preservation of task-critical information remain unsubstantiated.\n3、The core hierarchical framework, which decouples a high-level planner (VLM) from a low-level controller, is a well-established paradigm in robot learning. While the specific implementation is competent, the paper does not establish a significant architectural innovation over prior hierarchical systems.\n4、The anchor-based approach, while focused and efficient, inherently narrows the model’s perceptual field. The potential trade-off, specifically the risk of losing broader scene understanding compared to methods that process denser 3D representations, is not sufficiently discussed."}, "questions": {"value": "1、How does NoTVLA’s performance on tasks requiring complex 3D spatial reasoning compare against VLA models that were pre-trained or fine-tuned on 3D-centric datasets? This would help quantify the trade-offs of your decoupled approach.\n2、Could you provide a comparative analysis between your kinematics-based keyframe selection and other sparsification strategies to demonstrate its specific advantages in preserving task-critical information?\n3、Could you discuss potential failure modes or task types where the focused perception of the anchor-based approach might be limiting due to a lack of broader scene context?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7cLffdzsb6", "forum": "GtyMclqDqY", "replyto": "GtyMclqDqY", "signatures": ["ICLR.cc/2026/Conference/Submission105/Reviewer_Lpbg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission105/Reviewer_Lpbg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839126453, "cdate": 1761839126453, "tmdate": 1762915451908, "mdate": 1762915451908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework (NoTVLA)  for a multi-step VLA system that first predicts two image-based keypoints, and then, conditioning on those two keypoints with included depth information, predicts a trajectory key points on the image plane. These key points are projected to the world frame using depth and fit with a spline curve to generate high-frequency controls. They evaluate on Robotwin2 and on AGIBot environments, outperforming baselines across tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper's method is novel and offers a clear interface between high-level keypoint reasoning and low-level control. The empirical results are strong and comprehensive: the method outperforms both per-task expert policies (e.g., ACT, DP, DP3) and generalist models (π0, RDT) across the RoboTwin2 and AGIBot benchmarks. Moreover, the paper demonstrates gains in data and computational efficiency (Figure 4)."}, "weaknesses": {"value": "The main reason for my score is the missing details for their baselines. More clearly:\n\n* The paper does not clearly describe the training setups for its baselines. In particular, it is unclear what data the single-task experts are trained on, and whether pi0 was fine-tuned on the same trajectories used to train NoTVLA. Without these details, it is difficult to assess these comparisons. \n* Including a comparison to works such as Hamster [1] and LLARVA [2] would help show the benefits of the two-stage key point and then spline fitting method over other similar VLA approaches. \n* Some abbreviations (e.g., DP) are introduced without being defined\n* I found the main framework figure distracting (Fig. 1), and it did not seem to have a natural direction for me to parse. Some of the terms in it, such as (u, v) and Bn, were not defined until later in the methods section. \n\n\n[1] Yi Li, Yuquan Deng, Jesse Zhang, et al. HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation.\n\n[2] Dantong Niu, Yuvan Sharma, Giscard Biamby, et al.  LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning."}, "questions": {"value": "* Is the Pi-0 generalist policy fine-tuned on your training dataset from the RoboTwin tasks?\n* What were the failure modes of your method? Does, for example, the target object being occluded cause your method to fail?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1FRPm1DZEa", "forum": "GtyMclqDqY", "replyto": "GtyMclqDqY", "signatures": ["ICLR.cc/2026/Conference/Submission105/Reviewer_41yV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission105/Reviewer_41yV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955528063, "cdate": 1761955528063, "tmdate": 1762915451754, "mdate": 1762915451754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an approach (NoTVLA) that trains a vision-language-action (VLA) model on an intermediary waypoint-based action space (as opposed to the raw EEF position action space). The full process of predicting actions from the VLA looks like the following: (1) given the image and language instruction, the VLA predicts the pixel XY coordinates of the next waypoint (e.g., the pixel coordinates of the object to pick up). (2) A depth sensor (e.g., an RGBD camera) is queried to obtain depth at this pixel coordinate. (3) The VLA is queried again with the image, language instruction, and now pixel XY location and depth to produce N action tokens. These tokens describe the path to take to reach from the current robot position to the waypoint predicted in the first step, with each token encoding an intermediary XY pixel location, depth prediction, gripper state, and EEF orientation. (4) Finally a motion planner is used to generate a smooth trajectory that connects these waypoints and the full trajectory segment is played open-loop. The authors argue that the reason to choose this intermediary action space instead of the original action space as most VLAs do is that (1) action prediction across robot embodiments or very diverse robot states looks more similar, which encourages feature sharing, and (2) this waypoint prediction problem is less of a departure from the original base VLM pre-training than traditional VLA training, allowing for less VLA training compute to be spent and better generalization since the model retains more of its pre-training. On a few simulated benchmarks the authors compare their approach against imitation learning (e.g., diffusion policy) and VLA (e.g., pi0) baselines and find that their approach generalizes better and gets better success rates."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(1) While the use of intermediary action spaces such as waypoints has been studied extensively and is thus not new, I like the authors’ motivation of this design decision in the context of VLAs (see summary for the motivation), though admittedly said motivations were a bit difficult to parse from the writing. In particular, the fact that VLA training tends to require a significant departure from the base VLM is not a widely discussed phenomenon (as far as I’m aware), so it is a strength that this paper has identified this problem and proposed a solution for it.\n\n(2) The technical approach is sound and is justified by ablations (section 4.2.1), and experiments are reasonably thorough (though there seems to be no real-world experiments, which is a weakness).\n\n(3) The empirical results agree with the motivations. Their VLA needs much less compute to train because it is less of a departure from the base VLM, and it also generalizes better than baselines like pi0 to slightly OOD task variations (table 3)."}, "weaknesses": {"value": "(1) Seemingly no real-robot experiments to validate their approach\n\n(2) The authors claim that a strength of their approach is that their policies do not require a wrist mounted camera. However this statement can only be made if the authors have trained policies both with and without wrist mounted cameras, and empirically found that they are not needed. Also I don’t buy that wrist cameras can truly be avoided — there are many tasks for which the arms will occlude the shoulder camera, making wrist cameras necessary. \n\n(3) In the abstract and introduction the authors frequently use the term \"catastrophic forgetting”, but do not sufficiently explain in what context catastrophic forgetting is happening. Is the “forgetting” happening when the base VLM is fine-tuned into a VLA? Or is it happening when the VLA is then further fine-tuned on specific downstream tasks? My assessment after reading the paper is that the authors are referring to the former, but this could be made more clear. \n\n(4) Several statements (concentrated especially in the introduction) are not clear, and it’s very difficult to parse what they mean. Further, many claims are simply unsubstantiated. Listing a few:\n\n- Lines 41 to 45: “catastrophic forgetting […] is aggravated […] by an over-reliance on dense […] action trajectories […] when fine-tuned on a new task the model may overwrite previously consolidated competencies, degrading prior task performance” —> it is unclear to me what this statement means. My approximate takeaway is that the authors are trying to say that action prediction is a significant departure from the base VLM capabilities.\n\n- Similarly lines 46-50: “The root cause of this fragility often lies in the prevailing training paradigm […]”. The authors should avoid making statements like “the root cause of X is Y” without some sort of evidence. This evidence can take the form of prior work citations, or of an easy to follow step-by-step logical argument.\n\n- Line 53: “mitigating catastrophic forgetting” —-> this statement doesn’t explain *how* the approach mitigates catastrophic forgetting. My interpretation is that it makes the VLA action prediction task look a lot more like the base VLM pre-training tasks, but this needs to be explicitly mentioned.\n\n- Figure 1 caption and rest of the introduction: same thing. In what context is catastrophic forgetting happening? Why do the authors think it is happening? Why does their approach solve this?\n\n(5) The use of waypoints diminishes the applicability of the method to only pick-and-place tasks. I.e., can non-pick-and-place tasks, like T-shirt folding, stirring, or tool use be solved by the same method? How will waypoints be selected in such cases?\n\n(6) It seems that with the proposed approach, entire trajectory segments, i.e., everything connecting two key-frames (like a gripper opening and the subsequent gripper closing), are executed completely open-loop. By open-loop I mean the VLA is not inferenced again until the motion planner executes the full trajectory segment. This is both a strength and a weakness; it is a strength in that the motion segment will likely be smooth, but it is a weakness if something unexpected happens while the motion segment is being played, like the object falling from the robot’s grasp, that would require reactivity and more closed-loop control.\n\n(7) The acceleration threshold used to pick keyframes needs to be selected by the data-collector manually per task (line 240), which increases the burden on the data collector."}, "questions": {"value": "(1) In Figure 4 (left), is the x-axis scale correct? The number of training steps for pi0 seems an order of magnitude too large.\n\n(2) There are also a number of questions in the weaknesses section that I would appreciate be addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u94mYBe8G2", "forum": "GtyMclqDqY", "replyto": "GtyMclqDqY", "signatures": ["ICLR.cc/2026/Conference/Submission105/Reviewer_gus3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission105/Reviewer_gus3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762039031279, "cdate": 1762039031279, "tmdate": 1762915451620, "mdate": 1762915451620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed Narrowing of Trajectory VLA (NoTVLA), which tries to solve the catastrophic forgetting issues of fine-tuning VLA models. The paper attributes this issue to the dense trajectory representation, and addresses the issue by applying Keyframe Selection on training data, and performing Anchor Point prediction (APP) with Anchor-conditioned token generation (ACTG) during inference. The actions are decoded with spline-based detokenizer for smooth and high-frequency trajectories. Experiments demonstrate its effectiveness."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is generally well-written and clear. \n2. The paper provides very extensive experiments in both simulation and reality, that show promising results on NoTVLA’s manipulation capabilities, generalizability, and the preserved language reasoning capabilities. The system is also training-efficient.3\n3. The analysis provides rich insights on the language understanding degradation, zero-shot generalization, training efficiency, *e.t.c*."}, "weaknesses": {"value": "## \n\n1. The Kinematics-Based Keyframe Selection relies on an acceleration threshold $\\alpha$ that is \"hand-designed per task.\" This introduces a manual, expert-in-the-loop step that could hinder scalability.\n2. The APP and ACTG modules requires the depth information of the scene. While the VLA system has a good performance, a sensitivity analysis on the depth noise or bias is necessary to show how the system would work with different depth sensors and environments.\n3. The 2D anchor representation could limit some tasks. Additional 2D or 3D representation (*e.g.*, via visual visual prompting, robot-object constraints, *e.t.c.*) could be discussed for comprehensiveness.\n4. Some editorial and writing issues\n    1. While the experiments validate the hypothesis of “dense fine-tuning is the cause of catastrophic forgetting”, the reason or the mechanism is not intuitively explained, especially at the beginning of the paper. This causes confusion and doubts.\n    2. Figure and table captions are not self-contained."}, "questions": {"value": "The language capabilities degradation, while remedies, still occurs as suggests by Tab. 6. Do you have any insight into which specific language reasoning skills (e.g., spatial, logical, mathematical) are most affected by this sparse fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l8UJmZtDIJ", "forum": "GtyMclqDqY", "replyto": "GtyMclqDqY", "signatures": ["ICLR.cc/2026/Conference/Submission105/Reviewer_FiUa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission105/Reviewer_FiUa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762255308555, "cdate": 1762255308555, "tmdate": 1762915451430, "mdate": 1762915451430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}