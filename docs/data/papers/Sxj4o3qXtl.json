{"id": "Sxj4o3qXtl", "number": 18651, "cdate": 1758289740838, "mdate": 1759897089725, "content": {"title": "ATGen: Adversarial Reinforcement Learning for Test Case Generation", "abstract": "Large Language Models (LLMs) show remarkable code generation capabilities but often produce imperfect code with subtle bugs. A critical bottleneck for improving code quality is the scarcity of high-quality test cases. Existing approaches, primarily based on Supervised Fine-Tuning (SFT) over static datasets, are limited in their ability to discover novel bugs and struggle with the fundamental trade-off between generating error-triggering inputs and maintaining correct expected outputs. To address these limitations, we reframe test case generation as an iterative, adversarial process. We introduce ATGEN (Adversarial Test Generator), a novel framework that trains a test case generator via Reinforcement Learning (RL) in an adversarial loop with an evolving code generator. Instead of learning from a fixed set of bugs, our test generator is dynamically trained to create \"attacking\" I/O pairs for buggy code that is itself being iteratively generated. This process is guided by a reward function that explicitly balances the dual objectives of maximizing the bug detection rate and maintaining high output accuracy. Extensive experiments show that ATGEN dramatically outperforms the state-of-the-art SFT-based approach, UTGen, improving IO Accuracy by nearly 40 absolute points (71.56% vs. 31.83%) and more than doubling the Attack Rate (34.02% vs. 16.24%). The adversarial curriculum is particularly effective for hard-to-detect bugs, achieving an attack rate more than double that of the strongest baseline. Furthermore, tests generated by ATGEN serve as a more effective filter in Best-of-N code generation, significantly closing the gap to the human expert upper bound. Our work establishes a new and more effective paradigm for automated test generation and debugging for LLMs.", "tldr": "", "keywords": ["Test Case Generation", "Reinforcement Learning", "Large Language Models", "Code Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a4af93d5568c92c4d013d5fd1bfe80863367b299.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents ATGen, which formulates unit test generation in an adversarial RL setting. The test generator model is optimized for IO acc and attack success, two important quality measurements for unit tests, and another code generator model is used to generate adversarial code that passes the current test case but fails on the gold test. This forms a loop that enables self-improving test case generation, and results show that ATGen improves both IO acc and attack success by a large margin and which can transfer to better code generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is sound, and the formulation of both test generation and adversarial setup is sensible.\n2. Although it can be costly, the adversarial setup captures the incremental progress needed for the test generator to improve, and the authors explore different ways of sampling to reduce costs.\n3. ATGen generates test cases that largely improve IO acc and attack success, which are two important factors to measure the test quality."}, "weaknesses": {"value": "1. The author mentioned the cost during adversarial code sampling and proposed two modes, but there is no analysis showing the cost and performance comparison.\n2. The main evaluations focus on IO acc and attack success, while the results and discussion on improving downstream code generation are less focused. Best-of-N using the generated test case is a good choice, but more code generator models and datasets are expected to see the effectiveness and generalizability.\n3. Lacking analysis on how to combine the three types of reward (i.e., how to decide their weight) and what their ranges are.\n\nPlease see the questions for other minor weaknesses."}, "questions": {"value": "1. In line 196, why does raising an execution error also get a positive attack reward? Doesn't it mean that the generated code is flawed and has nothing to do with the quality of the generated tests?\n2. What are the ranges of all three rewards, and how do the authors decide the weights for each of them?\n3. ATGen shows large improvements on both IO acc and attack success, but I wonder what the intersect performance would be, i.e., the union of IO acc and attack success, compared to the baselines. As UTGen and the authors mentioned, there is inherently a trade-off between these two.\n4. What is the code generator model used in the \"ATGEN as a Best-of-N Filter\" analysis? More code generator models can be helpful to prove the effectiveness and generalizability.\n5. The adversarial setup here only asks the model to generate adversarial code, but does not update for better code generation quality. Is it possible to optimize the test generator and code generator jointly?\n6. Missing a period in line 454.\n7. Can the generated test case improve frontier models like GPT-5 / Claude 4.5 Sonnet?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8jQ61nRTug", "forum": "Sxj4o3qXtl", "replyto": "Sxj4o3qXtl", "signatures": ["ICLR.cc/2026/Conference/Submission18651/Reviewer_n7wV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18651/Reviewer_n7wV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761342857917, "cdate": 1761342857917, "tmdate": 1762928357840, "mdate": 1762928357840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ATGEN, a RL-based framework for test case generation, which trains a generator to optimize both output accuracy and attack success. By introducing an adversarial training paradigm, ATGEN enables the discovery of more complex and subtle code flaws compared to traditional static datasets. The authors demonstrate the utility of ATGEN in both code generation inference and trainin, showcasing its potential to improve test case generation in dynamic environments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear and Readable Writing: The manuscript is well-structured, with clear and concise writing that facilitates easy comprehension.\n\nStraightforward Motivation: The motivation behind the work is solid, with a direct and convincing argument for applying Reinforcement Learning (RL) to test case generation."}, "weaknesses": {"value": "Lack of Novelty in Techniques: The application of RL to test case generation is conceptually straightforward and has been explored in previous works. The reward function design, while effective, does not introduce significant challenges or innovations.\n\nLack of Precision in Metrics: The metric \"Attack Rate\" is used in the evaluation but primarily reflects recall (but not real recall) rather than precision. It would be beneficial to include more comprehensive statistics, such as precision, to provide a clearer and more balanced evaluation of the model's performance. The datasets, metrics, and baselines used for more comprehensive evaluations can be referred to in TrickCatcher[1].\n\nLimited Evaluation Against Related Work: Some relevant studies that have already explored RL in test case generation, e.g. ACECODER[2], are not included in the main results of the evaluation section. \n\nMinor Spelling and Grammar Errors: There are a few minor spelling and grammatical mistakes that need to be addressed for improved clarity and professionalism.\n\n[1] Kaibo Liu, Zhenpeng Chen, Yiyang Liu, Jie M. Zhang, Mark Harman, Yudong Han, Yun Ma, Yihong Dong, Ge Li, and Gang Huang. (2025). LLM-Powered Test Case Generation for Detecting Bugs in Plausible Programs. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 430–440, Vienna, Austria. Association for Computational Linguistics.\n[2] Zeng, H., Jiang, D., Wang, H., Nie, P., Chen, X., & Chen, W. (2025). Acecoder: Acing coder rl via automated test-case synthesis. arXiv preprint arXiv:2502.01718."}, "questions": {"value": "Potential Circular Reasoning in \"Adversarial Loop\": The concept of the \"adversarial loop,\" which is central to the paper’s contribution, raises concerns about potential circular reasoning. Specifically, using a more powerful large language model (LLM)—which may inherently have better code generation capabilities—to train the test generator may limit the generator’s potential. This creates a fixed difficulty ceiling, as the performance of the test generator will be ultimately bounded by the capabilities of the LLM used during training. A more detailed discussion on how this limitation is addressed or mitigated would be beneficial.\n\nClarification Needed on Dataset Usage for SFT Baselines: The manuscript does not clearly explain how the dataset is utilized in the context of supervised fine-tuning (SFT) baselines. Further clarification on this point is necessary to ensure the reproducibility and fairness of the results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AXRSgkanb7", "forum": "Sxj4o3qXtl", "replyto": "Sxj4o3qXtl", "signatures": ["ICLR.cc/2026/Conference/Submission18651/Reviewer_B73m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18651/Reviewer_B73m"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761628230735, "cdate": 1761628230735, "tmdate": 1762928357453, "mdate": 1762928357453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ATGEN, a novel framework for automatic test case generation that uses adversarial reinforcement learning to improve the reliability of code generated by LLMs. Unlike existing test generation approaches that rely on static datasets, ATGEN introduces a dynamic adversarial loop between two agents: a test generator and an adversarial code generator. The code generator continuously produces harder buggy programs, while the test generator learns to craft tests that can detect these increasingly sophisticated bugs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is very well written.\n\nThe approach that uses adversarial reinforcement learning to solve the test generation problem is novel and very interesting.\n\nThe evaluation is well designed."}, "weaknesses": {"value": "# Limitations of evaluation benchmarks\n\nThe benchmarks used for evaluation appear to be simplified and not fully representative of real-world programming challenges. There are two issues with these benchmarks: 1) they are designed for code generation, not test generation; 2) they have the data leakage issue where their test cases might have appeared in the training data of Qwen models. Consequently, the claimed improvements might not translate to realistic software engineering scenarios. It is recommended that the authors evaluate their approach on benchmarks that are designed for test generation purposefully, especially contamination-free benchmarks, such as UnLeakedTestBench (https://arxiv.org/pdf/2508.00408).\n\n# Lack of discussion and comparison with Mutation Testing\n\nThe approach adopted in this paper uses buggy versions to trigger more effective test generation. The idea is similar to mutation testing, a widely studied test assessment method. In mutation testing, faults are injected purposely to check whether the tests are strong enough to detect the bugs. I recommend discussing such a connection in the paper. It is also interesting to explore the effectiveness of using mutants (with more subtle changes) instead of faulty versions generated by LLMs to conduct adversarial RL. \n\n# Coverage is not reported\n\nThe approach does not report code coverage in the evaluation, which is an important metric for measuring the effectiveness of test inputs, and is widely adopted in industry."}, "questions": {"value": "What is the coverage of the generated tests?\n\nWhat are the potential risks of data leakage in the evaluation process?\n\nHow does the model perform on test generation benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6Wfu2AUFJO", "forum": "Sxj4o3qXtl", "replyto": "Sxj4o3qXtl", "signatures": ["ICLR.cc/2026/Conference/Submission18651/Reviewer_GVXW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18651/Reviewer_GVXW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924989361, "cdate": 1761924989361, "tmdate": 1762928357028, "mdate": 1762928357028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ATGen, a reinforcement learning (RL) framework for automated test case generation that integrates an adversarial training loop. The test generator is trained to produce input-output pairs that are both correct (Output Accuracy) and bug-revealing (Attack Success). Meanwhile, a code generator adversarially creates progressively harder buggy programs, forming a dynamic curriculum. This design aims to overcome the “fixed-difficulty ceiling” of prior static datasets (e.g., UTGen). Empirically, ATGen outperforms strong baselines (UTGen, GPT-4 series, Qwen models) on APPS and Codeforces subsets. The authors also show downstream applications: using ATGen as (1) a Best-of-N filter to select higher-quality generated code, and (2) a reward signal for RL-based code generation training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel and interesting method: The combination of adversarial reinforcement learning and test generation is creative and technically relevant, representing a step beyond static SFT-based approaches like UTGen.\n\n\n2. Good results on benchmark datasets: The model achieves improvements in both IO Accuracy and Attack Rate, especially on APPS and Codeforces tasks, showing clear empirical gains.\n\n\n3. Clear structure and thorough experiments: The paper is well-organized with detailed experiments, ablation studies, and multiple baselines\n\n\n4. Practical downstream validation: Using ATGen-generated tests as a reward signal for RL-based code generation is an excellent demonstration of the method’s broader utility and potential to generalize beyond testing."}, "weaknesses": {"value": "1. Limited domain and dataset generalization: The evaluation focuses only on a single type of domain and dataset, algorithmic coding problems from APPS and Codeforces. It would be helpful to show ATGen’s performance on more real-world software domains, such as repository-level test generation and API testing. Moreover, it would be interesting to see if training ATGen on one dataset (e.g., APPS) generalizes to another dataset (e.g., HumanEval).\n2. Unclear technical details, as listed below:\n    - The choice and training of the base model for UTGen comparison are not fully described. It is unclear whether UTGen and ATGen share the same base model and training dataset, making fairness difficult to assess.\n    - The evaluation assumes that the “ground-truth test suite” (T_gold) is complete. Without quantitative measurement such as coverage metrics, it’s uncertain if this actually holds.\n    - I could not find which buggy programs are considered when computing Attack Rate. Did the paper specify it somewhere?\n3. The non-adversarial model (ATGen w/o Adver) is already significantly better than the baselines. The adversarial training only brings very marginal improvements.\n4. Lack of qualitative examples: No examples of generated tests or adversarial programs are shown. Such examples would help illustrate what kinds of bugs ATGen detects or fails to detect, and how its generated tests differ qualitatively from UTGen’s.\n\n\n5. Clarity and novelty overlap: While the adversarial RL setup is novel for test generation, parts of the approach resemble existing self-play or adversarial curriculum ideas. The paper could better situate itself relative to frameworks like self-play RL in AlphaZero-style."}, "questions": {"value": "Please address my comments in the \"Weaknesses\" section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7V1L2NKlFv", "forum": "Sxj4o3qXtl", "replyto": "Sxj4o3qXtl", "signatures": ["ICLR.cc/2026/Conference/Submission18651/Reviewer_PaRD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18651/Reviewer_PaRD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762324805700, "cdate": 1762324805700, "tmdate": 1762928356459, "mdate": 1762928356459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}