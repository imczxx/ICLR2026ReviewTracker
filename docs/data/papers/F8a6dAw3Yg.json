{"id": "F8a6dAw3Yg", "number": 8072, "cdate": 1758058122941, "mdate": 1759897810057, "content": {"title": "Safe In-Context Reinforcement Learning", "abstract": "In-context reinforcement learning (ICRL) is an emerging RL paradigm where the agent, after some pretraining procedure, is able to adapt to out-of-distribution test tasks without any parameter updates.\nThe agent achieves this by continually expanding the input (i.e., the context) to its policy neural networks.\nFor example,\nthe input could be all the history experience that the agent has access to until the current time step. \nThe agent's performance improves as the input grows, without any parameter updates.\nIn this work,\nwe propose the first method that promotes the safety of ICRL's adaptation process in the framework of constrained Markov Decision Processes.\nIn other words,\nduring the parameter-update-free adaptation process,\nthe agent not only maximizes the reward but also minimizes an additional cost function.\nWe also demonstrate\\footnote{All the implementations will be made publicly available and are now located in the supplementary materials.} that our agent actively reacts to the threshold (i.e., budget) of the cost tolerance.\nWith a higher cost buget, the agent behaves more aggressively, and with a lower cost budget, the agent behaves more conservatively.", "tldr": "", "keywords": ["reinforcement learning", "in-context learning", "safe reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3e2638b6368abb8f68a090b481dd58ae02633d5d.pdf", "supplementary_material": "/attachment/7f87bd14e28856d5b59d8c1c36e8b2b8ee7e50f3.zip"}, "replies": [{"content": {"summary": {"value": "This work presents the first approach to achieving safe ICRL under CMDP, enabling safe adaptation to OOD scenarios without requiring parameter updates. The proposed method first performs safe supervised pretraining in an offline manner, followed by further refinement through EPPO in the online environment. Experiments conducted in the SafeDarkRoom and SafeDarkMujoco environments demonstrate the method’s strong OOD adaptation capability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work investigates a novel problem setting, Safe ICRL, and proposes a principled solution to address it.\n- To account for the unique characteristics of ICRL, where multiple trajectories are used as history, the authors improve upon the conventional primal–dual framework by proposing the EPPO algorithm, which updates the Lagrange multiplier based on the maximum cost observed across multiple trajectories.\n- Experiments conducted in manually designed environments demonstrate the method’s ability to achieve robust OOD adaptation."}, "weaknesses": {"value": "- The authors should more clearly and explicitly motivate the study of this problem. From my perspective, Safe ICRL does not appear to be a compelling research problem. First, the authors use a “dark” setup in their experimental environments—turning off all radar and local observation information and only retaining the agent’s own state—and create OOD scenarios by changing obstacle and target locations under this restricted setting. However, I doubt such tasks occur in real-world applications, since sensors like radar are typically available. With radar, changes in obstacle distribution might not constitute true OOD scenarios. Second, ICRL requires continual exploration and trajectory collection to adapt to new environments. While this is acceptable in standard RL, in Safe RL such exploration repeatedly generates new unsafe interactions, which can be risky.\n- The motivation for the proposed method is insufficiently clear. The stated goal of Safe ICRL is to enable more efficient adaptation to OOD scenarios, yet the main technical contribution appears to be online reinforcement pretraining—a technique that is common in online safe RL. The authors should more thoroughly explain why reinforcement pretraining improves OOD adaptation. Is it because the online phase yields more or higher-quality safe trajectories for training? If the benefit is merely due to increased quantity or quality of data, then the claimed motivation rooted in ICRL may be inconsistent.\n- The experimental environments are relatively simple and too few. The authors should evaluate on more tasks in SafetyGymnasium, e.g., construct multi-task scenarios in MuJoCo Velocity tasks by varying safe velocity thresholds, etc.\n- The experimental baselines are insufficient.\n    - The supervised pretraining baseline is purely offline, whereas the proposed method is offline + online; the settings and amounts of training data therefore differ, making the comparison unfair and insufficient to demonstrate the advantage of the proposed method.\n    - The paper does not adequately compare EPPO with other online safe RL algorithms (e.g., RCPO [1], CVPO [2], CAL [3], RESPO [4]); only a comparison with a naive primal–dual method in the appendix for a single environment is shown, and cost results are not provided, which is unconvincing.\n    - The authors should also compare with more recent baselines in the ICRL literature to demonstrate the effectiveness of their cost-oriented design in the safe setting.\n    - Additionally, in OOD experiments, comparisons should be made to meta-safe-RL methods under consistent offline/online settings.\n\n[1] [Reward constrained policy optimization](https://arxiv.org/abs/1805.11074)\n\n[2] [Constrained variational policy optimization for safe reinforcement learning](https://proceedings.mlr.press/v162/liu22b.html)\n\n[3] [Off-policy primal-dual safe reinforcement learning](https://arxiv.org/abs/2401.14758)\n\n[4] [Iterative reachability estimation for safe reinforcement learning](https://proceedings.neurips.cc/paper_files/paper/2023/hash/dca63f2650fe9e88956c1b68440b8ee9-Abstract-Conference.html)"}, "questions": {"value": "Were the reported cost values in the experiments normalized? If yes, what kind of normalization was used? If not, given that the cost values are already small in the zero-shot setting, does this indicate that the experimental environments are too simple in terms of safety constraints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "82gW9ZCFgm", "forum": "F8a6dAw3Yg", "replyto": "F8a6dAw3Yg", "signatures": ["ICLR.cc/2026/Conference/Submission8072/Reviewer_aqYG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8072/Reviewer_aqYG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760509054731, "cdate": 1760509054731, "tmdate": 1762920063920, "mdate": 1762920063920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Safe In-Context Reinforcement Learning (Safe ICRL) by introducing Constrained Markov Decision Processes (CMDPs) into the in-context RL framework.  The approach consists of two parts: Safe Supervised Pretraining and Safe Reinforcement Pretraining, each with its own optimization algorithm designed to handle CMDP constraints.  Experiments are conducted mainly on the SafeDarkRoom and SafeDarkMujoco environments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-   The paper provides an attempt at theoretical analysis of the proposed optimization method."}, "weaknesses": {"value": "1.  The CMDP formulation in the paper appears to consider only a single cost function, which limits generality.\n    \n2.  In Theorem 1, the logical order of the proof is somehow  problematic — the existence of the fixed point  should be established before proving that it corresponds to a primal optimal solution.\n    \n3.  The experimental evaluation is insufficient. The paper does not compare against established ICRL baselines, making it difficult to assess effectiveness and safety guarantees relative to the state of the art. \n    \n4.  The language and structure of the paper need further polishing and clarification."}, "questions": {"value": "1.   For the proposed iterative algorithm:  Does it converge to a fixed point?  \n2.  Supervised pretraining behavior: In SafeDarkMujoco-Point and SafeDarkMujoco-Car, supervised pretraining does not produce the expected monotonic increase in reward with episode return. What explains this mismatch ？\n3.  Cost-to-go vs. return: Figure 2b shows no clear correlation between cost-to-go and return, suggesting that cost-to-go does not effectively control cost tolerance under supervised training. Why does this occur？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bCmr4S2m8y", "forum": "F8a6dAw3Yg", "replyto": "F8a6dAw3Yg", "signatures": ["ICLR.cc/2026/Conference/Submission8072/Reviewer_C8L7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8072/Reviewer_C8L7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823982326, "cdate": 1761823982326, "tmdate": 1762920063293, "mdate": 1762920063293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces and formalizes the problem of Safe In-Context Reinforcement Learning. The goal is for an agent, after a pretraining phase, to adapt to new, out-of-distribution tasks and satisfy safety constraints without any parameter updates. The agent must rely solely on its context to learn and adapt its behavior. Authors propose safe supervised pretraining that imitates learning histories conditioned on return- and cost-to-go, similar to algorithm distillation and decision transformer; and safe reinforcement pretraining that iteratively updates the policy and Lagrangian multipliers in an online manner, called Exact Penalty Policy Optimization (EPPO). Experiments on variants of previous safe/meta-RL benchmarks show that EPPO can achieve good performance while adapting to cost constraints, but safe supervised pretraining struggles in some environments."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Tackles an interesting problem with potential importance in areas like robotics.\n  \n- Evaluation is performed in OOD environments, a rather challenging setup.\n  \n- Theoretical guarantees are provided."}, "weaknesses": {"value": "- Both methods the authors proposed seem to be rather straightforward extensions of existing algorithms with limited technical novelty.\n  \n- Comparison with baselines is lacking. While it is stated that methods in this paper are parameter-update-free, they can arguably still be compared with safe meta-RL approaches with parameter updates under fair setups (e.g. same online interaction budget).\n  \n- The authors discussed the issue of the number of evaluation episodes in the methods section but did not conduct any OOD experiments in this regard."}, "questions": {"value": "- How does the methods compare with existing safe meta-RL approaches?\n  \n- Can EPPO maintain its performance when given OOD episode number $K$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qqX9wMtEE3", "forum": "F8a6dAw3Yg", "replyto": "F8a6dAw3Yg", "signatures": ["ICLR.cc/2026/Conference/Submission8072/Reviewer_GrcY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8072/Reviewer_GrcY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996926907, "cdate": 1761996926907, "tmdate": 1762920062894, "mdate": 1762920062894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the safety problem in in-context reinforcement learning. The problem is formalized as maximizing the accumulated return from different episodes in context, while ensuring each episode does not violate the cost constraint. The paper proposes EPPO, a safe reinforcement pre-training algorithm that solves the problem and demonstrates its effectiveness through experiments. In particular, EPPO modifies the dual problem of the original optimization problem so that the optimization does not need the knowledge of in-context episodes, while being provably equivalent to optimizing the original problem."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper studies a relevant problem to an emergent training paradigm, a.k.a. the safety problem in ICRL.\n2. The proposed algorithm EPPO provably solves the original optimization problem (maximizing reward constrained by fixed cost budget), while being parameter-free in the number of evaluation episodes.\n\nI am not familiar with the literature on ICRL, so I give a low confidence score."}, "weaknesses": {"value": "1. Though the paper mentioned applications of ICRL in language tasks, the safety framework does not seem appropriate for languages. RL in language models is usually not formalized in the way of MDP, and it is usually hard to define a cost function.\n2. The approach requires that in the pre-training phase, we already know what the cost function is, whereas this is not realistic in a lot of practical scenarios.\n3. The writing is confusing in some paragraphs. For example, I do not understand why the policy in reinforcement pre-training also depends on CTG and RTG.\n\nMinor typo:\n1. Line 163: can depend"}, "questions": {"value": "1. Is the framework described in the paper applicable to language tasks? If yes, how would you solve the problem of defining the cost function, and that RL in language model is usually not formalized in MDP?\n2. Success of pretrained models greatly depends on the usage of offline data, and their general capability to adapt to different downstream tasks. Is it possible to find a way so that we do not need knowledge about the safety constraints during pre-training?\n3. Compared to regular training (without considering safety), how much does safety pre-training impact the capabilities of the models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "iMC6PiqxVu", "forum": "F8a6dAw3Yg", "replyto": "F8a6dAw3Yg", "signatures": ["ICLR.cc/2026/Conference/Submission8072/Reviewer_7od8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8072/Reviewer_7od8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762040869302, "cdate": 1762040869302, "tmdate": 1762920062527, "mdate": 1762920062527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}