{"id": "HW1Vmjr6In", "number": 4848, "cdate": 1757775973687, "mdate": 1759898009215, "content": {"title": "Unmasking the Success of Mixture of Experts: Noisy Features, Sparse Experts", "abstract": "Mixture of Experts (MoEs) allow deep neural networks to grow in size without incurring large inference costs. Theories to explain their success largely focus on enhanced expressiveness and generalization compared to single expert models. We identify a novel mechanism: MoEs can outperform single experts by utilizing activation sparsity amidst feature noise, even without increase in parameter count. This enables MoEs to achieve superior generalization performance, robustness, training convergence speed and sample complexity. Our results further offer a theoretical basis for techniques like MoEfication, which transform dense layers into MoE architectures by exploiting activation sparsity. Experiments on synthetic data and standard real-world language tasks support our theoretical insights.", "tldr": "We prove tht MoEs can exploit activation sparsity to achieve high robutness to feature noise.", "keywords": ["mixtures of experts", "theory", "deep learning", "feature noise", "robustness"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/349754b8d5c19c32cc8c32c962fc4e24c9fae830.pdf", "supplementary_material": "/attachment/1072093f2c9e6369d45aee3c4fd91693ed20864a.zip"}, "replies": [{"content": {"summary": {"value": "The paper develops a theoretical framework to explain why sparse Mixture-of-Experts (MoE) models outperform dense models, focusing on their robustness to feature noise, generalization behavior, and sample efficiency. Specifically, the authors analyze MoEfication framework introduced by Zhang et al., which converts dense transformer layers into sparse, fine-grained MoE. For such frameworks, the authors derive analytical properties of MoEs and conduct a few empirical experiments to validate their key claims."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important issue of a theoretical understanding of Mixture-of-Experts (MoE) models, which are increasingly popular, especially in frontier LLM models.\n\n2. The authors provide interesting theoretical insights, particularly about the robustness and sample efficiency of sparse models compared to dense ones.\n\n3. This area of work nicely complements more empirical studies on MoE vs dense transformers properties (e.g., \"Scaling Laws for Fine-Grained Mixture of Experts” - Krajewski et al., 2024), and is overall very important for the design of MoE architectures."}, "weaknesses": {"value": "1. The paper presents a narrative where sparse MoE LLMs such as DeepSeek and post-hoc MoEfied models are the same class of models, but I am not convinced these models are conceptually and practically different. Models trained from scratch as MoEs (large experts, top-k routing) are typically designed to scale capacity, whereas MoEfication and activation-sparsity techniques (TEAL, activation thresholding) are often used to reduce compute and use a very small subset of activations. The paper does not clearly separate these settings or justify why the theory for one should transfer to the other. This should be explicitly stated in the paper.\n\n2. In particular, TEAL and thresholded activation methods are not true MoEs with expert routing; they sparsify dense layers without explicit token routing between experts. The paper uses TEAL results to support routing-based explanations, which mix different mechanisms as the same class of models.\n\n3. Using a subset of small experts derived from the trained dense model is not the same as using large experts within MoE LLM, and such experts e.g. in DeepSeek also possess activation sparsity; therefore, it is unclear to me how the noise robustness of MoEfied model can be connected to make claims about noise robustness of frontier models, as the paper narrative suggests. The FFN architectures used for most of the experiments, e.g. T5, are also very different than architectures used for frontier MoEs these days, making the cases analysed in the paper and claimed generality of the findings to LLM MoEs even more disconnected.\n\n4. The experiments in the paper are very brief and mostly boil down to running existing methods such as TEAL or MoEfication. They do not test the main theoretical claims in realistic LLM-like settings (e.g., modern FFN variants, GLU-FFN, large experts, or trained-from-scratch MoEs). As a result, the empirical support for the core claims (noise robustness, faster convergence, and better sample efficiency) is weak.\n\n5. Statements such as “We verify the relevance of our theoretical setup with experiments on standard large language model benchmarks” are misleading: the experiments are limited to SST classification with MoEfied-T5, which might have been standard 5 years ago, but is not very convincing in the context of today's language models. Similarly, claims about faster convergence and better sample efficiency are primarily theoretical and weakly supported by experiments.\n\n6. In Figure 4 and related plots, the performance differences between sparse and dense models appear small and fluctuating, but the paper interprets these as evidence for robustness to the noise; the same data could equally support a “no substantial difference” claim if the authors opted into such a narrative.\n\n7. There is at least one missing latex reference (line 458). Overall, the writing seems very focused on theoretical results and fails to convincingly transfer these findings to practical applications and gains, especially due to the above-mentioned issues with the experiments."}, "questions": {"value": "1. How does the theory presented in the paper translate to activation sparsity methods like TEAL, where \"experts\" are a single neurons, there is practically no router but only a heuristic to choose which activations to select? \n\n2. Under the provided theoretical framework, are there any assumptions about the expert granularity and size, e.g., how does the size of the expert affect any of the theoretical findings in the paper? Empirically, many papers, such as the one I cited above from Krajewski et al., or even the results from Figure 4, seem to vary depending on the expert size."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1fmKjpF6hG", "forum": "HW1Vmjr6In", "replyto": "HW1Vmjr6In", "signatures": ["ICLR.cc/2026/Conference/Submission4848/Reviewer_krG9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4848/Reviewer_krG9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761391486159, "cdate": 1761391486159, "tmdate": 1762917608277, "mdate": 1762917608277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a theoretical explanation for the empirical success of Mixture-of-Experts (MoE) models. Instead of focusing on increased expressivity or clustered data structures, the authors argue that MoEs outperform dense models due to their robustness to feature noise—a phenomenon arising when sparse experts operate under activation sparsity. Experiments on synthetic data and real-world settings (e.g., MoEfication of T5-base on SST-2) support the theory: activation-sparse MoEs maintain higher accuracy under word- and character-level noise."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality: The paper presents a new MoE mechanism on feature noise robustness.\n\n2. Soundness: The papers provide theoretical results on the generalization, robustness, convergence, and sample complexity of MoE, supported by formal proofs.\n\n3. Clarity: The authors are explicit about simplifying assumptions (e.g., perfect gating, linear models), making the scope of their claims transparent."}, "weaknesses": {"value": "1. The connection between linear model in Eq.(1) and MoE is not clear. What are mixture weights (gating or router)? What are experts?\n\n2. The analysis relies heavily on a linear block-diagonal model and assumes perfect gating, which may oversimplify real-world MoEs. \n\n3. While Theorem 1 claims near-perfect routing is feasible under modular data, in real-world LLMs, routing errors are nontrivial. The effect of imperfect gating on robustness or generalization is underexplored.\n\n4. This paper is of theoretical nature, so I suggest that the authors present Theorem 1 formally in the main text rather than an informal one, since it is an important result of the paper."}, "questions": {"value": "1. How sensitive are your theoretical results to the assumption of perfect routing? \n\n2. Could the proposed framework extend to nonlinear experts or soft gating functions beyond the linear setup?\n\n3. Could you discuss whether this robustness mechanism relates to implicit regularization or benign overfitting phenomena observed in linear regression?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RtaM2e5Itf", "forum": "HW1Vmjr6In", "replyto": "HW1Vmjr6In", "signatures": ["ICLR.cc/2026/Conference/Submission4848/Reviewer_ePen"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4848/Reviewer_ePen"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761519797144, "cdate": 1761519797144, "tmdate": 1762917608032, "mdate": 1762917608032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- MoEs outperform dense models by exploiting activation sparsity in the presence of feature noise, even with equal parameter counts. This contrasts with existing theory that focuses on increased expressiveness from more parameters or clustered data structures.\n\n- Analyzes a block-diagonal linear regression setting where features have modular structure but are corrupted by Gaussian noise. Proves that sparse (MoE-like) estimators achieve: (1) better generalization (Theorem 2), (2) enhanced robustness to input perturbations when routing is correct (Theorem 3), (3) faster training convergence (Theorem 5), and (4) empirically better sample efficiency. The key insight: sparse experts compartmentalize noise exposure to lower-dimensional subspaces ($d_i < d$), while dense models suffer from noise across all dimensions.\n\n- Proves that routing in MoEfication reduces to supervised classification (Theorem 1) with polynomial sample complexity, unlike joint optimization in traditional MoEs. This explains why converting dense models to MoEs via activation clustering can succeed beyond just inference speedup—the resulting modular structure provides robustness advantages.\n\n- Demonstrates that (1) activation patterns in Llama-2/3 exhibit latent block-diagonal structure revealed through magnitude pruning, and (2) activation-sparse T5 models are measurably more robust to word and character noise on SST-2 than dense models."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies a fundamentally new explanation for MoE success—robustness to feature noise through compartmentalized noise exposure—rather than the standard focus on increased capacity or expressiveness. This is original both conceptually (feature noise vs. label noise) and practically (provides theoretical grounding for MoEfication beyond inference speedup). The insight that modular architectures are inherently more robust because each expert only suffers from noise in its $d_i$-dimensional subspace rather than the full $d$-dimensional space is actionable.\n\n- This work provides an analysis covering generalization (Theorem 2), robustness under correct routing (Theorem 3), failure modes under mis-routing (Theorem 4), convergence speed (Theorem 5), and sample complexity (Section 4.5)."}, "weaknesses": {"value": "- While the authors invoke the Linear Representation Hypothesis (LRH) and Neural Tangent Kernel (NTK) regime to justify their linear setup, this fundamentally mischaracterizes how modern LLMs actually work. (1) LRH is about representation geometry, not function approximation: LRH states that concepts are linearly separable in representation space, not that the mapping from inputs to representations is linear. The authors conflate linear probing (reading out already-computed nonlinear features) with linear function approximation (their model). (2) NTK regime requires specific initialization/training: NTK linearization only holds in the lazy training regime (infinite width, small learning rates near initialization), which modern LLMs explicitly violate through aggressive feature learning. (3) Showing robustness holds in a 2-layer ReLU network on synthetic data with perfect block structure by construction doesn't validate that the linear insights transfer to 32-layer transformers with cross-attention, residual connections, and layer norms processing natural language where block structure is only approximate. The gap between their theoretical setup and actual LLM architecture is substantial and inadequately addressed.\n\n- The experiments fail to directly test the paper's main theoretical predictions. (1) No validation of generalization advantage (Theorem 2): Figure 3 shows excess risk curves on synthetic data with known ground truth, but there are no experiments comparing test error of dense vs. sparse models on real LLM tasks. The SST-2 experiments (Figure 4) measure robustness to noise, not generalization on clean test data. (2) The conjecture that sparse models are more sample-efficient is supported only by synthetic experiments (Figure 3). There are no learning curves on real tasks showing sparse models reach target performance with fewer samples. Furthermore, in practice MoEs require more training in order to properly synchronize the router with the experts. (3) Theorem 1 claims near-perfect routing is achievable, but the T5 experiments assume perfect routing without measuring actual routing accuracy or analyzing how routing errors affect the observed robustness gains.\n\n- All experiments use MoEfication (converting pre-trained dense models). How do the robustness benefits compare to training MoEs from scratch with learned routing? This is critical because Theorem 4 shows mis-routing can be catastrophic, yet no experiments measure routing failures or compare fixed vs. learned routing. \n\n- Only word swaps and keyboard typos are tested. What about adversarial perturbations, semantic-preserving paraphrases, or domain shift—do the robustness benefits hold broadly or only for specific noise types?"}, "questions": {"value": "- (1) For Theorem 2 (generalization): Report test error (not robustness to noise) of dense vs. sparse T5 models on clean SST-2 test set, along with validation curves during training to show if sparse actually generalizes better. (2) For Theorem 5 (convergence speed): Measure and plot training loss curves for dense vs. MoEfied models from scratch (not just using pre-trained checkpoints), showing whether sparse experts converge faster in wall-clock time and number of gradient steps. (3) For sample complexity (Section 4.5): Provide learning curves on real tasks (e.g., SST-2 with varying training set sizes from 100 to full dataset) comparing how many samples each approach needs to reach 90%, 92%, 94% accuracy. Without these experiments, the theoretical claims remain unvalidated in practice.\n\n- Sensitivity analysis: In your synthetic experiments, gradually degrade the block-diagonal structure (e.g., add 10%, 20%, 30% off-diagonal noise) and measure how robustness advantages degrade. At what level of block structure violation do the benefits disappear? This would clarify whether your theory requires idealized perfect blocks or remains valid under realistic approximate modularity.\n\n- Theorem 1 claims near-perfect routing is achievable, but Theorem 4 shows catastrophic failure under mis-routing. For the SST-2 robustness experiments: (1) Report the routing accuracy (% of samples assigned to correct expert) for clean inputs and noisy inputs separately—does word/character noise cause routing failures? (2) Ablation study: Artificially inject routing errors at varying rates (5%, 10%, 20% random mis-routing) and measure how robustness degrades. At what error rate does sparse become worse than dense (the crossover point where Theorem 4's failure mode dominates)? (3) Compare fixed routing (from k-means) vs. learned routing (trainable gating network)—does learned routing maintain advantages under noise or does it fail per Theorem 4? This would clarify the practical viability of the robustness claims when routing is imperfect."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q6OTy7wa0W", "forum": "HW1Vmjr6In", "replyto": "HW1Vmjr6In", "signatures": ["ICLR.cc/2026/Conference/Submission4848/Reviewer_PUca"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4848/Reviewer_PUca"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877466318, "cdate": 1761877466318, "tmdate": 1762917607823, "mdate": 1762917607823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}