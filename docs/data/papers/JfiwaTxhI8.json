{"id": "JfiwaTxhI8", "number": 24572, "cdate": 1758358057410, "mdate": 1763506279951, "content": {"title": "Epistemic Uncertainty Quantification To Improve Decisions From Black-Box Models", "abstract": "Distinguishing a model's lack of knowledge (epistemic uncertainty) from inherent task randomness (aleatoric uncertainty) is crucial for reliable AI. However, standard evaluation metrics of confidence scores target different aspects. AUC and accuracy capture predictive signal, proper scoring rules capture overall uncertainty, and calibration metrics isolate part of the epistemic uncertainty but ignore heterogeneity of the errors within bins, known as grouping loss. We close this evaluation gap by introducing  asymptotically consistent and sample-efficient lower-bound estimators for the grouping loss and excess risk, i.e. suboptimality of a prediction. Our estimators complement existing calibration metrics to provide a more complete, fine-grained assessment of epistemic uncertainty. Applied to LLM question-answering with inherent aleatoric noise, our estimator reveals substantial grouping loss which decreases with model scale but is amplified by instruction tuning. The local nature of our estimators provides actionable insights: they automatically identify subgroups with systematic over- or under-confidence for interpretable audits. We also demonstrate that it reveals better the need of post-training. Finally, we leverage our estimator to design efficient LLM cascades that defer to stronger models, achieving higher accuracy at a lower cost than competing approaches.", "tldr": "We introduce a novel estimator of epistemic uncertainty in confidence scores that reveals local miscalibration and supports improved decision-making and more efficient LLM cascades.", "keywords": ["Epistemic uncertainty", "Excess risk", "Uncertainty quantification", "Aleatoric", "LLM", "Deferral", "Calibration"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dc10087f39ad0b887cbc542420683666d517382f.pdf", "supplementary_material": "/attachment/92b848536d80d987b177fed448bda94444b1a586.zip"}, "replies": [{"content": {"summary": {"value": "This paper considers the excess risk, focussing on the grouping loss, of predictors. In particular, it proposes an estimator for these quantities in the context of proper scoring rules. The estimators are shown to have desirable properties such as consistency and unbiasedness, and instantiations with the 0-1 loss are considered. The method is evaluated on a semi-synthetic setting with known ground-truth probabilities and on a range of experiments involving LLMs and real data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper covers a valuable topic and does a good job of explaining exactly why considering the epistemic uncertainty quantification and, specifically, grouping loss, is important by including illustrative examples.\n- The provided solution is reasonable, satisfies desirable properties, and performs well empirically.\n- The paper reads well, and has clear notation and exposition for the formal results.\n- There is a good range of empirical settings."}, "weaknesses": {"value": "- The paper focuses on the binary setting, which is, of course, fairly limited.\n\nThere seems to be a leftover comment in the Appendix p. 13, l. 697."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u5iXHMAJte", "forum": "JfiwaTxhI8", "replyto": "JfiwaTxhI8", "signatures": ["ICLR.cc/2026/Conference/Submission24572/Reviewer_wB4E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24572/Reviewer_wB4E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928121316, "cdate": 1761928121316, "tmdate": 1762943126295, "mdate": 1762943126295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces asymptotically consistent and sample-efficient lower-bound estimators for the grouping loss and excess risk, i.e., suboptimality of a prediction, serving as a complement to existing calibration metrics. The authors leverage the proposed estimator to design efficient LLM cascades that defer to stronger models, achieving higher accuracy at a lower cost than competing approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work paper is well structured, balancing the theoretical analysis and experimental validations.\n\n2. Proofs for the related propositions are given in the appendix.\n\n3. The extensive evaluations demonstrated the performance improvements convincingly."}, "weaknesses": {"value": "1. Would the author be more explicit in indicating what the resources of the epistemic uncertainty are considered in the proposed method?\n\n2. Minor: There is a lack of a REPRODUCIBILITY STATEMENT in the main body.\n\n3. Minor: In the appendix, there is an unnecessary comment in green remaining. line 699."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ugKq4tOdZt", "forum": "JfiwaTxhI8", "replyto": "JfiwaTxhI8", "signatures": ["ICLR.cc/2026/Conference/Submission24572/Reviewer_SEdh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24572/Reviewer_SEdh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982354873, "cdate": 1761982354873, "tmdate": 1762943126066, "mdate": 1762943126066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose lower-bound estimators for grouping loss and excess risk, designed to address deficiencies in standard metrics (AUC, accuracy, calibration error) that conflate model confidence without properly distinguishing epistemic error or bias. Experiments cover semi-synthetic, tabular, and LLM question answering tasks, including cost-sensitive settings and cascades."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is generally well written. Extensive notational and methodological setup supports reproducibility and rigour.\n\n- It advances epistemic uncertainty estimation using consistent, efficient lower-bound estimators,\n\n- It addresses a critical gap for high-stakes AI applications."}, "weaknesses": {"value": "- It does have dense mathematical sections which might make it harder to digest. Intuitive diagrams could help.\n\n- Computational considerations for large-scale or real-time applications are missing. Scalability and latency constraints need more discussion.\n\n- An ablation on tree depth is missing."}, "questions": {"value": "- What are the practical limits for inference-time overhead with honest tree partitioning?\n\n- How does the proposed approach adapt to free text tasks?\n\n- How to select tree hyperparameter for new tasks or domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W1bMEeDAKm", "forum": "JfiwaTxhI8", "replyto": "JfiwaTxhI8", "signatures": ["ICLR.cc/2026/Conference/Submission24572/Reviewer_6jMk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24572/Reviewer_6jMk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998703602, "cdate": 1761998703602, "tmdate": 1762943125709, "mdate": 1762943125709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose estimators of “grouping loss” and “excess risk”, and apply them to three decision problems: predicting the benefit of LLM finetuning, assessing LLM confidence, and routing queries between multiple LLMs."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Originality: the proposed estimators appear to be novel.\n\nQuality: Figures 2, 4 and 6 communicate the authors’ results well.\n\nClarity: a good amount of the low-level text is easy to follow.\n\nSignificance: query routing is a potentially impactful application."}, "weaknesses": {"value": "I believe the technical motivation of this work is unsound.\n\n- First, describing an aleatoric-epistemic uncertainty decomposition as “crucial for reliable AI” and using that as the premise of the rest of the paper is not appropriate. The aleatoric-epistemic view has been shown to be incoherent, and subjective model-based uncertainties alone are not a legitimate basis for assessing model reliability (Bickford Smith et al, 2025).\n- Second, dismissing standard evaluation metrics—particularly proper scoring rules, which are very well established and have strong theoretical foundations (Savage, 1971)—in favour of new quantities ought to be extremely well argued from a technical perspective, yet it isn’t. Instead the authors offer an assortment of statements about various metrics, then quickly land on “grouping loss” and “excess risk” as the quantities of apparent interest.\n\nEven if we were to set this aside, the practical contribution is hard to get behind.\n\n- In Section 4.2 we see results for predicting the benefit of LLM finetuning. The practical significance of these is unclear. We see a scatter plot of the “excess risk” estimator against “cost reduction”, but the magnitudes of these quantities are hard to interpret from the plot. This is a nonstandard setup that needs explaining better or supplementing with a more practically relatable setup. It also appears a basic baseline method is missing: why not just evaluate the “cost” on the training set, if this is what we care about?\n- In Section 4.3 there isn’t a well-defined task or notion of success. Instead the authors just report confidence results (confidence is not a quantity they are proposing) in Figure 1 and “grouping loss” results in Figure 4. What are we to do with these?\n- In Section 4.4 the focus is on routing a query to an appropriate LLM, balancing prediction costs against predictive accuracy. The results in Figure 5 appear to be positive, although the choice to stop at $t=0.001$ gives a false impression of how expensive the proposed method can be. This high cost is clear from Figure 6 (right): even with the baseline method in a high-cost configuration ($\\lambda=100$), the proposed method is up to ~60 percent more expensive, and the payoff in terms of accuracy is small, at less than ~3 percentage points. At a higher level it seems the extent to which the results are positive here is highly contingent on the dataset having many examples that can be accurately classified by the smaller models.\n\n---\n\nBickford Smith et al (2025). Rethinking aleatoric and epistemic uncertainty. ICML.\n\nSavage (1971). Elicitation of personal probabilities and expectations. Journal of the American Statistical Association."}, "questions": {"value": "Can you provide a technical argument for the insufficiency of proper scoring rules? Note that any appeal to the idea of epistemic/reducible uncertainty would have to be rigorously derived, not just stated as important.\n\nSection 4.2:\n\n- Can you clarify the cost matrices you used?\n- Can you explain the axis magnitudes in Figure 3?\n- Why not just evaluate the “cost” on the training set, if this is what we care about?\n\nSection 4.3: what is the concrete task or notion of success you have in mind?\n\nSection 4.4: can you extend the lines in Figure 5 to include $t=0$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Po5cBVsL1U", "forum": "JfiwaTxhI8", "replyto": "JfiwaTxhI8", "signatures": ["ICLR.cc/2026/Conference/Submission24572/Reviewer_xMCH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24572/Reviewer_xMCH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762294228936, "cdate": 1762294228936, "tmdate": 1762943125463, "mdate": 1762943125463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}