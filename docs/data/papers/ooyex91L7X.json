{"id": "ooyex91L7X", "number": 11124, "cdate": 1758189838380, "mdate": 1763727896439, "content": {"title": "Zeroth-Order Sharpness-Aware Learning with Exponential Tilting", "abstract": "Classic zeroth-order optimization approaches typically optimize for a smoothed version of the original function, i.e., the expected objective under randomly perturbed model parameters. This can be interpreted as encouraging the loss values in the perturbation set to be small on average. Popular sharpness-aware minimization (SAM) objectives, however, focus on the largest loss within the neighborhood to arrive at flat minima more effectively. In this work, we connect zeroth-order optimization (and its corresponding objectives) with SAM approaches explicitly, grounded in an exponential tilting objective that provides a natural transition between the $\\texttt{average}$ and the $\\texttt{max}$. We explore new zeroth-order algorithms to solve a $\\textit{soft}$ SAM objective parameterized by a tilting parameter $t$ that covers the average and min-max formulation as special cases, as well as precise characterizations of the sharpness notions of the tilted SAM framework. Practically, our approach can be used as a gradient-free and memory-efficient alternative to SAM variants, and it achieves better generalization compared to vanilla zeroth-order baselines on wide downstream tasks, including classification, multiple choice, and language generation.", "tldr": "", "keywords": ["Zeroth-Order Optimization", "Sharpness-Aware Minimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e8a253f4eb7243a77cfbb2d30c649f240d515e43.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes to apply t-SAM objective on top of MeZo to replace other SAM variants. Empirical studies show that it's effective and generalizes better."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This is a simple and straightforward idea and it's well motivated.\n2. I enjoy reading about the insights from section 4.3.\n3. Interesting ablation study on t"}, "weaknesses": {"value": "1. The empirical results seem very inconsistent across benchmarks against MeZo and doesn't support the claim of the paper. (table 2, table 3 and table 4)\n2. limited novelty, it appears to me as a direct application of t-SAM objective on top of MeZo."}, "questions": {"value": "1. Why does TSAM consistently outperform SAM and SGD, whereas the counterpart ZEST has mixed results?\n2. Why is it necessary to compare noisy label? The gradient of MeZo itself is already very noisy. Would it be more informative if you can provide an ablation study on k (number of perturbations), comparing the two methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Hq70F5fkBx", "forum": "ooyex91L7X", "replyto": "ooyex91L7X", "signatures": ["ICLR.cc/2026/Conference/Submission11124/Reviewer_6vgJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11124/Reviewer_6vgJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761448289579, "cdate": 1761448289579, "tmdate": 1762922298386, "mdate": 1762922298386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the Tilted Sharpness-Aware Minimization (t-SAM) objective and applies the zeroth-order optimization technique to solve this optimization problem. The t-SAM objective has a special structure and the format of the zeroth-order gradient estimator presents its challenge in obtaining an unbiased estimation. To address this issue, this paper further proposes the bias-correction method. The empirical results validate the performance of these approaches and validate the improved generalization ability of minimizing t-SAM objective using zeroth-order method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper has made solid LLM experimental results, which include sufficient baselines in a famous LLM training baseline over multiple commonsense reasoning datasets. The results are expected; minimizing SAM will improve the generalization ability and the performance in test set indeed validates this statement. Also, this paper has included a new bias correction approach for the gradient estimation in minimizing t-SAM. This result is new as in the standard optimizaiton problem, the zeroth-order gradient estimation is unbiased when the step is sufficiently small."}, "weaknesses": {"value": "1. The main contribution of this work seems to be evaluating the zeroth-order gradient for the t-SAM objective function. As the t-SAM objective is given from another paper, the theoretical contribution of this work seems to be not strong. \n\n2. The bias correction seems to sound. However, I would prefer to see some synthetic examples showing that the bias is indeed corrected. \n\n3. Given that the theoretical contribution is not strong, I may expect to see more large sacle experiments. However, the current experiments are taken on small LLMs."}, "questions": {"value": "Given these weaknesses, I may have following questions:\n\n1. In introduction, the author claims \"We show that our method can identify and conservatively avoid minima with large curvatures in any direction, while vanilla methods cannot (Section 4).\" Is Section 4 a formal theorem or just some intuitive explainations on why it can identify and conservatively avoid minima with large curvatures? \n\n2. I hope the author could further clarify the main theoretical contributions. Evaluating the zeroth-order gradient for the t-SAM objective function given in Theorem 3.1 seems to be too weak as its only theoretical contribution.\n\n3. I would encourage the author to include additional visualization on the bias of each gradient in Theorem 3.1; also, it would be better if we may see the bias is scaled in the rate $O(1/\\sqrt{d})$ as commented at the bottom on page 3.\n\n4. I am actually concerned about the correctness of the $O(1/\\sqrt{d})$ rate. Has it been proved somewhere? \n\n5. I would also encourage the author to include some vision tasks just as the t-SAM objective's paper *Tilted Sharpness-Aware Minimization* did. \n\n6. I am not very sure how the parameter $t$ is chosen. When $t$ tends to infinite, the objective function becomes the desired SAM objective function. Does it mean that we shold make $t$ as large as possible?  However, in Appendix B.8, we cannot make $t$ too large. Do I misunderstand something?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oC2OAPo3TV", "forum": "ooyex91L7X", "replyto": "ooyex91L7X", "signatures": ["ICLR.cc/2026/Conference/Submission11124/Reviewer_myiW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11124/Reviewer_myiW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761503370791, "cdate": 1761503370791, "tmdate": 1762922297959, "mdate": 1762922297959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ZEST, a zeroth-order sharpness-aware optimization method that uses exponential tilting to interpolate between average-loss and worst-case SAM objectives. The approach introduces a curvature-sensitive regularizer that encourages flatter minima while remaining fully gradient-free and memory-efficient. Experiments show consistent improvements over MeZO with the comparable computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a unified framework that bridges zeroth-order optimization and sharpness-aware minimization (SAM) through exponential tilting. This idea provides a continuous spectrum between average-loss and worst-case objectives.\n\n2. Experiments show consistent improvements over MeZO."}, "weaknesses": {"value": "1. The experiments are conducted only on relatively small models. These are modest compared to current state-of-the-art LLMs (e.g., LLaMA-7B, Mistral-7B, OPT-6.7B). As a result, it would be more convincing if the proposed ZEST framework scales to modern large-parameter settings.\n\n2. The evaluation mainly focuses on classic datasets such as GLUE, SQuAD, and ReCoRD. These benchmarks are saturated and may not reflect the difficulty or diversity of current NLP tasks. It would strengthen the paper to include contemporary datasets such as SuperGLUE, MMLU."}, "questions": {"value": "How does ZEST perform compared with other zeroth order methods such as those in [1] and [2]?\n[1] Variance‑reduced Zeroth‑Order Methods for Fine‑Tuning Language Models (Gautam et al., 2024)\n[2] Revisiting Zeroth‑Order Optimization for Memory‑Efficient LLM Fine‑Tuning: A Benchmark (Zhang et al., 2024)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ch3CmgouqC", "forum": "ooyex91L7X", "replyto": "ooyex91L7X", "signatures": ["ICLR.cc/2026/Conference/Submission11124/Reviewer_YGy9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11124/Reviewer_YGy9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796669797, "cdate": 1761796669797, "tmdate": 1762922297631, "mdate": 1762922297631, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new zeroth-order optimization algorithm (ZEST) that uses exponential tilting technique to recover the smooth spectrum of sharpness-aware objectives. Particularly, it considers the tilted sharpness-aware minimization (t-SAM) objective, which is the smooth approximation of the SAM objective. When $t\\to \\infty$, this objective function tends to the standard SAM objective. Then this work uses the divergence theorem to approximate t-SAM gradients and obtain the tilted zeroth-order gradient estimator. Then naive plug-in and bias-corrected plug-in are proposed to obtain this derived estimator, leading to the ZEST algorithm. Further sharpness analysis is presented to provide the explicit bias of the t-SAM objective under two common perturbations. Lastly, experiments are conducted on RoBERTa models to validate the theoretical findings and the empirical performances of the ZEST algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper provides a simple and clear formula for the tilted zeroth-order gradient. It utilizes the structure of the gradient of t-SAM objective.\n\n2. The sharpness analysis provides a solid foundation for the soundness of the proposed approach. The explicit dependence of $R_t$ on the relevant parameters deepens the understanding on this new algorithm.\n\n3. The authors validate ZEST on a diverse set of tasks (classification, QA, generation) and models (RoBERTa, OPT). The consistent improvements over the MeZO baseline , especially on noisy data, demonstrate the method's effectiveness."}, "weaknesses": {"value": "1. The core t-SAM objective is not novel to this work; it is adopted from Li et al. (2024) . Adapting this objective to the zeroth-order setting is good but not the same as inventing the objective. \n\n2. The theoretically superior \"Bias-Corrected\" estimator does not consistently outperform the \"Naive\" estimator in Table 2 and Table 3. It weakens the motivation of designing the biased-corrected plug-in."}, "questions": {"value": "1. As the bias-corrected plug-in does not consistently outperform the naive plug-in in these experiments, does the bias-correction simply not provide a significant benefit? What is the point of proposing this method?\n\n2. In Algorithm 1, the update step in Line 11 is written inside the loop over $i=1,2,\\dots,k$. This implies the model parameter $x$ is updated $k$ times per iteration. Should the update in Algorithm 1 be a single step using the sum (i.e., $x\\leftarrow x - \\eta \\frac{t\\rho} \\sum_i w_i v_i$)? \n\n3. When $t=20$, the t-SAM objective is more closed to the SAM objective but its performance shown in Figure 3 is not better (in Noisy MNLI and Noisy SNLI). Is there any intuitive explanation on this  phenomenon? \n\n4. In Figure 3, why are these curves not in the same length.\n\n5. Can the authors quantify the theoretical gap between the t-SAM objective and the original SAM objective? A simple example validating how the t-SAM solution approaches the SAM solution as $t$ increases would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VjIWh7JwlC", "forum": "ooyex91L7X", "replyto": "ooyex91L7X", "signatures": ["ICLR.cc/2026/Conference/Submission11124/Reviewer_2S7G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11124/Reviewer_2S7G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762863844972, "cdate": 1762863844972, "tmdate": 1762922297194, "mdate": 1762922297194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}