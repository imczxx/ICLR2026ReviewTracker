{"id": "If37U8qzHc", "number": 9251, "cdate": 1758116344548, "mdate": 1759897735272, "content": {"title": "Towards Reversible Model Merging For Low-rank Weights", "abstract": "Model merging aims to combine multiple fine-tuned models into a single set of weights that performs well across all source tasks. While prior work has shown that merging can approximate the performance of individual fine-tuned models for each task, it largely overlooks scenarios where models are compressed into low-rank representations$\\textemdash$ either through low-rank adaptation (LoRA) or post-training singular value decomposition (SVD). We first demonstrate that applying conventional merging methods to low-rank weights leads to severe performance degradation in the merged model. Motivated by this phenomenon, we propose a fundamentally different approach: instead of collapsing all adapters into one set of weights, we construct a compact basis (e.g., an equivalent of holding two or more models) from which original task-specific models can be recovered via linear combination. This reframes merging as generating a reconstruction-capable model space rather than producing a single merged model. Crucially, this allows us to ''revert'' to each individual model when needed, recognizing that no merged model can consistently outperform one specialized for its task. Building on this insight, we introduce our method, Reversible Model Merging (RMM), an efficient, data-free, and flexible method that provides a closed-form solution for selecting the optimal basis of model weights and task-specific coefficients for linear combination. Extensive experiments across diverse datasets and model scales demonstrate that RMM consistently outperforms existing merging approaches, preserving the performance of low-rank compressed models by a significant margin.", "tldr": "", "keywords": ["Model Merging", "Low-rank", "SVD", "LLM", "LoRA"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6954d639b3c852186e3886750e40858e14ab2c22.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper relaxes the model merging problem into a compression–reconstruction setting. Based on this setting, this paper proposes a simple reversible model merging method that provides a closed-form solution to the optimization-based compression objective. The experiments demonstrate that the proposed method can maintain relatively good performance while significantly reducing the number of stored parameters."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed method is simple and easy to implement.\n2. The paper is clearly written and easy to follow."}, "weaknesses": {"value": "1. This paper relaxes the model merging problem by assuming that the task index is known during inference. Compared with the conventional setting of model merging, this setting is overly simplified and reduces the practical applicability of the method.\n2. The paper lacks evaluations on the latest models.\n3. There is no comparison with model merging approaches based on LoRA, such as [1].\n4. The experimental results appear underwhelming — with 69% of the parameters retained, the performance drops by more than 10%, which seems difficult to justify.\n\n[1] Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering, ICLR 2025."}, "questions": {"value": "1. A more appropriate baseline could be a comparison with quantized LoRA methods, which also aim to reduce storage requirements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "95UFGA0BCz", "forum": "If37U8qzHc", "replyto": "If37U8qzHc", "signatures": ["ICLR.cc/2026/Conference/Submission9251/Reviewer_Cgv9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9251/Reviewer_Cgv9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761130297991, "cdate": 1761130297991, "tmdate": 1762920904042, "mdate": 1762920904042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Reversible Model Merging (RMM), a new method for combining multiple low-rank models (e.g., LoRA adapters). Instead of creating a single, often poorly performing, merged model, RMM learns a compact shared basis from which individual task-specific models can be accurately reconstructed on demand. This approach offers a tunable trade-off between storage and performance, and empirical results show it significantly outperforms existing merging techniques on compressed models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Effectively addresses the poor performance of standard merging techniques on low-rank models by reframing the problem as model reconstruction rather than aggregation.\n* Demonstrates significant performance gains over established baselines across multiple model architectures (RoBERTa, ViT), tasks, and compression ranks.\n* Offers an efficient solution for managing numerous task-specific models, with storage costs growing sublinearly with the number of tasks."}, "weaknesses": {"value": "*   **Insufficient Discussion of Practical Overheads:** The primary trade-off of RMM is performance vs. storage, but another key factor is computational overhead at inference time. The paper does not quantify the latency introduced by the reconstruction step, which must be performed for each task. This could be a non-trivial cost, especially in latency-sensitive applications. Furthermore, the framework relies on an \"oracle router\" to select the correct task index `i`, and the practical feasibility and computational cost of this routing mechanism are not discussed.\n*   **Lack of Connection to Intrinsic Dimensionality:** The core idea of RMM—finding a low-dimensional basis that can represent multiple task-specific updates—is closely related to the concept of intrinsic dimensionality in fine-tuning, e.g., [1]. Research has shown that the updates required for fine-tuning often lie in a very low-dimensional subspace. A discussion of how RMM relates to this concept and potentially a comparison to methods that explicitly estimate and use this intrinsic dimensionality could provide deeper insight into why RMM is effective and place it in a broader theoretical context.\n*   **Limited Baselines for \"Separate Merging\":** The paper argues against \"Combined merging\" on the grounds of storage inefficiency and proceeds to compare RMM only against baselines using the \"Separate merging\" strategy. While the efficiency argument is valid, the empirical case for RMM would be stronger if it included a performance comparison against a \"Combined merging\" baseline (even if just for a smaller model or layer). This would help disentangle how much of the baselines' failure is due to the inherent flaws of their merging logic versus the limitations imposed by the separate merging of low-rank factors.\n*   **Clarity and Precision of Mathematical Formulation:** While the overall method is clear, the notation in Section 5 could be more precise. The formulation for RMM is presented generically using `x_i` and `X` without explicit indices for the layer or the specific row/column being processed. Since the algorithm is applied independently to each row of matrix `A` and each column of matrix `B` for every layer, incorporating this into the notation would make the description less abstract and easier to map directly to the implementation.\n\n[1] Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jsvvBA5nWi", "forum": "If37U8qzHc", "replyto": "If37U8qzHc", "signatures": ["ICLR.cc/2026/Conference/Submission9251/Reviewer_c6xK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9251/Reviewer_c6xK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761579746877, "cdate": 1761579746877, "tmdate": 1762920903703, "mdate": 1762920903703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Reversible Model Merging (RMM), a model merging method which creates a compact shared basis where individual merged models can be reconstructed. The experiments show that RMM outperforms several merging methods while being more storage efficient."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel perspective of relaxing the merging problem to allow maintaining more than just a single merged model.\n2. Diverse task suites and model types in the experiments."}, "weaknesses": {"value": "1. Limited comparison. Many modern SVD-based merging methods show good performance merging LoRA checkpoints (e.g. KnOTS [1], ISO [2], DRM [3]), as well as other non-SVD methods (e.g. PEFT’s cat [4]), though they aren’t compared or at least discussed.\n2. Practical relevancy of merging low-rank compressed checkpoints is not clearly illustrated, reference to previous works supporting this is also lacking.\n3. Natural baselines such as “combined” merging the checkpoints then compress, are not discussed or compared.\n\n[1] https://arxiv.org/abs/2410.19735\n[2] https://arxiv.org/abs/2502.04959v3\n[3] https://arxiv.org/abs/2505.23117\n[4] https://huggingface.co/blog/peft_merging"}, "questions": {"value": "1. I'm not quite convince regarding the relevancy or importance of “reversible.” If the individual checkpoints are already compressed, they should already be suitable for storage. Or if merged checkpoints need to be reversible, why merge them in the first place?\n2. Usefulness of the storage reduction is debatable. As the merging is done post-training, the merged weight delta can simply be summed into the base weight, no extra parameters are to be kept.\n3. Why are the tasks inconsistent between models? (QNLI, MRPC, SST-2, MNLI, QQP, RTE. CoLA, STS-B for RoBERTa vs. QNLI, MRPC, SST-2, MNLI, QQP,  BoolQ for OPT). What are the rationale for picking these subsets?\n\n\nWriting\n1. “an important limitation remains largely overlooked: even the most sophisticated merging strategies consistently fail to match the performance of the original individual finetuned models.” is not necessarily appropriate. As it’s not overlooked, but more of a goal the community is making progress toward.\n2.  “This failure stems from two key factors: the limited expressive capacity inherent in low-rank representations, and misalignment of task-specific subspaces, each optimized independently, leading to severe interference when combined” needs further elaboration or citation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CwGkkemtWU", "forum": "If37U8qzHc", "replyto": "If37U8qzHc", "signatures": ["ICLR.cc/2026/Conference/Submission9251/Reviewer_PQZt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9251/Reviewer_PQZt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842946437, "cdate": 1761842946437, "tmdate": 1762920903285, "mdate": 1762920903285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new framework for merging multiple fine-tuned models that have been compressed using low-rank methods such as LoRA or post-training SVD. Traditional model merging techniques often fail when applied to low-rank models, leading to severe performance degradation due to limited representational capacity and task interference. To address this, the authors introduce Reversible Model Merging (RMM), which redefines merging as constructing a compact, reconstruction-capable model space rather than producing a single merged model. RMM learns a small set of shared basis components and task-specific coefficients through a closed-form SVD-based optimization, allowing each original model to be approximately reconstructed when needed. Experiments on multiple NLP and vision benchmarks show that RMM consistently outperforms existing merging methods like Task Arithmetic, TIES, and DARE, achieving much higher accuracy while maintaining efficient storage. The approach offers a tunable trade-off between performance and memory by adjusting the number of basis components, scaling efficiently to large multi-task or federated learning settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### Novel framing of the merging problem:\nThe paper redefines model merging as constructing a reversible and reconstruction-capable subspace rather than producing a single merged model. This is a clear conceptual shift from prior approaches that treat merging as a one-shot averaging process.\n\n### Theoretical formulation with closed-form solution:\nUnlike heuristic or sign-based methods (e.g., TIES, DARE), RMM provides a principled SVD-based framework that explicitly minimizes reconstruction error, offering mathematical interpretability and data-free optimization.\n\n### \tCompatibility with low-rank compression:\nThe method operates directly on low-rank representations (LoRA, PT-SVD), addressing a practical gap where previous merging techniques fail due to limited expressivity and subspace misalignment.\n\n### Flexible trade-off between performance and storage:\nThe introduction of the basis size p as a tunable hyperparameter enables a controllable balance between reconstruction fidelity and memory cost—an important design for scalable multi-task or federated learning scenarios."}, "weaknesses": {"value": "### Lack of clarity in reconstruction generalization:\nThe paper does not clearly analyze whether the learned basis generalizes to unseen tasks or domains. RMM’s reversibility is validated only for known task models, which may restrict its applicability to dynamic or continual learning settings.\n\n### Scalability validation remains limited in model scale:\nWhile Section 6.3 demonstrates sublinear storage scaling across tasks, the evaluation is confined to relatively small models such as RoBERTa-base and OPT-1.3B. It remains unclear whether RMM maintains the same efficiency and reconstruction fidelity on larger-scale architectures. Validation on more recent and larger models (e.g., Qwen2.5-3B/7B or Qwen3-8B) would better demonstrate the robustness and scalability of the proposed method in realistic multi-billion-parameter settings.\n\n### Lack of evaluation on modern LLM benchmarks:\nThe evaluation focuses primarily on classical NLP tasks (e.g., GLUE) and small-scale models. However, compared to recent works such as DARE, which include results on AlpacaEval, GSM8K, and HumanEval, this paper lacks validation on contemporary instruction-following or reasoning benchmarks that better reflect current LLM capabilities. Including such benchmarks would strengthen the empirical evidence for RMM’s effectiveness in modern large-scale settings."}, "questions": {"value": "Tables 1 and 2 present the relative storage cost, but have you also measured the actual GPU memory consumption during inference or model reconstruction? Including such results could provide stronger evidence of RMM’s practical efficiency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RZvLDt3PFE", "forum": "If37U8qzHc", "replyto": "If37U8qzHc", "signatures": ["ICLR.cc/2026/Conference/Submission9251/Reviewer_YYBh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9251/Reviewer_YYBh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920220776, "cdate": 1761920220776, "tmdate": 1762920902701, "mdate": 1762920902701, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}