{"id": "tpkBiKShwV", "number": 24214, "cdate": 1758354197489, "mdate": 1763176692242, "content": {"title": "Semi-Supervised Diseased Detection from Speech Dialogues with Multi-Level Data Modeling", "abstract": "Detecting medical conditions from speech acoustics is fundamentally a weakly-supervised learning problem: a single, often noisy, session-level label must be linked to nuanced patterns within a long, complex audio recording. This task is further hampered by severe data scarcity and the subjective nature of clinical annotations. While semi-supervised learning (SSL) offers a viable path to leverage unlabeled data, existing\naudio methods often fail to address the core challenge that pathological traits are not uniformly expressed in a patient's speech. We propose a novel, audio-only SSL framework that explicitly models this hierarchy by jointly learning from frame-level, segment-level, and session-level representations within unsegmented clinical dialogues. Our end-to-end approach dynamically aggregates these multi-granularity features and generates high-quality pseudo-labels to efficiently utilize unlabeled data. Extensive experiments show the framework is model-agnostic, robust across languages and conditions, and highly data-efficient—achieving, for instance, 90\\% of fully-supervised performance using only 11 labeled samples. This work provides a principled approach to learning from weak, far-end supervision in medical speech analysis.\nThe code is available at \\url{https://anonymous.4open.science/r/semi_pathological-93F8}.", "tldr": "", "keywords": ["Pathological Speech Detection", "Semi-Supervised Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/4af45c5ee66d447bdf93e788fef3fdd83d296d5a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a semi-supervised framework for speech-based disease detection, focusing on Alzheimer’s and depression. It is clearly written and easy to follow, with well-organized sections and intuitive multi-level modeling (session-, clip-, and frame-level). The single-stage pseudo-label updating strategy is elegant and efficient, and the figures and tables are informative and well-presented.\n\nHowever, the paper has several major weaknesses. The title and framing are misleading, suggesting a universal “speech disease detection” model without sufficient clinical grounding or literature coverage. The evaluation metrics (mainly F1) provide limited insight for healthcare relevance, and the work lacks explainability or clinical validation. The experimental analysis, while technically solid, offers little real-world or scientific interpretation, and the overall experimental scope is narrow.\n\nOverall, while the paper is well-written and explores an interesting technical direction, it lacks depth in clinical understanding and healthcare relevance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is clearly written and easy to follow — it took only 2–5 minutes to grasp the main idea. It proposes an AI-based semi-supervised framework for disease detection from speech dialogues, focusing on binary classification for Alzheimer’s and depression. The organization is concise, with well-structured sections and a straightforward motivation. The idea of leveraging multi-level modeling (session-, clip-, and frame-level) to address weak supervision and limited labels is novel and intuitive. The paper also provides solid experiments across two datasets, demonstrating the model’s efficiency with limited labels and its robustness across languages and encoder architectures. Figures and tables are clear and informative, helping readers understand the hierarchical model structure and performance trends. The proposed single-stage pseudo-label updating strategy is elegant and contributes to better efficiency and stability compared to standard multi-stage SSL approaches."}, "weaknesses": {"value": "This paper has several significant technical flaws and lacks important references.\n\n* Title and Clinical Framing. The use of the title “Diseased Detection from Speech” is inappropriate. I am not sure whether the authors have any clinical background in speech and language disorders, but the detection of each speech-related condition (e.g., Parkinson’s, ALS, Aphasia) is fundamentally different. It is impossible to claim the development of a universal “speech disease” detection system without clinical validation. Such detection must be condition-dependent. In this paper, the generalization is purely based on the datasets used — Depression and Alzheimer’s. Therefore, the title should explicitly state “Depression and Alzheimer’s Detection” unless the method is proven robust across a broad range of disorders. I would also suggest the authors engage with clinicians or speech-language pathologists to better understand the clinical context. Additionally, the literature review is very limited. For example, citing only Strimbu & Tavel (2010) and Califf (2018) is insufficient to represent the entire field of speech-based disease detection.\n\n* Evaluation Metrics. The choice of F1 score as the main evaluation metric is questionable. If the goal is to develop an AI model for healthcare applications, the evaluation should reflect clinical or human-centered alignment — for example, how well the AI model correlates with human expert scores. Accuracy and F1 alone provide limited insight. Speech patterns may overlap across conditions (e.g., a healthy individual with fatigue may exhibit similar patterns to early-stage patients). The current binary setup (two classes) oversimplifies the clinical landscape. If additional classes were introduced, the F1 score would fluctuate without offering meaningful interpretability.\n\n* Lack of Explainability and Clinical Evidence. A critical weakness lies in the absence of interpretability or clinical validation. If the model classifies a sample as “Alzheimer’s,” what evidence supports that decision? How does the model justify its output? In medical AI, interpretability is essential. Without it, the system resembles a black box — a clinician would never accept a diagnosis simply because “the model predicted 80% Alzheimer’s.” This reflects a lack of fundamental understanding of AI in healthcare.\n\n* Limited Experimental Insight. The experimental analysis, including ablations such as “Architectural Robustness Across Encoders,” provides minimal scientific or clinical insight. The results show incremental performance changes but offer no interpretation about what those differences imply for real-world or clinical deployment.\n\n* Insufficient Experimentation. The overall scale of the experiments is quite limited. The work reads more like an exploratory report rather than a rigorously validated study.\n\nOverall Assessment. The authors appear to lack basic knowledge of AI healthcare principles and should perform a much broader literature review before continuing this line of research. I give a score of 2, acknowledging that the team did conduct some experiments, which reflects a positive exploratory effort."}, "questions": {"value": "What do you think is the core task in speech healthcare problems — specifically in disordered speech detection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OLsN4UasSX", "forum": "tpkBiKShwV", "replyto": "tpkBiKShwV", "signatures": ["ICLR.cc/2026/Conference/Submission24214/Reviewer_ZPiZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24214/Reviewer_ZPiZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624059046, "cdate": 1761624059046, "tmdate": 1762942998937, "mdate": 1762942998937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "thTopOlhky", "forum": "tpkBiKShwV", "replyto": "tpkBiKShwV", "signatures": ["ICLR.cc/2026/Conference/Submission24214/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24214/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763176691505, "cdate": 1763176691505, "tmdate": 1763176691505, "mdate": 1763176691505, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of semi-supervised disease classification from long clinical speech conversations, where only a single conversation-level label is available per sample. To overcome the mismatch between coarse supervision and fine-grained acoustic variability, the authors propose a multi-granularity learning framework that jointly models conversation-, segment-, and frame-level representations. Specifically, conversation-level embeddings provide global supervision; segment-level pseudo-labels are generated online through confidence-based updates; and frame-level features are regularized via EMA-based teacher–student consistency.\nThe framework aims to progressively propagate limited supervisory signals from coarse to fine temporal resolutions while maintaining temporal coherence. Experiments are conducted on two clinical speech datasets—EATD (Chinese depression detection) and ADReSSo21 (English Alzheimer’s detection)—using multiple pretrained speech encoders (WavLM, HuBERT, wav2vec2). Results show consistent improvements under partial-label settings and modest gains even under full supervision. Ablation studies confirm the benefit of combining multi-level objectives and online pseudo-labeling."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper addresses a well-motivated and practically relevant problem of semi-supervised learning from conversation-level labels in clinical speech analysis.\n2.The multi-granularity framework (conversation, segment, frame) is conceptually clear and technically coherent, combining pseudo-labeling and EMA-based consistency in a single-stage pipeline."}, "weaknesses": {"value": "1.While the hierarchical integration is elegant, the method remains conceptually close to existing semi-supervised frameworks such as FixMatch, Mean Teacher, and BYOL-like consistency learning. The absence of comparisons with such strong baselines weakens the claimed novelty. The improvement could partially stem from task-specific tuning rather than fundamentally new learning principles.\n2.Key implementation details are under-specified, particularly regarding pseudo-label confidence computation, thresholding strategy, update frequency, and the weighting coefficients (α,β,γ) of the multi-level losses. These factors critically affect the stability and reproducibility of the results. The lack of sensitivity analysis or ablation makes it unclear whether the reported gains generalize across datasets or rely on delicate hyperparameter tuning.\n3.The datasets are relatively small and imbalanced (e.g., only 162 cases in EATD). Reported standard deviations are large, yet no statistical significance tests are provided. Consequently, some performance gains might not be statistically reliable. Additionally, the generalization claims across languages and disorders are not fully substantiated by controlled cross-domain experiments.\n4.The cross-lingual generalization claims are insufficiently supported. Although the study involves both Chinese (EATD) and English (ADReSSo21) datasets, each model is trained and tested within its own domain. The paper does not include any explicit cross-lingual transfer or domain-shift experiments, even though such evidence is central to the stated goal of building language-agnostic disease detection models."}, "questions": {"value": "1. Could the authors specify how the pseudo-label confidence is computed (e.g., maximum class probability, temperature-scaled logits, entropy)? How are the update interval k and confidence threshold chosen or adapted across datasets and label ratios? \n2. The training objective includes multiple loss terms weighted by (α,β,γ). How are these hyperparameters selected? Are they tuned jointly or fixed?\n3. Since segment-level labels are derived from conversation-level predictions, what measures are taken to prevent error reinforcement during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NukZ6oV2Jg", "forum": "tpkBiKShwV", "replyto": "tpkBiKShwV", "signatures": ["ICLR.cc/2026/Conference/Submission24214/Reviewer_CgfG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24214/Reviewer_CgfG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879217658, "cdate": 1761879217658, "tmdate": 1762942998626, "mdate": 1762942998626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an approach to detect medical conditions from speech by leveraging frame-level, segment-level, and session-level representations obtained from unsegmented clinical dialogues. The paper claims that the proposed approach is model-agnostic, robust across languages and conditions, and highly efficient when labeled data is scarce, reporting performance as high as 90%.\n\nThe paper points to a GitHub repo, however the repository is empty and provides no information about the implementation of the approach, but only points to the data resources that have been cited in the paper."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important problem of having limited labeled data where training models that can generalize well is often difficult. While the paper is well motivated, and the problem statement is well defined, the strength of the paper does not provide sufficient details on the implementation."}, "weaknesses": {"value": "The paper points to a GitHub repo, however the repository is empty and provides no information about the implementation of the approach, but only points to the data resources that have been cited in the paper. \n\nThere are several issues with the draft:\n(1) The title has a typo: \" .. Diseased Detection ... \" it should be \" .. Disease Detection.\n(2) At multiple points in the paper, the statements were made as too general \"... standard architectures such as ... can be used\", \"This can be accomplished either by adding an additional layer to model global information or by employing a temporal attention mechanism for fusion\" ... these statements should be corrected and the paper should clearly state what was actually done in the work presented."}, "questions": {"value": "(1) It is shown in equation (3) that pooling was performed, which pooling was used and why? Was it imperially decided or heuristically?\n\n(2) \"In contrast to prior work, this approach avoids the strong assumption that every utterance from a patient exhibits pathological features...\" >> not clear how the assumption is tackled in this work. it will be useful to clearly elaborate how this assumption is addressed in this work.\n\n(3) \"We analyze the evolution of pseudo-label quality ...\" >> it is not clear how the pseudo-labels were generated.\n\n(4) \"We evaluated our framework with three popular audio encoders: wav2vec2, HuBERT, and WavLM (Tables 5 and 6) ..\" Neither table 5 or 6 specify any result from HuBERT."}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "The paper seems to have some strange errors, starting with a typing error in title where it says \"Diseased Detection\"\n\nAt multiple points in the paper, the statements were made as too general \"... standard architectures such as ... can be used\", \"This can be accomplished either by adding an additional layer to model global information or by employing a temporal attention mechanism for fusion\" ... these statements should be corrected and the paper should clearly state what was actually done in the work presented.\n\nThe paper states: \"\"We evaluated our framework with three popular audio encoders: wav2vec2, HuBERT, and WavLM (Tables 5 and 6) ..\" Neither table 5 or 6 specify any result from HuBERT."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D7NlS8sUky", "forum": "tpkBiKShwV", "replyto": "tpkBiKShwV", "signatures": ["ICLR.cc/2026/Conference/Submission24214/Reviewer_PfDY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24214/Reviewer_PfDY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962472835, "cdate": 1761962472835, "tmdate": 1762942997914, "mdate": 1762942997914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}