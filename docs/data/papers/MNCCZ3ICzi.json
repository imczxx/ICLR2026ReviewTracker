{"id": "MNCCZ3ICzi", "number": 16382, "cdate": 1758263992361, "mdate": 1759897244241, "content": {"title": "FedReLa: Imbalanced Federated Learning via Re-Labeling", "abstract": "Federated learning has emerged as the foremost approach for decentralized model training with privacy preserving. The global class imbalance and cross-client data heterogeneity naturally coexist, and the mismatch between local and global imbalances exacerbates the performance degradation of the aggregated model. The agnosticism of global minority classes poses significant challenges for data-level methods, especially under extreme conditions with severe class deficiencies across clients. In this paper, we propose FedReLa, a novel data-level approach that tackles the coexistence of data heterogeneity and class imbalance in federated learning. By re-labeling samples with a feature-dependent label re-allocator, FedReLa corrects the biased decision boundaries without requiring knowledge of the global\nclass distribution. This modular, model-agnostic approach can be integrated with algorithmic methods to offer consistent improvements without any extra communication burden. Through extensive experiments, our method significantly improves the accuracy of minority classes and the overall accuracy on step-wise-imbalanced and long-tailed datasets, outperforming the previous state of the art.", "tldr": "", "keywords": ["Federated Learning", "Imbalanced Learning", "Long-tailed Learning", "Data Heterogeneity"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5886a51bc70e1938196d58b8df1db8fb7b6ee2db.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper targets a realistic FL setting where global class imbalance coexists with cross-client heterogeneity. It proposes a data-level “one-shot label reallocation” approach (FedReLa). Without relying on global priors or extra communication, the method uses the global model’s local posteriors, within-class z-score normalization, and a tanh thresholding step to identify “suspicious majority-class samples” near minority-class regions and probabilistically relabels them as minority. The goal is to collectively “push back” biased decision boundaries at both local and aggregated levels to mitigate imbalance. The paper claims plug-and-play integration, very low overhead, and composability with algorithm-level methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper focuses on a more realistic and challenging combination in FL: global imbalance + heterogeneity + local-global mismatch. This is a relevant and important problem.\n- The solution is data-level, requires no global priors or auxiliary data, and can be easily integrated into existing FL pipelines—practically attractive.\n- The one-shot local forward pass is low-cost, introduces no extra trainable parameters or communication, and is thus deployment-friendly."}, "weaknesses": {"value": "- The paper models the aggregated global posterior as a weighted sum of local posteriors, whereas FedAvg averages parameters, and neural-network posteriors are not linearly additive. Deriving the aggregated decision boundary via “posterior averaging” is not rigorous. The authors should justify or correct the assumption that the global posterior equals a weighted sum of local posteriors. If this assumption does not hold, do Lemma 3 and the aggregated-boundary analysis still stand? Please provide empirical evidence quantifying the discrepancy between predictions from parameter-averaged models and those from weighted averaging of local predictions.\n-  The theoretical analysis appears restricted to binary classification. Can it be extended to the multi-class case? The theory adopts a Bayesian decision-boundary perspective: strategic label reallocation is equivalent to adjusting effective prior ratios, which pulls back biased boundaries at local/global levels and improves minority/tail recognition—purportedly without global priors or extra communication. Related ideas appear in [1][2]; what are the precise differences and advantages of this work compared to [1][2]? Please add a focused discussion. Also, FedETF seems to pursue a similar effect; why is it meaningful to use FedReLa jointly with FedETF rather than redundantly?\n-  Appendix A’s Example 1 is vague. Please clarify what, exactly, the example is intended to illustrate. Also, in FL, one may use uniform averaging rather than weighted averaging—how would that change the conclusions?\n-  Despite the theoretical discussion, the proposed method seems essentially heuristic. Please clarify the precise relationship between the method and the theory—what parts of the method are directly justified by the theory, and what parts are heuristic design choices?\n- Why use independent Bernoulli sampling rather than a single Categorical draw? If multiple ones occur and you then take argmax, does this introduce bias?\n- Why prefer label reallocation over approaches such as SMOTE-like interpolation to synthesize new samples? What advantages does FedReLa offer relative to such feature-space/data-space augmentation methods under FL constraints?\n- The paper does not appear to provide code. Could the authors release reproducible code?\n\n[1] Federated Learning with Label Distribution Skew via Logits Calibration\n\n[2] Aligning model outputs for class imbalanced non iid federated learning"}, "questions": {"value": "Refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XL7QCtcKO4", "forum": "MNCCZ3ICzi", "replyto": "MNCCZ3ICzi", "signatures": ["ICLR.cc/2026/Conference/Submission16382/Reviewer_A4Ud"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16382/Reviewer_A4Ud"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825249166, "cdate": 1761825249166, "tmdate": 1762926504463, "mdate": 1762926504463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FedReLa, a data-level approach for improving federated learning in both global and local class imbalance and data heterogeneity. The key idea is to employ probabilistic, feature-dependent re-labeling of samples using posterior probabilities estimated from a shared global model, without requiring global class prior knowledge or extra communication. Extensive experiments on Fashion-MNIST, CIFAR-10, and CIFAR-100 datasets, under various imbalance and heterogeneity settings, show notable improvements in minority class and overall accuracy compared to strong baselines and state-of-the-art methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper offers a careful, mathematically grounded analysis of how re-labeling affects Bayesian decision boundaries in both local and global models. \n2. FedReLa is modular and model-agnostic."}, "weaknesses": {"value": "1. While the design uses the global model to estimate posterior probabilities for label reallocation, the paper does not thoroughly analyze how inaccuracies in these posteriors—especially early in training or under extreme class absence scenarios—might lead to over-flipping or even degrade minority class representation. \n2. While the method is designed to work without minority samples (by only flipping labels into minority categories), it is not sufficiently explained what happens if a class is entirely absent from the whole federation or present in vanishingly small quantities. Is the method robust to “missing labels” at the global level?\n3. The risk of introducing noisy labels, especially if the global model is overfitting or miscalibrated. The paper should provide concrete safeguards or error analysis here.\n4. Some presentation choices reduce readability. e.g., the indexing of notations and the mixture of probabilistic and empirical normalization steps."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9LWJdOENQs", "forum": "MNCCZ3ICzi", "replyto": "MNCCZ3ICzi", "signatures": ["ICLR.cc/2026/Conference/Submission16382/Reviewer_pgJT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16382/Reviewer_pgJT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905936571, "cdate": 1761905936571, "tmdate": 1762926503984, "mdate": 1762926503984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a data based federated learning (FL) approach to tackle the challenges of class imbalance and class heterogeneity. The authors introduce asymmetric, feature-based label noise into local data. The main contributions are: \n- It does not require access to the global class distribution \n- It utilizes existing global models to inject label noise without extra communication or local computations  \n- It does not depend on any specific model architecture."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The strengths have been outlined in the summary."}, "weaknesses": {"value": "- The method heavily relies on posterior estimates from the global model to inject noise. This is susceptible to model bias, especially in severely imbalanced scenarios.\n\n- The novelty of the method is incremental. Essentially adding noise FL might be new, but it builds on prior concepts from label-noise learning. A better explanation on novelty should be included. \n\n- There needs to be a better understanding of the z-score calculation. In particular computationally. How much does it cost to calculate it. \n\n- The experiments are carried out on a small number of clients. It is needed to run experiments on larger number of clients to empirically validate the method."}, "questions": {"value": "The strengths have been outlined in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KkT4iacX2k", "forum": "MNCCZ3ICzi", "replyto": "MNCCZ3ICzi", "signatures": ["ICLR.cc/2026/Conference/Submission16382/Reviewer_dFWi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16382/Reviewer_dFWi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949102768, "cdate": 1761949102768, "tmdate": 1762926503577, "mdate": 1762926503577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FedReLA, aiming to address heterogeneous and long-tailed data distributions by re-labeling majority-class samples as minority ones, thereby expanding the decision boundaries of minority classes. Experimental results demonstrate improvements over existing approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The overall presentation is clear and well-organized.\n- The paper is easy to follow."}, "weaknesses": {"value": "- The main weakness of this paper lies in the lack of evaluation on large-scale datasets, such as ImageNet-LT and Places-LT. Such direct label-space enlargement may face challenges when applied to large-scale scenarios.\n- The paper only provides two Fed-LT comparisons, FedETF and FedLoGe, which are insufficient to demonstrate the superiority of the proposed method. More recent approaches should be included.\n- It is interesting that the proposed method improves the performance of the majority classes when applied to Fed-LT approaches (as shown in Tab. 2). Intuitively, re-labeling the majority-class samples as minority ones should compromise majority-class performance. Could you provide more explanations (ideally with empirical analysis) for this phenomenon?"}, "questions": {"value": "Please refer to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tv7cja9gbd", "forum": "MNCCZ3ICzi", "replyto": "MNCCZ3ICzi", "signatures": ["ICLR.cc/2026/Conference/Submission16382/Reviewer_BkrP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16382/Reviewer_BkrP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983569495, "cdate": 1761983569495, "tmdate": 1762926503162, "mdate": 1762926503162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}