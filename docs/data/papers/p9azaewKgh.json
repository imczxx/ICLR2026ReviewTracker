{"id": "p9azaewKgh", "number": 14518, "cdate": 1758237825625, "mdate": 1759897365336, "content": {"title": "From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?", "abstract": "Large Vision Models (LVMs) are emerging tools for transferring cross-modal knowledge to time series, but this potential is not well understood. This work addresses the gap by investigating LVMs for both high-level (classification) and low-level (forecasting) tasks. Our aim is to not only assess whether LVMs can succeed, but also reveal why they succeed or fall short. Through a comparative benchmark covering 4 LVMs, 8 imaging methods, 18 datasets, and 26 baselines, we identify the strengths and limitations of LVMs, as well as strategies for adapting them to time series modeling. Our findings indicate while LVMs are effective for time series classification, they face notable challenges in forecasting - the best LVM forecaster is limited to specific model types and imaging methods, exhibit biases toward forecasting periods, and struggle to leverage long look-back windows. We hope our findings can serve as both a cornerstone and a practical guide for advancing LVM- and multimodal-based solutions to different time series tasks.", "tldr": "", "keywords": ["Time Series Analysis", "Large Vision Models"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5b6bda3e6009843db9e3ef9ad71e71a10e460b78.pdf", "supplementary_material": "/attachment/52333d78dcdffd9f09e89cce46d5435c0069dd94.zip"}, "replies": [{"content": {"summary": {"value": "This paper benchmarks LVMs for time-series analysis by converting time-series data into image representations. It systematically evaluates 4 LVMs (ViT, Swin, MAE, SimMIM), 8 imaging techniques, and 18 datasets across classification and forecasting tasks. The authors analyze pretraining paradigms, imaging methods, decoder vs. encoder roles, and context length sensitivity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. First comprehensive study to jointly analyze LVMs across TSC and TSF using diverse image encodings.\n2. Provides a valuable reference for researchers exploring multimodal or vision-inspired time-series models."}, "weaknesses": {"value": "1. Most of the forecasting results rely on UVH/MVH imaging applied to relatively periodic datasets. It’s not clear how well these conclusions hold for more irregular, noisy, or non-periodic signals.\n2. The claim that decoders matter more than encoders in forecasting is interesting, but the current evidence is mostly correlational. A simple decoder-swapping or partial-finetuning experiment could make this stronger.\n3. The authors note that performance drops for longer look-back windows, potentially due to image resolution limits, but this isn’t experimentally verified. It’s unclear whether the weaknesses observed (e.g., poor long-context forecasting) are intrinsic to LVMs or simply due to architectural mismatch, because the approach reuses vision models mostly as-is, with only changes to heads or decoders. \n4. The central assumption that converting time series into 2D image structures enables vision transformers to generalize to sequential modeling is interesting but not fully justified. Time and space are fundamentally different dimensions; the study doesn’t discuss what inductive biases are gained or lost by this transformation. For example, local temporal continuity is often disrupted in GAF/UVH encodings, and the benefit of spatial locality learned by ViTs is not clearly transferable to time-dependent modeling.\n5. The paper tests many imaging techniques (GAF, RP, UVH, etc.), but it remains unclear why certain encodings align better with LVM features. There’s little analysis of the representational geometry, e.g., whether these encodings preserve correlations, periodicities, or value scales in a meaningful way. The success of UVH/MVH feels more empirical."}, "questions": {"value": "1. How robust are the UVH/MVH findings to non-periodic datasets or those with exogenous inputs (e.g., with irregular seasonality or abrupt regime shifts)?\n2. Have you tested simple augmentations (phase shifts, randomized boundaries) to mitigate the UVH bias? You have shown evidence that UVH introduces a copy-period shortcut during forecasting, leading to biased predictions. Did you experiment with simple augmentations to disrupt the implicit periodic alignment? I am wondering whether the bias is structural to the imaging method or sensitive to data alignment choices.\n3. The paper concludes that forecasting success in self-supervised LVMs (MAE, SimMIM) mainly stems from pre-trained decoders rather than encoders. Could decoder-swapping experiments (e.g., MAE encoder + SimMIM decoder and vice versa) validate this more directly? Did you observe consistent trends when fine-tuning only the decoder versus only the encoder?\n4. The authors report that forecasting performance declines when the input length exceeds roughly 1k steps, hypothesizing this stems from fixed image resolution constraints. Have you tried multi-scale or variable-resolution inputs (e.g., tiled or pyramid representations) to test whether LVMs can better exploit extended contexts when provided with hierarchical image patches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MbFx0XrZAi", "forum": "p9azaewKgh", "replyto": "p9azaewKgh", "signatures": ["ICLR.cc/2026/Conference/Submission14518/Reviewer_1Fj9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14518/Reviewer_1Fj9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761340525926, "cdate": 1761340525926, "tmdate": 1762924912284, "mdate": 1762924912284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper conducts the comprehensive study on the utility of Large Vision Models for time series analysis, covering classification and forecasting tasks. It evaluates 4 LVMs, 8 imaging methods, 18 datasets, and 26 baselines. Key findings show LVMs excel at TSC by leveraging pre-trained semantic recognition capabilities, with GAF as the optimal imaging method. However, TSF poses greater challenges, self-supervised LVMs such as MAE paired with UVH imaging perform best but suffer from period bias and limited long look-back window utilization. Ablation analyses confirm LVMs transfer pre-trained knowledge, capture temporal order, and rely more on decoders for forecasting. While LVMs trade higher computational cost for superior TSC performance, their TSF effectiveness is constrained by task-specific limitations, providing foundational insights for multimodal time series research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It systematic explore large vision models for time series classification and forecasting, covering diverse models, imaging methods, datasets, and baselines to fill existing research gaps.\n2. In-depth mechanism analysis reveals key insights and quantifies temporal pattern capture, providing essential support for future optimizations.\n3. It identifies optimal task-specific configurations and targeted fine-tuning strategies, enabling efficient real-world implementation.\n4. Rigorous benchmark comparisons  and analysis experiments ensure reliable conclusions, establishing a credible reference for subsequent research."}, "weaknesses": {"value": "1. The study mentions that Large Vision Models have numerous limitations in time series forecasting tasks. However, leveraging the feature extraction capability of LVMs for TSF and integrating them with time series through cross-modal fusion may help avoid visual limitations and improve overall performance. The current work lacks an investigation into cross-modal fusion involving visual modalities.\n2. The study evaluates 8 imaging methods, and from Table 16, UVH achieves the best performance while MVH ranks second. Other imaging methods perform significantly worse—frequency-domain methods such as Wave and STFT do not yield better results through imaging. Does this indicate that frequency-domain graphs are unsuitable as inputs for LVMs? For MVH and UVH, why does MVH underperform UVH, and would the results differ for datasets with variable dependencies? Additionally, could simultaneous input of MVH and UVH bring performance gains?\n3. The study lacks comparisons with state-of-the-art large time series models (e.g., Chronos-2, Moirai2), which may undermine the comprehensiveness of LVMs’ performance evaluation."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kznvnHm0uT", "forum": "p9azaewKgh", "replyto": "p9azaewKgh", "signatures": ["ICLR.cc/2026/Conference/Submission14518/Reviewer_fb57"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14518/Reviewer_fb57"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645236071, "cdate": 1761645236071, "tmdate": 1762924911841, "mdate": 1762924911841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a comprehensive benchmark study investigating the effectiveness of Large Vision Models (LVMs) for time series analysis. The authors evaluate four LVMs (ViT, Swin, MAE, SimMIM) across two representative tasks: time series classification (TSC) and time series forecasting (TSF). Through extensive experiments with 8 imaging methods and 18+ baselines on 18 datasets, the paper finds that LVMs excel at TSC but face significant challenges in TSF due to limitations in encoder utilization, inductive biases, and long look-back window handling. The work provides detailed ablation studies and practical insights for adapting LVMs to time series tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The paper covers a substantial experimental scope (4 LVMs, 8 imaging methods, 18 datasets, 26 baselines) with well-designed ablation studies (RQ1-RQ10). The breadth of analysis is commendable and provides genuine value to the community.\n(2) The discovery that pre-trained decoders contribute more than encoders in TSF (RQ8) is genuinely interesting and counterintuitive. The analysis of the period-based imaging bias (RQ9) with formal characterization (Lemma 1) provides actionable insights.\n(3) The temporal perturbation experiments (RQ6, Table 4) effectively demonstrate that LVMs do capture temporal information, strengthening claims about their utility beyond pattern matching.\n(4) The paper is generally well-organized with clear research questions guiding the narrative. Figures 1-3 effectively summarize the methodology and key findings."}, "weaknesses": {"value": "(1) The paper is purely an empirical benchmark study without methodological contributions. While valuable, such studies typically require either exceptional insights or novel proposed solutions. \n(2) The paper identifies what fails (encoders, long windows) but provides limited mechanistic understanding of why.\n(3) The claim that forecasting is \"low-level\" and requires numerical inference needs deeper investigation beyond decoder architecture.\n(4) The connection to recent multimodal approaches (mentioned briefly) deserves more engagement.\n(5) Using ImageNet-derived normalization for time series images may introduce bias. Ablation on normalization strategy is absent.\n(6) Why variate-independence assumption for all tasks? For TSC, joint multivariate modeling might capture interactions."}, "questions": {"value": "(1) Decoder Importance (RQ8): Can you provide more analysis on what the decoders learn that aids TSF? Attention visualizations or learned representations would be illuminating. Is this specific to MAE/SimMIM architectures or general?\n(2) Periodic Bias Generalization: Lemma 1 is elegant for UVH, but how do other imaging methods induce biases? Can similar formal analysis be provided for GAF or MVH? Does this explain why they underperform for TSF?\n(3) Why limit to 8 TSF datasets? Larger evaluation (similar to TSC's 10 datasets) would strengthen claims. Would results change with higher-dimensional time series (e.g., multivariate > 20 dimensions)?\n(4) The recommendation to fine-tune only norm layers for TSF seems counterintuitive. Is this an artifact of small datasets or a fundamental limitation of LVMs for low-level tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sk6EU5Vlmq", "forum": "p9azaewKgh", "replyto": "p9azaewKgh", "signatures": ["ICLR.cc/2026/Conference/Submission14518/Reviewer_EKwA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14518/Reviewer_EKwA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897172406, "cdate": 1761897172406, "tmdate": 1762924911300, "mdate": 1762924911300, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}