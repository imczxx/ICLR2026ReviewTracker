{"id": "zAtrBcGsyf", "number": 8242, "cdate": 1758075759316, "mdate": 1759897796751, "content": {"title": "Multimodal Representation Learning Conditioned on Semantic Relations", "abstract": "Multimodal representation learning has advanced rapidly with contrastive models such as CLIP, which align image-text pairs in a shared embedding space. However, these models face the limitations: (1) they typically focus on image-text pairs, underutilizing the semantic relations across different pairs. (2) they directly match global embeddings without contextualization, overlooking the need for semantic alignment along specific subspaces or relational dimensions. To address these issues, we propose Relation-Conditioned Multimodal Learning (RCML), a framework that learns multimodal representations under natural-language relation descriptions to guide both feature extraction and alignment. Our approach constructs many-to-many training pairs linked by semantic relations and introduces a relation-guided cross-attention mechanism that modulates multimodal representations under each relation context. The training objective combines cross-modal and intra-modal contrastive losses, encouraging consistency across both modalities and semantically related samples. Experiments on different datasets show that RCML,consistently outperforms strong baselines on both retrieval and classification tasks, highlighting the effectiveness of leveraging semantic relations to guide multimodal representation learning.", "tldr": "", "keywords": ["Multimodal Representation Learning", "Relation-Conditioned Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/687f1347a8c2d3a7913c9ee3d207758295995cd6.pdf", "supplementary_material": "/attachment/b9c0f7a92e4087fadbb66b495df07f0071202f78.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new multimodal representation learning framework that conditions on the semantic relations between different samples. Beyond leveraging paired text–image data, it further defines and explores inter-sample relationships and utilizes these connections to guide feature extraction and alignment. The framework is optimized using contrastive losses among image–text, text–image, text–text, and image–image pairs to enhance both cross-modal and intra-modal consistency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- It is interesting to exploit unpaired data by modeling potential semantic relations between samples.\n\n- The proposed method is evaluated on three downstream tasks, namely the relation-guided retrieval, relation type prediction, and relation validity prediction, where the results demonstrate performance gains over existing multimodal representation learning approaches."}, "weaknesses": {"value": "- The proposed framework appears specifically tailored for recommendation systems. However, in the evaluation, it is compared mainly against general-purpose multimodal representation learning methods, which may not be a fair comparison. The authors should also consider including baselines from existing recommendation approaches.\n\n- The paper misses several important related works in multimodal representation learning, such as VAST: Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset (NeurIPS 2023), as well as relevant discussions on existing recommendation systems.\n\n- The presentation quality needs improvement. The current version contains typographical errors, missing spaces, and incomplete references (e.g., “?”). The authors should carefully proofread and polish the manuscript."}, "questions": {"value": "Can the proposed method be generalized and applied to tasks beyond recommendation systems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V0QHSRTRB1", "forum": "zAtrBcGsyf", "replyto": "zAtrBcGsyf", "signatures": ["ICLR.cc/2026/Conference/Submission8242/Reviewer_SLmC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8242/Reviewer_SLmC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761318924579, "cdate": 1761318924579, "tmdate": 1762920187738, "mdate": 1762920187738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Relation-Conditioned Multimodal Learning (RCML), a framework designed to enhance multimodal representation learning by explicitly incorporating semantic relations between items. The core idea is to move beyond the simple pairwise alignment of image-text pairs (as in CLIP) and instead model many-to-many relationships described by natural language. The method achieves this through a novel relation-guided attention mechanism, where the embedding of a relation's text description modulates the feature extraction from image and text modalities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The primary contribution, using natural language descriptions of relations to directly condition the representation learning process, is to the best of my knowledge novel and addresses a gap in the current vision-language model. \n\n2. RCML consistently and significantly outperforms strong baselines, including CLIP, SigLIP, and ImageBind, on several well-designed tasks. \n\n3. The paper evaluates the model on three distinct tasks (retrieval, type prediction, validity prediction) that effectively test the claimed capabilities. The ablation studies (Fig. 4) clearly demonstrate the importance of each proposed component, especially the inter-sample relations and their semantic descriptions."}, "weaknesses": {"value": "1. In the main retrieval task (4.3.1), the strategy for providing relational context to baseline models (i.e., concatenating the relation text with the item description) may not be the strongest form of comparison. It is unclear if this simple concatenation allows models like CLIP to effectively leverage the relational information. A more rigorous baseline would involve fully fine-tuning a model like CLIP on this task, which would better isolate the architectural benefits of RCML's explicit modulation mechanism.\n\n2. The relation type prediction task (4.3.2) demonstrates RCML's capabilities but lacks a comparison against any baseline. A simple baseline, such as a lightweight classifier trained on top of concatenated frozen features from a standard CLIP model, would help interpreting RCML's performance and establish the difficulty of the task.\n\n3. While the paper mentions the limitations of pairwise methods, the introduction (lines 38-51) could be strengthened by citing relevant work on relational learning from other fields (e.g., knowledge graph-based recommendation) to better situate the problem in the broader literature. The paper also contains several missing citations (marked with \"?\") in the related work section, that suggests writing still needs some polishing.\n\n4. There is growing evidence that alignment emerges between latent spaces without explicit optimization to enforce it (few examples: [1,2]) . I think it would be beneficial to briefly discuss how this work positions with respect to those, especially for [2] that explicitly makes use of relations between instances.\n\n---\n\n[1] Huh,  et al, \"The Platonic Representation Hypothesis,\" 2024.\n\n[2] Moschella, et al, \"Relative representations enable zero-shot latent space communication,\" in ICLR, 2023."}, "questions": {"value": "1. The framework uses the same CLIP text encoder for both item descriptions and the natural language relation descriptions. This implies that these two types of text live in the same embedding space. Could the authors elaborate on this design choice? Have the authors considered whether this is optimal, or if using separate projectors or encoders for relations might offer benefits?\n\n2. An ablation to show that relations cannot learned implicitly from the data would improve the manuscript. \nWould an experiment where a standard CLIP model is fully fine-tuned on the relation-guided retrieval task be possible? This would help clarify whether the performance gains come from the proposed architecture or simply from training/fine-tuning on relational data.\n\n3. In the relation-guided retrieval task (4.3.1), the manuscript report results on \"36 out of 40 metrics.\". If my understanding is correct, these should be 5 similarity scores across 8 datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IgbxCIF9P1", "forum": "zAtrBcGsyf", "replyto": "zAtrBcGsyf", "signatures": ["ICLR.cc/2026/Conference/Submission8242/Reviewer_ykbu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8242/Reviewer_ykbu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559865702, "cdate": 1761559865702, "tmdate": 1762920186442, "mdate": 1762920186442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces relation-conditioned multimoidal learning, that incorporates relationships between samples for CLIP like encoder models. Specifically, the authors make use of inter-sample relationship by proposing a relation-guided attention mechanism as well as a hybridf loss function. Experiments showed improvements on classification and retrieval tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- the paper is somewhat easy to read for which the reviewer appreciates!"}, "weaknesses": {"value": "- there has been a lot of work that tries to add prior knowledge of relationships between samples in the contrastive learning world. lots of different ways people have tried contrast samples with. in general not a fan of these line of work and not sure how much this paper contributes to that body of work.\n- the baselines compared are old (<=2023) and not even sure how it's a valid comparison given their parameter count, training dataset size, etc. are all different. \n- natural language relationships are not naturally occuring, they are generated suing clustering it seems (looking at Appendix) - correct me if i'm wrong. the authors might want to be clear about that these relationship does not come for free or naturally labeled in the dataset. the authors might also want to provide some analysis on how well this step introduces noisy relationships that might not help with learning.\n- the evaluation is mostly on product oriented datasets as opposed to more general benchmarks. I would recommend authors benchmark its method on a few more datasets that is more broadly applicable.\n- the authors should also compare with baselines that make use of semantic relationships."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qbNztwzkhi", "forum": "zAtrBcGsyf", "replyto": "zAtrBcGsyf", "signatures": ["ICLR.cc/2026/Conference/Submission8242/Reviewer_bTXS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8242/Reviewer_bTXS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945286519, "cdate": 1761945286519, "tmdate": 1762920185900, "mdate": 1762920185900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a learnable pooling layer that can be plugged into any modality specific branch of a CLIP like model. The idea is to make this pooling conditional on a relation encoding, a vector encoding of a text description of what the aggregation should focus on. This allows the model to produce modality specific embeddings that can be compared under a given relation instead of relying on generic CLIP similarities.\nExperiments on two datasets show promising results, and there are ablations for the main hyperparameters involved."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The main idea of relation-conditioned representation learning is interesting and addresses a real limitation of standard CLIP objectives.  \n- Section 4.4 (sensitivity analysis) is a solid ablation. It explores the effect of β and provides insight into how the conditioning mechanism behaves.  \n- Using frozen CLIP backbones (ViT-B/32) keeps the setup clean and isolates the proposed contribution.\n- The results are consistent across datasets.\n- The code is attached and it looks structured and replicable.\n- Figure 3 is beautiful!"}, "weaknesses": {"value": "### Clarity and terminology\n- **Abstract (013–015)**: the expression “semantic relations across each pairs” is too vague. Even after reading the introduction, it’s not clear what kinds of relations are being modeled. If the goal is to keep the abstract general, it would still help to include one or two examples, or clarify this in the introduction. This is the central concept of the paper and should be grounded with concrete cases from the start.  \n- **Introduction (036–037)**: the list of “several directions” should include references (at least one per direction). The same applies to line 050 for the graph-based approaches.\n\n### References and formatting\n- Some references appear broken or missing (e.g., 095–096, 117–118). It might be useful to search for “?” in the PDF to find them.  \n\n### Method and conceptual clarity\n- **Section 3 (last paragraph of 3.2)**: it seems there are no relation-specific negatives. If that’s correct, it’s a bit counterintuitive, since it would mean the model only learns to make items with *any* relation closer, regardless of which relation it is. Clarify whether that is the intention or if some relation-level contrast is used.  \n- **Section 3.3**: this part is difficult to follow. Please confirm if this interpretation is correct:  \n  > The implementation adds a modality-specific modulating layer on top of each CLIP branch. This layer performs a pooling step conditioned on a property encoded by the text branch. The resulting embedding is then interpolated with the standard CLS/EOT embedding, controlled by β.  \n  If this matches the authors’ design, it would help to rewrite this section more clearly along those lines.  \n\n### Experiments and reporting\n- **Section 4.1 (265–266)**: “each relation is annotated with a natural-language description indicating its semantic context.” Since the paper previously treats the relation as an abstract concept, it would help to give a couple of concrete examples and mention that these descriptions come directly from the dataset.  \n- **Section 4.2 (last sentence)**: the claim “with significantly more parameters than ours” should include an approximate quantification and a pointer to Table 2.  \n- **Section 4.3.2 (end of page 6)**: “We report Top-3 Accuracy on all datasets where relation types are sufficiently meaningful to support evaluation.” Specify which datasets this applies to and how, so readers know what “sufficiently meaningful” means here.  \n- **Section 4.3.3 (supervised relation validation)**: the experiment lacks detail. It’s not clear how many samples the classifiers are trained on, what the ratio of positive to negative examples is, or what training strategy is used.  \n- **Section 4.4**: the sensitivity analysis is valuable, but it would be even stronger if paired with an analysis of how much of the original CLIP performance is retained. That would show the trade-off between relation specialization and general-purpose ability.  \n\n### Table 1 and metrics\n- Consider adding the **average rank (or MRR)**. Hit@5 only tells whether the correct item appears in the top 5, not its exact position. Average rank provides finer resolution. The code already computes MRR, so this should be easy to add, for example in the appendix as a mirrored table.  \n- Clarify **how many samples** are used to compute the metrics. Section 4.3.1 mentions one positive and twenty negatives per query, but the total number of queries per domain is not reported.  \n- Report **standard deviation** across runs. Appendix A.1 mentions three seeds but no variance. Reporting mean ± std would make the results more interpretable in terms of stability and significance.  \n\n### Minor\n\n- **Introduction (031)**: “compatible” would fit better than “unified”.  \n- Formatting issues occur in several citations (e.g., 106–107, 118, 291–292), including parentheses and spacing. The [`cleveref`](https://ctan.org/pkg/cleveref?lang=en) package could simplify this.  \n- There are small inconsistencies in punctuation spacing that should be corrected.  \n- **Section 3 (137)**: the statement “unlike traditional contrastive learning that operates only on matched pairs” is not fully accurate. Standard contrastive methods also use negative pairs, not only the diagonal of the similarity or distance matrix."}, "questions": {"value": "1. Regarding Section 3 (last paragraph of 3.2): are there relation-specific negatives, or is the objective indeed relation-agnostic once a connection exists?  \n2. On Section 3.3: can you confirm whether the interpretation provided under \"Method and conceptual clarity\" matches your implementation?  \n3. For Table 1: how many total test samples per domain are used to compute the metrics, and what's the MRR?  \n4. For Section 4.3.3: how many samples were used to train and evaluate the binary classifiers, and what was the positive/negative ratio?  \n5. For Section 4.1: can you provide examples of the “natural-language descriptions” mentioned? I'm assuming these are directly from the datasets, is it correct?  \n7. For Section 4.4: how much of the original CLIP performance is preserved after relation conditioning?  \n8. Can you confirm that the CLIP baseline uses the OpenAI ViT-B/32 weights and not OpenCLIP or LAION implementations?  \n\n### Different CLIP baseline\n\nThe current CLIP baseline shows that standard CLIP embeddings are too generic to compare product encodings under a specific relation. That’s fine and expected: there’s nothing in CLIP that makes it focus on the relation itself. So I see it more as a baseline justifying the need of **conditioning on the relation**, in support of the research question of this paper.\n\nA fairer baseline to compare against, still with no training, would be to compare the product encodings `zA` and `zB` (whatever their modality) **under** the similarity they each have with respect to the relation encoding `zr`. This gives a value in `[-1, 1]`,  the product of two cosine similarities, that reflects how much the two samples align with the relation.\n\n```python\ndef relation_similarity(zA, zB, zr):\n    zA, zB, zr = [x / x.norm() for x in (zA, zB, zr)]\n    return (zA @ zr) * (zB @ zr)\n```\n\nAnd it can be used for both ranking (as it is) or to tune a threshold for classification purposes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SB62Nkk4nt", "forum": "zAtrBcGsyf", "replyto": "zAtrBcGsyf", "signatures": ["ICLR.cc/2026/Conference/Submission8242/Reviewer_MwnY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8242/Reviewer_MwnY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956219248, "cdate": 1761956219248, "tmdate": 1762920185457, "mdate": 1762920185457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}