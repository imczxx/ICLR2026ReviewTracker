{"id": "ARHCFvgx6G", "number": 10800, "cdate": 1758182172542, "mdate": 1759897627798, "content": {"title": "VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning", "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language tasks yet remain limited in long video understanding due to the limited context window. Consequently, prevailing approaches tend to rely on uniform frame sampling or static pre-selection, which might overlook critical evidence and unable to correct its initial selection error during its reasoning process. To overcome these limitations, we propose VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control their visual focus during reasoning. Starting from a coarse low-frame-rate overview, VideoZoomer invokes a temporal zoom tool to obtain high-frame-rate clips at autonomously chosen moments, thereby progressively gathering fine-grained evidence in a multi-turn interactive manner. Accordingly, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase on a curated dataset of distilled exemplar and reflection trajectories, followed by reinforcement learning to further refine the agentic policy. Extensive experiments demonstrate that our 7B model demonstrates diverse and complex reasoning patterns, yielding strong results across a broad set of long video understanding and reasoning benchmarks. These emergent capabilities allow it to consistently surpass existing open-source models and even rival proprietary systems on challenging tasks, while achieving superior efficiency under reduced frame budgets.", "tldr": "", "keywords": ["Multimodal LLM", "Reinforcement Learning", "Video Understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/73120f3e7ed872c08ddb1f7e92c52ef46f377448.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an agentic video processing framework: the model is first provided with several frames at a low frame rate and then it can use the temporal zoom tool to obtain high-frame-rate clips at autonomously chosen moments. The framework is worked in a multi-turn manner. The training undergoes a two-phase recipe: a cold-start supervised fine-tuning phase and a reinforcement learning phase. And the authors construct a cold start dataset with 11k samples and diverse reasoning patterns. The resulting model VideoZoomer achieves a remarkable performance improvement over the baseline Qwen2.5-VL 7B."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper constructs a cold start dataset with diverse reasoning patterns, especially the reflection data. Construction reflection data from self-generated failure CoTs is interesting and reasonable, which could be a better way to collect high-quality CoT data.\n2. The model is evaluated on a broad range of benchmarks, including long video understanding benchmarks and long video reasoning benchmarks. The robust improvements across multiple benchmarks validate the effectiveness of the proposed methods.\n3. The paper also demonstrates that VideoZoomer can further equip a frame selector for frame initialization to improve the performance, showing that the method is orthogonal to other dynamic frame sampling methods."}, "weaknesses": {"value": "1. This work uses LongVideoReason as the training dataset and constructs a cold start training dataset. And the reflection data is constructed from incorrect rollouts. The rollouts are multi-step CoTs, and the errors may occur at any step, CoTs for zoom-in or CoTs for answers.  However, it seems that the LongVideoReason dataset does not have gt timespan annotations. How to determine whether the CoTs for zoom-in are correct or not?\n2. The cold start dataset is relatively small.\n3. The ablation study on reflection cold start data is not convincing, as more sft data is used.\n4. The performance of Qwen2.5-VL 7B baseline on long video understanding benchmarks is relatively low with 128 frames. Can you provide more inference details?\n5. The framework is a multi-step reasoning process. A common failure case is that the reasoning can not stop (i.e. the model zooms in on the video endlessly or the model does not provide a valid answer within the budget). How to handle this failure case during training and inference?\n6. Compared with other dynamic frame sampling works, this work introduces another sampling variable, 'fps'. And in the provided cases, Figures 4 and 8, fps=8 is extremely high compared to other sampling strategies. Can authors show how the predicted fps affects the performance?\n7. The 'fotmat' in Equation (1) is a typo."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HG1PTQcgw6", "forum": "ARHCFvgx6G", "replyto": "ARHCFvgx6G", "signatures": ["ICLR.cc/2026/Conference/Submission10800/Reviewer_xgg2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10800/Reviewer_xgg2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578370848, "cdate": 1761578370848, "tmdate": 1762922014423, "mdate": 1762922014423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "VideoZoomer is an agentic long-video QA framework that starts with a coarse “glance” and then issues targeted zoom calls over selected time spans to gather fine-grained evidence before answering. It’s trained in two stages—cold-start supervised trajectories (including reflection to correct failures) followed by reinforcement learning with rewards that encourage accurate, well-formatted, and purposeful tool use. Experiments across diverse long-video understanding and reasoning benchmarks show consistently better accuracy–efficiency trade-offs than single-pass baselines, and the approach remains compatible with stronger initial frame selectors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces a clear agentic framework that couples coarse “glance” perception with targeted temporal zooming, yielding a principled separation between broad coverage and fine-grained evidence acquisition.\n\n2. This paper evaluates across diverse long-video understanding and reasoning benchmarks, with the largest gains on tasks that require precise temporal detail, supporting the method’s intended use case."}, "weaknesses": {"value": "1. The zoom tool is basically one-dimensional: A lot of long-video questions hinge on tiny textual clues (scoreboards, signs, on-screen text), and just “adding frames” won’t reliably capture those.\n\n2. The cold-start data comes from external frontier models (e.g., GPT-style teachers). That brings possible style bias\n\n3. Multi-round zooming can be expensive in practice."}, "questions": {"value": "1. How does accuracy change with 0/1/2/3/4 zoom calls? Is there a point of diminishing returns?\n\n2. When the model zooms the wrong time span, does it recover in later rounds, or does it lock in and fail?\n\n3. How often does the model answer correctly without making any zoom calls, and what types of questions and accuracy are those?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ixwZMlmaOG", "forum": "ARHCFvgx6G", "replyto": "ARHCFvgx6G", "signatures": ["ICLR.cc/2026/Conference/Submission10800/Reviewer_mPz9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10800/Reviewer_mPz9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867065745, "cdate": 1761867065745, "tmdate": 1762922013892, "mdate": 1762922013892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VideoZoomer, an agentic model enabling MLLMs to dynamically adjust their visual focus during reasoning. Beginning with a low-frame-rate overview, the model uses a temporal zoom tool to capture high-frame-rate clips at key moments, progressively gathering fine-grained evidence. The training process involves supervised fine-tuning on curated datasets, followed by optimization with DAPO. Experimental results demonstrate that the 7B model exhibits diverse and sophisticated reasoning capabilities, achieving strong performance in long video understanding tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper contributes a training dataset comprising 11,000 trajectories, which is used to enhance the tool-calling capabilities of models.  \n\n- The case visualizations presented in the paper are good"}, "weaknesses": {"value": "- The technical contributions of the paper are limited, as its main novelty lies in providing a curated training dataset to enhance the tool-calling capabilities of models.\n\n- The method adds a bonus to tool-call rewards when the final answer is correct, which increases the unnecessary frequency of tool use. For example, the model may continue calling tools unnecessarily, retrieving irrelevant clips even after it already has the correct answer, ultimately inflating the reward.\n\n- While the paper mentions using 11,000 trajectories for the cold-start phase of training, it does not specify the data used during the RL phase, leaving gaps in transparency and reproducibility."}, "questions": {"value": "- Will further scaling up the training dataset continue to improve the model’s performance?  \n\n- The paper mentions starting with 64 frames as input, gradually increasing by 16 frames each time. However, in Figure 1 (the right part), the number of input frames for VideoZoomer seems inconsistent with this description.  \n\n- Will the code and the training dataset used in the paper be open-sourced?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SPWLv32WSv", "forum": "ARHCFvgx6G", "replyto": "ARHCFvgx6G", "signatures": ["ICLR.cc/2026/Conference/Submission10800/Reviewer_JYRh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10800/Reviewer_JYRh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991714924, "cdate": 1761991714924, "tmdate": 1762922013470, "mdate": 1762922013470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the video LLM with self-bootstrapped clip selection capability by SFT and RL to reduce the frame sampled and inference cost. This paper presented VideoZoomer, a agentic framework integrating <video_zoom> tool call and <think> thinking abilities, to select video clips for its processing. The proposed VideoZoomer method can reach outperforming accuracies compared to previous methods while using less input frames. The author also performed detailed ablation studies on cold-start, reflection, RL, and score design."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) This paper is one of the first cohort to explore the frame selection method by agentic RL in video LLM domain, delivering substantial novelty. Also, the agentic RL framework is not simply adopted from the image/language domain to the video domain. Instead, it incorporates tool calling and methods designed specifically for videos (e.g., temporal zoom-in, on-policy reflection).\n\n2) The resulted agentic model by GRPO training, outperforms the baseline model by a large margin, especially on the long video understanding task. The author also detail key components in proposed RL framework, showing that the cold-start and reflection finetuning is essential for good RL models in reasoning video frame selection model, which is of great valuable insights."}, "weaknesses": {"value": "Overall this paper is of great technical value and soundness, but there are some minor concerns listed below:\n\n1) There are several minor typos across texts, including line 151: \"stragety\", line 277: \"fotmat\" and more.  The author should perform grammar and word check throughout the paper;\n\n2) Are other capabilities of video LLMs (Qwen 2.5-VL) well maintained? Like short video captioning?\n\n3) Qwen 2.5-VL is known to lack of native <think></think> reasoning capabilities. The authors performed off-policy warm-start and on-policy reflection SFT to enable the reasoning capability but is it robust when the input data is out of distribution of SFT training data, like very simple CLEVRER data? Just curious and I want to hear the authors' insights into it."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GvAcUCPoZJ", "forum": "ARHCFvgx6G", "replyto": "ARHCFvgx6G", "signatures": ["ICLR.cc/2026/Conference/Submission10800/Reviewer_21rm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10800/Reviewer_21rm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991788450, "cdate": 1761991788450, "tmdate": 1762922012872, "mdate": 1762922012872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}