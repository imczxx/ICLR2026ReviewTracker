{"id": "kOIclg7muL", "number": 16901, "cdate": 1758270117956, "mdate": 1759897211840, "content": {"title": "TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models", "abstract": "Recent advances in multimodal time series learning underscores a paradigm shift from analytics centered on basic patterns toward advanced time series understanding and reasoning. However, existing multimodal time series datasets mostly remain at the level of surface alignment and question answering, without reaching the depth of genuine reasoning. The absence of well-defined tasks that genuinely require time series reasoning, along with the scarcity of high-quality data, has limited progress in building practical time series reasoning models (TSRMs). To this end, we introduce Time Series Reasoning Suite (TSR-Suite), which formalizes four atomic tasks that span three fundamental capabilities for reasoning with time series: (1) perception, acquired through scenario understanding and causality discovery; (2) extrapolation, realized via event-aware forecasting; and (3) decision-making, developed through deliberation over perception and extrapolation. TSR-Suite is the first comprehensive time series reasoning suite that supports not only thorough evaluation but also the data pipeline and training of TSRMs. It contains more than 23K samples, of which 2.3K are carefully curated through a human-guided hierarchical annotation process. Building on this foundation, we introduce TimeOmni-1, the first unified reasoning model designed to address diverse real-world problems demanding time series reasoning. The model is trained in multiple stages, integrating a mixture of task scenarios, novel reward functions, and tailored optimizations. Experiments show that TimeOmni-1 delivers strong out-of-distribution generalization across all tasks and achieves a high rate of valid responses. It significantly improves causality discovery accuracy (64.0% vs. 35.9% with GPT-4.1) and raises the valid response rate by over 6% compared to GPT-4.1 on the event-aware forecasting task.", "tldr": "", "keywords": ["Time series reasoning", "multimodal time series", "time series models", "time series"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e78bad90487d763f2ccf4653886f11c5f87dc45d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces TSR-Suite, a dataset covering time series perception, extrapolation and decision-making tasks. This dataset is large in size so that it could support both evaluation and training of time series reasoning models. With the collected training trajectories, the paper introduces TIMEOMNI-1, the first unified reasoning model to tackle time series reasoning tasks. TIMEOMNI-1 achieves superior performance over current landscape of time series reasoning models. Extensive ablation and analysis studies confirms the effectiveness of CoT SFT + RL and multi task joint training paradigm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-positioned and well-motivated, easy to follow and very clear in structure. \nThe paper clearly identifies limitations in current time series benchmarks for time series reasoning and introduces the new dataset with clear goals and well-established data annotation pipeline. \nThough the training paradigm is not novel, the adaptation to time series domain demonstrated great execution. \nAnalysis and ablation studies reveal insights that are very useful for the community."}, "weaknesses": {"value": "The experimental results on time series reasoning is purely performed on the newly proposed dataset. Although there are certain limitations on existing time series reasoning datasets, it would be helpful to see TIMEOMNI-1's performance on some established time series reasoning benchmarks/datasets."}, "questions": {"value": "For event aware-forecasting, how is the reward designed since the output should be a variable length sequence. \n\nHow do you split in vs out of distribution data? is it based on data domain?\n\nare different time series domain roughly evenly distributed across task types to prevent certain task types biased towards certain time series application domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KmGyOFIvtp", "forum": "kOIclg7muL", "replyto": "kOIclg7muL", "signatures": ["ICLR.cc/2026/Conference/Submission16901/Reviewer_ruC5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16901/Reviewer_ruC5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857897183, "cdate": 1761857897183, "tmdate": 1762926932444, "mdate": 1762926932444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a complete framework for time series reasoning that combines a multi task benchmark and a unified training recipe for large language models. The benchmark, called TSR Suite, covers perception, extrapolation, and decision making with four atomic tasks. The dataset contains more than twenty three thousand examples with a carefully curated subset of hierarchical chain of thought traces for supervision. The model, called TIMEOMNI 1, is trained in two stages. First, supervised fine tuning injects time aware priors using a small set of high quality reasoning trajectories. Second, reinforcement learning encourages correct and well structured solutions using task grounded rewards. Discrete tasks are scored by accuracy. Sequence prediction uses an error based reward that maps the mean absolute error into a bounded score and a length counting reward to ensure the model outputs the correct number of steps. Joint training across the four task families is shown to improve transfer. The model reports sizable gains on causality and decision oriented tasks while preserving general reasoning strength on non time series benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles time series as a reasoning problem and operationalizes this view with a benchmark that spans perception, extrapolation, and decision making. This scope is broader than pure forecasting and motivates models that can explain and decide.\n\n2. In this paper, task design is aligned to evaluation. Multiple choice tasks allow objective grading and sequence prediction uses metric aligned rewards. This makes the reinforcement signal faithful to the intended competence.\n\n3. The training pipeline is relative simple to adopt. A small number of high quality trajectories teaches decomposition. Task rewards then refine the behavior without requiring domain specific architectures. The division of labor between supervised fine tuning and reinforcement learning is clear and effective.\n\n4. In the paper, evidence for cross task transfer is convincing. The model benefits from learning perception and extrapolation before decision tasks. Zero shot improvements on harder tasks suggest that the benchmark composition is synergistic rather than siloed.\n\n5. The model does not only fit a single dataset but shows robustness when the distribution shifts. The paper also checks general reasoning benchmarks to show that specialization does not degrade broader ability."}, "weaknesses": {"value": "1. The contributions mix dataset construction, task design, training recipe, and model evaluation without a clear hierarchy. A contribution map that separates what is new in task formulation from what is new in learning would improve the paper.\n\n2. Although the benchmark size is large, the carefully verified chain-of-thought portion is relatively small. The paper would be stronger with an ablation that quantifies how many verified trajectories are needed and how quality trades off with quantity.\n\n3. Several reported gains are sizable, but the paper does not consistently provide confidence intervals or significance tests for both in-domain and out-of-domain settings.\n\n3. In this paper, reward sensitivity is under explored. A sensitivity sweep and a report of failure cases would help others reproduce the effect.\n\n4. The approach still depends on strict output formatting with explicit think and answer tags. This is practical for research but can be brittle in production. The paper does not explore robustness to prompt variations or noisy instructions.\n\n5. Comparisons to adjacent time series agent benchmarks and live decision settings are brief. Positioning against systems that test forward looking planning or interactive tools would help define the boundary of the proposed benchmark."}, "questions": {"value": "1. Can you provide the training curves related to RL part?\n\n2. What is the effect of the reward design choices? Do different changes alter stability during reinforcement learning?\n\n3. How sensitive is performance to the number and quality of verified chain of thought trajectories used in supervised fine tuning? can you provide a scaling plot and a minimum viable count that preserves most of the gains.\n\n4. How does TIMEOMNI 1 compare to recent live or interactive benchmarks such as FutureX and TSAIA in settings that require forward prediction with delayed feedback? What design choices would be needed to support interactive or tool based decision loops?\n\n[1] Zeng, Zhiyuan, et al. \"Futurex: An advanced live benchmark for llm agents in future prediction.\" arXiv preprint arXiv:2508.11987 (2025).\n\n[2] Ye, Wen, et al. \"When LLM Meets Time Series: Can LLMs Perform Multi-Step Time Series Reasoning and Inference.\" arXiv preprint arXiv:2509.01822 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4Eh5TJG6ZS", "forum": "kOIclg7muL", "replyto": "kOIclg7muL", "signatures": ["ICLR.cc/2026/Conference/Submission16901/Reviewer_2VxW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16901/Reviewer_2VxW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892049665, "cdate": 1761892049665, "tmdate": 1762926931921, "mdate": 1762926931921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TSR-Suite, a comprehensive time series reasoning dataset containing 23K+ QA pairs across four reasoning-critical tasks (scenario understanding, causality discovery, event-aware forecasting, and decision-making), and TIMEOMNI-1, a unified reasoning model trained via a two-stage curriculum (supervised fine-tuning followed by reinforcement learning). The work addresses important limitations of existing time series QA datasets by systematically designing tasks that genuinely require reasoning capabilities and ensuring context sufficiency. The hierarchical Chain-of-Thought annotation process is well-designed, and the experimental results demonstrate strong performance improvements over baseline models, particularly in causality discovery (64.0% vs 35.9% with GPT-4.1).\n\nHowever, the paper has several significant limitations that prevent it from meeting the ICLR acceptance threshold: (1) The model architecture is not clearly described, particularly how time series data is encoded and processed. The paper appears to use text-based serialization of numerical sequences, but this design choice is not explicitly justified or compared with embedding-based approaches. (2) The scalability concerns are not adequately addressed—the dataset appears limited to short sequences and low-dimensional data, with no evidence of handling long sequences (hundreds/thousands of time steps) or high-dimensional multivariate signals (dozens of dimensions). (3) Related work coverage is insufficient, missing important recent work such as ChatTime and ITFormer that directly address time series QA tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **Systematic Task Design**: The paper provides a principled approach to formulating reasoning-critical time series tasks through two design principles: requiring genuine reasoning and ensuring context sufficiency. The four tasks form a progressive pathway from perception to extrapolation to decision-making, which is well-motivated.\n* **High-Quality Dataset Construction**: TSR-Suite represents a substantial contribution with 23K+ curated samples. The hierarchical CoT annotation pipeline (LLM Analyzer → Human Reviewer → LLM Rewriter) is innovative and ensures both quality and scalability in data construction.\n* **Comprehensive Evaluation**: The experimental evaluation is thorough, covering both in-distribution and out-of-distribution generalization across diverse domains. The joint training experiments demonstrating mutual gains across capabilities provide valuable insights.\n* **Clear Problem Identification**: The analysis of limitations in existing datasets (Figure 1) is well-executed and provides strong motivation for the proposed work."}, "weaknesses": {"value": "### 1. Model Architecture Omission\n\n**Critical Issue**: The paper fails to clearly describe how time series data is processed and encoded into the language model. From the examples (e.g., sequences shown as `[1.2, 3.4, 5.6, ...]`), it appears that numerical time series are simply serialized as text strings and fed directly to the LLM. However, this design choice is never explicitly stated, justified, or compared with alternative approaches.\n\n**Why This Matters**:\n\n- **Reproducibility**: Without clear specification of input encoding, it is difficult to reproduce the results.\n- **Novelty Assessment**: It is unclear whether the approach introduces architectural innovations or simply relies on LLM text understanding capabilities.\n- **Comparison Baseline**: The choice of text serialization vs. embedding-based approaches (as used in OpenTSLM, ChatTS, etc.) should be explicitly discussed.\n\n**Specific Questions Unanswered**:\n\n- How are long sequences handled? Are they truncated, sampled, or aggregated?\n- What is the token budget for time series data vs. context?\n- How are multiple time series (Task 2) represented in the input?\n- Is there any preprocessing, normalization, or feature extraction?\n\n**Comparison with Related Work**:\n\n- **OpenTSLM** uses explicit architectural components (SoftPrompt or Flamingo-style Perceiver Resampler) for time series encoding.\n- **Time-LLM** uses PatchTST-style tokenization with reprogramming layers.\n- The current paper should justify why a simpler text-based approach is sufficient or preferred.\n\n### 2. Scalability and Efficiency Concerns\n\nThe paper does not address scalability limitations that are critical for practical deployment:\n\n**Sequence Length Limitations**:\n\n- All examples show short sequences (12-24 time steps).\n- There is no evidence of handling longer sequences (hundreds or thousands of steps), which are common in real-world applications (e.g., hourly data over years).\n- With text serialization, long sequences would consume substantial token budgets and face LLM context window limitations.\n\n**Dimensionality Limitations**:\n\n- The paper focuses on univariate time series (single-dimensional) or at most pairs of sequences.\n- Real-world applications often involve multivariate signals (e.g., 12-lead ECG with 12 dimensions, sensor arrays with 50+ channels).\n- No discussion of how high-dimensional multivariate time series would be handled.\n\n**Efficiency Concerns**:\n\n- Text serialization of numerical data is token-inefficient compared to learned embeddings.\n- For a sequence of length T, text serialization requires ~T tokens (one per value), whereas embedding-based approaches can compress this.\n- The computational cost of processing long text sequences through LLMs is quadratic in sequence length.\n\n**Missing Analysis**:\n\n- No ablation studies on sequence length or dimensionality.\n- No efficiency/computation cost analysis comparing text-based vs. embedding-based approaches.\n- No discussion of context window limitations or how they were handled.\n\n### 3. Dataset Scope Limitations\n\nWhile TSR-Suite is substantial in sample size (23K+), the scope appears limited:\n\n**Short Sequences**: Examples suggest sequences are typically short (12-24 steps). Real-world time series reasoning often requires much longer histories.\n\n**Low Dimensionality**: Tasks focus on univariate sequences or pairs. High-dimensional multivariate scenarios are not addressed.\n\n**Limited Complexity**: The chosen tasks (scenario understanding, causality discovery) may not fully capture the complexity of reasoning over diverse time series modalities (e.g., sensor fusion, multi-scale temporal patterns).\n\n**Missing Validation**: There is no evidence that the proposed approach would scale to:\n\n- Sequences with 1000+ time steps\n- Multivariate signals with 20+ dimensions\n- Complex sensor fusion scenarios\n\n### 4. Insufficient Related Work Coverage\n\nSeveral important recent works on time series QA and reasoning are not adequately discussed:\n\n**Missing Works**:\n\n- **ChatTime** (Ye et al., 2025): \"When LLM Meets Time Series: Can LLMs Perform Multi-Step Time Series Reasoning and Inference\" - directly addresses time series reasoning with LLMs, including multi-step reasoning capabilities.\n- **ITFormer** (and related work): Methods for bridging time series and natural language for multi-task temporal-textual QA should be compared.\n- **Time-R1** (Luo et al., 2025): While cited, the comparison could be more detailed, especially regarding reasoning frameworks.\n\n**Why This Matters**:\n\n- These works represent the current state-of-the-art in time series + LLM integration.\n- Without proper comparison, it is unclear what novel contributions TIMEOMNI-1 provides beyond existing methods.\n- The architectural choices (text-based vs. embedding-based) should be positioned relative to these alternatives.\n\n### 5. Technical Details Omitted\n\n**Missing Information**:\n\n- Input preprocessing pipeline (normalization, scaling, handling of missing values)\n- Tokenization strategy for numerical values (precision, formatting)\n- How task-specific prompts are constructed\n- Details of the reward function design (mentioned but not fully specified in main text)"}, "questions": {"value": "## Questions\n\n1. **Architecture Clarification**: Can the authors provide a clear description of how time series data is encoded and input to the model? Specifically:\n\n   - Is numerical data converted to text strings? If so, what is the exact format?\n   - Are there any learnable embeddings or encoders for time series, or is it purely text-based?\n   - How are multiple time series (e.g., Task 2) represented in the input prompt?\n2. **Scalability Validation**:\n\n   - Have the authors tested their approach on longer sequences (e.g., 500+ time steps)? What are the limitations?\n   - Can the approach handle multivariate time series with 10+ dimensions? How would this be encoded?\n   - What is the computational efficiency compared to embedding-based approaches (e.g., OpenTSLM)?\n3. **Design Choice Justification**:\n\n   - Why was text serialization chosen over learned embeddings? What are the trade-offs?\n   - How does this approach compare to methods like Time-LLM or OpenTSLM in terms of efficiency and effectiveness for long sequences?\n4. **Dataset Characteristics**:\n\n   - What are the typical sequence lengths in TSR-Suite? What is the distribution?\n   - Are there examples with sequences longer than 100 time steps? If not, why not?\n   - Why were only univariate or paired sequences chosen? Were multivariate scenarios considered?\n5. **Related Work Comparison**:\n\n   - How does TIMEOMNI-1 compare with ChatTime, which also addresses multi-step time series reasoning?\n   - What are the key differences from ITFormer and similar methods?\n   - Why should text-based serialization be preferred over architectural innovations like those in OpenTSLM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rMb5wawXqw", "forum": "kOIclg7muL", "replyto": "kOIclg7muL", "signatures": ["ICLR.cc/2026/Conference/Submission16901/Reviewer_XR2U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16901/Reviewer_XR2U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921894231, "cdate": 1761921894231, "tmdate": 1762926931361, "mdate": 1762926931361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a reasoning model for time series data. The authors develop a new dataset by augmenting existing time series data and train a model with SFT and RL to conduct multistep reasoning on a variety of tasks. The results show that this prior enables successful training and increased performance on the proposed dataset."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It's true that the majority of prior work focused more on evaluating models than training them to be better at these tasks\n- The ablations are appropriate and useful for understanding how the model works\n- The human-generated reasoning traces are a valuable contribution to the field"}, "weaknesses": {"value": "A serious weakness of the paper is that the authors only evaluate on the dataset they propose. It's not particularly surprising that training on a dataset improves in-distribution performance. I'd be more impressed if the authors showed that their model improves performance on third party benchmarks (including those they reference), or that it mitigates this \"overthinking\" problem that they motivate their dataset with. \n\nThe evaluation section and tables mention an \"OOD\" dataset, but it's not clear how this is constructed or in what sense it is out of distribution."}, "questions": {"value": "- \"It remains unclear which tasks genuinely demand reasoning capabilities over time series.\" I'm not sure what this means. Several recent works (including those you cite like Merrill et al 2024 and Cow et al. 2024) define tasks that require reasoning over time series. \n- Line 157: \"Many questions are overly simple and straightforward, where invoking reasoning leads to over-thinking\" and Line 185 \"QA-pairs must reward reasoning\". Why is it a problem that reasoning doesn't provide an increase in performance? Is this really an issue with the benchmark, or does it say more about the models themselves?  If the models are overthinking a reasonable, realistic question then my instinct would be to improve the models, not find some set of questions where this wasn't the case."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DAl8CTeErJ", "forum": "kOIclg7muL", "replyto": "kOIclg7muL", "signatures": ["ICLR.cc/2026/Conference/Submission16901/Reviewer_YkbD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16901/Reviewer_YkbD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942768965, "cdate": 1761942768965, "tmdate": 1762926930788, "mdate": 1762926930788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}