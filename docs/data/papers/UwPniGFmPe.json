{"id": "UwPniGFmPe", "number": 22719, "cdate": 1758334774767, "mdate": 1759896850825, "content": {"title": "Learning from Observational Outcomes: Toward Causally-Aligned Language Model Fine-Tuning", "abstract": "Large language models are being widely used across industries to generate text that contributes directly to key performance metrics, such as medication adherence in patient messaging and conversion rates in content generation. Pretrained models, however, often fall short when it comes to aligning with human preferences or optimizing for business objectives. As a result, fine-tuning with good-quality labeled data is essential to guide models to generate content that achieves better results. Controlled experiments, like A/B tests, can provide such data, but they are often expensive and come with significant engineering, logistical, and ethical challenges. Meanwhile, companies have access to a vast amount of historical (observational) data that remains underutilized. In this work, we study the challenges and opportunities of fine-tuning LLMs using observational data. We show that while observational outcomes can provide valuable supervision, directly fine-tuning models on such data can lead them to learn spurious correlations. We present empirical evidence of this issue using various real-world datasets and propose DeconfoundLM, a method that explicitly removes the effect of known confounders from reward signals. In simulation experiments, DeconfoundLM more accurately recovers causal relationships and mitigates failure modes of methods that assume counterfactual invariance, achieving over 16% higher objective score than ODIN and other baselines, when entangled confounding is present.", "tldr": "", "keywords": ["Machine Learning for Decision Making", "Causality in Science", "Large Language Models (LLMs)", "Reward Modeling", "Causal Inference"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e710e846043a6bd3b31a46e3c07dd899c0526603.pdf", "supplementary_material": "/attachment/9ead385f656b1163084afddc2e4151bcdfce6c9b.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses the challenge of using large language models (LLMs) to optimise a specific task objective (reward) when only historical observational data are available for post-training. In such settings, distribution shifts between the observational data and the deployment environment—often caused by confounding variables (e.g. seasonality)—can bias the learned reward signal. As a result, directly fine-tuning the LLM on this observational data risks reinforcing spurious correlations rather than genuine causal relationships. To mitigate this, the authors propose a method to estimate a “deconfounded” reward function using an instrumental variable (IV) approach. This procedure aims to remove the influence of confounding factors and recover the causal effect of model actions on the true reward. The LLM is then (I believe) trained using this deconfounded reward estimate, rather than the reward observed in the historical dataset. Through empirical evaluation on a semi-synthetic dataset, the authors demonstrate that this IV-based correction leads to improved downstream performance and more robust generalisation to the true deployment environment."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Using observational data for the purpose of LLM training and fine-tuning is an important problem with many real world applications.\n- The authors provide many datasets and examples where training on observational data could lead to performance benefits."}, "weaknesses": {"value": "1. **Lack of details:** My biggest issue with this paper is that the proposed method and experiments (particularly in Section 4) are not described in details sufficient to fully understand the contributions of this work, evaluate their validity and ensure the reproducibility of results. Most importantly:\n    - What causal assumptions are imposed on the set of confounders for DefoncoundLM to work? (See also the question below)\n    - What does DeconfoundLM really entail? How exactly is the contribution rom the observed outcomes “removed” (l. 320)?\n    - After it has been removed, why happens next? Is the “confounder-adjusted reward” used as a reward function for RL?\n    - If RL is used for post-training, what kind of optimisation algorithm is used? PPO? GRPO?\n    - Does the method involve training a linear probe on the LLM embeddings? If yes, what is the architecture of this probe? If no, how is the reward predicted in Table 2?\n    - For the baselines described in l. 383, how are the baselines exactly constructed and what loss was used for training?\nI also provide additional questions in the ‘Question’ section below. I consider providing exhaustive answers to these questions a necessary step towards improving the paper, without which it is very difficult for me to evaluate the quality of this work.\n2. **Limited novelty:** Results in Section 3 seem to validate the standard principles of the causal modelling: if you do not account for confounding in your modelling framework, your results will learn spurious correlations that will not generalise to the randomised (rather than observational) setting. The results in this paper seem to validate that this is also the case when fine-tuning language models (or training regression/classification heads on their embeddings), showing that LLMs follow the training dynamic imposed by the non-causal loss function, as expected. While this validation is nice, it provides limited novel insight or contributions.\n3. **Limited evaluations:** The proposed method is only evaluated on a single semi-synthetic dataset. More dataset, with different relationships between confounders and the outcome function, would be necessary to further see the level of improvement the method can provide, and further illuminate under which circumstances it provides most benefits."}, "questions": {"value": "- In line 255 you say claim that ‘the results emphasise the need to scale regularisation appropriately with model capacity to maintain generalisation’. However, in your results in Figure 5b, isn’t it the case that $\\lambda=10000$ provides best performance across all model sizes? If yes, where is the above conclusion coming from?\n- In the paragraph opening section 4, I think there is some confusion around the notation. In particular, what is the relationship between the auxiliary features $\\tilde{\\mathbf{X}}_i$, the observed features $\\tilde{\\mathbf{F}}_i$ and the confounders $\\mathbf{C}_i$? Are they overlapping or disjoint sets? A causal diagram would be very helpful in understanding the relationships between different variables. Further, to make this estimation problem well-defined, it would be great to state what exact assumptions are imposed on the set of confounders and the general observational data. For example, is the set of observed confounders sufficient to disentangle the effect of the textual “action” on the outcome Y (unconfoundedness)? Answering this question rigorously and in detail would also make it clearer under which circumstances we can expect most gains from using this method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LXqdT7MD2U", "forum": "UwPniGFmPe", "replyto": "UwPniGFmPe", "signatures": ["ICLR.cc/2026/Conference/Submission22719/Reviewer_tWw4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22719/Reviewer_tWw4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566362471, "cdate": 1761566362471, "tmdate": 1762942356167, "mdate": 1762942356167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles how to fine-tune language models on observational data (like clicks or engagement) without letting them overfit to spurious confounders such as timing or popularity. It proposes DeconfoundLM, a method that tries to isolate the causal effect of the model’s text on outcomes and train on that signal, and shows in controlled experiments that this can outperform standard RLHF/DPO-style approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Importance of the problem: leveraging observational data is important, and taking into account confounders is even more crucial based on this context.\n\nEmpirical evidence for the problem tackled: the stackexchange and Upworthy experiments illustrate the potential of the LLM for learning spurious correlations from observational data. The Upworthy study also shows a nice insight: larger LMs overfit more strongly to confounded observational signals and require much heavier regularization."}, "weaknesses": {"value": "Observed confounders: the method assumes access to known confounders and corrects for them, but the paper doesn’t clearly specify  how these confounders are encoded in practice with DeconfoundLM, or how robust the method is to missing/mismeasured confounders.\n\nVioloation of the IV assumption: the method relies on an IV-style correction and assumes the IV only affects reward via popularity in the example in 4.1. However, the exclusion restriction is quite strong and deserved more discussion.\n\nHPT tuning: regularization strength is chosen using experimental ground truth CTR, not purely observational data. This gives the observational models access to oracle feedback that wouldn’t exist in a purely observational setting.\n\nExperimental setup: the method is validated using only one synthetic experiment, which is not sufficient. Furthermore, this synthetic setup is designed to satisfy the assumptions that the authors make (additive structure, exclusion,...) More experiments are required to illustrate the benefits of the method, for example training on real world observational data and evaluating on held-out A/B tests.\n\nLack of details: it is not clear how to plug the paper's method into RLHF/DPO pipelines. Should we deconfound before training the reward model, then do standard RLHF? Should we modify DPO-style objectives?"}, "questions": {"value": "See weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pbqvgBPkq7", "forum": "UwPniGFmPe", "replyto": "UwPniGFmPe", "signatures": ["ICLR.cc/2026/Conference/Submission22719/Reviewer_2hgS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22719/Reviewer_2hgS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761577803780, "cdate": 1761577803780, "tmdate": 1762942355820, "mdate": 1762942355820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the risk of values of fine-tuning LLMs with historical observational data. It proposes a novel fine-tuning method, called DeconfoundLM, the debias the influence of observed confounders from the reward signals.\n\nThe paper provided well-controlled experiments to demonstrate the pitfalls and potentials  in SFT and DPO setting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The experimental design is clear and interesting to demonstrate the influence of latent confounders on fine-tuning.\n- The proposed DeconfoundLM combined rigorous methods from causal inference to debias the influence from confounders."}, "weaknesses": {"value": "- The paper is quite compact, and some important contents are not presented in the main body. This makes it difficult to have a detailed review. Please refer to the question part."}, "questions": {"value": "- How does the proposed DeconfoundLM actually work? Readers may expecting a set of detailed equations with concrete examples. In addition, it would be much better to provide an algorithm box.\n- What is the background behind the MIND dataset? and how to compare and interpret the *W*, *C*, and *E* metrics (are they defined?) in Table 1 and 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ogxOyFPVQL", "forum": "UwPniGFmPe", "replyto": "UwPniGFmPe", "signatures": ["ICLR.cc/2026/Conference/Submission22719/Reviewer_FfU6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22719/Reviewer_FfU6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724143315, "cdate": 1761724143315, "tmdate": 1762942355376, "mdate": 1762942355376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}