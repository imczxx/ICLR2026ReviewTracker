{"id": "4xqi0zfC9i", "number": 22427, "cdate": 1758330894652, "mdate": 1759896867009, "content": {"title": "Trading Complexity for Expressivity: Theoretical Exploration of Linear and Causal Token Mixing Strategies", "abstract": "We revisit token mixing in sequence models through a unified, causal linear framework that separates two effects: (i) direct one-step influence of inputs on outputs and (ii) recurrent propagation of information through past outputs. This perspective encompasses major architectures -- including attention, state-space models, and hybrids -- while exposing simple design parameters that govern efficiency and expressivity. We show that every causal linear mixer can be written in this form, where computation reduces to solving a triangular system with well-understood numerical properties.\n    The framework generalizes the recurrence equations of SSMs and linear attention by allowing each state to depend on multiple past states rather than only the immediate predecessor. This unlocks new tradeoffs between decoding speed, cache size, and ability to model long-range dependencies.\n    Building on this view, we design structured recurrence patterns that provably achieve any desired complexity -- trading runtime for expressivity in a principled way. Together, these results provide a unified toolkit for understanding and designing efficient, expressive token mixers across model families.", "tldr": "", "keywords": ["token mixing", "linear recurrence"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dd9a0f83bab633489eaa3d10354fce1218862bf6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "1. **Framework**: The paper introduces a framework to study sequence mixers that decomposes the mixer into two components: (i) one step influence, and (ii) past outputs influence. \n\n    This is general enough to capture Attention and recurrent models like Mamba-2. With some simple math, the sequence mixer is equivalent to: $\\mathbf{y} =  (\\mathbf{I}-\\mathbf{B})^{-1}\\mathbf{A}\\mathbf{x}$, where $\\mathbf{B}$ is lower triangular matrix that encodes the locality/connections between tokens.\n\n2. **Metrics**: They introduce two metrics, which they show are equivalent, namely Shortest Information path and Cogestion to quantify the distance between two interacting tokens, or equivalently the information flow through any token. They claim that the metrics capture a trade-off between expressivity and complexity in sequence mixers.\n\n3. **Experiments**: The paper considers the following baselines: Attention, Local Attention, Local Recurrence, Quadratically-separated mixer, Exponentially-separated mixer and evaluate on (i) Language Modeling (ii) Copy (iii) Associative Recall (iv) Multi-hope recall"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is an easy read and is well written.\n2. The framework is simple, interesting and novel. \n3. The metrics chosen to quantify connected-ness/congestion are interesting and novel."}, "weaknesses": {"value": "### Cache-Efficient Version\n\n1. The proposed cache-efficient decoding modifies the recurrence pattern at inference is hacky and takes the model out of its training distribution. \n2. The paper lacks thorough experiments evaluating whether this OOD inference behavior affects generalization. This needs to be validated across larger model scales (e.g., 750M–1.3B) and multiple downstream tasks.\n\n### Language Modeling Experiments\n\n1. The chosen context length of 512 is shorter than standard practice 2048-4096. This possibly limits the evaluation of long range dependencies.\n2. The authors do NOT mention the model size. I estimate it to be around 6*(12*512*512) + 50257*512 = 44M. which is too small to draw meaningful conclusions. Results at larger scales (>= 350M) are needed to make credible claims.\n3. For instance, authors report a 4-5 PPL gap between Transformers and SSD-like (one step recurrent) models, which is inconsistent with known results that SSD-like models are competitive with transformers. This could possibly be due to (i) insufficient tuning of hyperparameters like LR (ii) scale of the models chosen (iii) parameterization  chosen for A,B; authors must sweep amongst the best known parameterizations (e.g. SSD) (iv) Proper normalization and initialization of the new architectures (which authors left as future work). These sweeps need to be done to make the claim that “adding more connections improves performance”, which at this moment is not well supported.\n\n### Synthetic Experiments\n\n1. The issues (iii,iv) pointed out in (3) also apply to the synthetic experiments.\n2. There are only minor gains in recall experiments. Given that $f(i)=i^2+1$ has a constant distance of 4 between any two tokens, I would expect almost 100% performance in recall.\n3. It would be interesting to see how the models behave when the sequence length for the synthetics is scaled up.\n\n### Lack of efficient kernels/latency evaluation\nI understand that the authors mention that this is beyond the scope of this paper, but providing an efficient code for testing is important. I would recommend that the authors look into fast CUDA inverse solvers and report latency."}, "questions": {"value": "1. It is unclear why the authors chose to train on 13B tokens for a 44M model. According to the Chinchilla scaling law, an optimal compute allocation would suggest training on roughly 900M?\n\n2. Could the authors do a short comparison of their work against [1]? Both this work and Chimera use an inverse-operator for token mixing, but (i) have different motivations to derive this, and (ii) pursue different claims; Chimera generalizes SSMs across domains whereas this work focuses on improving LM capabilities. \n\n---\n\n[1]:  Chimera: State Space Models Beyond Sequences. Aakash Lahoti, Tanya Marwah, Ratish Puduppully, Albert Gu"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uXxLqGa6d8", "forum": "4xqi0zfC9i", "replyto": "4xqi0zfC9i", "signatures": ["ICLR.cc/2026/Conference/Submission22427/Reviewer_Bkak"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22427/Reviewer_Bkak"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880315019, "cdate": 1761880315019, "tmdate": 1762942216183, "mdate": 1762942216183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a unified operator-theoretic view of causal linear token mixing in sequence models. The authors claim that any causal linear mixer can be written as  $y=(I−B)^{−1}Ax$ where A represents direct input mixing and B represents recurrent structure. This yields a framework for reasoning about the trade-off between computational complexity and expressivity. Finally, the paper provides an empirical study to illustrate their results."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is generally well-written, apart from some inconsitencies between sections (see weaknesses). The paper also investigates an interesting theoretical angle regarding expressivity and computational complexity. Finally, the empirical results are interesting, but it is hard to judge them given the framing of the paper."}, "weaknesses": {"value": "The paper presents a theoretical framework to investigate expressivity vs. computational complexity, however this theoretical framework does either not capture any existing token mixing strategies (attention, SSMs, modern RNNs) or is entirely redundant (as explained below). Unfortunately, I'm unable to tell which of these two options is the case as the definition of equation (1) is ill-defined.\nConventially, a state-space model is written as two equations (state dynamics and output equation):\n$ x_t = A_t x_{t-1} + B_t u_t,  y_t = C_t x_t $, with y the output and u the input.\nEquation (1) is formulated differently, i.e., $ y = A x + B y $, with y the stacked outputs and x the stacked inputs (I'm aware that above state space equation is elementwise and this is stacked, we will come back to this in a second). If we take this at face value, this framework does only encompass existing token mixing strategies (attention, SSMs, modern RNNs) iff B=0 always, as no SSM feeds back its output in a layer (this is done in LSTMs, but not in any SSM). Therefore, any characterization of SSMs in the paper and especially in Appendix B is wrong. This is apparent in [1,2] that unify attention and SSMs in one framework and in the line of fast weight programmer (DeltaNet) work [3,4]. The other option is that eq. (1) is meant to model the state dynamics directly, i.e., would correspond to $x_t = A_t x_{t-1} + B_t u_t$, albeit in stacked form. If this is the case, the reformulation $x = (I - B)^{-1}Au$ is trivial and well-known, as it only rolls out the elementwise form $x_t = A_t x_{t-1} + B_t u_t$ into a stack form. In control theory this is the difference between open-loop and closed-loop, see e.g. [5,6], with the latter doing this roll-out explicitly in the appendix. In SSM research, this has also been noted multiple times, e.g. [2,7]. Therefore, the framework either does not accurately capture existing SSMs or the framework reduces directly to the standard formulation, which in both cases render the subsequent results questionable or irrelevant.\n\nThe results shown in Sec. 4, make no use of the framework, e.g., Prop. 4.1. makes use of function f, which is not defined and not connected to the dynamics (i.e. A, B) at all. Additionally, there is no discussion how these results should feature in the model design.\nGiven this divide, the connection of Sec. 4 to the main theory in Sec. 3 is unclear and therefore the empirical results are hard to assess.\n\n[1] Dao & Gu (2024), \"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\", https://arxiv.org/abs/2405.21060\n\n[2] Sieber et al. (2024), \"Understanding the differences in foundation models: Attention, state space models, and recurrent neural networks\", https://proceedings.neurips.cc/paper_files/paper/2024/file/f271a36160097fbdb06a9adeb1605343-Paper-Conference.pdf\n\n[3] Schlag et al. (2021), \"Linear Transformers Are Secretly Fast Weight Programmers\", https://proceedings.mlr.press/v139/schlag21a/schlag21a.pdf\n\n[4] Yang et al. (2024), \"Gated Delta Networks: Improving Mamba2 with Delta Rule\", https://arxiv.org/abs/2412.06464\n\n[5] Anderson et al. (2019), \"System Level Synthesis\", https://arxiv.org/abs/1904.01634\n\n[6] Sieber et al. (2021), \"A System Level Approach to Tube-based Model Predictive Control\", https://arxiv.org/abs/2103.02460\n\n[7] Gu et al. (2022), \"Efficiently Modeling Long Sequences with Structured State Spaces\", https://arxiv.org/abs/2111.00396"}, "questions": {"value": "1. How are SSMs, e.g. Mamba-2, represented in the framework you present?\n2. How does eq. (1) relate to the framework proposed in [2]?\n3. In case, you are adding B as a new design parameter, i.e., adding some additional recurrences (this would partly alliviate my main point above) similar to LSTMs: How does this effect the implementation of your approach? Would it suffer the same drawbacks as LSTMs, which are inherently sequential in nature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QLN565qdiH", "forum": "4xqi0zfC9i", "replyto": "4xqi0zfC9i", "signatures": ["ICLR.cc/2026/Conference/Submission22427/Reviewer_sHBW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22427/Reviewer_sHBW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916610710, "cdate": 1761916610710, "tmdate": 1762942214624, "mdate": 1762942214624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a unified view of causal linear models in which any layer can be written as $y=(I−B)^{-1}Ax$. Specifically, it considers $A$ as an input mixer and $B$ as \"multi-hop\" propagation. The authors then proceed with designing simple and translation-invariant sparsity patterns on $B$ via a function $f$, which depends only on the position of tokens. \nWith that, the authors show a tradeoff between time and memory cache, and then propose add a cache-efficient variant to keep memory under the same cost as the time complexity. \nExperiments with small language models and synthetic recall tasks showcase the impact of different pattens induced by $f$."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper gives a neat linear view of causal token mixing, $y = (I - B)^{-1}Ax$. This can be a useful view to reason about the cost vs. expressivity tradeoff.\n\n- By parameterizing recurrence with a simple position-dependent function $f$, the paper shows how to trade per-token time/cache for path length and congestion.\n\n- The cache-efficient variant is interesting and can have practical impacts: it seeks to keep the \"working set\" small while maximizing \"reachability\" properties.\n\n- Even if small, the synthetic experiments do test exactly the properties the theory talks about such as multi-hop recall and congestion."}, "weaknesses": {"value": "- The framework proposed in this paper assumes that information will propagate along the induced fixed pattern from $f$ (the model is only given those edges), and thus the model is not actually \"forced\" to use all of them. That is, in practice, the network could fail to route information through the intended paths (e.g., sinking to just a few of them), which makes the \"reachability\" argument weaker empirically. I believe this can also make training harder or lead to suboptimal results. Further experiments with different model types (e.g., with SSMs, linear recurrent models, recurrent models, sparse transformers) would definitely strength the paper's main claims and allow readers to take more insightful conclusions.\n\n- The complexity gains come from a fixed translation-invariant pattern. Of course, a learned or content-based sparsity would be more flexible but I guess that would break the clean theory and the possible tradeoffs.\n\n- As noted by the authors, to actually leverage the time/cache cost induced by $f$, one needs specialized kernels that exploit the sparse pattern. It is not clear from the paper whether this can be done with existing attention implementations (e.g., FlexAttention) or whether each choice of $f$ would require its own kernel.\n\n- I think the empirical section is too small given the ambition of the method. It covers tiny models (e.g., GPT-2), no long-context benchmarks, no comparison to strong linear recurrent baselines, and, perhaps more importantly, no wall-clock or memory benchmarks. As of now, it's hard to tell if the ideas in this paper can actually improve current sequential models."}, "questions": {"value": "- Can you provide empirical evidence (e.g., via gradient/path tracing) that, during training, information about far tokens is in fact routed through the paths prescribed by $f$?\n\n- What happens if the task's \"true dependencies\" do not align with the fixed pattern? \n\n- Can the proposed mixers be implemented efficiently using existing flexible-attention frameworks (e.g. FlexAttention)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5AGL0mvdQy", "forum": "4xqi0zfC9i", "replyto": "4xqi0zfC9i", "signatures": ["ICLR.cc/2026/Conference/Submission22427/Reviewer_tbQc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22427/Reviewer_tbQc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996990328, "cdate": 1761996990328, "tmdate": 1762942214402, "mdate": 1762942214402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a theory connecting recurrant architectures (e.g. RNNs) with input \"instantaneous\" token-mixing architectures (e.g. Transformers). The framework is based on the following observation. Let $x$ be the input $y$ by the output, and consider the linear equation expressing $i$th output embedding as a function of the first $i$ input token embeddings and $i-1$ output token embeddings,\n\n - $y_i = \\sum_{j=1}^i \\alpha_{ij}x_j + \\sum_{j=1}^{i-1} \\beta_{ij}y_j$,\n \nConsider stacking the embeddings to obtain matrices $X$, $Y$, and ordering the $\\alpha,\\beta$ coefficient into triangular matrices $A,B$ in the natural way. Then the previous equation is $Y = AX + BY$ and, since $B$ has a zero-diagonal, one can rearrange $Y = (I-B)^{-1}AX$.\n\nFrom this observation, the authors theoretically examine different attention patterns, which I think correspond to different sparsity patterns on A and B. The authors examine how many recurrences (repeat applications of $(I-B)^{-1}A$?) it takes for a given token to affect another one. They present bounds on this \"mixing\" in terms of the time complexity for generating the $n$ token. The effect of caching is explored as well.\n\nThe paper also includes experiments on synthetic data on well-known tasks in the interpretability literature: copying the input, copying a part of the input corresponding to a given key, and a bounded recursion of this task."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- A clear model of deep NNs which places RNNs and transformers on tuneable spectrum is very appealing. As in any theoretical paper, the model in this paper makes simplifying assumptions (linearity) on deep NNs; nonetheless, if it were able to provide insightful theorems which then lead to compelling experimental results, this paper would be a significant contribution to the community.\n- I have not seen the $Y= (I-B)^{-1}AX$ decomposition before, at least not in this form. In this regard the paper is original."}, "weaknesses": {"value": "Despite significant effort, I was not able to parse most of the theoretical claims made in the paper. I suggest the authors look at examples of theoretical literature (e.g. the book of Shalev-Shwartz and Ben-David) for examples of what it means for a theory to be mathematically sound. It could be simply that I (a theoretical computer scientist) am not the taret audience for this paper; I acknowledge that NeurIPS has multiple theory communities in it, and I am giving this review a low confidence score to account for this event.\n\n- With theoretical work, it is crucial to clearly explain what the assumptions being made are, for all the results (should) rely on these assumptions. These assumptions are typically given in a \"Definition\" environment containing formal mathematics. These were missing from Section 3.\n- A and B do not explicitly appear in any of the propositions or corollaries. It would be helpful to explain how Section 4 relates to Section 3, because right now (despite trying) I could not understand the formal connection.\n- Is Proposition 4.2 actually a definition? Currently, equation (3) therein appears to claim that the \"distance\" between (i,j) is equal to the expression on the right hand side. However, it is not clear what the _left hand side_ is referring to. Is there some graph or other metric space to which $d$ is referring to? What is a path between tokens?\n- Similarly, in section 4.1.2 $\\mathcal{G}$ is not defined and I do not know what it is.\n- Since the domain of $f$ is finite length strings over natural numbers (which is the standard interpretation of $\\mathcal{N}^*$), none of its uses are well defined. What is $t - f(0)$ when $f(0)$ is a string of natural numbers?\n- I could not understand Section 4.2\n\nIn particular, I would suggest move Section 3.2 into an \"extended related work\" appendix, it seems to be an advanced discussion of comparison to specific literature. Instead, for the benefit of most readers who aren't specialists in this line of work, use this valuable space early in the paper to formally define the objects used in the claims that follow. In addition, I suggest including concrete examples early on, before presenting a general theory (e.g. present the case of $f(i) = 2^i$ and concretely explain what ML setting this captures, and what your theorem is saying about this setting.)"}, "questions": {"value": "- What is a path between tokens?\n- What is the distance in Equation 3?\n- How is the graph $\\mathcal{G}$ formally defined?\n- What is the formal definition of \"the cache efficient pattern induced by $f$\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Sdmd4a0dcC", "forum": "4xqi0zfC9i", "replyto": "4xqi0zfC9i", "signatures": ["ICLR.cc/2026/Conference/Submission22427/Reviewer_zhwP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22427/Reviewer_zhwP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997825723, "cdate": 1761997825723, "tmdate": 1762942214116, "mdate": 1762942214116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}