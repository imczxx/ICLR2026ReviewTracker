{"id": "tXHoLHkm8O", "number": 23384, "cdate": 1758342974544, "mdate": 1759896817684, "content": {"title": "CCKS: Cooperative CPU-GPU Scheduling for Fused Kernels on Coherent Architectures", "abstract": "Executing modern ML workloads as sequences of discrete GPU kernels leads to significant hardware underutilization because of kernel launch, data movement, and CPU-GPU synchronization overheads. Recent advancements in kernel fusion reduce small kernel launch overhead by consolidating many small kernels into a single, persistent kernel. However, existing fusion techniques delegate complex scheduling logic to the GPU itself—a task for which its architecture is ill-suited. This on-GPU scheduling creates critical inefficiencies, as its control-intensive, synchronization-heavy logic is fundamentally mismatched with the GPU's parallel microarchitecture, and leads to stalled threads during synchronization, and high-overhead collection of global state.\n\nWe propose CCKS (Cooperative Coherent Kernel Scheduler), a novel framework that leverages tightly-integrated, cache-coherent CPU-GPU architectures such as the NVIDIA Grace Hopper Superchip for fused kernel scheduling. CCKS offloads the scheduling of fused kernels to the host CPU, treating it as a dedicated co-processor. In our design, the GPU's role is simplified to that of an efficient information provider and decision executor. This division of labor is enabled by a near-zero overhead, cache-coherent interface that exposes GPU runtime state and allows the CPU to make and propagate scheduling decisions asynchronously concurrently. To facilitate our approach, we introduce an innovative programming framework that automatically generates the requisite CPU scheduler and GPU code from a high-level description. Our evaluation shows that CCKS achieves up to 20% performance improvement over state-of-the-art kernel fusion frameworks on representative ML workloads.", "tldr": "Kernel fusion is bottlenecked by on-GPU scheduling. We offload scheduling to a tightly integrated CPU \"co-pilot\" using cache-coherent links, significantly improving kernel fusion performance.", "keywords": ["Kernel Fusion", "GPU Scheduling", "Systems for ML", "GPU Archiecture"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9600fc58060f1d6934534d5deb76e65ad0377cc7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents CCKS, a framework for fused kernel scheduling on tightly coupled CPU/GPU systems, like Grace Hopper."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Impressive engineering, and great results on a challenging problem to improve performance on shared-memory systems."}, "weaknesses": {"value": "- Can this technique generalize beyond Nvidia Grace Hopper?\n- How much of the benefit is truly due to CCKS, versus the underlying shared-memory paradigm of Grace Hopper?"}, "questions": {"value": "At a high level, I like this idea, and I believe the authors did a solid job explaining the challenges and presenting their solution. My main question is, \"Isn't something like this supposed to already exist on Grace Hopper?\" The fact that it doesn't (if it doesn't) highlights that this is important work. However, under the hood, I'm struggling to understand if the speed up is indeed due to CCKS's improved scheduler, or the fact that Grace Hopper is the first, real, shared memory system between CPU/GPU? By virtue of having such direct shared memory, how much benefit is CCKS providing to the end user, versus the CUDA compiler eventually enabling CCKS's ideas?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SwCG5q7jOb", "forum": "tXHoLHkm8O", "replyto": "tXHoLHkm8O", "signatures": ["ICLR.cc/2026/Conference/Submission23384/Reviewer_S4m5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23384/Reviewer_S4m5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998431673, "cdate": 1761998431673, "tmdate": 1762942637414, "mdate": 1762942637414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper leverages the cache coherence in modern CPU–GPU systems such as the NVIDIA Grace Hopper Superchip to develop a cache-coherent kernel scheduling framework (CCKS). The framework introduces three techniques: speculative enqueue, batch commit, and CPU bypass, to improve scheduling efficiency. CCKS is integrated into two existing LLM inference systems, Pod-Attention and Mirage."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The motivation is clearly articulated and grounded in practical limitations of existing on-GPU scheduling.\n\nThe paper is generally well written and easy to follow.\n\nThe proposed optimisation techniques are simple yet effective. For instance, in speculative enqueue, when the speculation is incorrect, the committed task can be simply overwritten by the correct task."}, "weaknesses": {"value": "- Grace Hopper is no longer NVIDIA’s latest GPU architecture. It would be good to discuss whether CCKS remains applicable to newer generations such as Blackwell, and what hardware assumptions (e.g., cache coherence model or interconnect behaviour) are required.\n\n- The integration details of CCKS within Pod-Attention and Mirage are limited. More implementation specifics on how speculative enqueue, batch commit, and CPU bypass are realized within these frameworks would improve clarity.\n\n- The paper does not explicitly discuss characteristics of ML workloads that make them particularly suited to CCKS. The current design appears general to GPU workloads. Please elaborate on why ML inference workloads especially benefit from the proposed mechanisms, or what properties (e.g., kernel granularity, dependency patterns) motivate this focus.\n\n- From a venue-fit perspective, this paper is primarily a systems contribution aimed at improving runtime efficiency rather than a study of representation learning. Its relevance to ICLR may therefore be a question."}, "questions": {"value": "The paper states that “traditional inference systems for LLMs execute a sequence of discrete GPU kernels to perform computation.” Could the authors provide a more concrete example illustrating how LLM inference generates a sequence of small kernels, which makes the associated launch and teardown overheads become non-negligible?\n\nWhen mentioning that “each kernel launch incurs a setup and teardown cost,” it may strengthen the presentation to explicitly link the setup phase to the enqueueing process on the GPU command queue, which is one of the key motivations for the proposed speculative enqueue mechanism.\n\nIn speculative enqueue, when a speculation is incorrect, the committed task is overwritten by the correct one. Are there any negative side effects (e.g., wasted memory traffic, resource contention, or timing delays) from frequent mis-speculations? A discussion or quantitative measurement would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PyGhsKYNuR", "forum": "tXHoLHkm8O", "replyto": "tXHoLHkm8O", "signatures": ["ICLR.cc/2026/Conference/Submission23384/Reviewer_iDsr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23384/Reviewer_iDsr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762036142775, "cdate": 1762036142775, "tmdate": 1762942636906, "mdate": 1762942636906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new scheduling framework, CCKS, for fused persistent GPU kernels that leverages emerging cache-coherent CPU–GPU architectures such as NVIDIA’s Grace Hopper with low latency high bandwidth links. Recent fused kernel systems execute scheduling logic on the GPU that cause inefficiency due to control-heavy, serial scheduling running on SIMT hardware. CCKS instead offloads this scheduling to the CPU, with optimizations such as speculations and scheduling bypass to make this more efficient."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Tackles an important and challenging problem \n* An interesting un-intuitive approach to offload scheduling to the CPU \n* Reasonable approach that uses speculation and CPU bypass when needed\n* The paper is well motivated\n* Significant speedups over baseline approaches"}, "weaknesses": {"value": "* The approach relies on a ultra-low latency, high-bandwidth, cache-coherent interconnect between the CPU and GPU. It would be good to see what the impact of latency is and when this approach becomes feasible. \n* While the proposed approach is quite interesting, it does add a lot of scheduling complexity and non-determinism in performance to the scheduling pipeline\n* Somewhat narrow in applicability, as this would be only useful when using fused persistent kernels. \n* Some prior works missing, e.g., \"ACE: Efficient GPU Kernel Concurrency for Input-Dependent Irregular Computational Graphs\", Durvasula et al., PACT 2024\n* There are many typos and grammatical errors in the paper. Please fix them."}, "questions": {"value": "Please comment on/address weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ux3zGrauco", "forum": "tXHoLHkm8O", "replyto": "tXHoLHkm8O", "signatures": ["ICLR.cc/2026/Conference/Submission23384/Reviewer_D16m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23384/Reviewer_D16m"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762116861409, "cdate": 1762116861409, "tmdate": 1762942636643, "mdate": 1762942636643, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for scheduling work on the GPU from the CPU exploiting the tight coupling on NVIDIA's Hopper for this purpose."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is relatively easy to read."}, "weaknesses": {"value": "The paper appears to me to not accurately describe the key related works it sets out to build upon.  To me it appears neither Wu et al., 2025 or Spector et al., 2024 employ on GPU scheduling (which makes little sense).  My understanding is the scheduling in those papers done offline by the compiler when generating the kernels before they are run.  Also, I didn't see mention of persistent kernels in the OSDI paper from Wu et al., 2025.   Similarly, the statement \"The core innovation enabling kernel fusion is an on-GPU scheduler\" seems inaccurate or needs some clarification.  Kernel fusion can be done statically before running the code.\n\nThe optimizations in Figure 4 and 5 are not explained in nearly sufficient detail to understand where the supposed benefits are coming from.   Partly, that can be blamed on the format of ICLR which has a very limited page budget.  More details could have been provided in an appendix.  Or better, yet, submit to a systems conference where you get twice as much space in the main text.  At a systems conference I would expect a more thorough explanation (with data) of the source of the problem being tackled. \n\nLine 303: \"Speculative enqueue: The CPU scheduler speculatively prepares task data structures and copies\nthem into GPU memory queues in advance, even when prior GPU tasks are still running.\"  -- GPUs already do this kind of thing since the first CUDA enable GPUs.   The whole point of async memcpy and streams is to allow the CPU to load up work into a ring buffer queue of tasks that are read in by the GPU as the GPU completes work.  GPUs work this way for graphics as well (not just compute).  \n\nI don't see mention of CUDA graphs, which seems related.\n\nTypos: \"imporve fused kernel\", \"tighyly integrated\""}, "questions": {"value": "Is code available?  I didn't see any supplemental materials or links.  \n\nThis is a systems paper, which seems a bit outside the normal scope for ICLR.  To my judgment this paper would likely get rejected at flagship systems conferences that are all interested in work on ML, so why should it be published at ICLR?  \n\nWhere in Wu et al. (2025) [Mirage paper at OSDI 2025] is there a description that matches the following text from this submission \"This model designates one or more SMs to act exclusively as schedulers Wu et al. (2025).\"?  The words \"designate\" and \"scheduler\" do not seem to appear in the OSDI 2025 paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QVY2gsHU5s", "forum": "tXHoLHkm8O", "replyto": "tXHoLHkm8O", "signatures": ["ICLR.cc/2026/Conference/Submission23384/Reviewer_yXoY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23384/Reviewer_yXoY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23384/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763103190877, "cdate": 1763103190877, "tmdate": 1763103190877, "mdate": 1763103190877, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}