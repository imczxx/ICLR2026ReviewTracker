{"id": "hrnSqERgPn", "number": 10504, "cdate": 1758174151901, "mdate": 1759897647016, "content": {"title": "THE PATH OF LEAST RESISTANCE: GUIDING LLM REASONING TRAJECTORIES WITH PREFIX CONSENSUS", "abstract": "Large language models achieve strong reasoning performance, but inference strategies such as Self-Consistency (SC) are computationally expensive, as they fully expand all reasoning traces. We introduce PoLR (Path of Least Resistance), the first inference-time method to leverage prefix self-consistency for compute-efficient reasoning. PoLR clusters short prefixes of reasoning traces, identifies the dominant cluster, and expands only a subset of promising paths, preserving the accuracy benefits of SC while substantially reducing token usage and latency. Our theoretical analysis, framed via mutual information and entropy, explains why early reasoning steps encode strong signals predictive of final correctness. Empirically, PoLR consistently matches or exceeds SC across GSM8K, Math500, AIME 2024/2025, and GPQA-Diamond, reducing token usage by up to 60% and wall-clock latency by up to 50%. Moreover, PoLR is fully complementary to adaptive inference methods (e.g., Adaptive Consistency, Early-Stopping SC) and can serve as a drop-in pre-filter, making SC substantially more efficient and scalable without requiring model fine-tuning.", "tldr": "", "keywords": ["Speculative reasoning", "LLM inference optimization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b2b30642396b5d8e1d423fcf05476cf22aa4c5f0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an inference method called PoLR to address the issue of high computational costs in the inference process of LLM. Traditional Self-Consistency decoding requires sampling and expanding multiple complete reasoning paths, which improves accuracy but incurs significant computational overhead. The core idea of PoLR is to utilize prefix consistency during inference: first, generate short prefixes of multiple reasoning paths, cluster them, and only expand the paths in the main clusters, thereby significantly reducing ineffective computations. This method does not require retraining the model and can be used as a plug-in during the inference stage. The main contributions of the paper include: proposing the first method that leverages prefix consistency to optimize self-consistency decoding in the inference stage, reducing the number of generated tokens by up to approximately 60% and inference latency by about 50% without compromising accuracy. Additionally, the authors explain through theoretical analysis from an information-theoretic perspective why early reasoning prefixes contain strong signals for predicting the final correct answer, and verify the effectiveness of the PoLR method on mathematical, common sense, and scientific reasoning benchmarks through extensive experiments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper introduces a highly novel inference strategy. Unlike existing work that either requires model fine-tuning or adaptively halts sampling based on answer agreement, PoLR introduces clustering based on early-stage reasoning prefixes. This is the first method to leverage prefix consistency at inference time for self-consistency decoding. The proposed idea is distinct from prior approaches and presents a fresh, efficient angle for improving LLM inference.\n\nThe theoretical analysis is solid and well-motivated. Using mutual information and conditional entropy, the paper shows that prefix clusters carry predictive signals about final answer correctness. It introduces the notion of cluster skew and formally proves that higher skew leads to greater token efficiency. The paper successfully connects theory and practice, and the theoretical results help explain the method’s robustness and efficiency.\n\nThe experiments are comprehensive and convincing. PoLR is evaluated on GSM8K, MATH500, AIME24/25, and GPQA-DIAMOND, covering a broad spectrum of reasoning difficulty. It is tested on LLMs ranging from 1.5B to 32B parameters and across different architectures and training paradigms. Results consistently show 40–60% token savings without accuracy degradation—sometimes even improving accuracy. PoLR is also shown to complement adaptive methods, offering additional savings. Results are averaged over 10 runs with reported standard deviations, ensuring statistical reliability. This section is a strong point of the paper."}, "weaknesses": {"value": "Limitations of the method's applicability: There is a high degree of consensus in the early steps of PoLR hypothesis reasoning, meaning that the prefixes of most sampling paths converge to the main pattern. However, this assumption may not be fully valid for certain tasks. If the problem-solving paths of a question are highly divergent or the reasoning path corresponding to the correct answer is not the mainstream pattern, PoLR may miss important reasoning branches, thereby affecting accuracy.\n\nDependency on hyperparameters and clustering strategies: PoLR needs to pre-set the prefix length Lp and the methods for embedding and clustering prefixes. The optimal prefix truncation length and clustering method may vary across different tasks. If the prefix is too short, it may fail to capture sufficient reasoning signals; if it is too long, it may introduce irrelevant information and affect the clustering effect. In addition, simple word frequency embedding may not cover diverse expressions with semantic equivalence. Although the authors tried dense vector embedding and pointed out that its gain is limited and the overhead is large, the parameter selection in the clustering step itself still needs to be carefully adjusted in practical applications to ensure robustness."}, "questions": {"value": "Generality to Non-Math Tasks: Most evaluations are on math, STEM, and QA-style reasoning. Have you tested PoLR on more open-ended tasks where prefix similarity may be weaker?\n\nVoting within Noisy Clusters: When the dominant cluster includes low-quality traces, SC’s  majority voting might fail. Has the team considered weighting cluster votes by internal consistency or agreement confidence?\n\nFailure Cases: While PoLR generally preserves accuracy, there are cases with slight degradation. Can the authors provide qualitative analysis or examples to understand what types of reasoning problems lead to this drop?\n\nCluster Selection Robustness: PoLR selects the dominant cluster for expansion. In settings where reasoning traces diverge meaningfully, how sensitive is performance to this choice? Did you try expanding the top-k clusters instead of only the largest one?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "aBMJVEfyOq", "forum": "hrnSqERgPn", "replyto": "hrnSqERgPn", "signatures": ["ICLR.cc/2026/Conference/Submission10504/Reviewer_jggQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10504/Reviewer_jggQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10504/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761523914450, "cdate": 1761523914450, "tmdate": 1762921791378, "mdate": 1762921791378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PoLR (Path of Least Resistance), an inference-time method that improves the efficiency of reasoning in large language models (LLMs) without sacrificing accuracy. The key idea is to exploit prefix consistency, i.e., the observation that early reasoning steps (prefixes) across different reasoning traces tend to converge before diverging."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper builds on a compelling empirical finding: early reasoning steps in LLMs already encode signals predictive of correctness. This observation is both intuitively appealing and empirically validated, where clustering short prefixes achieves nearly identical accuracy to full SC.\n2. The paper notes some accuracy drops (especially on AIME24/25) but doesn’t deeply analyze why PoLR fails there. Qualitative error analyses or visualizations of cluster distributions could clarify how prefix diversity correlates with performance loss."}, "weaknesses": {"value": "1. Uses simple TF–IDF clustering, which may miss deeper semantic similarities between reasoning traces.\n\n2. Accuracy drops on some datasets (e.g., AIME24/25) are not fully explored or explained.\n\n3. Mutual information argument is mostly qualitative and lacks empirical validation.\n\n4. Clustering approach may not scale efficiently when many samples (large N) are generated.\n\n5. Some sections are dense, with mixed implementation and theoretical details; figures could be clearer."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "C4GyMLSBSf", "forum": "hrnSqERgPn", "replyto": "hrnSqERgPn", "signatures": ["ICLR.cc/2026/Conference/Submission10504/Reviewer_jah9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10504/Reviewer_jah9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10504/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917914730, "cdate": 1761917914730, "tmdate": 1762921789918, "mdate": 1762921789918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose PoLR, a method to reduce the computational cost of SC decoding in LLMs. Instead of expanding all sampled reasoning paths, PoLR clusters short prefixes and expands only those in the dominant cluster. The method is shown to preserve accuracy while drastically reducing token usage and latency."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper conducts comprehensive experiments to demonstrate the effectiveness of the proposed method. The main results show that it consistently outperforms existing approaches.\n\n2. The idea is both novel and compelling, and PoLR is straightforward to implement and fully compatible with existing language models."}, "weaknesses": {"value": "I don't have many comments regarding the weaknesses; however, the primary issue is that the paper is not well written and is difficult to follow. For instance, I had to consult referenced papers to understand prerequisite concepts such as the definition of a prefix and the detailed observations related to prefix consistency. It would be better if the paper were more self-contained. Nonetheless, given the strong experimental results, I lean toward a positive assessment of the paper."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LW40Rit5oK", "forum": "hrnSqERgPn", "replyto": "hrnSqERgPn", "signatures": ["ICLR.cc/2026/Conference/Submission10504/Reviewer_yLoa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10504/Reviewer_yLoa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10504/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931381390, "cdate": 1761931381390, "tmdate": 1762921789374, "mdate": 1762921789374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PoLR, an inference-time optimization that tackles the high computational cost of Self-Consistency (SC). PoLR generates short reasoning prefixes, clusters them to find a consensus, and only extends the paths from the dominant cluster. This approach significantly reduces token usage and latency on complex reasoning tasks while maintaining or even improving accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Innovative & Practical: Cleverly solves the high cost of SC by using prefix consensus to filter paths early, a novel and practical approach.\n- High Efficiency: Achieves impressive results, reducing token/latency costs by ~50% without sacrificing accuracy.\n- Well-Supported: Backed by solid theoretical analysis (information theory, structural skew) and extensive experiments across multiple models and benchmarks.\n- Plug-and-Play: A simple, training-free method that is easy to implement and complements other optimization techniques."}, "weaknesses": {"value": "- While the MI and skew analyses are insightful, they are not empirically validated. Quantitative measures linking $I(Z;Y)$ to observed behavior are missing.\n- Cluster-Dependent: The method's success hinges on identifying the correct dominant cluster; it could potentially filter out correct but less common reasoning paths.\n- Limited on Certain Tasks: Shows weaker performance on tasks with low lexical overlap (e.g., GPQA-DIAMOND), where the prefix consistency signal is faint.\n- Hyperparameter Discussion: Could benefit from a deeper analysis of the clustering method choice and TF-IDF configurations."}, "questions": {"value": "1. If the dominant cluster leads to incorrect reasoning, does PoLR amplify “high-confidence errors”? Could multi-cluster expansion or confidence estimation help?\n2. Could the prefix length be adjusted dynamically via sampling temperature for adaptive balance between easy and hard problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cvdo9xJUnS", "forum": "hrnSqERgPn", "replyto": "hrnSqERgPn", "signatures": ["ICLR.cc/2026/Conference/Submission10504/Reviewer_w1dY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10504/Reviewer_w1dY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10504/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998007026, "cdate": 1761998007026, "tmdate": 1762921788486, "mdate": 1762921788486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}