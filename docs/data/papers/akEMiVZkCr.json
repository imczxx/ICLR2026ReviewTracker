{"id": "akEMiVZkCr", "number": 14081, "cdate": 1758228150893, "mdate": 1759897391364, "content": {"title": "Uncertainty-Aware Gradient Descent via Online Bootstrapping", "abstract": "Standard gradient descent methods yield point estimates with no measure of confidence. This limitation is acute in overparameterized and low-data regimes, where models have many parameters relative to available data and can easily overfit. Bootstrapping is a classical statistical framework for uncertainty estimation based on resampling, but naively applying it to deep learning is impractical: it requires training many replicas, produces post-hoc estimates that cannot guide learning, and implicitly assumes comparable optima across runs—an assumption that fails in non-convex landscapes. We introduce Twin-Bootstrap Gradient Descent, a resampling-based training procedure that integrates uncertainty estimation into optimization. Two identical models are trained in parallel on independent bootstrap samples, and a periodic mean-reset keeps both trajectories in the same basin so that their divergence reflects local (within-basin) uncertainty. During training, we use this estimate to sample weights in an adaptive, data-driven way, providing regularization that favors flatter solutions. In deep neural networks and complex high-dimensional inverse problems, the approach improves calibration and generalization and yields interpretable uncertainty maps.", "tldr": "We make bootstrapping practical for deep learning by training two \"twin\" models whose divergence provides a live uncertainty signal that regularizes the training process to find more robust solutions.", "keywords": ["Optimization; Bootstrapping; Model Uncertainty; Gradient Descent; Regularization; Deep Learning; Calibration;"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/433cb83bf877801666e6315f36f2c3197b2994b8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Twin-Bootstrap Gradient Descent, a way to integrate classical bootstrapping into optimization. Tested on datasets like cifar-10"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Originality: Novel integration of bootstrapping into optimization loop (online vs post-hoc)\n\n- Quality: Thoughtful experiments to validate the different mechanisms\n\n- Clarity: Well-motivated problem, clearly written paper\n\n- Significance: Addresses important calibration problem & more efficient than full bootstrapping"}, "weaknesses": {"value": "- Insufficient experiments: only CIFAR-10/VGG-16, which are very small scale for modern CV. At minimum would need imagenet scale and bigger models, at least ResNets or vision transformers. \n\n- Missing Baselines: missing MC Dropout, SWAG, variational inference, temperature scaling, \n\n- CIFAR-10 underperformance: 79.46% seems very low. VGG models can get 90+ scores, so does this harm performance \n\n- Limited ablations: more ablations would be nice to better understand the mechanisms\n\n- Insights: No characterization when Twin-Boot helps vs fails - why good on seismic but modest on CIFAR-10?"}, "questions": {"value": "- Larger scale: Have you tested on ImageNet size datasets and bigger models ResNets, Transformers? Is there a specific reason why this can’t scale\n\n- Baseline comparisons: please can you add the aforementioned baselines\n\n- Low performance: why is cifar-10 so low?\n\n- When does it help: Can you characterize problem properties where Twin-Boot excels? Like where does it do well and where is it weak?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8OwVEuQY3I", "forum": "akEMiVZkCr", "replyto": "akEMiVZkCr", "signatures": ["ICLR.cc/2026/Conference/Submission14081/Reviewer_Eo6i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14081/Reviewer_Eo6i"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761600775551, "cdate": 1761600775551, "tmdate": 1762924560225, "mdate": 1762924560225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Twin-Boot, a dual-network optimization scheme where two models are trained in parallel on independent mini-batches; their divergence approximates bootstrap-style uncertainty, and periodic mean-resets keep them in the same basin. This uncertainty is then used to inject adaptive noise into gradient descent. Experiments on toy tasks, CIFAR-10, and a seismic inversion example show moderate gains."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Simple, clearly described algorithm combining ensemble variance with uncertainty-aware regularization.\n\nIntuitive visualization of uncertainty evolution; clean toy experiments demonstrate behavior.\n\nGood connection to the bootstrap idea and to uncertainty calibration literature."}, "weaknesses": {"value": "1. Conceptual confusion around “uncertainty.”\n\nThe central framing—estimating uncertainty during training—is ill-defined.\nThe difference between two evolving parameter sets does not represent epistemic or predictive uncertainty in the Bayesian or ensemble sense; it merely reflects optimization instability.\nBootstrapping assumes independent fits on fixed data; here both models share the same data distribution and correlated stochastic gradients.\nThus, the quantity being measured is better interpreted as gradient variance or local training noise, not genuine uncertainty.\nThis makes the “uncertainty-aware” label misleading—the method is a form of noise-scaled regularization, not uncertainty quantification.\n\n2. Very similar to existing two-network and teacher-student frameworks.\n\nTwin-Boot’s structure closely mirrors well-known methods designed for learning under noisy or unstable supervision, where two models are kept partially synchronized to stabilize updates:\n\nMean Teacher (Tarvainen & Valpola, 2017) — maintains a student and an EMA teacher; both are kept close to filter noise in targets.\n\nCo-teaching / DivideMix (Han et al., 2018; Li et al., 2020) — two networks train each other on data they believe to be clean, relying on disagreement as a signal of uncertainty.\n\nBootstrapped DQN (Osband et al., 2016) — parallel learners estimate epistemic uncertainty via disagreement.\n\nSWA / SAM (Izmailov et al., 2018; Foret et al., 2021) — use weight averaging or perturbation to smooth optimization.\nThe mean-reset in Twin-Boot plays almost the same stabilizing role as the EMA in Mean Teacher, except the paper reinterprets this stabilization term as “bootstrap consistency.” The resemblance to these prior approaches is so strong that the claimed novelty—“online bootstrapping for uncertainty-aware optimization”—is largely a change in terminology, not mechanism.\n\n3. Theoretical claims are heuristic.\n\nTreating the twin divergence as a bootstrap variance has no formal grounding; both networks are correlated and do not correspond to independent resamples. No analysis links this divergence to predictive calibration or Bayesian posterior spread.\n\n4. Experimental validation is limited.\n\nExperiments are small-scale and exclude the most relevant baselines (SWAG, SGLD, SAM, or teacher-student noise-robust training). Reported gains (~0.5–1%) do not justify doubling computation, and no calibration or robustness metrics are included despite the uncertainty claim."}, "questions": {"value": "Meaning of “uncertainty.”\nWhat exactly does the twin divergence represent? Is it epistemic, gradient variance, or just optimization noise? Without a clear probabilistic interpretation, the term uncertainty feels misplaced.\n\nBootstrap justification.\nSince both models share the same data and correlated gradients, how is this a valid bootstrap estimator? Show evidence that twin variance correlates with predictive uncertainty or calibration metrics.\n\nRelation to teacher–student frameworks.\nThe setup and mean-reset are strikingly similar to Mean Teacher and co-training methods for noisy or semi-supervised learning. How is this different beyond terminology?\n\nAblation and mechanism.\nWhat part of the method actually improves performance—the dual setup, mean-reset, or noise scaling? Provide controlled comparisons.\n\nMissing baselines.\nWhy exclude direct competitors like SWAG, SGLD, or SAM, which already handle noise or uncertainty during optimization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JQqacUxCns", "forum": "akEMiVZkCr", "replyto": "akEMiVZkCr", "signatures": ["ICLR.cc/2026/Conference/Submission14081/Reviewer_u8E2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14081/Reviewer_u8E2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762145940453, "cdate": 1762145940453, "tmdate": 1762924559533, "mdate": 1762924559533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript addresses the limitation of standard SGD, which only provides point estimates of model parameters without corresponding uncertainty quantification. Borrowing ideas from bootstrapping, the authors introduce a twin-bootstrap strategy to integrate uncertainty estimation into the optimization process through resampling. This method successfully identifies more robust and flatter solutions. The effectiveness of the method is demonstrated through experiments on both toy datasets and the CIFAR-10 benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This manuscript presents a novel model learning and optimization framework capable of performing parameter uncertainty estimation and utilizing this information to guide the optimization process. A key theoretical innovation lies in its reformulation of classical bootstrapping as an online two-sample estimator. This conceptual shift effectively turns what is typically a post-hoc analysis into an actionable signal that directly informs the optimization trajectory."}, "weaknesses": {"value": "The manuscript currently contains several issues regarding the formulation of equations and definition of symbols that require refinement. Key experimental settings are not clearly specified, which hinders the reproducibility of the reported results. Furthermore, the theoretically claimed capabilities—such as favoring flatter solutions and improving generalization—are not sufficiently validated in the experimental section. More rigorous ablation studies and comparative analyses are needed to substantiate these claims."}, "questions": {"value": "1.In Section 3.1, the letter 'l' is used to denote multiple distinct variables, leading to notational ambiguity.\n\n2.The equations in the manuscript are currently unnumbered, which hinders cross-referencing and discussion. Furthermore, the meaning of the superscript in Line 152 appears inconsistent with the subscript notation used in the equation on Line 148, creating ambiguity.\n\n3.Line 206, it is stated that per-unit grouping yields identical results to other methods. However, it would be valuable to discuss its potential disadvantages compared to alternatives like layer-wise grouping. The authors are also encouraged to offer practical advice for future researchers on selecting grouping strategies.\n\n4.The selection of the hyperparameter K appears empirical. We recommend conducting an ablation study to systematically evaluate its impact on final performance.\n\n5.What kind of noise are investigated in section 4.1.2?\n\n6.For the experiments in Section 4.2, the sizes of the subsets D*_1 and D*_2 should be specified. Please provide the rationale for the chosen sizes and clarify whether any pre-trained models were utilized in this section. \n\n7.The manuscript does not sufficiently demonstrate the advantages of the proposed method over a naive ensemble of two independent models. A comparative analysis is needed to justify the choice of the proposed framework over this simpler baseline.\n\n8.The theoretical potential of the proposed method to escape local minima and converge towards a global optimum should be discussed, with supporting evidence from the empirical results or relevant literature.   \n\n9.The claimed generalization ability is not conclusively demonstrated by the experimental results. More rigorous evidence, such as testing on more diverse or challenging benchmarks, is suggested to substantiate this claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qnVWt85Xfl", "forum": "akEMiVZkCr", "replyto": "akEMiVZkCr", "signatures": ["ICLR.cc/2026/Conference/Submission14081/Reviewer_EHLs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14081/Reviewer_EHLs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762163254303, "cdate": 1762163254303, "tmdate": 1762924558933, "mdate": 1762924558933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}