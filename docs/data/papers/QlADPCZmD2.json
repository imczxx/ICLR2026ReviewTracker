{"id": "QlADPCZmD2", "number": 4422, "cdate": 1757678137771, "mdate": 1759898033388, "content": {"title": "CrossSparse-MoE: Adaptive Sparsity and Cross-Channel Expert Routing for Time Series Forecasting", "abstract": "Time series forecasting under limited data remains challenging due to model overfitting and insufficient structural regularization. In this work, we uncover a sparsity-oriented scaling phenomenon: as training data increases, model parameters naturally become sparser—even in simple linear models. This observation motivates the introduction of learned sparsity as an effective prior to improve model generalization under data-scarce regimes. We propose CrossSparse-MoE, a lightweight forecasting framework that enhances model expressiveness while promoting adaptive sparsity. Built upon a linear backbone, CrossSparse-MoE incorporates cross-channel convolutions to capture short-term inter-variable dependencies and employs a Mixture-of-Experts (MoE) module with non-linear MLPs. A learnable gating network dynamically routes temporal segments to specialized experts, while L1 regularization encourages parameter sparsity without imposing rigid structural constraints. Extensive experiments on multiple benchmarks demonstrate that CrossSparse-MoE consistently outperforms state-of-the-art baselines, particularly in low-data scenarios, validating the effectiveness of combining structural flexibility with learned sparsity.  \nCode is available in Appendix.", "tldr": "", "keywords": ["Time Series Forecasting"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/711814c414c98a48e0018d2db44bf671ba6131c9.pdf", "supplementary_material": "/attachment/1b74ba9ee71197171b4286da66942f426c9a3d23.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces CrossSparse-MoE, a lightweight time series forecasting framework that combines cross-channel convolutional embeddings with a Mixture-of-Experts architecture. It identifies a sparsity-oriented scaling law showing that model parameters become naturally sparser as training data increases. The method leverages L1 regularization to promote adaptive sparsity, improving generalization in low-data regimes. Extensive experiments across ten benchmarks demonstrate state-of-the-art performance and high computational efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a observation of a sparsity-oriented scaling law in time series forecasting, offering a fresh perspective on data-driven regularization. The proposed CrossSparse-MoE framework effectively combines adaptive sparsity with modular expert routing, achieving strong results with high efficiency. The work is clearly written, and demonstrates consistent improvements across diverse benchmarks, highlighting both its technical quality and practical relevance."}, "weaknesses": {"value": "The main limitation lies in the theoretical grounding of the proposed sparsity-oriented scaling law, which is described heuristically without formal derivation or quantitative fitting. The architectural novelty is moderate, as CrossSparse-MoE builds upon established concepts from MoLE and Time-MoE with added L1 regularization. The ablation analysis could be expanded to study the number of experts, gating behavior, and α interpolation dynamics. Finally, robustness experiments under distribution shifts or noise perturbations would strengthen the claim that adaptive sparsity improves generalization."}, "questions": {"value": "1. Can the authors provide quantitative evidence for the proposed sparsity-oriented scaling law, such as fitting the parameters ($C_1$, $C_2$, $\\beta$ ) in Eq. (3) or showing trends across datasets?\n\n2. How sensitive is the model to the number of experts and the gating network capacity? Would larger expert pools or deeper gates improve performance or cause overfitting?\n\n3. Could the authors analyze expert specialization, for instance by clustering input patterns or reporting activation diversity, to better support claims of modularity and interpretability?\n\n4. How does CrossSparse-MoE perform under domain shift or noisy sensor conditions, where sparsity might enhance robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WTHou1JSU7", "forum": "QlADPCZmD2", "replyto": "QlADPCZmD2", "signatures": ["ICLR.cc/2026/Conference/Submission4422/Reviewer_ZpDZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4422/Reviewer_ZpDZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761670210370, "cdate": 1761670210370, "tmdate": 1762917356858, "mdate": 1762917356858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CrossSparse-MoE, a time series forecasting model that combines adaptive sparsity and MoE routing. It is based on a sparsity-oriented scaling law, showing that model sparsity naturally increases with more data. The model uses cross-channel embeddings and temporal expert routing with L1 regularization to improve generalization under limited data. Experiments show state-of-the-art accuracy and efficiency across several benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation, from the perspective of parameter sparsity, is quite interesting."}, "weaknesses": {"value": "1. The Introduction is not written rigorously. In line 38, it is stated that most existing works rely on high-quality labeled datasets, which is not accurate. In time series forecasting, training data usually comes from real-world time series, where part of the sequence is used as input and another part as the label. Therefore, there is no issue of label quality. In addition, in the second paragraph (lines 41–42), the cited papers mainly aim to improve model efficiency, not robustness under limited data.\n\n2. The preliminary study on parameter sparsity is not convincing enough. The analysis mainly focuses on linear models, lacking experiments or discussions on more complex architectures such as Transformer-based, CNN-based, or MLP-based models.\n\n3. The paper lacks novelty, as the MoE framework has already been widely used in many existing works.\n\n4. The writing is not concise, and the readability is relatively poor. For example, the content in Section 4.1 reflects well-known domain knowledge and could be moved to the appendix. Furthermore, the connection between the preliminary study and the proposed method is not clearly established."}, "questions": {"value": "1. Could you explain why high-quality labeled datasets are often limited?\n\n2. Which figure does Figure 3.3 refer to?\n\n3. In Figure 3, what does the vertical axis represent? Why does RLinear drop significantly while iTransformer shows almost no change?\n\n4. How are the conclusions from the preliminary study integrated into the MoE framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uizMesWFCJ", "forum": "QlADPCZmD2", "replyto": "QlADPCZmD2", "signatures": ["ICLR.cc/2026/Conference/Submission4422/Reviewer_izTy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4422/Reviewer_izTy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944216626, "cdate": 1761944216626, "tmdate": 1762917356453, "mdate": 1762917356453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CrossSparse-MoE, a lightweight time series forecasting framework designed to address overfitting in low-data regimes through dynamic parameter sparsity. The main csontribution in this paper is the identification of a empirical observation called \"sparsity-oriented scaling law\" where the model parameters naturally become sparser as training data increases, even without explicit regularization. Building on this insight, the authors propose a hybrid architecture combining: (1) cross-channel convolutional embeddings for inter-variable dependencies, and (2) a Mixture-of-Experts (MoE) module with L1 regularization to encourage adaptive sparsity. Experiments on several benchmark datasets demonstrate state-of-the-art performance, particularly in low-resource settings."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The main strengths of the paper include,\n1. The sparsity-oriented scaling law observation is interesting and well-documented across multiple datasets (ETTh2, Weather, Electricity, Traffic, etc.) and the authors provided clear evidence of increasing weight sparsity with training data volume in their plots.\n2. The proposed CS-MoE achieves best performance in several benchmark datasets compared against strong baselines (PatchTST, DLinear etc) while achieving super efficiency of just 3.27ms inference time vs competitors and having only 1.69M params (orders of magnitude smaller than baselines). \n3. The paper includes clear ablation studies for 1. Contribution of individual components of the CS-MoE framework. 2. Sensitivity of \\lambda parameter in L1 regularization."}, "weaknesses": {"value": "The main weakness of the paper include,\n1. The sparsity-oriented scaling law observation is only validated on linear projection layers of simple models (RLinear, iTransformer's linear components). No analysis is conducted for other layer types such as attention layers, non-linear MLP layers, or convolutional layers. This severely limits the applicability of the claimed \"scaling law\" to broader model architectures. The paper claims this is a general phenomenon but only demonstrates it for a specific layer type.\n2. The paper claims about state-of-the-art performance in few-shot settings of the proposed CS-MoE framework but it does not compare against recent pre-trained time series models (TimesFM, Chronos, Lag-Llama, Moment, etc.) which are specifically designed for low-data scenarios. So, this claim cannot be justified unless further evidence is provided. \n3. The paper claims to \"prevent model overfitting for sparse time series\" but provides no specific anecdotes (or) metrics for sparse/irregular time series."}, "questions": {"value": "1. What constitutes a \"sparse time series\" in your context? Can you provide metrics/anecdotes on datasets with missing values, irregular sampling, or high # of zero-values?\n2. What is the sensitivity to the number of experts? \n3. Can you validate the sparsity-oriented scaling law for other layer types apart from linear layers?\n4. Can you compare the benchmark with some of the pre-trained time series models as they are specifically designed for zero-shot scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9oo95dSGyw", "forum": "QlADPCZmD2", "replyto": "QlADPCZmD2", "signatures": ["ICLR.cc/2026/Conference/Submission4422/Reviewer_K7nC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4422/Reviewer_K7nC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976635874, "cdate": 1761976635874, "tmdate": 1762917356079, "mdate": 1762917356079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to explain why model performance improves after training on large amounts of data from the perspective of weight sparsity, which is quite insightful. Based on this insight, it designs a lightweight sparse model using L1 regularization, which shows good prediction accuracy and low model overhead on multiple datasets."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper attempts to explain why model performance improves after training on large amounts of data from the perspective of weight sparsity, which is quite insightful. Based on this insight, it designs a lightweight sparse model using L1 regularization, which shows good prediction accuracy and low model overhead on multiple datasets"}, "weaknesses": {"value": "1. It seems natural that after the model is trained on a larger amount of data, it learns to extract important patterns and reduce the weights of redundant features, making the learned patterns clearer. Similar patterns have also been mentioned in other papers. The connection to sparsity here seems a bit forced.\n\n2. The innovation is limited. The method in the paper is simply a combination of L1 regularization and multiple dense multi-predictor experts applied to the prediction task. The use of a mixture of experts is very similar to that used in MoLE.\n\n3. The experiments in the paper are not very convincing. For example, the ablation study was only conducted on the relatively small ETTh1 and Exchange datasets. Additionally, some designs, such as alpha, were not ablated.\n\n4. Figure 5 is not very clear. The MOE predictor does not correspond to the steps in Figure 5."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4COzSxAoTC", "forum": "QlADPCZmD2", "replyto": "QlADPCZmD2", "signatures": ["ICLR.cc/2026/Conference/Submission4422/Reviewer_wy7n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4422/Reviewer_wy7n"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991419764, "cdate": 1761991419764, "tmdate": 1762917354434, "mdate": 1762917354434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}