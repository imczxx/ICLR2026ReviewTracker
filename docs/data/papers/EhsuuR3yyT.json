{"id": "EhsuuR3yyT", "number": 7510, "cdate": 1758025487792, "mdate": 1763560241707, "content": {"title": "ChatAni: Language-Driven Multi-Actor Animation Generation  in Street Scenes", "abstract": "Generating interactive and realistic traffic participant animations from instructions is essential for autonomous driving simulations. Existing methods, however, fail to comprehensively address the diverse participants and their dynamic interactions in street scenes. In this paper, we present ChatAni, the first system capable of generating interactive, realistic, and controllable multi-actor animations based on language instructions. To produce fine-grained, realistic animations, ChatAni introduces two novel animators: PedAnimator, a unified multi-task animator that generates interaction-aware pedestrian animations under varying task plans, and VehAnimator, a kinematics-based policy that generates physically plausible vehicle animations. For precise control through complex language, ChatAni employs a multi-LLM-agent role-playing approach, using natural language to plan the trajectories and behaviors of different participants. Extensive experiments demonstrate that ChatAni can generate realistic street scenes with interacting vehicles and pedestrians, benefiting tasks like prediction and understanding. All related code, data, and checkpoints will be open-sourced.", "tldr": "", "keywords": ["Physics-based Character Animation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7b7a46bfef5e0c208def1ed678d04e806627c0b2.pdf", "supplementary_material": "/attachment/49b3730fe2433e2bd45d31b9432e513f3f819d73.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents ChatAni, a system that generates interactive, physically plausible animations of multi-agent traffic scenes (pedestrians and vehicles) from natural-language commands. It employs a hierarchical architecture: high-level scene planning and interaction design are handled by a multi-LLM-agent framework, while low-level execution is delegated to two specialized animators—PedAnimator, a physics-based unified controller for pedestrian interactions, and VehAnimator, a kinematic controller ensuring physically valid vehicle motions. Experiments demonstrate that ChatAni can synthesize complex interactive scenarios and that its synthetic data improves downstream autonomous-driving tasks such as prediction and scene understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tPedAnimator introduces a “body-masked AMP” strategy—adversarial motion priors with the body region occluded—that effectively reconciles the conflict between adhering to motion priors and accomplishing task-specific interactions.\n2.\tRather than stopping at visually pleasing animations, the paper goes further to demonstrate the value of its synthesized data in boosting the performance of autonomous-driving prediction and scene-understanding models.\n3.\tThe authors have constructed an end-to-end system that takes language as input and generates multi-actor animations as output."}, "weaknesses": {"value": "1.\tThe so-called “language-driven interaction” in the paper is, in reality, language-based classification and invocation of a fixed set of predefined interaction types. As described (“Multi-agent interaction tasks are trained based on predefined types, with the LLM classifying interaction behaviors, enabling the animator to execute specified interactions.”), the system cannot generalize to entirely new interaction categories unseen during training. If a user were to issue an instruction such as “one pedestrian trips and another pedestrian steps forward to help,” the current framework would be unable to handle it.\n2.\tDuring interactive-task training, PedAnimator replaces the interactee with a passive “Box,” so the model effectively learns one-way object manipulation rather than genuine bidirectional interaction. This mismatch undermines the paper’s stated goal of producing physically realistic “interactions.”\n3.\tVehAnimator is listed as a separate contribution; however, its core merely applies the standard bicycle kinematic model to smooth trajectories and does not constitute a research-level contribution. Most of the methods in PedAnimator, apart from the body-masked AMP, likewise lack novelty."}, "questions": {"value": "1.\tPlease clarify how the system would respond when confronted with a completely new interaction command that does not belong to any predefined type? Would it be necessary to redesign the reward function from scratch and retrain PedAnimator entirely? Does this imply that the system’s interaction capability is closed and non-extensible?\n2.\tPlease clarify the theoretical or experimental basis for the claimed generalization ability of training with passive “box” objects while testing against dynamic, complex “characters”? Might this simplification in the training lead the model to learn unrealistic shortcuts that produce unnatural behavior when confronted with real characters?\n3.\tIn the “vehicle-hit-pedestrian” scenario shown in Fig. 5, PedAnimator and VehAnimator are trained independently. How is the physical consequence of the collision transmitted and resolved between these two separate controllers? For instance, how is the vehicle’s momentum converted into an impact force on the pedestrian model? The current framework appears unable to handle such inter-agent contact dynamics in a physically consistent manner.\n4.\tThe planning evaluation involved only 10 users, and the animation evaluation involved 15 users. Are conclusions drawn from such small sample sizes reliable? Moreover, were the participants domain experts or members of the general public? What criteria were used to assess user preferences?\n5.\tIn the VLM scene-understanding task, the authors fine-tuned the model with only 100 frames of augmented data—an extremely small amount. The observed performance gain is more likely due to the model \"memorizing\" these specific hazardous scenes rather than a genuine improvement in generalization ability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BVQFM2J1fS", "forum": "EhsuuR3yyT", "replyto": "EhsuuR3yyT", "signatures": ["ICLR.cc/2026/Conference/Submission7510/Reviewer_bbNe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7510/Reviewer_bbNe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761610696494, "cdate": 1761610696494, "tmdate": 1762919619430, "mdate": 1762919619430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a system for generating traffic scenes that are interactive (through natural language) and realistic involving both high level and low level trajectory and animation of vehicles and pedestrians. the system comprises high level LLM agent (oracle), per actor agent and low level ped, and vehicle animators.  \nThe oracle splits the user instruction to per actor instructions and interaction groups. The actor agents then tap into the map and other actors in their interaction groups to set keypoints. The animators then realize the panned trajectory and behaviour."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(+) i see several novelties in this work: system level, i think this paper makes a stride towards a more holistic solution. work like \"Language Conditioned Traffic Generation\" did introduce natuaral language control but only of vehicles. This work extends it to pedestrians as well. On the other hand, works like trace and pace used physics based RL to correct for waypoint instructions but dealt only with pedestrian animation and had a policy per task. Aside for the system level improivement i identify methodological contribusion like using masking to unify policies; body masked AMP to allow naturl animation of non interacting body parts. to some extend the usage of a bicycle model to enforce plausible vehicle trajectories within a RL framework is also novel  in the context of language driven traffic control\n\n(+) interactivity in traffic scenario generation is critical. I like that the system handles it in a layered fashion. The oracle agent establishes high level interaction groups that capture intent -- these are planned and intended interactions. Then to handle unplanned interactions that can occur during inference liek collision, the obstacle aware collision avoidance kicks in.\n\n(+) impressive overall results, good supplementary video organization"}, "weaknesses": {"value": "(-) The system operates as a forward directed pipeline where the actor agents decide on the global trajectory before animation execution begins. while sensible, this lacks a crucial closed-loop feedback mechanism, meaning the system can only handle minor, local corrections but cannot re-generate a  new plan in response to unforeseen events,  or potential large deviations during the animation. see for example Trace and Pace where the physics based RL could feedback to the diffusion model planner as inference time guidance.\n\n(-) i'm concerned that the reliance on the proprietary model GPT-4 API would restrics scalebility due to cost, and reproducibility. Seeing this framework inplemented with Llama would have been great\n\n(-) The oracle agent operates purely on language inputs without explicit access to the scene's map. This risks generating high-level plans that are spatially implausible, shifting the entire burden of geometrical consistency onto the lower-level Actor Agents.\n\n(-) Limited use cases: the authors state that \"Multi-agent interaction tasks are trained based on predefined types\" like pushing and patting. This constraint contradicts the claim of rich, language-driven controllability, as the system would be unable to execute or adapt to novel physical interactions outside of its fixed training categories."}, "questions": {"value": "please refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mYE4L8S6h8", "forum": "EhsuuR3yyT", "replyto": "EhsuuR3yyT", "signatures": ["ICLR.cc/2026/Conference/Submission7510/Reviewer_bGam"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7510/Reviewer_bGam"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919096523, "cdate": 1761919096523, "tmdate": 1762919619060, "mdate": 1762919619060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ChatAni, a system designed to generate interactive and realistic traffic participant animations from natural-language instructions. The system integrates three key modules: a PedAnimator, a VehAnimator, and a Multi-LLM-Agent Role-Playing Framework. The system aims to produce controllable, multi-actor traffic animations that can serve applications such as autonomous driving simulation and behavior prediction.\n\nThe paper tackles an important and timely problem and demonstrates a well-structured and technically sound integration of several existing models. However, its novelty is unclear, many methodology descriptions are ambiguous, and results are insufficiently convincing in terms of realism, controllability, scalability, and repeatability. These concerns limit the contribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Timely and Relevant Problem:\n\nThis paper addresses the emerging challenge of language-driven multi-actor traffic animation, with clear relevance to autonomous driving and simulation research.\n\n\n2. Well-Structured Framework and Nice Integration:\n\nThe paper proposes an integrated system combining pedestrian and vehicle animators with multi-LLM-agent coordination, forming a coherent overall design. It effectively merges motion generation, control policy learning, and language-based planning to enable interactive and controllable scene synthesis.\n\n3. Practical Potential and Openness:\n\nThe paper demonstrates applicability to autonomous driving scenarios and plans to open-source code and data, enhancing the work’s potential impact."}, "weaknesses": {"value": "1. Clarity and Technical Contributions\n\nWhile the framework is well-motivated, the originality of the core techniques remains unclear. Many components appear to be adapted or directly taken from existing methods, yet the paper does not clearly indicate what has been modified or newly proposed. For example, the “Action Hierarchical Control” module sounds similar to the PULSE framework—was it directly adopted, or modified? If unchanged, this should be explicitly stated; if extended, the modifications and their impact should be clarified. Similar ambiguities appear throughout the system description, and the rationale behind some design choices is not well explained.\n\n2. Result Quality and Script Adherence\n\nThe result videos reveal issues that raise concerns about controllability and consistency with the input scripts. In the first example, a vehicle suddenly turns sharply to avoid a pedestrian—an unscripted and abnormal event not mentioned in the textual input. The script’s instruction, “a hurried vehicle changed lane,” either does not occur or occurs at the wrong time, suggesting that the generated sequence does not faithfully follow the script. Such discrepancies indicate limited user control and weak alignment between textual instructions and generated behaviors. While the inclusion of some unscripted events can enhance realism, they should remain subtle and should not overshadow the scripted events.\n\n3. Physical Plausibility\n\nDespite the claim of producing physically plausible motions, several results appear unrealistic:  In the first example, a pedestrian hit by a car immediately stands up—clearly non-physical behavior. In the second example, a vehicle overtaking a taxi makes an unnecessary rightward swerve before returning to its lane, producing unnatural motion. These observations suggest that the VehAnimator’s kinematic constraints or control mechanisms are not sufficient to ensure realism in all scenarios.\n\n4. Quantitative Accuracy and Repeatability\n\nThe paper does not discuss trajectory accuracy or repeatability. For instance: How accurately do generated trajectories follow specified positions, speeds, or timings if such quantitative details appear in the language instructions? Will the same textual script consistently produce the same animation? For autonomous driving applications—where scenario reproducibility and precision are critical—these issues significantly limit the system’s reliability and potential utility.\n\n5. Task Scope and System Coverage\n\nFrom Figure 3, it appears that PedAnimator is limited to three interaction types: pushing, patting, and arm-around-shoulder walking. If so, these assumptions should be explicitly stated and discussed. The paper demonstrates only three multi-actor scenarios, which are insufficient to validate the claimed generality. Showing a wider variety of traffic situations (e.g., crossing behaviors, group interactions, or complex multi-lane maneuvers) would strengthen the paper.\n\n6. Evaluation and Scalability\n\nThe evaluation based on collision rate (Section 4.2.2) may not be an appropriate measure of realism. Collisions can occur due to unrealistic conditions (e.g., extreme deceleration), so a low or high collision rate alone does not indicate quality or fidelity. Moreover, the demonstrations involve fewer than ten traffic participants, and the map structures appear simplistic. Realistic street scenes often contain dozens of vehicles and pedestrians, intersections, or roundabouts. The current system’s scalability—in both scene complexity and number of participants—is unclear and should be addressed.\n\n7. Related Work\n\nThe related work section overlooks prior simulation-based methods in multi-character and traffic animation. Claims such as\nLine 107–108: “these methods do not involve interaction behaviors between multi-pedestrians or pedestrian-vehicle”\nLine 116–118: “these works generally do not directly consider physical and kinematic constraints...”\nare inaccurate. Several existing works explicitly consider both interaction and physical constraints, such as:\n\nShadow Traffic: A Unified Model for Abnormal Traffic Behavior Simulation, Computers & Graphics, 2018\n\nGenerating Believable Mixed-Traffic Animation, IEEE T-ITS, 2015\n\nCity-Scale Traffic Animation Using Statistical Learning and Metamodel-Based Optimization, SIGGRAPH, 2017\n\nAlso, the term “traffic flow” should be avoided, as it typically refers to macroscopic traffic modeling, while this paper operates at a microscopic animation level.\n\n8. Additional Comments\n\nLine 282: The smoothness reward combines angular and positional terms with different units; appropriate weighting seems necessary.\n\nLine 390: The claim about “corner case simulation” lacks supporting examples.\n\nLine 583–584: The cited paper has now been published in ICLR 2024; please update the reference.\n\nIn the third demo video, the authors should indicate which characters are controlled by which inputs (e.g., MoCAP, T2M, or interaction targets), perhaps via color labels or annotations, to improve interpretability."}, "questions": {"value": "In the rebuttal, I would appreciate clarification on \n\n(1) what aspects of the proposed framework are technically novel beyond system integration, \n\n(2) how the ambiguous modules (e.g., hierarchical control) are implemented or modified from prior work, \n\n(3) how realism and controllability are quantitatively or qualitatively evaluated, \n\n(4) whether the system outputs are repeatable for identical input scripts, and (5) how the approach scales to more complex traffic scenes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Rl0Zv8VdKp", "forum": "EhsuuR3yyT", "replyto": "EhsuuR3yyT", "signatures": ["ICLR.cc/2026/Conference/Submission7510/Reviewer_PCmj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7510/Reviewer_PCmj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924502970, "cdate": 1761924502970, "tmdate": 1762919618692, "mdate": 1762919618692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ChatAni, a system designed to generate multi-agent traffic trajectories involving both pedestrians and vehicles. ChatAni leverages a large language model (LLM) to produce high-level commands and initializations for each participant. These commands are then passed to reinforcement learning (RL)-based low-level control policies that generate detailed motion behaviors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an interesting and relevant problem in multi-agent simulation and animation.\n\n2. The manuscript is clearly written and easy to follow."}, "weaknesses": {"value": "1. The technical novelty appears limited. The system seems to be a relatively straightforward integration of existing components: LLMs for high-level reasoning and AMP/RL-based controllers for low-level motion generation. Both of these components have been widely studied in the literature. The main contribution appears to be the composition of these modules into a traffic simulation framework rather than the introduction of a new algorithmic or conceptual advance.\n\n2. The scalability of the approach is unclear. The presented experiments involve only a small number of entities, leaving open questions about whether the system can handle large-scale scenarios with dozens or hundreds of agents.\n\n3. There are still some dirty, hard-coded components, such as the collision handling in appendix S4.3."}, "questions": {"value": "1. Could the authors clarify what differentiates ChatAni from prior systems that combine high-level planning (via LLMs) with low-level control (via RL)? What are the key technical insights or contributions that make this work novel?\n\n2. Have the authors attempted to scale the system to more complex and crowded environments, such as airport pickup zones or train stations, where random events like pullovers, loading/unloading, or emergent interactions occur?\n\n3. Can ChatAni generate or handle long-tail corner cases—such as a vehicle door opening unexpectedly and obstructing others? Demonstrating such robustness or diversity would significantly strengthen the empirical results and highlight the system’s potential beyond proof-of-concept integration."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EO9lhW1obv", "forum": "EhsuuR3yyT", "replyto": "EhsuuR3yyT", "signatures": ["ICLR.cc/2026/Conference/Submission7510/Reviewer_qBHn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7510/Reviewer_qBHn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967152904, "cdate": 1761967152904, "tmdate": 1762919618110, "mdate": 1762919618110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}