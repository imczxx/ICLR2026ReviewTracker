{"id": "weh7JpT1lF", "number": 19351, "cdate": 1758295555826, "mdate": 1759897043801, "content": {"title": "RestoRect: Degraded Image Restoration via Latent Rectified Flow & Feature Distillation", "abstract": "Current approaches for restoration of degraded images face a critical trade-off: high-performance models are too slow for practical use, while fast models produce poor results. Knowledge distillation transfers teacher knowledge to students, but existing static feature matching methods cannot capture how modern transformer architectures dynamically generate features. We propose 'RestoRect', a novel Latent Rectified Flow Feature Distillation method for restoring degraded images. We apply rectified flow to reformulate feature distillation as a generative process where students learn to synthesize teacher-quality features through learnable trajectories in latent space. Our framework combines Retinex theory for physics-based decomposition with learnable anisotropic diffusion constraints, and trigonometric color space polarization. We introduce a Feature Layer Extraction loss for robust knowledge transfer between different network architectures through cross-normalized transformer feature alignment with percentile-based outlier detection. RestoRect achieves better training stability, and faster convergence and inference while preserving restoration quality. We demonstrate superior results across 15 image restoration datasets, covering 4 tasks, on 8 metrics.", "tldr": "Degraded Image Restoration via Latent Rectified Flow \\& Feature Distillation", "keywords": ["Image Restoration", "Latent Rectified Flow", "Knowledge Distillation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ebb7e5183ade95b6f58f0cb318b6015674757a65.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes RestoRect, a novel framework that reformulates knowledge distillation as a generative rectified flow process. Instead of static feature matching, the student learns to synthesize teacher features through dynamic latent trajectories. The method also introduces FLEX loss (cross-normalized feature alignment) and integrates physical priors (Retinex, diffusion, HVI color). Experiments on 15 datasets show consistent gains over state-of-the-art models with fewer inference steps and better perceptual quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel concept: The paper introduces a fresh perspective by reframing knowledge distillation as a generative rectified flow process. This idea is original, conceptually sound, and well-motivated by recent advances in flow-based modeling.\n\nStrong empirical results: RestoRect consistently outperforms prior approaches across multiple datasets and metrics, while requiring significantly fewer inference steps — demonstrating both effectiveness and efficiency."}, "weaknesses": {"value": "Limited theoretical grounding: The connection between rectified flow and knowledge distillation is mostly intuitive; a more formal theoretical treatment would strengthen the paper.\n\nAblation depth: While ablations are provided, they do not fully disentangle the effects of each component (Rectified Flow, FLEX loss, Retinex prior).\n\nEfficiency analysis: The paper lacks concrete comparisons in terms of FLOPs, parameter counts, or runtime benchmarks to substantiate efficiency claims."}, "questions": {"value": "see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nmftk5YnBp", "forum": "weh7JpT1lF", "replyto": "weh7JpT1lF", "signatures": ["ICLR.cc/2026/Conference/Submission19351/Reviewer_MwNg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19351/Reviewer_MwNg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761670883620, "cdate": 1761670883620, "tmdate": 1762931288227, "mdate": 1762931288227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on image enhancement, particularly tasks involving color or lighting restoration, such as low-light enhancement. It uses heterogeneous knowledge distillation to improve the efficiency of generative restoration models. However, the paper lacks a thorough comparison of efficiency metrics (e.g., inference time and FLOPs). Furthermore, most of the compared methods are not generative restoration methods, which raises questions about the reasonableness and fairness of the experimental comparisons."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper presents a variety of experiments, which fully demonstrate the effectiveness of the proposed method, although its efficiency remains questionable."}, "weaknesses": {"value": "I have some questions about the technical details and novelty of this paper.\n\n1. Spatial Channel Layer Normalization (SCLN) uses a flattened spatial-channel dimension to calculate the mean and variance during normalization. Does this lead to significant computational overhead? Current LayerNorm has a well-developed fused CUDA kernel; would changing its computational logic affect the acceleration on CUDA? Furthermore, ablation experiments with SCLN seem to lack numerical metrics. I hope to see a significant improvement in model performance in terms of fidelity or aesthetics after using SCLN.\n2. This paper introduces the HVI color space as an auxiliary constraint. What is the fundamental difference between this and HVI-CIDNet? I also noticed the use of LPIPS-VGG for training the teacher model. Would the absence of this constraint lead to a performance decrease?\n3. The core of the FLEX proposed in this paper is to utilize cross-norm to address feature distribution mismatch, which seems to highly overlap with [1].\n4. Is the distillation method proposed in this paper superior to other heterogeneous distillation algorithms [2]?\n5. Use \\citep{} instead of \\cite{}.\n\n[1] CrossNorm and SelfNorm for Generalization Under Distribution Shifts. ICCV 2021.\n\n[2] Heterogeneous Knowledge Distillation using Information Flow Modeling. CVPR 2020."}, "questions": {"value": "Many of the tasks listed in the paper are related to lighting. Could some re-lighting models be used to restore lighting and thus solve these image enhancement tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "omPtEDD8Ky", "forum": "weh7JpT1lF", "replyto": "weh7JpT1lF", "signatures": ["ICLR.cc/2026/Conference/Submission19351/Reviewer_jwUY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19351/Reviewer_jwUY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896086856, "cdate": 1761896086856, "tmdate": 1762931287538, "mdate": 1762931287538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed a teacher student framework for the task of non-blind image restoration with paired ground truths for training. Task specific restoration networks are trained for different restoration objectives. The authors claimed SOTA results in almost all tasks and datasets. The authors claimed novelty in using latent rectified flows for knowledge transfer and novel normalization and a new feature loss."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The authors put in significant effort to test their proposed framework over several datasets and restoration tasks. However, much is left desired in presentation quality."}, "weaknesses": {"value": "- Writing and presentation is poor and unclear in general. Some pointers and suggestions are here to help the authors improve the article for future submission. The article is difficult to read and hard to follow.   \n- Major issue: *Retinex is NOT physics based\\!* It is a study of the human, biological, color vision system. It is OK to draw inspiration from retinex, by using this hypothesised code as presented in Land 1977, but this should not be called physics based. This should be referred to as biologically inspired.  \n- Do not simply list a huge bunch of slightly related papers in the related works section. Instead, selectively choose key background material, and briefly cover the key ideas that are of note to the proposed method. For example, in the work, the use of retinex inspired information coding seems to be an important part of the architecture design, explain what it is and what it’s hypothesized effect is in the proposed architecture. Is the use new? Or are there prior works that also use this biologically inspired coding scheme? This also sets the stage for subsequent ablation studies, showcasing the importance of ideas, either completely novel, novel use/application, or novel interpretation.   \n- Figure 2 is extremely unclear. I’m unable to understand what each component is and how they relate to each other. There’s a rather involved training workflow as I understood from repeated re-reading, this can be presented in a clear manner as a flow char of sorts. Reused components in each stage can also be marked out and referred to.   \n- There are far too many acronyms this makes reading the results extremely difficult  \n- Make better use of table and figure captions\\! They should have more substantial description in them, at least include a key insight from the data in the table or the images shows. For example: Fig 4\\. Qualitative results on for low-light enhancement on 4 datasets. Why are there boxes in the images? What do you want the readers to pay attention to in these boxes?"}, "questions": {"value": "* Since this is a teacher-student framework, how well does the teacher perform?  \n* For each task, is the teacher network the same?  \n* How is the teacher network selected? Is the teacher network a well performing network from a previous work?  \n* How is the student network selected? And again, is the design based on any previous work?  \n* Is the same architecture used for all restoration tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BMdKZIetXK", "forum": "weh7JpT1lF", "replyto": "weh7JpT1lF", "signatures": ["ICLR.cc/2026/Conference/Submission19351/Reviewer_wXE6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19351/Reviewer_wXE6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899668390, "cdate": 1761899668390, "tmdate": 1762931287098, "mdate": 1762931287098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies degraded image restoration through knowledge distillation framed as a generative rectified-flow process from teacher to student. A teacher transformer is trained with physics-inspired auxiliaries (e.g., Retinex, anisotropic diffusion, polarized HVI color) and attention stabilizers (SCLN, QK-norm). The student learns velocity fields along straight-line feature paths and uses a FLEX loss combining cross-normalization, percentile masking, and resolution weighting. The claimed benefits are few-step inference and competitive quality across multiple restoration tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. A cohesive rectified-flow perspective on feature-level distillation that is implementable with simple Euler updates and clear path parameterization.\n2. A concrete, reproducible teacher recipe that mixes classical priors with modern transformers and documents key loss weights and stabilization tricks.\n3. A two-stage student training pipeline that is logically structured and easy to follow.\n4. The FLEX loss addresses teacher–student distribution mismatch in a targeted way via student-statistic normalization and percentile outlier handling.\n5. The few-step inference angle is practical for deployment scenarios where latency is constrained."}, "weaknesses": {"value": "1. The teacher pipeline appears to use ground-truth signals during pretraining or feature extraction, risking supervision leakage and motivating an LQ-only teacher-target variant.\n2. Student-side ablations are shallow because the effects of rectified flow, FLEX, trajectory consistency, and step-size scheduling are not disentangled and there is no sensitivity to percentile, SNR, or step count.\n3. Positioning against modern transformer-aware KD remains largely narrative without controlled head-to-head experiments on attention, token, or logit distribution alignment.\n4. Evidence across datasets and metrics is uneven, with limited discussion of fidelity–perceptual trade-offs and missing analysis of clear failure cases.\n5. The proposed method needs to be tested on other image restoration tasks (such as super-resolution and denoising)."}, "questions": {"value": "1. Are any KD targets derived from inputs containing ground truth (e.g., concatenated [LQ, GT] or GT-based Retinex)? If so, what is the performance when targets are computed from LQ-only inputs?\n2. Can you add per-component ablations: rectified flow on/off, FLEX on/off, and within FLEX: cross-normalization only vs. +percentile vs. +resolution weighting, with sensitivity to the percentile threshold and SNR/step-size settings?\n3. How does the method compare to attention-/token-level transformer KD under identical student backbones and training budgets?\n4. Can you expand results across all stated restoration categories with both fidelity (PSNR/SSIM) and perceptual (LPIPS/CLIP-IQA/FID) metrics, and discuss cases where metrics disagree?\n5. Will you release code, training/evaluation scripts, and the exact list of selected inference steps and hyperparameters to ensure full reproducibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L8rdItHw8x", "forum": "weh7JpT1lF", "replyto": "weh7JpT1lF", "signatures": ["ICLR.cc/2026/Conference/Submission19351/Reviewer_Dxbb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19351/Reviewer_Dxbb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988002181, "cdate": 1761988002181, "tmdate": 1762931286598, "mdate": 1762931286598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}