{"id": "lkQ8ljR0Fc", "number": 11438, "cdate": 1758198992824, "mdate": 1759897575566, "content": {"title": "In-Place Feedback: A New Paradigm for Guiding Large Language Models in Multi-Turn Reasoning", "abstract": "Large language models (LLMs) are increasingly studied in the context of multi-turn reasoning, where models iteratively refine their outputs based on user-provided feedback. Such settings are crucial for tasks that require complex reasoning, yet existing feedback paradigms often rely on issuing new messages. LLMs struggle to integrate these reliably, leading to inconsistent improvements. In this work, we introduce in-place feedback, a novel interaction paradigm in which users directly edit an LLM’s previous response, and the model conditions on this modified response to generate its revision.\nEmpirical evaluations on diverse reasoning-intensive benchmarks reveal that in-place feedback achieves better performance than conventional multi-turn feedback while using $79.1$\\% fewer tokens.\nComplementary analyses on controlled environments further demonstrate that in-place feedback resolves a core limitation of multi-turn feedback: models often fail to apply feedback precisely to erroneous parts of the response, leaving errors uncorrected and sometimes introducing new mistakes into previously correct content.\nThese findings suggest that in-place feedback offers a more natural and effective mechanism for guiding LLMs in reasoning-intensive tasks.", "tldr": "We introduce in-place feedback, a novel interaction paradigm in which users directly edit an LLM’s previous response, and the model conditions on this modified response to generate its revision.", "keywords": ["LLM", "Large Language Model", "Feedback", "In-place Feedback", "Local correction", "Multi-turn"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d515a8b24c32f15ad700f53a1d33d76aa740f181.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces in-place feedback, an interaction method where users directly edit a large language model’s (LLM) previous response, and the model generates a new output conditioned on the edited context. This approach achieves stronger refinement performance on reasoning benchmarks while being more token-efficient—requiring fewer input and output tokens.\n\nOverall, I believe the paper’s main weakness lies in the lack of compelling real-world applications, and I have listed several concerns below. At this stage, I do not think the paper meets the acceptance threshold; however, if the authors can provide strong and convincing responses to these issues, I would be willing to raise my score."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to understand.\n2. The proposed method is novel and may provide some insights for the community.\n3. The experiments are substantial."}, "weaknesses": {"value": "1. The practical value of this paper seems limited. In many cases, users ask LLMs questions about topics they themselves don’t fully understand, making it difficult to provide effective feedback—or even to determine whether the model’s response is correct or incorrect. Consequently, the proposed method appears to lack compelling real-world applications.\n2. In-place feedback essentially provides the model with more prior information, so it's not surprising that it outperforms standard multi-turn interaction. In contrast, some approaches—such as [1]—improve response quality without requiring external human input, which arguably offers greater practical value.\n\n[1] Madaan, Aman, et al. \"Self-refine: Iterative refinement with self-feedback.\" Advances in Neural Information Processing Systems 36 (2023): 46534-46594.\n\n3. Regarding the experimental results, readers would likely appreciate seeing precise numerical values; therefore, presenting the findings exclusively through figures may not be the most effective approach."}, "questions": {"value": "1. The paper’s evaluation appears to be conducted primarily on tasks with known ground-truth answers. However, it remains unclear how the proposed method would perform on datasets where the correct answer is not available—such as in real-world, partially observable, or proprietary settings. Addressing this gap would be important for assessing the method’s applicability in more practical, open-ended scenarios.\n\n2. In practice, users most commonly rely on larger-sized models. Has the author experimented with the proposed method on very large models (e.g., those with tens or hundreds of billions of parameters)?\n\n3. How exactly is the pruning of reasoning context relevant to the edited segment, as mentioned in Section 2.3, performed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3pUnG3T07i", "forum": "lkQ8ljR0Fc", "replyto": "lkQ8ljR0Fc", "signatures": ["ICLR.cc/2026/Conference/Submission11438/Reviewer_pZrn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11438/Reviewer_pZrn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11438/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760617205840, "cdate": 1760617205840, "tmdate": 1762922553510, "mdate": 1762922553510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates in-place feedback as an alternative feedback method to conversational, multi-turn feedback. Empirically, the paper shows in-place feedback is more effective than multi-turn feedback, and has additional benefits in saving compute. Section 4 presents an in-depth analysis on ZebraLogic, using 5 metrics to diagnose how models incorporate feedback, and further validated the benefits of in-place feedback."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This is a well-written and well-executed paper. The paper takes an unique perspective and studies the interface for language models to incorporate feedback. I'm convinced that in-place feedback is a useful and effective method, and I find many experiments and findings in the paper interesting and insightful."}, "weaknesses": {"value": "* Currently the paper uses gpt-5-mini to automate the human users who provide feedback. This is a reasonable approximation but also creates a gap towards the claimed use case. The paper could be further improved by doing small batch experiments with real humans providing feedback to LLMs. \n* Additionally the datasets used in the paper (MMLU, MATH, GPQA) are very different from real-world use cases where human inspects LLM answers and provide feedback. Validation in more realistic settings could be helpful or worth investigating in future work."}, "questions": {"value": "* How does gpt-5-mini perform on the datasets you used? Since it's used to provide feedback to other models, it will be helpful to report its performance and consider it as an upper bound.\n* Does in-place feedback work in a \"self-refine\" setting? Currently you used gpt-5-mini as the \"teacher\" and three weaker models as the \"student\". I wonder if this method is still more useful when the teacher and the student are the same model.\n* Do you have any hypothesis on _why_ in-place feedback is more effective than multi-turn feedback? E.g., Is this relevant to any known limitations or mechanisms of transformers?\n\n(Willing to adjust my rating if these questions are addressed.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UqTsIHod6Y", "forum": "lkQ8ljR0Fc", "replyto": "lkQ8ljR0Fc", "signatures": ["ICLR.cc/2026/Conference/Submission11438/Reviewer_6ugG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11438/Reviewer_6ugG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11438/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761633886518, "cdate": 1761633886518, "tmdate": 1762922553034, "mdate": 1762922553034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates an improved interaction paradigm for large language models (LLMs) in multi-turn reasoning settings, in-place feedback. This method enables users directly edit the model’s previous response, and the LLM generates a revision based on this modified version. Through experiments on multiple reasoning-intensive benchmarks, the paper shows that in-place feedback yields better performance and higher feedback precision compared to conventional multi-turn feedback, while also reducing token usage. Additional analyses in controlled environments demonstrate that this approach effectively mitigates a key weakness of standard feedback — the model’s tendency to misapply corrections or introduce new errors. Overall, the study presents in-place feedback as a more natural and efficient strategy for guiding LLMs in complex reasoning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper presents a very simple method, which makes it easy to follow.\n2. The case figures in this paper are very clear and effectively illustrate the main ideas presented in the work.\n3. The experimental setup is straightforward and reproducible."}, "weaknesses": {"value": "1. The proposed approach demonstrates limited innovation and seems to focus more on an engineering-oriented implementation than on advancing fundamental research.Describing it as a new paradigm may be overstated.\n2. In Figure 1, the third failure mode — “the feedback is applied but causes errors in subsequent reasoning steps” — does not appear to be unique to multi-turn feedback. Such errors are often uncontrollable, as large language models themselves tend to produce mistakes progressively even when the earlier reasoning steps are correct. Therefore, categorizing this issue as a failure mode specific to multi-turn feedback does not seem reasonable.\n3. The proposed in-place edit approach’s interaction mode shifts much of the difficulty in problem-solving to the user. In a standard multi-turn setting, users only need to provide feedback on what the model misunderstood (as shown in Figure 1, where the model misinterprets “2 more balls are added to the total” as “each”). In contrast, the proposed method requires users to fully understand each step of the model’s reasoning, identify the specific errors, and then modify them directly. This paradigm may work for relatively simple mathematical problems where users can easily spot mistakes, but it becomes impractical for more complex problems in which users themselves may not be able to detect the model’s errors."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0tmZIaHBb5", "forum": "lkQ8ljR0Fc", "replyto": "lkQ8ljR0Fc", "signatures": ["ICLR.cc/2026/Conference/Submission11438/Reviewer_Xh9G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11438/Reviewer_Xh9G"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11438/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807695491, "cdate": 1761807695491, "tmdate": 1762922552461, "mdate": 1762922552461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Researchers found that letting users directly edit an AI's wrong answers works much better than just telling the AI \"that's wrong, fix it\" - the AI fixes mistakes more accurately, uses 79% fewer tokens, and doesn't accidentally break parts that were already correct."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. **Thorough testing** - Multiple datasets and models show consistent improvements\n2. **Identifies why failures happen** - Pinpoints exactly how traditional feedback breaks (corrupting correct parts, ignoring feedback, creating new errors)"}, "weaknesses": {"value": "### 1. Critical methodology gap with \"user edits\"\nThe paper states multiple times that the method involves \"user directly edit an LLM's previous response\" and \"user feedback is applied as an edit,\" but in the real experiments, the authors used GPT-5-mini instead as the human editor.\nIf the authors intended to use real human/users as the editors, the method requires significant human labor to identify error locations and provide precise edits, which limits real-world applicability. If the authors intended to use an AI agent as an alternative to the human editors, then the whole writing in the paper has a huge flaw, where throughout the paper, it shows the advantages of involving real users in the feedback editing stage.\nThere is no additional training for GPT-5-mini to identify which parts in the response need to be replaced. Meanwhile, GPT-5-mini is rather a strong model compared to the models in the paper (Gemma-3-4b, Qwen2.5-7B, Llama-3.1-8B). An ablation study is needed to show if different models used in the editing stage would change the final result. If the in-place feedback models are the weaker models like Gemma-3-4b, Qwen2.5-7B, Llama-3.1-8B, would it still bring improvement to the tasks?\n### 2. Missing key related work\nThe paper is missing key related work, such as the intrinsic self-correction idea in https://arxiv.org/abs/2310.01798. At the same time, while the proposed method highlighted the advantages of letting users directly edit the LLM's responses, recent works (https://arxiv.org/abs/2409.12822) demonstrate that LMs can produce hard-to-detect errors, which raises questions about the reliability of human-provided in-place edits. The authors should make more discussion on how human-provided in-place edits would work in the mentioned tasks and on the failure modes of human-provided in-place edits.\n### 3. Flawed token efficiency analysis\nThe token efficiency part in Line 243 makes faulty analysis. It says \"For input tokens, multi-turn feedback appends new turns to the dialogue history, causing token usage to grow linearly with the number of turns\" and \"multi-turn feedback accumulates the full dialogue history.\" In early literature in 2023, there are already papers on multi-turn/iterative reasoning that don't keep the whole dialogue history as the prompt, for example Reflexion (https://arxiv.org/abs/2303.11366), MemGPT (https://arxiv.org/abs/2310.08560), MemPrompt (https://aclanthology.org/2022.emnlp-main.183.pdf). The paper also lacks proper citations.\n### 4. Limited experimental scope and presentation\nNo analysis of how the method scales to longer, more complex tasks. Task selection is narrow, focusing primarily on mathematical and logical reasoning without exploring other domains. The chosen datasets do not cover variations of reasoning scenarios and can hardly be called comprehensive experiments. Testing is restricted to relatively small open-source models (Gemma-3-4b, Qwen2.5-7B, Llama-3.1-8B). There is no evaluation on state-of-the-art models, more recent reasoning-optimized models, or proprietary models like GPT and Claude. Additionally, the paper doesn't have a comprehensive table for all results, making it hard to see what their improvements are by percentage."}, "questions": {"value": "1. This paper identifies failure modes as corrupting correct parts, ignoring feedback, and creating new errors. Have you counted what percentage of the 3 different failure modes occur in different tasks, and to what extent do the baseline and your approach fix these failure modes?\n2. Recent literature found that multi-turn refinement usually stops improving after the first round. What is the first-round performance for your in-place feedback approach?\n3. What are the reasons if, after one round of in-place feedback, the answer is still incorrect? What failure modes remain?\n4. Have you tested scenarios where the feedback itself is incorrect or misleading? How robust is in-place feedback to erroneous edits?\n5. Have you analyzed the quality of edits made by GPT-5-mini? How often does it correctly identify the minimal edit span versus over-editing or under-editing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3GZXSnbMih", "forum": "lkQ8ljR0Fc", "replyto": "lkQ8ljR0Fc", "signatures": ["ICLR.cc/2026/Conference/Submission11438/Reviewer_x7Uc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11438/Reviewer_x7Uc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11438/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851608381, "cdate": 1761851608381, "tmdate": 1762922552009, "mdate": 1762922552009, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}