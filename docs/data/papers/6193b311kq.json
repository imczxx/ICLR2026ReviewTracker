{"id": "6193b311kq", "number": 2556, "cdate": 1757144840380, "mdate": 1762947086683, "content": {"title": "Quantize-then-Rectify: Accelerating VQ-VAE Training in Latent Feature Space", "abstract": "Visual tokenizers are pivotal in multimodal large models, acting as bridges between continuous inputs and discrete token. Nevertheless, training high-compression-rate VQ-VAEs remains computationally demanding, often necessitating thousands of GPU hours. This work demonstrates that a pre-trained VAE can be efficiently transformed into a VQ-VAE by controlling quantization noise within the VAE's tolerance threshold. We present \\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs to enable rapid VQ-VAE training with minimal computational overhead. By integrating \\textbf{channel split quantization} to enhance codebook capacity and a \\textbf{post rectifier} to mitigate quantization errors, ReVQ compresses ImageNet images into at most $512$ tokens while sustaining competitive reconstruction quality (rFID = $0.82$). Significantly, ReVQ reduces training costs by over two orders of magnitude relative to state-of-the-art approaches: ReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours, whereas comparable methods require 4.5 days on a 32 A100 GPUs. Experimental results show that ReVQ achieves superior efficiency-reconstruction trade-offs.", "tldr": "", "keywords": ["VQVAE", "tokenizer", "autoencoder", "efficiency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/71abe590ce4db63a6c197f87f204bd7053d0eae7.pdf", "supplementary_material": "/attachment/bab99b81fb2dba33c5c7437f8e9c6899a2d6244a.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces ReVQ, a novel framework for efficiently transforming pre-trained VAEs into high-compression VQ-VAEs for vision-language model applications. ReVQ leverages a channel split quantization and a post rectifier to overcome quantization errors. The proposed method drastically improves training efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-  ReVQ framework enables rapid training requiring only a single NVIDIA 4090 and less than a day.\n\n- The authors conduct various experiments to show the effectiveness of the proposed results, both quantitatively and qualitatively."}, "weaknesses": {"value": "- The authors emphasize that the ReVQ framework offers efficiency improvements. However, the core approach mainly leverages existing VAE and VQ-VAE components with only incremental modifications, rather than introducing fundamentally new architectures. The novelty of the overall approach is very limited, especially since VAE training itself is also time-consuming. It is unreasonable for the authors to ignore this part of the computation time.\n\n- Both qualitative and quantitative evaluations are also not particularly strong. As shown in Figure 5, even with 262,144 codebook entries, the reconstruction result of ReVQ is still full of artifacts. As shown in Table 1, compared with VQGAN-LC, ReVQ has worse PSNR (22.267 vs 23.800) and LPIPS (0.122 vs 0.120) scores when using a larger codebook (262,144 vs 100,000).\n\n- The layout and writing of the paper could be improved. The article spends a lot of space introducing traditional VQ-VAE, but put the implementation details of the proposed channel split quantization to the appendix."}, "questions": {"value": "- If a frozen VAE combined with a learnable VQ does not outperform a VQ-VAE trained from scratch, what is the practical advantage or main contribution of your proposed approach? This question is especially important given the availability of many open-source VQ-VAE models that can be used directly. Could the authors clarify the unique benefits or scenarios where ReVQ provides significant value over existing alternatives?\n\n- Figure 4b requires further explanation. Could the authors clarify how this figure demonstrates that channel split outperforms spatial split, and explain what specific metrics or evidence support this claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1OWAJeWhLY", "forum": "6193b311kq", "replyto": "6193b311kq", "signatures": ["ICLR.cc/2026/Conference/Submission2556/Reviewer_eamF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2556/Reviewer_eamF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761534520944, "cdate": 1761534520944, "tmdate": 1762916280972, "mdate": 1762916280972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Cm1ppFS64Z", "forum": "6193b311kq", "replyto": "6193b311kq", "signatures": ["ICLR.cc/2026/Conference/Submission2556/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2556/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762947084955, "cdate": 1762947084955, "tmdate": 1762947084955, "mdate": 1762947084955, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the problem of high training computational cost in VQ-VAE, this paper proposes the ReVQ framework, which quickly converts a pre-trained VAE into a VQ-VAE, significantly reducing training overhead. The paper emphasizes the superiority of ReVQ in the efficiency-compression trade-off, providing an accessible visual tokenizer for large multimodal models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed ReVQ framework drastically cuts VQ-VAE training cost, greatly improving accessibility. It achieves efficiency-compression trade-off with low rFID, competitive PSNR/SSIM/LPIPS, and high-fidelity reconstruction. Extensive ImageNet evaluations and solid theoretical analysis further enhance the work’s rigor."}, "weaknesses": {"value": "ReVQ fails to match state-of-the-art methods at extremely high compression ratios, limiting its application in extreme compression scenarios. Additionally, the experimental testing domain is narrow with insufficiently extensive evaluations, which may compromise the method's generalizability. Generalizability is not sufficiently validated. While the manuscript mentions plans to explore more modalities in the future, no evidence is provided in the current work."}, "questions": {"value": "1. Regarding the limitation at extremely high compression ratios, how does the author plan to optimize the design of the rectifier?  \n2. Although the inactive reset strategy is effective, can more rigorous theoretical analysis (e.g., convergence proof) be provided?  \n3. How can ReVQ be extended to multimodal tasks such as video reconstruction or audio? Are there plans to validate its performance on other multimodal datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1ohineEmnN", "forum": "6193b311kq", "replyto": "6193b311kq", "signatures": ["ICLR.cc/2026/Conference/Submission2556/Reviewer_FCxv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2556/Reviewer_FCxv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967713598, "cdate": 1761967713598, "tmdate": 1762916280813, "mdate": 1762916280813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Quantize-then-Rectify (ReVQ): convert a pre-trained VAE (DC-AE) into a VQ-VAE via channel split quantization with Non-Activation Reset to improve codebook utilization, and add a decoder-only rectifier trained with ℓ2 loss on \\(Z_e\\) to correct quantization error."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  Clean separation of quantizer and rectifier \\(g\\); only the rectifier is trained with \\(\\|Z_e - g(q(Z_e))\\|_2^2\\), keeping optimization simple.\n2. Non-Activation Reset is a direct, epoch-level heuristic to re-seed dead codes and mitigate codebook collapse under nearest-neighbor VQ.\n3. Capacity diagnosis. Empirical exponential relation between token length \\(B\\) and the minimum number of codebooks \\(M\\) needed to keep MSE low clarifies compression pressure."}, "weaknesses": {"value": "1) Noise study is not tied to the actual quantizer. The variance ≤ 0.3 tolerance is measured via Gaussian noise on DC-AE latents, not from the channel-split quantization process (its distribution, variance, or anisotropy). As a result, it does not guide codebook size or bit allocation.\n\n2) Loss is misaligned with the evaluation target. The rectifier is trained only with latent-space ℓ2 on \\(Z_e\\) (decoder-only), without perceptual or adversarial terms. This weakly couples the learned correction to image-space quality, especially at high compression.\n\n3) Scaling behavior remains unresolved. The approach is reported to compress to at most 512 tokens and implies exponential codebook growth beyond that. There is no rate–distortion analysis or entropy-regularized design to control capacity vs. fidelity at larger scales."}, "questions": {"value": "N/A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0TRlEVbroJ", "forum": "6193b311kq", "replyto": "6193b311kq", "signatures": ["ICLR.cc/2026/Conference/Submission2556/Reviewer_MKFd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2556/Reviewer_MKFd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001627144, "cdate": 1762001627144, "tmdate": 1762916280684, "mdate": 1762916280684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel  framework that efficiently transforms a pre-trained VAE into a VQ-VAE by integrating channel split quantization to enhance codebook capacity and a post redtifier to mitigate quantization errors. Main results are conducted on ImageNet which achieves reducing training costs by over two orders of magnitude comparing with SOTA methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Figures are informative and clear, which are easy to follow. \n2. The experimental results are appealing which obtains significant training efficiency and compression ratio improvement. \n3. Motivations and corresponding methods are explained clearly."}, "weaknesses": {"value": "1. Since pre-training a VAE also demands training time and GPU resources and previous methods do not include a pre-training stage, the proposed converting VAE into ReVQ method should add the pertaining time into consideration. Therefore, I'm curious about the total training cost including pre-training VAE stage and converting into ReVQ stage. Authors should compare the training cost and performance between VQ-VAE (trained from scratch) and ReVQ (pre-train + convert)\n2. In Line 245-246, the proposed method sort each codex at the end of the epoch, what's the cost of this process and the proportion defined to determine which code can be discovered as non-activation? Authors should add relevant explanations and experimental comparison between different proportion setting about this part. \n3. Although authors compare the performance between spatial and channel split strategy, there's no training cost comparison between them. Meanwhile, how about a hybrid spatial/channel split. For instance, for som sensitive channels, space split can be applied while channel split applied for the rest to achieve a trade-off between efficiency and performance."}, "questions": {"value": "See weaknesses. I would raise my score if most of my concerns are solved (mainly the first and third point in weaknesses)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FsggSQYFzZ", "forum": "6193b311kq", "replyto": "6193b311kq", "signatures": ["ICLR.cc/2026/Conference/Submission2556/Reviewer_qyBb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2556/Reviewer_qyBb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762328840989, "cdate": 1762328840989, "tmdate": 1762916280542, "mdate": 1762916280542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}