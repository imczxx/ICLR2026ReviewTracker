{"id": "dbzsXGWLvB", "number": 22805, "cdate": 1758335688426, "mdate": 1759896845325, "content": {"title": "TRELLISWorld: Training-Free World Generation from Object Generators", "abstract": "Text-driven 3D scene generation holds promise for a wide range of applications, from virtual prototyping to AR/VR and simulation. However, existing methods are often constrained to single-object generation, require domain-specific training, or lack support for full 360-degree viewability. In this work, we present a training-free approach to 3D scene synthesis by repurposing general-purpose text-to-3D object diffusion models as modular tile generators. We reformulate scene generation as a multi-tile denoising problem, where overlapping 3D regions are independently generated and seamlessly blended via weighted averaging. This enables scalable synthesis of large, coherent scenes while preserving local semantic control. Our method eliminates the need for scene-level datasets or retraining, relies on minimal heuristics, and inherits the generalization capabilities of object-level priors. We demonstrate that our approach supports diverse scene layouts, efficient generation, and flexible editing, establishing a simple yet powerful foundation for general-purpose, language-driven 3D scene construction. We will release the full implementation upon publication.", "tldr": "A training-free framework for text-driven 3D scene generation that repurposes object-level diffusion models by tiling and blending overlapping regions.", "keywords": ["Generative Models", "3D Scene Generation", "Text-to-3D", "Diffusion Models", "Training-Free Inference"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5521d337b68a4c85c1b8572e05085e368a7f152c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces TRELLISWorld, a training-free framework for large-scale 3D scene generation. The main contribution is that it enables scene-level generation without requiring scene-level training data, leveraging object-level pretrained 3D generative models (Trellis) instead.\nThe method divides a 3D scene into multiple overlapping patches. For patches with shared regions, a weighted averaging over the overlapping areas is applied during generation to ensure local consistency. Ablation studies show that using a 3D cosine mask for blending produces smoother transitions than direct averaging.\nCompared with SynCity, TRELLISWorld achieves a slight improvement on CLIP Mean, but at the cost of significantly higher generation time and computational resources."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Training-free and data-efficient: The method does not rely on scene-level 3D data; it extends an object-level pretrained model to generate full 3D scenes.\n2. Effective patch blending: The use of a 3D cosine mask during blending helps preserve the central region of each generated patch while enabling smooth transitions at the edges."}, "weaknesses": {"value": "1. Evaluation inconsistency: For perceptual alignment comparison, the paper states that 18 close-distance views were uniformly sampled, yet the presented examples are mostly long-distance views. Showing close-up comparisons would provide a more convincing evaluation.\n2. Limited controllability: The method only supports text-based control, which limits fine-grained scene manipulation. Designing prompts for each patch can be cumbersome, even though the authors mention using LLMs to generate multiple prompts. However, how these prompts are assigned to specific patches to ensure coherent spatial layout is unclear.\n3. Low-resolution figures: The figures are generally low in resolution, making it difficult to assess the fine-grained 3D details of generated objects."}, "questions": {"value": "1. The overall quality of the results is hard to assess since most examples only show low-resolution overviews of entire scenes. It is strongly recommended that the authors include close-up visualizations and higher-resolution renderings to better demonstrate scene quality.\n2. As the method relies on Trellis, a text-to-3D model, each patch requires a corresponding text prompt. However, the paper lacks a detailed explanation of how prompts are managed or distributed across patches (e.g., after generating multiple city-related prompts via LLMs, how are they spatially allocated to form a coherent scene?).\n\nThings to improve the paper that did not impact the score:\n- Figure 2 should be moved to page 4, closer to where it is referenced.\n- In Formula (2), please clarify the meaning of the $\\mathcal{O}$ notation and $[\\cdot]_f$.\n- For a tile size of 4×3×1, what is S? In Figure 3(a), it seems the layout is divided into 8×6 tiles. So is S = 0.5?\n- In Figure 4, “without blending” is misleading since simple averaging aggregation is also a form of blending. Consider using “average” and “weighted average” or others instead.\n- The tiled decoder mentioned in the ablation study is difficult to locate in the Method section; please reorganize the text to highlight this component more clearly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q0BDLJCpQQ", "forum": "dbzsXGWLvB", "replyto": "dbzsXGWLvB", "signatures": ["ICLR.cc/2026/Conference/Submission22805/Reviewer_7XGV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22805/Reviewer_7XGV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722872729, "cdate": 1761722872729, "tmdate": 1762942393874, "mdate": 1762942393874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to lift the ability of 3D object generation models to 3D world space. It introduces a training-free approach, trellisworld, to achieve 3D world generation. Based on a text-to-3D model, trellisworld denoises multiple 3D tiles in parallel and use an average weighting mechanism to aggregate the results at each timestep, thereby generating coherent 3D world."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Overall, the paper introduces a training-free method to generate 3d world using a 3D object generator. It provides a simple and effective method to achieve meaningfull applications."}, "weaknesses": {"value": "The originality of the paper is somehow limited. The article claims the difference between them and syncity is that syncity depends on image inpainting, but this is merely a difference in the conditional mechanism. Aside from this conditional mechanism, the overall pipeline, which involves generating tiles and then blending, is very similar. Furthermore, the mechanisms of tile diffusion and blending mentioned in the paper are very similar to those of MultiDiffusion [1], and I haven't seen any effective strategies specifically designed for 3D.\n\n[1] MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation"}, "questions": {"value": "1. The performance of TRELLIS in text-to-3D geneartion is worse than in image-to-3D generation. Therefore, I'm wondering if using text as the conditioned condition might have a lower performance, as I understand that generalization and controllability should be worse than models conditioned on images.\n2. Quality of figures in the paper should be improved.\n3. What is the maximum number of tiles that Trellisworld can generate simultaneously? How does the model's performance change as the number of simultaneously generated tiles increases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gJyzjpgfah", "forum": "dbzsXGWLvB", "replyto": "dbzsXGWLvB", "signatures": ["ICLR.cc/2026/Conference/Submission22805/Reviewer_exxc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22805/Reviewer_exxc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815756757, "cdate": 1761815756757, "tmdate": 1762942393417, "mdate": 1762942393417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose TRELLISWorld, a training-free framework for text-driven 3D scene generation, which composes complex scenes by leveraging pre-trained text-to-3D object diffusion models.\nInstead of training an end-to-end scene generator, TRELLISWorld decomposes the scene noise into multiple object-level subregions (“chunks”) and employs a cosine-weighted re-aggregation strategy to efficiently synthesize large-scale 3D environments.\nCompared to the state-of-the-art method SynCity, TRELLISWorld achieves superior visual quality and significantly faster inference, demonstrating the effectiveness of its modular and scalable generation approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. State-of-the-Art Results\nThe proposed TRELLISWorld achieves superior CLIP score performance compared to the recent state-of-the-art method SynCity, while also requiring less computational resources and delivering faster inference speed. This demonstrates the efficiency and scalability of the training-free design.\n\n2. Comprehensive Ablation Studies\nThe authors present comprehensive qualitative ablation studies on key components—Tiled Diffusion, Blending, and Tiled Decoder—clearly illustrating the contribution of each to the final scene generation quality. These studies effectively highlight how each module enhances visual coherence and overall realism."}, "weaknesses": {"value": "1. Heavy Reliance on the Base Model\nAs acknowledged in the manuscript, the proposed method—being training-free—is inherently limited by the capabilities of its underlying base model, TRELLIS. Consequently, the overall performance and generalization ability are closely tied to the pretrained model’s strengths and weaknesses, which may restrict the method’s applicability across diverse domains.\n\n2. Lack of Quantitative Ablation Studies\nWhile the qualitative ablation studies provide valuable insights, the paper would benefit from quantitative analyses to numerically assess the contribution of each component. Such evaluations would help clarify how elements like Tiled Diffusion, Blending, and Tiled Decoder quantitatively influence the final output quality and performance."}, "questions": {"value": "1. Could the authors investigate how the performance changes when the base model is replaced with alternatives to TRELLIS? Such an analysis would help assess the generality and adaptability of the proposed framework.\n\n2. It is assumed that the stride size used in the tiled generation process may influence the final performance. Could the authors conduct additional experiments with varying stride sizes to analyze its impact on scene quality and consistency?\n\n3. Could the authors provide quantitative ablation results (e.g., CLIP score) without the proposed components to clarify the contribution of each module to the overall performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "n1MrTgWm7c", "forum": "dbzsXGWLvB", "replyto": "dbzsXGWLvB", "signatures": ["ICLR.cc/2026/Conference/Submission22805/Reviewer_9vZM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22805/Reviewer_9vZM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879026346, "cdate": 1761879026346, "tmdate": 1762942393238, "mdate": 1762942393238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets the training-free goal for 3d scene generation. The authors propose TRELLISWorld, which reframes text-to-3D scene synthesis as a multi-tile denoising problem. The overlapping 3D regions are generated by a pretrained object-level model and blended with cosine-weighted averaging. The experiments show qualitative results with advantages over SynCity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The core idea of using tiled diffusion with cosine blending to smoothen the inter-tile transition is straightforward with easy-to-understand intuition.\n\nThe method description is clear and the implementation provides some details, though it's doubtful if it's sufficient for readers to reperform w/o open-sourced codes. \n\nThe results show clear advantages over the peering work Syncity.\n\nThe limitation section acknowledges its base-model dependence and lack of object disentanglement."}, "weaknesses": {"value": "As mentioned in the strength, the method is quite straightforward, therefore the impact heavily lies in the provision of the tool as opensourced code to the community, as SynCity has done.\n\nThe innovative contribution is more an incremental improvement of Trellis, thus whether it meets the standard as a standalone paper in ICLR may need further discussion.\n\nThe work is heavily depending on the base object generator, which limits the contribution. \n\nThe comparison is mainly against SynCity while other recent 3Descene generation works referenced in the related work sections are largely missing.\n\nThe computation cost analysis is too simple, without showing any memory/runtime tests scaling with tile counts or comparisons to optimized SDS/LRM pipelines."}, "questions": {"value": "1. Could the authors provide complete implemenation details with full metric setups?\n2. Could the authors broaden the tests addressing the comments in the weakness, e.g. scale up with tile count to test computation, add other 3d scene baselines etc.?\n3. Does the proposal only work for static scene? Any idea how to make it work on dynamic scene?\n4. Can this proposal function as well on other base generator besides Trellis? Could the authors test the performance impact across a few other base generators?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sC6SHObpO3", "forum": "dbzsXGWLvB", "replyto": "dbzsXGWLvB", "signatures": ["ICLR.cc/2026/Conference/Submission22805/Reviewer_RN1P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22805/Reviewer_RN1P"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950305429, "cdate": 1761950305429, "tmdate": 1762942393058, "mdate": 1762942393058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}