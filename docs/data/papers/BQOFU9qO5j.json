{"id": "BQOFU9qO5j", "number": 12216, "cdate": 1758206383617, "mdate": 1763660744891, "content": {"title": "SASFT: Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs", "abstract": "Large Language Models (LLMs) have impressive multilingual capabilities, but they suffer from unexpected code-switching, also known as language mixing, which involves switching to unexpected languages in the model response. This problem leads to poor readability and degrades the usability of model responses.\nHowever, existing work on this issue lacks a mechanistic analysis and shows limited effectiveness.\nIn this paper, we first provide an in-depth analysis of unexpected code-switching using sparse autoencoders and find that when LLMs switch to a language, the features of that language exhibit excessive pre-activation values. Based on our findings, we propose $\\textbf{S}$parse $\\textbf{A}$utoencoder-guided $\\textbf{S}$upervised $\\textbf{F}$ine$\\textbf{t}$uning (SASFT), which teaches LLMs to maintain appropriate pre-activation values of specific language features during training. Experiments on five models across three languages demonstrate that SASFT consistently reduces unexpected code-switching by more than 50\\% compared to standard supervised fine-tuning, with complete elimination in four cases. Moreover, SASFT maintains or even improves the models' performance on six multilingual benchmarks, showing its effectiveness in addressing code-switching while preserving multilingual capabilities. The code and data are available at https://anonymous.4open.science/r/SASFT-71CC.", "tldr": "", "keywords": ["LLMs", "interpretability", "multilingualism"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/82e2e0c3585fce8a6a9e0121cd974eb0d94c3cfe.pdf", "supplementary_material": "/attachment/70c40cdf0e33283a0e77b8802581ed79a753c369.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of unexpected code-switching in multilingual Large Language Models (LLMs). The authors first conduct a mechanistic analysis using sparse autoencoders, identifying that the issue is caused by the excessive pre-activation of language features. Based on this finding, they propose Sparse Autoencoder-guided Supervised Finetuning (SASFT), a novel method that trains LLMs to control these pre-activation values. Experimental results on five models and three languages show that SASFT reduces unexpected code-switching by over 50% (eliminating it entirely in four cases) while maintaining or even improving performance on standard multilingual benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This work's key strength lies in its mechanistic, root-cause analysis of the code-switching problem, moving beyond superficial fixes. By using sparse autoencoders, it identifies the core issue: excessive pre-activation of language features.\nThe proposed solution, SASFT, is highly effective, consistently reducing unexpected code-switching by over 50% and even eliminating it entirely in several cases. Crucially, it achieves this without compromising performance, as it maintains or even improves the models' capabilities on standard multilingual benchmarks.\nIn short, its main strengths are its diagnostic depth, highly effective solution, and ability to fix the problem without sacrificing general multilingual proficiency.\nThe paper is well-written and easy to follow."}, "weaknesses": {"value": "1.Limited scope of evaluation: The solution is only tested on three languages and five models. Its effectiveness across a wider range of languages, especially low-resource ones, remains unverified.\n\n2.Uncertain generalizability: The method's performance is demonstrated on \"six multilingual benchmarks,\" but it is unclear if it generalizes well to other critical tasks like reasoning, complex translation, or creative writing.\n\n3.Computational overhead: The approach relies on sparse autoencoders, which likely introduce significant computational cost and complexity compared to standard fine-tuning, a trade-off not mentioned.\n\n4.Lack of comparative baselines: While it outperforms standard supervised fine-tuning, it is not compared against other specialized techniques aimed at reducing code-switching, making its relative advancement unclear,like the works listed in related works.\n\n5.Superficial treatment of intentional Code-Switching: The method focuses on \"unexpected\" code-switching but may risk suppressing intentional and culturally appropriate code-switching (e.g., in bilingual communities), potentially reducing linguistic flexibility."}, "questions": {"value": "1.how were these code-swithing issues for testing are constructed or prompted? Overall, it seems that this issue occurs relatively infrequently and is difficult to reproduce, especially across different models. Such cases seldom happen, making it challenging to trace and resolve the problem on a large scale.\n\n2.Page 4 Line 184， how many unexpected code-switching responses are collected？\n\n3.Have you analyzed for different languages which are prone to code-switch with each other?\n\n4.On which layers were the main supervised SFT experiments conducted? Regarding the time efficiency of autoencoder computation, does using more layers lead to higher computational efficiency, and how was this trade-off balanced in the ablation studies?\n\n5.Why compare with RL (GRPO), it is more likely to make alignment during the RL phase, not for multi-lingual extension.\n\n6.According to Table2, the proposed method does not show a significant improvement in effectiveness.\n\n7.Figure 6, pls give detailed description for multi-layer, like the specific number of layers and which layer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y0iFGJbzJ3", "forum": "BQOFU9qO5j", "replyto": "BQOFU9qO5j", "signatures": ["ICLR.cc/2026/Conference/Submission12216/Reviewer_aWvq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12216/Reviewer_aWvq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550736662, "cdate": 1761550736662, "tmdate": 1762923164072, "mdate": 1762923164072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel method, SASFT, to mitigate the issue of unexpected code-switching in large language models (LLMs). The method is grounded in Sparse Autoencoders (SAEs), which are used to identify language-specific feature. An auxiliary loss is introduced during supervised fine-tuning to suppress pre-activation values of irrelevant language features, thereby reducing unexpected code-switching. The method is evaluated on five multilingual LLMs across six benchmarks and three code-switching target languages, and shows strong reduction in code-switching rates, while mostly maintaining or improving performance on standard benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses an underexplored yet practically important problem, unexpected code-switching, which impacts user experience and model usability.\n\n2. Demonstrates consistent reductions in code-switching across various models and languages, outperforming previous methods (e.g., GRPO) in most settings.\n\n3. The paper offers detailed analysis on factors such as layer depth and feature selection.\n\n4. The paper is clearly written and easy to follow."}, "weaknesses": {"value": "1. The method for identifying language-specific features relies on rankings without justification. This introduces sensitivity to hyperparameter selection and limits the interpretability of the results, especially in multilingual settings where features may vary across tasks.\n\n2. The paper reports a substantial +327% increase in Korean code-switching under the GRPO method, but does not provide sufficient explanation for this anomaly. A deeper analysis is needed to clarify the cause of such a drastic change.\n\n3. It remains unclear why SASFT underperforms on certain benchmarks, such as MMLU and HellaSwag, for the Qwen3-8B model. This raises questions about robustness and generalizability across tasks."}, "questions": {"value": "1. How does the proposed method perform on multilingual mathematical reasoning tasks, such as MGSM？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5uvkS4klTl", "forum": "BQOFU9qO5j", "replyto": "BQOFU9qO5j", "signatures": ["ICLR.cc/2026/Conference/Submission12216/Reviewer_iweR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12216/Reviewer_iweR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894746116, "cdate": 1761894746116, "tmdate": 1762923163132, "mdate": 1762923163132, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of unexpected code-switching in multilingual large language models (LLMs). The authors first employ Sparse Autoencoders (SAEs) to analyze the internal representations of LLMs and discover that unexpected language switches are correlated with over-activation of target-language-specific features. Based on this finding, they propose a novel fine-tuning method, SASFT (Sparse Autoencoder-guided Supervised Finetuning), which introduces an auxiliary loss term during supervised fine-tuning to constrain the pre-activation values of specific language features below a threshold, thereby reducing activations of irrelevant languages. Experiments on five models and three target languages (Chinese, Russian, and Korean) show an average >50% reduction in unexpected switching, including four cases of complete elimination, while maintaining or improving performance on six multilingual benchmarks.\n\nMain Contributions:\n- Using sparse autoencoders, the paper reveals that when a model is about to unexpectedly switch to language L, the language-specific features of L show significantly elevated pre-activation values in the residual stream.\n- Proposes SASFT, a training-stage approach that suppresses the activation of irrelevant language features without requiring inference-time intervention.\n- Provides comprehensive experiments demonstrating that SASFT is effective, robust, and preserves multilingual capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a Sparse Autoencoder-guided Supervised Finetuning (SASFT) approach that combines sparse autoencoders with supervised fine-tuning. \n2. The effectiveness of SASFT is validated through extensive experiments across multiple language pairs and model families. The results demonstrate consistent mitigation of code-switching phenomena in diverse multilingual settings.\n3. Experimental evidence indicates that the proposed method effectively reduces unintended language switches, thereby improving the accuracy, consistency, and usability of multilingual model outputs."}, "weaknesses": {"value": "1. Limited Baseline Comparison：The paper only compares SASFT with GRPO, which, although relevant, is insufficient to establish the method’s relative advantage.\n2. Lack of Mechanistic or Causal Analysis：While the paper empirically observes that the pre-activation values of target-language features increase prior to code-switching and validates this via directional ablation, this evidence remains correlational. The work does not provide a mechanistic explanation of why such activation leads to language switching, nor tests the causal hypothesis.\n3. Potential Model-Specific Bias. The reported improvements may partially stem from inherent differences in model multilingual balance, rather than from SASFT’s general efficacy. The paper does not control for or analyze how such model-specific language priors influence the observed reductions in code-switching."}, "questions": {"value": "1. Could the authors provide a more comprehensive comparison to strengthen the empirical validity of SASFT?\n2. The paper suggests that increased pre-activation of language-specific features precedes unexpected code-switching. Have the authors examined whether artificially increasing the activation of a non-target language feature can induce code-switching?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "82WiIoR9Cn", "forum": "BQOFU9qO5j", "replyto": "BQOFU9qO5j", "signatures": ["ICLR.cc/2026/Conference/Submission12216/Reviewer_fhn1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12216/Reviewer_fhn1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932878295, "cdate": 1761932878295, "tmdate": 1762923162249, "mdate": 1762923162249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies unexpected code-switching in multilingual LLMs and links the phenomenon to unusually high pre-activation on language-specific SAE features. Building on this observation, it proposes SASFT: add an auxiliary loss during SFT that penalizes pre-activation of irrelevant language features across several layers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear story from SAE analysis to a concrete training modification.\n2.  The results are good to show the proposed method's effectiveness.\n3. The paper is well-writen and easy to follow."}, "weaknesses": {"value": "1. GRPO is run with only 10k samples (1k per language). That seems light for a control-behavior objective. Consider stronger RL baselines , or simple supervised baselines that directly penalize language-ID tokens . Without stronger baselines, it’s hard to attribute gains purely to SASFT. Does the author could ensure the GRPO have true convergence?\n2. The study focuses on zh/ru/ko. It’s unclear if results hold for low-resource scripts (e.g., Amharic, Khmer), closely-related Latin languages where CS is subtler (es/pt/fr/it)? \n3. SASFT relies on high-quality SAEs and on accurate language-feature identification; for Qwen the authors train their own SAEs, for others they reuse published ones. How sensitive are results to (1)SAE training corpora, dimensionality, sparsity target? (2)Which layer(s) the features are extracted from? (3) Feature drift after fine-tuning (do features stay monosemantic)?"}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tf6cXBY8qM", "forum": "BQOFU9qO5j", "replyto": "BQOFU9qO5j", "signatures": ["ICLR.cc/2026/Conference/Submission12216/Reviewer_dtNW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12216/Reviewer_dtNW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762784687764, "cdate": 1762784687764, "tmdate": 1762923161884, "mdate": 1762923161884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}