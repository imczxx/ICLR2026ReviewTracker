{"id": "6rCsaBOQON", "number": 18864, "cdate": 1758291589849, "mdate": 1759897076735, "content": {"title": "FedCF: Fair Federated Conformal Prediction", "abstract": "Conformal Prediction (CP) is a widely used technique for quantifying uncertainty in machine learning models. In its standard form, CP offers probabilistic guarantees on the coverage of the true label, but it is agnostic to sensitive attributes in the dataset. Several recent works have sought to incorporate fairness into CP by ensuring conditional coverage guarantees across different subgroups. One such method is Conformal Fairness (CF). In this work, we extend the CF framework to the Federated Learning setting and discuss how we can audit a federated model for fairness by analyzing the fairness-related gaps for different demographic groups. We empirically validate our framework by conducting experiments on several datasets spanning multiple domains, fully leveraging the exchangeability assumption.", "tldr": "", "keywords": ["Fairness", "Conformal Prediction", "Federated Learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/43cb4c29e06a7736ebfed01968506cfdbffcfecc.pdf", "supplementary_material": "/attachment/e9b5bbfceeb28da69831642738ba48dd433790be.zip"}, "replies": [{"content": {"summary": {"value": "This paper extends the Conformal Fairness (CF) framework [1] to federated learning (FL).  The paper suggests the framework provides a way to “audit” the fairness of FL models.  The extension relies on bounding the coverage of the conformal prediction set as a sum of statistics generated by the clients.  The server does a gradient search for the parameter, lambda, (size of the prediction set) that minimizes the squared error between the model’s coverage gap and a desired closeness level (c).  This threshold is then used by all clients in making their predictions.  The FedCF extension is evaluated on 4 datasets using APS, DAPS, and RPAS conformity measures.  Experimental results verify that FedCF can tradeoff fairness disparity against prediction set size in a manner that is plausible for use in auditing the fairness of FL models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The proposed extension of conformal fairness to the federated setting is interesting and timely.  \n\n+ Use of gradient search for coverage parameter, lambda, is an improvement over what was done in the original CF paper [1]."}, "weaknesses": {"value": "-\tThe bound on the coverage gap in Thm 3.1 is essential for the paper’s federated extension, but there are no results (experimental or theoretical) estimating how tight the bound is.  One would always expect the bounds obtained in a distributed setting to be worse than those that could be obtained using all the available information.  It is important to know how much one loses in estimating the coverage gap in this manner.\n\n-\tThe paper is missing any meaningful discussion of model performance.  Based on my reading of the paper, FedCF is a post-processing approach to achieving fairness in FL in which all clients send statistics to the server from which the coverage threshold, lambda, is selected and that threshold is then used by all clients.  Essentially, we achieve fairness by assigning positive outcomes to less qualified individuals, until the target for fairness disparity has been achieved.  This comes at a cost of reducing the model’s performance (accuracy, f-score, whatever) on the entire population.  So the paper’s results and the “audit” are of limited value if one can’t assess how much performance is lost in achieving the fairness objective.   Moreover, it should be possible to estimate what the “optimal” performance would be for a given lambda.  It would be good to see how FedCF performance fairs with respect to this optimal cost."}, "questions": {"value": "Questions:\n\n1)\tHow tight is the bound in Thm 3.1 (experimental or theoretical)?\n\n2)\tHow does model performance decrease with reducing fairness disparity?  How does this achieved performance compare to what is optimally achievable either in a centralized or federated setting? \n\n3)\tWhile this paper focuses on its extension of the CF framework in [1], I was also curious about how it differed from the Federated Conformal Prediction framework in [2]?  \n\nAdditional Remarks:\n\n1)\tI thought the writing was a bit disorganized, since I felt I had to hunt through the paper to understand how FedCF enforces fairness across the clients.  The paper might benefit from some reorganization in the introduction and background sections in a way that builds up more systematically to the FedCF algorithm.\n\n2)\tThe paper needs to address the model performance lost when the clients use the prediction set threshold determined by the server.   \n\n3)\tThe predict set threshold is used by all clients.  I think it might be useful to examine whether the set thresholds should be client specific.  I suspect this can be one way of recovering model performance lost by having all clients use the same threshold.\n\nReferences:\n\n[1] Vadlamani, et al. “A generic framework for conformal fairness”, ICLR 2025\n\n[2] Lu, et al., ”Federated conformal predictors for distributed uncertainty quantification”, ICML 2023"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wZbtkyLJfY", "forum": "6rCsaBOQON", "replyto": "6rCsaBOQON", "signatures": ["ICLR.cc/2026/Conference/Submission18864/Reviewer_EQi5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18864/Reviewer_EQi5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856771593, "cdate": 1761856771593, "tmdate": 1762930830612, "mdate": 1762930830612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explore fair federated learning by conformal prediction, which is an extension of conformal fairness to the federated learning. They audit a federated model for fairness by analyzing the fairness-related gaps for different demographic groups. Numerical experiments is rich."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors introduce a descent-based conformal fairness optimization to solve fair federated learning problems. \n- The authors provide a criterion to update learning rate in algorithm.\n- The writing is good."}, "weaknesses": {"value": "- There may be some mistakes in the core algorithm--descent based CF optimization (see questions).\n- Numerical results are not good enough and the efficiency (the average prediction set size is large).\n- This paper is an intersection of conformal federated learning and conformal  fairness algorithms?"}, "questions": {"value": "- Descent-based CF optimization, descent or ascent? You are from a very small $\\lambda_0$? and $b_{t+1}$ should be positive. Then Lines 10 and 14 is a gradient ascent with momentum.\n- Line 8 of Algorithm 1, do you need to return $\\lambda_{opt}$ here? If not, $\\Delta \\lambda$ may be negative. Are there some typos in Line 11 of Algorithm 1 to ensure $\\Delta \\lambda$ not to be negative.\n- I did not understand the dots or dotted line in Figure 1.\n- The efficiency is bad compared with baseline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rXaY1h1W68", "forum": "6rCsaBOQON", "replyto": "6rCsaBOQON", "signatures": ["ICLR.cc/2026/Conference/Submission18864/Reviewer_py2j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18864/Reviewer_py2j"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911576668, "cdate": 1761911576668, "tmdate": 1762930830162, "mdate": 1762930830162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors introduce the Federated Conformal Fairness (FedCF) approach, which extends the Conformal Fairness framework to a federated setting. The paper introduces two communication protocols that allow for a balance between communication overhead and privacy.\nAuthors conduct experiments on data from different domains, namely tabular, graph, and image tasks, and demonstrate that FedCF can control fairness disparities with acceptable trade-offs in set size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Overall, the paper is easy to read, and the idea is clear.\n\n2. The evaluation of the models is reasonable, and the choice of experiments is diverse (but see Weaknesses)."}, "weaknesses": {"value": "Some of the points below are not necessary weaknesses, but rather questions that I did not understand during review. Can the authors kindly address them?\n\n1. In experiments, only datasets with a relatively small number of clients are used (at most 51 [line 313]). In practice, however, there could be millions of clients. Even if the performance of the proposed framework is reasonable for the moderate number of clients, it could potentially degrade with K. I think an experiment on a dataset with a large number of clients (e.g., Stack Overflow) can resolve this issue.\n\n2. Another practical challenge for federated learning is partial participation, when clients can be unresponsive from one communication round to another. However, the algorithm appears not to consider this situation. Will this situation create any challenges for the framework? Could this lead to bias?\n\n3. As an additional limitation of experiments, datasets were partitioned to create clients rather than collected in a native FL way. Therefore, the evaluation lacks real-world heterogeneity.\n\n4. This is rather a question. Currently, the approach tunes a single global $\\ lambda$ for all clients, which may be inefficient given the real-world heterogeneity among clients. Can one incorporate multiple thresholds (per client or per group of clients) into the framework?\n\n5. In practice, new clients can join the distributed system with their new local datasets. How can the FedCF adopt them?"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RB1j9OUyyq", "forum": "6rCsaBOQON", "replyto": "6rCsaBOQON", "signatures": ["ICLR.cc/2026/Conference/Submission18864/Reviewer_jdor"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18864/Reviewer_jdor"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920198036, "cdate": 1761920198036, "tmdate": 1762930829387, "mdate": 1762930829387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FedCF, a framework that brings conformal fairness into federated learning by using coverage decomposition along with two communication protocols. Its main contribution is in adapting and integrating existing conformal-fairness methods for use in federated settings, while offering relatively limited novel algorithmic advances."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Fair federated learning is an important and timely research direction given the increasing deployment of ML systems in sensitive domains.\n \n2. The experimental results preliminarily demonstrate that the proposed method can control fairness disparities within specified bounds."}, "weaknesses": {"value": "1. The work primarily combines existing methods without substantial algorithmic innovation. While the extension of conformal fairness to federated settings is reasonable, the approach is relatively straightforward and lacks deeper theoretical understanding.\n \n2. The experimental setup is quite far from a realistic federated setting. Since the datasets are split artificially rather than coming from natural sources, the evaluation may fail to reflect the true data heterogeneity that is central to federated learning.\n \n3. The exchangeability assumption across clients creates tension with federated learning principles. Since federated learning is specifically designed to handle heterogeneous, non-IID data distributions, this assumption potentially undermines the method's applicability in practice.\n \n4. Communication costs scale unfavorably with the number of sensitive attributes, particularly for the privacy-preserving variant. This scalability concern may limit practical deployment in settings with multiple fairness considerations.\n \n5. The evaluation lacks comparison with alternative federated fairness approaches, making it difficult to assess relative performance. Additionally, the efficiency degradation observed with fairness constraints, especially using interval bounds, triggers concerns about practical utility."}, "questions": {"value": "1. How does the method handle data heterogeneity in realistic federated environments where the exchangeability assumption may not hold?\n \n2. Can you provide a detailed analysis of communication costs and scalability when the number of clients and sensitive attributes increases significantly?\n \n3. How does FedCF compare against other existing federated fairness approaches in terms of both fairness guarantees and computational efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9uIDu89wIJ", "forum": "6rCsaBOQON", "replyto": "6rCsaBOQON", "signatures": ["ICLR.cc/2026/Conference/Submission18864/Reviewer_LJua"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18864/Reviewer_LJua"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994583857, "cdate": 1761994583857, "tmdate": 1762930828532, "mdate": 1762930828532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}