{"id": "VbmV3rs284", "number": 7512, "cdate": 1758025505721, "mdate": 1759897848552, "content": {"title": "Fed-DIP: Federated Domain Generalization by Synergizing Implicit Disentanglement and Context-Aware Prompting", "abstract": "Federated Domain Generalization (FedDG) seeks to train a model on decentralized data from multiple source domains that can generalize effectively to unseen target domains. A fundamental challenge lies in achieving robust feature disentanglement—separating domain-invariant from domain-specific features—which is critical for generalization but severely hindered by the data-isolated nature of Federated Learning. Existing methods often struggle with this, leading to incomplete decoupling and limited model performance. To address this, we propose Fed-DIP, a novel framework that introduces an Implicit Decoupling Distillation mechanism. This mechanism achieves fine-grained feature separation by comparing logit outputs for local image regions, all without direct data access. This allows for the robust aggregation of domain-invariant knowledge while critically preserving rich, domain-specific information at the client side. Furthermore, to unlock the potential of this preserved local knowledge, we introduce the Context-Aware Prompt Encoder (CAPE). Unlike prior works that rely on selective prompting from a fixed set, CAPE is a fully generative solution. It dynamically synthesizes adaptive, end-to-end optimizable text prompts directly from local visual features. These generated prompts provide nuanced, contextual guidance, enabling the model to effectively leverage domain-specific insights for more robust and accurate decision-making. Extensive experiments on benchmarks including PACS, VLCS, OfficeHome, and DomainNet demonstrate that our method achieves state-of-the-art (SOTA) performance, validating the effectiveness and superiority of our framework.", "tldr": "", "keywords": ["Federated Learning Domain Generalization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4a44ec7fafff5b91f83f63d6f4584241469d8bdd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces FedDIP, a federated learning framework designed for domain generalization. It integrates two key modules: Multi-scale Implicit Decoupling Distillation (MIDD), which aligns intermediate features across clients to promote domain-invariant representation learning, and Context-Aware Prompt Encoder (CAPE), which leverages prompt-based conditioning to adapt local models to unseen domains. The method is evaluated on standard benchmarks (e.g., PACS, Office-Home) and demonstrates improved performance over existing approaches."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Combining multi-scale feature distillation (MIDD) with prompt-based adaptation (CAPE) is an interesting strategy for enhancing domain generalization in federated settings"}, "weaknesses": {"value": "1. The interaction between MIDD and CAPE is not deeply analyzed, making it hard to assess whether their effects are complementary or redundant.\n2. Theoretical justification for why multi-scale feature alignment leads to better domain invariance is limited and could be strengthened."}, "questions": {"value": "1. The proposed framework assumes that aligning multi-scale intermediate features (via MIDD) leads to domain-invariant representations, but this assumption lacks formal justification. Given that such features may encode both domain-specific and task-relevant information, how does the method avoid suppressing useful signals? Furthermore, is there empirical evidence that combining MIDD with CAPE results in a synergistic effect, rather than redundant or overlapping contributions?\n2. How sensitive is the performance to the scale levels used in MIDD? Would using fewer or different layers (e.g., early vs. late) change the effectiveness of the distillation?\n3. Since CAPE introduces prompt tokens into the encoder, how does its adaptation generalize to completely unseen domains with different semantic distributions?\n4. Has the method been tested under heterogeneous data distributions (non-IID) across clients? If so, how robust is the framework to extreme distributional shifts?\n5. Can the proposed method scale to larger federated settings (e.g., dozens of clients, varying compute) without compromising communication or convergence efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZEba1Kb7cy", "forum": "VbmV3rs284", "replyto": "VbmV3rs284", "signatures": ["ICLR.cc/2026/Conference/Submission7512/Reviewer_Knga"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7512/Reviewer_Knga"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761695105009, "cdate": 1761695105009, "tmdate": 1762919619356, "mdate": 1762919619356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Fed-DIP, a framework for Federated Domain Generalization (FedDG) that aims to train models across multiple source domains without sharing raw data, while maintaining strong generalization to unseen target domains. The method introduces two key components: (1) Multi-scale Implicit Decoupling Distillation (MIDD), which performs logit-level, multi-scale knowledge distillation between the global and local models to implicitly separate domain-invariant and domain-specific features under data isolation; and (2) Context-Aware Prompt Encoder (CAPE), which generates adaptive text prompts from local visual features to enhance domain-specific representation. Built on a ViT-CLIP backbone with lightweight adapters, Fed-DIP achieves new state-of-the-art results on standard benchmarks including PACS, OfficeHome, VLCS, and DomainNet."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper is well written and easy to follow. The motivation, problem setting, and overall framework are clearly introduced, and the methodology is explained with sufficient clarity and structure. Figures and equations effectively illustrate the core ideas, making the technical flow easy to understand;\n2.The proposed method achieves consistently strong results across multiple standard FedDG benchmarks, including PACS, OfficeHome, VLCS, and DomainNet. It outperforms several recent state-of-the-art approaches such as FedMaPLe, FedPR, and PLAN, demonstrating both the effectiveness and robustness of the proposed framework."}, "weaknesses": {"value": "1.Although the paper claims to achieve disentanglement between domain-invariant and domain-specific representations through the MIDD module, there is no explicit experiment or visualization to validate this claim. The authors mainly report classification accuracy, but they do not provide a quantitative analysis of whether the extracted features actually exhibit domain invariance or reduced domain discrepancy. It would significantly strengthen the paper if the authors could include an additional experiment similar to the “Domain discrepancy of extracted features” analysis in ALOFT [1], where inter-domain feature distances are measured to quantitatively evaluate feature invariance; \n[1] Guo, Jintao, Na Wang, Lei Qi, and Yinghuan Shi. \"Aloft: A lightweight mlp-like architecture with dynamic low-frequency transform for domain generalization.\" In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 24132-24141. 2023.\n2.In Table 3, the full model and its ablated variants show inconsistent behavior across datasets. Specifically, the fourth row, which includes more components, performs worse than the third-row configuration on the OfficeHome dataset (84.97% vs. 82.43%), despite having additional modules that are expected to improve performance. The authors should provide further analysis or explanation to clarify why this degradation occurs and how the interaction between these modules affects model generalization;\n3.Since the proposed framework introduces additional adapter modules into each Transformer block, it would be helpful to report the corresponding computational overhead or communication cost."}, "questions": {"value": "The paper would benefit from further clarification on several points raised in the weaknesses. In particular, it remains unclear how the proposed framework truly achieves feature disentanglement, since no direct quantitative or visual evidence is provided. The authors are encouraged to include experiments similar to the “Domain discrepancy of extracted features” analysis in ALOFT [1] to validate the claimed invariance. Additionally, the inconsistent results observed in the ablation study and the lack of computational cost analysis deserve further explanation. A more detailed discussion of these aspects would make the paper’s contributions and claims more convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "q2WHbEbjRS", "forum": "VbmV3rs284", "replyto": "VbmV3rs284", "signatures": ["ICLR.cc/2026/Conference/Submission7512/Reviewer_oWjg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7512/Reviewer_oWjg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823859260, "cdate": 1761823859260, "tmdate": 1762919618890, "mdate": 1762919618890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework for federated domain generalization, called Fed-DIP, integrates implicit feature disentanglement and context-aware prompt generation. Its dual-adapter design enables the model to separate and utilize both domain-invariant and domain-specific knowledge without raw data sharing. Extensive experiments on standard benchmarks demonstrate SOTA generalization performance across unseen domains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The paper introduces combination of implicit feature disentanglement and context-aware generative prompting for federated domain generalization task.\n\n(2) The proposed Fed-DIP demonstrates consistent SOTA performance across multiple DG benchmarks."}, "weaknesses": {"value": "(1) Related works section is very poor. No discussion of current prompt learning based Federated VLM methods. Lots of works need to be cited, [1]-[6]\n\n(2) $\\beta$ is missing in eq. 15.\n\n(3) Questions are asked below.\n\n\n[1] Global and Local Prompts Cooperation via Optimal Transport for Federated Learning, CVPR 2024\n\n[2] FedMVP: Federated Multi-modal Visual Prompt Tuning for Vision-Language Models, ICCV 2025\n\n[3] Federated Text-driven Prompt Generation for Vision-Language Models, ICLR 2024\n\n[4] Harmonizing Generalization and Personalization in Federated Prompt Learning, ICML 2024\n\n[5] FedPHA: Federated Prompt Learning for Heterogeneous Client Adaptation, ICML 2025\n\n[6] Mixture of Experts Made Personalized: Federated Prompt Learning for Vision-Language Models, ICLR 2025"}, "questions": {"value": "(1) How are the generated prompts through visual guided text adapter, integrated with class tokens, as the total input tokens in CLIP are limited?\n\n(2) Why does MIDD consider multiple level of scales? Is the multi-scale distillation truly improves disentanglement compared to single-scale or does it overfit?\n\n(3) What is the effect of $\\lambda$ in eq. 7 ? Why is it not considered in the ablation study of section 5.2?\n\n(4) Current works like FedOTP [1], FedMVP [2], FedPHA [3] should be considered as baselines. \n\n\n[1] Global and Local Prompts Cooperation via Optimal Transport for Federated Learning, CVPR 2024\n\n[2] FedMVP: Federated Multi-modal Visual Prompt Tuning for Vision-Language Models, ICCV 2025\n\n[3] FedPHA: Federated Prompt Learning for Heterogeneous Client Adaptation, ICML 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CXFM5QVKqM", "forum": "VbmV3rs284", "replyto": "VbmV3rs284", "signatures": ["ICLR.cc/2026/Conference/Submission7512/Reviewer_PwF6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7512/Reviewer_PwF6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762120053431, "cdate": 1762120053431, "tmdate": 1762919618551, "mdate": 1762919618551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Fed-DIP (Federated Domain Generalization via Implicit Disentangled Distillation and Generative Prompting), a framework that enhances the generalization ability of vision-language models (VLMs) under federated non-IID settings. The approach has two main components: (1) Multi-scale Implicit Disentangled Distillation (MIDD), which separates global and local knowledge by aligning teacher-student logits at multiple spatial scales without exchanging raw data, and (2) Context-Aware Prompt Encoder (CAPE), which dynamically generates text-side prompts conditioned on instance-level visual context. Through selective gradient routing, the domain-invariant adapter $A_{di}$ is optimized toward shared global consensus, while the domain-specific adapter $A_{ds}$ captures local domain characteristics that are not aggregated. The proposed method is evaluated on PACS, VLCS, OfficeHome, and DomainNet, showing consistent improvements over existing federated domain generalization and prompt-based methods, including FedCLIP, PromptFL, and FedAPT. Ablation and sensitivity analyses further highlight the complementary roles of MIDD and CAPE and demonstrate favorable trade-offs between performance and communication efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Conceptual novelty: The method unifies two important directions, federated domain generalization and prompt-based VLM adaptation, in a coherent framework.\n\n- Algorithmic clarity in high level design: The MIDD component provides a clean way to separate global and local knowledge without data sharing, while CAPE dynamically conditions textual prompts on visual context.\n\n- Strong empirical results: The model consistently outperforms both CNN-based and ViT-based federated DG methods on four benchmarks.\n\n- Efficiency: Only lightweight adapters are communicated, achieving competitive results with significantly reduced communication cost."}, "weaknesses": {"value": "- Missing formal definitions: The similarity weight $\\gamma(s,i)$ is never formally defined. Its computation method must be clearly specified.\n- Inconsistency in loss scheduling: The description of the loss weight $\\lambda$ scheduling and the omission of the coefficient $\\beta$ in the total loss equation create confusion about the actual implementation.\n- Teacher mechanism ambiguity: The paper mixes two implementations (local teacher vs. server-broadcasted logits), leading to ambiguity about where and how teacher logits are computed.\n- Communication cost mismatch: Reported values (2.019 MB vs 3.399 MB per round) conflict across text and figures, undermining reproducibility of the claimed efficiency.\n- CAPE under-specified: The number of prompt tokens, their layer-wise injection strategy, and whether prompts are shared across classes are not detailed, which limits reproducibility.\n- Notation and readability: Several key symbols are undefined at first use; a concise notation table would improve clarity. \n- Related work coverage: The paper omits foundational prompt-learning works such as *CoOp* [1], *CoCoOp* [2], and *ProDA* [3], which are essential for positioning CAPE’s generative contribution.\n- Privacy discussion missing: The logit exchange step introduces potential information leakage that is not acknowledged or mitigated. Discussion of possible privacy-preserving mechanisms would be valuable.\n\n[1] Zhou, Kaiyang, et al. \"Learning to prompt for vision-language models.\" International Journal of Computer Vision 130.9 (2022): 2337-2348.\n\n[2] Zhou, Kaiyang, et al. \"Conditional prompt learning for vision-language models.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n\n[3] Lu, Yuning, et al. \"Prompt distribution learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022."}, "questions": {"value": "1. How exactly is $\\gamma(s,i)$ computed and normalized? Which distance metric is used, and is temperature scaling applied?\n    \n2. Does the complementary KL term risk divergence when teacher and student outputs are nearly orthogonal? What stabilizers (e.g., entropy regularization or clipping) are employed?\n    \n3. Is the teacher model computed locally from the previous global parameters, or are averaged logits broadcast by the server? Please provide a single authoritative description.\n\n4. How are the text prompts injected into CLIP’s text transformer? Are they concatenated as prefixes or inserted between class tokens?\n    \n5. Can the authors clarify the discrepancy in communication cost numbers and provide the exact accounting of each component?\n    \n6. Was any privacy-preserving mechanism (e.g., differential privacy, quantization, or noise) applied to the transmitted logits?\n\nPlease refer to the Weaknesses section for the remaining questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jQaiSE7Srj", "forum": "VbmV3rs284", "replyto": "VbmV3rs284", "signatures": ["ICLR.cc/2026/Conference/Submission7512/Reviewer_oTKo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7512/Reviewer_oTKo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762621100082, "cdate": 1762621100082, "tmdate": 1762919618221, "mdate": 1762919618221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}