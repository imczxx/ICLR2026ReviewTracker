{"id": "I4jBCglUOI", "number": 9992, "cdate": 1758154786255, "mdate": 1759897681533, "content": {"title": "Self-Discriminative Optimization for Video Diffusion Models", "abstract": "Recent preference alignment strategies have gained traction in large language models (LLMs) and are now being extended to broader generative domains. Approaches such as Direct Preference Optimization have been adapted to diffusion models by leveraging human-labeled preferences or auxiliary score models to distinguish ``winner'' from ``loser''. \nHowever, these methods face two key challenges: (1) the optimization process often overfits to the score model, resulting in suboptimal generation quality; and (2) the results generated from the same text prompt exhibit significant divergence, resulting in limited effective gradients and reduced training efficiency. These limitations are further exacerbated in video generation, where evaluation is more complex and inference is slower. In this work, we introduce Self-Discriminative Optimization that using only a handful of real samples, unlocks markedly higher-quality generation. First, we introduce self-degradation that applies frequency-domain reweighting to the latent representations from real samples, yielding degraded samples that more closely match the model’s original output distribution. This leads to controlled distortions such as low-quality, temporal inconsistency and object deformation.\nWe then use these real/degraded pairs as positive and negative examples to fine-tune the pretrained model discriminatively with automatically assigned, reliable labels. \nBy exploiting the richer gradients from these controllable degradation pairs, our experiments demonstrate substantial gains in structural quality and semantic alignment using only a handful of high-quality samples and minimal fine-tuning.", "tldr": "", "keywords": ["Video Generation; Post-training; Diffusion Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/deae97b1b8e513cf3e2038480518d79b6d7533c5.pdf", "supplementary_material": "/attachment/4a0027ff1916872daebb868d488c775154c3baaa.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Self-Discriminative Optimization (SDO), a new post-training method for video diffusion models that improves quality without human labels or reward models.\nSDO introduces self-degradation, perturbing real video latents in the frequency domain to create controlled “degraded” samples that resemble low-quality generations. The model is then fine-tuned to distinguish real vs. degraded pairs, providing a stable, label-free supervision signal.\nApplied to CogVideoX-2B/5B, SDO improves structural fidelity, motion consistency, and semantic alignment with minimal data (≈2k videos) and 500 LoRA steps.\nCompared to LoRA tuning, DPO, and DDO, it achieves higher VBench and lower FVD/KVD scores, showing better temporal smoothness and fewer artifacts.\nThe method is efficient, avoids overfitting to score models, and stabilizes optimization, though its performance depends on the diversity of fine-tuning data."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Proposes a novel label-free optimization method that learns from real videos and their degraded counterparts generated via the diffusion model itself. This design eliminates the need for human annotations, significantly reducing alignment cost while providing reliable supervision.\n- Demonstrates consistent improvements across different model scales (CogVideoX-2B and 5B), showing the method’s scalability and general applicability to various diffusion backbones."}, "weaknesses": {"value": "- The paper provides almost no quantitative results, and the evaluation relies mainly on classical metrics such as FVD, which are poorly correlated with human perceptual quality and therefore lack persuasiveness for assessing modern video generation models [1]. The only relatively new metric used is VBench, but only the total score is reported. Reporting only the total VBench score is insufficient because it is biased toward temporal consistency, meaning that results that sacrifice dynamics can still achieve deceptively high scores [2], [3], [4]. Therefore, please present all individual VBench metric values.\n- The absence of any human evaluation further weakens the argument. Based on the reported quantitative results, the performance gains appear marginal from a human perception standpoint. To address this concern, the authors should conduct a Gold Human pairwise comparison experiment against prior methods, as this has become the standard in recent video generation papers [5], [6], [7] [8].\n- The choice of base models is also insufficient. The paper does not discuss whether the proposed method could yield performance gains on state-of-the-art video diffusion models, such as Wan 2.1 or HunyuanVideo. Without such discussion or experiments, it remains unclear whether the method is effective and scalable to the latest, large-scale models, limiting the generality and practical significance of the results.  \n- Finally, the ablation studies are far from adequate. While it is clear that FFT is used to decompose frequency components and assign weights, the paper does not discuss the thresholding strategy. In addition, does this method actually achieve better performance efficiency in terms of training steps or computation cost, compared to prior methods? Moreover, it would be valuable to analyze not only the overall generation quality measured by benchmarks, but also how the proposed alignment method affects generation diversity.\n\n[1] Ge, et al. On the Content Bias in Fr ́echet Video Distance. CVPR2024.  \n[2] Liao, et al. Evaluation of Text-to-Video Generation Models: A Dynamics Perspective. NeurIPS2024.  \n[3] Liu, et al. VideoDPO: Omni-Preference Alignment for Video Diffusion Generation. CVPR2025.  \n[4] Oshima, et al. Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search. NeurIPS2025.  \n[5] Wu, et al. Boosting Text-to-Video Generative Model with MLLMs Feedback. NeurIPS2024.  \n[6] Hila, et al. VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion\nGeneration in Video Models. ICML2025.  \n[7] Shaulov, et al. FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video Generation. NeurIPS2025.  \n[8] Wu, et al. DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models. NeurIPS2025."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R7czfKxzqx", "forum": "I4jBCglUOI", "replyto": "I4jBCglUOI", "signatures": ["ICLR.cc/2026/Conference/Submission9992/Reviewer_mPBn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9992/Reviewer_mPBn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792115965, "cdate": 1761792115965, "tmdate": 1762921421106, "mdate": 1762921421106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Self-Discriminative Optimization that leverages human-designed degradation schemes to as low-quality negative samples. In comparison, real data are used as positive samples. The samples are then used as the negative/positive pair for optimizing DPO objective. The entire training process does not require human label and can be more easily implemented in large scale. In addition, the paper proposes diffusion time-dependent $\\beta(t)$ to better optimize DPO objective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The presentation is clear, and the method is clearly understandable.\n- The presented method is simple and compatible with most video generative models."}, "weaknesses": {"value": "- It is unclear what data the model is trained on. If SDO is trained on significantly better data than baseline such as VideoDPO whose score is reported from the original paper, then the gain may not be entirely due to the method itself.\n\n- Why for CogV-5B there is no DPO baseline? Since the novelty of the work is on how one obtains the positive/negative pairs, it would be useful to ablate by swapping this with the default process DPO obtains the positive/negative pairs (human ratings).\n\n- Scores like VBench are still not so reliable as a quality metric. How does the performance increase correlate with human ratings? For example, does performance increase in VBench mean that SDO perform better in human user study?"}, "questions": {"value": "I would like the author to address my concerns above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HcGVyR8hrz", "forum": "I4jBCglUOI", "replyto": "I4jBCglUOI", "signatures": ["ICLR.cc/2026/Conference/Submission9992/Reviewer_LMu8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9992/Reviewer_LMu8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803608352, "cdate": 1761803608352, "tmdate": 1762921420678, "mdate": 1762921420678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Self-Discriminative Optimization (SDO). It’s a label-free preference alignment framework for T2V diffusion models. Unlike DPO or DDO, which rely on human annotations or unpaired data, SDO constructs paired supervision automatically by introducing a self-degradation mechanism. The method perturbs real video samples in the high frequency domain to produce degraded counterparts while maintaining semantic correlation. The model is then fine-tuned to distinguish between real and degraded samples. Experiments on CogVideoX demonstrate improvements in VBench, FVD, and temporal consistency metrics using only a small number of high-quality samples."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis paper introduces the self-degradation, which eliminates the need for human annotations or additional score models.\n2.\tThe authors clearly identify two key challenges in prior alignment methods: overfitting to score models and limited gradient signal.\n3.\tResults on CogVideoX-2B/5B show notable gains over both DDO and VideoDPO, particularly in temporal consistency and low-level structure preservation."}, "weaknesses": {"value": "1.\tThe effectiveness of frequency-domain reweighting has only been empirically demonstrated, lacking theoretical support or validation through human evaluation experiments. High-frequency information often contains edge and texture details; therefore, simple reweighting operations are not equivalent to “low quality.”\n2.\tAddressing the gradient problems is an important motivation of this paper. The authors claim that the timestep-aware β(t) improves stability, but they provide no mathematical or visual explanation, nor do they show its actual impact on the optimization process (e.g., loss curves or gradient norms). The core design of the paper, self-degradation, also appears to have no clear connection to this motivation.\n3.\tSDO use the fixed empirical values for hyper-parameters K and w. However, the frequency distribution of different videos varies greatly, so fixed K and w cannot adapt to such variations.\n4.\tThe experimental setup is limited. The paper compares SDO primarily against DDO and VideoDPO. It would be better to compare to other baselines, such as RLHF-based approaches. Besides, the results of ablation study are not presented clearly."}, "questions": {"value": "1.\tCould you offer some theory analysis to validate the effectiveness of frequency-domain reweighting? Or could you provide some experimental results to prove the alignment between frequency-domain reweighting and human evaluation?\n2.\tWhat specific effects does the timestep-aware adaptive scaling factor have on the optimization process? Is there any visual evidence to support it?\n3.\tWhy the discriminative signal between real and self-degraded pairs provides better gradients than unpaired DDO? Could you provide some theoretical discussion?\n4.\tIf the 3DFFT is replaced with other frequency analysis methods, what impact will it have on the final performance? What’s the advantage of 3DFFT over other methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "trwp7gaMTs", "forum": "I4jBCglUOI", "replyto": "I4jBCglUOI", "signatures": ["ICLR.cc/2026/Conference/Submission9992/Reviewer_zRx3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9992/Reviewer_zRx3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849619868, "cdate": 1761849619868, "tmdate": 1762921419918, "mdate": 1762921419918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Self-Discriminative Optimization (SDO) to fine-tune video diffusion models. SDO first yielding degrades real samples by frequency-domain reweighting, and then uses these real/degraded pairs as\npositive and negative examples of DPO to optimize vido diffusion models. SDO demonstrate substantial gains\nin structural quality and semantic alignment than DPO and LoRA fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This present clear motivation and the corresponding method  to deal  with the problem  of fine-tuning video diffusion models. \n\n2.  Self-degradation is a simple but effective method to generate positive and negative paris. \n\n3. Experimental results demonstrate SDO achieves superior performance than the existing\npost-training methods, with only a handful of high-quality\nsamples and minimal fine-tuning"}, "weaknesses": {"value": "1. The generation of positive and negative pairs is core step of DPO. Although self-degradation is a effective method to yield these pairs, other approaches using reward model, e.g.,MLLM, ImageReward,HPSv2,  to classify the generative images as positive and negative samples   can also achieve this target. This process maybe cost more time, but many acceleration methods are able to quickly generate samples. I would like to see such comparison in the paper.\n\n2. The authors claim they use minimal fine-tuning cost, but the comparison with respect to  training hours and traning data is missing.\n\n3. How does SDO determine the papameters of self-degradation? What  if high-frequency components are reserved?"}, "questions": {"value": "V-bench is unable to evaluate the motion extent of generated videos. The  static videos usually get high score. How do authors measure the quality of video motion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EbKgLC3cqC", "forum": "I4jBCglUOI", "replyto": "I4jBCglUOI", "signatures": ["ICLR.cc/2026/Conference/Submission9992/Reviewer_qMKW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9992/Reviewer_qMKW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762584548596, "cdate": 1762584548596, "tmdate": 1762921419473, "mdate": 1762921419473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}