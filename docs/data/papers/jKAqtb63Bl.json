{"id": "jKAqtb63Bl", "number": 19917, "cdate": 1758300596946, "mdate": 1759897012313, "content": {"title": "PACR: Progressively Ascending Confidence Reward for LLM Reasoning", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has significantly improved LLM reasoning, but its sparse, outcome-based reward provides no guidance for intermediate steps, slowing exploration. We propose Progressively Ascending Confidence Reward (PACR), a dense, model-intrinsic reward computed directly from the model’s evolving belief in the correct answer. PACR encodes the inductive bias that, along a well-formed reasoning trajectory, the probability of the ground-truth answer should have a generally ascending trend. We provide empirical and theoretical analysis validating that such an inductive bias constrains the exploration search space to regions richer in logically sound reasoning. We demonstrate that PACR accelerates exploration, reaches reward saturation with fewer trajectories, and yields improvements on multiple benchmarks. Our results suggest that dense, model-intrinsic shaping signals can make RLVR training more effective and reliable. Code will be released.", "tldr": "We propose Progressively Ascending Confidence Reward (PACR), a dense, model-intrinsic process reward computed directly from the model’s evolving belief in the correct answer for reasoning in LLMs.", "keywords": ["Large Language Models", "Reasoning", "Process Reward"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f794b722731ef9fcea7d52d6e78e596343fb0407.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes PACR (Progressively Ascending Confidence Reward), a dense reward signal for training LLM reasoning via reinforcement learning. The core idea is to reward intermediate reasoning steps based on whether they increase the model's probability of the ground-truth answer. The authors provide empirical observations and theoretical justification for this approach, implementing both sparse (trajectory-level) and dense (step-level) variants."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Conceptual clarity: The idea of using internal confidence dynamics as a reward shaping signal is intuitive and well-motivated.\n\nImplementation simplicity: PACR does not require an external reward model, making it lightweight and practical.\n\nEmpirical validation: The paper includes quantitative results on multiple math reasoning benchmarks (MATH500, AIME, AMC, etc.) showing some performance gains."}, "weaknesses": {"value": "## Flawed Theoretical FoundationProposition 1 is trivial and doesn't validate the method's utility:\n\nThe proposition proves that E[C_k] ≥ 0 when h_k is sampled from π_θ(·|q, Y_gt, H_<k) (the \"oracle policy\"). \nHowever, during actual RL training, steps are sampled from π_θ(·|q, H_<k) without access to Y_gt. \nThe proof merely shows that conditioning on the answer increases confidence in that answer—this is tautological. \nThe critical gap: the paper never establishes that actually sampled steps (without Y_gt conditioning) will exhibit this property. \nThis undermines the entire theoretical motivation for the method. \n\n## Weak and Inconsistent Empirical Results\n\nTable 1 shows marginal and unstable improvements:\n\n- Many improvements are within noise margins (e.g., +0.6, +0.8 on MATH500)\n- I admire the authors' frankness in posting those results. Several datasets show degradations (e.g., Minerva: -2.9 for Sparse-PACR on 1.5B; AMC: -1.2 for Sparse-PACR on 7B)\n- The average improvements (≤3.0 points) are modest given the added computational complexity\n- No statistical significance testing provided despite claiming results over 3 seeds\n\n## Circular Reasoning in Observational Studies\n\nObservation 1 (Section 4.1) lacks proper controls:\n\n- Finding that correct trajectories have more positive C_k steps could simply mean: correct reasoning leads to correct answers (obvious)\n- No analysis controlling for confounding factors (e.g., trajectory length, model confidence calibration)\n- Causality is unclear: does confidence growth cause correctness, or does correctness cause confidence growth?\n\nObservation 2's methodology is questionable:\n\n- Using GPT-5 to judge \"logical coherence\" introduces significant subjectivity and potential bias\n- The distinction between \"coherent\" and \"spurious\" reasoning that reaches correct answers is ill-defined\n- High confidence in a spurious path might actually indicate a model failure rather than validate the reward signal"}, "questions": {"value": "Why not compare against actual trained process reward models?\n\nCan you show results with statistical significance tests?\n\nWhat happens when the model is highly confident in the wrong answer—does PACR still provide useful signal?\n\nCompare against recent dense-reward or implicit-reward baselines (e.g., ConfPO, TLCR, DAPO, DeepSearch)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DuFuPtUFRQ", "forum": "jKAqtb63Bl", "replyto": "jKAqtb63Bl", "signatures": ["ICLR.cc/2026/Conference/Submission19917/Reviewer_1VLi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19917/Reviewer_1VLi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760492041285, "cdate": 1760492041285, "tmdate": 1762932085791, "mdate": 1762932085791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Progressively Ascending Confidence Reward, called PACR, to improve LLM reasoning inside the RL with verifiable reward setting. The key idea is simple. While the model writes a chain of thought, at each step the authors compute how much the log probability of the true answer increases, and they use this confidence gain as a dense, model intrinsic reward. They give two variants, Sparse PACR at trajectory level and Dense PACR at step level, and combine them with GRPO under RLVR. On three Qwen math models, the paper reports faster exploration and small but consistent gains on AIME 2024, AMC 2023, MATH500, Minerva Math, and OlympiadBench."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper gives a very clear inductive bias: along a good reasoning path, confidence in the ground truth should tend to go up. Turning this into a dense reward that needs no extra reward model is practical and clean.\n2. The paper checks three open models and five math benchmarks. The main table shows Dense PACR improves average pass@1 over a strong Dr GRPO baseline, for example +2.5 on the 1.5B model and +3.0 on the 7B model."}, "weaknesses": {"value": "1. Experiments only cover math datasets. Many recent results also evaluate general reasoning and code. It is not clear if PACR transfers beyond numeric answers or beyond tasks where the final answer is exactly verifiable, for example, long form QA or proofs with multiple valid forms. Comparison to broader settings in R1-style training or DAPO-like systems would strengthen the claim.\n2. The proof shows that the expected confidence gain is non-negative when steps are drawn from the ground truth conditioned policy. In practice, training never samples from that oracle. This gives a nice intuition but a weak guarantee for the real policy, and it leaves open how often confidence will increase under noisy steps. A discussion that connects the oracle gap to measured gains would help.\n3. Because the reward comes from the model’s own probability of the final answer, the model may learn to raise that probability early by adding hints or formatting patterns that correlate with the answer tokens, without improving reasoning quality. PRM works try to avoid such reward hacking with different credit assignment, for example, min form credit or verifier designs that measure true progress. A safety check or a negative control would be useful."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nrluyVoTF5", "forum": "jKAqtb63Bl", "replyto": "jKAqtb63Bl", "signatures": ["ICLR.cc/2026/Conference/Submission19917/Reviewer_8r9w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19917/Reviewer_8r9w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971236855, "cdate": 1761971236855, "tmdate": 1762932085125, "mdate": 1762932085125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Arguably, dense process rewards can lead to greater improvement with RL; however, acquiring quality process rewards can be costly. This paper introduces PACR, a novel method to enhance Reinforcement Learning with Verifiable Rewards (RLVR) for LLM reasoning. It provides a dense, model-intrinsic reward based on the idea that along a successful reasoning trajectory, the model's confidence in the correct final answer should progressively increase. The correlation between this increase in confidence and outcome correctness implies that it can serve as a process reward."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a key problem in RL training: that dense rewards are hard and expensive to acquire.\n\nThe paper's presentation is clear. The supporting evidence (observations 1, 2, and 3) for the method is mostly relevant and well thought-out.\n\nThe gain from the methods is good over the baseline Dr.DRPO , though in some certain test datasets it is negative."}, "weaknesses": {"value": "The experiment is slightly limited, with only one training dataset and three models, and one baseline algorithm (Dr.GRPO). I would like to see one reasoning model (e.g., DeepSeek-R1-Distill-Qwen-1.5B) tested to see if the effectiveness of your proposed process reward still holds. Also, report the accuracy on AIME 2025.\n\nI dislike the inclusion of Section 4.2, as it makes the proposed method look deep, whereas the key to the proof is really the artificial \"oracle policy assumption\". I think that Section 4.2 doesn't meaningfully justify your proposed process reward, or you can leave it in the appendix.\n\nThe authors only evaluate with pass@1 greedy decoding, yet it is debated whether RL post-trained models increase sampling efficiency (better pass@1) while reducing coverage (lower pass@k for large k compared to the base model). Therefore, the authors should also report $\\text{pass@K}$ with a positive temperature.\n\nThe author can use this new process-reward for beam search. In my opinion, this is an important experiment, as if beam search leads to improvement, then I'm more confident that RL's improvement comes mainly from the process-reward and not your other design choices (e.g. mixing with outcome reward, mix-max scaling)."}, "questions": {"value": "How do the authors justify the process-wise group normalization (in lines 363, before the Min-Max scaling explanation)? Does there exist similar method in the literature? Does the way they divide the sequence into steps of varied lengths have any impact on this?\n\nIt is hard for me to read the small colored numbers in Table 1.\n\nWhat is the setting of Figure 8?\n\nQualitatively, it would be nice to attempt to find failure cases when high growth doesn't correspond to an informative step, or when low growth doesn't correspond to an uninformative or wrong step. I can see in Figure 12 and 13 steps that have negative, large magnitude growth that are nevertheless informative.\n\nI'm also concerned about the way you divide the sequence into steps. It is very hard to do that nicely by just pattern. I can see that you have many short steps. Did you consider other alternatives?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eDPOj5w7D7", "forum": "jKAqtb63Bl", "replyto": "jKAqtb63Bl", "signatures": ["ICLR.cc/2026/Conference/Submission19917/Reviewer_PwTw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19917/Reviewer_PwTw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149559608, "cdate": 1762149559608, "tmdate": 1762932084738, "mdate": 1762932084738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PACR, a dense, model-intrinsic reward for reinforcement learning in LLM reasoning. Instead of relying solely on sparse, outcome-based signals, PACR rewards stepwise increases in the model’s confidence in the ground-truth answer. The authors show that this “confidence ascent” acts as a strong training signal for training and can empirically enhance RL training performance. Experiments on multiple math-reasoning benchmarks (e.g., MATH500, AIME24, AMC23) demonstrate that PACR improves RL training performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem addressed in this paper is important. Incorporating more dense reward information into current RL pipelines remains an underexplored direction.\n\n\n2. The proposed idea is reasonable and aligns with recent studies showing that a model’s reasoning confidence often correlates with the correctness of its answers, suggesting that such confidence signals could be valuable for training.\n\n\n3. The writing is clear and easy to follow."}, "weaknesses": {"value": "1. The approach for determining the reasoning step appears rather ad-hoc. It is unclear how this mechanism would transfer to other domains such as code generation, where the output often contains many new lines. Would this lead to an excessive number of reasoning steps for tasks involving long-context generation?\n\n2. The training process seems to introduce additional computational overhead, particularly as the generation length increases, which could significantly inflate the number of reasoning steps. A detailed analysis of the training cost and the number of reasoning steps would help clarify the efficiency of the proposed method.\n\n3. The experiments use a maximum generation length of only 3k tokens, which is much shorter than the 40k-token context supported by the Qwen3-4B model. Given that the proposed method may introduce substantial overhead for long-context scenarios, it would be valuable to examine its scalability with respect to context length to better assess its effectiveness.\n\n4. Another possible explanation for the observed performance improvement could be more stable training due to the absence of zero-advantage samples. In vanilla GRPO, many responses have zero advantage and thus contribute nothing to training (though dynamic sampling can alleviate this issue). Since PACR avoids this, it would be helpful to compare against (or combine with) GRPO with dynamic sampling to determine whether the gains primarily stem from denser rewards or from improved training stability."}, "questions": {"value": "listed in weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1L1xoXR5W7", "forum": "jKAqtb63Bl", "replyto": "jKAqtb63Bl", "signatures": ["ICLR.cc/2026/Conference/Submission19917/Reviewer_5e8n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19917/Reviewer_5e8n"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155374318, "cdate": 1762155374318, "tmdate": 1762932084130, "mdate": 1762932084130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}