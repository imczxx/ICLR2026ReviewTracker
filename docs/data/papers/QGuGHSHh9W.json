{"id": "QGuGHSHh9W", "number": 16690, "cdate": 1758267685895, "mdate": 1759897224732, "content": {"title": "MORE: Multi-Objective Adversarial Attacks on Speech Recognition", "abstract": "The emergence of large-scale automatic speech recognition (ASR) models such as Whisper has greatly expanded their adoption across diverse real-world applications. Ensuring robustness against even minor input perturbations is therefore critical for maintaining reliable performance in real-time environments. While prior work has mainly examined accuracy degradation under adversarial attacks, robustness with respect to efficiency remains largely unexplored. This narrow focus provides only a partial understanding of ASR model vulnerabilities.\nTo address this gap, we conduct a comprehensive study of ASR robustness under multiple attack scenarios. We introduce MORE, a multi-objective repetitive doubling encouragement attack, which jointly degrades recognition accuracy and inference efficiency through a hierarchical staged repulsion–anchoring mechanism. Specifically, we reformulate multi-objective adversarial optimization into a hierarchical framework that sequentially achieves the dual objectives. To further amplify effectiveness, we propose a novel repetitive encouragement doubling objective (REDO) that induces duplicative text generation by maintaining accuracy degradation and periodically doubling the predicted sequence length. Overall, MORE compels ASR models to produce incorrect transcriptions at a substantially higher computational cost, triggered by a single adversarial input. Experiments show that MORE consistently yields significantly longer transcriptions while maintaining high word error rates compared to existing baselines, underscoring its effectiveness in multi-objective adversarial attack.", "tldr": "", "keywords": ["Speech recogntion; speech processing; model robustness"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f9eace9d1a0d66ea0ff49a0ef8cdf43eadcb654.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MORE (Multi-Objective Repetitive Doubling Encouragement), an adversarial attack designed to jointly degrade accuracy and efficiency of automatic speech recognition (ASR) systems, particularly the Whisper family. The authors introduce a two-stage hierarchical optimization framework: (1) a “repulsion” stage that maximizes transcription errors using cross-entropy loss, and (2) an “anchoring” stage that prolongs decoding by suppressing the end-of-sentence (EOS) token and introducing a Repetitive Encouragement Doubling Objective (REDO) that induces periodic repetition in the generated transcripts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper explores a less-examined dimension of ASR robustness: efficiency robustness, i.e., how adversarial perturbations can cause excessive computation via unending decoding loops.\n\nThe multi-objective formulation combining accuracy degradation and inference slowdown is conceptually interesting and fills a niche left by prior single-objective adversarial attacks such as PGD, MI-FGSM, and SlothSpeech.\n\nIntroducing a hierarchical optimization (repulsion–anchoring) and a specialized REDO mechanism to induce repetition is a creative way to operationalize efficiency degradation.\n\nThe proposed algorithm is clearly described with mathematical formulation (Eqs. 5–9, Algorithm 1) and supported by a thorough ablation (Table 3) showing the effect of removing each component.\n\nComparative results on five Whisper model sizes (tiny→large) and two datasets show consistent trends: MORE produces much longer transcripts while keeping WER extremely high."}, "weaknesses": {"value": "Only whisper family is tested.\nAll experiments are confined to the Whisper family, which share the same encoder–decoder architecture and CTC-free autoregressive decoding. This limits the claim of “comprehensive ASR robustness analysis.” Evaluating at least one non-Whisper or CTC-based model (e.g., wav2vec 2.0 + CTC, DeepSpeech-CTC) would strengthen generality.\n\nThe REDO repetition objective relies on autoregressive token prediction and EOS suppression.\nFor ASR systems using sliding-window inputs or CTC loss, token duplication beyond the receptive window would not be guaranteed, since predictions are frame-aligned and not dependent on previous outputs.\nThe paper does not explain how the method would ensure repetition or efficiency degradation under such architectures.\n\nThe paper claims to be the first “multi-objective attack,” but the proposed combination of accuracy and efficiency optimization largely merges existing adversarial attack objectives (for WER maximization) with repetition-inducing or EOS-suppression tricks that have been studied in text generation (e.g., Xu et al., 2022).\nThe paper does not articulate what fundamentally distinguishes this setup from prior adversarial or repetition-loop attacks.\nThe “hierarchical repulsion–anchoring” design could be presented as a general training heuristic rather than a conceptual breakthrough.\n\nNits: Break long sentence like \" To enhance attack effectiveness, we not only reduce the likelihood of EOS but also explicitly emphasize a competing token, the token with the second largest probability, since reinforcing this alternative both reduces EOS dominance and steers the model toward an alternative continuation.\""}, "questions": {"value": "Scope and unique challenge:\nWhat makes attacking efficiency fundamentally different or more challenging than simply maximizing output length or suppressing EOS?\nHow does the proposed hierarchical optimization overcome specific difficulties not addressed by existing repetition-loop analyses?\n\nModel generality:\nWhy were only Whisper models tested? Have you attempted the same attack on CTC-based ASR systems (e.g., wav2vec 2.0 CTC, Conformer)?\nWould REDO or EOS-suppression work when the model’s decoding process lacks autoregression?\n\nRepetition mechanism validity:\nFor models using a sliding-window decoder and limited spectrogram context, how can you ensure that repeated content remains predictable given input truncation? Does the attack rely on the model’s cache or hidden-state persistence? If so, how does window size affect repetition length?\n\nMeasurement of “efficiency degradation”:\nCould you provide actual runtime or FLOP comparisons, not just token lengths, to substantiate efficiency loss?\nHow do hardware or decoding-beam settings influence the attack effect?\n\n\nThe paper cites SlothSpeech as the only efficiency-targeting method, but earlier “loop attacks” or “repetition failures” in text generation (e.g., Xu et al., 2022) already described similar failure modes. How is MORE conceptually distinct from combining an existing adversarial loss with an EOS penalty?\n\nPotential defenses: Given the repetition-loop nature of the attack, have you tested whether simple decoding constraints (e.g., repetition penalty, token-frequency cap, or beam search with anti-loop heuristics) neutralize the effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "23mpI2PRYv", "forum": "QGuGHSHh9W", "replyto": "QGuGHSHh9W", "signatures": ["ICLR.cc/2026/Conference/Submission16690/Reviewer_v3bB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16690/Reviewer_v3bB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761510312378, "cdate": 1761510312378, "tmdate": 1762926743893, "mdate": 1762926743893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an attack on automatic speech recognition (ASR) models, targeting both accuracy (ie, aiming to poison model accuracy) and sequence length (trying to produce longer sequences) at the same time. The method is a training time attack, where the attack is done hierarchically (first optimize for accuracy loss, then for producing longer sequence length), with the proposed sequence length loss including two components: i) try to suppress end-of-sentence token probability, and ii) try to encourage sentence repetition, which is a known problem that occurs also in ASR models trained without any attacks. The proposed method is tested with several Whisper family models on LibriSpeech & LJ-Speech datasets, and benchmarked against several existing adversarial attack methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) Leveraging a naturally occurring failure mode for the attack seems like a nice idea.\n\n2) The paper is mostly clear and easy to read (although see Questions for some commenting on this).\n\n3) The proposed method is benchmarked against multiple existing methods, and the results also include a good ablation study on the individual components."}, "weaknesses": {"value": "1) The reporting of experimental results should be improved (see Questions for details).\n\n2) There is no implementation/source code available (although according to the Reproducibility statement it might be released later).\n\n3) Eg lines 107-111: I do not find the argument for the need to combine the accuracy and sequence length attacks too convincing; any working accuracy degradation attack will make a given system unusable even if it is lighting fast. To me, the interesting part here is the possibility of having more effective attacks against robust (whether due to intrinsic properties of the model, cf eg Shah et al. 2025: Speech robust bench, or possible due to some defense method) models via a multiobjective attack. Also, lines 120-123: this makes very little sense to me, consider rephrasing.\n\n4) There are no experiments nor even any discussion on how existing defense methods work against the proposed attack"}, "questions": {"value": "## Questions and comments for the authors, in decreasing order of importance:\n\n1) Please include some variation measure, eg, standard error of the mean for all the results.\n\n2) Please clarify how each of the hyperparameters were tuned (including for the baselines, and were these tuned separately for each method or how)?\n\n3) How does the computational complexity of the proposed method compare with the existing methods?\n\n4) Some steps in the proposed method are presented in a bit overly complex and jarring way, especially the repulsion step: as far as I can tell, this is basically just doing standard adversarial attack to lower the utility of the model via gradient-based optimization with simple modified loss, but this is somehow written in a grandiose way, including by calling it repulsion for some reason. I think it would improve the paper if you present this step as what it is, ie, basically a standard utility attack step (where you could probably also use any existing reasonable utility attack without affecting the results too much).\n\n5) Tables 1-3: I do not understand how bolding is used, please make this explicit, or better yet, just highlight the best (and preferably also mark all methods eg within 1 standard error of the best)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U83SvxELFG", "forum": "QGuGHSHh9W", "replyto": "QGuGHSHh9W", "signatures": ["ICLR.cc/2026/Conference/Submission16690/Reviewer_xABV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16690/Reviewer_xABV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761650367676, "cdate": 1761650367676, "tmdate": 1762926743425, "mdate": 1762926743425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MORE, a multi-objective adversarial attack framework targeting both attack efficacy and inference efficiency in automatic speech recognition systems. The method combines a hierarchical repulsion, anchoring optimization scheme: first degrading transcription accuracy and then prolonging decoding through EOS suppression with the repetitive encouragement doubling objective, which induces structured repetition in generated text. Experiments across two benchmarks show that MORE significantly increases both word error rate and output length, outperforming accuracy-only (standard attacks PGD, FGSM variants). Ablations confirm each component’s contribution, and complexity analysis plus ethical discussion round out the work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The joint focus on accuracy and efficiency robustness is original and well-motivated.\n\nAcross five Whisper models, the proposed MORE method consistently yields longer, incorrect outputs (strong attack efficacy), clearly outperforming baselines.\n\nThe authors discuss potential misuse and propose mitigations, which strengthen the paper’s responsibility stance.\n\nThe appendix rigorously analyzes computational cost, showing depth of understanding."}, "weaknesses": {"value": "The hierarchical optimization's convergence or general properties are not analyzed mathematically. More theoretical analyses should be included.\n\nIf I understand correctly, using output length as a proxy for computational efficiency is reasonable but somewhat coarse. Can authors provide some more efficiency analyses?\n\nOnly Whisper-based ASR models are tested. The paper should include the test of other architectures.\n\nThere is no black-box or transfer evaluation against closed models or API systems. It seems that all attacks are white-box."}, "questions": {"value": "How sensitive is the proposed MORE method to hyperparameters like doubling period or the weighting between losses?\n\nHow would the approach perform on non-English or noisy real-world speech data?\n\nWould a reinforcement-learning formulation offer a better balance between the two objectives?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l8kd36BQMz", "forum": "QGuGHSHh9W", "replyto": "QGuGHSHh9W", "signatures": ["ICLR.cc/2026/Conference/Submission16690/Reviewer_tS9e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16690/Reviewer_tS9e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991643575, "cdate": 1761991643575, "tmdate": 1762926742932, "mdate": 1762926742932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Large-scale automatic speech recognition (ASR) models like Whisper are widely used, making robustness to small input changes crucial. Prior studies mainly address accuracy loss under adversarial attacks, robustness with respect to efficiency remains largely unexplored. To fill this gap, the authors propose MORE (Multi-Objective Repetitive Doubling Encouragement), an attack that jointly reduces recognition accuracy and inference efficiency. Using a hierarchical repulsion–anchoring mechanism, MORE sequentially optimizes these objectives. Its REDO (Repetitive Encouragement Doubling Objective) promotes duplicative text generation, degrading accuracy while doubling sequence length. Experiments show MORE yields longer, error-prone transcriptions at higher computational cost, exposing new multi-objective vulnerabilities in ASR models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The experiments consider multiple DL models and datasets, with seemingly more rigorous evaluation methodologies than many previous papers in this area."}, "weaknesses": {"value": "(1) Even though the paper claims to focus on robustness with respect to efficiency against attacks, the experimental results appear to primarily emphasize accuracy or, in some cases, the trade-off instead. However, accuracy has already been extensively explored in prior works (Raina et al., 2024; Raina & Gales, 2024; Olivier & Raj, 2022b; Madry et al., 2018a; Dong et al., 2018; Wang & He, 2021; Gao et al., 2024). Therefore, the overall contribution of this paper seems quite limited.\n\n(2) Lack of a Clear Threat Model. It is unclear what assumptions the paper makes regarding the attack setting. Is this a black-box or a white-box attack? What knowledge does the attacker have about the victim model and dataset? These aspects need to be clearly specified in the paper. I recommend that the authors add a dedicated subsection titled “Threat Model” to explicitly describe the assumptions and settings, so that readers can better understand the scope and validity of the proposed method.\n\n(3) Insufficient Experimental Evaluation. The experimental analysis in the paper is not sufficient. In particular, the transferability of adversarial attacks is a crucial aspect when evaluating the effectiveness of an attack method. However, the paper does not include any experiments assessing transferability. It remains unclear whether an attack generated on one model can be successfully transferred to and applied against another model. Including such an evaluation would significantly strengthen the empirical validation of the proposed approach.\n\n(4) Finally, the writing needs significant improvement. In critical parts of the paper, it is hard to tell what the authors did in terms of experimentation and analysis, or what motivated the choices they made."}, "questions": {"value": "Please refer to my comments for more details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U20bLnsg4Y", "forum": "QGuGHSHh9W", "replyto": "QGuGHSHh9W", "signatures": ["ICLR.cc/2026/Conference/Submission16690/Reviewer_P3t3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16690/Reviewer_P3t3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762040512465, "cdate": 1762040512465, "tmdate": 1762926742388, "mdate": 1762926742388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}