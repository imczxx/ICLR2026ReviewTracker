{"id": "P61Mb28dnY", "number": 5975, "cdate": 1757949189028, "mdate": 1759897941690, "content": {"title": "StreamAgent: Towards Anticipatory Agents for Streaming Video Understanding", "abstract": "Real-time streaming video understanding in domains such as autonomous driving and intelligent surveillance poses challenges beyond conventional offline video processing, requiring continuous perception, proactive decision making, and responsive interaction based on dynamically evolving visual content. However, existing methods rely on alternating perception-reaction or asynchronous triggers, lacking task-driven planning and future anticipation, which limits their real-time responsiveness and proactive decision making in evolving video streams. To this end, we propose a StreamAgent that anticipates the temporal intervals and spatial regions expected to contain future task-relevant information to enable proactive and goal-driven responses. Specifically, we integrate question semantics and historical observations through prompting the anticipatory agent to anticipate the temporal progression of key events, align current observations with the expected future evidence, and subsequently adjust the perception action (e.g., attending to task-relevant regions or continuously tracking in subsequent frames). To enable efficient inference, we design a streaming KV-cache memory mechanism that constructs a hierarchical memory structure for selective recall of relevant tokens, enabling efficient semantic retrieval while reducing the overhead of storing all tokens in the traditional KV-cache. Extensive experiments on streaming and long video understanding tasks demonstrate that our method outperforms existing methods in response accuracy and real-time efficiency, highlighting its practical value for real-world streaming scenarios. The code is available in the supplementary material.", "tldr": "", "keywords": ["Streaming Video Understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d884af1c3f7f1ecbe3d87ad3db560a2e768a1bc5.pdf", "supplementary_material": "/attachment/d7bc84b6158de29b6846c0cbbd2b27152ca3c5f3.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces cache compression, early response, and a memory structure to achieve lightweight response generation,  \nand proposes an interactive architecture that leverages the model's agentic capability to collect fine-grained information through precise image-level control.  \nSpecifically, the proposed model (1) predicts the development of future events within the video stream,  \n(2) determines whether the current moment provides sufficient information for response generation,  \nand (3) incorporates a tool-utilization mechanism to capture visual details."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Technically, the implementation appears efficient, reflecting good observations on existing cache optimization and attention mechanisms.  \nIn addition, the paper introduces the concept of \"Proactive\" planning, allowing the model to anticipate and prepare for future streaming information through planning, which appears original.  \nThe attempt to focus on real-time streaming environments and explore application domains different from offline LMMs is also meaningful."}, "weaknesses": {"value": "- The definition of the Heuristic Score F = G + λU is abstract; the detailed computation procedure for G/U, the setting of λ, and the evaluator (which model performs the scoring) are not described.\n- Although the prompt-based control structure is disclosed, the connection between G/U evaluation and the planning stage remains unclear.\n- The criteria for \"proactive response\" or the statistics on response timing (e.g., observed frame ratio, average response delay) are not reported, making it difficult to quantitatively verify how effectively the proposed mechanism works.\n- It is unclear whether the tool-use capability emerged from Qwen2.5VL's pretraining or was realized through the proposed planning procedure.\n- Aside from the framework's conceptual interest, the actual performance improvement compared to the Qwen2.5VL backbone is minimal or even degraded, suggesting that the ability to use tools may not have contributed significantly to answer accuracy."}, "questions": {"value": "- Please clarify the procedure for computing G and U in Equation (3).\n- Can we assume that tool selection directly relies on the agentic capability inherently provided by Qwen2.5VL?\n- In line 11 of Algorithm 1, what is the criterion for P̂ \"not requires additional information\"? Is it when U = 0? Please provide details.\n- Does StreamAgent operate effectively with other architectures? If the model performing planning/tools is fixed while the interacting model is replaced with a different series, does performance still improve?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MwCbTMhuUB", "forum": "P61Mb28dnY", "replyto": "P61Mb28dnY", "signatures": ["ICLR.cc/2026/Conference/Submission5975/Reviewer_mkwF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5975/Reviewer_mkwF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903626829, "cdate": 1761903626829, "tmdate": 1762918386166, "mdate": 1762918386166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for anticipatory and proactive real-time video understanding in streaming settings. The work target continuous online domains such as autonomous driving and surveillance, and introduces mechanisms for proactive, query-driven decision-making and long-term memory management to surpass limitations in current online/offline video question-answering systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The idea introduced goes beyond perception-reaction models; it integrates tools for temporal and spatial anticipation, query-based reasoning, and iterative planning in real-time streaming video. These are highly practically applicable.\n* Unlike prior reactive or binary-trigger systems (VideoLLM-online, Dispider), this work explicitly models temporal anticipation through three planning modes (Reactive, Proactive, Speculative) scored via an A*-inspired heuristic balancing immediate utility $G$ and future utility $U$. This design addresses premature response errors."}, "weaknesses": {"value": "* The paper mentions \"zoom in,\" \"object tracking,\" and \"detailed captioning\" as tools, but, perhaps I missed them, never quantifies how often each tool is invoked, their individual success rates, or their failure modes. While these tool use is highlighted, the exploration and comparative study of varying tool types and their influences is relatively narrow.\n* Although effective, the planning combines heuristic scoring with agentic approaches. The robustness of these heuristics as video and query complexity scale is not fully interrogated, especially for rare or ambiguous scenarios."}, "questions": {"value": "* What happens when tracking fails or zoom crops the wrong region? Are there redundant tool calls?\n* It seems to me that the method is highly tuned for query-driven, real-time scenarios. If so, won’t it underperform for generic or artificially complex video reasoning tasks, lacking universal adaptability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AlypvvP7G1", "forum": "P61Mb28dnY", "replyto": "P61Mb28dnY", "signatures": ["ICLR.cc/2026/Conference/Submission5975/Reviewer_aovd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5975/Reviewer_aovd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986166105, "cdate": 1761986166105, "tmdate": 1762918385429, "mdate": 1762918385429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces StreamAgent to address real-time responsiveness and proactive decision-making in evolving video streams. StreamAgent anticipates temporal intervals and spatial regions likely to contain task-relevant information, enabling proactive and goal-driven responses. By integrating question semantics and historical observations, the agent predicts the temporal progression of key events, aligns current observations with expected future evidence, and dynamically adjusts its perception and actions. \n\nTo ensure efficiency, the authors propose a streaming KV-cache memory mechanism that constructs a hierarchical memory structure, allowing selective recall of relevant tokens. This design enables efficient semantic retrieval while reducing the computational overhead of storing all tokens traditionally required for inference."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to follow\n\n- The figures are intuitive."}, "weaknesses": {"value": "- The proposed streaming KV-Cache heavily borrows from StreamChat[1], with relatively incremental modifications.  \n\n- The paper utilizes Qwen-VL-3B as the planning model; however, for complex problems, such a lightweight model struggles to perform adequate planning and often suffers from hallucination issues.  \n\n- The performance of the proposed agent is inferior to that of a single model, indicating insufficient planning and answering capabilities.  \n\n- What tools can the agent invoke during its operation? How does tool usage differ across various tasks？\n\n- The paper suggests attending to task-relevant regions or continuously tracking subsequent frames, but this approach may negatively impact new tasks, especially in multi-turn dialogue scenario\n\n[1] Xiong H, Yang Z, Yu J, et al. Streaming video understanding and multi-round interaction with memory-enhanced knowledge[J]. arXiv preprint arXiv:2501.13468, 2025."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2ueau9NycQ", "forum": "P61Mb28dnY", "replyto": "P61Mb28dnY", "signatures": ["ICLR.cc/2026/Conference/Submission5975/Reviewer_PqWc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5975/Reviewer_PqWc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997684798, "cdate": 1761997684798, "tmdate": 1762918384595, "mdate": 1762918384595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}