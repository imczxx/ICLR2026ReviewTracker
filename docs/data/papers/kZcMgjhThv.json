{"id": "kZcMgjhThv", "number": 19819, "cdate": 1758299687428, "mdate": 1759897017446, "content": {"title": "ConsumerBench: Benchmarking Generative AI Applications on End-User Devices", "abstract": "The recent shift in Generative AI (GenAI) applications from cloud-only environments to end-user devices introduces new challenges in resource management, system efficiency, and user experience. This paper presents ConsumerBench, a comprehensive benchmarking framework designed to evaluate the system efficiency and response time of GenAI models running on end-user devices. Unlike existing benchmarks that assume exclusive model access on dedicated GPUs, ConsumerBench simulates realistic multi-application scenarios executing concurrently on constrained hardware. Furthermore, ConsumerBench supports customizable workflows that simulate complex tasks requiring coordination among multiple applications. ConsumerBench captures both application-level metrics, including latency and Service Level Objective (SLO) attainment, and system-level metrics like CPU/GPU utilization and memory bandwidth. Through extensive experiments, ConsumerBench reveals inefficiencies in resource sharing, unfair scheduling under greedy allocation, and performance pitfalls of static model server configurations. The paper also provides practical insights for model developers and system designers, highlighting the benefits of custom kernels tailored to consumer-grade GPU architectures and the value of implementing SLO-aware scheduling strategies.", "tldr": "", "keywords": ["Benchmark", "Local Platform", "SLO", "Latency", "Large Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f8bd3596812d4cc9c505b375d3833ae2ee3a701b.pdf", "supplementary_material": "/attachment/0574f53e03537db9f983366e63e6aa8b6f9e21f9.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces ConsumerBench, a benchmarking framework designed to evaluate the runtime and system efficiency of Generative AI (GenAI) applications running on end-user devices (e.g., laptops, smartphones). Unlike existing benchmarks that assume dedicated hardware, ConsumerBench emulates multi-application, resource-constrained environments, measuring both application-level SLOs (latency, throughput) and system-level metrics (CPU/GPU utilization, memory bandwidth, power).\n\nExperiments on consumer GPUs (RTX 6000) reveal:\n1. Greedy GPU allocation causes starvation for lightweight, latency-sensitive applications.\n2. Static GPU partitioning improves fairness but lowers utilization.\n3. Shared inference servers with fixed configurations can cause conflicting SLO satisfaction.\n\nThe authors distill design insights for architecture-aware kernels, SLO-aware scheduling, and configurable inference servers, highlighting gaps in current GenAI runtime systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies a critical and underexplored problem: efficient concurrent execution of heterogeneous GenAI workloads on consumer devices.\n2. This paper provides an end-to-end benchmarking suite with user-configurable DAG-based workflows, automated metric collection, and extensible APIs for custom apps.\n3. This paper is easy to follow. Well-structured experiments reveal non-trivial interactions between applications.\n4. The authors provide implementation details, configuration examples, and supplementary materials for replication."}, "weaknesses": {"value": "1. I’m not entirely sure, but NVIDIA MPS might support dynamic resource allocation rather than static partitioning. You can start the NVIDIA MPS and run multiple jobs on the same set of GPUs without additional configuration. I think this could serve as a new baseline.\n2. The evaluation only considers greedy and static partitioning, without implementing or comparing against dynamic schedulers (e.g., kernel-level preemption or SLO-prioritized scheduling). This limits the depth of system insights. In fact, several resource allocation papers have explored inference scheduling, for example, Fairness in Serving Large Language Models (OSDI). The authors could consider including additional baselines from this line of work.\n3. The study measures latency and utilization but lacks user-centric Quality of Experience (QoE) metrics or perceptual thresholds that better represent real usage.\n4. The title formatting and the “Anonymous authors” placeholder in the template look a bit unusual."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qlYmLaijV3", "forum": "kZcMgjhThv", "replyto": "kZcMgjhThv", "signatures": ["ICLR.cc/2026/Conference/Submission19819/Reviewer_em9g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19819/Reviewer_em9g"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760916967551, "cdate": 1760916967551, "tmdate": 1762999984502, "mdate": 1762999984502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ConsumerBench, a benchmarking framework for evaluating Generative AI applications running concurrently on end-user devices. Users specify applications, models, service-level objectives (SLOs), and inter-application dependencies in a YAML configuration file, which the system compiles into a directed acyclic graph (DAG) of application instances. ConsumerBench then executes and measures these workloads to expose inefficiencies in resource management, GPU sharing, and SLO-aware scheduling under multi-application concurrency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel concurrency focus: The paper addresses multi-application inference, a relatively unexplored yet practically important problem for end-user AI systems.\n2. The experimental results yield clear, practical takeaways for developers seeking to improve performance and fairness when multiple generative AI applications share hardware resources.\n3. The use of YAML-based configuration and the DAG-based task execution model make the framework easily extensible — new applications, models, or metrics can be integrated with minimal effort."}, "weaknesses": {"value": "1. Despite claiming a focus on “end-user devices,” all experiments are performed on a single workstation with an RTX 6000 GPU. Evaluations on consumer-class GPUs (e.g., RTX 4060/4070) or integrated accelerators would strengthen the paper’s external validity.\n2. Each task uses a fixed model configuration. Demonstrating results across multiple models per modality would better validate that the benchmark’s findings generalize beyond specific architectures.\n3. The paper discusses SLO-aware resource allocation but does not evaluate any dynamic or adaptive scheduling strategies. This weakens the central argument and leaves open the question of how ConsumerBench would perform under more optimal schedulers."}, "questions": {"value": "1. Have you considered expanding the benchmark to include more task types and custom metrics, allowing users to choose flexible workload combinations for evaluation?\n2. What was the reasoning behind using only one hardware setup? Would you consider evaluating on more representative consumer devices or cross-vendor platforms to demonstrate generality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2lyARFO7ea", "forum": "kZcMgjhThv", "replyto": "kZcMgjhThv", "signatures": ["ICLR.cc/2026/Conference/Submission19819/Reviewer_9PWZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19819/Reviewer_9PWZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761607041038, "cdate": 1761607041038, "tmdate": 1762999984557, "mdate": 1762999984557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel benchmark called ConsumerBench, which helps evaluates performances of multi-application scenarios executed concurrently on constrained hardware. This is a useful benchmark given the intensive requirements of applications these days. The paper also showcases an example that helped provide potential insights that can be drawn from using the benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper identifies an useful, practical gap in benchmarking applications under constrained on-device resources. It also presents careful thought out workflow and analysis, along with an example usecase where the use of such a benchmark/framework might help bring more insights on the concurrent execution and where the limits might occur. The findings are well written and presented with clarity."}, "weaknesses": {"value": "The paper provides a benchmark that is useful in terms of software aspects for applications running concurrently on constrained resources. However, it hasn't mentioned any consideration for hardware impacts, which can further influence the performance of the applications on the end-user devices. Furthermore, the paper could explain more on the usecases and usefulness of the benchmark, such as how the workflow/findings scale and provide systematic insights across different architecture and types of end-user devices."}, "questions": {"value": "- Can this benchmark be used in a container or sandbox setting, such as to simulate and understand the performance of target applications on a device before actual implementation/purchase? The main usecase seems to be for an existing device.\n- The paper identified several factors affecting performance. To what extent can the bottleneck and processes identified in the analysis be used to better schedule the applications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2c2nndmmRI", "forum": "kZcMgjhThv", "replyto": "kZcMgjhThv", "signatures": ["ICLR.cc/2026/Conference/Submission19819/Reviewer_rMux"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19819/Reviewer_rMux"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652733385, "cdate": 1761652733385, "tmdate": 1762999984501, "mdate": 1762999984501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces consumerbench, a benchmarking application for four GenAI workloads. The benckmark is used to benchmark the performance of the four applications on a 2018 RTX600 GPU, with an Intel Xeon Gold 6126 CPU (2.60GHz, 24 cores) and 32GB of system memory (DRAM). The results show that three out of the four applications tested, achieve their required SLOs, with the third one not achieving the slo for 1.5% of the audio samples."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Benchmarking hardware is an important problem. This paper aims to improve the current State-of-the-art of edge/small device benchmarking by benchmarking workflows of tasks."}, "weaknesses": {"value": "My main issue is with the novelty and depth of the work. For example, as a benchmark, when comparing the suggested benchmark with PalmBench (cited in the paper), the authors have a much more limited set of applications/Models. When it comes to insights from the experiments, the results are very well known. For example, the KTransofrmer project (open-sourced with a paper in SOSP 2025) has been setup to solve many of the insights discussed. \n\nAnother issue, for a benchmarking paper, one typically needs to run on many configurations. Going back to the PalmBench paper, they test with three operating systems on nine different hardware platforms. Testing on only two platforms does not allow for a good enough evaluation.\n\n\nWriting:\n1- The paper almost exclusively cites Arxiv papers/versions of the paper with a handful of exceptions. Please fix this!"}, "questions": {"value": "1. Can you expand your experiments to more models, scenarios, and more hardware/OS configurations?\n2. Besides the wokflow, what else is different from PalmBench?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lQY4Uooknc", "forum": "kZcMgjhThv", "replyto": "kZcMgjhThv", "signatures": ["ICLR.cc/2026/Conference/Submission19819/Reviewer_pqy1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19819/Reviewer_pqy1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001792189, "cdate": 1762001792189, "tmdate": 1762999984526, "mdate": 1762999984526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}