{"id": "ctspw4CqbS", "number": 12905, "cdate": 1758211517656, "mdate": 1759897478022, "content": {"title": "Plan and Budget: Effective and Efficient Test-Time Scaling on Reasoning Large Language Models", "abstract": "Large Language Models (LLMs) have achieved remarkable success in complex reasoning tasks, but their inference remains computationally inefficient. We observe a common failure mode in many prevalent LLMs, overthinking, where models generate verbose and tangential reasoning traces even for simple queries. Recent works have tried to mitigate this by enforcing fixed token budgets, however, this can lead to underthinking, especially on harder problems. Through empirical analysis, we identify that this inefficiency often stems from unclear problem-solving strategies. To formalize this, we develop a theoretical model, BAM (Budget Allocation Model), which models reasoning as a sequence of sub-questions with varying uncertainty, and introduce the E3 metric to capture the trade-off between correctness and computation efficiency. Building on theoretical results from BAM, we propose Plan-and-Budget, a model-agnostic, test-time framework that decomposes complex queries into sub-questions and allocates token budgets based on estimated complexity using adaptive scheduling. Plan-and-Budget improves reasoning efficiency across a range of tasks and models, achieving up to 70% accuracy gains, 39% token reduction, and 193.8% improvement in E3. Notably, it elevates a smaller model (DS-Qwen-32B) to match the efficiency of a larger model (DS-LLaMA-70B), demonstrating Plan-and-Budget’s ability to close performance gaps without retraining. Our code is available at anonymous.4open.science/r/P-and-B-6513/.", "tldr": "", "keywords": ["Large Language Model", "Test-Time Compute", "Reasoning", "Effectiveness", "Efficiency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2bdcf0406a5225dca2bb6ad4d3ca10c82f04cf95.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes \"plan and budget\", a test-time framework that improves reasoning efficiency in large language models by dynamically allocating computational effort. It addresses reasoning miscalibration by decomposing each query into sub-questions and assigning adaptive token budgets based on estimated uncertainty. The approach is a two-step method (plan + adaptive budget) that requires no retraining and is supported by the budget allocation model, a theoretical framework for uncertainty-aware token allocation. Experimental results show consistent efficiency and accuracy gains over vanilla decoding and other budget-based baselines across multiple datasets and models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The evaluation is broad and comprehensive, covering multiple datasets and model sizes, which strengthens the generality of the results.\n\n\n* The paper provides theoretical guidance through the Budget Allocation Model (BAM), offering a principled foundation for the proposed approach.\n\n\n* The writing is clear and well-structured, making the ideas easy to follow and the contributions well-presented."}, "weaknesses": {"value": "- The novelty is limited. Prior work, such as [1] already explored token allocation at the request level, while this paper applies a similar idea at a finer granularity by decomposing each request into multiple sub-requests. How fundamentally different is this from existing allocation frameworks?\n\n\n\n- The paper does not justify why adaptive budgeting is necessary instead of allowing unlimited tokens with early termination for unpromising or converged reasoning chains. Would early stopping achieve similar or better efficiency without requiring explicit budget allocation?\n\n\n\n- There is no direct comparison to [1] or other related approaches that use early termination or reasoning truncation to reduce computational cost. Including these baselines would better contextualize the contribution.\n\n\n\n- The method requires an additional LLM call for the planning step, which increases the inference cost and latency. How significant is this overhead relative to the total efficiency gains?\n\n\n\n- The use of entropy as a heuristic for estimating uncertainty is not well justified. Could a learned or predictive uncertainty estimator perform better and make the framework more robust?\n\n\n\n- The approach assumes that each task can be reliably decomposed into subtasks, which may not hold for all reasoning domains. How does the method behave when such a decomposition is noisy or ambiguous?\n\n\n\n- The theoretical–practical gap is unclear. Although the paper presents formal equations, many quantities are later approximated heuristically. This raises the question: how much of the observed performance comes from the theoretical model versus empirical tuning?\n\n[1] Efficiently serving llm reasoning programs with certaindex. Fu, Yichao and Chen, Junda and Zhu, Siqi and Fu, Zheyu and Dai, Zhongdongming and Qiao, Aurick and Zhang, Hao."}, "questions": {"value": "See the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jRgK5jvYzn", "forum": "ctspw4CqbS", "replyto": "ctspw4CqbS", "signatures": ["ICLR.cc/2026/Conference/Submission12905/Reviewer_f6TR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12905/Reviewer_f6TR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700718213, "cdate": 1761700718213, "tmdate": 1762923683736, "mdate": 1762923683736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical problem in LLM reasoning, balancing computational efficiency and reasoning quality at test time. The authors introduce PLAN-AND-BUDGET, a lightweight, model-agnostic framework that combines structured query decomposition with adaptive token allocation. Building on a formal Budget Allocation Model (BAM), the framework distributes computation dynamically across sub-questions based on uncertainty estimates, mitigating both overthinking and “underthinking” in reasoning tasks. The study shows that PLAN-AND-BUDGET improves reasoning efficiency across multiple tasks and models without sacrificing accuracy.\n\nI personally like this paper very much. I think it addresses a very important issue in reasoning. This contribution also has the potential to connect with many real-world applications involving reasoning models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. I find the motivation of this study very compelling. It addresses a fundamental issue in LLM reasoning, the need to balance reasoning depth and computational budget, and offers a promising approach grounded in uncertainty theory.\n2. The theoretical framework (BAM) provides a solid foundation, and the derivation of the E3 metric as an efficiency-aware measure is well-justified.\n3. I think the empirical evaluation is thorough, including multiple reasoning benchmarks and model scales. The reported improvements in efficiency and comparable accuracy across datasets are impressive and convincingly support the paper’s claims. \n4. The paper is well written and logically organized. The logic flow is very clear!"}, "weaknesses": {"value": "1. A small suggestion. The paper derives two key principles for effective reasoning: (1) reasoning should be structured; and (2) computation should be adaptive focus. For example, the paper mentions: “Inspired by human problem-solving strategies.” While I agree with both ideas and prior works are well cited in the literature, the derivation would be strengthened by citing more prior work when the strategies are introduced in the framework section, which can ground the proposed methods in solid scientific studies.\n2. I noticed that while accuracy remains comparable, the ROUGE scores in some instruction-following tasks slightly decrease. It might be useful for the authors to discuss whether this trade-off arises from task differences and how PLAN-AND-BUDGET might be adjusted for scenarios requiring richer generative outputs."}, "questions": {"value": "1. This is just a question I am curious about. Upon reviewing the tables, I noticed that for some tasks, such as TravelPlanner, the token consumption is not significantly reduced. I would be interested to hear the authors’ thoughts on whether there might be more aggressive or adaptive strategies to further reduce token usage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I don't find any potential ethical concerns in this paper."}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HJxygUr1qh", "forum": "ctspw4CqbS", "replyto": "ctspw4CqbS", "signatures": ["ICLR.cc/2026/Conference/Submission12905/Reviewer_Mzjt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12905/Reviewer_Mzjt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761782145890, "cdate": 1761782145890, "tmdate": 1762923683457, "mdate": 1762923683457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the inefficiency of reasoning processes in large language models (LLMs), where long chains of thought (CoT) can sometimes help performance but also lead to two failure modes: overthinking and underthinking. The authors argue that these issues stem from uncalibrated budget usage—that is, the model’s inability to decide how much computation (tokens) to spend on each sub-question. To address this, the paper introduces a theoretical framework, BAM (Budget Allocation Model), which formulates an optimal trade-off between correctness and computation efficiency based on uncertainty reduction. Building on BAM, the authors propose PLAN-AND-BUDGET, a model-agnostic, test-time method that decomposes complex reasoning queries into sub-questions and allocates token budgets adaptively according to estimated difficulty. Extensive experiments across multiple open-source and closed-source LLMs (e.g., DeepSeek-R1, Qwen-32B, o4-mini) and diverse benchmarks show that PLAN-AND-BUDGET achieves comparable or improved reasoning accuracy while significantly reducing token usage."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is detailed and transparent, making it easy for readers to understand and replicate the proposed approach.\n\n2. The method design is rigorous: using uncertainty as a proxy for reasoning difficulty and allocating computation where it provides the highest marginal gain is both intuitive and elegant.\n\n3. The experiments are comprehensive, covering multiple models (both open- and closed-source) and diverse benchmarks, demonstrating strong generalizability."}, "weaknesses": {"value": "1. Although the method is conceptually rigorous, Table 3–5 show that the method may still hurt accuracy compared to full-length baselines.\n\n2. Some of the simpler baselines (e.g., Planned Vanilla or Global Budget) already achieve noticeable token savings with minimal performance loss. This raises the question of whether the added complexity of PLAN-AND-BUDGET always justifies its gains."}, "questions": {"value": "1. Based on my understanding of Eq. (6) in the BAM formulation, the strategy prioritizes medium-difficulty sub-questions while allocating fewer tokens to very easy or very hard ones. Could this behavior introduce bias, making the model less likely to attempt challenging reasoning steps?\n\n2. For the different baselines, are all models given the same total token budget limit? If so, how is this limit determined in practice?\n\n3. From my understanding, the method will add certain constraints in the prompt (e.g., “use less than B itokens”). This raises a question: how reliably do LLMs follow these token-limit instructions in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nGl5YCzCJL", "forum": "ctspw4CqbS", "replyto": "ctspw4CqbS", "signatures": ["ICLR.cc/2026/Conference/Submission12905/Reviewer_fET4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12905/Reviewer_fET4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793594103, "cdate": 1761793594103, "tmdate": 1762923683191, "mdate": 1762923683191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies reasoning miscalibration in LLMs, overthinking on easy cases and underthinking on hard ones, and proposes a test‑time framework, PLAN‑AND‑BUDGET, to allocate computation adaptively. The authors first introduce a theoretical Budget Allocation Model (BAM) that models per‑subquestion epistemic uncertainty and derives a closed‑form optimal token allocation under a query‑level budget. They then operationalize this with (i) a Plan step that decomposes a query into sub‑questions and estimates relative difficulty weights, and (ii) a Budget step that assigns local budgets using simple decay schedules (linear/polynomial/exponential/cosine). A new compute‑aware metric, balances accuracy/quality and average decoding tokens. Experiments on MATH‑500, NaturalInstructions, and TravelPlanner with four models (including DS‑Qwen‑32B, DS‑LLaMA‑70B, QwQ‑32B, o4‑mini) show consistent efficiency gains without accuracy loss; e.g., on MATH‑500, DS‑LLaMA‑70B improves (E^3) from 4.55 (Global Budget) to 5.89 (Plan‑and‑Budget), and on TravelPlanner DS‑Qwen‑32B improves from (E^3=0.16) to (0.47) (Polynomial schedule), effectively narrowing the gap to larger models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear formulation & theory: BAM provides a principled lens on how to distribute limited compute across subproblems. \n2. Simple, model‑agnostic implementation: Works at prompt time; no training; adds a lightweight planning LLM whose contribution is controlled by planned baselines. \n3. Consistent gains across tasks/models: Large (E^3) improvements with stable or improved accuracy; front‑loaded schedules (polynomial/cosine) perform best on complex tasks. \n4. Bridging model sizes: The method helps smaller models approximate the efficiency of larger ones on TravelPlanner. \n5. Reproducibility details & prompts provided."}, "weaknesses": {"value": "1. Uncertainty signal not directly validated.\n   The budgeting step is motivated by uncertainty reduction, yet the experiments do not present a correlation between used proxies (e.g., prefix‑entropy drop) and actual downstream gains, nor do they show a head‑to‑head between decay‑only vs. truly measured uncertainty allocation. An ablation that (a) measures token‑level entropy and (b) reallocates budgets on‑the‑fly based on it would strengthen the central claim. \n\n2. Metric definition vs. datasets.\n   (E^3=A^2/T) is defined using accuracy yet ROUGE is used as (A) on NaturalInstructions. The consequences of using a graded similarity metric (not bounded like accuracy) should be discussed; consider normalizing to ([0,1]) and reporting sensitivity of rankings to the choice of metric. \n\n3. Cross‑model comparability of token counts.\n   Since tokenization and API reporting differ, (T) is not strictly comparable across models; thus the statement that \"o4‑mini consistently achieves the highest (E^3)\" may partly reflect tokenizer efficiency rather than true compute efficiency. Adding characters generated, wall‑clock latency, and $ cost would provide a more robust, model‑agnostic view. \n\n4. Notation and clarity.\n   Clean up Eq. (7) and unify symbols for difficulty vs. schedule multipliers. Clarify the semantics of (\\beta) (is higher (\\beta) \"easier\" or \"more complex\"?), and restate the unimodality intuition with a small plot or example. \n\n5. Ablations on the planner.\n   While the planner’s standalone performance is low, it still shapes problem structure and budgets. Add ablations varying planner quality (e.g., weaker/stronger planner; random decomposition; noisy difficulty weights) to test robustness, and report the extra token overhead of planning vs. savings in reasoning tokens. \n\n6. Adaptive vs. fixed decay.\n   The best schedules are front‑loaded. It would be natural to choose the schedule per instance via quick signals (e.g., early prefix entropy), approaching the BAM optimum more closely. A small controller choosing among schedules could be evaluated."}, "questions": {"value": "1. Uncertainty proxies: Which proxies were actually used in experiments (entropy drop, self‑consistency, verifier loss), and where do they enter the allocation beyond the position‑based decay? Can you report correlations between these proxies and realized per‑step utility (accuracy gains vs. tokens)? \n2. Eq. (7) and weights: Please confirm the intended formula (b_{ij}=\\frac{w_{ij}d_{ij}}{\\sum_k w_{ik}d_{ik}}B_i). Also, how are difficulty weights (w_{ij}) produced in non‑math tasks? Do they come from the same planner JSON in Appendix H? \n3. Metric sensitivity: If (A) is ROUGE (0–100) versus accuracy (0/1), how sensitive are the (E^3) conclusions to rescaling (A) to ([0,1])? Could you also report (A/T) to compare with prior \"accuracy per token\" conventions? \n4. Token comparability: Given different tokenizers/reporting, do results change when using characters, latency, or cost as (T)? A small subset analysis would clarify cross‑model claims. \n5. Planner overhead and robustness: How often does planning increase total tokens (planning+reasoning) compared to Vanilla? Please provide distributions (not only averages) and an ablation with no planning but local budgets to separate the two contributions. \n6. When does it hurt? Are there regimes where global budgeting wins (e.g., very short tasks)? Diagnostics (e.g., by difficulty bin) would be helpful. \n7. Since mainstream SOTA LLMs do not support token precision thinking effort controling, I am unaware of the importance of doing budget controllon. Please ellaborate this if possible."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PbSNcQy7lz", "forum": "ctspw4CqbS", "replyto": "ctspw4CqbS", "signatures": ["ICLR.cc/2026/Conference/Submission12905/Reviewer_P17L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12905/Reviewer_P17L"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959665190, "cdate": 1761959665190, "tmdate": 1762923682962, "mdate": 1762923682962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Plan-and-Budget is a test-time framework to address \"reasoning miscalibration\" in LLMs - where models either overthink (waste tokens on simple queries) or underthink (fail on hard queries).  (1) Plan step decomposes queries into sub-questions using a lightweight LLM, estimating complexity for each; (2) Budget step allocates token budgets to sub-questions using decay-based scheduling (more tokens to early/uncertain steps). Introduces BAM - modeling uncertainty reduction as inverse power law, deriving optimal allocation as b_ij = B * (c_ij*β_ij)^(1/(β_ij+1)) / Σ(c_ik*β_ik)^(1/(β_ik+1)). Proposes E3 metric = A^2/T (accuracy squared over tokens) to jointly measure quality and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I feel like this addresses real problem - overthinking/underthinking is genuine issue in reasoning LLMs. Theoretical grounding via BAM provides principled justification beyond heuristics. E3 metric is sensible - A^2/T appropriately weights correctness over pure efficiency unlike A/T. Model-agnostic approach works across multiple architectures without retraining. Comprehensive experiments across three diverse domains with consistent improvements. Clear presentation of decay scheduling strategies (linear/polynomial/exponential/cosine). Nice result showing smaller model matching larger model efficiency. Ablations show both planning and budgeting contribute gains."}, "weaknesses": {"value": "Core assumption is questionable? - uncertainty decomposition (epistemic vs aleatoric) requires Bayesian treatment but paper doesn't actually compute posterior p(theta|D), just hand-waves with \"Monte Carlo approximation\" in Appendix B without showing how this applies to deterministic transformer inference. BAM's power law U_epistemic = c/b^β is asserted not derived - why inverse power law specifically? Parameters c_ij and β_ij are never actually estimated, making Eq 6 theoretical only. Decay schedulers are admitted heuristics that don't implement BAM's optimal allocation. Gap between theory (BAM requires knowing c_ij, β_ij) and practice (use position-based decay) is huge - BAM feels like post-hoc justification for simple heuristic. Additional LLM call for planning adds overhead not fully accounted for in token counts. Complexity estimation by planner LLM (LLaMA-3.1-8B with 48.76% MATH accuracy) is unreliable - how can weak model judge difficulty? Table 3-5 show planning alone helps substantially (Planned Vanilla vs Vanilla) - suggests decomposition is doing most work, not budget allocation. Some results show minimal gains - NaturalInstructions improvements are modest (2.33-> 2.86 E3 for DS-Qwen). E3 metric has quadratic bias toward high accuracy - 90% accuracy with 1000 tokens (E3=81) beats 80% with 500 tokens (E3=128) which seems backwards for efficiency metric. No comparison to simpler baselines like adaptive stopping based on model confidence."}, "questions": {"value": "How do you actually estimate c_ij and β_ij in practice? Paper claims BAM is theoretical foundation but never shows these parameters can be computed. Is the planner LLM's complexity estimation reliable given its poor standalone performance (48.76% on MATH)? Can you compare token counts including planning overhead vs end-to-end latency? Does polynomial decay (best performer) actually implement BAM's allocation or is it just a heuristic that happens to work? Why does E3 use A^2 not A - this heavily penalizes any accuracy drop, is that appropriate for efficiency metric? What happens if you just use confidence-based early stopping without decomposition? Table 3 shows cosine scheduling gets 5.92 E3 vs 5.26 global budget on DS-Qwen MATH - is 12% gain worth the added complexity? How sensitive is performance to number of sub-questions (paper uses 2-5)? Can you show examples where BAM's theoretical allocation differs from decay heuristics and which performs better?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "D8Uy8TXvhp", "forum": "ctspw4CqbS", "replyto": "ctspw4CqbS", "signatures": ["ICLR.cc/2026/Conference/Submission12905/Reviewer_V5hD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12905/Reviewer_V5hD"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007097539, "cdate": 1762007097539, "tmdate": 1762923682729, "mdate": 1762923682729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}