{"id": "0RYazbfSzW", "number": 2808, "cdate": 1757256909593, "mdate": 1759898126105, "content": {"title": "Efficient Benchmarking of Functional Connectivity Modeling via Structure-aware Core-set Selection", "abstract": "Benchmarking the hundreds of available functional connectivity (FC) models on large fMRI datasets is critical for reproducible neuroscience, but is often computationally infeasible, with full-scale comparisons requiring months of compute time. This creates a critical bottleneck, hindering data-driven model selection. To break this bottleneck, we address the challenge of FC benchmarking by introducing a pre-analytical step: selecting a small, representative core-set whose sole purpose is to preserve the relative performance ranking of FC models. We formulate this as a ranking recommendation problem and propose Structure-aware Contrastive Learning for Core-set Selection (SCLCS), a self-supervised framework to select these core-sets.  SCLCS first uses an adaptive Transformer to learn each sample's unique FC structure. It then introduces a novel Structural Perturbation Score (SPS) to quantify the stability of these learned structures during training, identifying samples that represent foundational connectivity archetypes. Finally, it combines this stability-based ranking with a density-aware sampling strategy to ensure the selected core-set is both robust and diverse. On the large-scale REST-meta-MDD dataset, SCLCS preserves the ground-truth model ranking with just 10% of the data, outperforming state-of-the-art (SOTA) selection methods by up to 23.2% in ranking consistency (nDCG@k). To our knowledge, this is the first work to formalize core-set selection for FC model benchmarking, making previously intractable large-scale model comparisons feasible.", "tldr": "We frame functional connectivity benchmarking task as a ranking recommendation problem and propose a self-supervised core-set selection framework that achieves up to 23.2% higher ranking stability than baselines at a 10% sampling rate.", "keywords": ["Functional Connectivity Benchmark", "Core-set Selection", "Network Modeling", "Structure-aware Sampling"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5c6fd62440cfabecf32089a51094660d5f609428.pdf", "supplementary_material": "/attachment/d07b7ea7575becfe57e8b73eec1b8b08ba2375ee.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a core-set selection framework to more effectively benchmark many SPI measures on large fMRI datasets. This method first selects a small but representative subset of subjects, and then use it to rank the SPI methods. The goal is not to maximize the accuracy of any single model, but to preserve the relative ranking between SPI methods in terms of downstream task performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear and convincing. If we can preserve the ranking of SPIs using only a small subset, then SPI selection for a known downstream task becomes easier and faster.\n\n2. The paper provides a well-defined evaluation objective. Instead of reporting absolute accuracy only, the authors explicitly measure how well the core-set preserves the global ordering of SPIs."}, "weaknesses": {"value": "1. The paper claims to promote “diversity” in the core-set, but it is not fully clear which notion of diversity is being optimized. Is it diversity in learned functional structure, or in subject identity, or something else? Could this diversity sampling still cluster subjects along other confounders (for example age, site, motion level), in a way that systematically benefits certain SPIs? The authors should analyze whether the selected core-sets are imbalanced along basic covariates, and whether such imbalance correlates with SPI rankings.\n\n2. This work treats the learned attention matrix $A(X)$ as if it reflects some functional relationship between brain regions and is presented as if it were neurobiologically meaningful, but this is not yet justified. Good performance on fingerprinting or diagnosis does not prove that $A(X)$ corresponds to a biologically valid network. in principle it could simply be a discriminative mathematical signature with no stable neurophysiological interpretation. The paper would be much stronger if it (a) explained the neuroscientific meaning the authors believe $A(X)$ captures, and (b) showed visualizations or qualitative analysis of what patterns are considered “stable” for different tasks (e.g., MDD vs. CN). \n\n3. SPS is defined from training dynamics, i.e., how stable a subject’s learned structure is across epochs. However, SPS may depend on optimizer choice, learning rate schedule, or even small architectural changes in the encoder. The paper should include these sensitivity analysis. Without this, it is hard to know whether SPS is an intrinsic property of the data/subject, or just an artifact of one specific training run.\n\n4. All experiments are built around one clinical setting (MDD). This raises several questions:   \nWould the method still work in a setting where each subject has multiple different scan types (e.g., resting-state fMRI and task fMRI)? If you try to pull all scans from the same subject together in the contrastive loss, does the approach still identify “stable structure,” or does it break because the functional state changes?   \nWhat happens in datasets where each subject only has a single scan (no multiple time segments for contrastive learning)? Is the method still applicable, or does it fundamentally rely on repeated measures per subject?"}, "questions": {"value": "1. The authers mentioned that different papers can draw contradictory scientific conclusions from the same SPI. What makes you believe that you can build a unified SPI based on such agreement? \n\n2. In several settings, the “random” selection baseline performs surprisingly well, sometimes close to or even better than more carefully designed selection strategies. This is especially noticeable at certain sampling rates. How should we interpret this? Is random performing well because the dataset is already large and diverse, so almost any 30–50% subset is representative? Or is it because some SPIs are relatively insensitive to which subjects are chosen? Also, why do we sometimes see that an intermediate sampling rate (e.g., 30%) looks worse than 10% or 50%? Is there a theoretical or empirical explanation for these non-monotonic patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "skPBnQXRx3", "forum": "0RYazbfSzW", "replyto": "0RYazbfSzW", "signatures": ["ICLR.cc/2026/Conference/Submission2808/Reviewer_JU9n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2808/Reviewer_JU9n"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880923151, "cdate": 1761880923151, "tmdate": 1762916385556, "mdate": 1762916385556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SCLCS, a core-set selection method for comparing various functional connectivity operators based on statistical pairwise interactions (SPIs) fairly and efficiently. By leveraging the convergence stability of attention matrices, structure perturbation score (SPS), SCLCS evaluates each sample’s structural representativeness and uses this to select the representative core-set for ranking different SPIs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Raises an unaddressed, important research question in the field of neuroimage analysis (efficient comparison between different SPIs)\n- Theoretical rigor is well aligned with the research motivation."}, "weaknesses": {"value": "- Ultimately, the work could provide further insight to the neuroscience community if SCLCS could be used to derive experimental suggestions on which SPIs are suggested to be representative for constructing the functional connectivity matrices. The experiments had a limited scope on validating that the efficiency and robustness of the core-set selection of SCLCS.\n- Connectivity patterns constructed from different SPIs can be significantly susceptible to different pre/post-processing of the BOLD timeseries. Stability across different BOLD pre/post-processing could have been addressed."}, "questions": {"value": "### Major\n- Please provide robustness analysis results across different-sized windows for the sliding-window approach, and see if the method is robust across the window size.\n- Please provide results on the effect of different pre-processing pipelines, at least the selection of different atlases, for the rebuttal.\n- Please provide an interpretation of the ranking result across different SPIs. Would there be a recommended SPI for constructing the connectivity matrix? If so, why would that SPI be representative in terms of neuroscientific literature?\n\n\n### Minor\n- Notational precision can be further revised. (e.g., inconsistent use of $X$ and $\\mathbf{X}$)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6D43HxkWZI", "forum": "0RYazbfSzW", "replyto": "0RYazbfSzW", "signatures": ["ICLR.cc/2026/Conference/Submission2808/Reviewer_sSb7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2808/Reviewer_sSb7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977864282, "cdate": 1761977864282, "tmdate": 1762916385377, "mdate": 1762916385377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the prohibitive cost of benchmarking functional connectivity (FC) “signal processing indices” (SPIs) by reframing core-set selection as a **ranking-preservation** problem rather than single-model accuracy. It proposes **SCLCS**, a structure-aware, self-supervised framework that (i) encodes sample-wise FC structure with a modified Transformer, (ii) scores sample stability via a **Structural Perturbation Score (SPS)** over training epochs, and (iii) augments selection with **density-balanced sampling** to avoid top-k brittleness. On REST-meta-MDD (904 subjects; DMN 33 ROIs; 4,520 temporal segments), SCLCS preserves SPI rankings substantially better than nine strong baselines and maintains balanced subject/class coverage, while being orders-of-magnitude cheaper than full benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Well-motivated objective and metricization.**  \n  Focusing on cross-model *ranking* (not accuracy) matches the real need in SPI selection; nDCG@k is a sensible measure. \n- **Principled structure-aware selection.**  \n  SPS is theoretically grounded (mixture-driven perturbation; stationarity/ergodicity) and operationally tied to the encoder’s attention dynamics; density-balanced sampling addresses the known failure mode of top-k. \n- **Comprehensive empirical study and practicality.**  \n  The study spans two downstream tasks, nine baselines, coverage/fairness diagnostics, and compute analysis showing that a small one-time selection cost can replace ~990 CPU-days of exhaustive SPI evaluation."}, "weaknesses": {"value": "1. **External validity limited by parcellation and preprocessing choices.**  \n  Experiments are confined to 33 DMN ROIs (Dosenbach-160) with global signal regression; it remains unclear whether conclusions (e.g., SPS behavior, density effects, ranking stability) hold for whole-brain parcellations (e.g., Schaefer, AAL), alternative TRs, or non-GSR pipelines.\n2. **Ground-truth ranking restricted to *fast* SPIs.**  \n  To keep evaluation tractable, the “full-set” ranking is computed on a subset of SPIs (<1s/sample). This could bias findings toward operators with particular computational/statistical properties; evidence of transfer to slower/iterative SPIs is limited.\n3. **Surrogate discriminability and stability assumptions.**  \n  Ranking relies on a discriminability score derived from Spearman correlations; SPS assumes stationarity/ergodicity and measures attention perturbations. Although supported by analyses, sensitivity to the discriminability proxy and to encoder hyperparameters (heads, depth, temperature) is not fully characterized.\n4. **Self-supervision via subject identity may conflate fingerprinting with diagnosis.**  \n  Positive pairs are temporal segments from the same subject. While this is apt for fingerprinting, its alignment with clinical separability (MDD vs. HC) is indirect; the paper partially explores label influence, but a principled study of supervision choices (subject vs. site vs. phenotype) is missing. \n5. **Site effects and dataset shift under-analyzed.**  \n  Coverage balance (subject/class) is reported, but explicit *site-aware* balance and ranking stability across sites (or leave-site-out coresets) are not provided; this is critical for multi-site rs-fMRI. \n6. **Theory–practice gap for universality claim.**  \n  The “universal approximator for SPIs” is empirically checked on 16 operators; many SPIs (e.g., cointegration tests) are statistical procedures with thresholds/optimization loops. Clarification is needed on the operator class for which approximation guarantees are intended and how approximation error propagates to ranking.\n7. **End-to-end wall-clock benefits not fully quantified.**  \n  The paper convincingly contrasts selection cost vs. exhaustive benchmarking (~990 CPU-days), but does not report actual *end-to-end* benchmarking speedups (e.g., core-set size vs. hours on a realistic cluster), which would clarify practical impact.\n\n**Minor**\n- Fig. 1 caption/text: \"**Construstive** Learning\" → \"**Contrastive** Learning.\""}, "questions": {"value": "1. **Parcellation & preprocessing robustness.**  \n  How do core-set ranking nDCG@k and coverage metrics change under alternative parcellations (e.g., Schaefer-200) and without GSR? Please include sensitivity to window length/stride (70/35 TRs vs. others). \n2. **SPI subset bias.**  \n  Can you report results where the full-set ranking includes a stratified sample of *slow* SPIs (e.g., iterative/graphical models) to test transfer beyond <1s/sample operators? Even a small but representative slow-SPI slice would help. \n3. **Supervision choice.**  \n  Beyond subject identity, have you tried *site-ID* or *multi-task* (subject + site + MDD/HC) supervision when computing SPS? Does supervision alter SPS distributions and downstream ranking stability? \n4. **Site-aware analysis.**  \n  Please provide site-conditioned ranking preservation (per-site nDCG@k) and site-aware coverage (selected samples per site) or a leave-site-out core-set test to assess generalization under dataset shift.\n5. **Universality scope.**  \n  What is the formal class of SPIs covered by the universality claim? For statistical-test SPIs (e.g., Johansen cointegration), how does approximation error affect ranking, and can you bound nDCG degradation as a function of operator approximation error? \n6. **Compute benefits in practice.**  \n  Could you report wall-clock benchmarks (e.g., 10%/20% core-set vs. 100% full benchmark) on a standard 32–128 vCPU cluster to quantify realized speedups?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mAEr1536mo", "forum": "0RYazbfSzW", "replyto": "0RYazbfSzW", "signatures": ["ICLR.cc/2026/Conference/Submission2808/Reviewer_7LHN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2808/Reviewer_7LHN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998963630, "cdate": 1761998963630, "tmdate": 1762918302331, "mdate": 1762918302331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an idea for performing core-set (small, representative subset of data that preserves some performance metric) selection for functional connectivity model benchmarking in neuroimaging. The core-set selection is based on statistical pairwise interaction type measures and preserving ranking among methods that use SPIs for modeling functional connectivity. They introduce a technique called SCLCS (Structure-aware Contrastive Learning for Core-set Selection)—a self-supervised Transformer framework that learns sample-specific FC structure representations, defines a score (structural perturbation score) to measure structural stability and use a density-based sampling technique to ensure the resulting core-set is both robust and diverse. \n\nThe authors show experimental results on the REST-meta-MDD dataset, where their method achieves up to 23% higher ranking consistency than other coreset selection methods with a fraction (10%) of the data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "While the concept of core-set selection is not new, to my knowledge, this is the first work that is undertaking this problem for ranking functional connectivity performance. This is the main strength of the paper. \n\nThe issue of core-set selection has been mostly ignored in neuroimaging, mainly because the datasets are generally small. However, as the number of datasets are growing in size, the number of algorithms are also getting contributed and tested, the problem of benchmarking is suddenly becoming important. Thus the overall concept that the paper proposes does make a contribution in the right direction\n\nThe overall theoretical framework of ranking preservation, the SCLCS, as well as the adapted transformer architecture is sound and is a novel contribution for this application."}, "weaknesses": {"value": "The main weakness in the paper is that the ranking is based on Statistical Pairwise Interactions (SPIs). Although the authors test 130 SPIs, these are functional connectivity estimators and not end to end predictive machine learning models. SPIs are generally used in classical fMRI analyses and not widely used in modern ML based approaches including deep learning. \n\nIn modern data-driven models (graph neural nets, attention-based architectures), the functional network connectivity structure is learned jointly with metric optimization, and thus make SPI type analysis unnecessary. \n\nSince the method is mainly based on SPIs,  it has limited adaptability to non-SPI workflows/benchmarks. The model ranking objective assumes each model produces a connectivity matrix via a pairwise operator. Further, the structure-aware feature extraction is built around this notion of symmetric, subject-level matrices. Finally the evaluation metric nDCG makes sense only if multiple SPIs are given as input for comparison. The benchmarking and core-set selection strategy therefore applies directly to workflows that explicitly compute connectivity features before classification. \nThus the method may not generalize to deep learning models trained end-to-end on fMRI data, graph-neural network type approaches that infer connectivity structure and optimize it internally before prediction as well as other fMRI prediction algorithms, where the internal structure is not amenable for SPI comparison. \nThe experimental evaluation is only done on a single dataset and only two downstream tasks to evaluate SPI discriminability: brain fingerprinting (distinguishing individuals based on subject ID), which probes for fine-grained, subject-specific structures, and MDD diagnosis, which relies on cohort-level patterns."}, "questions": {"value": "Are SPIs still widely used for prediction in fMRI?\n\nTheorem 1 assumes row-stochasticity of matrices, but can it be directly applied to the real valued symmetric matrices arising from SPIs? \n\nIs model ranking a valid goal?\n\nCan the authors comment on the adaptability of the method to deep learning models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AGTuUdSAFI", "forum": "0RYazbfSzW", "replyto": "0RYazbfSzW", "signatures": ["ICLR.cc/2026/Conference/Submission2808/Reviewer_83WM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2808/Reviewer_83WM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762040217173, "cdate": 1762040217173, "tmdate": 1762916384854, "mdate": 1762916384854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}