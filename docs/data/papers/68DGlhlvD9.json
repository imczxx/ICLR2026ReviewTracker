{"id": "68DGlhlvD9", "number": 5383, "cdate": 1757906108989, "mdate": 1759897978669, "content": {"title": "UltraLLaDA: Scaling the Context Length to 128K for Diffusion Large Language Models", "abstract": "Diffusion LLMs have attracted growing interest, with plenty of recent work emphasizing their great potential in various downstream tasks; yet the long‑context behavior of diffusion LLMs remains largely uncharted. We present a case study of post‑training techniques for extending the context window of diffusion LLMs (i.e., LLaDA) without retraining from scratch. We show that a simple modification to the standard Rotary Positional Embeddings (RoPE) extension effectively accommodates the probabilistic modeling inherent in the diffusion process, enabling stable scaling to longer context ranges. We further compare masking strategies used during post‑training and analyze their impact on optimization stability and long‑range recall. Instantiating these insights, we introduce UltraLLaDA, a diffusion LLM with a 128K‑token context window that, in our empirical evaluation on long‑context tasks, significantly outperforms training‑free baselines. Our experimental results highlight the special positional extension as a key lever for scaling diffusion LLMs to extended contexts and offer practical guidance for practitioners seeking 128K‑scale context via efficient post‑training.", "tldr": "", "keywords": ["diffusion language model; long context LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/140533f74b2ff87a2295acf62d92d4f719207249.pdf", "supplementary_material": "/attachment/5c0ccc94b07cc559b04e5514b7bed9cd93463e3b.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces UltraLLaDA, a method for extending the context window of diffusion-based Large Language Models (LLMs) to 128,000 tokens through an efficient post-training process. The research addresses a critical and largely unexplored area, as the long-context capabilities of diffusion LLMs have not been systematically studied.   \n\nThe authors propose two main contributions:\n\nDiffusion-aware NTK: A novel adaptation of the Neural Tangent Kernel (NTK) method for scaling Rotary Positional Embeddings (RoPE). The key insight is that diffusion models, with their bidirectional attention mechanism, learn a much wider range of relative positions during pre-training (approximately twice the context length) compared to auto-regressive models. By accounting for this property, the authors develop a more suitable RoPE scaling factor that enables stable extrapolation to very long contexts.   \n\nMasking Strategy Analysis: The paper systematically investigates data packing and attention masking strategies to mitigate \"cross-document interference\" during long-context fine-tuning—a significant challenge for models with global bidirectional attention. It compares adaptive attention masking (which blocks attention between concatenated documents) and end-of-document (EOD) token concatenation against a naive direct concatenation baseline.   \n\nEmpirically, UltraLLaDA demonstrates remarkable performance. It achieves 100% accuracy on the \"Needle-in-a-Haystack\" (NIAH) retrieval task at the full 128K context length. Across a suite of benchmarks, including Perplexity, LongBench, and RULER, UltraLLaDA consistently and significantly outperforms the base LLaDA model and a training-free extension baseline (LongLLaDA), with the performance gap widening as context length increases. Ablation studies confirm that both the Diffusion-aware NTK and the use of boundary-aware masking strategies are essential for achieving these results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Addresses a novel and important problem: long-context extension for diffusion LLMs. As diffusion models gain traction, understanding how to scale their context window is crucial for their practical application and competitiveness.   \n\nThe model's performance is a standout feature. Achieving perfect 100% accuracy on the 128K NIAH task is a powerful demonstration of the method's effectiveness in long-range information retrieval. The consistent and significant improvements over baselines across multiple diverse benchmarks (PPL, LongBench, RULER) provide robust evidence supporting the authors' claims.   \n\nThe core technical contributions are simple, intuitive, and clearly justified. The adaptation of NTK scaling is based on a clear-eyed observation of the architectural differences between diffusion and auto-regressive models. Furthermore, the systematic study of masking strategies provides valuable, practical insights for training such models.\n\nThe paper includes thorough ablation studies that successfully isolate the impact of each key component (the NTK variant and the masking strategy). This experimental rigor greatly strengthens the validity of the conclusions and clearly demonstrates that both proposed techniques are necessary for the final performance."}, "weaknesses": {"value": "The narrow set of baselines, all comparisons are internal to the diffusion model LLaDA and LongLLaDA. Considering that this is relatively new in diffusion LLM, this is understandable, but perhaps could consider migrating other methods commonly used in auto-regression models for comparison, such as PI and YARN, to provide more insights.\n\nThe appendix reveals that UltraLLaDA's performance on standard short-context benchmarks degrades after long-context fine-tuning. This is a critical trade-off common in context extension methods but is not addressed in the main body of the paper. Acknowledging and analyzing this limitation in the main text would provide a more balanced and complete picture of the method's characteristics.   \n\nThe evaluation could be strengthened by incorporating more challenging long-context reasoning benchmarks to provide a more comprehensive assessment of the model's capabilities beyond information retrieval."}, "questions": {"value": "Q1: In Section 3.2, the explanation provided for $T_{\\text{cap}}$ and $T_{\\text{Ecap}}$ being twice as large in diffusion LLMs compared to auto-regressive LLMs is intuitive. However, the argument would be significantly strengthened by a more detailed theoretical derivation or formal proof to rigorously support this conclusion.\n\nQ2: The paper compares the model's training-free performance at critical dim = 64 versus 70. It would be insightful to see an analysis of performance at other proximal values (e.g., 69 or 71). Investigating whether 70 represents an optimal or near-optimal setting would provide stronger validation for the optimization process of the Diffusion-aware NTK method.\n\nQ3: The evaluation of long-context capabilities currently concentrates primarily on retrieval tasks. To provide a more comprehensive assessment, please consider including benchmarks that evaluate long-context reasoning abilities, such as the relevant tasks in LongBench v2.\n\nQ4: The experiments cap the maximum sequence length extension at 128k. Could the authors clarify the rationale for this specific limit? It would be beneficial to understand the method's performance at even greater lengths or, alternatively, to discuss the method's ultimate scaling limit (e.g., what is the maximum extension factor this approach can effectively achieve?)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kAv8K162O5", "forum": "68DGlhlvD9", "replyto": "68DGlhlvD9", "signatures": ["ICLR.cc/2026/Conference/Submission5383/Reviewer_S1Du"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5383/Reviewer_S1Du"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5383/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761564738368, "cdate": 1761564738368, "tmdate": 1762918032559, "mdate": 1762918032559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents UltraLLaDA, a method for extending the context length of diffusion-based large language models to 128K tokens. The approach introduces a diffusion-aware NTK scaling technique and explores various masking strategies for handling long-context documents. Experimental results demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n\n2. The proposed diffusion-aware NTK scaling method is simple yet effective, showing a certain level of novelty.\n\n3. The paper conducts extensive experiments that clearly demonstrate the effectiveness of the proposed approach."}, "weaknesses": {"value": "1. The proposed diffusion-aware NTK scaling is primarily based on an empirical assumption that a diffusion LLM can naturally handle a wider range of relative positions, with $T_{cap} \\sim 2T_{train}$ and $T_{Ecap} \\sim 2T_{target}$. It would be more convincing if the paper provided deeper theoretical justification or analysis for this assumption.\n\n2. The investigation of different masking techniques for long documents shows limited novelty, as similar findings have already been discussed in prior studies on large language models[1,2].\n\n[1] LongRoPE2: Near-Lossless LLM Context Window Scaling\n\n[2]  The Llama 3 Herd of Models, https://arxiv.org/abs/2407.21783"}, "questions": {"value": "See the weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9pJZjaygIn", "forum": "68DGlhlvD9", "replyto": "68DGlhlvD9", "signatures": ["ICLR.cc/2026/Conference/Submission5383/Reviewer_ev1L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5383/Reviewer_ev1L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5383/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808985000, "cdate": 1761808985000, "tmdate": 1762918032354, "mdate": 1762918032354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UltraLLaDA, a post-training approach designed to extend the context window of diffusion-based large language models (specifically LLaDA) from 4K to 128K tokens. The authors make two main contributions: first, they develop a diffusion-aware NTK scaling method that refines standard RoPE extrapolation to better suit the bidirectional attention mechanisms of diffusion models, using a context cap of approximately twice the training length rather than the single-length limit typical of autoregressive models. Second, they explore several masking strategies for managing multi-document concatenation during training, comparing adaptive masking, the use of explicit end-of-document (EOD) tokens, and direct concatenation. UltraLLaDAdemonstrates strong performance on long-context benchmarks such as NIAH, LongBench, and RULER, outperforming the training-free baseline, LongLLaDA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Systematic study of post-training methods for extending context in diffusion LLMs, addressing an underexplored area.\n2. Well-motivated technical approach. The diffusion-aware NTK modification is intuitive, accounting for bidirectional attention means the model sees roughly 2x the relative position range during training.\n3. Comprehensive evaluation. Multiple benchmarks (PPL, NIAH, LongBench, RULER) across context lengths up to 128k tokens\n4. Lightweight post-training (600 steps) makes the approach accessible."}, "weaknesses": {"value": "1. Single base model. All experiments use only LLaDA-8b as the base. Generalization to other diffusion LLMs or different model sizes is unclear.\n2. LongLLaDA baseline cannot be evaluated beyond 32k, making comparisons incomplete at the longest contexts.\n3. Masking strategy conclusions unclear. Tables 4-5 show adaptive masking and EOD concatenation trading advantages at different lengths, but the paper doesn't provide clear guidance on which to use when. The difference between all methods are very small and could be attributed to noise.\n4. No comparison against YaRN, a more widely used RoPE interpolation method."}, "questions": {"value": "1. Can you provide theoretical or empirical analysis showing that diffusion models actually learn relative positions in the range [-2T_train, 2T_train] during pre-training?\n2. Why was NTK-aware/ABF RoPE scaling used instead YaRN? Is there a intrinsic problem with YaRN that prevents it from being adapted to diffusion models? SoTA models are mostly always using YaRN instead of NTK scaling.\n3. Have you tested this approach on other diffusion LLMs or other model sizes? Would it work without finetuning on larger model sizes? What about other architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Up2B2e6pmO", "forum": "68DGlhlvD9", "replyto": "68DGlhlvD9", "signatures": ["ICLR.cc/2026/Conference/Submission5383/Reviewer_eBLu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5383/Reviewer_eBLu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5383/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957245276, "cdate": 1761957245276, "tmdate": 1762918032121, "mdate": 1762918032121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UltraLLaDA, a diffusion LLM post-trained from LLaDA with a modified RoPE scaling method to extend the context length to 128K. UltraLLaDA proposes a diffusion-aware NTK/RoPE scaling method. The authors argue that standard NTK-aware RoPE scaling (as used in autoregressive long-context extension) is suboptimal for diffusion LLMs because diffusion models use bidirectional attention rather than causal attention. They propose a modified scaling that better reflects the positional distance statistics of bidirectional denoising. They then conduct long-context post-training on 64K length packed sequences, exploring several strategies for handling multiple concatenated documents: naive concatenation, EOD-token concatenation, and adaptive attention masking. The experimental results show that UltraLLaDA maintains near-perfect Needle-in-a-Haystack retrieval up to 128K and keeps perplexity stable out to 128K, whereas both the original LLaDA and the training-free method LongLLaDA collapse much earlier."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Good practical significance for diffusion LLMs.**  \n   Ultra-long context capability in diffusion LLMs has not been well explored. This work provides a realistic recipe to reach 128K with stable retrieval and usable perplexity, which is a meaningful capability milestone for dLLMs.\n\n2. **Comprehensive study of the proposed diffusion-aware NTK.**  \n   The paper presents a detailed analysis of the proposed diffusion-aware NTK, comparing it with LongLLaDA's baseline NTK, and shows the empirical improvement of diffusion-aware NTK in the training-free setting. This analysis is convincing.\n\n3. **Comprehensive experimental results and ablations.**  \n   This work conducts experiments on various long-context tasks, including Needle-in-a-Haystack, LongBench, and RULER, for all baselines (LLaDA, LongLLaDA) and UltraLLaDA trained with different sentence packing strategies. This thorough comparison supports the proposed method.\n\n4. **Clear writing.**  \n   The paper is clearly written and easy to follow."}, "weaknesses": {"value": "1. **Core novelty feels incremental.**  \n   The main contribution of this work is diffusion-aware NTK. While the motivation (bidirectional vs. causal attention) is reasonable, the scaling rule itself shows only a moderate improvement over LongLLaDA’s baseline scaling when evaluated without post-training. From Table 4 and Table 5, even with post-training, the improvement for 4K–16K context lengths still seems small. The three long-context post-training sentence-packing strategies explored in this paper are also existing methods in autoregressive post-training. Applying and comparing them in the diffusion LLM setting is empirically valuable but not conceptually new.\n\n2. **Limited analysis of non-retrieval reasoning at longer context lengths.**  \n   Most ultra-long-context evaluations are on NIAH and perplexity stability. The paper does evaluate on LongBench with 16K context length and RULER at 32K, but there is less evidence for complex multi-document synthesis or instruction following at 64K–128K.\n\n3. **Lack of evaluation on more models.**  \n   The paper only conducts experiments on LLaDA model. There are multiple kinds of diffusion LLMs: models trained from scratch in diffusion style like LLaDA, models converted from AR like Dream, and block-diffusion-style models. This work should apply the proposed method to more diffusion LLMs to demonstrate generalization."}, "questions": {"value": "1. Could you evaluate UltraLLaDA on more kinds of tasks beyond PPL and NIAH at longer context lengths? Are there any failure cases at 128K context length (because the current NIAH evaluation results seem perfect)?\n\n2. Could you apply the proposed method to models like Dream (which is trained from AR model) and SDAR-series models (which are also trained from AR models with block diffusion style)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZxXuKRy8RY", "forum": "68DGlhlvD9", "replyto": "68DGlhlvD9", "signatures": ["ICLR.cc/2026/Conference/Submission5383/Reviewer_BN3Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5383/Reviewer_BN3Q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5383/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762140693286, "cdate": 1762140693286, "tmdate": 1762918031914, "mdate": 1762918031914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}