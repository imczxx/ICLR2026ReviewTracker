{"id": "nhFebxzbTl", "number": 21661, "cdate": 1758320248738, "mdate": 1759896910016, "content": {"title": "metabeta - A fast neural model for Bayesian mixed-effects regression", "abstract": "Hierarchical data with multiple observations per group is ubiquitous in empirical sciences and is often analyzed using mixed-effects regression. In such models, Bayesian inference gives an estimate of uncertainty but is analytically intractable and requires costly approximation using Markov Chain Monte Carlo (MCMC) methods. Neural posterior estimation shifts the bulk of computation from inference time to pre-training time, amortizing over simulated datasets with known ground truth targets. We propose metabeta, a transformer-based neural network  model for Bayesian mixed-effects regression. Using simulated and real data, we show that it reaches stable and comparable performance to MCMC-based parameter estimation at a fraction of the usually required time.", "tldr": "This paper showcases a pretrained neural model that efficiently approximates posterior inference for mixed-effects regression.", "keywords": ["neural posterior estimation", "mixed-effects regression", "approximate bayesian computation"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3b4e51983ccb3b5389ea579cc8c76d619c430617.pdf", "supplementary_material": "/attachment/92f127a7498e6c985f5c7dd1f40ce215899cada2.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents a simulation-based inference approach for Bayesian mixed-effects regression called metabeta. Hierarchical models are an important and widely used class of models, and efficient inference for them remains an active research topic. The authors train a single model that supports multiple priors, instead of training one model per prior as in previous work, and they employ a transformer-based architecture. The approach aims to achieve MCMC-level accuracy with greatly reduced inference time. The paper is clearly written, well-motivated, and addresses an interesting and timely problem in Bayesian inference for hierarchical models. It builds upon recent advances such as those by Habermann et al. and  Hollmann et al., extending neural posterior estimation to a broader mixed-effects setting with multiple priors. This extension is both conceptually interesting and practically meaningful."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "This paper addresses an important and timely problem: Bayesian inference for hierarchical models. The proposed metabeta framework is interesting, combining transformer-based neural posterior estimation with amortization across multiple priors. The approach shows promise and could meaningfully reduce inference cost in mixed-effects regression. Strengths are\n- Novel and well-motivated application of neural posterior estimation to mixed-effects regression, and effectively leveraging the available likelihood for post-hoc refinement.\n- The integration of multiple priors into a single amortized inference model represents a significant step toward general-purpose Bayesian inference.\n- Comprehensive benchmark suite covering both in-distribution and out-of-distribution data, which supports the claims of generalisation and robustness.\n- Clear presentation and placement within the existing literature, helping the reader understand the contributions of the approach."}, "weaknesses": {"value": "HMC diagnostics are not reported and convergence is doubtful \n\nThe paper reports “divergence and strong outliers” (l.129) in HMC runs but does not provide diagnostics and the authors do not define what constitutes “outliers” (l. 129). Moreover, they do not report effective sample sizes, divergence counts or a similar convergence diagnostics. Unconverged chains should be excluded and metrics computed on all converged runs. The current MAD-based chain selection may bias variance estimates, potentially causing the selected chain of HMC to underestimate posterior variance. This should be justified or replaced with a more standard convergence diagnostic. If convergence issues persist, HMC might need a longer warm-up or more iterations.\n\nMore concretely:\nFigure 1D shows a discrepancy between HMC and metabeta. Given that HMC has asymptotic convergence guarantees, and that HMC uses the “true priors and generative model” (l.128) , this raises questions about whether metabeta is overconfident (e.g., for $\\beta_0$) or whether HMC was improperly tuned. If the latter, longer runs or a different parameterization may be required. Please clarify the setup and report diagnostics. To support the claim that both pipelines are correctly specified, including at least one simple case with a known analytical posterior could be helpful to demonstrate that HMC converges and metabeta matches it. Even when HMC turns out to perform better, the amortisation part is still very valuable. Interesting would also be to compare the methods, when the prior is misspecified.\n\nA quantification of the differences between HMC and metabeta in the posterior predictive distributions would further strengthen the results.\n\nMinor comments\n- In Section 2.1, $S$ is undefined (l.95).\n- “we simulate hierarchically structured datasets using PyTorch“ (l.110), what is the benefit of using PyTorch here for simulation?\n- Figure 1 needs to be improved in terms of overall readability: missing labels (panel C), unreadable fonts, neural networks are badly visible, and too small text in subplots. Make clear that D is only inference time, not training time.\n- Font sizes in all main figures are too small. In general, the resolution of the figures should be improved. \n- The importance sampling refinement is a nice idea. Showing results without this step to isolate its contribution would further improve the paper.\n- The code is not provided, preventing verification of the claim that open-source software and pretrained models are or will become available. Please check out https://anonymous.4open.science/."}, "questions": {"value": "The authors claim that prior work “at best nullifies the runtime advantage of NPE” (l. 68). How does this compare to the authors’ method? In particular there are serval situations, which are not discussed enough in the opinion of the reviewer, such as:\n\n- How can missing data be handled? Many hierarchical datasets contain incomplete observations, and prior work uses masking to circumvent this issue (e.g., “All-in-one simulation-based inference“ by Gloeckler et al.).\n\n- The paper claims full amortization, yet separate models are trained for different parameter dimensionalities. This weakens the claim of applicability for practitioners if the needed parameter dimensionality is not available. The authors should clarify early in the manuscript that amortization holds only for fixed dimensionality $d$ and group structure $q$ and that multiple models are trained. Also, here they could connect to recent work, such as \"Compositional amortized inference for large-scale hierarchical Bayesian models\" by Arruda et al.\n\n- How are priors incorporated as inputs: through direct parameterization or learned embeddings? What about priors which are not part of the training data? This should be clarified in the main maunscript and potentially the relation to Whittle et al. “Distribution Transformers: Fast Approximate Bayesian Inference with On-The-Fly Prior Adaptation“ discussed, which seems closely related.\n\nPlease report computational cost for training metabeta, including total training time for the different models and resource usage. This is essential for assessing amortization trade-offs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WtGzmjqsoy", "forum": "nhFebxzbTl", "replyto": "nhFebxzbTl", "signatures": ["ICLR.cc/2026/Conference/Submission21661/Reviewer_4rdr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21661/Reviewer_4rdr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760988568723, "cdate": 1760988568723, "tmdate": 1762941878133, "mdate": 1762941878133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a meta-learning framework for estimating the posterior distribution of coefficients in a Bayesian linear mixed-effects model, leveraging transformer architectures in a manner similar to TabPFN. To enhance the calibration of the predictive distribution, an additional post-hoc refinement stage—such as importance sampling or conformal prediction—is incorporated. The proposed approach is evaluated on both synthetic and semi-synthetic toy datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Employing transformer-based meta-learning for diverse forms of amortized Bayesian inference is a compelling and important research direction. This work represents a valuable contribution to this growing area."}, "weaknesses": {"value": "As noted by the authors, a key limitation is that a pretrained metabeta model can only be applied to datasets with a specific number of fixed effects and random effects. This constraint reduces the practical impact of the approach, as the model must be retrained for each new dataset with differing dimensionality. In such cases, the benefit of amortization becomes less compelling than using e.g., MCMC directly.\n\nAnother limitation is that all experiments are conducted on small-scale synthetic and semi-synthetic datasets, leaving the performance on real-world datasets uncertain."}, "questions": {"value": "1. Could the authors describe the model details of the normalizing flow, particularly clarifying how the input s is utilized within the flow?\n\n2. Did the authors employ both post-hoc refinement methods in the experiments, or was only one method used?\n\n3. Could the authors provide experimental results on real-world datasets? Although the ground-truth effects are unknown, the model’s performance could be assessed by comparing the prediction accuracy of a linear mixed-effects model whose coefficients are estimated using metabeta or HMC."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "96uA0UdMTY", "forum": "nhFebxzbTl", "replyto": "nhFebxzbTl", "signatures": ["ICLR.cc/2026/Conference/Submission21661/Reviewer_dsuQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21661/Reviewer_dsuQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761692726050, "cdate": 1761692726050, "tmdate": 1762941877844, "mdate": 1762941877844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for neural amortized Bayesian inference (ABI) for linear mixed-effects (multilevel/hierarchical) models. The results are overall good but the contribution may be a bit small in light of existing literature. I am short on time due to the semester start. Apologies if my reviews are a bit short. I am happy to engage in reviewer discussion should be concerns not be clear."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper works on an important topic. \n- The applied and combined methods are sensible.\n- The presentation is easy to follow for somone familiar with mixed-effects models."}, "weaknesses": {"value": "- The contribution is overall small. Already previous papers provide ABI for multilevel models (and are correctly cited in the paper). The main addition here is the amortization over prior hyperparameters, but this has also been suggested in other places (https://arxiv.org/abs/2310.11122), althought admittedly not in a multilevel context.\n- The HMC baseline seems to be incorrectly or at least not well implemented. For such simple multilevel models, HMC in PyMC or other PPLs should not struggle with any convergence or recovery issues. I assume your parameterization of the model wasn't quite right or optimal. Consider using a non-centered parameterization for the random effects. Or compare with an existing implementation of such model, e.g., via the brms R package using Stan as PPL backend, or bambi in python using PyMC as backend. \n- The authors only focus on *linear* multilevel models where the error distribution is Gaussian. This unecessarily restricts the flexibility of the framework. \n- No correlations between random effects are considered. \n- The general formulation in terms of design matrices X and Z suggest the possibility of multiple grouping factors and corresponding random effects. Yet, your implementation just supports a single grouping factor as far as I can tell. I don't expect you to generalize your framework to multiple grouping factors right away. Just make this point more explicit. \n- The term \"transformer-based\" perhaps oversells the point a bit that you use set transformer as summary networks."}, "questions": {"value": "- The authors mention the aim of releasing a pretrained version of the model for free use. However, I am not sure if an \"aim\" is already a contribution of the paper. How far is the pretrained version?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TCzualRNO1", "forum": "nhFebxzbTl", "replyto": "nhFebxzbTl", "signatures": ["ICLR.cc/2026/Conference/Submission21661/Reviewer_58pV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21661/Reviewer_58pV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833977390, "cdate": 1761833977390, "tmdate": 1762941877395, "mdate": 1762941877395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes metabeta, an amortized Bayesian inference framework for mixed-effects\n(hierarchical) regression. The method draws on techniques from simulation-based\ninference (SBI), particularly neural posterior estimation (NPE) with normalizing flows\nand permutation-invariant set encoders. By pretraining on many simulated hierarchical\ndatasets under varying priors, metabeta learns to approximate posteriors over both\nglobal and local parameters of linear mixed-effects models (LMEs). At inference time,\nusers can provide their own priors and data, and the network outputs approximate\nposteriors within seconds—potentially replacing MCMC for common hierarchical modeling\ntasks. To improve calibration, the authors add post-hoc importance sampling (IS) using\nthe analytic LME likelihood and conformal prediction to adjust coverage. Experiments\ncompare metabeta to Hamiltonian Monte Carlo (HMC) on toy and real data, showing large\nspeed gains and competitive accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Timely and practically motivated: The work targets a real bottleneck (computational\n  cost of Bayesian mixed-effects regression) and adapts amortized SBI tools for this\n  setting. This is an interesting application domain for amortized inference.\n- Clear architecture design: The hierarchical set-transformer encoder combined with\n  flow-based posterior heads is a reasonable and interpretable choice. The post-hoc\n  importance sampling and conformal calibration steps are conceptually clean and\n  computationally lightweight.\n- Empirical performance: On toy and real hierarchical datasets, metabeta achieves\n  parameter recovery and coverage comparable to HMC at a fraction of inference time. The\n  results demonstrate that amortized models can produce usable posterior approximations\n  in classical regression problems.\n- Potential impact: If validated under fair comparison, such amortized inference could\n  make Bayesian mixed-effects modeling far more accessible to applied fields (social\n  sciences, bioinformatics, etc.) where MCMC remains the default."}, "weaknesses": {"value": "### Conceptual framing and related work\n\n- The paper is not truly an SBI setting: the LME simulator and likelihood are\n  analytically known. metabeta uses SBI tools for amortized efficiency, not because\n  inference is likelihood-free. This distinction should be made explicit and more prominent.\n- The historical narrative around NPE and BayesFlow is inaccurate. Neural Posterior\n  Estimation (also the amortized version) was introduced by Papamakarios et al. (2016) and extended by Lueckmann et\n  al. (2017) and Greenberg et al. (2019). BayesFlow (Radev et al., 2020) later provided\n  a practical amortized inference framework with set encoders, not transformers.\n  Transformer-based amortized inference (e.g., Whittle et al., 2025; Mittal et al.,\n  2025; Reuter et al., 2025) represents a distinct and more recent line of work. The\n  related work section should reflect this chronology.\n- The related work section omits key hierarchical SBI approaches such as Rodrigues et\n  al. (NeurIPS 2021). The absence of these citations distorts context and weakens the\n  claim of novelty.\n\n### Methodology and baselines\n\n- The HMC comparison is potentially unfair. Divergences in HMC typically signal poor\n  tuning rather than algorithmic failure. [Non-centered parameterizations](https://sjster.github.io/introduction_to_computational_statistics/docs/Production/Reparameterization.html), robust\n  step-size tuning, multiple chains, and R-hat diagnostics are standard. The authors\n  should verify that best practices were followed; otherwise, the claimed accuracy\n  advantage is not meaningful.\n- Missing other baselines for hierarchical inference: The comparison currently focuses on HMC, but omits several established fast Bayesian or approximate inference methods that directly apply to mixed-effects models. For example: Variational Inference (VI) offers scalable approximate posteriors and would provide a relevant amortized or single-dataset baseline. INLA (Integrated Nested Laplace Approximation), a deterministic and highly efficient method for latent Gaussian models, widely used for Bayesian mixed-effects and spatial models; often matches MCMC accuracy at a fraction of the cost. Laplace / GLMM approximations — the classical second-order Gaussian approximation around the MAP, as implemented in standard GLMM software. Including these would contextualize metabeta's speed and accuracy gains relative to well-known fast alternatives rather than only to a potentially under-tuned HMC baseline.\n- The posterior quality is inconsistent: in the toy example (Fig. 1C), metabeta produces irregular shapes for otherwise Gaussian-like posteriors. This raises questions about the flow architecture and whether the networks are overflexible or underregularized.\n- Importance sampling is introduced without sufficient justification. If NPE is trained\n  on the correct generative model, IS should not be required. The authors should explain\n  whether IS corrects training–prior mismatch or residual amortization bias. Similar\n  post-hoc IS refinements have been proposed in SBI (e.g., Dax et al., 2023) and should\n  be cited.\n- Evaluation metrics focus on parameter RMSE and correlation, which are not appropriate\n  for Bayesian inference. The true parameter need not coincide with posterior means.\n  Calibration metrics such as simulation-based calibration (SBC) or log-probability of\n  the true parameters would be more informative.\n- Runtime comparison is anecdotal (“orders of magnitude faster”) and lacks hardware\n  disclosure or amortized cost estimates. Practitioners need wall-clock times on\n  standardized setups.\n\n### Presentation and reproducibility\n\n- Figure 1 is difficult to interpret; caption variables do not align with figure\n  notation. Posterior density plots based on kernel estimates obscure the fact that the\n  NPE model defines a parametric PDF.\n- Quantitative results: Table 1 lacks error bars or multiple-seed repetitions, making it\n  unclear how stable the reported metrics are.\n- Coverage statements (“good coverage”) are qualitative; comparative or numerical values\n  are needed.\n- The software contribution is underspecified. Code is “hidden” for review, but an\n  anonymous repository is easily possible. Since the method’s accessibility depends on\n  this, it should be part of the submission.\n- Tone in the introduction (“prohibitively long inference times”) overstates the\n  limitations of MCMC."}, "questions": {"value": "1) Clarification of training data generation: How are the simulated training datasets\n   (X, Z) constructed? Are they drawn from distributions intended to mimic real-world\n   predictors, or are they generic Gaussian designs? How does the method generalize if\n   real data differ strongly from the training simulations?\n2) Role of real data: If the model is trained entirely on synthetic data, what exactly\n   is the pipeline for applying it to real datasets? Are priors and covariate\n   distributions assumed to match?\n3) Calibration discrepancy: The paper attributes poor calibration to the forward-KL\n   objective’s mass-covering tendency, whereas SBI literature (e.g., Hermans et al.\n   2022), often finds flows too narrow. Can the authors reconcile this difference? Might\n   the issue arise from too little training data or too simplistic flows? \n4) Alternative posterior networks: Could score-based or flow-matching estimators\n   mitigate the need for IS and conformal calibration? These avoid the forward-KL bias\n   entirely.\n5) Importance sampling diagnostics: What are the effective sample sizes and weight\n   variances of the IS correction? Without them, it is hard to assess stability.\n6) Scope beyond Gaussian LMEs: Does metabeta handle generalized mixed-effects models\n   (e.g., logistic, Poisson)? If not, how would the method need to change to support\n   non-Gaussian likelihoods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W9ucY9aUNS", "forum": "nhFebxzbTl", "replyto": "nhFebxzbTl", "signatures": ["ICLR.cc/2026/Conference/Submission21661/Reviewer_xGrJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21661/Reviewer_xGrJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843058930, "cdate": 1761843058930, "tmdate": 1762941876053, "mdate": 1762941876053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}