{"id": "oml3PWSYcc", "number": 2752, "cdate": 1757235317644, "mdate": 1759898129339, "content": {"title": "BuildArena: A Physics‑Aligned Interactive Benchmark of LLMs for Engineering Construction", "abstract": "Engineering construction automation aims to transform natural language specifications into physically viable structures, requiring complex integrated reasoning under strict physical constraints. While modern LLMs possess broad knowledge and strong reasoning capabilities that make them promising candidates for this domain, their construction competencies remain largely unevaluated. To address this gap, we introduce \\proj, the first physics-aligned interactive benchmark designed for language-driven engineering construction. It contributes to the community in four aspects: (1) a highly customizable benchmarking framework for in-depth comparison and analysis of LLMs; (2) an extendable task design strategy spanning static and dynamic mechanics across multiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for supporting construction based on language instructions; (4) a baseline LLM agentic workflow that effectively evaluates diverse model capabilities. On eight frontier LLMs, \\proj comprehensively evaluates their capabilities for language-driven and physics-grounded construction automation. We release the code at https://anonymous.4open.science/r/BuildArena-9B7B/ to benefit construction automation in engineering applications.", "tldr": "We provide BuildArena, a physics‑aligned interactive benchmark that tests the engineering construction capabilities of frontier LLMs.", "keywords": ["Engineering construction", "LLM", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ae5c509d087fa08fb64e21dba1a9eef3a13c251.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces BuildArena, a physics-aligned benchmark designed to evaluate LLMs on engineering construction tasks. The framework provides three task categories (Support, Transport, Lift) of varying difficulty, an open-source library to interact with the Besiege physics simulator via language, and a baseline multi-agent workflow. Experiments on eight frontier LLMs demonstrate that while models show basic competence, they systematically fail at tasks requiring precision, hierarchical assembly, and robust spatial reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1: The work addresses a gap by proposing the first benchmark for evaluating LLMs on physically-grounded, multi-step assembly. The problem is well-motivated.\n\n2: The tasks are systematically designed around clear engineering difficulty dimensions (e.g., precision, compositionality). The evaluation is thorough, with extensive trials and an insightful breakdown of failure modes, which provides clear takeaways.\n\n3: The 3D Spatial Geometric Computation Library is a nice engineering contribution that enables this line of research and allows the community to build upon it."}, "weaknesses": {"value": "1: The paper is best understood as a benchmarking paper. Its main contribution is the evaluation framework itself, not a new AI methodology. The multi-agent workflow is an application of existing patterns (e.g., AutoGen-style collaboration) and feels more like a necessary piece of engineering to enable the experiments rather than a novel contribution.\n\n2: The analysis shows that models fail and what they fail on (e.g., spatial conflict), but it lacks a deep dive into why. For instance, it doesn't offer hypotheses or analysis on what specific training data, architectural choices, or fine-tuning techniques might explain the performance gap between models. The paper stops short of suggesting how to improve LLMs for this domain.\n\n3: The benchmark only tests one-shot construction. A key engineering skill is iterative refinement based on testing and failure analysis. By not including a \"closed-loop\" where the model receives simulation feedback to fix its design."}, "questions": {"value": "The analysis shows Grok-4 is exceptionally good at precision tasks. Do authors have any hypotheses about its architecture or training that might explain this significant advantage over other models?\n\nThe primary failure mode is \"spatial conflict.\" Do authors see this as a fundamental limitation of current LLMs in tracking world state, or a problem that could be addressed with better prompting or specialized tool use?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "buI2m5C0T9", "forum": "oml3PWSYcc", "replyto": "oml3PWSYcc", "signatures": ["ICLR.cc/2026/Conference/Submission2752/Reviewer_TqAt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2752/Reviewer_TqAt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2752/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761157874466, "cdate": 1761157874466, "tmdate": 1762916360342, "mdate": 1762916360342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BuildArena, a physics-aligned, interactive benchmark to test whether LLMs can transform natural-language instructions into physically feasible 3D constructions. It introduces (i) a customizable framework with three task families—Support, Transport, Lift—each with three difficulty levels, (ii) an open 3D Spatial Geometric Computation Library mirroring Besiege’s build logic, and (iii) a five-role agentic workflow for plan-draft-review-build guidance. Experiments on eight frontier LLMs show non-trivial but limited abilities (e.g., low success on high-precision Lift), with Grok-4 strongest overall."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The work fills a clear gap by jointly evaluating language → spatial reasoning → physics feasibility rather than isolated planning or static reasoning. The benchmark is thoughtfully designed (multi-task, multi-level, logged simulations) and reasonably clear, with concrete metrics per task and illustrative figures for the pipeline and workflow; the released code further supports adoption. Empirically, the analysis (tables/figures on success, failure modes, and cost) surfaces stable weaknesses in compositionality/precision and suggests that more tokens do not automatically yield better constructions."}, "weaknesses": {"value": "- Scope realism. Besiege-style modular assembly is still “toy engineering”; claims about engineering automation should be scoped more conservatively.\n- Motivation→evidence gap. The paper claims to “comprehensively evaluate diverse capabilities,” but provides no ablations over prompts, outer loops, or alternative workflows to show the pipeline itself is necessary and beneficial.\n- Baselines/ablations. No comparisons to simpler agents (e.g., single-LLM ReAct/Plan-Execute) or workflow ablations (remove Reviewer/Guidance, fewer turns).\n- External validity. It is unclear how results transfer to CAD/robotic assembly engines (e.g., MuJoCo/Isaac Gym) or to non-modular component sets beyond Besiege."}, "questions": {"value": "- **Primary contribution**. Is the central contribution the benchmark, the open geometric library, the five-role LLM workflow, or the end-to-end pipeline? Please state the primary claim explicitly and rank the others as supporting elements.\n- **Pipeline necessity**. Does the five-role workflow significantly outperform simpler controllers (single-agent ReAct / Plan-Execute)? Provide ablations removing Reviewer or Guidance and limiting loop depth, with ∆success/indicator/invalid-action rates.\n- **Prompt/decoding sensitivity**. How sensitive are outcomes to prompt variants, few-shot vs. zero-shot, and decoding (temperature, top-p)? Please report variance to establish pipeline robustness rather than prompt idiosyncrasy.\n- **Transfer**: Any evidence BuildArena policies/skills transfer to CAD or to robot-assembly environments (MuJoCo/Isaac Gym)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gdl9mLx5gT", "forum": "oml3PWSYcc", "replyto": "oml3PWSYcc", "signatures": ["ICLR.cc/2026/Conference/Submission2752/Reviewer_fcZf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2752/Reviewer_fcZf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2752/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820096103, "cdate": 1761820096103, "tmdate": 1762916360077, "mdate": 1762916360077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce BuildArena as a first physics-aligned interactive benchmark for language-driven engineering construction. They construct three task categories - Support, Transport, and Lift. Furthermore, an open source version of spatial geometric computation library is presented. The benchmark is evaluated on eight closed source models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the proposed framework is comprehensive\n- addresses an underexplored area (construction)\n- development of an open source spatial geometric computation library is good for the research community"}, "weaknesses": {"value": "- the analysis remains mostly in an aggregate form and qualitative breakdown is missing. In fig 6, it would be good to have insights into why spatial conflicts occur.\n- as mentioned in the limitations section, the framework does not have a feedback loop between the simulator results and construction results.\n- the evaluation seems to conflate model capabilities with the specific 5-agent workflow and I do not know if the poor performance reflects model limitations or suboptimal orchestration."}, "questions": {"value": "1. How do you ensure physics consistency between your spatial geometric library and Beseige’s internal engine?\n2. Have you tried using a higher fidelity engine (maybe MuJoCo)?\n3. Have you tested simpler workflows? How sensitive are the results to the 5-agent setup you currently adopt?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a1Ryxjy1xz", "forum": "oml3PWSYcc", "replyto": "oml3PWSYcc", "signatures": ["ICLR.cc/2026/Conference/Submission2752/Reviewer_mYoq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2752/Reviewer_mYoq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2752/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988486445, "cdate": 1761988486445, "tmdate": 1762916359851, "mdate": 1762916359851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BuildArena, a physics-aligned interactive benchmark for evaluating LLMs in language-driven engineering construction. The benchmark enable models to construct and test 3D structures under physical constraints using natural language. \nThe paper evaluates eight major LLMs across physics-based construction tasks, reporting success rates, performance indicators, and token-cost analyses. Results show that while models exhibit rudimentary 3D construction skills and creative strategies, they still fail at compositional precision, hierarchical assembly, and spatial conflict resolution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- BuildArena pioneers the integration of language, physics simulation, and 3D assembly for LLM benchmarking.\n\n- The paper is well-structured, systematically progressing.\n\n- BuildArena establishes a foundational benchmark for evaluating LLMs’ physics-grounded reasoning and interactive 3D construction."}, "weaknesses": {"value": "- The benchmark currently performs single-round evaluation. There is no closed-loop feedback integrating simulation outcomes into iterative model improvement or self-correction.\n\n- The 3D Spatial Geometric Computation Library, though impressive, mirrors only a subset of Besiege's physics primitives. As the authors admit, limited module diversity constrains object complexity and realism.\n\n- The evaluation metrics capture outcome quality but miss process-level score, i.e., whether models optimize design efficiency, robustness trade-offs, or use feedback adaptively.\n\n- Since tasks involve 3D spatial reasoning, a natural baseline would include VLMs."}, "questions": {"value": "- Whether the paper use specific prompt across different LLMs?\n\n- How model perform when using multi-round CoT?\n\n- How do BuildArena tasks compare to human performance under textual-only conditions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aYSsMMXRju", "forum": "oml3PWSYcc", "replyto": "oml3PWSYcc", "signatures": ["ICLR.cc/2026/Conference/Submission2752/Reviewer_6rvr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2752/Reviewer_6rvr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2752/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762285016314, "cdate": 1762285016314, "tmdate": 1762916359678, "mdate": 1762916359678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}