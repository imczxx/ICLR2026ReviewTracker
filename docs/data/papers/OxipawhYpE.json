{"id": "OxipawhYpE", "number": 12499, "cdate": 1758208220879, "mdate": 1759897505622, "content": {"title": "Segmental Advantage Estimation: Enhancing PPO for Long-Context LLM Training", "abstract": "Training Large Language Models (LLMs) for reasoning tasks is increasingly driven by Reinforcement Learning with Verifiable Rewards (RLVR), where Proximal Policy Optimization (PPO) provides a principled framework for stable policy updates. However, the practical application of PPO is hindered by unreliable advantage estimation in the sparse-reward RLVR regime. This issue arises because the sparse rewards in RLVR lead to inaccurate intermediate value predictions, which in turn introduce significant bias when aggregated at every token by Generalized Advantage Estimation (GAE). To address this, we introduce Segmental Advantage Estimation (SAE), which mitigates the bias that GAE can incur in RLVR. Our key insight is that aggregating $n$-step advantages at every token(as in GAE) is unnecessary and often introduces excessive bias, since individual tokens carry minimal information. Instead, SAE first partitions the generated sequence into coherent sub-segments using low-probability tokens as heuristic boundaries. It then selectively computes variance-reduced advantage estimates only from these information-rich segment transitions, effectively filtering out noise from intermediate tokens.  Our experiments demonstrate that SAE achieves superior performance, with marked improvements in final scores, training stability, and sample efficiency.  These gains are shown to be consistent across multiple model sizes, and a correlation analysis confirms that our proposed advantage estimator achieves a higher correlation with an approximate ground-truth advantage, justifying its superior performance.", "tldr": "We propose Segmental Advantage Estimation (SAE) as a superior alternative to GAE, which improves PPO training for LLMs by calculating advantages at the boundaries of semantic segments rather than at the noisy token-level.", "keywords": ["Large Language Models", "Reinforcement Learning with Verifiable Rewards", "Proximal Policy Optimization", "Advantage Estimation", "Long-Horizon Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9980b6e7904e929211601be0b205729f4b31320e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper focuses on the problem of instability of GAE estimation of token-level PPO method, when used in a typical RLVR setting with terminal reward. Most of the contemporary solutions employ $\\lambda=1$ further amplifying the instability in training an accurate value estimate.\n\nTo mitigate this issue, authors propose a clever solution of segmenting the responses into chunks of high probablity tokens and reducing the overall effective \"actions\" in a response trajectory from MDP perspective. They call this method Segmental Advantage Estimation (SAE). SAE effectively reduced the number of steps in a response trajectory from number of tokens to number of segmented chunks and in-effect reduces the inefficiency in estimating the advantage estimate.\n\nExperiments on multiple model scales across standard math datasets and evals, show that SAE consistently outperforms PPO ($\\lambda=1$ and adaptive $\\lambda$) and GRPO."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Well motivated problem of instablity of GAE estimation in RLVR where $\\lambda$ is set to 1\n- Insightful solution to focus on segments of response for GAE estimation instead of per token\n- Theoretical analysis to justify that SAE reduces the bias in estimation.\n- Emprirical analysis showcasing SAE has highest correlation with true Advantage compared to other baselines in a controlled setting."}, "weaknesses": {"value": "- The SAE method uses a fixed threshold of 0.2 on the probability to decide the segments. I would have preferred an abilation study for the choice of this parameter.\n- I would prefer to have SAE compared with the simple baseline of fixed length segments from the theoretical analysis of section 4.2. For example, what is the effect when I naive let chunks to be of size $M=100$ or $200$ tokens irrespective of the probablity. Does the choice of segmentation method matter towards the downstream performance of SAE?"}, "questions": {"value": "1. I would have preferred to see some analysis of average segment length for different model sizes in a practical setting. The paper will greatly benefit with more analysis and the effects of varying the probablity threshold $p$ on the segment size.\n2. Did authors try other segmentation methods such as entropy instead of raw token probability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wQRGXYSLeu", "forum": "OxipawhYpE", "replyto": "OxipawhYpE", "signatures": ["ICLR.cc/2026/Conference/Submission12499/Reviewer_yZzM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12499/Reviewer_yZzM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965672240, "cdate": 1761965672240, "tmdate": 1762923372131, "mdate": 1762923372131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Segmental Advantage Estimation (SAE) method to improve value estimation in PPO algorithms. SAE first partitions the generated sequence into coherent sub-segments, using low-probability tokens as heuristic boundaries, and then treats each segment as an action for GAE computation. Experiments are conducted to demonstrate the effectiveness of the proposed method compared to standard PPO."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and easy to follow.\n- Accurate value estimation is crucial for PPO algorithms. The idea of segmenting responses based on low-probability tokens is intuitive and makes sense.\n- Experiments are conducted to demonstrate the effectiveness of the proposed method compared to standard PPO."}, "weaknesses": {"value": "My main concern is the lack of comparison with related baseline:\n-  There is no comparison with the mentioned related works, such as VC-PPO and VAPO\n-  Previous studies have proposed computing GAE at the step level (e.g., by splitting sequences using special tokens such as ‘\\n’) [1]. This paper is closely related to those approaches, and a comparison with them would help better demonstrate the effectiveness of the proposed method.\n\n[1]Chen, Guoxin, et al. \"Alphamath almost zero: process supervision without process.\" Advances in Neural Information Processing Systems 37 (2024): 27689-27724"}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iD8HmaKXWH", "forum": "OxipawhYpE", "replyto": "OxipawhYpE", "signatures": ["ICLR.cc/2026/Conference/Submission12499/Reviewer_ikv3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12499/Reviewer_ikv3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976408909, "cdate": 1761976408909, "tmdate": 1762923371627, "mdate": 1762923371627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Segmental Advantage Estimation (SAE) to improve Proximal Policy Optimization (PPO) for training Large Language Models (LLMs) on long-horizon reasoning tasks with verifiable rewards (RLVR).  It aims to address the unreliable advantage estimation in sparse-reward settings, where traditional Generalized Advantage Estimation (GAE) amplifies bias by performing token-level bootstrapping using noisy value predictions. SAE mitigates this by first partitioning the generated sequence into semantically coherent segments, using low-probability tokens as heuristic boundaries, and then selectively computing advantages only at these segment transitions. This reduces bootstrapping bias by filtering out noise from intermediate, low-information tokens."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The proposed method in this paper is practically elegant, as its recursive formulation allows for seamless integration into existing PPO frameworks with minimal computational overhead. \n(2) The empirical evaluation is thorough, benchmarking against strong baselines like GRPO and adaptive PPO variants across multiple out-of-distribution test sets (AIME, AMC). The consistent performance gains across 4B, 8B, and 14B model sizes strongly support the method's robustness and scalability."}, "weaknesses": {"value": "(1) While the use of low-probability tokens is intuitive, it is an unsupervised method that may not always align perfectly with true semantic boundaries, potentially introducing its own form of noise.\n(2) The evaluation is confined to mathematical reasoning. While this is a canonical domain for RLVR, the paper does not demonstrate SAE's efficacy in other long-context scenarios like code generation or complex dialogue, limiting the claimed generality of the approach."}, "questions": {"value": "(1) How might more sophisticated, learned segmentation strategies (e.g., leveraging an auxiliary model or syntactic features) further improve the performance and robustness of SAE compared to the current probability-based heuristic?\n(2) The paper sets the segmentation threshold p=0.2 universally. How sensitive is the performance of SAE to this hyperparameter, and could an adaptive or dynamically learned threshold offer benefits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zUxRRelluw", "forum": "OxipawhYpE", "replyto": "OxipawhYpE", "signatures": ["ICLR.cc/2026/Conference/Submission12499/Reviewer_BbQN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12499/Reviewer_BbQN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045681532, "cdate": 1762045681532, "tmdate": 1762923371341, "mdate": 1762923371341, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}