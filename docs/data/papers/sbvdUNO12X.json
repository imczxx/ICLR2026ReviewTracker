{"id": "sbvdUNO12X", "number": 20511, "cdate": 1758306923837, "mdate": 1759896974023, "content": {"title": "HypoVeil: A Hypothesis-Driven Pragmatic Inference-Time Control Framework for Privacy–Utility-Aware LLM-Agent Dialogue", "abstract": "Large language model (LLM) agents are increasingly used as personal assistants with privileged data access, raising privacy concerns not just from training, but also from information disclosed during conversations at inference time. The key tradeoff is providing enough information to accomplish tasks while minimizing unintended disclosure; yet, prior evaluations show LLMs still struggle to consistently respect contextual privacy norms. We introduce HYPOVEIL, an inference time privacy method that combines a hypothesis-driven mental model with pragmatic decision-making. The agent maintains a dimension-aware belief store composed of concise natural language hypotheses about the counterpart’s knowledge, goals, and likely interpretations, then couples it with a Rational Speech Act (RSA) module that selects utterances by maximizing task utility minus privacy cost under the current hypothesis. To showcase the effectiveness of our method, we create and test on V-BENCH, a benchmark where two agents must interact in multi-turn privacy scenarios, structured as Party B strategically probing for information and Party A needing to collaborate without violating contextual privacy norms. Across GPT-4o, Llama-3.1-8B, and Gemma-3-27B, our method (Mental Model w/ RSA) significantly improves the privacy–utility trade-off, increasing the trade-off score by 5.2\\% on average, reducing privacy risk by 6.4\\%, and increasing helpfulness by 2.8\\% over the baseline. These findings indicate that a hypothesis-driven mental model combined with pragmatic reasoning at inference time provides a practical path to privacy-preserving and context-aware LLM agents.", "tldr": "We introduce HypoVeil, an inference-time method that couples a hypothesis-driven belief store with a pragmatic RSA planner to optimize utility and minimize privacy cost. HypoVeil yields a significant improvement over our baselines on V-Bench.", "keywords": ["Test Time Inference", "Hypothesis-Driven", "Pragmatic decision", "Rational Speech Acts"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a50308177a402772655944229cc0c907be63bbb0.pdf", "supplementary_material": "/attachment/be1cddde5d981282f34733c37fde1d2e46c2d6b0.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes an inference-time mechanism to improve privacy preservation in conversational LLM agents. \nThe framework integrates (1) a hypothesis-driven mental model, maintaining natural-language “belief hypotheses” about interlocutors (their knowledge, motives, and likely interpretations), with (2) a Rational Speech Act (RSA)-based planner that selects utterances by maximizing task utility minus context-sensitive privacy cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper correctly isolates the under-addressed issue of inference-time privacy leakage—distinct from training-time memorization—and grounds it in contextual-integrity theory.\n2. The combination of hypothesis-driven ToM reasoning with Rational Speech Act pragmatics for privacy control is original and theoretically elegant.\n3. The six-dimension mental model (Knowledge, Request, Motive → Strategic Direction, Information Gaps, Privacy) is intuitively designed and operationally defined."}, "weaknesses": {"value": "1. The method’s components—hypothesis tracking and RSA reasoning—each have prior art; the contribution is mainly integrative rather than algorithmically new.\n2. Evaluation only on synthetic CI scenarios; no real-world human-LLM dialogue or deployment case, limiting ecological validity.\n3. Both privacy and helpfulness metrics depend on automated judgments by the same or similar model family, risking circular evaluation bias.\n4. Reported improvements (≈ 5 %) are small relative to the noise levels (± 3–4 %), raising questions about effect robustness beyond statistical significance.\n5. Few concrete dialogue examples illustrating successful vs. failed privacy behavior; interpretability is claimed but not demonstrated.\n6. V-BENCH scenarios, though CI-grounded, are generated by LLMs (GPT-4o) and thus may encode stylistic artifacts or training bias."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qFpUtEBJkl", "forum": "sbvdUNO12X", "replyto": "sbvdUNO12X", "signatures": ["ICLR.cc/2026/Conference/Submission20511/Reviewer_ND7A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20511/Reviewer_ND7A"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571612529, "cdate": 1761571612529, "tmdate": 1762933937128, "mdate": 1762933937128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HypoVeil, an inference-time framework that enhances LLM agents’ privacy–utility trade-off by combining a hypothesis-driven mental model with a Rational Speech Act planner. The system explicitly models a conversation partner’s knowledge and motives to decide what to reveal or conceal. A new benchmark, V-BENCH, evaluates privacy-sensitive multi-turn dialogues. Experiments on GPT-4o, Llama-3.1-8B, and Gemma-3-27B show that HypoVeil improves privacy and helpfulness over strong baselines, demonstrating a principled approach to privacy-aware reasoning for LLM agents."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper examines LLM privacy through the lens of Contextual Integrity (CI), a newly-presented and well-suited framework for analyzing privacy dynamics in modern agent-based dialogues."}, "weaknesses": {"value": "1. My main concern is how much contributions and insights that the paper can bring here. There has been extensive work on LLM privacy. While I am not familiar with prior work on CI, does all these previous methods cannot be evaluated under this CI settings? What benefits can HypoVeil offer compared to existing approaches in similar directions?\n2. It will be much better to have a pseudo-code for HypoVeil to help readers understand all the steps listed from Section 3.\n3. From my understanding, the core idea of HypoVeil is to generate multiple candidate utterances and select the one that minimizes privacy leakage. Similar ideas have been explored in other line of work of LLM privacy. For instance, the Exponential Mechanism, a standard approach in differential privacy, selects an optimal output from a set of options based on scores computed from sensitive data. This mechanism has inspired several prior studies [1, 2, 3] on privacy preservation in LLMs.\n\n[1] Amin, Kareem, et al. \"Private prediction for large-scale synthetic text generation.\" EMNLP 2024.\n\n[2] Flemings, James, Meisam Razaviyayn, and Murali Annavaram. \"Differentially private next-token prediction of large language models.\" ACL 2024.\n\n[3] Bhusal, Bishnu, et al. \"Privacy-Aware In-Context Learning for Large Language Models.\" ICLR 2024."}, "questions": {"value": "1. Is it possible to extend this setting (Party A v.s. Party B) to a multi-party (>2) conversation? It will fit more in modern multi-agent systems.\n2. How is the Leak score calculated in Eq. 2? It is stated that it comes from the Privacy/Sensitivity dimension. How are these values calculated originally in Sec. 3.1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iSHnZEDdj7", "forum": "sbvdUNO12X", "replyto": "sbvdUNO12X", "signatures": ["ICLR.cc/2026/Conference/Submission20511/Reviewer_5JEp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20511/Reviewer_5JEp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967657148, "cdate": 1761967657148, "tmdate": 1762933936703, "mdate": 1762933936703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to use hypothesis framework to manage what an agent should share or not with the counterpart. It relies on six-dimension model and introduces a benchmark with many scenarios to test it. The paper provides a nice outline for future agentic development and deployment of AI Assistants with access to user information."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Paper addresses a key problem in privacy for AI Agents --> how can we know what information can be shared or not\n- The paper adds hypothesis and RSA view of the communication\n- Paper also introduces the benchmark for multiple scenarios to test multi-turn privacy \n- experiments demonstrate strong improvements over baseline methods"}, "weaknesses": {"value": "- It is not clear how this method will work under attacks that attempt to steal data from the agent. Similar to prompt injections [1], [2] and [3] show that agents are easily persuaded to share information. It might be great to have a discussion. \n- Furthermore, given the proposed hypothesis method the adversary can attempt to manipulate the the hypothesis by the model to force it to increase trust and share more information\n- It is possible to isolate the agent before interactions by minimizing the data [2], however it might require assuming some hypothesis before interactions. It might be great to have a discussion on these tradeoffs. \n\n\n[1] Agentdam: Privacy leakage evaluation for autonomous web agents, Arxiv'25\n[2] AirGapAgent: Protecting Privacy-Conscious Conversational Agents, CCS'24\n[3] The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration, Arxiv'25"}, "questions": {"value": "addressing threat model questions might significantly strengthen the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8RsV8AWtXh", "forum": "sbvdUNO12X", "replyto": "sbvdUNO12X", "signatures": ["ICLR.cc/2026/Conference/Submission20511/Reviewer_J1KV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20511/Reviewer_J1KV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974915776, "cdate": 1761974915776, "tmdate": 1762933936178, "mdate": 1762933936178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **HypoVeil**, an inference-time privacy framework that combines hypothesis-driven reasoning with pragmatic decision-making. By maintaining a belief store of hypotheses about the interlocutor’s knowledge and intentions, and integrating a Rational Speech Act (RSA) module to optimize utility–privacy trade-offs, the method enables agents to act contextually while preserving privacy. Its effectiveness is demonstrated on **V-Bench**, a benchmark designed for multi-turn privacy-sensitive interactions."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The evaluations are well designed, effectively accounting for the trade-off between privacy and helpfulness.\n2. The proposed method operates entirely at inference time, requiring no additional training, and can be readily applied to both small models (e.g., Llama 3.1) and large ones (e.g., GPT-4o).\n3. Research on enhancing inference-time reasoning through contextual integrity is of great importance, and I believe this paper is strongly motivated in that direction."}, "weaknesses": {"value": "1. Clarity and Motivation\n\nThis paper is generally difficult to grasp in terms of its overall idea, motivation, and methodology. For example, *hypothesis* seems to play a central role according to the title, yet Figure 1 (the overview) does not reflect this concept at all. Moreover, the Introduction does not sufficiently explain what *hypotheses* are proposed in this paper or how they guide the reasoning process of LLM-agents. I noticed a brief mention of this in Line 150 (Section 2), but it is far from adequate. Since the main innovation of the paper appears to lie in its hypothesis-driven design, the authors should provide a clearer and more detailed explanation—possibly supported by concrete data examples or illustrative figures.\n\nAdditionally, several key terms are not well defined. For instance, the notions of *evidence* in Section 3 are hard to interpret. The word *evidence* first appears in Line 168, but its meaning and role are unclear—why would multi-turn dialogue require *evidence*? I assume these terms (along with others such as *utterance*) may originate from social science theories, but since most of the target audience is likely unfamiliar with that background, it would be helpful to include explicit definitions or explanations.\n\n2. Experiments\n\nThe experiments are not sufficiently extensive. From what I can tell, Figures 2 and Tables 1–3 all appear to be derived from the same experiment runs. And the conclusions drawn from them are not particularly informative.\n\n3. Methodology\n\nI cannot clearly identify the novelty of the proposed method, as it is not sufficiently explained how the introduced hypotheses contribute to improving reasoning performance."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QLdrpPXMYo", "forum": "sbvdUNO12X", "replyto": "sbvdUNO12X", "signatures": ["ICLR.cc/2026/Conference/Submission20511/Reviewer_wqAs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20511/Reviewer_wqAs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979089986, "cdate": 1761979089986, "tmdate": 1762933935700, "mdate": 1762933935700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers the setup of two LLM agents interacting with each other, and focuses on the problem that an LLM agent may 'overshare' information that is not contextually relevant, thereby resulting in privacy violation. Similar to some recent works, the paper studies this problem of 'unintended' disclosures in LLM agents from the perspective of contextual integrity [Nissenbaum 2004]. The paper proposes an inference-time privacy method called HypoVeil. Each party maintains a 'mental model' in terms of one-sentence hypothesis, list of embeddings of conversation history, and calibrated confidence scores. This is used for planning the response in upcoming turns.  The proposed method also leverages so-called Rational Speech Act (RSA). The paper also proposes a benchmark V-Bench for evaluating utility versus privacy in LLM agent interactions."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The problem that LLM agents may disclose unintended information during interactions is practically important, and has recently started to get attention. This makes the paper timely. \n* The paper leverages ideas from contextual integrity, which provides an interesting platform for designing solutions to handle the privacy problem."}, "weaknesses": {"value": "* There are two big concerns. First, the paper does not put the contributions into the context with respect to recent works on privacy of interactions between LLM agents. \n    - Comparisons with prior contextual privacy benchmarks, especially PrivacyLens [Shao et al., 2024] and Magpie [Juneja et al., 2025] are cursory and vague. Lack of comparison in terms of technical aspects makes it difficult to asses the novelty of contributions of the V-Bench benchmark. The paper also does not include experimental results of prior benchmarks such as PrivacyLens. It would have been helpful to show what leakage aspects (or scenarios with specific technical examples) are covered by V-Bench that PrivacyLens or Magpie fail to capture. See Questions section for details\n    - The paper does not consider some of the relevant works such as AirgapAgent [Bagdasarian et al., CCS 2024] and Firewalls [Abdelnabi et al., 2025].\n* Second, several concepts in the proposed method of HypoVeil lack technical details, thereby limiting the technical rigor of the paper. For instance, many sub-routines in Algorithm 1 and 2 are not defined in detail. This makes understanding the technical details tricky and would also make reproducibility challenging. \n\nReferences:\n1. Bagdasarian et al., \"AirGapAgent: Protecting Privacy-Conscious Conversational Agents\", SIGSAC Conference on Computer and Communications Security (CCS), 2024\n2. Abdelnabi et al., \"Firewalls to Secure Dynamic LLM Agentic Networks\", 2025"}, "questions": {"value": "- Can the authors expand on technical differences between V-Bench and prior benchmarks PrivacyLens [Shao et al., 2024] and Magpie [Juneja et al., 2025]? The related works section mentions that \"However, they leave open key needs for inference-time study: a testbed that stresses multi-turn, calibrated overlap to create privacy protection pressure, strategic probing than a single response. Our V-Bench addresses these gaps by ...\". However, this description is vague and does not capture technical differences. The paper claims to make CI tuples explicit, but PrivacyLens also has CI tuples, and the paper indeed samples CI seeds from PrivacyLens. It is important to make the differences more explicit with technical/conceptual rigor. \n- Several questions remain open: Is the only difference of V-Bench due to multi-turn consideration? Are there explicit scenarios where prior benchmarks fail to capture privacy leakage? Are there experiments showing numerical comparisons with prior benchmarks?\n- How does HypoVeil compare against prior contextual privacy baselines such as AirgapAgent and Firewalls? \n- What is the difference between simple and chain-of-thought baselines? Does simple mean use LLM agent as-is and does CoT simply use a different prompt? It will be helpful to add more details. \n- For GPT-4o and  Llama3-8b, \"mental w/ rsa\" achieves higher helpfulness than simple and CoT baselines. (In Gemma3-27b, helpfulness scores are very close.) This seems counterintuitive. Simple baselines without privacy requirements often achieve higher helpfulness at the cost of privacy. Can the authors provide details on how \"mental w/ rsa\" improves helpfulness while also reducing privacy leakage? Explicit examples showing qualitative improvement will be very helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4cx8OYnIAz", "forum": "sbvdUNO12X", "replyto": "sbvdUNO12X", "signatures": ["ICLR.cc/2026/Conference/Submission20511/Reviewer_XNdJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20511/Reviewer_XNdJ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762832406452, "cdate": 1762832406452, "tmdate": 1762933935308, "mdate": 1762933935308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}