{"id": "bQTlwMXXwW", "number": 12474, "cdate": 1758208064701, "mdate": 1759897507655, "content": {"title": "DVC-SGRL: Adapting MLLMs for Temporally Precise Dense Video Captioning via Semantically Guided Reinforcement Learning", "abstract": "Dense Video Captioning (DVC) aims to localize and describe multiple events within untrimmed videos. While methods using Multimodal Large Language Models (MLLMs) show promise, their ability to precisely localize event boundaries remains a significant limitation. This weakness stems from a reliance on supervised fine-tuning with cross-entropy loss, which frames timestamp prediction as a classification task. In this formulation, the model learns only to match timestamps exactly, with no awareness of how close a prediction is to the ground truth. This limits its ability to interpret time as a continuous signal, hindering accurate event localization. To address this, we introduce DVC-SGRL, a reinforcement learning framework that provides semantically guided temporal supervision, enabling general-purpose MLLMs to be successfully adapted for dense video captioning. Our approach leverages the model's powerful captioning abilities to improve its weaker temporal localization through a novel matching mechanism and corresponding rewards mechanism. Our semantically-guided reward function uses strong matches in caption content to create robust learning signals for refining event boundaries. This ``soft alignment\" approach, which decouples the evaluation of content and timing, offers far more informative supervision than standard classification losses. Experimental results demonstrate that DVC-SGRL achieves significant improvements in both localization and captioning performance, ultimately reaching state-of-the-art results on YouCook2 and ActivityNet Captions.", "tldr": "", "keywords": ["Dense video caption", "Multimodal large language model", "Reinforcement learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d2029467070841a5e8a2229de3d1e2e388e20860.pdf", "supplementary_material": "/attachment/37eafdf8e7ca9035b18b58f9b70dd0fd315cf688.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents DVC-SGRL, a reinforcement learning framework for dense video captioning that adapts multimodal large language models to achieve temporally precise event localization and fluent caption generation. It introduces semantically guided rewards that align predicted and reference events based on caption similarity, allowing strong linguistic understanding to guide boundary refinement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Strong experimental results:** DVC-SGRL achieves superior performance on both YouCook2 and ActivityNet, surpassing prior state-of-the-art methods in both localization and captioning quality.\n2. **Sound method design:** The approach effectively addresses the temporal insensitivity of cross-entropy loss through GRPO-based reinforcement learning, providing a well-motivated and efficient solution that avoids architectural modifications or additional temporal tokens."}, "weaknesses": {"value": "1. **Limited novelty:** While temporal sensitivity is indeed an important challenge, prior works [1, 2, 3] have already explored solutions to this issue. The main advancement of DVC-SGRL lies in applying GRPO to dense video captioning, which, although effective, represents an incremental rather than fundamentally novel contribution.\n2. **General applicability**: The method is highly tailored toward dense video captioning. Therefore, general capability of the model beyond dense video captinoing might be limited."}, "questions": {"value": "1. Are there specific reason or ablations on choice of $\\alpha, \\beta, \\gamma, \\delta$ for reward calculation? \n2. Does DVC-SGRL also capable of using speech input as Vid2Seq? Or does it already uses speech input?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZRPdBTTNCZ", "forum": "bQTlwMXXwW", "replyto": "bQTlwMXXwW", "signatures": ["ICLR.cc/2026/Conference/Submission12474/Reviewer_Ldg9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12474/Reviewer_Ldg9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894833837, "cdate": 1761894833837, "tmdate": 1762923351561, "mdate": 1762923351561, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a fundamental weakness in existing multimodal Large Language Model (mLLM) approaches for Dense Video Captioning (DVC): imprecise temporal localization since mLLM is trained with token classification. To address this, the paper proposes DVC-SGRL, a two-stage training framework. Stage 1 performs standard SFT to align a general-purpose MLLM with the task format, notably representing time as natural language strings (e.g., \"01:35 - 01:42\") to avoid architectural changes. Stage 2, the core innovation, uses reinforcement learning (specifically, GRPO) with a semantically-guided reward function. This function first matches predicted events to ground-truth events based on caption similarity, not temporal overlap. This \"soft alignment\" allows the MLLM's strong semantic captioning ability to provide a learning signal for its weaker temporal localization, using a reward that combines caption quality, semantically-matched localization, and a separate localization-only score."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to follow.\n- To the best of my knowledge, this is the first paper, which adapts GRPO to a dense video captioning task.\n- From the author’s experiments, the proposed DVC-SGRL achieves the best performance compared to other baselines on two benchmarks (ActivityNet and YouCook2). The performance gain seems to be meaningful. In particular, from Table 3, the reinforcement learning-based training strategy shows the performance improvement over other training strategies."}, "weaknesses": {"value": "- More deeper analysis of the semantic matching strategy is required. Compared to existing RL-based mLLMs designed for temporal grounding, the core and original method of this paper is a semantically-guided reward formulation. But, there is a lack of deeper analysis of the semantic matching strategy.\n    - First, I wonder why the author applies the Hungarian algorithm to find the optimal assignment. Since the Hungarian algorithm performs one-to-one matching, it may be problematic when the number of predicted captions is different from that of reference captions. It would be better if the paper discussed how to resolve this case.\n    - Second, for the Hungarian matching, the paper only uses pairwise caption similarities as a measurement. But, in the DVC task, not only semantic matching but also temporal matching should be considered.\n    - Third, I wonder how robust the Hungarian matching algorithm is. It would be better if the author included the analysis concerning the performance of the Hungarian matching algorithm.\n    - Forth, I think that employing SODA [1] as a verifiable reward function can play a role in matching rationale. Could you include the experimental results of the model trained with SODA as a verifiable reward function?\n- There are some missing baselines such as VTimeLLM, VideoLLaMA2, and VidChain.\n\n[1] Fujita, Soichiro, et al. \"Soda: Story oriented dense video captioning evaluation framework.\" ECCV, 2020."}, "questions": {"value": "The paper seems to apply the same reward value to all the captions in the sequence generated by multimodal LLMs. But, I think that the better way is to apply different reward values to each caption since the caption’s quality is different."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "W43T2xLJLd", "forum": "bQTlwMXXwW", "replyto": "bQTlwMXXwW", "signatures": ["ICLR.cc/2026/Conference/Submission12474/Reviewer_LjyV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12474/Reviewer_LjyV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957335476, "cdate": 1761957335476, "tmdate": 1762923351067, "mdate": 1762923351067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper highlights a limitation of standard cross-entropy training in dense video captioning: timestamps are treated as discrete labels, so the model cannot distinguish between predictions that are slightly off versus significantly incorrect.\nTo address this, the authors introduce DVC-SGRL, a two-stage training pipeline.\nIn the first stage, supervised fine-tuning teaches the model the structural format and linguistic patterns required for DVC.\nIn the second stage, GRPO-based reinforcement learning incorporates the proposed semantically guided matching strategy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors propose DVC-SGRL, a two-stage training pipeline consisting of supervised fine-tuning followed by reinforcement learning. They show that this ordering (SFT → RL) yields the best performance compared to other training sequences.\n- They design a composite reward with four components: caption reward, caption-matched localization reward, traditional IoU-based localization reward, and a format reward.\n- They adopt human-readable timestamps to avoid relying on special time tokens during supervised training.\n- The method achieves strong results on YouCook2 and ActivityNet, outperforming baseline models.\n- The paper includes comprehensive ablation studies that demonstrate the effectiveness of the approach across multiple design choices."}, "weaknesses": {"value": "- How sensitive is the method to the weighting coefficients in the reward function? \n- How effective is the use of human-readable timestamps? Although the authors list this as a contribution, there is no analysis demonstrating its impact on performance.\n- How does the number of training epochs for the SFT and RL stages affect the final performance?"}, "questions": {"value": "- In Table 3, the Recall value is incorrectly bolded."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vKFB71sxNW", "forum": "bQTlwMXXwW", "replyto": "bQTlwMXXwW", "signatures": ["ICLR.cc/2026/Conference/Submission12474/Reviewer_37Wb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12474/Reviewer_37Wb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964090930, "cdate": 1761964090930, "tmdate": 1762923350522, "mdate": 1762923350522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Dense Video Captioning (DVC) requires models to both localize events in untrimmed videos and describe them. While VLMs are promising for this task, they struggle with precise event localization. This stems from their reliance on supervised fine-tuning (SFT) which treats timestamp prediction as a classification task. In this formulation, all incorrect timestamps are treated equally regardless of their proximity to the ground truth, preventing the model from interpreting time as a continuous signal.\n\nThe paper introduce DVC-SGRL, a two-stage training framework designed to adapt general-purpose VLMs for precise DVC. The model is taught to express event boundaries using natural language strings, rather than specialized time tokens, maintaining compatibility with pretrained models. Then uses an IoU based reward function to further train it. The model is evaluated on YouCook2 and Acitivtynet."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed approach is different from prior works and the experiments show it is beneficial. However, prior works treating timestamps as text are common, such as Vid2Seq. \n\nThe approach maintains full compatibility with pre-trained MLLMs. It does not require architectural modifications, which is nice."}, "weaknesses": {"value": "The novelty is a bit limited, as it is mostly just a new reward function for dense captioning tasks. It is further questionable how meaningful the ground truth timestamps are, as historically there has always been disagreement and ambiguity in the timestamps of actions from human annotators. \n\nThe reward function is also a bit concerning. The implementation details reveal that all weighting coefficients (α,β,γ,δ) are set to 1. While simple, it is highly improbable that a uniform weighting is optimal across different datasets with varying densities of events (e.g., cooking steps in YouCook2 vs. sparse activities in ActivityNet). It is also unclear how important each of the components of the reward function are. The ablation in table 2 doesn't have huge differences between the settings, so it isn't clear if they are statistically significant.\n\nThe authors admit that their autoregressive design for predicting event boundaries \"limits temporal precision on longer videos\". Representing time purely as text tokens (\"MM:SS\") in a single sequence can lead to drifting errors or context window issues in very long untrimmed videos with high event density.\n\nThe paper claims to train a \"single, unified model\" that avoids dataset-specific tuning. However, this \"generalist\" model is only trained on a combination of two datasets: YouCook2 and a subset of ActivityNet . These are both relatively standard, activity-centric datasets, and the only datasets the paper is evaluated on. A true \"generalist\" DVC model should be robust to vastly different video domains"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3LaFpPLh24", "forum": "bQTlwMXXwW", "replyto": "bQTlwMXXwW", "signatures": ["ICLR.cc/2026/Conference/Submission12474/Reviewer_jAFC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12474/Reviewer_jAFC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762219982970, "cdate": 1762219982970, "tmdate": 1762923350112, "mdate": 1762923350112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}