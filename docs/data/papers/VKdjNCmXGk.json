{"id": "VKdjNCmXGk", "number": 20295, "cdate": 1758304505670, "mdate": 1759896985642, "content": {"title": "Closed-form Last Layer Optimization", "abstract": "Neural networks are typically optimized with variants of stochastic gradient descent. Under a squared loss, however, the optimal solution to the linear last layer weights is known in closed-form. We propose to leverage this during optimization, treating the last layer as a function of the backbone parameters, and optimizing solely for these parameters. We show this is equivalent to alternating between gradient descent steps on the backbone and closed-form updates on the last layer. We adapt the method for the  setting of stochastic gradient descent, by trading off the loss on the current batch against the accumulated information from previous batches. Further, we prove that, in the neural tangent kernel regime, convergence of this method to an optimal solution is guaranteed. Finally, we demonstrate the effectiveness of our approach compared with standard SGD on a squared loss in several supervised tasks -- both regression and classification -- including Fourier Neural Operators and Instrumental Variable Regression.", "tldr": "For l2 loss, we leverage closed form solution on the last layer with an additional proximal term, to update last layer and the neural network parameters.", "keywords": ["Optimization", "proximal optimization", "closed-form optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e360a1a9cc77f30c146239638a1f3432f86238b2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes training with the closed-form last-layer solution (under squared loss): at each step, recompute the final linear weights exactly from current features and update only the backbone by gradient descent. By the envelope theorem, this avoids back-propagating through the matrix inverse and is equivalent to alternating a backbone GD step with an exact least-squares solve for the last layer. For minibatch SGD, the authors add a proximal regularizer that pulls the new last-layer solution toward the previous one, preventing per-batch overfitting; the resulting loop alternates (i) a backbone GD step and (ii) a closed-form ridge solve with proximal coupling, admitting an approximate Kalman filter / RLS interpretation. A backbone-first update order works best empirically.\n\nTheoretically, optimizing the induced implicit loss $L^{*}(\\theta)$ is non-convex with possible spurious critical points, but in the infinite-width NTK regime (with a positive-definite kernel and sufficient feature rank) gradient descent converges to a global optimum. Empirically, on regression (FNO/PDEs and DFIV) the method converges faster and attains lower MSE than $\\ell\\_{2}$-SGD, and in DFIV removes the need for a costly final refit. On CIFAR-10/100, closed-form $\\ell\\_2$ training consistently beats $\\ell\\_2$-SGD and can match or exceed cross-entropy on CIFAR-100, though on ImageNet cross-entropy remains stronger. Overall: a simple, practical algorithm with supporting theory and broad empirical gains, especially for squared-loss training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel Optimization Approach: The idea of enforcing the last layer to be optimal at each step is a fresh departure from standard end-to-end SGD. It exploits a known closed-form solution in an innovative way to simplify the optimization problem, essentially performing exact minimization over last-layer weights in each iteration. This two-timescale strategy has appeared in theory, but this work is the first to turn it into a practical training method integrated with SGD.\n\n - Theoretical Insight: The paper offers non-trivial theoretical contributions. It analyzes the implicit loss landscape when the last layer is always optimal, proving that while this landscape is generally non-convex with potentially bad critical points, gradient descent in the infinite-width NTK limit will avoid those and find a global minimizer. This convergence theorem (Theorem 4) under positive-definite kernel assumptions is a reassuring theoretical justification for the method’s efficacy. Moreover, the use of the envelope theorem to show that backpropagation through the closed-form solution is unnecessary (Theorem 1 and 2) is a nice theoretical simplification that saves computation.\n\n - Practical Stochastic Algorithm: The paper identifies and tackles the key challenge of mini-batch training (last-layer overfitting to each batch) by introducing a proximal regularization to the closed-form update. This is a simple and effective fix that keeps the last layer update stable and coupled to the backbone’s progress, unlike a naive moving average approach which could decouple and diverge. The resulting algorithm (Algorithm 1) is easy to implement and can plug into existing training pipelines, as it essentially alternates a usual backbone SGD step with solving a small linear system for the last layer.\n\n - Empirical Performance: Across regression tasks, closed-form last-layer training yields lower MSE and faster convergence than vanilla SGD, with especially strong gains in small-batch settings (the proximal update stabilizes stochasticity). On DFIV, it outperforms small-batch baselines and nearly matches costly two-stage refitting—eliminating that step. In classification, squared-loss with closed-form updates consistently beats $\\ell_2$-SGD on CIFAR-10/100, and on CIFAR-100 it even modestly outperforms cross-entropy. Overall, results show both optimization speedups and better generalization in many cases."}, "weaknesses": {"value": "- Restriction to Squared Loss: A notable limitation is that the method inherently relies on the squared loss to obtain a closed-form solution for the last layer. This means it cannot be directly applied to tasks where cross-entropy or other non-quadratic loss functions are standard. In classification, using a squared error surrogate is somewhat non-standard and requires a heuristic at prediction time (taking an argmax of outputs since they don’t form a probability distribution). While the authors show this can work reasonably, it forgoes the probabilistic interpretation and other benefits of the softmax cross-entropy framework. The approach’s success on CIFAR-100 vs. cross-entropy is intriguing, but on larger-scale tasks like ImageNet it underperforms cross-entropy training, indicating that the benefits of the closed-form update may not overcome the loss function mismatch when the number of classes is very large or the problem is more complex. This limits the method’s applicability in scenarios where using the true cross-entropy loss (or others like hinge, etc.) is essential.\n\n - Theoretical Gaps for Finite Networks: The convergence guarantee provided (Theorem 4) is in the infinite-width NTK regime, which is a strong assumption that may not hold in practice. For finite networks, the loss $L^(\\theta)$ can have non-global critical points, such as trivial feature representations that zero out gradients. The paper does not prove that gradient-based training will avoid these bad critical points in general finite settings. While standard deep networks typically don’t get stuck in completely uninformative representations, it’s theoretically possible that this new training objective could introduce different failure modes. The authors do not report observing such issues in practice (and indeed their method found good solutions in experiments), but a formal understanding for finite width is lacking. This leaves a slight gap in the theoretical guarantees while in contrast, conventional end-to-end training has well-understood critical point structures in overparametrized settings (e.g. no bad local minima under certain assumptions), it’s less clear for the $L^(\\theta)$ objective.\n\n - Computational Overhead: Another concern is the extra computation required for the closed-form updates. Each iteration involves solving a d×d linear system (or inverting a matrix of size equal to the feature dimension) to compute $W^*(\\theta)$. In the experiments, the feature dimension (d) was modest (e.g. 256 or 512), and this overhead was manageable, but for very high-dimensional features or extremely large output layers, this step could become a bottleneck. The paper does not report runtime comparisons or discuss strategies to mitigate this cost. It’s possible to use efficient linear algebra or warm-start techniques (given successive updates are on similar data), but such optimizations are not explored. Therefore, the scalability of the approach to very large networks or datasets (where d or the number of classes is in the thousands) remains a bit unclear.\n\n - Hyperparameter Sensitivity: The introduction of the proximal regularization coefficient $\\lambda$ (and the interaction with the ridge parameter β, if any) adds an extra hyperparameter that needs tuning. The paper does include an ablation showing how performance varies with $\\lambda$ and with the ridge coefficient. It appears that the method’s performance can be sensitive to the choice of $\\lambda$, especially for smaller batch sizes where this term is critical – too large $\\lambda$ might slow adaptation of the last layer, too small might reintroduce instability. The need to tune $\\lambda$ (and potentially β and learning rate jointly) makes the method a bit more complex to use than standard SGD (which typically only needs a learning rate schedule and perhaps a weight decay). While this is not a major flaw, it means practitioners must budget some effort for hyperparameter search to fully realize the benefits of the approach.\n\n - Combining with Other Optimizers: The study found that using Adam (an adaptive optimizer) for the backbone parameters degraded performance relative to SGD. This suggests the closed-form last layer strategy might not be trivially compatible with all optimizer styles. The authors hypothesize that Adam’s internal state (momentum of gradients) might conflict with the idea of immediately resetting the last layer to optimal each time. Similarly, one might wonder if adding momentum to the backbone SGD could interfere with the two-timescale dynamics. The paper focuses on basic SGD; the limited exploration of optimizer variants is a minor weakness, as it leaves unclear whether the method can reap benefits from momentum or adaptive learning rates (which are often important in state-of-the-art training regimes). It may be that the closed-form update provides sufficient acceleration that such techniques are less important, but some discussion or experiments on this would strengthen the work."}, "questions": {"value": "1. The closed-form last layer update requires solving a linear system at every iteration. Have the authors considered more efficient or scalable ways to implement this? For instance, could one exploit the approximate Kalman filter interpretation to update $W$ incrementally (reusing the previous inverse or using Sherman-Morrison updates) instead of recomputing from scratch each time? Any discussion on how the method scales with increasing feature dimension or number of classes would be valuable – e.g. is the matrix inversion step ever a bottleneck in practice, and how might one mitigate this for very large models?\n\n2. The analysis shows that $L(\\theta)$ can have spurious critical points (e.g. the feature map producing zero outputs is stationary). In practice, did the authors ever observe training getting “stuck” in a bad state, or does random initialization and SGD dynamics reliably avoid those trivial solutions? It would be helpful if the authors could elaborate on why, despite the non-convexity of $L(\\theta)$, the method seemed to find good solutions (especially in finite-width networks not covered by NTK theory). Are there any conditions or initialization strategies needed to ensure convergence to a good optimum when using this closed-form approach?\n\n3. The introduction of the proximal regularization coefficient $\\lambda$ raises the question of how to choose it. The paper provides an ablation, but could the authors offer more guidance on this? For example, should $\\lambda$ be scaled with the batch size or learning rate in some way? Is it essentially acting as an “effective batch size” or memory factor for the last layer updates? Additionally, the method still includes the ridge regularizer β in principle – in the experiments, was β set to zero (relying only on $\\lambda$), or did the authors keep a small β as well? Clarifying the role of β versus $\\lambda$ in practice (and whether one can simply set β=0 and treat $\\lambda$ as a replacement) would help practitioners understand how to configure the training objective.\n\n4. Since the closed-form strategy is tied to the squared loss, have the authors considered approximating or extending it to other losses? For instance, is there an analogue for cross-entropy (perhaps using a softmax pseudo-inverse or a one-step Newton update for the last layer)? The results on CIFAR-100 are intriguing in that the squared-loss with closed-form updates actually outperformed cross-entropy SGD. In contrast, on ImageNet the cross-entropy still had the edge. What do the authors believe explains this difference? Is it the larger number of classes, the maturity of cross-entropy tuning, or something about the loss landscapes? Any insight here could point to how one might combine the benefits of closed-form updates with the cross-entropy loss (or whether that is a promising direction at all). It would be interesting to know if a hybrid approach was attempted, for example, training with the closed-form $\\ell_2$ method and then fine-tuning or calibrating with cross-entropy, and how that performed.\n\n5. The finding that Adam performed worse than SGD in this framework is thought-provoking. Could the authors shed more light on why an adaptive optimizer or momentum might interfere with the closed-form last layer updates? Does the closed-form update essentially act like a large adaptive step for the last layer, making additional momentum unnecessary or even harmful for the backbone? It would be useful to know if the authors tried variants like adding momentum to the backbone SGD, or if they have recommendations on optimizer choice. Understanding this could help users avoid combinations that degrade performance, and it might reveal interesting interactions between fast two-timescale updates and optimizer internal dynamics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZMN4pk6f4w", "forum": "VKdjNCmXGk", "replyto": "VKdjNCmXGk", "signatures": ["ICLR.cc/2026/Conference/Submission20295/Reviewer_JUgn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20295/Reviewer_JUgn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535725521, "cdate": 1761535725521, "tmdate": 1762933765182, "mdate": 1762933765182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common rebuttal comment."}, "comment": {"value": "We thank all the reviewers for their feedback. Below, we provide wall-clock times for our method on CIFAR-100. We will add these tables to the revised version of the paper.\n\n# Wall-clock times on CIFAR-100\n\nWe consider ResNet18 and ResNet50 backbones, where add an additional last layer of a given dimensionality (**Last Layer Dim**). We report steps per second (SPS) metrics as well as total time (Time) for training a model for 100 epochs on CIFAR-100 using either our method (Algorithm 1) or cross entropy (CE). We report metrics for different batch sizes and last layer dimensions. Moreover, we report `rSPS = SPS(OURS) / SPS(CE)` (higher means our method is faster than CE) and `rTime = Time(Ours) / Time (CE)` (lower means our method is faster than CE). We use A100 GPU.\n\n**Take-aways**:\n\nFirst, we observe that for small batch size (32), rTime of our method increases from 0.92 (last layer dim = 128) to 2.37 (last layer dim = 4096) for ResNet18, and from 1.66 (last layer dim = 128) to 1.84 (last layer dim = 4096) for ResNet50. This means that for small batch sizes, as we increase the last layer dimension, our method becomes significantly slower than cross entropy. However, we also notice that the rTime decreases as we increase the model size from ResNet18 to ResNet50. This highlights the fact that as the model size increases, the computation required for the last layer becomes relatively smaller compared to the computation of the backbone.\n\nSecond, we observe that for large batch size (1024), `rTime` only increases from 1.09 (last layer dim = 128) to 1.18 (last layer dim = 4096) for ResNet18; and from 1.16(last layer dim = 128) to 1.20 (last layer dim = 4096). Finally, for ResNet18, we see that for batch size = 4096, `rTime` basically stays very similar (from 1.12 to 1.14). This highlights that in the large batch size regime, our method is roughly 10-15% slower than cross entropy.\n\n\n## Detailed metrics for ResNet18\n\n| Batch Size | Last Layer Dim | SPS (Ours) | SPS (CE) | rSPS (Ours) / (CE) | Time (Ours) | Time (CE) | rTime (Ours) / (CE) |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| 32 | 128 | 89.98 | 81.02 | 1.11 | 2164.20 | 2348.11 | 0.92 |\n| 32 | 256 | 87.35 | 78.97 | 1.11 | 2214.96 | 2400.73 | 0.92 |\n| 32 | 512 | 78.48 | 79.86 | 0.98 | 2413.57 | 2374.19 | 1.02 |\n| 32 | 1024 | 70.23 | 81.89 | 0.86 | 2633.73 | 2325.33 | 1.13 |\n| 32 | 2048 | 51.47 | 79.66 | 0.65 | 3426.96 | 2383.31 | 1.44 |\n| 32 | 4096 | 30.41 | 82.44 | 0.37 | 5488.86 | 2318.55 | 2.37 |\n| 128 | 128 | 40.57 | 41.24 | 0.98 | 1293.17 | 1277.97 | 1.01 |\n| 128 | 256 | 39.59 | 40.74 | 0.97 | 1320.46 | 1281.52 | 1.03 |\n| 128 | 512 | 38.77 | 40.72 | 0.95 | 1333.75 | 1282.12 | 1.04 |\n| 128 | 1024 | 35.47 | 40.44 | 0.88 | 1422.86 | 1291.61 | 1.10 |\n| 128 | 2048 | 29.76 | 41.14 | 0.72 | 1614.35 | 1278.58 | 1.26 |\n| 128 | 4096 | 20.92 | 40.54 | 0.52 | 2140.42 | 1294.91 | 1.65 |\n| 1024 | 128 | 4.66 | 5.07 | 0.92 | 1055.30 | 967.64 | 1.09 |\n| 1024 | 256 | 4.68 | 5.04 | 0.93 | 1046.71 | 975.81 | 1.07 |\n| 1024 | 512 | 4.68 | 5.09 | 0.92 | 1050.68 | 966.34 | 1.09 |\n| 1024 | 1024 | 4.66 | 5.05 | 0.92 | 1051.08 | 972.14 | 1.08 |\n| 1024 | 2048 | 4.57 | 5.10 | 0.89 | 1072.17 | 963.36 | 1.11 |\n| 1024 | 4096 | 4.21 | 5.02 | 0.84 | 1164.10 | 982.72 | 1.18 |\n| 4096 | 128 | 1.11 | 1.24 | 0.89 | 1084.92 | 966.99 | 1.12 |\n| 4096 | 256 | 1.09 | 1.24 | 0.88 | 1097.92 | 968.55 | 1.13 |\n| 4096 | 512 | 1.08 | 1.24 | 0.87 | 1105.35 | 963.45 | 1.15 |\n| 4096 | 1024 | 1.10 | 1.23 | 0.90 | 1089.28 | 973.90 | 1.12 |\n| 4096 | 2048 | 1.09 | 1.23 | 0.89 | 1100.39 | 974.79 | 1.13 |\n| 4096 | 4096 | 1.08 | 1.24 | 0.88 | 1107.10 | 971.15 | 1.14 |\n\n\n## Detailed metrics for ResNet50\n\n| Batch Size | Last Layer Dim | SPS (Ours) | SPS (CE) | rSPS (Ours) / (CE) | Time (Ours) | Time (CE) | rTime (Ours) / (CE) |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| 32 | 128 | 36.96 | 42.94 | 0.86 | 4735.09 | 2855.43 | 1.66 |\n| 32 | 256 | 41.18 | 42.84 | 0.96 | 4240.65 | 4056.31 | 1.05 |\n| 32 | 512 | 35.85 | 39.59 | 0.91 | 4903.87 | 3496.42 | 1.40 |\n| 32 | 1024 | 35.22 | 43.28 | 0.81 | 4903.89 | 4002.80 | 1.23 |\n| 32 | 2048 | 30.85 | 41.52 | 0.74 | 5482.36 | 4173.49 | 1.31 |\n| 32 | 4096 | 21.68 | 41.97 | 0.52 | 7605.07 | 4133.77 | 1.84 |\n| 128 | 128 | 16.53 | 18.62 | 0.89 | 2645.24 | 2384.37 | 1.11 |\n| 128 | 256 | 16.37 | 18.49 | 0.89 | 2659.99 | 2397.10 | 1.11 |\n| 128 | 512 | 16.15 | 18.30 | 0.88 | 2698.11 | 2423.22 | 1.11 |\n| 128 | 1024 | 15.45 | 18.52 | 0.83 | 2801.06 | 2405.96 | 1.16 |\n| 128 | 2048 | 14.31 | 18.44 | 0.78 | 3000.61 | 2409.51 | 1.25 |\n| 128 | 4096 | 11.95 | 18.05 | 0.66 | 3532.58 | 2448.23 | 1.44 |\n| 1024 | 128 | 2.26 | 2.59 | 0.87 | 2154.44 | 1875.26 | 1.15 |\n| 1024 | 256 | 2.25 | 2.61 | 0.86 | 2151.89 | 1866.88 | 1.15 |\n| 1024 | 512 | 2.24 | 2.60 | 0.86 | 2163.26 | 1874.76 | 1.15 |\n| 1024 | 1024 | 2.23 | 2.59 | 0.86 | 2174.04 | 1876.25 | 1.16 |\n| 1024 | 2048 | 2.19 | 2.57 | 0.85 | 2216.16 | 1889.88 | 1.17 |\n| 1024 | 4096 | 2.12 | 2.56 | 0.83 | 2286.04 | 1898.44 | 1.20 |"}}, "id": "Q0zUEuRgRz", "forum": "VKdjNCmXGk", "replyto": "VKdjNCmXGk", "signatures": ["ICLR.cc/2026/Conference/Submission20295/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20295/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20295/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763116450699, "cdate": 1763116450699, "tmdate": 1763116450699, "mdate": 1763116450699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper exploits the fact that the square loss of a neural network is typically quadratic as a function of the last layer weights, therefore optimal values of the weights can be expressed in closed form (as a function of the other parameters).\nThey show that optimization of the other parameters does not require differentiating through the closed form solution, and provide a few fixes to the problem of overfitting small batches by the solution. \nSome experiments on PDE and image classification show that the proposed method outperforms SGD on square loss, but it does not outperform standard cross entropy in most cases."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is extremely clear and straightforward.\n\n- The method is very easy to implement."}, "weaknesses": {"value": "- The idea is not quite new, other papers had used it even if some details are different.\n\n- Theoretical contributions are weak. Theorem 1 and 2 are trivial. I’m sure you can find them in many previous papers, perhaps in slightly different contexts. They are not really “Theorems”, they are just “chain rule.”\n\n- The section about NTK and Theorem 3 and 4 are also trivial. It’s obvious that a non-convex loss may have stationary points that are not global minimisers, and that is resolved by the NTK. This is all very well known. \n\n- Experimental results are underwhelming. Although the comparison with SGD on square loss is somewhat fair from the scientific point of view, it’s very weak, and it's not clear whether this method will be ever be practical at all. Cross-entropy wins most of the time, and it’s not clear what would happen when the method is compared with and/or extended to other optimisers that outperform SGD by large amounts. \n\nMinor:\n\n- I disagree with the statement: “Without the regularization to previous last layer solutions, our method is analogous to putting a large learning rate on the last layer.” If you put a large learning rate on W then optimization of W destabilize, even if loss is quadratic. Instead, your method is equivalent to a Newton step, that converges to the minimum of a quadratic loss in one step.\n\n- I believe Eq.(21) has a typo in the LHS second term, it should be a first derivative. \n\n- In the non-stochastic setting, it should be possible to show that Eq.(9) is necessarily better than standard gradient descent. That would have been a nice result (although still quite unsurprising)."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OucTec5fQA", "forum": "VKdjNCmXGk", "replyto": "VKdjNCmXGk", "signatures": ["ICLR.cc/2026/Conference/Submission20295/Reviewer_k9oX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20295/Reviewer_k9oX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926737919, "cdate": 1761926737919, "tmdate": 1762933764653, "mdate": 1762933764653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces new optimization methods that treat the last layer weights as an explicit function of the backbone parameters and optimize this function with respect to only the backbone parameters. The authors also extend this framework to a stochastic gradient descent (mini-batch) version. They prove the convergence guarantee under the Neural Tangent Kernel (NTK) regime. Experimental results demonstrate that the proposed methods perform better compared to baseline SGD across a variety of tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Rigorous derivation of closed-form last-layer optimization methods.\n- The extension to the SGD mini-batch setting with proximal loss is an interesting derivation.\n- Empirical results demonstrate better efficiency and accuracy compared to standard SGD across multiple tasks.\n- Theoretical analysis under the Neural Tangent Kernel (NTK) regime provides solid convergence guarantees in the infinite-width limit."}, "weaknesses": {"value": "- Typos and formatting issues detract slightly from readability and presentation quality.\n- The paper lacks a clear explanation of how the NTK regime theoretical analysis directly supports convergence guarantees of the two optimization methods introduced above.\n\nTypos:\n- In line 267, equation (18), the author should include the sum of all square losses\n- Line 289-290: “the initial function neural network function $\\phi$...”\n- Line 293-294, “The following result shows that if we make the slightly stronger assumption $\\textbf{that}$ the NGPK is positive definite...”\n\nFigures and references format:\n- Page 12 and page 21"}, "questions": {"value": "- In Figure 1, why does the curve for $W^\\ast(\\theta)$  appear to be multi-valued for some values of $\\theta$, while later it is treated as a function of $\\theta$? Could the authors clarify this discrepancy?\n- In equation (13), should the matrices $X$ and $Y$ correspond to mini-batch subsets $\\mathcal{B}_t$ instead of the full datasets?\n- In the experiments, is the \"$l_2$ c.f. ridge ($\\beta$)\" method implemented with mini-batches, or is it full-batch as suggested by the equation (9)?\n- How does the NTK regime theoretical analysis relate to the convergence theory of the two optimization methods introduced before?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E4t8vaimYu", "forum": "VKdjNCmXGk", "replyto": "VKdjNCmXGk", "signatures": ["ICLR.cc/2026/Conference/Submission20295/Reviewer_A5q7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20295/Reviewer_A5q7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762488681758, "cdate": 1762488681758, "tmdate": 1762933764354, "mdate": 1762933764354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an optimization method that leverages the closed-form solution for the linear last layer under a squared loss. The method treats the last layer as a function of the backbone parameters and optimizes only the backbone, which is shown to be equivalent to alternating gradient steps on the backbone with closed-form updates on the last layer.\n\n**Problem formulation**\n\nThe paper considers a model $f(x;W,\\theta)=W\\phi_{\\theta}(x)$, where $\\phi_{\\theta}$ is the neural network backbone and $W$ is the last linear layer. Since the optimal $W^{\\star}(\\theta)$ for a fixed $\\theta$ is known in closed-form via ridge regression, the authors reformulate the problem to optimize the backbone parameters $\\theta$ by minimizing the loss $\\mathcal{L}^{star}(\\theta):=\\mathcal{L}(W^{\\star}(\\theta),\\theta)$.\n\n**Main results. Provide one or two sentence summary**\n\nThe key theoretical result is that optimizing this reformulated loss does not require backpropagation through the complex closed-form solution; by the envelope theorem, the gradient $\\nabla_{\\theta}\\mathcal{L}^{*}(\\theta)$ is simply $\\nabla_{\\theta}\\mathcal{L}(W^{\\star},\\theta)$. This property is extended to a practical stochastic (proximal) version of the loss, and the method is proven to converge to a global minimum in the NTK regime.\n\n**Technical approach**\n\nTo adapt the method for stochastic gradient descent and prevent overfitting the last layer to minibatches, the authors introduce a proximal loss that regularizes the batch solution against the previous last layer estimate $W_t$. The practical algorithm (Algorithm 1) first updates the backbone parameters $\\theta_t$ via a standard gradient step (using $W_{t-1}$), and then computes the new last layer $W_t$ using the closed-form solution of this proximal loss based on the current batch and updated backbone $\\theta_t$.\n\n**Experiment**\n\nThe proposed proximal method is shown to outperform standard SGD on a squared loss across regression (Fourier Neural Operators, DFIV) and classification (CIFAR, ImageNet) tasks. The approach is particularly effective and stable across all batch sizes, unlike a naive closed-form ridge solution which performs poorly on small batches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper develops a practical and stable proximal-based algorithm that effectively leverages the closed-form last layer solution to accelerate training in stochastic, small-batch settings where naive closed-form updates would otherwise fail. Both theory and experiments are solid"}, "weaknesses": {"value": "."}, "questions": {"value": "."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "."}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "mFThS5CFw1", "forum": "VKdjNCmXGk", "replyto": "VKdjNCmXGk", "signatures": ["ICLR.cc/2026/Conference/Submission20295/Reviewer_aGYw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20295/Reviewer_aGYw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762813027141, "cdate": 1762813027141, "tmdate": 1762933763874, "mdate": 1762933763874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}