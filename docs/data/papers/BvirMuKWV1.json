{"id": "BvirMuKWV1", "number": 2864, "cdate": 1757292542189, "mdate": 1759898122107, "content": {"title": "When a Robot is More Capable than a Human: Learning from Constrained Demonstrators", "abstract": "Learning from demonstrations enables experts to teach robots complex tasks using interfaces such as kinesthetic teaching, joystick control, and sim-to-real transfer. However, these interfaces often constrain the expert's ability to demonstrate optimal behavior due to indirect control, setup restrictions, and hardware safety. For example, a joystick can move a robotic arm only in a 2D plane, even though the robot operates in a higher-dimensional space. As a result, the demonstrations collected by constrained experts lead to suboptimal performance of the learned policies. This raises a key question: Can a robot learn a better policy than the one demonstrated by a constrained expert? We address this by allowing the agent to go beyond direct imitation of expert actions and explore shorter and more efficient trajectories. We use the demonstrations to infer a state-only reward signal that measures task progress, and self-label reward for unknown states using temporal interpolation. Our approach outperforms common imitation learning in both sample efficiency and task completion time. On a real WidowX robotic arm, it completes the task in 11 seconds, 10x faster than behavioral cloning.", "tldr": "Inverse RL method to learn from and surpass expert demonstrations provided under interface constraints.", "keywords": ["Inverse Reinforcement Learning", "Learning from Observations", "Learning from Constrained Expert Demonstrations", "Robot Learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8f11af425c58270012baff3e56c4065007b37674.pdf", "supplementary_material": "/attachment/3c1b9fdcd51884ce7089b3cecf118f6593481c17.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents LfCD-GRIP, a new way to do imitation learning/learning from demonstration that considers the fact that the input expert demonstrations likely have some amount of \"constraint\" to them. The proposed approach seeks to learn better policies by using \"unconstrained\" actions. This is done by estimating the \"confidence\" in the \"proximity\" of different states to the goal, finding high-confidence states, and interpolating the proximity of the intermediate states. In this way a more generalized model of proximity can be used to guide the PPO RL loss function, which allows learning to visit states and take actions that may not have shown up in the expert demonstrations. The authors show results in simulation and on a real robot against several baselines, showing faster task performance, particularly in the setting where expert actions were constrained."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality\n- The approach of identifying high-confidence good states and then interpolating between them is novel to me\nQuality\n- The paper compares against several baselines and has ablations\nClarity\n* The graphs and visualizations are clear\n* The paper is generally well-written\n* The literature review is appropriately broad and deep for a problem like this \nSignificance\n- All expert demonstrations are constrained in some way, so this general idea and the problem being tackled has broad applicability across imitation learning."}, "weaknesses": {"value": "- The contribution of the paper, while novel, is fairly small. Proximity-based approaches are already published work (like the cited Lee et al. 2021). LfCD-GRIP only performs slightly better than the Proximity approach, with the small changes of estimating proximity confidence. Proximity interpolation is done by time, which is the same as Lee et al. 2021.\n- There are so many figures in the paper that there is not much room for discussion of results.\n* The results are not presented in a way that clearly shows that the proposed approach is better than prior work.\n\t* Graphs include error bars, but there is no indication of confidence levels or statistical significance.\n\t* No comparison to Ma et al. 2022, which was listed in the lit review and seems it seeks to solve almost the exact same problem.\n- The explanation of the proposed approach is incomplete.\n\t* Line 251 says \"we identify sub-trajectories where both endpoints are high-confidence, and use them as anchors for interpolation\", but there is no description of how high-confidence endpoints are determined. I had to go to the code to determine how it is done, and it's not a fixed threshold but more complicated. This is an important part of the approach and should be described in detail.\n* The experiments could more clearly illustrate the problem and proposed solution.\n\t* Most of the examples of a \"constrained\" expert are simply limiting the magnitude of the possible motion. This seems like an almost trivial constraint to overcome by using any amount of reward shaping during the RL process, like penalizing total time taken. However, I don't see evidence that this was tested against.\n\t* An experiment where the constraint was relaxed gradually (i.e. increasing the possible action interval in steps) to see that the proposed approach improves as the constraint is relaxed"}, "questions": {"value": "* Exactly how are \"high-confident\" states determined? I want to make sure I understood the code correctly.\n* What is the intuition around how Proximity-Drop performs almost universally worse than Proximity? Did you test LfCD-GRIP without Drop - it seems like it might perform better?\n* Conceptually, why wouldn't a technique like Proximity be able to learn to take diagonal actions in MiniGrid?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vh1YrYdh1N", "forum": "BvirMuKWV1", "replyto": "BvirMuKWV1", "signatures": ["ICLR.cc/2026/Conference/Submission2864/Reviewer_Q8E8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2864/Reviewer_Q8E8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761191394335, "cdate": 1761191394335, "tmdate": 1762916420357, "mdate": 1762916420357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for Learning from Constrained Demonstrations, Goal-proximity Reward InterPolation (LfCD-GRIP). LfCD-GRIP extends proximity-based IRL with confidence-guided reward propagation. The authors' formulation of LfCD is interesting, specifically focused on scenarios where the demonstrator is optimal within their constrained action space but where the robot has access to a broader set of actions. The methodology is well-designed and straightforward. The authors test their approach against several baselines and ablations of their proposed framework, and find that LfCD-GRIP has several benefits."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper has an interesting formulation of LfCD. I could see many applications where a robot may need to learn from LfCD.\n+ The paper is well-written and clear.\n+ The paper has ample results and a deployment on a real-world robot."}, "weaknesses": {"value": "- The authors should better explain the difference between their framework and those that learn from suboptimal demonstrations. To my knowledge, works on learning from suboptimal demonstration often have similar motivation and can be applied to the same problem statement posed in this paper. This would also help better highlight the novelty in the proposed work.\n- The results have very large standard deviations, and it is unclear whether LfCD-GRIP is actually outperforming other frameworks. A statistical significance analysis and explanation regarding large standard deviations would be beneficial.\n- The key result noted in the intro (100 seconds to 12 seconds) seems an overclaim given the large standard deviation in Figure 8."}, "questions": {"value": "1. Can the authors comment on why SSRR performs so poorly across these domains? From my knowledge of that framework, I cannot see a specific reason on why that framework should underperform GAIL by such a large margin.\n2. How does the second term in Equation 2 help avoid overgeneralization?\n3. Can you reply to the weaknesses noted above?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XhHEvTj3bV", "forum": "BvirMuKWV1", "replyto": "BvirMuKWV1", "signatures": ["ICLR.cc/2026/Conference/Submission2864/Reviewer_jvYJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2864/Reviewer_jvYJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761444591436, "cdate": 1761444591436, "tmdate": 1762916419683, "mdate": 1762916419683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on the setting where the demonstrator performance is hindered due to constraints in observations/actions.\nThe paper questions whether it is possible to create a policy, without said constraints, to outperform the constrained demonstrator using the generated demonstrations.\nThe paper proposes to learn a goal-proximity reward with a confidence-based proximity interpolator, that consists of a confidence estimator and a trajectory-wise interpolator.\nThe former estimates the confidence through Monte-Carlo dropout and aims to identify reliable observations, and the latter estimates low-confidence observations through interpolating the values between reliable observations.\nThe paper then conduct experiments on four simulated tasks and a real-world manipulation picking task, and demonstrate that the proposed method identifies out-of-constraint actions that result in shorter trajectories."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The introduced problem is well motivated, especially in the robotic setting where the demonstrator may have less degree-of-freedoms compared to the actualy embodiment.\n- The proposed idea is straightforward and easy-to-follow, and the writing is clear.\n- The analyses on MiniGrid environment and out-of-constraint actions are great to demonstrate that the proposed method identifies better actions beyond the demonstrator."}, "weaknesses": {"value": "I am happy to increase my score after these comments are addressed.\n- While the confidence estimation module will provide low variance on \"reliable observations\", the uncertainty might come from another source, e.g., having multiple demonstrators gathering data.\n\t- As we scale the number of demonstrators, this might become a degenerate problem where none of the demonstrations contain high-confidence states.\n- There are a handful of design/hyperparameter choices that I am unsure if it's well justified/experimented.\n\t- Decaying strategy for masking\n\t- What's considered as high confidence?\n\t- The choice of interpolation strategy\n\t- There are no sensitivity analysis on any hyperparameters, especially on the introduced modules\n\t\t- $\\delta$, $K$, etc.\n- Nit: It would be great if the paper shows the expert performance on all figures."}, "questions": {"value": "- About Eq. 5: Is log-scale goal proximity distances the same as $log(r_{end}) - log(r_{start})$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RiLjCz1TG9", "forum": "BvirMuKWV1", "replyto": "BvirMuKWV1", "signatures": ["ICLR.cc/2026/Conference/Submission2864/Reviewer_pvLM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2864/Reviewer_pvLM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761564006307, "cdate": 1761564006307, "tmdate": 1762916419401, "mdate": 1762916419401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LfCD, a learning from demonstration method that specializes scenarios where the demonstrators are \"constrained\" compared to the robot. LfCD extends the IRL framework by 1) using a state-only, learned proximity reward and 2) a mechanism to enhance fidelity of the proximity reward on OOD observations. The core idea of 2) is to provides reward target computed from linearly interpolating confident states to the less confident ones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper is well written and presented. The algobox provides a clear overview of the presented approach.\n+ The core idea of creating pseudo supervision using confident predictions makes a lot of sense"}, "weaknesses": {"value": "- It seems the presented approach only works in setups where the state space is 1) observable / low-dimensional and 2) smooth. If 1) or 2) does not hold, linear interpolation might not be a proper way to obtain the supervision targets. \n- While the presented approach makes sense, I don't quite see how it relates to the motivation of expert being constrained. The proposed method seems rather generic.\n- Similarly, I would like to see how the presented approach improves over the baseline, over various levels of the demonstrator being \"constrained\".\n- I'd like to see a more detailed analysis on the sensitivity of the hyperparameters of the method, such as the confidence/non-confidence threshold, data / compute of the pretraining of the network vs training etc."}, "questions": {"value": "Similar to the weakness section above. \n\n- Can the presented approach apply to non-smooth, or high-dimensional state space, or how can one extend the method to make it work on them?\n- How does the presented approach improve over the baseline over various \"constrained\" level of the demonstrator?\n- Can we get a more detailed analysis on the sensitivity of the hyperparameters of the method, such as the confidence/non-confidence threshold, data / compute of the pretraining of the network vs training etc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t2cTmAHlwA", "forum": "BvirMuKWV1", "replyto": "BvirMuKWV1", "signatures": ["ICLR.cc/2026/Conference/Submission2864/Reviewer_25Mi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2864/Reviewer_25Mi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966282863, "cdate": 1761966282863, "tmdate": 1762916419134, "mdate": 1762916419134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}