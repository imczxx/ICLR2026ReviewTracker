{"id": "b38RJbhG2Z", "number": 23482, "cdate": 1758344473034, "mdate": 1759896812481, "content": {"title": "Feature Augmentation of GNNs for ILPs: Local Uniqueness Suffices", "abstract": "Integer Linear Programs (ILPs) are central to real-world optimizations but notoriously difficult to solve. Learning to Optimize (L2O) has emerged as a promising paradigm, with Graph Neural Networks (GNNs) serving as the standard backbone. However, standard anonymous GNNs are limited in expressiveness for ILPs, and the common enhancement of augmenting nodes with globally unique identifiers (UIDs) typically introduces spurious correlations that severely harm generalization. To address this tradeoff, we propose a parsimonious Local-UID scheme based on d-hop uniqueness coloring, which ensures identifiers are unique only within each node’s d-hop neighborhood. Building on this scheme, we introduce ColorGNN, which incorporates color information via color-conditioned embeddings, and ColorUID, a lightweight feature-level variant.\nWe prove that for d-layer networks, Local-UIDs achieve the expressive power of Global-UIDs while offering stronger generalization. Extensive experiments show that our approach (i) yields substantial gains on three ILP benchmarks, (ii) exhibits strong OOD generalization on linear programming datasets, and (iii) further improves a general graph-level task when paired with a state-of-the-art method.", "tldr": "For ILPs, we propose a locally coloring-based methods that make feature augmentation more parsimony and thus generalizes better, while keeps the same expressive power.", "keywords": ["Integer Linear Programming; Feature Augmentation; Graph Neural Networks;  Learning to Optimize; Expressive Power"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e83e03230ad0bf21f82543ecb2e8fb2844424afb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed a coloring method as feature augmentation for learning to optimize ILP problems. First, the graph is properly colored so that every two d-neighbors of the same node acquire different colors, then the color information is encoded and treated as extra node features. The GNN then performs message passing with the augmented features. The authors show expressivity results and generalization bound."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation is novel and clear, instead of coloring the nodes differently in the same orbits, the authors proposed to color the nodes differently in d-neighbors, which is more parsimonious and requires fewer colors, and intuitively can generalize better.\n- The proposed method is very simple and neat, should be easy to implement. \n- The proposed method looks promising on some of the experiments."}, "weaknesses": {"value": "- I believe the paper writing can be improved a lot, some proofs can be moved the appendix, while some concepts should be better explained in the main paper. The paper seems to be written in haste, and a lot of concepts are not well explained, which limits readers' understanding of the paper. Also see my questions below. \n- The greedy coloring algorithm seems expensive when the d grows, as the degree of the power graph will be very high. \n- The experiment part also seems rushed out in haste. The authors reported results on d=1, but I believe they should test more on the d hyperparameter, as it is the most crucial parameter of the whole paper. And the experiments should repeat with a couple of random seeds. \n- __The derivation of the generalization bound is totally missing, no appendix attached!__"}, "questions": {"value": "- Is observation 1 supported by clear theory? Or just because some prior work all used shallow GNNs? If the latter, it is quite convincing to stick to shallow GNNs.\n- The authors proposed a greedy method to color the graph. Is the greedy approach characterized in terms of optimality? If the greedily colored graph uses too many unnecessary colors, the generalization bound might be violated. \n- The authors stated in the intro that: \"ColorGNN is value\u0002invariant: it automatically embeds colors without requiring predefined numeric encodings...\" But I cannot find how ColorGNN is actually implemented. The emb function in line 193 is unclear to me. \n- The authors claim that ColorUID is a light-weight version of ColorGNN, what is exactly the difference? In my opinion this should be clearly stated in the paper, in a mathematically way instead of just a few sentences. \n- It is not quite motivated to me why authors tested generalization performance on LPs, as far as I know LPs do not require such colorings to break the symmetry. And it makes little sense to test on Zinc. To test the generalization performance, it makes more sense to train on one ILP dataset and test on another."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VLCR21AKcX", "forum": "b38RJbhG2Z", "replyto": "b38RJbhG2Z", "signatures": ["ICLR.cc/2026/Conference/Submission23482/Reviewer_myYa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23482/Reviewer_myYa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761161629468, "cdate": 1761161629468, "tmdate": 1762942677650, "mdate": 1762942677650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel feature augmentation scheme for Graph Neural Networks (GNNs) applied to Integer Linear Programming (ILP) problems. The authors propose a \"Local-UID\" (Unique Identifier) scheme based on d-hop unique coloring, where nodes only need unique identifiers within their d-hop neighborhoods rather than globally. They introduce two architectures: ColorGNN (which uses color-conditioned embeddings) and ColorUID (a lightweight variant). The key insight is that for d-layer GNNs, local uniqueness within d-hop neighborhoods provides the same expressive power as global UIDs while requiring fewer identifiers and offering better generalization."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper provides rigorous theoretical analysis showing that Local-UIDs achieve the same expressive power as Global-UIDs (Theorem 1) while offering better generalization bounds\n* The greedy d-hop coloring algorithm (Algorithm 1) is efficient and practical, with clear complexity bounds.\n* The evaluation covers multiple ILP benchmarks and demonstrates strong out-of-distribution generalization on LP tasks.\n* The approach significantly reduces the number of required identifiers (e.g., 32 vs 420 on BPP dataset), which aligns well with the principle of parsimony in feature augmentation.\n* ColorGNN is value-invariant and can be seamlessly combined with other feature augmentation techniques like Orbit and Orbit+."}, "weaknesses": {"value": "* On SMSP dataset, the method underperforms Orbit/Orbit+ on Top-30% and Top-50% metrics, suggesting limitations when instance-specific information is crucial.\n* The paper mainly focuses on one specific way of incorporating colors (through embeddings). More architectural variants could strengthen the contribution."}, "questions": {"value": "* How does performance change with different values of d? Is there an optimal d for different problem types?\n* How sensitive is the method to different color assignments? Would different runs of the greedy algorithm produce significantly different results?\n* What is the actual runtime overhead of computing d-hop colorings on the largest graphs in your experiments?\n* Beyond ILPs and LPs, what other optimization problems could benefit from this approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wc0DYZUFzU", "forum": "b38RJbhG2Z", "replyto": "b38RJbhG2Z", "signatures": ["ICLR.cc/2026/Conference/Submission23482/Reviewer_qakW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23482/Reviewer_qakW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761299788248, "cdate": 1761299788248, "tmdate": 1762942677421, "mdate": 1762942677421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper seeks to address the challenge of solving integer linear programming (ILP) problems using Graph Neural Networks (GNNs). Since conventional GNNs are limited in their expressivity, the authors note that augmenting nodes with unique identifiers is useful but not generalizable. To address this limitation, they propose a Local-UID scheme based on d-hop uniqueness coloring, which ensures that identifiers are unique only within each node’s d-hop neighborhood. The paper provides some theoretical justification along with experimental analysis to validate the method."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The motivation of restricting node IDs to local d-hops is interesting and, to my knowledge, has not been studied previously.\n1. The writing of the paper is fairly clear, although the presentation could be improved."}, "weaknesses": {"value": "1. The paper is not clearly positioned. The abstract/introduction mentions the task of ILP as the main motivating factor. However, as the method develops, it appears the authors are addressing general GNN challenges. It is unclear why ILPs are taken as the main task rather than conventional tasks that have clear benchmarks for evaluation.\n1. If ILPs are the main motivation, why do the authors test on the ZINC dataset, which has no relevance to the method?\n1. The evaluation of the method is not comprehensive, with very few baselines included. The authors need to position the paper more clearly and thoroughly analyze the method across a variety of settings to demonstrate its benefits.\n1. The theoretical development of the idea is somewhat straightforward. I do not see any specific results that are non-trivial and reveal deeper insights."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u79rBs57FU", "forum": "b38RJbhG2Z", "replyto": "b38RJbhG2Z", "signatures": ["ICLR.cc/2026/Conference/Submission23482/Reviewer_rUR6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23482/Reviewer_rUR6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762079333859, "cdate": 1762079333859, "tmdate": 1762942677136, "mdate": 1762942677136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "It is known that node IDs can help the expressivity of GNNs but the paper argues that global IDs may harm generalization, hence it propsoes the use of local identifiers. Specifically, it proposes a unique local coloring of nodes for up to d hops. That means that each node in a d-hop neighborhood has a unique color. This is done via a greedy algorithm and yields substantial savings compared to global IDs.  This is then used to inform architecture design for a specific GNN. It is shown that a d layer GNN with local (d hop) ids is just as expressive as one with global ids.\nThe paper proves expressivity and generalization results and experiments on solving IPs and LPs.`"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and the different elements of the paper clearly support its main position. \n- The paper provides a good mix of theoretical and empirical support for its method.\n- The empirical performance looks solid and the theoretical results (I didn't check proofs thoroughly) make sense."}, "weaknesses": {"value": "- The motivation is a little vague. It is claimed that global IDs can lead to spurious correlations but is there any study in the literature that corroborates that? It seems you could argue about the merits of local features purely from an efficiency standpoint. But the paper actually proposes a design principle. I think for this kind of proclamation the paper requires stronger empirical evidence or more references to related work that support this position. For the record, I do think it's a reasonable proposal but if it's to be proposed as a design principle it should be well supported.\n- Discussion of related work seems inadequate. Previous papers have talked about GNN expressivity in various contexts (including solving combiantorial problems) so some comments on how this paper differs or connects to existing approaches would go a long way, e.g., [1] uses also local features and proves their expressivity. Also more discussion on relationships to other works like [2,3])  would be good. How does the local coloring proposed here compare to the weak coloring proposed in the paper by Sato et al. If those ideas are related I think it is important to connect to this literature even though those papers are not specifically about solving IPs. Similarly, what about positional encodings for GNNs as discussed in [4] (and references therein)? On that note, having a standard strong baseline such as Signnet[5] in the experiments would be nice.\n- For a d-layer GNN, do you need d-hop colorings or can you go way lower than that in practice? This perhaps connects to my comments about weak coloring above. This is a central contribution of the paper so I would be curious to see an ablation that compares to other local colorings.\n- In table 3 WA OOD MSE seems to vary quite a bit. How is it possible that it goes from 10^2 to 10^-1? Is the model training at all when there are no local IDs?\n\nOverall, this is an interesting paper that I am open to accepting. I start with a tentative score that I will update after the rebuttal.\n\n1. Vignac, Clement, Andreas Loukas, and Pascal Frossard. \"Building powerful and equivariant graph neural networks with structural message-passing.\" Advances in neural information processing systems 33 (2020): 14143-14155.\n2. Sato, Ryoma, Makoto Yamada, and Hisashi Kashima. \"Approximation ratios of graph neural networks for combinatorial problems.\" Advances in Neural Information Processing Systems 32 (2019).\n3. Sato, Ryoma, Makoto Yamada, and Hisashi Kashima. \"Random features strengthen graph neural networks.\" Proceedings of the 2021 SIAM international conference on data mining (SDM). Society for Industrial and Applied Mathematics, 2021.\n4. Grötschla, Florian, Jiaqing Xie, and Roger Wattenhofer. \"Benchmarking positional encodings for gnns and graph transformers.\" arXiv preprint arXiv:2411.12732 (2024).\n5. Lim, Derek, et al. \"Sign and basis invariant networks for spectral graph representation learning.\" arXiv preprint arXiv:2202.13013 (2022)."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kuIKy6K3QT", "forum": "b38RJbhG2Z", "replyto": "b38RJbhG2Z", "signatures": ["ICLR.cc/2026/Conference/Submission23482/Reviewer_jhyf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23482/Reviewer_jhyf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153296848, "cdate": 1762153296848, "tmdate": 1762942676960, "mdate": 1762942676960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}