{"id": "rXma48njj6", "number": 22701, "cdate": 1758334555771, "mdate": 1759896851679, "content": {"title": "WS-GRPO: Weakly-Supervised Group-Relative Policy Optimization", "abstract": "Group-Relative Policy Optimization (GRPO) has emerged as an effective approach for training language models on complex reasoning tasks by normalizing rewards within groups of rollouts. \nHowever, GRPO's group-relative advantage estimation critically depends on dense step-wise reward signals throughout the reasoning process. \nIn practice, obtaining such dense supervision requires expensive human annotations of intermediate reasoning steps or carefully designed step-wise reward functions. \nThis creates a significant challenge specific to group-relative methods: while GRPO performs best with dense intermediate feedback, \nreal-world scenarios often provide only sparse outcome supervision—such as final answer correctness or binary trajectory labels.\nWe propose Weakly-Supervised Group-Relative Policy Optimization (WS-GRPO), \nwhich addresses this unique limitation by learning to extract dense preference signals from sparse outcome supervision while preserving GRPO's group-relative normalization benefits. \nWS-GRPO operates in two phases: first, it trains a preference model to distinguish between successful and unsuccessful reasoning patterns using only trajectory-level outcomes; \nsecond, it leverages this learned preference model to provide step-wise weakly-supervised rewards that are combined with sparse terminal rewards during group-relative policy optimization. \nBy treating consecutive partial trajectories as preference pairs, our method generates dense feedback signals that complement GRPO's group normalization mechanism without requiring step-by-step human annotations.\nTheoretically, we provide comprehensive guarantees for WS-GRPO establishing preference model consistency under trajectory-level supervision, policy robustness to preference errors with controllable degradation rates, and generalization bounds that decompose error sources across policy learning, preference modeling, and their interaction. Our experiments on reasoning benchmarks demonstrate that WS-GRPO achieves competitive performance using only weak supervision, making group-relative policy optimization practical when detailed process supervision is limited.", "tldr": "WS-GRPO learns to extract dense step-wise rewards from sparse outcome supervision by training a preference model on trajectory pairs, enabling effective group-relative policy optimization without expensive intermediate annotations.", "keywords": ["Weak Supervision", "Large Language Models", "Group Relative Policy Optimization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f3474dc5a839f7750cfb91b222272e964a4cd66.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "GRPO performs well for multi-step reasoning but requires dense, step-wise rewards, which are costly to obtain. In practice, GRPO often relies only on sparse outcome labels, such as final correctness. The authors propose WS-GRPO, a method designed to address this limitation by extracting dense, step-level feedback from weak supervision through a preference learning scheme that distinguishes binary outcomes for trajectories while preserving GRPO’s inherent advantages. The authors present two theoretical analyses of their method and conduct experiments on two base model families and two factuality-based benchmarks to evaluate its effectiveness."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of using stepwise signals to transform sparse rewards into dense ones is interesting and timely. This approach is particularly valuable for critic-free GRPO applications, where learning signals may vanish due to all-positive or all-negative groups.\n\n- The overall structure of the work is solid, with clear motivation and some theoretical analysis supporting the proposed method."}, "weaknesses": {"value": "See below, I provide some suggestions for further improvement:\n\n- **[W1]** The overall manuscript is not clearly written; formatting issues and a lack of writing clarity significantly hinder readability. These issues should be addressed in the next revision. Specifically, please replace all instances of \\cite with \\citep or \\citet as appropriate to ensure correct citation formatting. Additionally, please fill in all missing content, e.g., in line 592: *\"Following (?Bartlett & Mendelson, 2002), …\"*, to maintain completeness.\n\n---\n\nThe effect and rationale for using the preference model are unclear. See detailed comments below:\n\n- **[W2]** The use of a small MLP with only 512 units to learn preferences is not convincing in terms of its effectiveness and generalizability. The authors do not provide training curves or results showing the model’s reward performance on the validation set.\n\n- **[W3]** It is unclear how stepwise reasoning quality is captured through the labeled dataset, as the labels are still based on final outcome correctness. The binary preference learning process continues to rely on outcome-level rewards, raising concerns about the rationale for using such a method to reflect reasoning quality. (Also see [W5].)\n\n- **[W4]** The necessity of using a \"weakly supervised\" method is not well justified. Why not directly apply DPO to learn preference pairs? No baselines are compared against this approach, making it difficult to assess the claimed advantages. Please clarify why your preference update scheme should be considered more effective than existing alternatives.\n\n- **[W5]** There are no validation or ablation results to support that the method accurately captures stepwise reasoning quality. Specifically, there is no quantitative analysis to substantiate the claim that *\"semantic representations of reasoning chains contain implicit quality signals\"* (lines 256–257).\n\n---\n\nThere are also several limitations and inaccuracies in the theoretical analysis, and it is difficult to see how the presented theory connects to stepwise reasoning quality:\n\n* **[W6]** From a high-level perspective, it is unclear how the theoretical objective reflects *stepwise* accuracy. The pairs $(\\tau_i^+, \\tau_i^-)$, according to your definition, are still trajectory-level and outcome-based. Therefore, the effectiveness of the preference model cannot be theoretically justified as capturing stepwise quality.\n\n* **[W7]** Even if both bounds are technically correct, their practical significance is unclear. The paper should include a dedicated discussion explaining the empirical meaning of the variables appearing in the bounds and, ideally, provide a rough estimation of their scale in real experiments.\n\n* **[W8]** In Eq. 13, you assume unbiasedness to claim that the bias term has zero mean. This appears to be the key link between outcome-based labels and stepwise quality. However, if my understanding is correct, this assumption is extremely strong and generally unrealistic for reasoning rollouts. It effectively assumes that *\"good results always come from good reasoning and bad results always come from bad reasoning\"*, which is arbitrary and contradicts the stated goal of distinguishing rollouts at the *stepwise* level. Since this assumption is used, not derived, to eliminate $R_{\\text{bias}}$ and obtain later results, the overall proof structure and claims relying on it should be revised.\n\n* **[W9]** The derivation from Eq. 25 to Eq. 28 is unclear. The argument appears to jump from a VC-bound on a loss function to an $\\ell_\\infty$ deviation bound for real-valued outputs passed through a sigmoid. This step is invalid because VC theory applies to binary classification, not directly to real-valued function classes.\n\n* **[W10]** In Eq. 24, you cannot use a uniform strong convexity constant for unbounded logits, as the binary cross-entropy loss is not globally strongly convex. You must either explicitly assume bounded logits or revise all related parts of the proof to address this issue.\n\n* **[W11]** In the RL theory section, clipping and KL regularization appear to be omitted from the analysis. If these factors are intentionally excluded, please state this clearly; otherwise, they should be incorporated into the theoretical framework for consistency with GRPO.\n\n---\n\nLimitation in GRPO experiment part: \n- **[W12]** For the benchmark in table 2, is the metric performance with $\\uparrow$ or $\\downarrow$? Why WS-GRPO performed even with lower accuracy than Dr. GRPO and GRPO on 75\\% of the benchmark?\n- **[W13]** Baseline performance before training isn't reported, i.e., it's unclear how the training in GRPO is effective on this benchmark and to assess the relative change? \n- **[W14]** The scope of experiment is narrow, as it's only conducted on two factual-based benchmarks; readers will be more interesting to see how your stepwise method could facilitate the tighter step-to-step thinking, e.g., Math benchmarks. Considering to add more experiments in math related area would strengthen your claim."}, "questions": {"value": "See Weakness part. Please address the concerns and questions in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rvOPQE2mc1", "forum": "rXma48njj6", "replyto": "rXma48njj6", "signatures": ["ICLR.cc/2026/Conference/Submission22701/Reviewer_5wrv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22701/Reviewer_5wrv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760464564095, "cdate": 1760464564095, "tmdate": 1762942348491, "mdate": 1762942348491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes WS-GRPO, a two-phase framework that enables group-relative policy optimization under weak supervision. Phase 1 trains a preference model using only trajectory-level outcomes. Phase 2 converts the learned preference signal into dense step-wise rewards by comparing consecutive partial trajectories, and mixes them with terminal rewards inside the GRPO objective. The authors also present a theory: preference consistency, policy robustness to preference error, and a generalization bound decomposing errors from both policy learning and preference modeling. Experiments on AI2-ARC and CommonsenseQA show “competitive” results and architecture sensitivity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Practical motivation**: turns sparse outcome labels into dense per-step feedback without costly process annotations, it’s useful when step-level PRM labels are unavailable.\n\n**Clear mechanism**: the “adjacent-prefix comparison” is implementable and easy to understand, which integrates cleanly with GRPO’s group-relative advantage.\n\n**Theoretical guarantees**: the paper provides preference-model consistency, linear-type robustness of the policy to preference errors, and a generalization bound, which together help justify moving from weak to dense supervision.\n\n**Problem fit**: the method directly targets a known pain point of GRPO; it benefits from dense process signals, while real data often provide only terminal outcomes."}, "weaknesses": {"value": "**Incremental novelty**: Pairing process rewards or PRMs with GRPO predates this work. For example, TP-GRPO[1] integrates a generative process reward model with GRPO, thereby injecting dense step-wise rewards into GRPO, and introduces a thought-level scheme and an off-policy pipeline.\n\n**Lack of experiments/models**: The paper evaluates only two reasoning datasets, which limits generality. Please broaden coverage to tasks that stress different skills: math (e.g., GSM8K, MATH, OlymMATH), tool-use (e.g., StableToolBench, ToolSANDBOX), and long-chain proofs. Model scope is also narrow, since experiments use only a 3B model. Add smaller and larger scales to show scalability and stability, for example, 1.5B and 7B from the Qwen2.5 family, and consider the latest Qwen3 variants where applicable.\n\n**Wording strength**: The abstract says GRPO “critically depends on dense step-wise rewards.” Literature supports “performs best / benefits from dense signals” rather than strict dependence; the claim should be softened or supported with broader evidence.\n\n**Hyperparameters**: Key hyperparameters are unspecified, and there is no ablation. Please report at least: (i) the preference-model encoder and head, (ii) the mixing weight between step-wise and terminal rewards, and (iii) GRPO group size and KL coefficient, then run small ablations on these. Without this, results are hard to reproduce and may reflect tuning rather than the method, which deep RL studies and reproducibility checklists caution about."}, "questions": {"value": "**Step segmentation**: How are “steps” defined in practice, lines or tokens? If you use textual heuristics, how robust are the results to the segmentation rule? Please provide a short ablation.\n\n**Trajectory-pair construction**: How do you form positive–negative trajectory pairs: number of samples K per prompt, handling ties (both correct or both wrong), duplicate removal, and class balancing across prompts? Please give some examples."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YE7iIOncAF", "forum": "rXma48njj6", "replyto": "rXma48njj6", "signatures": ["ICLR.cc/2026/Conference/Submission22701/Reviewer_vSVH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22701/Reviewer_vSVH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972660680, "cdate": 1761972660680, "tmdate": 1762942348102, "mdate": 1762942348102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WS-GRPO, a weakly-supervised extension of Group-Relative Policy Optimization. WS-GRPO trains a trajectory-preference model from sparse outcome labels, then treats consecutive partial trajectories as preference pairs to synthesize step-level rewards inside GRPO’s normalized advantage framework. The authors also provide theoretical analysis for WS-GRPO. On AI2-ARC and CommonsenseQA, WS-GRPO matches GRPO with zero step annotations, but performance is model/task-sensitive."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work frames step-level reward dependency as a weak-supervised problem, and uses a preference model to get step-wise signals without step labels.\n\n2. The writing is clear: a running example (Figure 1) visualizes how partial-trajectory pairs generate step rewards. The notations are consistent and easy to follow.\n\n3. The paper shows that trajectory-level outcome labels contain enough signal for consistent step-wise credit assignment."}, "weaknesses": {"value": "1. Empirical coverage is too narrow:\n  \n    (1) Only two datasets (ARC, CSQA) and only 3B-scale models are tested; no results on larger models or math reasoning benchmarks where GRPO is also popular.  \n    (2) Missing ablation on G (number of group roll-outs): GRPO’s variance reduction relies on large G; with G=8 the baseline itself may be under-powered, inflating WS-GRPO’s apparent competitiveness.  \n    (3) No comparison with dense PRM baselines (e.g., GRPO+PRM) — the paper claims to “approach” dense supervision but never measures the performance gap against the true upper-bound.\n\n2. Step-level credit assignment remains superficial:\n  \n    Equation (6) simply sums per-step preferences and then averages over length, so every step within a trajectory receives identical advantage, which is not fine-grained credit assignment. \n\n3. Sensitivity to preference quality is under-analyzed  \n\n   The preference model trained from sparse outcome labels may not perform well for step-level reward. The paper provides no analysis on failure cases: when preference model ranks wrong trajectories higher, WS-GRPO will wrongly amplifies their probability. \n\n4. Hyper-parameter fragility:\n \n    This work suppose fixed $\\lambda_1=1, \\lambda_2=5$ across both tasks and both models. No schedule or adjustments of parameters are explored."}, "questions": {"value": "Q1: Can you provide some results on math benchmarks (e.g., MATH) and on 7B-scale models? These are the domains where GRPO is widely applied.  \n \n   Q2: Pleasw include a dense-PRM baseline (e.g., GRPO + a small supervised PRM) so we can see how close WS-GRPO actually gets to the upper-bound as you mention in the paper.\n\n   Q3: Please report the accuracy of the preference model and correlate it with downstream policy performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h7nlekfVkw", "forum": "rXma48njj6", "replyto": "rXma48njj6", "signatures": ["ICLR.cc/2026/Conference/Submission22701/Reviewer_WXWE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22701/Reviewer_WXWE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983509244, "cdate": 1761983509244, "tmdate": 1762942346404, "mdate": 1762942346404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}