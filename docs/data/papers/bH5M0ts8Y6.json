{"id": "bH5M0ts8Y6", "number": 11669, "cdate": 1758202948803, "mdate": 1759897561952, "content": {"title": "VINCIE: Unlocking In-context Image Editing from Video", "abstract": "In-context image editing aims to modify images based on a contextual sequence comprising text and previously generated images. Existing methods typically depend on task-specific pipelines and expert models (e.g., segmentation and inpainting) to curate training data. In this work, we explore whether an in-context image editing model can be learned directly from videos. We introduce a scalable approach to annotate videos as interleaved multimodal sequences. \nTo effectively learn from this data, we design three proxy tasks: next-image prediction, current segmentation prediction, and next-segmentation prediction. \nAdditionally, we propose a novel multi-turn image editing benchmark to advance research in this area.\nExtensive experiments demonstrate that our model exhibits strong in-context image editing capabilities and achieves state-of-the-art results on two multi-turn image editing benchmarks. Despite being trained exclusively on videos, our model also shows promising abilities in multi-concept composition, story generation, and chain-of-editing applications.", "tldr": "We learn in-context image editing directly from videos using a block-wise causal diffusion transformer, achieving strong multi-turn image editing abilities.", "keywords": ["Image Editing", "Video Generation", "Diffusion Model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/49bbe96fc7658503a1a0b8a03e99daf23592be79.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes VINCIE, a video-driven approach to in-context image editing that learns multi-turn image editing directly from videos rather than paired images. In the method, videos are converted into interleaved multimodal sequences of sparsely sampled frames, VLM-written transition texts, and segmentation masks for regions of editing. The method leverages a DiT with block-wise causal attention trained on three proxy tasks: next-image prediction, current-segmentation prediction, and next-segmentation prediction. The authors also introduce MSE-Bench, a 5-turn editing benchmark, and report strong results on MSE-Bench and MagicBrush, with additional analyses on context, segmentation prediction, and data scaling."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed pipeline of training multi-turn image editing models from video data is novel and intuitively effective.\n2. The proposed MSE-Bench can benefit the image editing community and more properly evaluate multi-turn image editing models.\n3. VINCIE achieved strong multi-turn image editing performance compared to previous art."}, "weaknesses": {"value": "1. What are the training objectives for the segmentation prediction tasks NSP and CSP? Are they MSE or some other losses between the predicted and GT segmentation masks?\n2. Can the author provide some intuition on why adding the next segmentation prediction task during inference benefits MagicBruch scores but damages MSE-Bench scores (compared to CS->I)?\n3. From Table 4, it seems that the Dummy context is more effective than providing the editing history to the model. Is this the correct interpretation of this table? Does that mean the Dummy context itself is enough for multi-turn image editing?\n4. Based on Section 4.4, the camera movement and subject position shift problems can be solved by incorporating the segmentation prediction task during inference. Can the author elaborate more on why this prediction task helps stabilize the movement of the subjects in the image? Is it because the predicted segmentation mask always tends to be the same mask as in the previous editing round?\n5. The paper mentioned both full attention and block-wise causal attention in the method section. Which one is the method that is used in the final version of VINCIE? From Section 1, it seems that block-wise causal attention is actually used in VINCIE. If this is the case, what is the point of mentioning the full attention design? Or does the model support both attention mechanisms (across different editing turns) natively?\n6. I would suggest adding some description on how the model is used during inference, especially on the different modes that the model can be used, since the model accepts a wide variety of input conditions and these inputs can be quite flexible. Right now it is a bit confusing on how the model operates during inference: for example, when adding the segmentation mask prediction task during inference, will the predicted segmentation mask be used as part of the input condition to the model, or will it only be predicted, but not used during the next edited image prediction?\n\nOverall, I find the method and results to be promising, but I believe the current paper's presentation can be improved. I would be happy to raise my score if the concerns can be addressed."}, "questions": {"value": "See weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Df6rn4S77k", "forum": "bH5M0ts8Y6", "replyto": "bH5M0ts8Y6", "signatures": ["ICLR.cc/2026/Conference/Submission11669/Reviewer_6Gue"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11669/Reviewer_6Gue"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713952801, "cdate": 1761713952801, "tmdate": 1762922723699, "mdate": 1762922723699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "VINCIE introduces a novel approach to multi-turn image editing by directly leveraging video data. It treats sequential video frames and their natural transitions as in-context editing demonstrations, and designed three proxy tasks: next-image predic-\n047 tion, current segmentation prediction, and next-segmentation prediction to tickle this problem. Using a diffusion transformer architecture guided by predicted segmentation masks, VINCIE learns to localize and apply edits step-by-step in a context-aware manner without needing manually curated editing datasets. The method shows strong performance on new and established multi-turn editing benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1) This paper proposed a new perspective in constructing session-wise data with long, interleaved image-text context from native videos while prior works that used video for editing were mainly for constructing pair-wise data. This highlights the originality.\n\nS2) At first glance the designed approach is best suited for videos with a static viewpoint. When the camera moves, the region proposal and segmentation process can break down, leading to inaccurate edits or mask predictions. Large background or subject position shifts caused by camera movement are not explicitly addressed. But it is clever approach that predicting segmentation masks before editing could mitigates subject drift.\n\nS3) The paper is well written, with good motivation, and communicated clearly. Overall I enjoyed reading this work."}, "weaknesses": {"value": "W1) I find it surprising that the paper identifies MagicBrush (NeurIPS 2023) as the last major benchmark for multi-turn image editing, with no subsequent progress. Based on my recollection, ImgEdit-Bench (May 2025) also introduced evaluation protocols on multi-turn image editing. Including results on the ImgEdit-Bench benchmark would make the empirical comparison much more convincing."}, "questions": {"value": "Q1) In Visual Transition Annotation, authors leveraged VLM for captioning. While it makes sense to me to use VLM for generating detailed and coherent descriptions of each frame from multiple aspects, I am concerned of using VLMs to identify semantic and visual differences\nbetween the two frames when the two frames have minor differences. It is known that VLMs often fails in telling subtle difference between two images, as pointed out in The Dawn of LMMs and VIEScore. I question the robustness of this approach for fine-grained visual transition annotation, as critical details might be overlooked or misrepresented when using these models. Could the authors provide a case study or qualitative results focusing on more fine-grained edits (e.g. moving an object slightly to one direction, like only 1cm)?\n\nQ2) Regarding Regions-of-Editing, GroundingDINO and SAM2 may propagate errors in mask generation or transition description, especially in complex or cluttered scenes. Any measures are conducted to minimize the error impact in this pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aez0XNjRda", "forum": "bH5M0ts8Y6", "replyto": "bH5M0ts8Y6", "signatures": ["ICLR.cc/2026/Conference/Submission11669/Reviewer_yqUb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11669/Reviewer_yqUb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802591414, "cdate": 1761802591414, "tmdate": 1762922723403, "mdate": 1762922723403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VINCIE, a method for learning in-context image editing directly from video data. The key innovation is constructing training data by: (1) sampling frames from videos, (2) using VLMs to annotate visual transitions between frames, (3) generating segmentation masks for regions of interest using GroundingDINO and SAM2, and (4) training a diffusion transformer with three proxy tasks (next-image prediction, current segmentation prediction, next segmentation prediction). The authors introduce MSE-Bench, a challenging 5-turn editing benchmark, and demonstrate competitive performance against existing methods while showing scalability with increasing data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-structured with clear figures (Figures 1-3 effectively convey main ideas)\n2. Comprehensive appendix with implementation details\n3. Three proxy tasks (NIP, CSP, NSP) provide complementary learning signals\n4. Comprehensive ablation studies validate design decisions"}, "weaknesses": {"value": "1. The claim of learning from \"native videos\" is misleadingâ€”the method requires extensive preprocessing with VLMs, GroundingDINO, and SAM2. This is annotation-based learning, not purely native video learning\n2. Evaluation relies heavily on GPT-4o as judge, which may introduce bias despite correlation analysis (Table 7)\n3. Core technical novelty is limited: DiT architecture, segmentation prediction, and in-context learning are established techniques. The main contribution is the data construction pipeline\n4. Heavy reliance on existing models (VLM, GroundingDINO, SAM2) reduces the \"learning from video\" novelty\n5. Fair comparison at same dataset?"}, "questions": {"value": "see the Weaknesses.  Overall, I would not object to this paper being accepted."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ezp2C3AhC2", "forum": "bH5M0ts8Y6", "replyto": "bH5M0ts8Y6", "signatures": ["ICLR.cc/2026/Conference/Submission11669/Reviewer_bA7V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11669/Reviewer_bA7V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877903887, "cdate": 1761877903887, "tmdate": 1762922722907, "mdate": 1762922722907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VINCIE, a framework for in-context image editing using video data. The core premise is to learn this capability solely from native video data, bypassing the need for curated, paired image-editing datasets. The authors propose a scalable data pipeline that samples video frames and uses a Vision-Language Model to generate textual annotations of the visual transitions between them. It also generates segmentation masks for these \"Regions of Editing\" (RoEs). The model, a diffusion transformer, is trained using three proxy tasks: Next-Image Prediction (NIP), Current Segmentation Prediction (CSP), and Next-Segmentation Prediction (NSP). To evaluate their method, the authors introduce a new multi-turn benchmark, MSE-Bench, which consists of 5-turn editing sessions and is evaluated using GPT-4o. The results demonstrate that VINCIE achieves strong performance, outperforming many academic methods on MagicBrush and their new benchmark, and that the approach scales well with data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The central idea of using native video data to learn in-context editing makes sense. It cleverly reframes the problem, identifying videos as a natural, abundant, and scalable source of \"edit\" data (i.e., state transitions) that intrinsically contains the visual consistency and temporal context missing from static image-pair datasets. \n\n2. The data construction pipeline is sound, combining a VLM for high-level semantic transition annotation with grounding models (GroundingDINO + SAM2) for precise, low-level spatial localization (RoEs). The three proxy tasks (NIP, CSP, NSP) are well-motivated; using segmentation prediction not only as a training signal (CSP) but also as a predictive target (NSP) is a smart way to improve grounding and controllable generation.\n\n3. The paper is well-written, and the methodology is presented clearly. The figures are effective at illustrating the data pipeline and model architecture."}, "weaknesses": {"value": "1. The paper defines in-context editing as modifying an image based on a \"contextual sequence comprising text and previously generated images.\" This framing is functionally equivalent to multi-turn editing, where the primary role of the context is to serve as the input for the next step. This definition is quite narrow and overlooks a more common interpretation of \"in-context\" for image models: the ability to use one or more reference images in the context to provide new subjects, styles, or concepts for the current edit (e.g., \"add the dog from the second image with the style from the 4th image into this one\"). The paper's experiments all fit the multi-turn definition, and it's unclear if VINCIE can perform this more complex, composition-focused \"reference-based\" in-context editing.\n\n2. The evaluation, while thorough on MagicBrush and the proposed MSE-Bench, feels incomplete. It would be much more convincing to see a comparison on more recent benchmarks, such as GEdit-Bench and also evaluate on the EMUEdit benchmark? \n\n3. The primary evaluation for the newly proposed MSE-Bench relies on a \"success rate\" computed by GPT-4o. This is a significant concern. The reliability of current VLMs for nuanced, multi-step comparative judgment is questionable. The appendix suggests multiple images are fed simultaneously, which could easily confuse the model. My own experience suggests GPT-4o can struggle to correctly identify subtle edits or preserve consistency even in simple source-vs-target comparisons, let alone a complex 5-turn sequence where errors propagate. This evaluation method needs much stronger validation.\nAlso, for automatic evaluation, it would be beneficial to also report scores like the PF (Prompt following) and SC (semantic consistency) from recent work like VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation. These are designed for image editing and might offer a more reliable supplement to the GPT-4o-based success rate."}, "questions": {"value": "Natural videos excel at capturing state transitions like object addition/removal, pose changes, and motion. However, they rarely, contain examples of more \"creative\" or \"unrealistic\" edits that are common user requests. For example, drastic environmental changes (e.g., \"change the weather from snow to summer\"), complex style transfers (e.g., \"make this look like a Picasso painting\"), or material transformations (e.g., \"make the person out of glass\"). It is questionable whether a model trained only on natural video can generalize to these common editing tasks. I do agree that video data could be used in the pretraining, but using video data for image editing is not a novel concept. The paper's own related work section cites several recent methods that also \"Learn from Video for Image Generation.\" The paper differentiates itself by using \"long, interleaved image-text context\" rather than just frame pairs, but this feels more like a strong incremental improvement than a paradigm shift."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NskZ7ze7SK", "forum": "bH5M0ts8Y6", "replyto": "bH5M0ts8Y6", "signatures": ["ICLR.cc/2026/Conference/Submission11669/Reviewer_wj9T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11669/Reviewer_wj9T"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970562923, "cdate": 1761970562923, "tmdate": 1762922722563, "mdate": 1762922722563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VINCIE, a framework for in-context image editing using video data. The core premise is to learn this capability solely from native video data, bypassing the need for paired image-editing datasets. The authors propose a scalable data pipeline that samples video frames and uses a Vision-Language Model to generate textual annotations of the visual transitions between them. It also generates segmentation masks for these \"Regions of Editing. The final model is trained using three proxy tasks: Next-Image Prediction, Current Segmentation Prediction, and Next-Segmentation Prediction. To evaluate their method, the authors also introduce a new multi-turn benchmark, MSE-Bench, which consists of 5-turn editing sessions and is evaluated using GPT-4o. The results demonstrate that VINCIE achieves strong performance, outperforming many academic methods on MagicBrush and their new benchmark, and that the approach scales well with data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of using video data to learn in-context editing makes sense. It identifies videos as a scalable source of \"edit\" data (state transitions) that intrinsically contains the visual consistency and temporal context.\n\n2. The data construction pipeline is sound, combining a VLM for high-level semantic transition annotation with grounding models (GroundingDINO + SAM2) for precise, low-level spatial localization. The three proxy tasks are well-motivated. And using segmentation prediction not only as a training signal but also as a predictive target is a smart way to improve grounding and controllable generation.\n\n3. The paper is well-written, and the methodology is presented clearly. The figures are effective at illustrating the data pipeline and model architecture."}, "weaknesses": {"value": "1. The paper defines in-context editing as modifying an image based on a \"contextual sequence comprising text and previously generated images.\" This framing is functionally equivalent to multi-turn editing, where the primary role of the context is to serve as the input for the next step. This definition is quite narrow and overlooks a more common interpretation of \"in-context\" for image models: the ability to use one or more reference images in the context to provide new subjects, styles, or concepts for the current edit (e.g., \"add the dog from the second image with the style from the 4th image into this one\"). The paper's experiments all fit the multi-turn definition, and it's unclear if VINCIE can perform this more complex, composition-focused \"reference-based\" in-context editing.\n\n2. The evaluation only on MagicBrush[1] and the proposed MSE-Bench, feels incomplete. It would be much more convincing to see a comparison on more recent benchmarks, such as GEdit-Bench[2] and also evaluate on the EMUEdit[3] benchmark? \n\n3. The primary evaluation for the newly proposed MSE-Bench relies on a \"success rate\" computed by GPT-4o. This is a significant concern. The reliability of current VLMs for nuanced, multi-step comparative judgment is questionable. The appendix suggests multiple images are fed simultaneously, which could easily confuse the model. My own experience suggests GPT-4o can struggle to correctly identify subtle edits or preserve consistency even in simple source-vs-target comparisons, let alone a complex 5-turn sequence where errors propagate. This evaluation method needs much stronger validation.\nAlso, for automatic evaluation, it would be beneficial to also report scores like the PF (Prompt following) and SC (semantic consistency) from recent work like VIEScore[4]. These are designed for image editing and might offer a more reliable supplement to the success rate.\n\nThe presentation of the submission can be improved. I would be willing to raise my score if the authors address the concerns above\n\n[1] MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing\n\n[2] Step1X-Edit: A Practical Framework for General Image Editing\n\n[3] Emu Edit: Precise Image Editing via Recognition and Generation Tasks\n\n[4] VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation"}, "questions": {"value": "Natural videos excel at capturing state transitions like object addition/removal, pose changes, and motion. However, they rarely, contain examples of more \"creative\" or \"unrealistic\" edits that are common user requests. For example, drastic environmental changes (e.g., \"change the weather from snow to summer\"), complex style transfers (e.g., \"make this look like a Picasso painting\"), or material transformations (e.g., \"make the person out of glass\"). It is questionable whether a model trained only on natural video can generalize to these common editing tasks. I do agree that video data could be used in the pretraining, but using video data for image editing is not a novel concept. The paper's own related work section cites several recent methods that also \"Learn from Video for Image Generation.\" The paper differentiates itself by using \"long, interleaved image-text context\" rather than just frame pairs, but this feels more like a strong incremental improvement than a paradigm shift."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NskZ7ze7SK", "forum": "bH5M0ts8Y6", "replyto": "bH5M0ts8Y6", "signatures": ["ICLR.cc/2026/Conference/Submission11669/Reviewer_wj9T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11669/Reviewer_wj9T"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970562923, "cdate": 1761970562923, "tmdate": 1763650363727, "mdate": 1763650363727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}