{"id": "kzGkXpW4FT", "number": 20493, "cdate": 1758306771770, "mdate": 1759896974754, "content": {"title": "Identifying Robust Neural Pathways: Few-Shot Adversarial Mask Tuning for Vision-Language Models", "abstract": "Recent vision-language models (VLMs), such as CLIP, have demonstrated remarkable transferability across a wide range of downstream tasks by effectively leveraging the joint text–image embedding space, even with only a few data samples. Despite their impressive performance, these models remain vulnerable to adversarial attacks, raising significant concerns about their security and reliability in practical deployments. To address this issue, we propose Adversarial Mask Tuning (AdvMask), a method that effectively enhances the robustness of VLMs without directly modifying their pre-trained weights. Instead, our AdvMask learns a set of binary masks that selectively deactivate model parameters vulnerable to adversarial perturbations. By identifying robust neural pathways within the vision encoder, AdvMask facilitates the generation of features and predictions that are resistant to adversarial attacks. Furthermore, we introduce a Layer-wise Adaptive Feature Alignment (LAFA) loss, specifically designed to optimize AdvMask in few-shot scenarios. The LAFA loss adaptively aligns intermediate-layer features from clean and adversarial samples across each transformer block, enhancing the representational robustness of the model. Experimental results across multiple benchmarks confirm that our AdvMask approach substantially outperforms existing adversarial tuning techniques for VLMs, especially in few-shot settings.", "tldr": "To enhance the robustness of VLMs, we propose Adversarial Mask Tuning (AdvMask), a method that learns a set of binary masks that selectively deactivate model parameters vulnerable to adversarial perturbations.", "keywords": ["Vision-Language Models (VLMs)", "Adversarial Robustness", "Mask Tuning", "Robust Neural Pathways"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c1b0c81cfce82fa0495271d8b0cfc438d10edb8.pdf", "supplementary_material": "/attachment/2adf542460c8e459e7684b16127860ae2f161bd1.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces AdvMask to improve the adversarial robustness of in few-shot settings. Instead of fine-tuning model weights, it learns binary masks to identify and activate robust neural pathways, preserving stable features under attack. The proposed LAFA loss further enhances robustness by aligning intermediate features between clean and adversarial samples."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The setting of the task of this work is clear. The convert from Mask Tuning to Adv Mask Tuning is interesting and theoretical.\n\n2. The method framework is clearly introduced, while using visualization to help readers quickly understand the method.\n\n3. The paper provides sufficient experimental evidence and further insight."}, "weaknesses": {"value": "1. This work involves two-shot scenarios: few-shot training and zero-shot evaluation. The authors are advised to clearly explain and distinguish these in the introduction to facilitate understanding. For example, the phrase \"overfitting in a few-shot setting\" on line 54 and \"zero-shot robustness\" on line 55 may not be aligned settings, but their use together could easily lead to misunderstanding and confusion.\n\n2. In the ablations Table 17, Table 18, etc., the 16-shot performances of \"47.1\" and \"47.3\" seem to be different from the \"41.99\" given in Table 10. How are they obtained?\n\n3. If the author's training data uses 3.2% of the data, then is the training time also reduced to 3.2% compared to TGA-ZSR? I didn't see a direct comparison in the paper."}, "questions": {"value": "1. Are there any visualization or statistical results that show the specific situation of the final mask, and can we summarize in a regular way which weights are more important for adversarial and which need to be ignored?\n\n2. I'm curious if this few-shot approach would be applicable to the scenarios tuned for LVLM in FARE? Are there any challenges in doing so?\n\n[1] Schlarmann C, Singh N D, Croce F, et al. Robust clip: Unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models[J]. arXiv preprint arXiv:2402.12336, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8Ff0mSPu5v", "forum": "kzGkXpW4FT", "replyto": "kzGkXpW4FT", "signatures": ["ICLR.cc/2026/Conference/Submission20493/Reviewer_SmjY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20493/Reviewer_SmjY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761399812737, "cdate": 1761399812737, "tmdate": 1762933927515, "mdate": 1762933927515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates an interesting adversarial defense mechanism without explicitly tuning the model weights; instead, the proposed method optimizes a model weight-level mask across different layers to enhance robustness. In other words, the proposed method shields potentially vulnerable weights to adversarial robustness. Furthermore, the paper introduced a layer-wise adaptive feature alignment scheme to optimize such a model weight-level mask by aligning clean features and their adversarial counterparts in the few-shot setup. Experiments across diverse datasets and scenarios demonstrate the generalization capability of the proposed method. Further ablations justifies the efficacy of the design of the proposed pathway method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed idea is interesting. Instead of tuning the whole weight map of VLMs to improve robustness, the paper explores an alternative way by exploring the weight-level mask to remove some implicitly vulnerable model weights against adversarial attacks.\n2. The paper is well-written and organized. A detailed recap of previous works and a background introduction are given.\n3. Extensive experiments across diverse benchmarks and scenarios are provided. In addition, a series of ablation analyses is given to verify the effectiveness of each module. Insights in Figure 4 are also interesting."}, "weaknesses": {"value": "1. The proposed method might not be novel in the context of adversarial learning (for both single-modal and multimodal architectures). [a] has already explored the model weights connected with adversarial robustness. Although [a] is based on a single-modal architecture, its idea can also be easily extended to a multimodal backbone.\n\n2. The evaluated adversarial attacks are primarily low-intensity (low perturbation radius) attacks with eps=1/255. It's questionable that if the proposed method also exhibits robustness against stronger adversarial attacks with higher eps (e.g., 4/255, 8/255)\n\n3. Can the mask be regarded as part of the weights of VLMs? If so, I think that finetuning the VLM weights can also achieve the same effect, the mask would be mostly like some scailing of the standard adversarial finetuning, which means the proposed mask is an indirect adversarial finetuning. Then, it should achieve similar performance compared with adversarial finetunin.\n\n4. It seems that the mask is only for the image encoder. But the paper focuses on VLMs. In this case, it would be more appropriate to consider both branches, otherwise the work would be mostly similar to single-modal adversarial learning works.\n\n[a] Improving Generalization of Adversarial Training via Robust Critical Fine-Tuning (ICCV 2023)"}, "questions": {"value": "1. Can the authors evaluate the text-level (BERT-Attack) or joint image-text-level attacks (CO-Attack) in addition to image-level attacks only?\n\n2. In addition to image classification, VLMs are powerful in diverse visual-language tasks, e.g., image captioning, Visual question answering. Can the authors test some of them instead of classification?\n\n3. Can the proposed mask be a soft format instead of the 0-1 style?\n\n[b] BERT-ATTACK: Adversarial Attack Against BERT Using BERT (EMNLP-2020) \n\n[c] Towards Adversarial Attack on Vision-Language Pre-training Models (ACMMM 2022)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WpOCYBONyj", "forum": "kzGkXpW4FT", "replyto": "kzGkXpW4FT", "signatures": ["ICLR.cc/2026/Conference/Submission20493/Reviewer_kPP8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20493/Reviewer_kPP8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830527321, "cdate": 1761830527321, "tmdate": 1762933926436, "mdate": 1762933926436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AdvMask, a method to enhance adversarial robustness of Vision-Language Models (VLMs) in few-shot settings. Instead of modifying pre-trained weights or learning prompts, AdvMask learns binary masks that selectively deactivate parameters vulnerable to adversarial perturbations, effectively identifying robust neural pathways within the vision encoder. The authors introduce a new loss (LAFA) that aligns intermediate features between clean and adversarial samples with confidence-based weighting. Experiments on 11 datasets show AdvMask outperforms prompt-based baselines (AdvVP, AdvVLP, AdvMaPLe, FAP) in few-shot adversarial robustness while maintaining competitive clean accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": ". The concept of finding robust neural pathways via binary masks is creative and well-motivated.\n\n.The framing as deactivating vulnerable parameters rather than adding robust features is a fresh perspective on adversarial defense.\n\n. The experimental evaluation is thorough.\n\n. Substantial improvements over baselines across most datasets. \n\n.Clean accuracy recovers with more shots."}, "weaknesses": {"value": ". The model is trained with only 2 steps of PGD at a very small noise level (ε = 1/255) and tested mostly at the same level. This is weak to prove robustness and may hide gradient masking. The authors should test with stronger attacks (for example ε = 4/255) to make sure the binary mask and straight-through estimator do not block gradients.\n\n. The adaptive weight in LAFA uses the model’s own prediction confidence. At early stages, this confidence can be wrong, which may make the model ignore “hard but useful” samples. The authors could try using a teacher model or stop the gradient from this weight to prevent bias.\n\n. The paper claims that certain layers or heads are more robust, but there is no clear visualization. It would help to show which layers or attention heads are most often masked and whether this pattern is consistent across datasets or random seeds.\n\n. The paper argues that full fine-tuning overfits in few-shot cases, but this is not shown. A simple 16-shot full fine-tuning baseline would make this claim stronger.\n\n. The paper says the method is efficient, but Table 19 shows higher inference memory than some baselines. A simple memory breakdown would help clarify this.\n\n. Modern papers show that some defenses look strong until the attack is adapted to the defense itself. Here, the main defensive component is the mask and LAFA, so it would be good to test with attacks that target them directly."}, "questions": {"value": "1. Can you show a baseline where the model is fully fine-tuned with 16 samples per class?\n\n2. Which layers or heads are masked the most? Are these patterns stable across runs?\n\n3. Why did you choose the mask initialization values?\n\n4. Where does the method fail? For example, why is performance lower on Cars, Food101, and Aircraft datasets?\n\n5. Why is the inference memory higher than other lightweight methods?\n\n6. How does sparsity (number of masked weights) relate to robustness?\n\n7. Can you add stronger attacks (ε = 4/255)?\n\n8. Have you tested an adaptive attack that specifically targets the mask or LAFA?\n\n\n---- Additional Suggestions\n\n. Add a baseline where the mask is trained without adversarial samples to see if adversarial tuning is truly necessary.\n\n. Show how the mask changes during training (sparsity per epoch).\n\n. Include a small table showing scaling to a larger backbone such as ViT-L/14.\n\n. Report per-class accuracy to show which categories benefit the most."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rKGMHZEPDn", "forum": "kzGkXpW4FT", "replyto": "kzGkXpW4FT", "signatures": ["ICLR.cc/2026/Conference/Submission20493/Reviewer_RUtB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20493/Reviewer_RUtB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920552100, "cdate": 1761920552100, "tmdate": 1762933925861, "mdate": 1762933925861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an adversarial mask tuning (AdvMask) approach that searches for robust subnetwork within well-trained VLMs as a promising alternative which is highly parameter-efficient and is trained with Layer-wise Adaptive Feature Alignment (LAFA) loss. Experiments across various downstream datasets demonstrate that AdvMask consistently improves few-shot adversarial robustness over existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important problem of improving adversarial robustness in vision-language models (VLMs) with few-shot learning. The motivation is well-explained, and the paper is well-organized and clearly written. In figure 2, the authors provide a clear illustration of the proposed AdvMask method with different shots, which helps a lot in understanding the performance of the approach."}, "weaknesses": {"value": "1. The proposed method is strongly related to adversarial model pruning, which has been extensively studied in the literature (arxiv.org/pdf/2409.01249). Therefore, the novelty of the proposed method is limited. The authors should also consider comparing with these adversarial pruning methods.\n\n2. The improvement of the proposed method is not significant enough. From Table 4, adding AdvMask only improves around 0.7% robust accuracy and 1.5 % clean accuracy compared to directly using typical adversarial training.\n\n3. The experimental settings are not strong enough. The authors should evaluate the performance of the proposed method with more adversarial settings, e.g., epsilon=2/155 or 4/255. See questions."}, "questions": {"value": "1. Why is this method particularly effective for few-shot learning? What if using more training data?\n\n2. Since the model parameters are changed after masking, does the author adopt adaptive attacks (e.g., PGD attack with knowledge of the mask) during evaluation? If not, the evaluation is not fair.\n\n3. Can this method be better than existing zero-shot robust methods? e.g., simply adversarially train the entire model with one specific dataset like ImageNet?\n\n4. Does this mask have any interpretability? For example, are they similar for different datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XW4uBaGaRR", "forum": "kzGkXpW4FT", "replyto": "kzGkXpW4FT", "signatures": ["ICLR.cc/2026/Conference/Submission20493/Reviewer_LtP8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20493/Reviewer_LtP8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987659487, "cdate": 1761987659487, "tmdate": 1762933925422, "mdate": 1762933925422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}