{"id": "8HvWBamUkS", "number": 11782, "cdate": 1758203785588, "mdate": 1759897555061, "content": {"title": "Adaptive Curriculum Learning for RLHF with Influence-Based Cluster Bandits", "abstract": "Reinforcement learning (RL) plays a central role in post-training large language models (LLMs). Yet, existing RLHF pipelines typically rely on fixed or uniform sampling strategies, which fail to adapt to the model’s evolving learning state. This mismatch leads to wasted computation on less informative samples while neglecting instances with higher training impact, ultimately limiting efficiency, generalization, and performance gains.\nWe introduce an adaptive curriculum learning framework that integrates influence-based clustering with a multi-armed bandit (MAB) scheduler. Training data are partitioned into clusters defined by semantic and difficulty-related features, each treated as an arm in the MAB formulation. A Cluster Score (CS), updated via sliding-window influence functions, quantifies the dynamic importance of each cluster as the model evolves. This adaptive scoring drives the scheduler to balance exploitation of high-impact clusters with exploration of underrepresented regions, ensuring efficient learning while maintaining diversity. Unlike prior approaches that overfit to narrow high-reward subsets, our cluster-level sampling prevents redundancy and broadens representational coverage.\nExperiments with Group Relative Policy Optimization across mathematical reasoning benchmarks show that our method consistently accelerates convergence and improves generalization. These results highlight the value of distribution-level adaptive curricula in advancing RLHF for LLM training.", "tldr": "", "keywords": ["RLHF", "Curriculum Learning", "GRPO"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a63d30dc7c551ac23dc0be06a442b0f9f2ebf89.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an adaptive, distribution-level curriculum learning framework for RLHF that clusters training data by semantic, readability, lexical/syntactic, and difficulty features; each cluster is treated as a bandit arm. It estimates each cluster’s learning value with an influence-based score—approximating “inverse curvature × loss-gradient” via conjugate gradient, aggregated over a sliding window to track the model’s evolving state. A UCB scheduler then balances exploitation of high-impact clusters and exploration of under-sampled ones when selecting batches. On math-reasoning benchmarks (GSM8K, MATH, AIME24, MATH500) with small Llama/Qwen models, the method shows faster, more stable convergence and higher final performance than Random, AdaRFT, and DUMP; ablations further indicate influence scoring outperforms advantage and perplexity proxies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Commendable originality. The paper moves curriculum learning from single examples to data distributions. It first clusters the data by meaning, wording/readability, and difficulty, and treats each cluster as a bandit arm. During training it keeps a sliding window influence score that estimates how much more the model would benefit if we sample from that cluster now, and a UCB scheduler picks batches by favoring high-impact clusters while still exploring ones that are under-sampled. This setup cuts redundancy and noise from item-level picking, keeps good coverage of the data space, and adapts as the model changes, so compute is spent where it helps most and training becomes more sample-efficient and stable.\n2. Technically sound and scalable. The influence estimate explicitly approximates inverse-Hessian × loss-gradient via conjugate gradient with Hessian-vector products. And the paper discusses memory/compute cost and notes rapid CG convergence, plus cluster-level aggregation to keep it practical at LLM scale.\n3. Ablations support the mechanism. Replacing influence with advantage or perplexity degrades performance (especially on harder sets), indicating the influence-guided cluster scoring is the causal driver of the gains rather than incidental tuning.\n4. The baseline selection is solid. AdaRFT and DUMP are two strong, recent frameworks for RL-based LLM post-training with adaptive sampling methods. The comparisons are fair and the gains convincing."}, "weaknesses": {"value": "1. Experiments are only on mathematical reasoning benchmarks. There’s no non-math task. That makes it hard to judge how well the curriculum travels to noisier, subjective RLHF settings. A non-math benchmark (e.g., coding or instruction-following with a reward model) would help.\n2. Small models only. Results are limited to Llama-3.2-1B/3B and Qwen-2.5-1.5B. Without runs at ≥7B, we can’t tell if the method still helps on big models, or if its overhead outweighs the benefits."}, "questions": {"value": "1. Can you run at least one non-math or preference/RM-based task (e.g., instruction following with a reward model or pairwise preferences) to test whether the curriculum still helps under noisy/subjective rewards? Please report the same curves and final metrics you use for math.\n2. Your experiments use 1B–3B and 1.5B models only. I understand that re-training 7B or 13B models during the rebuttal may not be feasible, but could you explain how the influence computation and scheduling would scale in theory or practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yYTceA4okS", "forum": "8HvWBamUkS", "replyto": "8HvWBamUkS", "signatures": ["ICLR.cc/2026/Conference/Submission11782/Reviewer_TEci"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11782/Reviewer_TEci"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761218958459, "cdate": 1761218958459, "tmdate": 1762922808239, "mdate": 1762922808239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel curriculum learning framework for RLHF. The proposed approach clusters samples in the training data and performs adaptive selection and influence function estimation at the cluster level. By introducing a dual-level data selection mechanism, i.e., first choosing a cluster, then selecting samples within it, the framework achieves a balanced trade-off between exploration and exploitation in reinforcement learning. Moreover, this hierarchical design mitigates the computational overhead typically associated with curriculum strategies while maintaining the performance gains of curriculum learning. The method is evaluated within the GRPO framework across several mathematical reasoning benchmarks, using base models ranging from 1B to 3B parameters. Experimental results demonstrate consistent performance improvements, highlighting the general effectiveness of the proposed approach for RLHF fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well organized and clearly presented, making it easy to follow.\n- The proposed approach is conceptually intuitive and demonstrates strong practical potential."}, "weaknesses": {"value": "The primary concern lies in the empirical evaluation. The current range of base models is too limited to provide sufficient insight into the scalability and effectiveness of the proposed method. While it is generally unnecessary to conduct experiments on very large models, including a 7B-scale model is essential to substantiate the contribution of the proposed approach. Larger models are expected to generate higher-quality trajectories compared to smaller ones and are more robust against challenging samples, which implies that smaller models may benefit more from the proposed cluster-level curriculum sampling, whereas larger models may not exhibit the same degree of improvement. Furthermore, the results presented in Table 1 indicate that the Random strategy already serves as a relatively competitive baseline. Therefore, incorporating an additional experiment using a 7B-scale model would help clarify these observations and strengthen the empirical evidence supporting the proposed method."}, "questions": {"value": "The movement strategy of the sliding window within a cluster is not clearly described. Equation (2) indicates that the \"last k\" samples are included in the sliding window, but the precise meaning of \"last k\" remains ambiguous. Additionally, the procedure for selecting a batch of samples from the sliding window could be further clarified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lptRXGclYD", "forum": "8HvWBamUkS", "replyto": "8HvWBamUkS", "signatures": ["ICLR.cc/2026/Conference/Submission11782/Reviewer_q3ME"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11782/Reviewer_q3ME"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827570528, "cdate": 1761827570528, "tmdate": 1762922807643, "mdate": 1762922807643, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses inefficiencies in reinforcement learning from human feedback (RLHF) pipelines for large language models (LLMs), where fixed or uniform data sampling ignores the model’s evolving learning state. The authors propose an adaptive curriculum learning framework that integrates influence-function analysis with a multi-armed bandit (MAB) scheduler to guide dynamic data selection.\n\nConcretely, the training data are clustered by semantic and difficulty features; each cluster acts as a “bandit arm.” Using sliding-window influence scores approximated by Conjugate Gradient Hessian-vector products, the framework estimates each cluster’s training utility in real time. A UCB-based bandit then balances exploitation of high-impact clusters with exploration of underrepresented ones.\n\nThe system is implemented on Group Relative Policy Optimization (GRPO) and evaluated on mathematical reasoning datasets (GSM8K, MATH, AIME24, MATH500). Results show faster convergence and improved accuracy (e.g., +56.8% relative gain on AIME24) over strong baselines such as AdaRFT and DUMP. Ablations confirm that influence-based scoring yields more consistent gains than advantage- or perplexity-based sampling."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Clear motivation and scope: The inefficiency of static sampling in RLHF is well-articulated and timely, especially as LLM post-training scales.\n* Well-motivated system: each component in the system is well-motivated and demonstrated sufficient effort in integrating into LLM post training domain. This includes carefully designed features for clustering the dataset. The use of conjugate gradient as a surrogate for influence measure. Also, the non-stationary bandit arm formulation is sound.\n* Empirical strength: Consistent accuracy gains and faster convergence across multiple datasets and model sizes (Llama3.2-1B/3B, Qwen2.5-1.5B)."}, "weaknesses": {"value": "* Moderate novelty: The core innovation is architectural composition rather than new RL or CL theory. Influence functions and UCB are standard tools.\n* Potential computational overhead: Although CG approximation reduces cost, runtime analysis relative to baseline RLHF pipelines is not quantified. The paper also mentions a validation loss, but details about how the validation set is constructed is not provided.\n*  Dependence on feature engineering: Clustering relies on handcrafted lexical, syntactic, and semantic features; the framework may require tuning for other domains."}, "questions": {"value": "* How is the validation set constructed for evaluating the CG?\n* How sensitive is the result to the clustering results? Especially, what are the amount of engineering effort to construct the clusters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VGQLKMfkNr", "forum": "8HvWBamUkS", "replyto": "8HvWBamUkS", "signatures": ["ICLR.cc/2026/Conference/Submission11782/Reviewer_A9MV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11782/Reviewer_A9MV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960927664, "cdate": 1761960927664, "tmdate": 1762922806889, "mdate": 1762922806889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an adaptive curriculum learning framework for RLHF that dynamically identifies and samples the most useful training data as the model evolves. Training samples are grouped into clusters based on semantic and difficulty-related features, each treated as an arm in an MAB setup. A cluster score derived from influence functions computed over a sliding training window, quantifies each cluster's contribution to recent parameter updates. This influence score acts as the reward in an UCB policy, allowing the scheduler to balance exploitation of high-impact clusters with exploration of underrepresented regions. The result is a distribution-level adaptive curriculum that improves sample efficiency and enhances generalisation in RLHF training for large language models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Tackle an important problem of model-aware adaptive curriculum\n* This paper redefines and operationalises influence functions in RLHF, where none of the classical assumptions hold.\n* Treating influence as a reward signal to drive adaptive data selection policy."}, "weaknesses": {"value": "* The reward (influence score) is inherently stochastic and non-stationary (since the model keeps changing), which challenges the standard assumptions of UCB-based bandit algorithms.\n*  The clustering step which defines the arms of the bandit introduces an arbitrary, heuristic component that can strongly affect outcomes. The adaptive bandit may optimise efficiently within a poorly chosen partition, but not over the actual underlying data utility distribution. But how these clusters are formed i.e., how many there are and how features are weighted is manually chosen, not learned or validated in a principled way. \n* The clustering space is built from proxies of difficulty and semantics, not from model-internal or policy-driven features that truly govern learning efficiency. Because clustering is not updated during training, the model’s changing notion of “semantic similarity” or “difficulty” is ignored, reducing long-term adaptivity.\n* The asymptotic form hides a large constant factor: conjugate gradient iterations over Hessian-vector products make influence computation several times costlier than standard RLHF updates, challenging scalability to real LLMs\n* Expressing influence cost as $\\mathcal{O}(|\\theta|)$ is mathematically neat but hides architectural dependencies. In practice, transformer-based LLMs exhibit significant per-layer overhead that can make the constant factor large and hardware-unfriendly.\n* While computationally light compared to gradient updates, maintaining per-cluster influence windows and UCB scores incurs non-negligible memory and synchronisation cost which is not reflected in the asymptotic expression.\n* The trade-off parameter introduces a new hyper-parameter balancing cost and adaptivity, but its sensitivity and effect on convergence are not theoretically analysed or empirically ablated."}, "questions": {"value": "Kindly refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "st94h7Dsx7", "forum": "8HvWBamUkS", "replyto": "8HvWBamUkS", "signatures": ["ICLR.cc/2026/Conference/Submission11782/Reviewer_2YAE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11782/Reviewer_2YAE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995204487, "cdate": 1761995204487, "tmdate": 1762922806274, "mdate": 1762922806274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}