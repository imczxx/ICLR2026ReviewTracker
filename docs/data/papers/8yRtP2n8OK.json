{"id": "8yRtP2n8OK", "number": 9913, "cdate": 1758148587466, "mdate": 1763706711584, "content": {"title": "Learning a Game by Paying the Agents", "abstract": "We study the problem of learning the utility functions of no-regret learning agents in a repeated normal-form game.\nDiffering from most prior literature, we introduce a principal with the power to observe the agents playing the game, send agents signals, and give agents *payments* as a function of their actions.\nWe show that the principal can, using a number of rounds polynomial in the size of the game, learn the utility functions of all agents to any desired precision $\\varepsilon > 0$, for any no-regret learning algorithms of the agents.\nOur main technique is to formulate a zero-sum game between the principal and the agents, where the principal's strategy space is the set of all payment functions. \nFinally, we discuss implications for the problem of *steering* agents to a desired equilibrium: in particular, we introduce, using our utility-learning algorithm as a subroutine, the first algorithm for steering arbitrary no-regret learning agents without prior knowledge of their utilities.", "tldr": "We propose the first algorithm to learn a game from the actions of any no-regret learning agents by paying the agents.", "keywords": ["No-Regret Learning", "Inverse Game Theory", "Revealed Preference", "Steering"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0672a183503b67b5f2ae5dcea7c7a0acaf9f9cd0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies an active, non-equilibrium inverse game problem interacts with any no-regret players by giving payments and signals. The obejctive is to learn the utility functions of players under a equivalent game. The main idea is to cast the principal–agent interaction as a zero‑sum game where the principal’s mixed strategy is a payment vector. Then, authors consdier the problem of steering no-regret learning algorithm to desired result without the access to utility functions of players."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The zero‑sum formulation simple yet interesting, which requires almost no modeling of the agent’s learning rule beyond no‑regret property.\n\n- The results of this paper hold for arbitrary no‑regret learner, and authors also provide a lower bound implying the tightness of $\\epsilon$ and necessarity of $M$.\n\n- The proposed approaches are extended to the problem of steering no-regret learner without the access of utility function. Authors show that the principal-optimal CEP characterizes the value that the principal can achieve.\n\n- The paper is well-written and easy to follow. I appreciate that authors offer clear intuitions for the most of places."}, "weaknesses": {"value": "- The assumption in (1) needs to hold for all signals, which seems strong to me. In fact, many contextual algorithms ensure provable regret bounds on average over contexts, not uniformly over all contexts.\n\n- The sample complexity shown in Theorem 4.3 has polynomial dependence on M and C, but it does not match the given lower bound. \n\n- The rate of steering agents to achieve the optimal CEP is $T^{-1/4}$, which seems suboptimal."}, "questions": {"value": "- I would like to hear whether it is possible to relax the requirement on the signal in Theorem 4.3. Could the authors clarify or discuss potential directions for such relaxation?\n\n- In Section 3, it is mentioned that the knowledge of the mixed strategy $x_t$ could be relaxed. Could the authors elaborate on how the proposed algorithm should be modified in that case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SvHvSGI0CA", "forum": "8yRtP2n8OK", "replyto": "8yRtP2n8OK", "signatures": ["ICLR.cc/2026/Conference/Submission9913/Reviewer_2pKK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9913/Reviewer_2pKK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761282046788, "cdate": 1761282046788, "tmdate": 1762921370808, "mdate": 1762921370808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of learning a game by observing the agents play the game using no-regret algorithms and paying them to influence their behavior. The actor in this game seeks to learn the utilities of the game, up to strategic equivalence, through picking payments and providing signals to the agents in each round of play, under the assumption that the agents play arbitrary, possibly correlated, no-regret algorithms. It is important to note that the agents are in fact assumed to have contextual no-regret, i.e. they have vanishing regret for each signal/ no-regret over strategies mapping signals to actions. The main result of the paper shows that it is possible to learn the game in number of rounds polynomial in the size of the payoff matrix and that there are matching lower bounds, up to polynomial factors. \n\nThe paper first shows a slick method to estimate the utilities of a single agent using payments, by defining a two-player zero-sum game with a unique equilibrium and then shows how to construct no-regret dynamics to generate a sequence of payments that approaches this equilibrium, which in turn reveals the utility function of the agent.  This method is then lifted to learn the utilities of all agents in sequence in a multi-agent game, by rotating through the actions of all other players and using signals to fix their behavior for each learning phase. Other results in the paper include using this result to steer agents to desirable correlated equilibria while starting with no initial information about the game."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "The main strength of the paper is in posing a natural question about learning a game by observing no-regret behavior and making meaningful progress in answering it by extending existing technical machinery. In particular, the algorithm to find the utility of a single agent is an elegant construction to re-use no-regret dynamics to find the equilibria of a game despite not seeing full information feedback."}, "weaknesses": {"value": "The main weakness of the paper is the strong assumption made about the agents having contextual no-regret over the signals employed by the actor learning the game. This gives the actor tremendous leverage over the players practically for \"free\" since it allows them to \"forget\" the history of play by switching signals. This is exploited in obtaining an almost direct reduction from multiple players to one player. While this might well be a necessary assumption, there is insufficient technical justification for it."}, "questions": {"value": "Connected to the main weakness highlighted above -- how necessary is the assumption about no-regret for each signal? For instance, is it at all possible to learn the utilities of a multiplayer game with just a vanilla external regret assumtion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pBwBKwmXM8", "forum": "8yRtP2n8OK", "replyto": "8yRtP2n8OK", "signatures": ["ICLR.cc/2026/Conference/Submission9913/Reviewer_evWz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9913/Reviewer_evWz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762147984470, "cdate": 1762147984470, "tmdate": 1762921369110, "mdate": 1762921369110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Dear authors,\nI am finalizing my evaluation and will upload my full review within the next two days. Thank you for your patience.\n\nBest regards,\n—Reviewer"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "TBC"}, "weaknesses": {"value": "TBC"}, "questions": {"value": "TBC"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JvaiZhK3Kv", "forum": "8yRtP2n8OK", "replyto": "8yRtP2n8OK", "signatures": ["ICLR.cc/2026/Conference/Submission9913/Reviewer_bHow"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9913/Reviewer_bHow"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762313634567, "cdate": 1762313634567, "tmdate": 1762921368696, "mdate": 1762921368696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of learning the unknown utility functions of agents who repeatedly interact in a normal-form game while employing no-regret learning algorithms. Unlike most prior work in mechanism design or repeated games, the setting introduces a principal who has the ability to (i) observe the agents’ actions, (ii) send them signals, and (iii) issue payments that depend on their actions. The central question is whether the principal can efficiently recover the agents’ utilities under these constraints.\n\nThe authors show that, using a number of rounds polynomial in the size of the game, the principal can learn each agent’s utility function to arbitrary precision \\varepsilon > 0, regardless of which no-regret algorithm the agents use. The core technical idea is to cast the interaction between the principal and the agents as a zero-sum game, where the principal’s strategy space consists of all possible payment functions. Using this equilibrium perspective, the algorithm the authors design provably identifies utilities through carefully chosen payment perturbations.\n\nFinally, the paper discusses implications for steering agents toward desired equilibria, presenting what they claim to be the first general algorithm capable of steering arbitrary no-regret learners without prior knowledge of their utilities, by leveraging the learned utility estimates as a subroutine."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Please see the questions"}, "weaknesses": {"value": "Please see the questions"}, "questions": {"value": "**\n\n## Review – Detailed Questions and Clarification Requests\n\n**\n\nFirst, let me say that I personally like this paper. The problem is interesting, the setup is novel, and the results are potentially impactful. However, I have many technical questions, and for this submission cycle I prefer not to artificially separate “strengths” and “weaknesses” in the classical way—this style feels increasingly outdated in the age of LLM-augmented reviewing. Instead, I will focus on the substantive questions that will genuinely help me assess the contribution.\n\nIf the authors address these satisfactorily, I am prepared to champion the paper.\n\nI also want to apologize for the delay in sending my review.\n\n⸻\n\n1. **Concept of Signals / Context in No-Regret Algorithms**\n\nMy own research is primarily in normal-form and Markov games. For this reason, I would like to deeply understand the idea of signals and context in your model.\n\n*(1a) What does an FTRL or MWU algorithm “with context” actually look like?*\n\nYou state that agents run contextual no-regret algorithms. However, I am not sure what the canonical formalism is.\n\t•\tHow is the context incorporated into the update rule?\n\t•\tIs it equivalent to having one no-regret instance per context?\n\t•\tDoes the algorithm maintain separate weight vectors for each signal?\n\t•\tOr is there some shared information?\n\n*(1b) Please give a realistic (even toy) example where context naturally appears.*\n\nA motivating example would help:\nFor instance, in what real-world situation would a no-regret learner receive such a context signal, and why should this be part of the model?\n\n⸻\n\n**2. Why Should the Agents Obey the Signals?**\n\nThis is a central conceptual confusion for me.\n\n*(2a) Why should an agent cooperate with the principal’s signal and switch context or restart its algorithm?*\n\nIn realistic no-regret settings, the learner treats the environment as adversarial.\nSo why should they obey a request of the form:\n\t•\t“Now move to another context”\n\t•\t“Restart your algorithm”\n\t•\t“Reset your regret counters”\n\nWhat is the incentive?\n\n*(2b) Why wouldn’t an agent simply ignore the signal?*\n\nIf I am a no-regret learner designed for adversarial environments, why would I trust the principal’s request at all?\n\n*(2c) Isn’t the principal implicitly collaborating with the agent?*\n\nIt seems that the principal’s ability to steer requires a form of cooperation.\nIf they truly cooperated, wouldn’t they naturally play something like best response or some coordination equilibrium, instead of adversarial learning?\n\nThis conceptual gap is important for understanding your model.\n\n⸻\n\n**3. Proposition B.1 and the Definition of “Any-Time”**\n\nYou define a constant B that is supposed to be “any-time.”\nHowever, I do not understand why B should be independent of the current time step.\n\t•\tShouldn’t we have something like B_t for an any-time bound?\n\t•\tAs written, the definition looks more like an “earlier-time” condition.\n\nCould you clarify the intended meaning?\n\n⸻\n\n**4. Line 221 – Why Do All No-Regret Algorithms Ignore Additive Constants?**\n\nYou mention that for regret minimization only differences of utilities matter, not absolute values.\n\nWhile I understand this for standard definitions and classical algorithms (FTRL, MWU), I do not see why this must hold for all no-regret algorithms.\n\t•\tSome no-regret algorithms do depend on absolute values when the feedback signal is u - c.\n\t•\tWhy should we assume invariance to constant shifts holds universally?\n\nPlease clarify the assumptions you make on the learning rule.\n\n⸻\n\n**5. The “Obedience” Problem Revisited: Why Restart at a Signal?**\n\nThis is the conceptual issue that troubles me the most.\nWhen I play a no-regret algorithm, I am facing an adversary.\nThen suddenly the principal sends a signal telling me to restart or to adopt a new context.\nWhy should the agent comply?\n\nWhy wouldn’t a perfectly rational no-regret learner simply reject the signal and continue its algorithm, preserving consistency of its regret guarantees?\nIt truly feels as if the agent is collaborating with the principal.\nIf they really cooperate, why not use something stronger than no-regret (best response, a negotiating protocol, etc.)?\n\n⸻\n\n**6. No-Regret Against an Adversary – What Is the Realism of This Setting?**\n\nIf we treat actions as experts, what real-world system is being modeled?\nI would appreciate even a small but realistic example where:\n\t•\tagents run adversarial no-regret,\n\t•\tyet they also accept contextual signals from a principal,\n\t•\tand utilities are unknown.\nA concrete motivating example would help significantly.\n\n⸻\n\n**7. Lower Bound Argument – The C / (2\\sqrt{T}) Issue**\n\nYour lower bound uses the expression C / (2\\sqrt{T}).\nHere I have multiple technical questions:\n*(7a) Why would the agent choose a constant learning rate that makes this bound valid?*\n\nIn standard teaching (and standard theory), when an adversary has parameter $\\eta^*$:\n$f(\\eta) = \\frac{a}{\\eta} + b\\eta T$ we optimize \\eta to minimize regret.\nIf I run MWU with the optimal learning rate, how do I recover the expression $C / (2\\sqrt{T})$?\n\n*(7b) Should we simply choose C/2 larger than the optimized constant?*\n\nIf so, why would the indistinguishable agent want to play with rate $C/\\sqrt{T}$?\nThis part of the argument feels fragile, and perhaps could be strengthened or clarified.\n⸻\n\n**8. Algorithm 2 – Why Is Line 7 Inside the Third Loop?**\n\nLine 7 re-sends the signal.\nWhy must this step appear inside the third loop instead of outside?\nIs there a technical reason the signal needs to be repeatedly re-sent?\n\n⸻\n\n**9. Proofs of D.5 and D.2.1 – Request for Slow, Detailed Derivations**\n\nI struggled with both proofs, especially the treatment of:\n\t•\tCEP for non-entangled signals\n\t•\tCEP for entangled signals\n\nCould the authors provide a fully explicit derivation, with all algebraic steps?\nThe current exposition is very hard to follow.\n\n⸻\n\n**10. Steering and the Role of No-Swap Regret**\n\nIn the steering section, the final result applies to contextual no-swap regret learners.\nThis raises several conceptual questions:\n\n*(10a) I thought the whole paper concerns steering no-regret learners; why do we end up needing no-swap?*\n*(10b) No-swap is stronger than external no-regret; it is also non-manipulable.*\nThus, if I understand correctly, the algorithm only works because agents obey the signal.\nIf they ignored signals, the guarantee fails.\n\n*(10c) No-swap learners converge to the CEP. Isn’t this already expected?*\n\nIsn’t CEP convergence a standard property for no-swap algorithms?\nIs your corollary a standard result or is there a subtle caveat that differentiates your setting?\n\n*(10d) What does a “contextual no-swap” algorithm even look like?*\nIs there a simple version one could write explicitly?\n\n⸻\n\n## Closing\n\nPlease do not overlook any of these questions.\nThey are all essential for my evaluation.\n\nIf the authors can provide satisfactory clarifications and resolve the conceptual tensions above, I will be happy to champion the paper.\n\n⸻"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JvaiZhK3Kv", "forum": "8yRtP2n8OK", "replyto": "8yRtP2n8OK", "signatures": ["ICLR.cc/2026/Conference/Submission9913/Reviewer_bHow"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9913/Reviewer_bHow"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762313634567, "cdate": 1762313634567, "tmdate": 1763401973432, "mdate": 1763401973432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This game addresses a natural question of learning a player's utility functions over time and studies this from an online learning perspective. Although this is a standard problem, the paper differs from the literature in two important ways: first they assume that the players are not in equilibrium. Secondly, they assume that the players are playing a no regret algorithm. The main contribution is a polynomial time algorithm that learns the utilities upto an error of $\\epsilon$. They further show that they can guide the agent towards a particular correlated equilibrium with payments (also known as steering). \n\nThe model is as follows. In each round the principal payment functions $P^t_i: A_i \\to [0,B]$  gets added to the agent rewards. The principal can also send a private signal. The agent which maximizes the sum of the payment and their own utility will play a mixed strategy $x^t$ which is observed by the principal. The paper's central insight is to reduce this to a zero-sum game between the principal and agent. Effectively the unique optimal in this game is for the principal to offer a payment of $p = 1-u$ the utility, thus making the agent indifferent between the actions. Then the principal can read the utility function by simply taking $p=1-u$. This is because the utility is zero-mean and if the principal updates their weights according to a no-regret algorithm like projected gradient descent (note that the mixed strategy $x^t$ is the derivative of the principal's loss function $(u+p) \\cdot x^t$), it will eventually converge to a point where the optimal utility is $u+p$ is a constant. This is a simple but elegant idea that allows the principal to discover the agent's true utility. \n\nTo handle the general setting when there are $n$ agents, the principal uses signals to effectively freeze the other agent's actions."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I think the paper outlines a simple and beautiful idea to learn the utility of any agent in a normal formed game. The dependence and the  learning rate and the associated regret is tight."}, "weaknesses": {"value": "The complexity to learn scales exponentially on the number of agents and their action profiles. Their lower bound suggests that this task is too hard for many practical games. I also wonder a conference such as EC  might be a better fit than ICLR."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UT2Y399v3h", "forum": "8yRtP2n8OK", "replyto": "8yRtP2n8OK", "signatures": ["ICLR.cc/2026/Conference/Submission9913/Reviewer_zfeR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9913/Reviewer_zfeR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762907579907, "cdate": 1762907579907, "tmdate": 1762921368435, "mdate": 1762921368435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}