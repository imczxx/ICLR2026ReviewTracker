{"id": "8yRtP2n8OK", "number": 9913, "cdate": 1758148587466, "mdate": 1759897687055, "content": {"title": "Learning a Game by Paying the Agents", "abstract": "We study the problem of learning the utility functions of no-regret learning agents in a repeated normal-form game.\nDiffering from most prior literature, we introduce a principal with the power to observe the agents playing the game, send agents signals, and give agents *payments* as a function of their actions.\nWe show that the principal can, using a number of rounds polynomial in the size of the game, learn the utility functions of all agents to any desired precision $\\varepsilon > 0$, for any no-regret learning algorithms of the agents.\nOur main technique is to formulate a zero-sum game between the principal and the agents, where the principal's strategy space is the set of all payment functions. \nFinally, we discuss implications for the problem of *steering* agents to a desired equilibrium: in particular, we introduce, using our utility-learning algorithm as a subroutine, the first algorithm for steering arbitrary no-regret learning agents without prior knowledge of their utilities.", "tldr": "We propose the first algorithm to learn a game from the actions of any no-regret learning agents by paying the agents.", "keywords": ["No-Regret Learning", "Inverse Game Theory", "Revealed Preference", "Steering"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8c373d4a27882a68eb5d6129958dba2feed213fa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies an active, non-equilibrium inverse game problem interacts with any no-regret players by giving payments and signals. The obejctive is to learn the utility functions of players under a equivalent game. The main idea is to cast the principal–agent interaction as a zero‑sum game where the principal’s mixed strategy is a payment vector. Then, authors consdier the problem of steering no-regret learning algorithm to desired result without the access to utility functions of players."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The zero‑sum formulation simple yet interesting, which requires almost no modeling of the agent’s learning rule beyond no‑regret property.\n\n- The results of this paper hold for arbitrary no‑regret learner, and authors also provide a lower bound implying the tightness of $\\epsilon$ and necessarity of $M$.\n\n- The proposed approaches are extended to the problem of steering no-regret learner without the access of utility function. Authors show that the principal-optimal CEP characterizes the value that the principal can achieve.\n\n- The paper is well-written and easy to follow. I appreciate that authors offer clear intuitions for the most of places."}, "weaknesses": {"value": "- The assumption in (1) needs to hold for all signals, which seems strong to me. In fact, many contextual algorithms ensure provable regret bounds on average over contexts, not uniformly over all contexts.\n\n- The sample complexity shown in Theorem 4.3 has polynomial dependence on M and C, but it does not match the given lower bound. \n\n- The rate of steering agents to achieve the optimal CEP is $T^{-1/4}$, which seems suboptimal."}, "questions": {"value": "- I would like to hear whether it is possible to relax the requirement on the signal in Theorem 4.3. Could the authors clarify or discuss potential directions for such relaxation?\n\n- In Section 3, it is mentioned that the knowledge of the mixed strategy $x_t$ could be relaxed. Could the authors elaborate on how the proposed algorithm should be modified in that case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SvHvSGI0CA", "forum": "8yRtP2n8OK", "replyto": "8yRtP2n8OK", "signatures": ["ICLR.cc/2026/Conference/Submission9913/Reviewer_2pKK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9913/Reviewer_2pKK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761282046788, "cdate": 1761282046788, "tmdate": 1762921370808, "mdate": 1762921370808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of learning a game by observing the agents play the game using no-regret algorithms and paying them to influence their behavior. The actor in this game seeks to learn the utilities of the game, up to strategic equivalence, through picking payments and providing signals to the agents in each round of play, under the assumption that the agents play arbitrary, possibly correlated, no-regret algorithms. It is important to note that the agents are in fact assumed to have contextual no-regret, i.e. they have vanishing regret for each signal/ no-regret over strategies mapping signals to actions. The main result of the paper shows that it is possible to learn the game in number of rounds polynomial in the size of the payoff matrix and that there are matching lower bounds, up to polynomial factors. \n\nThe paper first shows a slick method to estimate the utilities of a single agent using payments, by defining a two-player zero-sum game with a unique equilibrium and then shows how to construct no-regret dynamics to generate a sequence of payments that approaches this equilibrium, which in turn reveals the utility function of the agent.  This method is then lifted to learn the utilities of all agents in sequence in a multi-agent game, by rotating through the actions of all other players and using signals to fix their behavior for each learning phase. Other results in the paper include using this result to steer agents to desirable correlated equilibria while starting with no initial information about the game."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "The main strength of the paper is in posing a natural question about learning a game by observing no-regret behavior and making meaningful progress in answering it by extending existing technical machinery. In particular, the algorithm to find the utility of a single agent is an elegant construction to re-use no-regret dynamics to find the equilibria of a game despite not seeing full information feedback."}, "weaknesses": {"value": "The main weakness of the paper is the strong assumption made about the agents having contextual no-regret over the signals employed by the actor learning the game. This gives the actor tremendous leverage over the players practically for \"free\" since it allows them to \"forget\" the history of play by switching signals. This is exploited in obtaining an almost direct reduction from multiple players to one player. While this might well be a necessary assumption, there is insufficient technical justification for it."}, "questions": {"value": "Connected to the main weakness highlighted above -- how necessary is the assumption about no-regret for each signal? For instance, is it at all possible to learn the utilities of a multiplayer game with just a vanilla external regret assumtion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pBwBKwmXM8", "forum": "8yRtP2n8OK", "replyto": "8yRtP2n8OK", "signatures": ["ICLR.cc/2026/Conference/Submission9913/Reviewer_evWz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9913/Reviewer_evWz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762147984470, "cdate": 1762147984470, "tmdate": 1762921369110, "mdate": 1762921369110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Dear authors,\nI am finalizing my evaluation and will upload my full review within the next two days. Thank you for your patience.\n\nBest regards,\n—Reviewer"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "TBC"}, "weaknesses": {"value": "TBC"}, "questions": {"value": "TBC"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JvaiZhK3Kv", "forum": "8yRtP2n8OK", "replyto": "8yRtP2n8OK", "signatures": ["ICLR.cc/2026/Conference/Submission9913/Reviewer_bHow"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9913/Reviewer_bHow"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762313634567, "cdate": 1762313634567, "tmdate": 1762921368696, "mdate": 1762921368696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This game addresses a natural question of learning a player's utility functions over time and studies this from an online learning perspective. Although this is a standard problem, the paper differs from the literature in two important ways: first they assume that the players are not in equilibrium. Secondly, they assume that the players are playing a no regret algorithm. The main contribution is a polynomial time algorithm that learns the utilities upto an error of $\\epsilon$. They further show that they can guide the agent towards a particular correlated equilibrium with payments (also known as steering). \n\nThe model is as follows. In each round the principal payment functions $P^t_i: A_i \\to [0,B]$  gets added to the agent rewards. The principal can also send a private signal. The agent which maximizes the sum of the payment and their own utility will play a mixed strategy $x^t$ which is observed by the principal. The paper's central insight is to reduce this to a zero-sum game between the principal and agent. Effectively the unique optimal in this game is for the principal to offer a payment of $p = 1-u$ the utility, thus making the agent indifferent between the actions. Then the principal can read the utility function by simply taking $p=1-u$. This is because the utility is zero-mean and if the principal updates their weights according to a no-regret algorithm like projected gradient descent (note that the mixed strategy $x^t$ is the derivative of the principal's loss function $(u+p) \\cdot x^t$), it will eventually converge to a point where the optimal utility is $u+p$ is a constant. This is a simple but elegant idea that allows the principal to discover the agent's true utility. \n\nTo handle the general setting when there are $n$ agents, the principal uses signals to effectively freeze the other agent's actions."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I think the paper outlines a simple and beautiful idea to learn the utility of any agent in a normal formed game. The dependence and the  learning rate and the associated regret is tight."}, "weaknesses": {"value": "The complexity to learn scales exponentially on the number of agents and their action profiles. Their lower bound suggests that this task is too hard for many practical games. I also wonder a conference such as EC  might be a better fit than ICLR."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UT2Y399v3h", "forum": "8yRtP2n8OK", "replyto": "8yRtP2n8OK", "signatures": ["ICLR.cc/2026/Conference/Submission9913/Reviewer_zfeR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9913/Reviewer_zfeR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762907579907, "cdate": 1762907579907, "tmdate": 1762921368435, "mdate": 1762921368435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}