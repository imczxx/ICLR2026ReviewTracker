{"id": "y6nhcCdQYd", "number": 7956, "cdate": 1758046087445, "mdate": 1763653002792, "content": {"title": "Improving LLM-based Global Optimization with Search Space Partitioning", "abstract": "Large Language Models (LLMs) have recently emerged as effective surrogate models and candidate generators within global optimization frameworks for expensive black-box functions. Despite promising results, LLM-based methods often struggle in high-dimensional search spaces or when lacking domain-specific priors, leading to sparse or uninformative suggestions. To overcome these limitations, we propose HOLLM, a novel global optimization algorithm that enhances LLM-driven sampling by partitioning the search space into promising subregions. Each subregion acts as a \"meta-arm\" selected via a bandit-inspired scoring mechanism that effectively balances exploration and exploitation. Within each selected subregion, an LLM then proposes high-quality candidate points, without any explicit domain knowledge. Empirical evaluation on standard optimization benchmarks shows that HOLLM consistently matches or surpasses leading global optimization methods, while substantially outperforming traditional global LLM-based sampling strategies.", "tldr": "HOLLM is a global optimization algorithm that adaptively partitions the search space and generates promising candidate solutions in them with LLMs.", "keywords": ["global optimization", "LLMs", "bayesian optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dbf8b03e97e83824d05a01267cdaa6d611d7006a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses black-box global optimization with LLMs and argues that asking an LLM to propose points globally often yields sparse, biased coverage—especially in higher dimensions or without domain priors. It introduces HOLLM, a loop that (i) adaptively partitions the search space with a KD-tree, (ii) treats each leaf as a meta-arm, (iii) scores leaves by combining best observed improvement, geometric size, and a variance-aware exploration bonus, and (iv) stochastically selects regions before prompting the LLM to generate local candidates, which are then evaluated and fed back. The method re-fits the KD-tree each round to avoid premature localization and uses a cosine-style schedule to front-load exploration. Experiments span continuous synthetic benchmarks (Hartmann, Rosenbrock, Rastrigin, Lévy), FCNet-9D hyperparameter optimization on four datasets, and three real-world continuous tasks (Penicillin, Vehicle Safety, Car Side Impact). HOLLM typically matches or shows modest gains over BO/ES and global-LLM baselines, but it is not consistently dominant."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Methodological novelty with a clean, coherent design. The paper marries KD-tree–based adaptive space partitioning with a bandit-style composite scoring rule—combining best observed improvement, geometric volume, and a variance-aware (UCB-V–like) exploration term—and then prompts the LLM to generate candidates locally within selected regions. Re-fitting the partition each round helps prevent premature commitment and keeps the search adaptive. Overall, the loop (partition, score, select, local LLM proposals, evaluate) is conceptually simple yet technically sound.\n2. Across diverse tasks, HOLLM’s trajectories are comparatively stable against strong baselines. Notably, on many settings the global-LLM variant tends to stall, whereas HOLLM continues to improve. This is likely because “partition + scoring + local sampling” mitigates sampling bias in LLM-guided optimization and reduces brittleness to model idiosyncrasies.\n3. Wide experimental coverage with appropriate baselines. The evaluation spans continuous synthetic functions, FCNet-9D hyperparameter optimization, and three real-world continuous tasks, and it compares against a solid suite of methods: BO (e.g., CQR, GP-EI), evolutionary strategies, random search, a carefully controlled global-LLM baseline, and RS+KD-Tree.\n4. Component and hyperparameter ablations that aid interpretation. Both the main text and appendix report ablations/visualizations for key knobs, such as the exploration annealing schedule (α), batch/selection sizes (b/M/k), and leaf capacity. This helps clarifying how each component contributes to performance and stability."}, "weaknesses": {"value": "1. Average gains are modest. On continuous tasks, HOLLM is clearly best only on a subset (Lévy, Rosenbrock, Car Side Impact, Vehicle Safety). On the other continuous benchmarks, there exist methods that outperform it by a visible margin; several curves overlap within error bars, so the advantage is not consistent. On FCNet-9D (discrete HPO), HOLLM and the global-LLM baseline are essentially overlapping—partitioning brings little benefit in this regime. Because the paper does not provide theory, the empirical results carry the core argumentative burden. Since improvements on continuous tasks are modest, and those on FCNet are near zero, it’s hard to argue broad superiority; the main benefit appears to be robustness rather than overall performance.\n2. LLM ablation is under-scoped. The multi-LLM comparison appears only on a single task (Vehicle Safety). To demonstrate the practicality of the algorithm, the paper should repeat the ablation across multiple continuous benchmarks."}, "questions": {"value": "1. Experimental results are not particularly strong in my view. Based on the experimental evidence, can you provide a detailed, results-driven explanation of what the algorithm actually contributes, with pointers to the specific tasks and figures?\n2. The multi-LLM ablation is only on Vehicle Safety. Can you repeat it on more tasks (1 additional continuous benchmarks and 1 discrete task)?\nI would consider increasing my score if you both (i) provide a detailed, results-grounded explanation of the algorithm’s contributions (answering my first question), and (ii) expand the LLM ablation to more tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SYaphIM8cI", "forum": "y6nhcCdQYd", "replyto": "y6nhcCdQYd", "signatures": ["ICLR.cc/2026/Conference/Submission7956/Reviewer_LeUX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7956/Reviewer_LeUX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761119921213, "cdate": 1761119921213, "tmdate": 1762919972338, "mdate": 1762919972338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses black-box global optimization with LLMs and argues that asking an LLM to propose points globally often yields sparse, biased coverage—especially in higher dimensions or without domain priors. It introduces HOLLM, a loop that (i) adaptively partitions the search space with a KD-tree, (ii) treats each leaf as a meta-arm, (iii) scores leaves by combining best observed improvement, geometric size, and a variance-aware exploration bonus, and (iv) stochastically selects regions before prompting the LLM to generate local candidates, which are then evaluated and fed back. The method re-fits the KD-tree each round to avoid premature localization and uses a cosine-style schedule to front-load exploration. Experiments span continuous synthetic benchmarks (Hartmann, Rosenbrock, Rastrigin, Lévy), FCNet-9D hyperparameter optimization on four datasets, and three real-world continuous tasks (Penicillin, Vehicle Safety, Car Side Impact). HOLLM typically matches or shows modest gains over BO/ES and global-LLM baselines, but it is not consistently dominant."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Methodological novelty with a clean, coherent design. The paper marries KD-tree–based adaptive space partitioning with a bandit-style composite scoring rule—combining best observed improvement, geometric volume, and a variance-aware (UCB-V–like) exploration term—and then prompts the LLM to generate candidates locally within selected regions. Re-fitting the partition each round helps prevent premature commitment and keeps the search adaptive. Overall, the loop (partition, score, select, local LLM proposals, evaluate) is conceptually simple yet technically sound.\n2. Across diverse tasks, HOLLM’s trajectories are comparatively stable against strong baselines. Notably, on many settings the global-LLM variant tends to stall, whereas HOLLM continues to improve. This is likely because “partition + scoring + local sampling” mitigates sampling bias in LLM-guided optimization and reduces brittleness to model idiosyncrasies.\n3. Wide experimental coverage with appropriate baselines. The evaluation spans continuous synthetic functions, FCNet-9D hyperparameter optimization, and three real-world continuous tasks, and it compares against a solid suite of methods: BO (e.g., CQR, GP-EI), evolutionary strategies, random search, a carefully controlled global-LLM baseline, and RS+KD-Tree.\n4. Component and hyperparameter ablations that aid interpretation. Both the main text and appendix report ablations/visualizations for key knobs, such as the exploration annealing schedule (α), batch/selection sizes (b/M/k), and leaf capacity. This helps clarifying how each component contributes to performance and stability."}, "weaknesses": {"value": "1. Average gains are modest. On continuous tasks, HOLLM is clearly best only on a subset (Lévy, Rosenbrock, Car Side Impact, Vehicle Safety). On the other continuous benchmarks, there exist methods that outperform it by a visible margin; several curves overlap within error bars, so the advantage is not consistent. On FCNet-9D (discrete HPO), HOLLM and the global-LLM baseline are essentially overlapping—partitioning brings little benefit in this regime. Because the paper does not provide theory, the empirical results carry the core argumentative burden. Since improvements on continuous tasks are modest, and those on FCNet are near zero, it’s hard to argue broad superiority; the main benefit appears to be robustness rather than overall performance.\n2. LLM ablation is under-scoped. The multi-LLM comparison appears only on a single task (Vehicle Safety). To demonstrate the practicality of the algorithm, the paper should repeat the ablation across multiple continuous benchmarks."}, "questions": {"value": "1. Experimental results are not particularly strong in my view. Based on the experimental evidence, can you provide a detailed, results-driven explanation of what the algorithm actually contributes, with pointers to the specific tasks and figures?\n2. The multi-LLM ablation is only on Vehicle Safety. Can you repeat it on more tasks (1 additional continuous benchmarks and 1 discrete task)?\nI would consider increasing my score if you both (i) provide a detailed, results-grounded explanation of the algorithm’s contributions (answering my first question), and (ii) expand the LLM ablation to more tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SYaphIM8cI", "forum": "y6nhcCdQYd", "replyto": "y6nhcCdQYd", "signatures": ["ICLR.cc/2026/Conference/Submission7956/Reviewer_LeUX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7956/Reviewer_LeUX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761119921213, "cdate": 1761119921213, "tmdate": 1763718519578, "mdate": 1763718519578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper introduces HOLLM, a global BBO algorithm that uses adaptive search space partitioning with LLM-based hypothesis proposal (within selected subregions) and evaluation (global)\n- It uses a bandit-style metric at the partition-level to trade off exploitation, geometric coverage, and statistical uncertainty\n- The algorithm follows the process of (1) partitioning the space, (2) ranking leaf regions, (3) sampling from within the regions of selected partitions, (4) using an LLM to propose local points, (5) evaluating and selecting the best proposals"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper gives an intuitive and empirical evidence of LLM failure modes in high-dimensional spaces (biased coverage, mode-seeking), which it addresses through searching/sampling within bounded subregions\n- The algorithm makes sense, combining KD-tree partitioning, UCB-style scoring/selection over the partitions (as arms), and LLM-buided local BO. While each of those components exist in some form in BO/MAB/hierarchical bandit literature, the particular design feels suitable and well-motivated\n- The arm-level scoring function and stochastic selection policy is novel (AFAIK), based on best-observed improvement (exploitation), normalized HV (geometric exploration), and UCB-V (uncertainty)"}, "weaknesses": {"value": "- The arm-level scoring functions feels slightly ad-hoc and slightly complicated, it is not clear why the HV-based term and UCB-V (as opposed to other UCB variants) is necessary and used. The HOLLM vs global LLM-based BO is a good ablation, but it would also be interesting to isolate the different terms (e.g., UCB-variance) to see if they matter in practice.\n- It would be interesting to understand the LLM performance specifically, e.g., how well the surrogate performance is vs a GP\n- One possible limitation of this work is that even with KD-tree partitioning, this might degrade in higher-dimensional spaces. Based on my understanding, the paper's benchmarks are mainly < 10D, it is not clear whether this partitioning will give proportional empirical improvements (say in 50D) where TURBO-style trust regions or CMA_ES covariance structures might be more robust\n- It is not clear to me why the LLM sampling/surrogate sees the global results, and not just region-specific results\n- Minor: the cosine annealing for exploration is sensible, but we don't see quantitative ablations around sensitivity.\n- Minor: how are the number of partition regions selected (I might have missed this), it feels like an important hyperparameter.\n- Minor: in the results section, it is a bit confusing to interpret (with maximize vs minimize), it would be hlepful to standardize sign convention and introduce visual hints\n- Minor: this is not a major concern in my mind, but as the work draws parallels to HOO/bandit-style UCB-V, it is worth pointing out that this field of literature cares particularly about regret/concentration analysis."}, "questions": {"value": "- The LLM surrogate does not include any uncertainty. How good is the mean prediction, especially with LLMs of different capabilities\n- Can the authors clarify the computational overhead (e.g., wall-clock time) for each aspect of their pipeline (e.g., dynamic re-partitioning, local LLM-based BO)\n- It would be interesting, but not a priority, to see results on benchmarks with more noise and understanding how the partitioning + variance-sensitive scoring function performs\n- How limiting are the axis-aligned partitions? Would this be applicable to different problems and where might it be a bottleneck?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dy3BjVanxm", "forum": "y6nhcCdQYd", "replyto": "y6nhcCdQYd", "signatures": ["ICLR.cc/2026/Conference/Submission7956/Reviewer_Y2NU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7956/Reviewer_Y2NU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571830472, "cdate": 1761571830472, "tmdate": 1762919971917, "mdate": 1762919971917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a key limitation of using Large Language Models (LLMs) for global optimization: their tendency to sample sparsely and inefficiently in high-dimensional search spaces. The authors propose HOLLM (Hierarchical Optimization with Large Language Models), a novel algorithm that integrates LLM-based candidate sampling with an adaptive search space partitioning strategy. HOLLM iteratively builds a KD-tree to divide the search space into smaller subregions. It then uses a bandit-inspired utility score to select the most promising subregions, effectively balancing exploration (large or uncertain regions) and exploitation (regions with good observed values). An LLM is then prompted to generate new candidate points specifically within these selected, smaller regions. The authors' empirical results on benchmark functions demonstrate that HOLLM matches or outperforms other global optimization methods and significantly improves upon a baseline \"global LLM\" sampler."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper clearly identifies and demonstrates a practical weakness of LLM-based samplers—their high bias and inability to cover a space effectively (as shown in Figure 1)—and proposes an intuitive solution.\n- The inclusion of a \"global LLM\" baseline is crucial, as it provides strong evidence that the partitioning framework itself, not just the use of an LLM, is responsible for the performance gains"}, "weaknesses": {"value": "- The paper's primary contribution appears incremental. The core components—hierarchical space partitioning (e.g., KD-trees) and bandit-based region selection—are well-established techniques in the black-box and hierarchical optimization literature (e.g., HOO, MABs). The method seems to primarily substitute a traditional sampler within this framework with an LLM, which may limit the work's fundamental novelty.\n- There is a notable disconnect between the paper's motivation and its empirical evaluation. The introduction suggests LLMs could unlock optimization for novel domains (e.g., language-based tasks) where traditional numeric methods are unsuitable. However, the experiments are confined to standard numerical benchmarks (synthetic functions, hyperparameter tuning) where classic methods are already effective. This evaluation fails to substantiate the primary motivating claim for using LLMs.\n- The framework's components, apart from the LLM sampler, are standard building blocks in black-box optimization. This reinforces the concern about incremental contribution, as it is unclear why an LLM is a necessary choice. The partitioning and selection mechanism  could seemingly be combined with other advanced samplers (e.g., from PSO, DE, or even a simple Gaussian sampler). The paper lacks a crucial ablation study to isolate and justify the unique benefit of using an LLM over these simpler, well-understood alternatives.\n- The results on synthetic benchmarks are difficult to interpret and potentially unconvincing. The authors should explicitly state the known global optima for these functions. There is a concern that some functions may have trivial solutions (e.g., $x^*=0$) that an LLM, due to its inherent biases, might guess easily. This suspicion is heightened by the surprisingly strong performance of the \"global LLM\" baseline, which reportedly outperforms most numeric methods (Table 4). This counter-intuitive baseline result calls the benchmark's difficulty and validity into question.\n- On the hyperparameter optimization tasks, the proposed HOLLM does not show a significant advantage over the simpler \"global LLM\" baseline. Given that the global method is far less complex, this result questions the practical utility of the added HOLLM framework. The paper needs to provide a cost-benefit analysis, including the computational overhead of the partitioning and scoring components. Furthermore, the work fails to disentangle whether the observed performance (of both HOLLM and the global baseline) is due to the algorithmic structure or simply the LLM's powerful, pre-trained \"intrinsic knowledge\"."}, "questions": {"value": "- What is the computational complexity (e.g., wall-clock time) of the HOLLM framework, specifically the KD-tree construction and region scoring, relative to the (presumably expensive) LLM sampling calls? How does this overhead scale as the number of evaluated points t increases?\n- The paper's motivation (Fig. 1) suggests LLMs are bad at sampling (i.e., biased). The method's success implies this bias is now a useful \"prior\". Could you elaborate on what specific, useful properties this \"meta-prior\" is assumed to have, especially when constrained to small subregions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4196I7hWMA", "forum": "y6nhcCdQYd", "replyto": "y6nhcCdQYd", "signatures": ["ICLR.cc/2026/Conference/Submission7956/Reviewer_dF12"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7956/Reviewer_dF12"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902237495, "cdate": 1761902237495, "tmdate": 1762919971593, "mdate": 1762919971593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method called HOLLM that partition the space of search using KD-Tree and then using a bandit-style algorithm that select from which part of the space sample new candidates points using LLMs. The motivation of the method bases on the observation that LLMs are not good by covering the space optimally. The authors show extensive results, including a comparison with a vanilla LLM implementation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Authors showcase extensive results demonstrating the applicability of HOLLM.\n- The authors provide ablation studies of the design choices on the appendix.\n- In general the paper is well written and presented."}, "weaknesses": {"value": "- My only doubt is that the authors develop a method based on the main motivation that LLMs are not efficient covering the space of solution. However, it seems the method not necessarilly needs an LLM in its design. On this point why LLMs are revelant for this method? My first assumption would be because LLMs have strong inductive bias about the problem and make them sample more efficiently. However, the inductive biases would be given by the context that is given to the LLM. This is a point that is not properly discussed in the paper. Can you elaborate on this point?\n- If HOLLM actually needs a LLM to perform well then it seems that is necessary a more extensive comparison between HOLLM and the LLM baseline to understand better why the first perform better than the second. Some intuition is given as motivation, but I guessed some metrics can be tracked to prove this point with the actual experiments."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zA47P5NBgw", "forum": "y6nhcCdQYd", "replyto": "y6nhcCdQYd", "signatures": ["ICLR.cc/2026/Conference/Submission7956/Reviewer_Uiwf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7956/Reviewer_Uiwf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995540575, "cdate": 1761995540575, "tmdate": 1762919971259, "mdate": 1762919971259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful and constructive feedback. Your detailed critiques and suggestions played a significant role in strengthening both the empirical evidence and the narrative of our paper. We are pleased that the reviewers recognized the core strengths of our work, namely, *the novelty aspect* (reviewers Uiwf, Y2NU, LeUX), *our core motivation* (reviewer dF12) and *the extensive empirical evaluations across diverse tasks* (reviewer Uiwf, LeUX). In response to the primary action items from the reviewers, we performed several crucial new experiments during this discussion phase, which we show in the **new Appendix E-J of the revised paper PDF (4.5 new added pages at the end of the file, with red-colored text)**. Due to the page constraint, we kept these additional experiments in the appendix, however, we will integrate the most important ones in the main paper upon acceptance. The table below summarizes these experiments.\n\n| Reviewer Action Item | Experiment Performed & Key Finding | Revised PDF Addition |\n| :--- | :--- | :--- |\n| **Deeper HOLLM vs. Global LLM comparison (Uiwf)** | Introduced a metric to quantify LLM sampling diversity. Results show that the global LLM rapidly suffers from *mode collapse*, while HOLLM maintains high exploratory behavior throughout. | Appendix E, Figure 16 |\n| **LLM vs. Simpler Samplers (dF12)** | HOLLM significantly outperformed KD-Tree + Random Search (RS) and KD-Tree + GP-EI, confirming the benefit of the localized LLM candidate sampler. | Appendix F, Figure 18 |\n| **Ablating Scoring Function Components (Y2NU)** | We evaluated the importance of the full composite scoring rule (including the geometric term and UCB-V); simpler variants like UCB1 led to worse performance. | Appendix F, Figure 17 |\n| **Multi-LLM Ablation (LeUX)** | We expanded the multi-LLM comparison in Figure 8 to another continuous and discrete task. We showed HOLLM's benefit is more prominent when using smaller open-weights LLMs. | Appendix I, Figure 20 |\n| **Surrogate Prediction Performance (Y2NU)** | We compared the LLM surrogate against Gaussian Processes (GPs) and TabPFN. The LLM demonstrated superior few-shot calibration and ranking accuracy on real-world tasks. | Appendix G, Table 6 |\n| **Performance under Observation Noise (Y2NU)** | We tested HOLLM on Vehicle Safety with noise injected in the function values. HOLLM shows robustness and the performance gap towards the global LLM baseline increases. | Appendix J, Figure 21 |\n| **Computational Complexity (dF12, Y2NU)** | We provided a formal Big-O analysis, concluding that the overhead coming from partitioning and region selection is asymptotically negligible compared to the dominant $\\mathcal{O}(t^2)$ LLM inference cost. | Appendix H, Table 7 |\n\n---\n\nThese extensive additions comprehensively address the reviewers' major concerns, *providing strong empirical and analytical justification* for combining the LLM's with our adaptive partitioning framework. We believe the paper is significantly stronger due to your insightful guidance and are happy to engage further in discussion if you have additional concerns. We respectfully request that you consider raising your score to reflect the substantial improvements made to the submission. Thank you again for your time and expertise."}}, "id": "kwdc5HuFxU", "forum": "y6nhcCdQYd", "replyto": "y6nhcCdQYd", "signatures": ["ICLR.cc/2026/Conference/Submission7956/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7956/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission7956/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763656022017, "cdate": 1763656022017, "tmdate": 1763656022017, "mdate": 1763656022017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}