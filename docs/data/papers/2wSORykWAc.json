{"id": "2wSORykWAc", "number": 7482, "cdate": 1758024053424, "mdate": 1759897850100, "content": {"title": "Prioritizing Faithfulness: Efficient Zero-Shot Novel View Synthesis with Adaptive Latent Modulation", "abstract": "The challenge of camera-controlled novel view synthesis (NVS) lies in balancing high visual fidelity with strict faithfulness to the source scene. We argue that current dominant approaches, which rely on finetuning large-scale diffusion models, often over-emphasize fidelity while struggling with faithfulness due to their generative nature. To address this, we propose a zero-shot NVS pipeline that prioritizes faithfulness and efficiency. Our method introduces two key contributions applied during inference: (1) Test-time Latent Homography Deformation, an on-the-fly homography optimization to deform latents for global motion consistency, and (2) Spatially Adaptive RePaint (SA-RePaint), an extension to RePaint that achieves both structural consistency and texture fidelity by introducing a mathematically-grounded, region-wise balancing of these two objectives. Our evaluations demonstrate substantial improvements in faithfulness and camera accuracy with competitive perceptual scores, highlighting a successful integration of faithfulness, quality, and efficiency. This work offers a promising direction for NVS that rebalances the focus towards greater authenticity.", "tldr": "Faithfulness-oriented zero-shot camera-controlled NVS with video diffusion model", "keywords": ["NVS", "Zero-shot"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/29344fa59ea38fbc9aaf6de03e8d45fd21ddd3ce.pdf", "supplementary_material": "/attachment/50f90f30e41d8e9e3b36e82023e47884b87c64ec.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a training-free method for generative novel view synthesis. The proposed method is based on a video diffusion model and introduces two new components compared to prior art: A inference-time optimization for homographies that are used to warp latents of different video frames, as well as a spatially varying noise level used in the inpainting process. The introduced changes are motivated by apparent 'spurious motion', as well as 'drifting synthesis' artefacts in related methods, where the former refers to subjects not remaining multi-view consistent even though having been observed from a novel view, while the latter refers to image (part) generations not following the camera motion."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a few relatively simple fixes that seem to be nonetheless effective for the task of generative inpainting. \nThe proposed spatially-adaptive RePaint variant seems novel and is well-motivated. Using the cross-attention of the video diffusion model as guidance signal to provide cross-frame correspondences is a neat trick in the absence of a clean, generated RGB video.\n\nThe paper is overall well-structured and provides an extensive amount of analysis and details on the proposed components, which facilitates reproducing the results."}, "weaknesses": {"value": "**Evaluation and Comparisons to Prior Work**\n\n(1)\nThe paper lacks comparison against multiple relevant related works. For example: InvisibleStitch [1], a method that additionally uses depth inpainting, which also resolves the issue of the proposed homography warping being not depth-aware or Stable Virtual Camera [2]. This makes me question whether all relevant related work has been cited and compared against.\n\n(2)\nThe generated novel views shown in the qualitative results are in general less convincing than the prior work Trajectory Crafter, which is also reflected in the quantitative analysis. However, the reported \"Faithfulness\" scores are better for the proposed method, which makes me wonder how these scores were computed. A more thorough explanation on \"valid\" regions used for these metrics would be commendable.\n\n(3)\nWhile the paper provides a comparison to prior works wrt. geometric consistency using TSED, it would be commendable to include MEt3R [3], which is a more robust metric.\n\n\n**Homography Estimation**\n\n(4)\nThe proposed homography estimation can only be computed on the masked co-visible image parts. However, for these parts, the estimated homography should generally be close to Identity, as the initial latent that is noised and subsequently denoised is the depth-warped input image. Related to that, Figure 3b is misleading. The $z_{0|t}$ is denoised from the (already) depth-warped $y$ (as explained in Sec. 4), not the non-warped input image.\n\n**Limited Applicability to More Recent Video Diffusion Models**\n\nThe proposed method does not directly extend to more recent video diffusion models that employ a VAE which introduces temporal compression, as the inpainting masks can then not be directly applied to the latents anymore.\n\n\n**Paper Writing and Figures**\n\nThe paper writing is often not easy to follow. E.g., the description of different RePaint variants, which is an important pre-requisite for the remainder of the paper, is not easy to parse, especially the indexing used.\n\nThe figure quality is generally not great, often pixelated. I would recommend the authors to include figures with increased resolution in a revised version. Also, the section labels in Fig. 3b do not match with the paper sections.\n\n[1] Engstler, Paul, et al. \"Invisible stitch: Generating smooth 3d scenes with depth inpainting.\" 2025 International Conference on 3D Vision (3DV). IEEE, 2025.\n\n[2] Zhou, Jensen, et al. \"Stable virtual camera: Generative view synthesis with diffusion models.\" arXiv preprint arXiv:2503.14489 (2025).\n\n[3] Asim, Mohammad, et al. \"Met3r: Measuring multi-view consistency in generated images.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025."}, "questions": {"value": "Following up on the above mentioned weaknesses, I would like to see clarifications regarding the following points:\n\n(1) \nIs the diffusion model backbone used in the different implementations of related works comparable to the used one in this work? Same goes for the depth estimation model used to compute the initial 3D information.\n\nCould you please include comparisons of your method against all relevant, openly available SOTA methods? E.g., InvisibleStitch or Stable Virtual Camera.\n\n(2)\nPlease detail how the faithfulness scores were computed. Which region was considered \"valid\"?\n\n(3)\nHow does the geometric consistency of generated novel views compare to prior art when evaluated using MEt3R?\n\n(4)\nHow strong is the warping that is usually introduced through the computed homographies? Could be shown through some proxy metric like a histogram of the introduced area change / IoU of warped and original image space."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "89z4QJroLq", "forum": "2wSORykWAc", "replyto": "2wSORykWAc", "signatures": ["ICLR.cc/2026/Conference/Submission7482/Reviewer_WhGA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7482/Reviewer_WhGA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761418688677, "cdate": 1761418688677, "tmdate": 1762919598937, "mdate": 1762919598937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Based on the video diffusion model, the authors propose a training-free novel view synthesis method. Compared to previous state-of-the-art work, they introduce homography optimization and Spatially Adaptive RePaint, demonstrating their effectiveness on datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It is a training-free approach that leverages pre-trained large-scale video diffusion models, promising potential for improved performance as video generation techniques advance.\n2. The generated videos exhibit high visual fidelity and competitive quality compared to other methods."}, "weaknesses": {"value": "1. There is a lack of comparison with Gaussian Splatting."}, "questions": {"value": "Although the primary contribution of the paper is in fidelity comparison, the background in the images of Figure 6 appears overly blurred. It would be better to replace these images with ones that better validate the experimental results' competitive fidelity.  (The figures in the appendix would be much better)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aAYSAAQJzZ", "forum": "2wSORykWAc", "replyto": "2wSORykWAc", "signatures": ["ICLR.cc/2026/Conference/Submission7482/Reviewer_Y4mM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7482/Reviewer_Y4mM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827911022, "cdate": 1761827911022, "tmdate": 1762919598512, "mdate": 1762919598512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a training-free pipeline for NVS task from a single source image, which focuses on the faithfulness to the input while achieving efficiency. The method follows a render-then-inpaint manner: it first lifts the source image to a 3D point cloud, renders views along a specified camera trajectory. Then it inpaints disoccluded regions with SVD via a modified RePaint technique. \n\nThe key contributions include: (1) test-time latent homography deformation, an optimization that aligns latent predictions with rendered images to prevent drifting synthesis and ensure motion coherence, and (2) SA-RePaint, which derives a per-pixel noise map to balance structural consistency and texture fidelity by matching local variances.\n\nThe authors claim that the proposed method outperforms existing methods in faithfulness and camera accuracy with competitive visual quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper proses to rebalance priorities in NVS of faithfulness and efficiency, which is often sidelined in favor of fidelity in generative approaches. The technical details are clearly derived (e.g., the closed-form solution for $\\Sigma$ in Theorem 1) and illustrative figures effectively convey the pipeline and trade-offs. \n\nOverall, this paper is well-written. This work also presents a new perspective for generative NVS scenario where faithfulness is critical and efficiency is also demanded."}, "weaknesses": {"value": "Given this is a training-free method for generative NVS, one weakness could be the assumption of the method.\nThe core method *Test-time Latent Homography Deformation* assumes largely planar or global motions (homography deformation), which may not handle complex parallax, non-rigid, and large-motion scenes well."}, "questions": {"value": "1. How does the method perform on inputs with significant depth variations or dynamic elements, given homography's global nature? Although a brief discussion of the limitation in given in the appendix, some examples of failure cases would help evaluate the limitations of current method.\n\n2. Since the proposed method relies on latent-space manipulations during inference and treats the video diffusion model as a black box, it appears naturally extendable to DiT architectures. Have the authors experimented with applying it to more recent DiT-based video models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xIrHuy1no6", "forum": "2wSORykWAc", "replyto": "2wSORykWAc", "signatures": ["ICLR.cc/2026/Conference/Submission7482/Reviewer_QBDc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7482/Reviewer_QBDc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924560795, "cdate": 1761924560795, "tmdate": 1762919597947, "mdate": 1762919597947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a training-free NVS method aimed at improving faithfulness and geometric consistency in diffusion-based novel view synthesis. It introduces a Latent Homography Deformation module to enforce content coherence and a Spatially Adaptive RePaint mechanism to address the structure-texture trade-off. The proposed method achieves competitive results in both quantitative and qualitative evaluations on several benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This work addresses the challenging faithfulness issues that typically exist in NVS methods relying on diffusion models and manages to mitigate them through a training-free solution.\n\n\n* The idea of applying homography warping in the latent space to address “drifting synthesis” issues is interesting and sound.\n\n\n* The concept of applying per-pixel noise levels is mathematically well illustrated.\n\n\n* The manuscript is well structured and easy to follow."}, "weaknesses": {"value": "* **The ablation study seems confusing.** As reported in Tab. 2, compared to the baseline, the introduced components bring only minor improvements, or even worse results, in terms of faithfulness. This contradicts the motivation of “prioritizing faithfulness.” A more thorough analysis would help clarify this issue.\n\n\n* **The improvements over other comparison models seem to mainly stem from the enhanced baseline.** As shown in Tab. 1, the proposed method performs significantly better than other state-of-the-art models. However, Tab. 2 shows that the final model performs even worse than the baseline. In this case, the superiority of the method may purely come from an improved baseline. It would be more convincing to apply the proposed modules to other baselines, such as Trajectory Crafter, to verify whether these introduced components genuinely contribute to the performance gains.\n\n\n* **The effectiveness of the “prefill” module is not analyzed.** As mentioned in L208, the re-projected image is first prefilled with a classical inpainting method before being fed into the VAE. It would be helpful to show how important this step is. Moreover, applying the same prefill step to other comparison methods, such as Trajectory Crafter, could help ensure a fairer comparison."}, "questions": {"value": "For the comparison figures (e.g., Fig. 6 and Fig. 7), it would be better to include the Ground Truth images to confirm whether the proposed method indeed maintains faithfulness better than others."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wno9tp9QHI", "forum": "2wSORykWAc", "replyto": "2wSORykWAc", "signatures": ["ICLR.cc/2026/Conference/Submission7482/Reviewer_5Z8V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7482/Reviewer_5Z8V"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009177892, "cdate": 1762009177892, "tmdate": 1762919597263, "mdate": 1762919597263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}