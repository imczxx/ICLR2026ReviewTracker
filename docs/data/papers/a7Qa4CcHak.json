{"id": "a7Qa4CcHak", "number": 14189, "cdate": 1758230079920, "mdate": 1759897385198, "content": {"title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces", "abstract": "AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 1.5: a carefully curated hard benchmark composed of 74 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 50% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work.", "tldr": "Terminal-Bench is a framework for creating hard, valuable, and realistic agent benchmarks", "keywords": ["benchmark", "dataset", "agents"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/67bc9c8db7bbfc6f39bdac9e427da21e9afe2065.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The work contributes a new benchmark, with 74 tasks across a range of domains, that test realistic terminal-based software capability. Each task is reasonably carefully reviewed and has undergone several quality checks. While there are no human baselines of these tasks, the creation process involves time and difficulty estimates. The agents tested perform reasonably well, providing confidence that there is some signal, while also not being close to saturation."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Overall, the paper is exceptionally strong:\n* It is well-presented, with clear information about the tasks, the task creation process, and agent testing methodology. \n* The tasks are well-reviewed, lending confidence that the benchmark is of high quality.\n* A large number of agents were tested\n* Failure analysis provides valuable information."}, "weaknesses": {"value": "The paper would have been stronger if:\n* we had human baselines (i.e. an expert or junior engineer was asked to complete the task in the same conditions), instead of time estimates as these might over/under estimate the actual time required (famously this is quite hard to accurately do)."}, "questions": {"value": "* What happened to the other tasks from the 229 that were not selected? Did they not pass quality checks? \n* Did you verify that the LLM judge that was used in failure analysis was accurate by manually verifying a small sample?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "U3DT6ZQNoD", "forum": "a7Qa4CcHak", "replyto": "a7Qa4CcHak", "signatures": ["ICLR.cc/2026/Conference/Submission14189/Reviewer_ptwJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14189/Reviewer_ptwJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922704531, "cdate": 1761922704531, "tmdate": 1762924644281, "mdate": 1762924644281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Terminal_Bench, a benchmark for evaluating AI agents' capabilities in terminal environments. The authors build a test suite containing various real-world terminal tasks, spanning file operations, system administration, and development workflows. Through systematic experiments on mainstream language models, the work reveals that even state-of-the-art models exhibit significant limitations when handling complex terminal interactions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Comprehensive task coverage**: The benchmark spans multiple levels from basic file operations to complex system configurations, offering good representativeness of real-world scenarios\n2. **Automated evaluation pipeline**: Establishes a fairly complete automated testing and scoring infrastructure, reducing subjectivity in assessment\n3. **Thorough empirical validation**: Systematic comparison across multiple mainstream models effectively reveals current technological limitations\n4. **Strong reproducibility**: Detailed task descriptions and evaluation scripts facilitate adoption and extension by other researchers\n5. **Insightful error analysis**: Categorizes and discusses failure modes, providing useful guidance for future improvements"}, "weaknesses": {"value": "1. **Limited diversity in certain categories**: While covering multiple task types, some categories (network configuration, distributed system management) have sparse representation, potentially failing to capture the full spectrum of real-world usage\n2. **Narrow evaluation metrics**: Predominantly relies on binary pass/fail assessment, lacking consideration of execution efficiency, code quality, and other important dimensions. The scoring mechanism for partial completion is insufficiently granular\n3. **Static environment limitations**: Testing environments are relatively static, not adequately simulating dynamic environmental changes, concurrent operations, and other complexities found in production settings\n4. **Insufficient treatment of user interaction**: Terminal operations frequently require user confirmation and interaction, but the benchmark's design doesn't adequately address this aspect\n5. **Security considerations absent**: The paper lacks substantive discussion of security risks associated with AI agents executing terminal commands, and doesn't propose corresponding safety mechanisms"}, "questions": {"value": "1. For complex multi-step tasks with state dependencies, how do you ensure evaluation fairness? Different execution paths may achieve the same goal - how does your system handle this variability?\n2. What was the methodology for task selection and design? Was there user research or expert consultation to validate task representativeness?\n3. For failed executions, does the evaluation system credit partially correct steps? How do you quantify degrees of \"near-correctness\"?\n4. Real-world terminal environments vary across system versions and configurations - how does the benchmark account for this environmental diversity?\n5. Have you considered extending the benchmark to include security testing, such as detecting whether models might execute potentially dangerous commands?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6FfzZ9DTkL", "forum": "a7Qa4CcHak", "replyto": "a7Qa4CcHak", "signatures": ["ICLR.cc/2026/Conference/Submission14189/Reviewer_xXm9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14189/Reviewer_xXm9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998145459, "cdate": 1761998145459, "tmdate": 1762924643771, "mdate": 1762924643771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a collection of 74 new agentic tasks that can be completed entirely using the terminal. The tasks were built using a crowdsourcing process with multiple steps of verification and testing. The performance of 18 different models across six different agent scaffoldings were tested on the tasks, showing a maximum average resolution rate of slightly below 35%. An error analysis was performed at both the trajectory and command level, showing that models display a balanced variety of errors when completing tasks, not indicating any single important bottleneck."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Thorough testing and verification process throughout task development, including algorithmic, LLM-powered, and human review.\n1. Builds on standards/checklists from prior work, like MAST and ABC\n1. Used a collaborative crowd sourcing process to build a relatively large number of diverse and difficulty tasks\n1. Developing a simple, shared scaffold as a reasonable point of comparison\n1. Detailed error analysis"}, "weaknesses": {"value": "1. It's possible that the comparisons would be more fair if the best/preferred scaffold for each agent were used rather than a shared scaffold (which might still favor one model over others).\n1. An LLM judge for error analysis was chosen based on agreement with a human annotator on 20 traces, but there are 74 tasks. I think this leaves open a large possibility of sampling bias.\n1. \"Most agents attempt tasks for less than 20 minutes.\" This seems very limiting, when most of the tasks are estimated to take humans at least an hour. Models are known to suffer from early stopping in agent tasks, it seems this was not controlled for.\n1. The tasks are all public, limiting the longevity of this benchmark"}, "questions": {"value": "1. \"We find that command failures calling executables that are not installed or not in PATH are the most frequent.\" Are you sure that the environments or agent scaffolds don't simply have bugs? Are the environments missing standard linux packages? Did you have humans try to complete the tasks in the same environment as the agents?\n1. Have you collected data from agents while attempting to limit their tendency to submit early?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PSgferXQXp", "forum": "a7Qa4CcHak", "replyto": "a7Qa4CcHak", "signatures": ["ICLR.cc/2026/Conference/Submission14189/Reviewer_56KK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14189/Reviewer_56KK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999153900, "cdate": 1761999153900, "tmdate": 1762924643157, "mdate": 1762924643157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}