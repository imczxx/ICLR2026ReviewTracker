{"id": "Q3MisVkuTu", "number": 12154, "cdate": 1758206019650, "mdate": 1763723401443, "content": {"title": "Oversmoothing, \"Oversquashing'', Heterophily, Long-Range, and more: Demystifying Common Beliefs in Graph Machine Learning", "abstract": "After a renaissance phase in which researchers revisited the message-passing paradigm through the lens of deep learning, the graph machine learning community shifted its attention towards a deeper and practical understanding of message-passing's benefits and limitations. In this paper, we notice how the fast pace of progress around the topics of oversmoothing and oversquashing, the homophily-heterophily dichotomy, and long-range tasks, came with the consolidation of commonly accepted beliefs and assumptions that are not always true nor easy to distinguish from each other. We argue that this has led to ambiguities around the investigated problems, preventing researchers from focusing on and addressing precise research questions while causing a good amount of misunderstandings. Our contribution wants to make such common beliefs explicit and encourage critical thinking around these topics, supported by simple but noteworthy counterexamples. The hope is to clarify the distinction between the different issues and promote separate but intertwined research directions to address them.", "tldr": "This paper challenges some of the common beliefs and assumptions in the graph machine learning community.", "keywords": ["oversmoothing", "oversquashing", "heterophily", "long-range propagation", "graph neural networks", "graph machine learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6f04f7ef2880368a084be95e550685887d0061e3.pdf", "supplementary_material": "/attachment/9bfe550f1dc67fcfba23dc3708cd52d744df0ada.zip"}, "replies": [{"content": {"summary": {"value": "This paper clarifies recent concepts in the graph machine learning community, such as oversmoothing and heterophily, and raises simple but noteworthy counterexamples."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well motivated as a position paper, covering a wide range of recent trends in graph machine learning.\n2. The paper is well structured, clarifying the nine beliefs step by step."}, "weaknesses": {"value": "1. Section 3.1: The example is too specific—showing a heterophilic graph where a DGN can achieve perfect classification. Most examples in this paper are toy problems that can be solved by simple graph functions (degree, distance). These problems don't require DGNs.\n2. Some explanations are insufficient. For example, in Section 3.3, the authors argue that distinguishing the task is what really matters. However, there's no further analysis of which graph tasks should consider heterophily/homophily or distance. The argument relies on only two specific examples (degree, distance). As a position paper that may direct the future of this area, a deeper dive into this question is necessary.\n3. Line 358: Is it correct to refer to Figure 2? Also, please explain **message filtering** in Errica et al. (2025) and Figure 4 (middle) more clearly, as readers may not be familiar with it."}, "questions": {"value": "1. Could you provide real-world examples of interactions between features, structure, and class labels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1VZbdOIKrl", "forum": "Q3MisVkuTu", "replyto": "Q3MisVkuTu", "signatures": ["ICLR.cc/2026/Conference/Submission12154/Reviewer_SstW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12154/Reviewer_SstW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761519405324, "cdate": 1761519405324, "tmdate": 1762923110774, "mdate": 1762923110774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission under review argues that several “folklore beliefs” in graph ML — oversmoothing, oversquashing, homophily vs heterophily, long-range reasoning — are overstated or misinterpreted. It provides counterexamples, simple constructions, and some empirical plots (e.g., 1–64 layer GNNs on Cora with different scalings). \n\n12154_Oversmoothing_Oversquash\n\n The paper also proposes to retire the overloaded word “oversquashing” and instead talk about “computational bottlenecks” vs “topological bottlenecks,” which it claims are distinct."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper openly challenges sloppy community narratives about depth, heterophily, and “oversquashing,” and reminds readers that degradation in deep GNNs is not the same thing as classic oversmoothing, and that heterophily alone does not doom GNNs. \n2. The paper proposes to split “oversquashing” into two ideas: “computational bottlenecks” (information from exponentially many nodes being crammed into a fixed hidden vector); “topological bottlenecks” (structural choke points / narrow cuts in the graph)."}, "weaknesses": {"value": "1. The primary claims of this paper overlap significantly with established findings in the GNN literature.\n* On Over-smoothing: The observation that removing feature transformations and non-linearities can boost performance in moderately deep GNNs has been demonstrated by prior work, such as SGC [1] and a KDD'22 paper [2]. These studies have already established that performance degradation in models of the depth used in this paper (e.g., 64 layers) is often attributable to feature transformation and non-linearity, rather than over-smoothing alone, which typically manifests in much deeper architectures.\n* On Heterophily: The challenges of heterophily have been extensively analyzed. A well-known paper [3] provides a comprehensive treatment of this topic. Unfortunately, the current work does not seem to offer new perspectives or insights beyond what was presented in [3].\n------\n2. Given that the paper addresses topics with a rich history of research, a deeper investigation from novel perspectives or a rigorous theoretical analysis would be expected. However, the current analysis lacks the necessary depth.\n* The investigation into over-smoothing is confined to a few simple settings on a single dataset (Cora). This limited experimental setup is not convincing enough to reveal generalizable patterns or provide robust evidence for the claims made.\n* The discussion on heterophily relies on reusing a toy example from [3] without introducing new conceptual or empirical contributions. A more compelling argument would require novel experiments or theoretical formulations.\n\n------\n3. About the statement ‘if different classes imply different feature distributions, why would one need a DGN rather than a simple MLP?’\n\nI also disagree with the way that’s phrased in the submission.\nEven if different classes have different feature distributions, classification boundaries may still be ambiguous (e.g., overlapping Gaussians, noisy features, etc.). A plain MLP on node features alone may misclassify nodes in that overlapping region.\nA message-passing layer can still add value by smoothing / denoising via neighbors: even if node i’s own features are ambiguous, aggregating its neighbors’ features can push it toward the correct side of the decision boundary. This is exactly the classic semi-supervised “feature smoothing” or “label propagation” intuition behind GCNs.\n\nSo “why would you ever need a GNN if features are already somewhat class-dependent?” is a false dichotomy. You still may want graph convolution to regularize local decision boundaries and reduce local noise — especially when each node alone is borderline but its neighborhood is consistent.\n\n----\n4. Difference between “computational bottleneck” and “topological bottleneck” is unclear. As a core concept to support your claims, it is not wise to put it to appendix. The main text should be self-contained."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "quvvl2vARf", "forum": "Q3MisVkuTu", "replyto": "Q3MisVkuTu", "signatures": ["ICLR.cc/2026/Conference/Submission12154/Reviewer_xBYT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12154/Reviewer_xBYT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552720026, "cdate": 1761552720026, "tmdate": 1762923110368, "mdate": 1762923110368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes nine common beliefs in Deep Graph Networks concerning issues such as over-smoothing, over-squashing, homophily/heterophily, and long-range tasks. It systematically elucidates their origins, identifies misconceptions through experimental validation and counterexamples, and aims to correct erroneous understandings, thereby contributing to the advancement of the graph learning field."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The article systematically examines prevalent misconceptions and imprecise definitions within the field of DGN. It offers a reflective analysis aimed at clarifying commonly misunderstood concepts such as over-smoothing and over-squeezing, thereby facilitating researchers' comprehension and preventing misinterpretations. This critical review encourages ongoing reflection on entrenched perspectives, fostering a more rigorous and accurate understanding in the domain.\n\nS2. The paper refines the understanding of the \"over-squashing\" problem, elucidating the relationships among over-smoothing, over-squashing, homophily/heterophily, and long-range tasks. By redefining these issues and their key aspects, the discussion aims to promote advancements in the development of DGN methodologies."}, "weaknesses": {"value": "W1. In refuting the claim that OSM is a property of all DGNs, the article relies solely on a simple experiment, which lacks rigorous proof and suffers from limited experimental evidence. The experiment, on one hand, uses only the Cora dataset, thereby failing to establish the universality of the conclusion; on the other hand, certain inferences merely suggest that the DE and RQ metrics, which reflect smoothing, are insufficiently convincing, yet they do not demonstrate that OSM is inevitably unavoidable. It is recommended that the authors provide more rigorous theoretical justification or more comprehensive experimental evidence when discussing the underlying causes of OSM.\n\nW2. The definitions of \"computational bottleneck\" and \"topological bottleneck\" rely primarily on intuitive explanations, lacking a unified mathematical framework or quantifiable metrics."}, "questions": {"value": "Q1. The paper suggests that research should focus on node separability rather than OSM metrics. Could you provide a novel, quantifiable \"node separability metric\" as an alternative?\n\nQ2. Refining the tasks of distinguishing \"computational bottlenecks\" from \"topological bottlenecks\" is meaningful. I hope the authors can further discuss whether there are actual examples of such distinctions in real-world graph data, whether these distinctions are feasible, and how to differentiate them: is it possible to propose theoretically distinguishable metrics?\n\nQ3. Do the conclusions drawn in the paper remain valid in the context of Transformer-based GNNs or Graph Diffusion Models?\n\nQ4. The paper highlights that certain common misconceptions lack universality. Could the authors further elaborate on the scope of applicability for traditional beliefs? For example, in Section 3.1, the counterexample \"if a node is at a distance greater than five from a specific node\" relates solely to the topological properties of the graph, independent of homophily or heterophily. Under what circumstances is it meaningful to discuss homophily and heterophily?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HUCqYzyxzW", "forum": "Q3MisVkuTu", "replyto": "Q3MisVkuTu", "signatures": ["ICLR.cc/2026/Conference/Submission12154/Reviewer_4Aqk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12154/Reviewer_4Aqk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136052795, "cdate": 1762136052795, "tmdate": 1762923109714, "mdate": 1762923109714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits and critiques nine widely discussed beliefs in the field of Graph Machine Learning (GML), covering issues such as over-smoothing (OSM), over-squashing (OSQ), homophily/heterophily, and long-range dependency. Through a series of counterexamples, the authors reveal common misconceptions, for instance, that over-smoothing is not inevitable; it depends on model architecture, hyperparameters, and evaluation metrics, and bears no direct causal relation to performance. The paper aims to clarify these misunderstandings and calls for a more rigorous research in graph learning."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper’s central motivation—to critically examine and challenge long-held opinions and assumptions in the GML community—is very interesting and valuable.\n\n2. The writing is clear and well-structured, using intuitive examples and accessible language to convey complex ideas effectively."}, "weaknesses": {"value": "1. Lack of justification for the representativeness of rebutted claims. The paper does not convincingly establish whether the criticized viewpoints truly represent mainstream consensus in the field. For instance, is the claim that “all GNNs inevitably suffer from over-smoothing” genuinely a widely accepted belief, or is it only mentioned in a limited subset of works? To make the critique compelling, the authors should first demonstrate the prevalence and influence of these viewpoints in prior literature.\n\n2. Insufficient theoretical and empirical validation. The arguments rely heavily on qualitative reasoning and illustrative examples, with very limited mathematical formalism or empirical validation. I am not convinced. To strengthen credibility, the paper should incorporate formal theoretical derivations, quantitative experiments, or counterfactual analyses that can substantiate its claims.\n\n3. The manuscript attempts to address too many issues simultaneously, resulting in a lack of depth and weakened persuasiveness. I strongly recommend focusing on one or two truly dominant misconceptions and providing a rigorous, well-supported, and empirically validated analysis of these specific points. For example, the classic paper “Adversarial examples are not bugs, they are features” offers a good example: it concentrates on a single misconception and dismantles it thoroughly through both theoretical modeling and empirical evidence.\n\n4. Several arguments (e.g., regarding differences between homophilic and heterophilic graphs) have already been discussed—explicitly or implicitly—in prior works. For example, the development of heterogeneous graph neural networks (HGNNs) stems from recognizing that aggregation mechanisms in homophilic graphs are inadequate for heterophilic structures."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qWJkImQI1k", "forum": "Q3MisVkuTu", "replyto": "Q3MisVkuTu", "signatures": ["ICLR.cc/2026/Conference/Submission12154/Reviewer_348B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12154/Reviewer_348B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762157348267, "cdate": 1762157348267, "tmdate": 1762923109122, "mdate": 1762923109122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Comment to Reviewers: Part 1"}, "comment": {"value": "We would like to provide a general comment to all reviewers, whom we deeply thank for their efforts. **We have uploaded a revised PDF version of the manuscript.** Below, we provide a general answer for the reviewers summarizing our clarifications.\n\nWe understand that this paper do not align with classical mainstream contributions of current research papers (new technical method and experiments), but we believe that the level of technical contribution required to conduct this analysis is high, and that the analysis performed is of high significance for the community (identify clear beliefs in the literature + show clear counterexamples + provide future lines of research). We believe that it is crucial to better clarify the scope of our work, before commenting on the concerns raised by the reviewers. We hope the reviewers will do the same with our answers.\n\nThank you again and we hope to engage into a constructive and meaningful discussion together.\n\n**_Scope_**\n\nWe appreciate the feedback and would like to clarify the paper's goal. Our aim is to raise the standard of conceptual clarity in GML by identifying, across a broad body of literature, nine prevalent beliefs, making their underlying assumptions explicit, and disproving **universal claims through counterexamples**, while pointing to prior rigorous theoretical and empirical results that are available but not as well-known. For beliefs stated as universal claims, a single valid counterexample suffices to refute the universal form. **Counterexamples effectively challenge universal claims, do not need to be complex, and are sufficient for refutation, which is intentional** and fundamental to our contribution.\n\n\n\nSpecifically, our paper offers a critical analysis: we (i) examine the assumptions behind each belief, (ii) disprove or qualify them using counterexamples and references to previous theoretical or empirical findings, and (iv) propose future research directions. This contribution does not require introducing a general method or benchmark to be of value to the graph machine learning community, which is the ultimate aim of this work. We hope the reviewers will acknowledge this.  \nWe also reckon the paper is timely and helpful, as it prevents misleading narratives (e.g., “OSM leads to accuracy drop”) from further spreading, thus steering the community's thinking and effort towards more impactful and solid directions.\n\n**_Representativeness_**\n\nRegarding the representativeness of the beliefs, for each claim, we compile numerous highly-cited papers that support or rely on it (e.g., 36 works supporting “OSM is a property of all DGNs” with about 17k citations; roughly 30 works on “OSM causes performance drops” with approximately 16k citations). We also cite sources that question these beliefs, demonstrating community engagement and importance. This is the core prerequisite for a demystification paper, and we believe we have met it.\n\n**_“Already known results”_**\n\nPrecisely because some of the beliefs are partially mentioned in some prior work, the field-level synthesis and boundary-setting are new and valuable. The reviews do not identify a prior work that (i) enumerates these nine beliefs, (ii) documents their prevalence, and (iii) systematically separates the valid core from the overstated universal claim by providing counterexamples. If the contributions now seem obvious to the reviewers, it is because we have achieved our goal of disseminating these concepts altogether. However, we have reasons to believe, by talking to many expert colleagues, that these concepts are not generally obvious nor well-known at all.\n\n\n**_Evidence, technical contributions, and insufficient validation_**\n\nWe understand that some reviewers have complained about the absence of novel technical or quantitative analyses, but this is a different kind of work.. Once more, the main contribution corrects universalized technical folklore. For such claims, **refuting the universal claims by counterexamples and references to existing theories and experiments is an appropriate strategy**. We have always included rigorous references and simple counterexamples to clarify assumptions, along with quantitative experiments and formalizations where needed. We cannot report all the theoretical conclusions from previous work in this paper, so we cite them to give readers the opportunity to dive deep into a specific topic. Additionally, the paper offers an important theoretical contribution by proposing to distinguish two bottlenecks in Oversquashing: a computational one related to receptive field growth and a topological one related to graph structure. Following R4Aqk's suggestion, we moved these definitions to the main paper and properly cited the appendix."}}, "id": "tLY5Z03U1B", "forum": "Q3MisVkuTu", "replyto": "Q3MisVkuTu", "signatures": ["ICLR.cc/2026/Conference/Submission12154/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12154/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission12154/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763712034645, "cdate": 1763712034645, "tmdate": 1763713611857, "mdate": 1763713611857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}