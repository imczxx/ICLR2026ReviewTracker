{"id": "QPIhPfeRmb", "number": 23179, "cdate": 1758340603788, "mdate": 1759896828308, "content": {"title": "SynSQL: Synthetic Database Generation for Robust Evaluation of Text-to-SQL Systems", "abstract": "A central challenge in test-time scaling for text-to-SQL is generating test databases that can reliably validate arbitrary queries, yet existing tools remain narrow in scope and limited in capability. We introduce SynSQL, a framework for synthesizing test databases conditioned on natural language questions and schema structure. Unlike prior approaches that generate data from gold queries, SynSQL leverages large language models to generate tables directly from question–schema alignment, while remaining compatible with gold queries when available for evaluation. The framework consists of a schema selector, a synthesizer, and a critic with iterative refinement, which jointly align semantic cues from the question with structural constraints from the schema to guide database synthesis. Experiments on the Spider and BIRD benchmarks demonstrate that SynSQL produces realistic, constraint-respecting databases that effectively stress-test text-to-SQL models. SynSQL not only complements the coverage of human-curated benchmarks but also outperforms prior test database generation methods across diverse schema complexities. On Spider, SynSQL achieves a 93.04% success rate, surpassing the original human-authored dataset (92.55%), and on BIRD it attains a 79.23% agreement rate, substantially higher than prior automated methods, all while operating without access to gold queries during data generation.", "tldr": "SynSQL is an LLM-driven framework for generating test databases to evaluate text-to-SQL systems using natural language questions and schemas. It complements existing human-curated datasets and outperforms prior test database generation methods.", "keywords": ["Text-to-SQL", "Test Data Generation", "Synthetic Data Generation", "Natural language Interfaces to Databases", "SQL Testing"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e06fa09c2585a6ff75a733e91dc4e78935f004e9.pdf", "supplementary_material": "/attachment/e3336cb4b57c5e194cd95a78d68e30b4afad993a.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a method to automatically generate synthetic databases for text to SQL evaluation using only the natural language question and the schema. The approach enforces schema constraints such as primary and foreign keys while generating realistic data. It uses a three step approach with schema selection, data generation, and a critic that applies feedback recursively up to three times. The synthetic datasets generated by GPT-4.1-mini, Gemini-2.5-Flash, and Qwen-3-8B are then used to evaluate text to SQL models DIN-SQL and DAIL-SQL and are compared against human curated datasets like Spider and BIRD. The authors argue that this method reduces dependency on manually curated data and enables evaluation in privacy sensitive or cold start scenarios. The experiments show that the generated data shows alignment with human curated datasets and can respect schema constraints better than human curated datasets in some cases."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The approach targets cold start and privacy preserving settings where only the schema and the question are available.\n\n- By enforcing schema constraints during generation, the method reduces errors common in human curated datasets and addresses a valid concern in benchmark design.\n\n- The paper identifies weaknesses in existing text to SQL datasets like inconsistencies (extra space, lower/upper case)."}, "weaknesses": {"value": "- The evaluation uses DIN-SQL and DAIL-SQL and older dataset versions, while newer versions such as Spider 2 and BIRD interact are not used, which limits relevance given the rapid progress in LLMs.\n\n- The data generators are weaker models, and stronger models might need less engineering to generate a synthetic dataset of good quality. While using a cheap model is desirable, the cost of generating an evaluation dataset with a strong model is lower because the dataset is generated infrequently.\n\n- The experimental design varies three weak generators and evaluates only two older systems. A stronger design would use a strong generator to produce a dataset  and then benchmark multiple systems on it.\n\n- The content and construction of the auxiliary knowledge K are not described, although the code and the appendix show it is evidence on BIRD and empty on SPIDER. This should be specified in the main text.\n\n- The post processing step may change semantics due to padding with NULLs or truncation, and its effects on the evaluation are unclear.\n\n- Main results lack details about vanilla baselines and how they are used to generate datasets.\n\n- The effects of schema reduction are insufficiently discussed, since extra columns might increase difficulty for model evaluation, and reducing to a subset may cause queries that work on the original dataset to fail (in other words they use the extra columns but get the correct output).\n\n- The prompts for the critic and the synthesizer include the placeholder {ONE_EXAMPLE}, and the source of this example is not specified.\n\n- The data validation step prior to the data critic might change the generated data, and it seems the results of this step are not provided back as feedback."}, "questions": {"value": "- The column expansion prompt enforces a limit of 3 columns. Is this step applied once or recursively?\n\n- When using DIN-SQL and DAIL-SQL, is the model GPT-4 as in the original work?\n\n- When generating inserts, if the number of values does not match the number of columns, is there a reason not to use structured output to enforce these constraints?\n\n- For the data critic, what was the reason to use a 1 to 10 score instead of a simple pass or fail for each criterion?\n\n- Minor typo, in line 451, 2 should be Figure 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WI0lAw8CiD", "forum": "QPIhPfeRmb", "replyto": "QPIhPfeRmb", "signatures": ["ICLR.cc/2026/Conference/Submission23179/Reviewer_wLfc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23179/Reviewer_wLfc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708991682, "cdate": 1761708991682, "tmdate": 1762942547700, "mdate": 1762942547700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduced SynSQL, a framework for generating test databases for text-to-SQL tasks by synthesizing tables directly from natural language questions and schema structure, rather than relying on gold queries. Experiments on Spider and BIRD benchmarks show that SynSQL outperforms prior automated methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper offers a fresh perspective on SQL data sythesis, which is more general and requires less input data. \n- The synthetic dataset shows on-par or even better results with human curated dataset.\n- Fully automated, easy to scale"}, "weaknesses": {"value": "- Baseline is very naive and not well established. Many comparsions are done against vanilla GPT4.1-mini and the gains are not surprising. Also can the authors elaborate more on the choice of DIN-SQL and DAIL-SQL vs the rest?\n- Enforcing case sensitivity doesn't correctly reflect the real-world cases. The synthetic dataset / query can be much cleaner than the real ones."}, "questions": {"value": "- When generating the relevant schema, why use different temperatures instead of multiple-sampling with a higher temperature or other more common sampling approach?\n- The synthesizer does not rely on reference queries, then how to make sure the result is differentiable and minimal?\n- Discussion or comparison with methods that generate synthetic natural language or SQL queries will be a lot interesting, since these two line of approaches are directly related."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TaRa3eDmbY", "forum": "QPIhPfeRmb", "replyto": "QPIhPfeRmb", "signatures": ["ICLR.cc/2026/Conference/Submission23179/Reviewer_wjhd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23179/Reviewer_wjhd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973323226, "cdate": 1761973323226, "tmdate": 1762942547388, "mdate": 1762942547388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a framework, SynSQL, to synthesize test databases by using LLMs for question understanding and aligning semantics with schema constraints. This framework has three stages: a schema selector, synthesizer, and critic.  The schema selector identifies relevant schema components and reduces the schema space. The synthesizer generates the test data based on the natural language question and reduced schema. Then, the critic reviews the quality of generated data and provides feedback for improvement. The resulting synthesized data complements the coverage of human-curated benchmarks and achieves higher success rate and agreement rate compared to human-authored datasets and past test generation methods respectively."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The problem of generating a good-coverage test data is well motivated.\nS2. The design of SynSQL is reasonable, especially the inclusion of Critic."}, "weaknesses": {"value": "W1. If the framework's performance is generally comparable to a human-generated benchmark, the contribution might be considered incremental. To enhance the framework's usefulness, the resulting benchmark should create more accurate and challenging scenarios that can better expose the limitations of current methods.\n\nW2. Although the framework is well-designed, there is a risk that LLMs may fail to diversify and instead repeat on a narrow range of prompt types. This could lead to the issue mentioned in lines 217-219 (about \"too many WHERE operations\" or other operations outside the scope), but instead this behavior can result in higher success rate and not as much prompt diversity."}, "questions": {"value": "Q1. What kinds of critic does the model normally give? The prompt in A.9 includes \"sufficient complexity\". How does the model usually interpret that? I ask this to understand how likely W2 might occur and whether the model is able to identify such a situation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LuorfdhYqk", "forum": "QPIhPfeRmb", "replyto": "QPIhPfeRmb", "signatures": ["ICLR.cc/2026/Conference/Submission23179/Reviewer_FQEo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23179/Reviewer_FQEo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996674338, "cdate": 1761996674338, "tmdate": 1762942547084, "mdate": 1762942547084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents SynSQL, a novel framework designed to create synthetic evaluation test databases for Text-to-SQL systems. The approach assumes input consisting of a natural language (NL) question, a database schema, and associated knowledge. The framework features a modular design: a Schema Selector to focus on relevant columns; a Synthesizer that leverages Large Language Models (LLMs) to populate tables based on question semantics; and a Critic with an iterative refinement loop to ensure data integrity and quality. Experiments conducted on the widely used Spider and BIRD benchmarks successfully demonstrate that SynSQL produces effective test data that complements human-curated datasets. Furthermore, the framework notably outperforms a prior automated baseline, validating its utility for robust Text-to-SQL evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a central, challenge in Text-to-SQL evaluation: the static nature of current benchmarks. \n- The framework successfully generates robust test databases without relying on the gold SQL query, which is a significant advancement over prior work and enables broader applicability in cold-start or privacy-sensitive scenarios."}, "weaknesses": {"value": "- The paper acknowledges that the Schema Selector may sometimes omit relevant tables or columns leading to gold queries returning empty results. This indicates that the recall-prioritizing ensemble expansion is not fully robust, especially when dealing with ambiguous schema mappings.\n- The framework’s reliance on LLMs for both data synthesis and the critic feedback loop makes it sensitive to: Misinterpretation of Question Intent: 32.1% of failures were attributed to SynSQL's misalignment with the expected gold query, such as case-sensitivity mismatches or generating overly specific values."}, "questions": {"value": "The ablation study highlights that the Schema Selector is critical, yet the error analysis shows that Schema Selection Omission accounts for nearly half of the failure cases (47.6%). Could the authors elaborate on the qualitative difference between selection failures caused by the \"w/o Expansion\" variant and the full SynSQL variant? Furthermore, how can the framework be practically extended to ensure higher recall on complex, sparsely connected schemas without dramatically increasing the column count and sacrificing minimalism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NGgTxdyc8M", "forum": "QPIhPfeRmb", "replyto": "QPIhPfeRmb", "signatures": ["ICLR.cc/2026/Conference/Submission23179/Reviewer_vdvq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23179/Reviewer_vdvq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152986736, "cdate": 1762152986736, "tmdate": 1762942546741, "mdate": 1762942546741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}