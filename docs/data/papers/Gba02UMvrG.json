{"id": "Gba02UMvrG", "number": 18632, "cdate": 1758289626120, "mdate": 1759897090709, "content": {"title": "Soft-Masked Diffusion Language Models", "abstract": "Diffusion models have demonstrated strong potential in language modeling, offering various advantages over traditional autoregressive approaches. \nTheir ability to generate and revise entire responses in parallel enables faster generation and built-in self-correction mechanisms.\nMost modern diffusion-based language models employ masked diffusion, where decoding involves iteratively processing masked tokens based on a binary decision: either retaining the mask or replacing it with the predicted token. \nHowever, this binary choice discards valuable predictive information when the mask is retained.\nTo address this limitation, we introduce \\textit{soft-masking (SM)}, a novel method that dynamically blends the embedding of the mask token with the embeddings of the top-$k$ predicted tokens from the previous decoding step, for each retained mask.\nThis provides the model with a more informative prior, preserving context from earlier computations and allowing partial information about masked tokens to propagate beyond a single step. \nWe propose a training methodology that adapts a pretrained masked diffusion language model to incorporate SM.\nWe demonstrate that continuing pretraining a 169M parameter model with SM leads to improved perplexity and MAUVE scores.\nFurthermore, we finetune two state-of-the-art diffusion models, Dream-7B and Dream-Coder-7B, with SM.\nSM consistently improves performance across multiple coding benchmarks, particularly in high-throughput settings.", "tldr": "We present soft-masking, a new method that improves masked diffusion language models by blending mask tokens with predictions from previous iterations to better preserve context.", "keywords": ["Masked diffusion language models", "continuous feedback", "code generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c4f6f8326716a0e6117246b2a4819af91da806e3.pdf", "supplementary_material": "/attachment/746d4bcaa096ad245cfe45f4e13fbda97ea76fba.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a soft-masking algorithm for masked diffusion models, replacing static one-hot encodings in inputs with dynamic, context-dependent representations for mask tokens. The algorithm blends continuous feedback from an auxiliary neural network with one-hot encodings, using a confidence-based weighting strategy based on the denoising network's probability outputs at masked dimensions.\n\nThe authors assessed their methodology from two perspectives:\n1. Language Modeling: They investigated if their method could improve the performance of pre-trained MDLMs through additional training steps with SM.\n2. Coding Task Performance: They evaluated the method's effectiveness on coding tasks, specifically when constrained by a limited computational budget after fine-tuning.\n\nAcross both benchmarks, the proposed method demonstrated good enhancements in sample efficiency. Furthermore, the authors emphasized the consistent improvement in performance, particularly when faced with a constrained computational budget during inference.\n\nAlthough the authors claim that this method is suitable for pre-training diffusion language models, no evidence about the effectiveness has been provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed method is sound, yielding strong improvements in the fine-tuning tasks. Moreover, the evaluation is well designed."}, "weaknesses": {"value": "While the idea and novelty are good, the analysis of the underlying mechanism of SM is insufficient. In particular, table 5 shows mixed results for SM feedback with finetuning compared to binary baselines at high compute budgets. Similarly, in Table 2, SM with finetuning underperforms Dream-7B at NFE=1/1.\n\nMy intuition is that soft-masking's \"activation\" of top-k tokens creates a biased learning environment, which could aid constrained inference but hinder generalization. Binary feedback, conversely, offers unbiased exploration for better generalization, albeit with higher computational cost. Analyzing soft-mask probability distribution during inference could clarify the algorithm's mechanism.\n\nThis method, if its low-compute gains stem from greedy top-k token exploitation, might be better suited for finetuning or post-training MDLMs, rather than pre-training from scratch as claimed.\n\nOther weaknesses:\n\n1. In Table 1, in most cases, binary feedback with additional 100k step training performs worse than the baselines with only 1M pre-training steps on MAUVE. Do the authors have any explanation for these results?\n\n2. Following 1, the baseline results on MAUVE with binary feedback seem inconsistent with those reported in Table 1 in [1]. Please can you elaborate?\n\n3. In addition to MAUVE, it’s worth reporting validation perplexity as well, which is a standard metric of language modeling. If the perplexity was biased by the confidence-based weighting, which is a greedy strategy, it’ll be useful to know how much bias this strategy can bring in.\n\n4. Lacking a comprehensive comparison against a wider range of models in language modeling, including AR, other MDLMs with different model sizes.\n\n\n[1] https://arxiv.org/abs/2503.00307"}, "questions": {"value": "1. Please can you also provide more detail about inference with SM?\n\n2. What do the two sub-columns under Binary-Feedback in Table 4 and 5 correspond to?\n\n3. To make the presentation consistent, it’ll be better to change the absolute NFE numbers in Table 1 to their corresponding ratio, same as in other Tables (3) and (4)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Jx0XISusDL", "forum": "Gba02UMvrG", "replyto": "Gba02UMvrG", "signatures": ["ICLR.cc/2026/Conference/Submission18632/Reviewer_E5oT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18632/Reviewer_E5oT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939394809, "cdate": 1761939394809, "tmdate": 1762928345372, "mdate": 1762928345372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This worked presents soft-masked diffusion models. The idea is quite simple, during the de-noising/decoding process, have the mask embeddings at each position retain some information about the top-k possible tokens from the previous de-noising step. This method is generally applicable to most diffusion models, the authors show that you can take an off-the-shelf diffusion LM and and either continue pre-training or fine-tune with their soft-masked method to achieve performance gains with a relatively small amount of compute."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "I'm very positive on this paper! The idea is very simple, but the authors conducted a fairly comprehensive amount of evaluations under various reasonable configurations and showed tangible performance gains. Overall I think this is a good paper and thus lean towards acceptance, although there are a still a few unanswered questions I would like to see answered."}, "weaknesses": {"value": "Overall I think the paper is well executed. There are a few things that I'd like to also see to get a better idea of the limitations and benefits of this method, I'll defer those to the questions section below."}, "questions": {"value": "1. How much cost does the does adding your SM module add to each forward pass of the model? It seems like it'd be marginal but I didn't see an exact description in this work.\n2. Do the authors have any experiments on pre-training from scratch using this method? Given the results form Fig 3 (the validation PPL decreasing very fast w.r.t MDLM), it seems like MDLM+SM might lead to effeciency gains during pre-training too, do the authors have any evidence or intution about this? (Follow up, when you say \"our MDLM+ SM\", is the only addition the SM module or is there some other differences on MLDM loss compared to Sahoo's original work?) \n3. Related to the above, how many updates steps are needed to update an MDLM model to use MDLM+SM, I see 100k steps of additional pre-training for the 169M paramedics model, how was this number chosen and generally how do the performance gains of adding +SM look for different compute budgets. This is partially touched on in the ablations, but can the authors comment a bit more on this please?\n4. I'm very curious about some of the ablations that were conducted, particularly on the choice of K. Appendix B is a good start, but I'm still a bit confused on a few things. Namely (1) Why does K=1 seem to have way worse Val PPL and other K choices in Fig 5 but in Table 4 K=1,3, and 5 are very similar?  Have you noticed in your training that the choice of K is quite dependent on the task/dataset? Furthermore, in your ablations on the time dependence of SM I see that SM is most useful in the early steps (which makes intuitive sense in my opinion, SM is most useful when there are more masked tokens), but isn't the time dependence also a function of the dataset and the choice of K? I see that using a learned K wasn't particularly useful in this work, but I'd appreciate some comments on this from the perspective of a user trying to choose a reasonable K for a given task."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TzEeHi2sDH", "forum": "Gba02UMvrG", "replyto": "Gba02UMvrG", "signatures": ["ICLR.cc/2026/Conference/Submission18632/Reviewer_GihD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18632/Reviewer_GihD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962476378, "cdate": 1761962476378, "tmdate": 1762928343890, "mdate": 1762928343890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Masked diffusion models have been recently introduced to text modeling, where the forward process is defined by transforming the valid tokens into [mask] tokens.\nDespite their great potential, the mask tokens may result in information loss during the denoising process.\nThe authors propose a framework, called soft-masking, to assign non-trivial embeddings for the mask tokens, which can retain the semantic information of the partially decoded sequence.\nThe authors also give a practical training pipeline, which finetunes a pre-trained masked diffusion model.\nThe experiments on unconditional text generation and code generation demonstrate the superiority of SM compared to baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The presentation is clear and easy to follow.\n- Although the information blank issue has been observed by concurrent works, I find the idea of using the mixture of top-k token indexes as mask token embedding is novel.\n- The authors validate the effectiveness of the proposed SM method through text and code generation experiments, which supports their claim."}, "weaknesses": {"value": "- The training methodology in Section 3.2 lacks a detailed derivation. Since the authors introduce an embedding for masked tokens, both the forward process and backward process have been changed. The authors then propose to use the two-pass method to train the the new SM model. However, this training method is rather heuristic and the authors do not provide any theoretical analysis to explain what we exactly do in the training process (e.g., maximizing the likelihood?)\n- The formulation in Line 202-206 only retains the top-k probabilities on the mask position of the current prediction step. This way, it seems that SM works by memorizing the predictions that have been made previously. To exclude the memorization effect, I think an important baseline should be fixing the MDLM pre-trained model and manually tuning the parameter $\\lambda$.\n- As a continuation of the above point, why not use a scheme to memorize all the historical predictions, rather than only the current prediction step?\n- The authors claim that the DLMs beat AR  in terms of accelerated sampling and controllable generation. Can the authors provide more evidence to support this claim?\n- In line 159, what do the authors mean by $f_\\theta=h\\odot g_\\theta$?\n- As the authors do not provide a likelihood estimate formula for SM, they should clarify how they compute the perplexity score for OpenWebText."}, "questions": {"value": "- In the training pipeline, the first model pass (Line 6, Alg 1) does not propagate the gradient. Why? How will the result change if we turn on the backpropagation in the first model pass?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MUJx3s0O3Y", "forum": "Gba02UMvrG", "replyto": "Gba02UMvrG", "signatures": ["ICLR.cc/2026/Conference/Submission18632/Reviewer_rBVW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18632/Reviewer_rBVW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762069996186, "cdate": 1762069996186, "tmdate": 1762928343251, "mdate": 1762928343251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces soft-masking, a method for improving masked diffusion LLMs by blending the mask state with a soft prediction from previous step before feeding into the denoiser network. It shows strong empirical improvements over models without soft-masking in continued pretraining."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The motivation is strong. The work addresses a key weakness of masked diffusion LLMs - where the state is a binary decision and cannot represent superpositions of different tokens. \n\n* The proposed fix is sensible and do not change the formulation of the probabilistic model, i.e., the model can still be trained with standard ELBO objective, with additional modification in the input of the neural network. \n\n* The empirical improvements over pure mask models in continued pretraining is very strong (PPL 23.14 -> 21.63 vs. 23.14 -> 22.88). When applying this method to finetuning Dream coding models, The performance gain is also significant, especially in the high-throughput settings which is the main regime people prefer diffusion over AR LLMs."}, "weaknesses": {"value": "* The proposed method requires two evaluations of the denoiser network in each training iteration. This makes the comparison to pure masked models unfair (the latter only need one network evaluation) given the same batch size. I would expect to see a comparison that matches the training flops. \n\n* There is a very closely-related prior method \"self-conditioning\" that the authors failed to prominently highlight. Although it is briefly mentioned in the related work, given the smilarity of the two approaches (in fact the soft-masking can be seen as self-conditioning in discrete state spaces), in my opinion they should be discussed in more detail (the authors should point out the similarity in the method section, e.g., the two-forward-passes training)."}, "questions": {"value": "Please see above weaknesses (i will consider increasing the score if they are sufficiently addressed)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kMEUkZpJO9", "forum": "Gba02UMvrG", "replyto": "Gba02UMvrG", "signatures": ["ICLR.cc/2026/Conference/Submission18632/Reviewer_wkhr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18632/Reviewer_wkhr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762209968009, "cdate": 1762209968009, "tmdate": 1762928342757, "mdate": 1762928342757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}