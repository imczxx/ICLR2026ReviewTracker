{"id": "3C1U86DcW4", "number": 3477, "cdate": 1757441029252, "mdate": 1763666079648, "content": {"title": "Belief-Based Offline Reinforcement Learning for Delay-Robust Policy Optimization", "abstract": "Offline–to–online deployment of reinforcement learning (RL) agents often stumbles over two fundamental gaps: (1) the sim-to-real gap, where real-world systems exhibit latency and other physical imperfections not captured in simulation; and (2) the interaction gap, where policies trained purely offline face out-of-distribution (OOD) issues during online execution, as collecting new interaction data is costly or risky. As a result, agents must generalize from static, delay-free datasets to dynamic, delay-prone environments.\nIn this work, we propose $\\textbf{DT-CORL}$ ($\\textbf{D}$elay-$\\textbf{T}$ransformer belief policy $\\textbf{C}$onstrained $\\textbf{O}$ffline $\\textbf{RL}$), a novel framework for learning delay-resilient policies solely from static, delay-free offline data. DT-CORL introduces a transformer-based belief model to infer latent states from delayed observations and jointly trains this belief with a constrained policy objective, ensuring that value estimation and belief representation remain aligned throughout learning. Crucially, our method does not require access to delayed transitions during training and outperforms naive history-augmented baselines, SOTA delayed RL methods, and existing belief-based approaches.\nEmpirically, we demonstrate that DT-CORL achieves strong delay-robust generalization across both locomotion and goal-conditioned tasks in the D4RL benchmark under varying delay regimes. Our results highlight that joint belief-policy optimization is essential for bridging the sim-to-real latency gap and achieving stable performance in delayed environments.", "tldr": "", "keywords": ["Delayed Reinforcement Learning", "Offline-to-Online Adaption"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/29350f52ffd81e76155980a92a39d4f49c6613a3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of learning delay-robust policies from delay-free offline datasets for deployment in delayed environments. The authors propose DT-CORL (Delay-Transformer belief policy Constrained Offline RL), which combines a transformer-based belief model with constrained policy optimization. The key innovation is joint training of belief prediction and policy optimization to handle the sim-to-real latency gap without requiring delayed transitions during training. The method is evaluated on D4RL tasks under various delay scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Solid theoretical framework: The reformulation from augmented-state delayed MDP to belief-based policy iteration is theoretically grounded with formal lemmas and a monotonic improvement guarantee (Proposition 4.3).\n\n2. Comprehensive experiments: Evaluation covers multiple D4RL environments with both deterministic and stochastic delays, multiple baseline comparisons, and thorough ablation studies on architecture and data availability."}, "weaknesses": {"value": "1. Inconsistent baseline performance: DT-CORL sometimes underperforms Augmented-COMBO in AntMaze tasks (especially medium-play and large-play) and at smaller delays (∆=4, 8), without adequate explanation of when or why model-based approaches might be preferable.\n\n2. Limited experimental scope: Experiments are restricted to low-dimensional D4RL tasks without vision-based observations, comparisons with recent model-based offline RL methods beyond COMBO, or real-world validation of sim-to-real transfer claims.\n\n3. Incomplete computational analysis: Training time, memory requirements, and scalability to delays beyond ∆=16 are not evaluated, and the impact of the transformer's larger parameter count (7.87M vs 1.80M) on sample efficiency is unclear."}, "questions": {"value": "1. Performance gaps with COMBO: Can you provide deeper analysis of when and why Augmented-COMBO outperforms DT-CORL? Is there a fundamental trade-off between model-based and belief-based approaches that depends on environment characteristics?\n\n2. Belief model architecture: Why is the transformer significantly better than diffusion models despite similar prediction accuracy (Figure 2)? Is it purely computational efficiency or are there other factors?\n\n3. Distribution mismatch: You claim joint training avoids distribution mismatch, but the policy still uses the same offline dataset. Can you quantify the distribution shift between training and deployment more rigorously?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZK2Jy0FlO9", "forum": "3C1U86DcW4", "replyto": "3C1U86DcW4", "signatures": ["ICLR.cc/2026/Conference/Submission3477/Reviewer_F3YW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3477/Reviewer_F3YW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760670288287, "cdate": 1760670288287, "tmdate": 1762916744328, "mdate": 1762916744328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an offline RL method that trains on delay-free offline data and is robust to online state-action delays. The proposed approach works by augmenting the state-action space via stacking a history trace and doing policy evaluation and optimization in the augmented space."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem setting is meaningful, and the approach itself is sound and interesting. \n2. The empirical performance of the proposed approach is good across a wide range of tasks."}, "weaknesses": {"value": "1. The authors assume a known delay window and construct an augmented dataset based on that delay window. However, assuming a known exact delay window for online deployment is rather unrealistic. If one uses a worst-case delay window for offline training, the proposed approach does not establish if training with a large delay window might still give reasonable online performance if the actual latency during deployment is smaller. \n\n2. The policy improvement statement they constructed seems insufficient; cf. my question below. \n\n3. The proposed approach heavily relies on the accuracy of the learned belief model. However, assuming being able to learn a good belief model offline without knowing the actual online latency seems to be a strong assumption - it essentially requires that the data distribution collected by the delay-free behavior policy is representative of the online closed-loop controller with delays. If this assumption is violated, everything proposed in this approach falls apart, and the authors do not have a formal statement established to address this point. As data coverage shift is a well-known open challenge in standard offline RL settings, the proposed approach seems to require even stronger assumptions on the dataset coverage. \n\n4. Section 4.1: Presentation can be improved: It is important to lead the presentation with the goal of each step. I personally got confused while reading this section and was trying to guess what the authors were trying to establish with each step. \n\n5. I feel that the ablation study can be strengthened: right now, the authors claim the benefit of jointly considering the delayed, augmented state belief model in policy evaluation and optimization steps by comparing to two customized baselines (CQL + IQL) equipped with a belief model, which does not seem to be an apples-to-apples comparison. I would appreciate a more direct ablation of the delay awareness from different components of the proposed approach. \n\n6. Minor:\n- Notation is vague at times: Lemmas 4.1 and 4.2, which value functions do the authors mean to bound? Any value functions?"}, "questions": {"value": "1. Proposition 4.3: There are two moving parts in this equation, both the estimated Q value and the policy pi. Having a higher estimated Q value does not necessarily imply a better policy, right? Also, the notation here is a bit arbitrary - the current way of writing the equation seems to be comparing the Q value for the exact same action, even though the new and old policies might pick different actions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rPCkxUaffM", "forum": "3C1U86DcW4", "replyto": "3C1U86DcW4", "signatures": ["ICLR.cc/2026/Conference/Submission3477/Reviewer_TYE4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3477/Reviewer_TYE4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657306770, "cdate": 1761657306770, "tmdate": 1762916744082, "mdate": 1762916744082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an offline RL method that trains on delay-free offline data and is robust to online state-action delays. The proposed approach works by augmenting the state-action space via stacking a history trace and doing policy evaluation and optimization in the augmented space."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem setting is meaningful, and the approach itself is sound and interesting. \n2. The empirical performance of the proposed approach is good across a wide range of tasks."}, "weaknesses": {"value": "1. The authors assume a known delay window and construct an augmented dataset based on that delay window. However, assuming a known exact delay window for online deployment is rather unrealistic. If one uses a worst-case delay window for offline training, the proposed approach does not establish if training with a large delay window might still give reasonable online performance if the actual latency during deployment is smaller. \n\n2. The policy improvement statement they constructed seems insufficient; cf. my question below. \n\n3. The proposed approach heavily relies on the accuracy of the learned belief model. However, assuming being able to learn a good belief model offline without knowing the actual online latency seems to be a strong assumption - it essentially requires that the data distribution collected by the delay-free behavior policy is representative of the online closed-loop controller with delays. If this assumption is violated, everything proposed in this approach falls apart, and the authors do not have a formal statement established to address this point. As data coverage shift is a well-known open challenge in standard offline RL settings, the proposed approach seems to require even stronger assumptions on the dataset coverage. \n\n4. Section 4.1: Presentation can be improved: It is important to lead the presentation with the goal of each step. I personally got confused while reading this section and was trying to guess what the authors were trying to establish with each step. \n\n5. I feel that the ablation study can be strengthened: right now, the authors claim the benefit of jointly considering the delayed, augmented state belief model in policy evaluation and optimization steps by comparing to two customized baselines (CQL + IQL) equipped with a belief model, which does not seem to be an apples-to-apples comparison. I would appreciate a more direct ablation of the delay awareness from different components of the proposed approach. \n\n6. Minor:\n- Notation is vague at times: Lemmas 4.1 and 4.2, which value functions do the authors mean to bound? Any value functions?"}, "questions": {"value": "1. Proposition 4.3: There are two moving parts in this equation, both the estimated Q value and the policy pi. Having a higher estimated Q value does not necessarily imply a better policy, right? Also, the notation here is a bit arbitrary - the current way of writing the equation seems to be comparing the Q value for the exact same action, even though the new and old policies might pick different actions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rPCkxUaffM", "forum": "3C1U86DcW4", "replyto": "3C1U86DcW4", "signatures": ["ICLR.cc/2026/Conference/Submission3477/Reviewer_TYE4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3477/Reviewer_TYE4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657306770, "cdate": 1761657306770, "tmdate": 1763761116180, "mdate": 1763761116180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents offline RL methods for delay robust policy optimization. Towards this, the paper introduces a transformer based belief model to infer latent states from delayed observations and jointly learns this model while performing offline policy optimization to ensure the estimation doesnt suffer from distribution shift issues."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Interesting problem, well motivated and reasonably well written paper."}, "weaknesses": {"value": "- The theory section is based on assumptions that aren't satisfied in practice even in locomotion tasks (defn 3.2), with discontinuities in the dynamics and / or rewards.\n- The theory doesnt sketch out sample complexity associated with estimation issues popping up due to delayed observations which, in the context of offline RL would be good to get a clear grasp of.\n- Intuitively, it feels like stochastic delays should be harder to estimate (at least, the estimators would have higher variance), but the methods appear to perform even better in this setup than the deterministic ones (e.g. table 1), which seems counterintuitive."}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PxBAHX5ecu", "forum": "3C1U86DcW4", "replyto": "3C1U86DcW4", "signatures": ["ICLR.cc/2026/Conference/Submission3477/Reviewer_Z5y8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3477/Reviewer_Z5y8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890171274, "cdate": 1761890171274, "tmdate": 1762916743891, "mdate": 1762916743891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DT-CORL, a method for learning delay-robust policies from purely delay-free offline data. The approach combines a transformer-based belief model to infer latent states from delayed observations with a constrained policy optimization objective, enabling effective deployment in environments with observation/action delays without requiring online interaction. Evaluations on D4RL benchmarks demonstrate strong performance against augmentation-based and belief-based baselines under varying delay settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Addresses a well-motivated and practical problem at the intersection of offline RL and robustness to delays, which is essentially important in real-world scenarios.\n\n2. The proposed integration of belief learning and policy optimization within an offline constraint framework is technically sound and justified by theoretical analysis.\n\n3. Empirical evaluation is comprehensive, covering multiple tasks, delay types, and delay lengths, with ablations validating design choices.\n\n4. The paper is clearly written, and the methodology is well-explained."}, "weaknesses": {"value": "1. While the transformer belief model shows advantages, its computational overhead relative to simpler models is non-trivial.\n2. The experimental validation is confined to standard simulation benchmarks (D4RL). The absence of validation on a physical system or a high-fidelity simulator with realistic latency weakens the claims of practical contribution."}, "questions": {"value": "1. Have the authors considered testing in higher-dimensional observation spaces (e.g., pixel-based tasks) to assess the scalability of the belief model?\n2. Could the belief model be made more lightweight without sacrificing predictive accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H5V0TAE7Qj", "forum": "3C1U86DcW4", "replyto": "3C1U86DcW4", "signatures": ["ICLR.cc/2026/Conference/Submission3477/Reviewer_zm9D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3477/Reviewer_zm9D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908076354, "cdate": 1761908076354, "tmdate": 1762916743687, "mdate": 1762916743687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}