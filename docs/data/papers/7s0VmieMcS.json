{"id": "7s0VmieMcS", "number": 12724, "cdate": 1758209773961, "mdate": 1759897491352, "content": {"title": "Verifying GNNs with Readout is Intractable", "abstract": "We introduce a logical language for reasoning about quantized aggregate-combine graph neural networks with global readout (ACR-GNNs). We provide a logical characterization and use it to prove that verification tasks for quantized GNNs with readout are (co)NEXPTIME-complete. This result implies that the verification of quantized GNNs is computationally intractable, prompting substantial research efforts toward ensuring the safety of GNN-based systems. We also experimentally demonstrate that quantized ACR-GNN models are lightweight while maintaining good accuracy and generalization capabilities with respect to non-quantized models.", "tldr": "", "keywords": ["graph neural network", "quantization", "verification", "theoretical complexity"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/879a78ff6b1145febff07ad88c347476617359d4.pdf", "supplementary_material": "/attachment/f63b87f900e3b0a4f19502983ba31c026e722467.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a logic called $q\\mathcal{L}$ that characterizes the expressivity of *quantized* Aggregate‚ÄìCombine‚ÄìReadout Graph Neural Networks (ACR GNNs), a widely used extension of vanilla message-passing GNNs for graph classification.\n\nThe authors use $q\\mathcal{L}$ to show (co)NEXPTIME completeness of verification tasks on quantized ACR-GNNs via reduction to SAT/VALIDITY on $q\\mathcal{L}$.\n\nIn their experiments, they illustrate on synthetic data and the protein‚Äìprotein interaction (PPI) dataset that quantization does not significantly degrade the performance of models."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- **(S1) Problem Impact**: The paper addresses and seems to close an existing gap in the characterization of GNNs posed by [S√§lzer et¬†al., 2025](https://arxiv.org/abs/2502.16244), and provides a formalism that describes an important and widely used class of GNNs/graph properties.\n\n- **(S2) Thorough analysis of $q\\mathcal L$**:The investigation of the proposed logic is thorough, with examples highlighting its ‚Äúreinterpretability‚Äù and compatibility with counting modal logic/description logic. While the main paper introduces $q\\mathcal{L}$ as an extension of $\\mathcal{L}\\_{\\text{quantGNN}}$ from [S√§lzer et¬†al., 2025](https://arxiv.org/abs/2502.16244) to add global aggregation, I personally like the additional connection to $K_‚ôØ$¬†[Nunn et¬†al., 2024](https://arxiv.org/abs/2405.00205).\n\n- **(S3) Self-Contained**: The paper provides ample explanation and background to follow the dense theoretical setting."}, "weaknesses": {"value": "## W1: Presentation\n\nThe write-up of the paper is poor in grammar and clarity of language, with a few questionable slips, the most apparent being: ‚Äú**NEXPTIME** is the class of problems decidable by a non-deterministic algorithm running in **polynomial time** in the size of its input,‚Äù where the authors clearly meant **exponential**.\n\nThe notation quietly changes after Section 3, where $\\mathrm{agg}\\_{g}$ suddenly becomes $\\mathrm{agg}\\_{\\forall}$ henceforth.\n\nTypesetting is strange, e.g., in lines 210‚Äì214, where the definition of the box operators is centered and the diamond operators are defined in line. After these definitions, the next sentence starts with a lower-case ‚Äúand.‚Äù\n\nIn general, Section 3: *Simulating modal logic in $q\\mathcal{L}$* is written in a confusing manner. The authors aim to show that they can re-express any formula with modal operators and connectives as an atomic formula by simulating these operators arithmetically. It is initially unclear why this is done (e.g., to simplify further proofs?).\n\nThe manuscript also uses up a lot of space on examples, but the central aspects---i.e., the ACR-GNN verification task definitions vt1, vt2, vt3, and the reason why these tasks are important in particular---are mentioned only in passing. As they are the central objects of investigation in the manuscript, this priority in levels of detail seems strange.\n\nWhile the examples are helpful to follow the manuscript, they are used a lot, take up a lot of space, are partially a bit more complex than necessary (e.g., the example in lines 190‚Äì200), and sometimes replace proper definitions or explanations of concepts---for example, the definition of subexpressions $E(\\phi)$ in Section 4.1.\n\n## W2: Motivation and Experiments\n\nI understand the motivation of the theoretical contribution, as global aggregation is commonly used in GNNs, and the authors do state that all ML models are quantized. \n\nThe experimental evaluation then goes on to demonstrate that quantization does not impact the performance of GNNs a lot. This setup seems largely irrelevant to the theoretical results and seems to undermine the motivation of the authors rather than support it. The experiments act as proof of concept of using quantized ACR GNNs in practice. \nIf the practicality of these models needs to be established, then it is questionable why they should be investigated from a theoretical point of view.\n\nI think the authors could make a much stronger experimental statement by taking (pretrained) state-of-the-art models from existing literature on tasks that the community cares about, and stressing that **because** of similar performance, their quantized surrogates can be verified as a proxy. A study of how much speedup in terms of *verification time* versus the *loss in performance* would then provide more support for the theoretical setup, and could act as a guideline for future research by pointing out the value of verifying strongly compressed surrogate models.\n\n\n## W3: Novelty\n\nWhile the contribution seems to address a gap in existing research, the manuscript (including the structure) seems heavily inspired by [S√§lzer et¬†al., 2025](https://arxiv.org/abs/2502.16244), with adaptations to support global readout. As such, a lot of the submitted manuscript feels like it repeats information and does not focus enough on the novel contribution and motivation of the investigated verification settings.\n\n## W4: Gap in Theoretical Results\n\nThe authors introduce $q\\mathcal{L}$ to formalize the computations of ACR-GNNs, and show by example that any computation of the GNN can be expressed in the logic. However, it seems that a formal statement is missing that proves that the logic and the class of GNNs are, in fact, **equally** expressive."}, "questions": {"value": "Of my listed weaknesses, I would appreciate a brief discussion of the authors to address my concerns in W2 and W4.\n\nIn addition, I have the following question to possibly expand on the contribution in the paper.\n\n- **Q1 Simulation of global readout**: One unexplored aspect of ACR verification is a result of¬†[Jogl et¬†al., 2023](https://proceedings.neurips.cc/paper_files/paper/2023/file/ebf95a6f3c575322da15d4fd0fc2b3c8-Paper-Conference.pdf), which proposes simulating, e.g., an ACR-GNN with an AC-GNN without global readout by using a graph transformation on input graphs. As ACR-GNNs can then apparently be simulated from simpler-to-verify GNNs (‚Äúonly‚Äù PSPACE-complete), the question arises why the difference arises, and whether a theoretical connection can be made. Does the translation of the formulas lead to a blowup of size?\n\n\n---\n\n## References\n\n- **S√§lzer, M., Schwarzentruber, F., & Troquard, N. (2025).** *Verifying Quantized Graph Neural Networks is PSPACE-complete.* IJCAI 2025. arXiv:2502.16244. <https://arxiv.org/abs/2502.16244>\n\n- **Nunn, P., S√§lzer, M., Schwarzentruber, F., & Troquard, N. (2024).** *A Logic for Reasoning About Aggregate‚ÄìCombine Graph Neural Networks (K‚ôØ).* IJCAI 2024. arXiv:2405.00205. <https://arxiv.org/abs/2405.00205>\n\n- **Jogl, F., Thiessen, M., & G√§rtner, T. (2023).** *Expressivity-Preserving GNN Simulation.* NeurIPS 2023. <https://proceedings.neurips.cc/paper_files/paper/2023/file/ebf95a6f3c575322da15d4fd0fc2b3c8-Paper-Conference.pdf>"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0fHG0DYYiX", "forum": "7s0VmieMcS", "replyto": "7s0VmieMcS", "signatures": ["ICLR.cc/2026/Conference/Submission12724/Reviewer_FTpY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12724/Reviewer_FTpY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12724/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761692140970, "cdate": 1761692140970, "tmdate": 1762923547134, "mdate": 1762923547134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this article, the authors present new results about the expressivity of GNNs.\nThe authors consider a quantized variant of ReLU GNNs, meaning that the numbers used in computations all have a finite number of bits. The main contribution of the authors is to design a certain logic called $q\\mathcal{L}$, to capture global readout. \n\nThe main contributions are:\n\n- The logic $q\\mathcal{L}$ is NEXPTIME, meaning that given, the language associated to $q\\mathcal{L}$, then recognizing if a formula based on this language can be satisfied, can be performed in with a non-deterministic Turing machine, for some polynomial (n is a parameter measuring the size of the formula).\n\n- Restricting to GNNs without global readout, by adapting their language $q\\mathcal{L}$ to that case yields a PSPACE-complete version, meaning that (i) it is in PSPACE: one can solve the same problem as above in polynomial space, and (ii) any problem in PSPACE can be reduced polynomially to a problem of satisfiability of a formula of the qL version without global readout.\n\n- Preliminary experiments on the impact of quantization on the performance and model size in practice"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The nature of the contribution is interesting and timely, in the line of research about the expressivity of GNNs.\n\n- This article can be very interesting for logicians, while still be relevant for a more general audience.\n\n- The illustrating experiments going along with the main theoretical contribution are interesting."}, "weaknesses": {"value": "- While the paper claims potential implications for the safety of GNNs, the connection between the theoretical expressivity results and concrete safety aspects is not made explicit. Clarifying this link would significantly strengthen the paper‚Äôs broader impact.\n\n- Given the technical nature of the paper, it would be helpful if the authors included a more accessible, high-level explanation of their main contributions for readers outside the logic community. \n\n- Some imprecision, right from the beginning of the paper: please see first Question below."}, "questions": {"value": "- It is said in first page (contributions): ``NEXPTIME is the class of problems decidable by a non-deterministic algorithm running in polynomial time in the size of its input''. Except if the size refers to the size of a compressed representation of the input string, this is incorrect to me. Rather, NEXPTIME should be the class of decision problems solvable by a non-deterministic Turing machine in exponential time, i.e., time $2^{p(n)}$ for some polynomial $p$. Please clarify.\n\n- typo: l. 392: ``we introduction'' ->  we introduce"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KcB3xCw1uj", "forum": "7s0VmieMcS", "replyto": "7s0VmieMcS", "signatures": ["ICLR.cc/2026/Conference/Submission12724/Reviewer_SHhJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12724/Reviewer_SHhJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12724/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908045136, "cdate": 1761908045136, "tmdate": 1762923546762, "mdate": 1762923546762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper establishes that verifying **quantized Aggregate‚ÄìCombine Graph Neural Networks with global Readout (ACR-GNNs)** is **(co)NEXPTIME-complete**, revealing that the verification of quantized GNNs is inherently intractable. To prove this, the authors introduce a logical language **qL**, which extends previous logics for GNN verification to handle **global readout** and **quantized arithmetic**. They provide reductions from qL to a quantized variant of **Quantifier-Free Boolean Algebra with Presburger Arithmetic (QFBAPAùïÇ)**, showing decidability and tight complexity bounds. They also consider a bounded-graph setting where verification becomes **(co)NP-complete** and provide a proof-of-concept verifier. Finally, they present experimental evidence that **quantized ACR-GNNs maintain accuracy** while reducing model size and inference cost, supporting the practical utility of quantized models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality**  \n- The paper provides the **first complete logical characterization** and **tight complexity bounds** for verifying quantized GNNs with global readout.  \n- The use of **qL logic** to capture global readout within the quantized framework is an elegant and novel extension of prior formalisms like K‚ôØ and FOC2.\n\n**Quality**  \n- The proofs are rigorous, adapting known techniques (e.g., Hintikka sets and reductions to QFBAPAùïÇ) to the quantized and global readout setting.  \n- The authors‚Äô reasoning links theoretical intractability with practical implications, motivating further research into scalable verification strategies.  \n- The bounded-vertex relaxation and prototype implementation show a **constructive direction** for future research.\n\n**Clarity**  \n- The structure is clear: the paper walks from formal definitions, through complexity proofs, to bounded relaxations and experiments.  \n- The motivating examples (e.g., verifying properties of ‚Äúdog‚Äù graphs) effectively illustrate the semantics of qL.  \n- The appendix and code references enhance reproducibility.\n\n**Significance**  \n- The (co)NEXPTIME-completeness result fills a major theoretical gap in GNN verification.  \n- The bounded-graph relaxation connects deep theory to **practical model checking**, potentially influencing future verification frameworks."}, "weaknesses": {"value": "1. **Practical implications remain limited**  \n   - Although the theoretical contribution is strong, the **practical relevance** of (co)NEXPTIME results could be elaborated‚Äîhow does this shape actual verification tool design?  \n   - The experiments, while informative, are **tangential** to the main verification focus.\n\n2. **Experimental evaluation is lightweight**  \n   - The experiments only test quantization effects on model accuracy and size, not the **actual verification performance**.  \n   - The absence of benchmarks comparing **verification time** or **SMT encoding scalability** leaves open questions about the applicability of qL in realistic settings.\n\n3. **Readout semantics assumption**  \n   - The fixed summation order assumption in global aggregation (noted in the limitations) is **non-standard in practice**, which might restrict the generality of the theoretical claims.\n\n4. **Notation density and accessibility**  \n   - Some sections (e.g., Hintikka sets and reduction construction) are mathematically dense and could benefit from illustrative diagrams or examples of intermediate steps."}, "questions": {"value": "1. **Verification tractability**  \n   - Given the (co)NEXPTIME-completeness, what verification techniques could still be practically feasible for small or structured graphs?  \n   - Can symbolic abstractions or over-approximations make qL-based verification tractable in practice?\n\n2. **Extension to other architectures**  \n   - How would the complexity results change for other GNN architectures, such as recurrent or attention-based models?  \n   - Could qL be extended to handle continuous activations or message-passing schemes beyond summation?\n\n3. **Bounded verification**  \n   - The bounded-vertex setting leads to (co)NP-completeness. Are there heuristics or practical solvers that can efficiently address this fragment?  \n   - How scalable is the provided proof-of-concept verifier when N grows beyond small graphs?\n\n4. **Quantization modeling**  \n   - The assumption of saturating arithmetic in ùïÇ simplifies reasoning, but how would modular or IEEE-style rounding affect decidability or complexity?  \n   - Are there concrete examples where quantization introduces or removes counterexamples compared to unquantized GNNs?\n\n5. **Experimental depth**  \n   - Can the authors provide **verification runtimes or success rates** for the bounded verifier on benchmark GNNs?  \n   - How does quantization influence the **verifiability** (not just accuracy) of ACR-GNNs in the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b4DVnf86Yc", "forum": "7s0VmieMcS", "replyto": "7s0VmieMcS", "signatures": ["ICLR.cc/2026/Conference/Submission12724/Reviewer_7D3e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12724/Reviewer_7D3e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12724/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974909992, "cdate": 1761974909992, "tmdate": 1762923546456, "mdate": 1762923546456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper narrows its scope to a critical yet underaddressed gap in GNN verification: the theoretical complexity of quantized Aggregate-Combine Graph Neural Networks with global readout (ACR-GNNs). While \"neural network verification is intractable\" is a broad community consensus, this work focuses explicitly on GNN-specific structures‚Äînamely, global readout (a core component for graph-level tasks like molecule classification or protein interaction prediction)‚Äîand quantized arithmetic (standard in real-world deployments). It introduces the logical language qL to formalize ACR-GNN computations and graph properties, proves that ACR-GNN verification tasks (sufficiency, necessity, consistency) are (co)NEXPTIME-complete, and contrasts this with the PSPACE-completeness of readout-free quantized GNNs. Complementing theory, the paper validates that quantized ACR-GNNs retain high accuracy ($\\pm$1% drop) with 60-74% size reduction and proposes a bounded-vertex relaxation (NP/coNP-complete) for practical verification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Before this paper, the community knew readout-free quantized GNN verification was PSPACE-complete, but readout‚Äôs impact was speculative. By proving (co)NEXPTIME-completeness, the work quantifies this impact: global readout pushes complexity into a higher class, meaning verification becomes exponentially harder with increasing input size. This clarity prevents wasted effort.\n\n- The paper‚Äôs (co)NP-complete bounded-vertex relaxation is not a random heuristic but a principled response to the (co)NEXPTIME result: by limiting counterexamples to graphs with N vertices, it leverages the boundary between \"unbounded intractability\" and \"bounded tractability.\" This may provide a roadmap for future work‚Äîe.g., optimizing N selection, or combining bounded verification with domain-specific constraints (e.g., molecule graphs have ‚â§100 atoms)‚Äîthat would be impossible without knowing the exact point at which intractability sets in."}, "weaknesses": {"value": "- The theoretical analysis and experiments focus exclusively on summation for local/global aggregation. However, other modern GNNs use max, mean, or attention-based aggregation‚Äîreadout for these variants may introduce different complexity patterns (e.g., max aggregation reduces dependency between distant vertices). The paper‚Äôs failure to extend bounds to non-summation aggregation limits its relevance to a narrow subset of industrial GNNs.\n\n- Extensive experiments validate quantized ACR-GNNs' accuracy/lightweight design but do not link to core verification challenges, feeling tangential to the paper‚Äôs central claim.\n\n- While technically rigorous, the (co)NEXPTIME-completeness result reinforces a known trend (readout exacerbates complexity) rather than offering a transformative insight, limiting broader field impact."}, "questions": {"value": "- For practical applications (e.g., molecular graph verification), how do you recommend choosing the maximum number of vertices N? Is there a way to determine N such that if no counterexample exists for N, it is unlikely to exist for larger graphs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hfEpXwWzdL", "forum": "7s0VmieMcS", "replyto": "7s0VmieMcS", "signatures": ["ICLR.cc/2026/Conference/Submission12724/Reviewer_3rCn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12724/Reviewer_3rCn"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission12724/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762929752757, "cdate": 1762929752757, "tmdate": 1762929752757, "mdate": 1762929752757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}