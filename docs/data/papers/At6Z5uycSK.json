{"id": "At6Z5uycSK", "number": 14841, "cdate": 1758244613630, "mdate": 1759897346099, "content": {"title": "Exact Subgraph Isomorphism Network for Predictive Graph Mining", "abstract": "In the graph-level prediction task (predict a label for a given graph), the information contained in subgraphs of the input graph plays a key role. In this paper, we propose Exact subgraph Isomorphism Network (EIN), which combines the exact subgraph enumeration, neural network, and a sparse regularization. In general, building a graph-level prediction model achieving high discriminative ability along with interpretability is still a challenging problem. Our combination of the subgraph enumeration and neural network contributes to high discriminative ability about the subgraph structure of the input graph. Further, the sparse regularization in EIN enables us 1) to derive an effective pruning strategy that mitigates computational difficulty of the enumeration while maintaining the prediction performance, and 2) to identify important subgraphs that contributes to high interpretability. We empirically show that EIN has sufficiently high prediction performance compared with standard graph neural network models, and also, we show examples of post-hoc analysis based on the selected subgraphs.", "tldr": "", "keywords": ["Graph Classification", "Sparse Learning", "Graph Mining", "Deep Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cdd8bb3b6495c2150e8637af310ab4310e33e1de.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes EIN, a graph-level prediction model that (i) represents each training graph by exact subgraph-isomorphism features (SIF) over all subgraphs up to size maxpat, (ii) learns a small, task-specific set of predictive subgraphs via group-sparse (‚Ñì‚ÇÇ,‚ÇÅ) regularization in a Graph Mining Layer (GML), and (iii) trains an FFN head on the selected SIF. To make the subgraph search feasible, it couples gSpan enumeration with a proximal-gradient‚Äìdriven pruning rule that bounds per-subgraph gradient norms and prunes entire subtrees of the mining tree; empirically, the method yields competitive accuracy and identifies a compact set of subgraphs that support post-hoc interpretability."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Exact, globally interpretable features. Using exact subgraph matches (not counts/heuristics) provides crisp semantics. Feature selection via group sparse yields a small, human-auditable subgraph set.\n- Clear algorithmic presentation. The optimization loop and working-set traversal are explicit."}, "weaknesses": {"value": "- Scalability and cost remain open. Even with pruning, the approach requires on-the-fly SIF generation/caching and many gSpan traversals. Stronger time/memory breakdowns and scale-up curves would help.\n- Baseline breadth. The main tables compare to GCN/GAT/GIN/PNA/GNN-AK/PPGN, but missing subgraph-aware SOTA baselines.\n- Number of total subgraphs scales exponentially with the number of nodes, which means the proposed method might be intractable for larger graphs. This could limit the impact of the paper.\n- The reviwer is concerning the expressivity of the proposed method. Since the size of subgraph is controled by 'maxpat', there could be some graphs that have the same SIF. For example, two cycles with length n and 2n ('maxpat'<n). Then, these two cycles will have the same SIF but they are not isomorphic. A ablation on 'maxpat' would help."}, "questions": {"value": "- Can the author explain, instead of l-1 nrom, why l-2 norm penalty is used to enforce sparsity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NdjbHnTFuE", "forum": "At6Z5uycSK", "replyto": "At6Z5uycSK", "signatures": ["ICLR.cc/2026/Conference/Submission14841/Reviewer_cxww"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14841/Reviewer_cxww"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965088232, "cdate": 1761965088232, "tmdate": 1762925198295, "mdate": 1762925198295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an Exact Subgraph Isomorphism Network (EIN) for the graph classification task. The authors proposed to use gSpan ‚Äî a graph-mining algorithm ‚Äî to construct a diverse set of subgraphs and use the membership of subgraphs within each graph as isomorphic features. To reduce the number of total subgraphs, a gradient pruning technique based on the gradient bound is proposed for adaptive subgraphs selection. EIN shows superior accuracy on synthetic datasets and competitive performance to GNN baselines on different real-world datasets, with added interpretability via post-hoc analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well-written, ideas are clearly presented and easy to follow.\n- The pruning rule is theoretically grounded and preserves prediction quality.\n- Experiments demonstrate effectiveness on hard synthetic datasets (e.g., Cycle and Cycle_XOR, where EIN achieves ~100% accuracy vs. GNNs' ~50-70%), with clear ablation on pruning rates.\n- The subgraph selection method enables post-hoc analysis which enhances the interpretability of the model."}, "weaknesses": {"value": "- Given previous work [1], in which the the membership score and the gradient pruning technique are proposed, I found the technical contribution of this work is not significant, which only adds a non-linear layer for the prediction model.‚Äã\n- Computational times could be exponentially scaled with number of nodes, and maxpat=10 limits applicability to larger graphs/subgraphs, broader scaling analysis and ablation on maxpat are needed.\n- The pruning based on the gradient of the lost w.r.t. the parameters, which is based on the current model‚Äôs knowledge of the data, and could be biased in early steps.\n- The code is not provided and the details of selected parameters are missing limit the application/reproducibility of the paper. \n\n[1] Tajima, Shinji, et al. \"Learning Attributed Graphlets: Predictive Graph Mining by Graphlets with Trainable Attribute.\" Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2024."}, "questions": {"value": "- A more detail comments on the differences against [1] is needed.\n- Analysis on the expressiveness of the model is highly recommended, it is not clear to me how expressive the proposed model is compared with other baseline GNNs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dJsHV4CYZd", "forum": "At6Z5uycSK", "replyto": "At6Z5uycSK", "signatures": ["ICLR.cc/2026/Conference/Submission14841/Reviewer_gLDX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14841/Reviewer_gLDX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970425221, "cdate": 1761970425221, "tmdate": 1762925193882, "mdate": 1762925193882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a graph representation learning framework that performs graph classification by leveraging the exact subgraph isomorphism features. This work groups sparse weights over all candidate subgraphs and captures interactions among sparsely selected subgraphs and bounds gradients during backpropagation to prune tree branches for tractable training. Experimental results show that EIN outperforms baselines for graph classification tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "¬∑ This work has great theoretical insight that showed exact isomorphism exceeds 1-WL expressivity.\n\n¬∑ The authors provide extensive reproducible codes to replicate their experiments.\n\n¬∑ The method is cleanly formalized with complete pseudocode using standard tools (gSpan, proximal gradient, backtracking)."}, "weaknesses": {"value": "¬∑ The experiment section in this paper does not have any recent baselines. The most recent baseline used in this paper is from 2019, while newer works exist that need to be used as baselines [1] [2] [3] [4] [5].\n\n¬∑ The ablation studies are minor (frequency vs binary, deeper FFN, etc.)\n\n¬∑ The entire framework seems to be dependent on the maxpat hyperparameter (maximum subgraph size). The computational complexity grows combinatorially with maxpat. The paper uses maxpat=10, but many meaningful substructures in domains like chemistry (e.g., functional groups) or social networks (e.g., small communities) might require larger patterns to be discriminative. The method provides no scalable path to discover these.\n\n¬∑ Theorem 2.1 shows that the effectiveness of the pruning hinges on the UB(H) being reasonably tight. If UB(H) is a loose over-estimate, the pruning will be far less effective than claimed, as many branches of the gSpan tree that should be pruned\n\nwill not be. The paper provides no analysis or empirical evidence regarding the tightness of this bound across different datasets.\n\n¬∑ The results for EIN+GIN are presented, but the analysis is shallow. It is unclear how the GNN and EIN components interact. Does the GNN learn complementary features that the subgraph features miss? Does the presence of the GNN change which subgraphs are selected by EIN? A deeper analysis of this interaction is needed to justify the combined architecture.\n\n¬∑ The method is fundamentally limited by the subgraph enumeration process. Even with a high pruning rate, the number of traversed nodes and the total computation time are immense. For example, the Cycle_XOR dataset, which only has 600 graphs, required over 10000s to train. This is orders of magnitude slower than standard GNNs. The approach does not scale to large-scale graph datasets like those in molecular biology or social networks, where graphs can have hundreds of nodes. \n\n\n\n[1] Yu, Zhaoning, and Hongyang Gao. \"Molecular representation learning via heterogeneous motif graph neural networks.\" International conference on machine learning. PMLR, 2022.\n\n[2] Yan, Zuoyu, et al. \"An efficient subgraph gnn with provable substructure counting power.\" Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2024.\n\n[3] Chen, Kaixuan, et al. \"Improving expressivity of gnns with subgraph-specific factor embedded normalization.\" Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2023.\n\n[4] Zhang, Muhan, and Pan Li. \"Nested graph neural networks.\" Advances in Neural Information Processing Systems 34 (2021): 15734-15747.\n\n[5] Zhao, Lingxiao, et al. \"From stars to subgraphs: Uplifting any GNN with local structure awareness.\" arXiv preprint arXiv:2110.03753 (2021)."}, "questions": {"value": "Please address the weaknesses listed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "ZI7PCHRV8M", "forum": "At6Z5uycSK", "replyto": "At6Z5uycSK", "signatures": ["ICLR.cc/2026/Conference/Submission14841/Reviewer_9Re5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14841/Reviewer_9Re5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762040403298, "cdate": 1762040403298, "tmdate": 1762925193373, "mdate": 1762925193373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Exact subgraph Isomorphism Network (EIN), a framework for graph-level prediction. EIN treats the 0/1 existence of all connected subgraphs found in the training dataset up to a certain size as a high-dimensional feature set.  It then uses an FFN to classify the features. The claimed key novelty is in the optimization of the framework which allows for significant pruning in gSpan-based subgraph enumeration, due to a derived gradient norm upper bound. The authors demonstrate EIN's effectiveness on various synthetic and real-world datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This is an interesting paper that re-invents a simple (yet effective) idea of using subgraph mining to do graph-level prediction. For a long time the bottleneck of executing this idea in practice is in subgraph enumeration. The authors innovatively and rigorously show that with proximal gradient descent and sparse regularization, we can have a nice optimization form that prunes the vast majority of subgraphs in the universe. \n2. The studied topic is certainly of importance, and empirical performance and analysis are solid."}, "weaknesses": {"value": "1. It seems to me that the the core theorem (Thm 2.1 and Coro. 2.1) may not hold if the feature is generalized to the count of subgraps rather than the 0/1 existence. If the theorems still hold, it would be very helpful to show it. If the theorem doesn't hold, I think working with only the count of subgraphs would not give the best expressiveness.\n2. It would be helpful to provide sensitivity analysis of  \"maxpat\" which is an important hyperparameter.\n3. Minor: \"for many unnecessarily ùêª\" in Line 148 should be  \"for many unnecessary ùêª\"; Figure 1 should have a caption that illustrates the method in place."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xaknKSbQfO", "forum": "At6Z5uycSK", "replyto": "At6Z5uycSK", "signatures": ["ICLR.cc/2026/Conference/Submission14841/Reviewer_kTFE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14841/Reviewer_kTFE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762374363154, "cdate": 1762374363154, "tmdate": 1762925192807, "mdate": 1762925192807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}