{"id": "mxjk4G4ATb", "number": 9663, "cdate": 1758133129359, "mdate": 1759897705872, "content": {"title": "Latency-Aware Contextual Bandit: Application to Cryo-EM Data Collection", "abstract": "We introduce a latency-aware contextual bandit framework that generalizes the standard contextual bandit problem, where the learner adaptively selects arms and switches decision sets under action delays. In this setting, the learner observes the context and may select multiple arms from a decision set, with the total time determined by the selected subset. The problem can be framed as a special case of semi-Markov decision processes (SMDPs), where contexts and latencies are drawn from an unknown distribution. Leveraging the Bellman optimality equation, we design the contextual online arm filtering (COAF) algorithm, which balances exploration, exploitation, and action latency to minimize regret relative to the optimal average-reward policy. We analyze the algorithm and show that its regret upper bounds match established results in the contextual bandit literature. In numerical experiments on a movie recommendation dataset and cryo-electron microscopy (cryo-EM) data, we demonstrate that our approach efficiently maximizes cumulative reward over time.", "tldr": "Algorithm design and regret analysis for a novel extension of contextual bandits latency where the learner adaptively selects arms and switches decision sets under action delays.", "keywords": ["latency-aware contextual bandits", "average reward maximization", "regret analysis", "cryo-electron microscopy application"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3068d6de1fbc21356c07e492f23df56120618ece.pdf", "supplementary_material": "/attachment/17544d4c867e2e6e735d2cac3ab5ee1fea9efe78.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors studied a latency-aware contextual bandit framework that extends standard contextual bandits by incorporating the action delays and formulates it as a special case of a semi-Markov decision process. The authors proposed the Contextual Online Arm Filtering (COAF) algorithm, which combines stochastic approximation and UCB exploration to balance reward and latency. The authors provided theoretical analysis of their algorithm, proving sublinear regret bounds. Finally, they conducted numerical experiments on MovieLens and cryo-EM data to demonstrate that COAF outperforms baselines and improves data collection efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed latency-aware model generalizes contextual and combinatorial bandits by explicitly accounting for temporal costs. This is a novel problem in the bandits literature.\n- The theoretical analysis appears sound and comprehensive, though I have not checked every proof in detail).\n- I also appreciate the discussion of the application to cryo-EM data collection, which highlights the real-world relevance of the framework. Modeling microscope exposure and movement as latency is a strong and realistic motivation that grounds the theoretical development."}, "weaknesses": {"value": "- While the latency-aware formulation is novel, COAF primarily builds on existing tools such as UCB and stochastic approximation. The conceptual combination is interesting but may feel incremental without deeper theoretical or algorithmic insights. Could you clarify whether the current results provide any new algorithmic intuition or theoretical implications for the broader bandit literature?\n\n- The numerical experiments, though illustrative, are relatively small-scale, so the insights they provide are somewhat limited. The cryo-EM evaluation appears to use simulated data, with experiments mainly comparing COAF to human microscopists. Could you offer some more comprehensive analysis here? E.g., an ablation study examining how COAF’s performance changes under different latency distributions. Similarly, the MovieLens experiments feel limited in scope, particularly in the choice of baselines. It would be informative to also compare against a number of standard contextual bandit algorithms.\n\n- Finally, it would be valuable for the authors to discuss additional application domains where the proposed latency-aware bandit framework could be beneficial, beyond the cryo-EM setting."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "t0F5SUqh7n", "forum": "mxjk4G4ATb", "replyto": "mxjk4G4ATb", "signatures": ["ICLR.cc/2026/Conference/Submission9663/Reviewer_rV3c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9663/Reviewer_rV3c"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761375809265, "cdate": 1761375809265, "tmdate": 1762921186127, "mdate": 1762921186127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study the contextual MAB problem where each action incurs a context dependent time cost and the goal is to maximize the reward per unit time. They develop new algorithm called COAF that jointly learns the reward model with UCB style confidence band and also learns the optimal average reward. They also showcase the performance of the algorithm with theoretical guarantees with regret bound in several regimes.  The theoretical work is supported with experiments conducted on two real world data from different domains showcasing the adaptability of the proposed setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem setting is clearly motivated with a proper use case of cryo em data collection and is designed to tackle similar use case.\n\nThe problem formulation has a generalization over contextual bandits, combinatorial semi bandits, which makes it solid. Also, COAF is supported by optimality equation and design with its dependence. \n\nThe experimentation is supported by real world data to show the working validation of the motivating example. Along with it, they also show their performance on other domain with MovieLens data to showcase the wide adaptability of the setting. \n\nHaving per arm feedback within combinatorial choice helps reduces variance and seems to be realistic for their application."}, "weaknesses": {"value": "The setting allows for switching to a new decision sets but don't signify the regime when it is optimal as supposed to exploiting. \n\nThe experimentation lacks proper baseline to compare the effectiveness of the proposed algorithm COAP. \n\nThe problem setting has IID assumption with ($X_j$ ,$A_j$ , $l_j$), however this might applications where nonstationary has to dealt with and taken into account."}, "questions": {"value": "Since Latency aware contextual bandits seems to be the special case of contextual bandits and contextual semi bandits, If the action space and context of the arm is reduced to be similar to stochastic bandits, Does Latency aware Contextual bandits reduce to Budgeted bandits ? If so, how does the regret bound compare in this scenario ? \n\nIf a learner is allowed to request a new action set, how does this switch take latency into account, Is it already a part of the latency of the original selection action set ? \n\nSince the work is motivated by cryo em data collection, In latency aware contextual bandit setting with COAF can it exploit all the structure in the latency observed rather than treating it as arbitrary ?\n\nAlso, For the cryo-EM, Does COAP outperforms the strong domain specific heuristic or is there any advantage of using a learned policy for cryo em data collection application ? \n\nAlso the numerical experimentation only involves a baseline comparison with the humans and Can any of the contextual bandits setting be adapted with mild relaxation to consider them for baseline evaluation ?\n\nOften case, since cryo EM data collection involves human microscopists, drift in instrumentation or user's action changes mid run. In that case, under a IID assumption ($X_j$ ,$A_j$ , $l_j$), how does the algorithm behave ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UplIKlYcGC", "forum": "mxjk4G4ATb", "replyto": "mxjk4G4ATb", "signatures": ["ICLR.cc/2026/Conference/Submission9663/Reviewer_iCp9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9663/Reviewer_iCp9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968609468, "cdate": 1761968609468, "tmdate": 1762921185868, "mdate": 1762921185868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates a latency-aware contextual bandit problem, where each action (arm) incurs a random latency drawn from an unknown distribution. To capture the impact of latency on decision-making, the authors model the problem as a Markov Decision Process (MDP) and derive the corresponding Bellman optimality equation. Building upon this formulation, they propose an arm filtering algorithm that balances exploration and exploitation by accounting for both reward and latency. The proposed approach is demonstrated through experiments on the MovieLens 1M dataset and a Cryo-EM experimental setting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper has the following strengths:\n- The paper provides a theoretical formulation by modeling the latency-aware contextual bandit problem as an SMDP and deriving the corresponding Bellman optimality condition.\n- The paper introduces a contextual online arm filtering (COAF) algorithm based on the derived Bellman condition and establishes regret bounds for both linear and general reward function settings.\n- The problem is well-motivated by a real-world Cryo-EM application, and the proposed method is empirically validated on both MovieLens 1M and Cryo-EM datasets."}, "weaknesses": {"value": "The weaknesses are described below.\n- Although the paper formulates the latency-aware contextual bandit problem as an MDP, it does not clearly justify why the proposed method is preferable to existing MDP-based solutions.\n\n- The arm filtering design and regret analysis follow relatively standard techniques, and the paper does not clearly articulate new analytical challenges introduced by latency or contextual dependencies.\n\n- The study focuses solely on the stochastic setting, which can already be addressed by conventional MDP algorithms. Extending the formulation to adversarial or non-stationary environments would make the contribution more compelling.\n\n- The impact of action latency on the learning rate or convergence behavior is not explicitly analyzed or reflected in the algorithmic design, despite being central to the problem motivation.\n\n- The experimental evaluation is limited to the proposed method without comparisons against existing delayed-feedback bandits [1,2] or MDP-based algorithms, which weakens the empirical evidence supporting the algorithm’s effectiveness.\n\n\n[1] Masoudian, S., Zimmert, J. and Seldin, Y., 2022. A best-of-both-worlds algorithm for bandits with delayed feedback. Advances in Neural Information Processing Systems, 35, pp.11752-11762.\n\n[2] Lancewicki, T., Rosenberg, A. and Mansour, Y., 2022, June. Learning adversarial markov decision processes with delayed feedback. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 36, No. 7, pp. 7281-7289)."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3dwB88g4oa", "forum": "mxjk4G4ATb", "replyto": "mxjk4G4ATb", "signatures": ["ICLR.cc/2026/Conference/Submission9663/Reviewer_f5wW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9663/Reviewer_f5wW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762232502253, "cdate": 1762232502253, "tmdate": 1762921185328, "mdate": 1762921185328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines a version of the contextual combinatorial bandit problem in which each action (a subset of arms) incurs a latency—a variable time cost. The goal is to maximize expected reward per unit of elapsed time rather than per round.\n\nThe authors model this as an average-reward semi-Markov decision process (SMDP) and derive a Bellman optimality equation of the form\nE_{(X,A,l)}\\!\\left[\\min_{A\\in\\mathcal{A}}\\{l(A)\\Gamma - \\sum_{i\\in A}\\mu_i\\}\\right] = 0,\nwhere \\Gamma represents the long-run average reward rate.\nThey then propose an algorithm, COAF (Contextual Online Arm Filtering), that combines a Robbins–Monro–type stochastic approximation for estimating \\( \\Gamma^\\* \\) with UCB-style exploration for learning the arm rewards \\mu_i(x).\n\nRegret bounds of order O(T^{3/4}) are proved under both linear and general function classes, and experiments on synthetic data (MovieLens) and a cryo-electron microscopy (cryo-EM) simulation are presented."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The latency-aware formulation is conceptually relevant to real scientific workflows.\n2) The mathematical derivations are careful and correct.\n3) The paper is generally well written and easy to follow.\n4) The cryo-EM example adds color and a nice application context."}, "weaknesses": {"value": "1.\tThe regret bound is very likely suboptimal.\n\t2.\tThere is no lower bound or discussion of optimality.\n\t3.\tThe “latency” feature mostly amounts to a time-rescaling, I think; it is not clear why this warrants a fundamentally new theory. \n\t4.\tThe experiments lack statistical rigor—no error bars or serious baselines.\n\t5.\tOverall novelty is modest: the algorithm is a straightforward hybrid of known tools (UCB + stochastic approximation)."}, "questions": {"value": "1.\tDo you believe the T^{3/4} rate is unavoidable, or is it simply an artifact of your analysis?\n\t2.\tWhen latencies are known and bounded, why can’t the setting be reduced to a contextual bandit with a random time clock?\n\t3.\tCould one obtain a sharper \\sqrt{T}-type result using a ratio or Dinkelbach-style formulation?\n\t4.\tWhat exactly does “throughput” measure in the cryo-EM experiment, and how does it relate to \\(\\Gamma^\\*\\)?\n\t5.\tPlease clarify whether the cryo-EM data come from real microscope logs or a synthetic simulator."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QsMQltprh7", "forum": "mxjk4G4ATb", "replyto": "mxjk4G4ATb", "signatures": ["ICLR.cc/2026/Conference/Submission9663/Reviewer_9zTZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9663/Reviewer_9zTZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762244965198, "cdate": 1762244965198, "tmdate": 1762921184692, "mdate": 1762921184692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}