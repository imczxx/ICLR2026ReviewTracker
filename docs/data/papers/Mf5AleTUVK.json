{"id": "Mf5AleTUVK", "number": 16241, "cdate": 1758262241639, "mdate": 1763602556766, "content": {"title": "In-The-Flow Agentic System Optimization for Effective Planning and Tool Use", "abstract": "Outcome-driven reinforcement learning has advanced reasoning in large language models (LLMs), but prevailing tool-augmented approaches train a single, monolithic policy that interleaves thoughts and tool calls under full context; this scales poorly with long horizons and diverse tools and generalizes weakly to new scenarios. Agentic systems offer a promising alternative by decomposing work across specialized modules, yet most remain training-free or rely on offline training decoupled from the live dynamics of multi-turn interaction. We introduce AgentFlow, a trainable, *in-the-flow* agentic framework that coordinates four modules (planner, executor, verifier, generator) through an evolving memory and directly optimizes its planner inside the multi-turn loop. To train on-policy in live environments, we propose *Flow-based Group Refined Policy Optimization* (Flow-GRPO), which tackles long-horizon, sparse-reward credit assignment by converting multi-turn optimization into a sequence of tractable single-turn policy updates. It broadcasts a single, verifiable trajectory-level outcome to every turn to align local planner decisions with global success and stabilizes learning with group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale backbone outperforms top-performing baselines with average accuracy gains of 14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on scientific tasks, even surpassing larger proprietary models like GPT-4o. Further analyses confirm the benefits of in-the-flow optimization, showing improved planning, enhanced tool-calling reliability, and positive scaling with model size and reasoning turns. Codebase is available at https://anonymous.4open.science/r/agentflow.", "tldr": "We introduce AgentFlow, a trainable agentic system, and Flow-GRPO, an on-policy RL algorithm that optimizes the planner \"in-the-flow\" by broadcasting a final outcome reward to all steps, enabling effective long-horizon planning and tool use.", "keywords": ["Reinforcement Learning", "Large Language Models", "Agentic Systems", "Tool Use", "Planning", "On-policy Optimization", "Sparse Rewards"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/18f2ab164de5393c73493cf1be9fa80a2df65cb6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AGENTFLOW, a trainable agentic system that enables multi-turn reasoning and tool use by optimizing its planner in the flow of execution. Traditional tool-augmented LLMs train a single, monolithic policy that struggles with long-horizon reasoning. AGENTFLOW instead decomposes reasoning into four interacting modules that coordinate through a shared evolving memory.\nThe key technical contribution is Flow-based Group Refined Policy Optimization (Flow-GRPO), an on-policy RL algorithm that converts sparse, long-horizon reinforcement learning into tractable per-turn updates by broadcasting a single trajectory-level reward to all turns and using group-normalized advantages for stability. This design allows for end-to-end optimization of the planner within the live agentic loop."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses the sparse reward problem in LLM-based reasoning and enables the training of modular, multi-tool systems in dynamic environments\n\n- strong empirical results"}, "weaknesses": {"value": "- representation should be improved; it is super hard to follow\n\n- The reliance on LLM-as-a-Judge rewards risks evaluation leakage. I also find the direct distribution of the global reward across turns unconvincing.\n\n- Although the results are mostly qualitative and good, the paper does not report the computational cost, which raises concerns about efficiency."}, "questions": {"value": "First of all, I would like to thank the authors for their work.\nI would like to ask how reward hacking is avoided when global rewards are distributed directly to each turn? In such cases, even undesirable behaviors within a successful trajectory could be reinforced, potentially leading to unnecessarily long or inefficient trajectories. Additionally, could the authors comment on the computational cost of the training process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5tONEMypI5", "forum": "Mf5AleTUVK", "replyto": "Mf5AleTUVK", "signatures": ["ICLR.cc/2026/Conference/Submission16241/Reviewer_1fZQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16241/Reviewer_1fZQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761685766463, "cdate": 1761685766463, "tmdate": 1762926396863, "mdate": 1762926396863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces AgentFlow, a trainable, in-the-flow agentic framework that coordinates four modules through an evolving memory, optimizing the planner on-policy within the multi-turn reasoning loop. It further proposes Flow-GRPO, a novel reinforcement learning algorithm that converts long-horizon, sparse-reward multi-turn optimization into tractable single-turn updates by broadcasting a trajectory-level reward to all turns. Experiments across benchmarks demonstrate substantial gains over state-of-the-art models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of in-the-flow reinforcement learning for agentic systems is both useful and interesting.\n2. The proposed Flow-GRPO provides a stable, elegant formulation for long-horizon credit assignment.\n3. The authors conduct comprehensive evaluation across multiple domains, outperforming competitive baselines."}, "weaknesses": {"value": "1. The authors conduct comprehensive experiments. It's better to add discussion on computational cost and training stability.\n2. The proposed method performs good on text-based tasks. How about the results on dynamic environments or multi-modal settings?\n3. Minor: It's better to add some task description presented in appendix in Figure 1 or 2 to make the solution more clear."}, "questions": {"value": "It's a well-written and comprehensive paper. Some potential improvements please refer to the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a3yy6mWcXG", "forum": "Mf5AleTUVK", "replyto": "Mf5AleTUVK", "signatures": ["ICLR.cc/2026/Conference/Submission16241/Reviewer_YoHg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16241/Reviewer_YoHg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761686703550, "cdate": 1761686703550, "tmdate": 1762926396478, "mdate": 1762926396478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of recognized strengths, new experiments, and analysis"}, "comment": {"value": "## Recognized Strengths and Contributions\n\nWe are grateful for the reviewers’ time and their insightful comments on our submission. We are encouraged that all reviewers consistently recognized our work as making strong and meaningful contributions to the development of trainable agentic LLM systems.\n\n**1. Novel and well-motivated agentic framework**\n\nAll reviewers highlighted the contribution of AgentFlow’s decomposition into specialized modules, which directly addresses the limitations of monolithic tool-augmented LLMs and static agentic pipelines, and was viewed as a **scalable and principled** architecture (VTBo). Reviewers emphasized that AgentFlow provides “an **innovative and well-motivated** design” (VTBo) and introduces a “**useful and interesting** idea” of in-the-flow training (YoHg).\n\n**2. Flow-GRPO as a significant methodological contribution**\n\nAll reviewers repeatedly highlighted the novelty and technical contribution of Flow-GRPO as “a **well-constructed** solution to the long-horizon, sparse-reward problem” (VTBo) and “a **stable, elegant** formulation for long-horizon credit assignment” (YoHg). Reviewer R3 similarly noted that the work “addresses the sparse reward problem in LLM-based reasoning” (1fZQ). Reviewers appreciated the conceptual clarity of converting trajectory-level rewards into token-level updates and its ability to train planners “within the live agentic loop” (1fZQ).\n\n**3. Strong empirical results across diverse benchmarks**\n\nAll reviewers praised the empirical strength of the paper, noting the “**impressive, comprehensive**” evaluations. The system consistently outperformed both open-source and proprietary models, with “**consistent, substantial** accuracy gains” (VTBo, YoHg), and Reviewer R3 also recognized the “**strong** empirical results” (1fZQ). The breadth of benchmarks (search, agentic, math, science) was acknowledged as a strength.\n\n**4. Clarity of methodology and practical impact for future work**\n\nReviewers highlighted the paper’s “**clear and robust methodological foundation**” (VTBo) and described it as a “well-written and comprehensive paper” (YoHg). Reviewer VTBo further noted that “the clarity of the code and modular design of AgentFlow suggest **strong potential for adoption by the community**,” emphasizing that “open-sourcing this framework would likely **facilitate further research and development in agentic LLM systems**”. Overall, the modular structure and open-source readiness were viewed as highly valuable for the broader community, enhancing reproducibility and practical impact.\n\n---\n## Summary of New Experiments and Analysis\n\n**1. Generalization to new tools, tasks, and modalities.**\n\nWe added experiments showing that AgentFlow generalizes (i) to **unseen tools** (5 extra, including irrelevant distractors), (ii) to **unseen tasks** (e.g., MedMCQA, MMLU-Clinical, beyond math+search training), and (iii) to **new modalities** via added **visual tools** on MathVista, VQA v2, and CLEVR-Math, all with consistent gains.\n\n**2. Scaling and stability across backbones and architectures.**\n\nWe extended Flow-GRPO to **smaller and larger backbones** (Qwen2.5-1.5B, 3B, 7B, and Llama 3.2-3B, Llama 3.1-8B) and showed **stable, monotonic improvements** across all ten benchmarks.\n\n**3. Robustness in dynamic / noisy tool environments and verifier effectiveness.**\n\nWe introduced **random tool-failure experiments** (extra error probability $p \\in {0.1, 0.2, 0.4, 0.6}$) and showed the tuned AgentFlow consistently outperforms the untuned system and degrades smoothly even at high noise levels. Verifier analysis shows it triggers continued reasoning in **≈98%** of error steps, effectively mitigating faulty tool calls.\n\n**4. Reward-design ablations and analysis of global broadcasting.**\n\nWe compared our **broadcasted final-outcome reward** against (i) backward-discounted, (ii) forward-discounted, and (iii) process-based step rewards on both 3B and 7B backbones, and found that **broadcasting consistently achieves the best average performance and stability**. We further implemented a **purely rule-based “Math Verify and Match”** reward (numeric / exact match) and showed similar trends but lower robustness across heterogeneous tasks, clarifying why we use an LLM-as-judge strictly as a **binary, final-only** verifier.\n\n**5. Computational cost and training-efficiency analysis.**\n\nWe added a detailed **cost breakdown** for training AgentFlow-7B with Flow-GRPO, highlighting its high efficiency.\n\n---\n## Updated Paper and Final Version Plan\n\nWe have revised **[the paper pdf (click here)](https://openreview.net/pdf?id=Mf5AleTUVK)** for clearer presentation, including improved figures and organization.\n\nWe will incorporate new experiments and analyses into the final version. Beyond the manuscript, we will also release a **video tutorial, live demo, and interactive doc** to make our work more accessible to the community; these resources will be released after anonymity is lifted."}}, "id": "1LWWKUTKfK", "forum": "Mf5AleTUVK", "replyto": "Mf5AleTUVK", "signatures": ["ICLR.cc/2026/Conference/Submission16241/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16241/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16241/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763602819730, "cdate": 1763602819730, "tmdate": 1763602819730, "mdate": 1763602819730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AgentFlow, a trainable agentic framework designed to address the challenges of tool-augmented reasoning in large language models (LLMs). By decomposing tasks into specialized modules (planner, executor, verifier, generator) and optimizing the planner through in-the-flow, on-policy training, AgentFlow achieves significant improvements in long-horizon tasks. The proposed Flow-GRPO algorithm tackles the sparse-reward, multi-turn credit assignment problem by converting it into tractable single-turn updates, aligning local decisions with global outcomes. Empirical evaluations across ten benchmarks demonstrate the framework’s effectiveness, with substantial accuracy gains over existing baselines and even larger proprietary models like GPT-4o."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The introduction of AgentFlow as a trainable and modular agentic framework is innovative, particularly in how it addresses key limitations of monolithic policies and static, offline training approaches. The decomposition into specialized modules is a well-motivated design that aligns with the principles of scalability and adaptability to diverse tools and tasks.\n2. The paper provides a clear and robust methodological foundation. The Flow-GRPO algorithm is a well-constructed solution to the long-horizon, sparse-reward problem, and the use of trajectory-level outcomes to guide local updates is both elegant and effective.\n3. The experimental results are impressive, showcasing consistent gains across a wide range of benchmarks, including search, agentic, mathematical, and scientific tasks. The 14.9% average improvement in search tasks and 14.5% gain in mathematical tasks highlight the framework’s practical value.\n4. The clarity of the code and modular design of AgentFlow suggest strong potential for adoption by the community. Open-sourcing this framework would likely facilitate further research and development in agentic LLM systems."}, "weaknesses": {"value": "1. While the framework demonstrates strong performance across selected benchmarks, the paper could provide more discussion on how well AgentFlow generalizes to entirely unseen tools or tasks, especially in zero-shot or low-resource settings.\n2. The on-policy training approach, while effective, may introduce additional computational complexity compared to training-free or offline methods. A discussion on the trade-offs between performance gains and computational costs would strengthen the narrative."}, "questions": {"value": "1. How does the Flow-GRPO algorithm scale with increasing model size or when dealing with a significantly larger number of tools? Are there any bottlenecks or limitations observed during training?\n2. How robust is AgentFlow in noisy or adversarial environments where tools may fail or provide incorrect outputs? Does the verifier module effectively mitigate such issues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7xQJxcig5q", "forum": "Mf5AleTUVK", "replyto": "Mf5AleTUVK", "signatures": ["ICLR.cc/2026/Conference/Submission16241/Reviewer_VTBo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16241/Reviewer_VTBo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900038126, "cdate": 1761900038126, "tmdate": 1762926395947, "mdate": 1762926395947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}