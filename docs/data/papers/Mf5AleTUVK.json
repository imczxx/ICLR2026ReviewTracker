{"id": "Mf5AleTUVK", "number": 16241, "cdate": 1758262241639, "mdate": 1759897252642, "content": {"title": "In-The-Flow Agentic System Optimization for Effective Planning and Tool Use", "abstract": "Outcome-driven reinforcement learning has advanced reasoning in large language models (LLMs), but prevailing tool-augmented approaches train a single, monolithic policy that interleaves thoughts and tool calls under full context; this scales poorly with long horizons and diverse tools and generalizes weakly to new scenarios. Agentic systems offer a promising alternative by decomposing work across specialized modules, yet most remain training-free or rely on offline training decoupled from the live dynamics of multi-turn interaction. We introduce AgentFlow, a trainable, *in-the-flow* agentic framework that coordinates four modules (planner, executor, verifier, generator) through an evolving memory and directly optimizes its planner inside the multi-turn loop. To train on-policy in live environments, we propose *Flow-based Group Refined Policy Optimization* (Flow-GRPO), which tackles long-horizon, sparse-reward credit assignment by converting multi-turn optimization into a sequence of tractable single-turn policy updates. It broadcasts a single, verifiable trajectory-level outcome to every turn to align local planner decisions with global success and stabilizes learning with group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale backbone outperforms top-performing baselines with average accuracy gains of 14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on scientific tasks, even surpassing larger proprietary models like GPT-4o. Further analyses confirm the benefits of in-the-flow optimization, showing improved planning, enhanced tool-calling reliability, and positive scaling with model size and reasoning turns. Codebase is available at https://anonymous.4open.science/r/agentflow.", "tldr": "We introduce AgentFlow, a trainable agentic system, and Flow-GRPO, an on-policy RL algorithm that optimizes the planner \"in-the-flow\" by broadcasting a final outcome reward to all steps, enabling effective long-horizon planning and tool use.", "keywords": ["Reinforcement Learning", "Large Language Models", "Agentic Systems", "Tool Use", "Planning", "On-policy Optimization", "Sparse Rewards"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/76c2b548541a23fcb316858b62bf1d6836f4fb6a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AGENTFLOW, a trainable agentic system that enables multi-turn reasoning and tool use by optimizing its planner in the flow of execution. Traditional tool-augmented LLMs train a single, monolithic policy that struggles with long-horizon reasoning. AGENTFLOW instead decomposes reasoning into four interacting modules that coordinate through a shared evolving memory.\nThe key technical contribution is Flow-based Group Refined Policy Optimization (Flow-GRPO), an on-policy RL algorithm that converts sparse, long-horizon reinforcement learning into tractable per-turn updates by broadcasting a single trajectory-level reward to all turns and using group-normalized advantages for stability. This design allows for end-to-end optimization of the planner within the live agentic loop."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses the sparse reward problem in LLM-based reasoning and enables the training of modular, multi-tool systems in dynamic environments\n\n- strong empirical results"}, "weaknesses": {"value": "- representation should be improved; it is super hard to follow\n\n- The reliance on LLM-as-a-Judge rewards risks evaluation leakage. I also find the direct distribution of the global reward across turns unconvincing.\n\n- Although the results are mostly qualitative and good, the paper does not report the computational cost, which raises concerns about efficiency."}, "questions": {"value": "First of all, I would like to thank the authors for their work.\nI would like to ask how reward hacking is avoided when global rewards are distributed directly to each turn? In such cases, even undesirable behaviors within a successful trajectory could be reinforced, potentially leading to unnecessarily long or inefficient trajectories. Additionally, could the authors comment on the computational cost of the training process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5tONEMypI5", "forum": "Mf5AleTUVK", "replyto": "Mf5AleTUVK", "signatures": ["ICLR.cc/2026/Conference/Submission16241/Reviewer_1fZQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16241/Reviewer_1fZQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761685766463, "cdate": 1761685766463, "tmdate": 1762926396863, "mdate": 1762926396863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces AgentFlow, a trainable, in-the-flow agentic framework that coordinates four modules through an evolving memory, optimizing the planner on-policy within the multi-turn reasoning loop. It further proposes Flow-GRPO, a novel reinforcement learning algorithm that converts long-horizon, sparse-reward multi-turn optimization into tractable single-turn updates by broadcasting a trajectory-level reward to all turns. Experiments across benchmarks demonstrate substantial gains over state-of-the-art models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of in-the-flow reinforcement learning for agentic systems is both useful and interesting.\n2. The proposed Flow-GRPO provides a stable, elegant formulation for long-horizon credit assignment.\n3. The authors conduct comprehensive evaluation across multiple domains, outperforming competitive baselines."}, "weaknesses": {"value": "1. The authors conduct comprehensive experiments. It's better to add discussion on computational cost and training stability.\n2. The proposed method performs good on text-based tasks. How about the results on dynamic environments or multi-modal settings?\n3. Minor: It's better to add some task description presented in appendix in Figure 1 or 2 to make the solution more clear."}, "questions": {"value": "It's a well-written and comprehensive paper. Some potential improvements please refer to the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a3yy6mWcXG", "forum": "Mf5AleTUVK", "replyto": "Mf5AleTUVK", "signatures": ["ICLR.cc/2026/Conference/Submission16241/Reviewer_YoHg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16241/Reviewer_YoHg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761686703550, "cdate": 1761686703550, "tmdate": 1762926396478, "mdate": 1762926396478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AgentFlow, a trainable agentic framework designed to address the challenges of tool-augmented reasoning in large language models (LLMs). By decomposing tasks into specialized modules (planner, executor, verifier, generator) and optimizing the planner through in-the-flow, on-policy training, AgentFlow achieves significant improvements in long-horizon tasks. The proposed Flow-GRPO algorithm tackles the sparse-reward, multi-turn credit assignment problem by converting it into tractable single-turn updates, aligning local decisions with global outcomes. Empirical evaluations across ten benchmarks demonstrate the framework’s effectiveness, with substantial accuracy gains over existing baselines and even larger proprietary models like GPT-4o."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The introduction of AgentFlow as a trainable and modular agentic framework is innovative, particularly in how it addresses key limitations of monolithic policies and static, offline training approaches. The decomposition into specialized modules is a well-motivated design that aligns with the principles of scalability and adaptability to diverse tools and tasks.\n2. The paper provides a clear and robust methodological foundation. The Flow-GRPO algorithm is a well-constructed solution to the long-horizon, sparse-reward problem, and the use of trajectory-level outcomes to guide local updates is both elegant and effective.\n3. The experimental results are impressive, showcasing consistent gains across a wide range of benchmarks, including search, agentic, mathematical, and scientific tasks. The 14.9% average improvement in search tasks and 14.5% gain in mathematical tasks highlight the framework’s practical value.\n4. The clarity of the code and modular design of AgentFlow suggest strong potential for adoption by the community. Open-sourcing this framework would likely facilitate further research and development in agentic LLM systems."}, "weaknesses": {"value": "1. While the framework demonstrates strong performance across selected benchmarks, the paper could provide more discussion on how well AgentFlow generalizes to entirely unseen tools or tasks, especially in zero-shot or low-resource settings.\n2. The on-policy training approach, while effective, may introduce additional computational complexity compared to training-free or offline methods. A discussion on the trade-offs between performance gains and computational costs would strengthen the narrative."}, "questions": {"value": "1. How does the Flow-GRPO algorithm scale with increasing model size or when dealing with a significantly larger number of tools? Are there any bottlenecks or limitations observed during training?\n2. How robust is AgentFlow in noisy or adversarial environments where tools may fail or provide incorrect outputs? Does the verifier module effectively mitigate such issues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7xQJxcig5q", "forum": "Mf5AleTUVK", "replyto": "Mf5AleTUVK", "signatures": ["ICLR.cc/2026/Conference/Submission16241/Reviewer_VTBo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16241/Reviewer_VTBo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900038126, "cdate": 1761900038126, "tmdate": 1762926395947, "mdate": 1762926395947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}