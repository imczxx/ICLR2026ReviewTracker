{"id": "TIaHBGnU7s", "number": 10782, "cdate": 1758181824874, "mdate": 1762935839801, "content": {"title": "Context-Aware Input Switching in Mobile Devices: A Multi-Language, Emoji-Integrated Typing System", "abstract": "Multilingual and emoji-integrated typing has become increasingly prevalent in mobile communications, especially among users who frequently switch between languages and expressive symbols in real-time conversations. This phenomenon is particularly pronounced in linguistically diverse regions where code switching, the practice of alternating between multiple languages within discourse, has become integral to natural communication patterns. However, existing mobile input systems largely rely on static language models that fail to capture the dynamic, context-dependent nature of multilingual typing, resulting in suboptimal user experiences characterized by frequent manual switching, prediction errors, and substantial latency overhead. To address these limitations, we introduce CAISS (Context-Aware Input Switching System), a novel neural architecture that revolutionizes multilingual mobile input through predictive language switching. Unlike traditional reactive systems, CAISS proactively anticipates user language transitions by modeling the complex interplay between linguistic patterns, temporal dynamics, application contexts, and social cues using a sophisticated multi-scale attention mechanism suitable for edge deployment. We construct a comprehensive multilingual-emoji typing dataset and evaluate CAISS against commercial baselines across six languages commonly used in code-switching contexts: English, Mandarin Chinese, Cantonese, Malay, Tamil, and Vietnamese. Our experimental results reveal substantial improvements over existing approaches, with CAISS achieving a 23.8\\% enhancement in switching accuracy and a remarkable 34.1\\% reduction in typing latency. The system's lightweight architecture, comprising only 2.5M parameters, enables real-time inference with sub-10ms latency on contemporary mobile processors while maintaining competitive performance across diverse linguistic and contextual conditions.", "tldr": "", "keywords": ["Context-Aware", "​​Input Switching", "Multilingual"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/1e19e3c6fcf3ee2460752a09b60bd81b9900ce45.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "n this paper, the authors introduced CAISS, a lightweight context-aware system that predicts multilingual and emoji input switching on mobile devices. The system achieves significant gains in accuracy, latency, and naturalness over existing keyboards. The system architecture and design are novel and practical. However, the paper seems to have limited language coverage, relies on synthetic data, and includes a limited-scale human evaluation, raising questions about the generalizability of the study presented in this paper."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "the system (caiss) proposed by this paper is very efficient, with only 2.5M params with under 10 ms latency and around 15 mb memory usage. This enables the model to run on standard mobile hardware, which makes it practical."}, "weaknesses": {"value": "Although it supports multiple languages. the paper covers six mainly asian languages, which is giving question on its diversity and representativeness. It is not clear on whether the system would generalize to other language families or multilingual contexts."}, "questions": {"value": "How were the contextual features such as linguistic, application, temporal, and social data encoded within the model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9KlXOscIJC", "forum": "TIaHBGnU7s", "replyto": "TIaHBGnU7s", "signatures": ["ICLR.cc/2026/Conference/Submission10782/Reviewer_odcd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10782/Reviewer_odcd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761382052377, "cdate": 1761382052377, "tmdate": 1762921998725, "mdate": 1762921998725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "hItZEqrqfo", "forum": "TIaHBGnU7s", "replyto": "TIaHBGnU7s", "signatures": ["ICLR.cc/2026/Conference/Submission10782/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10782/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762934779328, "cdate": 1762934779328, "tmdate": 1762934779328, "mdate": 1762934779328, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a system for improving mobile input by tackling the challenging phenomenon of code-switching and the utilization of expressive symbols (emojis). The main idea depends on moving beyond static LMs by examining a context-aware fusion architecture that integrates two primary information modalities. The proposed system predicts the next input modality whether it is a switch to a different language or the insertion of an emoji to provide real-time assistance. The authors demonstrate that this multimodal approach significantly outperforms standard unigram and static Markov-model-based baselines across multiple language pairs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1) The explicit modeling of emoji usage as a separate, albeit integrated, expressive modality is one of the main contributions.\n\nS2) The architecture successfully fuses long-term context with short-term, sparse expressive signals, which is a non-trivial fusion challenge.\n\nS3) Including a human naturalness assessment is one of the great approaches for code-switching tasks"}, "weaknesses": {"value": "W1) The current treatment of emojis appears primarily token-based. It limits to the understanding of a deeper modality analysis to determine if the system benefits from the semantic or affective content of the emojis, or merely their presence.\n\nW2) The authors do not provide any detailed ablation study isolating the contribution of the two modalities. \n\nSee questions."}, "questions": {"value": "Q1) What kinds of fusion layers were employed? It is required to provide some mechanisms by which the dense, sequential linguistic representation is combined with the potentially sparse, bursty expressive representation.\n\nQ2) Can the authors provide some detailed information of the computational overhead?\n\nQ3) Were the emoji embeddings initialized differently or were they learned purely de novo from the switching task? \n\nQ4) Could the authors provide a deeper error analysis? For example, what are the most common scenarios where the model predicts the wrong language versus predicting no switch when one was needed, and how do emoji inputs factor into these specific failure cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZhCMk7tEsf", "forum": "TIaHBGnU7s", "replyto": "TIaHBGnU7s", "signatures": ["ICLR.cc/2026/Conference/Submission10782/Reviewer_pWxv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10782/Reviewer_pWxv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755683785, "cdate": 1761755683785, "tmdate": 1762921998043, "mdate": 1762921998043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CAISS, a context-aware input switching system designed to enhance multilingual mobile typing by predicting when users need to switch languages. By integrating insights from code-switching linguistics, multilingual NLP, and context-aware computing, CAISS was rigorously evaluated on a newly curated multilingual typing dataset. Results show that CAISS improves language switching accuracy and reduces typing latency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The system enables context-aware prediction of when users are likely to switch languages or input modes—leveraging typing patterns, application context, and social cues—to reduce the number of user interruptions and minimize waiting latency.\n\n2. The paper evaluates the CAISS approach on a comprehensive multilingual typing dataset derived from real-world mobile usage patterns across six languages commonly used in multilingual settings, demonstrating its superior performance.\n\n3. This paper proposes a large-scale multilingual emoji-typing dataset that captures natural code-switching patterns in mobile communication."}, "weaknesses": {"value": "1. The paper provides an overly brief description of the model architecture, leaving readers without a clear understanding of its structural design, input–output flow, or data formatting.\n2. In the discussion of inference latency (Lines 314–323), the paper lacks sufficient detail about the hardware setup, making it difficult to form an intuitive sense of the reported latency figures.\n3. From a structural perspective, the paper’s primary novelty appears to lie in its fine-tuned dataset. Moreover, the baselines used for comparison are predominantly statistical methods, with very few neural-network-based approaches included for evaluation.\n4. It would greatly enhance the paper’s impact if the authors could explicitly discuss the trade-off between latency and the benefits of context-aware prediction, helping readers better appreciate the practical value of their approach."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3q2r7kVYXI", "forum": "TIaHBGnU7s", "replyto": "TIaHBGnU7s", "signatures": ["ICLR.cc/2026/Conference/Submission10782/Reviewer_Yrhf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10782/Reviewer_Yrhf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825752308, "cdate": 1761825752308, "tmdate": 1762921995929, "mdate": 1762921995929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study proposed a CAISS framework for automatically switching user language input in a mobile environment. Using a multi-scale attention encoder, a context-fusion model was trained to learn usage patterns and predict code-switching. This achieved an accuracy increase of 23.8% compared to the commercial model."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The model achieves approximately twice the latency reduction compared to manual switching.\n- If reproducibility is guaranteed in the paper, the proposed approach demonstrates the potential for real-time processing in mobile environments."}, "weaknesses": {"value": "- The discussion of prior work is insufficient. The related work section focuses on model design rather than aligning with the main claims. Existing research, such as [r1] and [r2] on multilingual and code-switching input, is not adequately compared or discussed.\n- The paper lacks a clear explanation of the model architecture and its novel contribution, making reproducibility low.\n- Data collection details are missing: no description of procedure, participant demographics, or ethical approval, despite mentioning human data collection.\n- Experimental setup is under-specified (e.g., no details on mobile device type or hardware).\n- The baseline is minimal and not well-aligned with prior research.\n- For real-time binary detection, precision and false positives are more critical than F1, but the paper does not analyze these metrics.\n\n\nReferences:\n\n[r1] Samih, Y., Maharjan, S., Attia, M., Kallmeyer, L., & Solorio, T. (2016). Multilingual code-switching identification via LSTM recurrent neural networks. In Proceedings of the Second Workshop on Computational Approaches to Code Switching (pp. 50–59). Austin, TX, USA, November 1, 2016. Association for Computational Linguistics. \n\n[r2] Tsoukala, C., Broersma, M., van den Bosch, A., & Frank, S. L. (2021). Simulating code-switching using a neural network model of bilingual sentence production. Computational Brain & Behavior, 4, 87–100. https://doi.org/10.1007/s42113-020-00088-6."}, "questions": {"value": "Please also address the points raised in the Weakness section.\n- How were the data collection and ethics approval processes conducted?\n- What are the novel aspects of the model architecture?\n- Are precision results and FP rates available?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rJRxdQUNpB", "forum": "TIaHBGnU7s", "replyto": "TIaHBGnU7s", "signatures": ["ICLR.cc/2026/Conference/Submission10782/Reviewer_UQHp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10782/Reviewer_UQHp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877424937, "cdate": 1761877424937, "tmdate": 1762921992530, "mdate": 1762921992530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}