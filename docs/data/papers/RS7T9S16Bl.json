{"id": "RS7T9S16Bl", "number": 7878, "cdate": 1758040699072, "mdate": 1759897824731, "content": {"title": "Music Flamingo: Scaling Music Understanding in Audio Language Models", "abstract": "We introduce Music Flamingo, a novel large audio–language model, designed to advance music (including song) understanding in foundational audio models. While audio–language research has progressed rapidly, music remains challenging due to its dynamic, layered, and information-dense nature. Progress has been further limited by the difficulty of scaling open audio understanding models, primarily because of the scarcity of high-quality music data and annotations. As a result, prior models are restricted to producing short, high-level captions, answering only surface-level questions, and showing limited generalization across diverse musical cultures. To address these challenges, we curate MF-Skills, a large-scale dataset labeled through a multi-stage pipeline that yields rich captions and question–answer pairs covering harmony, structure, timbre, lyrics, and cultural context. We fine-tune an enhanced Audio Flamingo 3 backbone on MF-Skills and further strengthen multiple skills relevant to music understanding. To improve the model's reasoning abilities, we introduce a post-training recipe: we first cold-start with MF-Think, a novel chain-of-thought dataset grounded in music theory, followed by GRPO-based reinforcement learning with custom rewards. Music Flamingo achieves state-of-the-art results across 10+ benchmarks for music understanding and reasoning, establishing itself as a generalist and musically intelligent audio–language model. Beyond strong empirical results, Music Flamingo sets a new standard for advanced music understanding by demonstrating how models can move from surface-level recognition toward layered, human-like perception of songs. We believe this work provides both a benchmark and a foundation for the community to build the next generation of models that engage with music as richly and meaningfully as humans do. Demo: https://musicflamingo.github.io", "tldr": "A foundational large audio-language model focused on music understanding and reasoning", "keywords": ["music", "audio", "multi-modal", "language model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3ec84cc33a0c36199ec9d9c820eda565784dac85.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Music Flamingo, a large audio–language model, designed to advance music understanding in foundational audio models. Music Flamingo is a fine-tuned model based on Audio Flamingo 3 backbone. This paper also introduces MF-Skills and MF-Think, two large-scale datasets containing music–caption and music–QA pairs designed to promote deliberate reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-\tMusic Flamingo achieves state-of-the-art results on 12 music understanding and reasoning benchmarks.\n-\tLow-level attributes are extracted using the MIR tool, and initial captions combined with these attributes are used to reduce hallucination, creating captions and QA data that include multifaceted information to construct MF-Skills data. Synthetic data is built with attention to hallucination by LLM, considering important characteristics in the music domain, and an effective approach has been proposed."}, "weaknesses": {"value": "-\tThe ablation study is missing. Although improvements are made based on AudioFramingo 3, the model is trained in three stages: w/MF-skill, w/MF-think, and GRPO with custom rewards. Having evaluation results for each stage would help confirm each contribution.\n-\tSome details about the dataset creation are not described."}, "questions": {"value": "- There seems no explanation of how and from where the 2M songs for MT-skills were collected. Additionally, is there a possibility that the data used for evaluation is included in the training data?\n- I could not find an explanation for why the number of examples increases from 2M songs to 3M examples in MT-Skills. Does this mean that captions are added to all 2M songs, and some songs also have QA, thus resulting in 3M examples including pairs of music-caption and/or music-QA?\n- Does AF3-St. 3 in Table 2 refer to the training data of the base model AudioFramingo3? Additionally, does 2.0 of MF-SFT mean adding MF-Skills twice? Are the other data also used in the training of AudioFramingo3 and then reused in fine-tuning for Music Framingo?\n- It would be beneficial to elaborate on how quality filtering is performed.\n- How were the categories in Figure 27 analyzed?\n- Will the created dataset and source code/checkpoints be made publicly available?\n- How were the inputs and outputs for ACC evaluation handled?\n- In Figure 3, the term \"MF-skills CoT\" is used, but the caption seems to explain it as \"MF-Think.\" It would be better to standardize the terminology.\n- It seems Figures 4 and 27 are identical, but only Figure 27 is mentioned in the text. It would be better to refer to Figure 4 and note in its caption that an enlarged version is included as Figure 27 in the Appendix."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zIdUP4h2Nd", "forum": "RS7T9S16Bl", "replyto": "RS7T9S16Bl", "signatures": ["ICLR.cc/2026/Conference/Submission7878/Reviewer_qEKD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7878/Reviewer_qEKD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7878/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761573666239, "cdate": 1761573666239, "tmdate": 1762919912357, "mdate": 1762919912357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Music Flamingo, a large multimodal model extending the Flamingo architecture to handle music, text, and symbolic representations jointly. The model is trained on large-scale music–text–symbol triplets and evaluated across multiple tasks, including music captioning, retrieval, tagging, and question answering. The paper aims to demonstrate the benefits of scaling and unified modeling for general-purpose “music understanding.”\n\nOverall, this is a solid and technically competent piece of work, with careful engineering, broad evaluation, and clear connections to the audio Flamingo. However, several conceptual and empirical gaps remain regarding the necessity of this architecture, its comparison to specialized systems, and its coverage of affective aspects of music understanding."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Comprehensive system design — The adaptation of the Flamingo multimodal framework to the music domain is well-motivated and technically executed. Training across multiple modalities (audio, symbolic, text) is a significant engineering contribution.\n\nUnified modeling framework — Unlike prior task-specific MIR models, the same model handles captioning, retrieval, tagging, and QA, demonstrating promising few-shot and zero-shot generalization.\n\nDataset and reproducibility — The paper provides a substantial and well-curated multimodal dataset and detailed training setup, which is a strength for community reuse."}, "weaknesses": {"value": "1. Necessity over specialized MIR pipelines\n\nWhile the unified approach is appealing, it remains unclear why such a large, general model is necessary for tasks that existing MIR pipelines already solve effectively (e.g., onset detection, chord recognition, instrument tagging).\nThe paper could better justify what emergent capabilities arise from this unified model that cannot be achieved by composing established MIR tools.\n\n\n2. Missing comparisons to domain-specialized music–language models\n\nThe paper positions Music Flamingo as a general music–language model but provides limited comparison to existing specialized systems -- I see it cited MuLLaMa but it seems there is no comparison?\n\n3. Conceptual scope and framing\n\n\"Music understanding\" has cognitive or perceptual components implied by that phrase, not only including facts like pitch, timbre, but also a focus on the feeling and emotional response. This is definitely a hard task and the seems not the focus here. That said, framing the contribution as scaling multimodal representations for music would be more accurate and modest, aligning better with the presented evidence."}, "questions": {"value": "Would you include case studies or analyses showing cross-task generalization or contextual reasoning that would be infeasible for traditional MIR pipelines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VMUSz5HdYG", "forum": "RS7T9S16Bl", "replyto": "RS7T9S16Bl", "signatures": ["ICLR.cc/2026/Conference/Submission7878/Reviewer_RXYf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7878/Reviewer_RXYf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7878/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640025767, "cdate": 1761640025767, "tmdate": 1762919911866, "mdate": 1762919911866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Music Flamingo (MF), an LLM-based model for music understading tasks. Authors created two large-scale datasets for music understanding tasks: MF-Skills, which contains 4M examples and MF-Think, which contains 300K CoT examples (derived from MF-Skills) for post-training MF to improve its reasoning abilities.  MF-Skills and MF-Think contain full-length music with detailed and multifaceted annotations, compared with previous music understanding datasets that often contain short and surface-level annotations.\n\nBased on the two datasets, authors developed MF by continue training the Audio Flamingo 3 (AF3) model. MF achieved SOTA performances on several public music understanding benchmarks. Human evaluation with music experts show that MF seems better compared with other models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- I think MF-Skills and MF-Think are very good resources for music understanding, where annotations and datasets are rare. The two  datasets could potentially benefit follow-up work of developing better models in this field.\n\n- SOTA performance on several music understanding benchmarks.\n\n- Adding a post-training stage to improve reasoning of the music understanding model is well motivated. Current music understanding   models tend to output surface-level outputs about a music. It is great to see adding post-training RL is also effective for music  understanding.  Authors also show that without post-training, performance drops on MMAU-Pro-Music and MuchoMusic."}, "weaknesses": {"value": "- MF stresses a lot on music with vocal (sec 3.1), and motivates to add several ASR datasets. While this motivation is valid, but non-vocal   music seem less covered in the discussions.\n\n- In the human evaluation from music experts (Tab4), it seems MF often attempts to provide deep details, but often inaccurate. This poses   some concerns on real-world use cases, beyond the good performance on benchmark numbers."}, "questions": {"value": "- Appendix F: how generalizable is this user study? Have you tried with more music/songs, or music without vocals?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The provided datasets contain music contents, which often involve strict copyright regulations. I think I worth confirming the legal compliance w.r.t. copyrights and license."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dLTcpyu8zY", "forum": "RS7T9S16Bl", "replyto": "RS7T9S16Bl", "signatures": ["ICLR.cc/2026/Conference/Submission7878/Reviewer_PZ26"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7878/Reviewer_PZ26"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7878/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879861038, "cdate": 1761879861038, "tmdate": 1762919911352, "mdate": 1762919911352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes Music Flamingo, a large-scale audio-language model established from a trained Audio Flamingo 3 model, plus 4 post-training steps (3 supervised fine-tuning, 1 RL policy update phase). At the same time, this work proposed MF-Skill and MF-Think, two large-scale synthesized datasets curated for the post-training steps of Music Flamingo. The proposed model achieved top performance on 13 benchmarks (Table 1)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Datasets and model proposed in this work are established on full-length songs rather than clips. This is important as some information can only be well determined by observing the full track (e.g. musical structure, mood, cultural context, etc.).\n\n- Making useful incorporation of speech-centric datasets, such as including multilingual ASR and multi-talker ASR to improve the model capability on bridging texts(e.g. lyrics) and phonemes (speech, vocal). \n\n- The annotations included in MF-Skills incorporated multiple important aspects with not only tags, but also captions and QA pairs. In particular, temporal information is comparatively scarce in public datasets. \n\n- Incorporating a reinforced learning step in the entire pipeline. It is insightful to see a successful adaption of GRPO for LALM.\n\n- Dataset and source code will be published for reproducibility."}, "weaknesses": {"value": "- There's no ablation study on how each post training step improves the model, which is important to show the significance of introducing reinforcement learning. It's also a pity that there's no further discussion about the stability issue of GRPO.\n\n- Although it's a positive sign that proposed work is aware of the dataset issue on cultural bias, according to Figure 4(the same as Figure 27?), the composition of MF-Skills is still western-centric. It can be understood that collecting dataset that covering everything is difficult, however, I believe it is still not sufficient to claim the current composition as \"moving from western culture to diverse culture\". For example, African, Middle east, India, East asia and Southeast asia music are still under-represented despite they all have huge audience population.\n\n- The reference of GRPO seems to be missing: https://arxiv.org/pdf/2402.03300"}, "questions": {"value": "- Regarding quality filtering, what is the frontier model used to verify the captions? Also, what's the model used for re-writing the captions?\n\n- How does the re-writing of MF-Think work? Is that done by repetitive re-generation followed by the verification from the said frontier model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Yg9VOPsnGi", "forum": "RS7T9S16Bl", "replyto": "RS7T9S16Bl", "signatures": ["ICLR.cc/2026/Conference/Submission7878/Reviewer_cqsC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7878/Reviewer_cqsC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7878/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908658898, "cdate": 1761908658898, "tmdate": 1762919910230, "mdate": 1762919910230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}