{"id": "4g3dTrLnoF", "number": 16061, "cdate": 1758259343620, "mdate": 1759897264761, "content": {"title": "DLM-One: Diffusion Language Models for One-Step Sequence Generation", "abstract": "This paper introduces *DLM-One*, a score-distillation-based framework for one-step sequence generation with continuous diffusion language models (DLMs). DLM-One eliminates the need for iterative refinement by aligning the scores of a student model’s outputs in the continuous token embedding space with the score function of a pretrained teacher DLM. We investigate whether DLM-One can achieve substantial gains in sampling efficiency for language modeling. Through comprehensive experiments on DiffuSeq—a representative continuous DLM—we show that DLM-One achieves up to $\\mathord{\\sim}500\\times$ speedup in inference time while maintaining competitive performance on benchmark text generation tasks used to evaluate the teacher models. We further analyze the method’s empirical behavior across multiple datasets, providing initial insights into its generality and practical applicability. Our findings position one-step diffusion as a promising direction for efficient, high-quality language generation and broader adoption of continuous diffusion models operating in embedding space for natural language processing.", "tldr": "", "keywords": ["Generative Modeling", "Score Distillation", "Non-autoregressive"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/766daf3b0bc0212fd3d67e1506254f25f5bdadaf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces DLM-one, a practical score distillation framework for continuous DLMs that enables one-step sequence generation without iterative denoising. This framework eliminates the need for iterative refinement by aligning the outpt scores of the student model in the continuous token embedding space with the score function of the pre-trained teacher DLM, significantly improving the sampling efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "As mentioned in this paper, DLM-One achieves up to 500\\times speedup in inference time, which means on the premise of ensuring quality, the computing cost and time consumption can be significantly reduces."}, "weaknesses": {"value": "Although the results are impressive, the paper may not have elaborated in sufficient detail on the contributions of different components to the final performance. For example, the impact of different choices of teacher models."}, "questions": {"value": "1.Since a comparison with the AR model is listed in the Appendix D, how does the DLM-one perform in handling open-domain creative text generation?\n2.Is the one-step sequence generation model first proposed in this paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "slQwFtr6E8", "forum": "4g3dTrLnoF", "replyto": "4g3dTrLnoF", "signatures": ["ICLR.cc/2026/Conference/Submission16061/Reviewer_gjET"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16061/Reviewer_gjET"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882467160, "cdate": 1761882467160, "tmdate": 1762926252347, "mdate": 1762926252347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to distill a diffusion language model for extreme acceleration, enabling generation in just one step, i.e., without iterative refinement. The authors introduce DLM-One, a score-distillation-based framework trained with a two-stage optimization scheme designed to mitigate the latency caused by the alternating update strategy of the score estimator. Experiments are conducted using DiffuSeq as the teacher model and evaluated on three sequence-to-sequence tasks. The results show that DLM-One achieves competitive performance compared to DiffuSeq, with performance gaps ranging from less than 1% to 5%, though it lags in terms of diversity. At the same time, it reduces sampling cost by up to ~500× compared to DiffuSeq."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper's focus on achieving one-step sequence generation with diffusion models is well-grounded and addresses a clear need for improved inference efficiency.\n* The work demonstrates a strategic adaptation of distillation techniques from vision to language, validating its effectiveness for text generation.\n* Through comprehensive ablation studies and discussion, the authors provide convincing verification for their data distillation approach with DiffuSeq."}, "weaknesses": {"value": "* All experiments in the paper were conducted using DiffuSeq. It remains unclear whether the proposed method can generalize effectively to other diffusion-based language models.\n* The empirical comparisons are primarily made against the teacher model. It would be valuable to include comparisons with other accelerated generation baselines and provide an analysis discussing the optimal model selection on the performance-efficiency trade-off."}, "questions": {"value": "1. The experiments are condected solely with DiffuSeq. How about the performance with other diffusion language models?\n2. Given that several related methods are discussed in Section 2.2, it would be valuable to include a comparative analysis of their performance and inference speed relative to DLM-One, in order to better situate its trade-offs in the landscape of efficient generation models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "DyZZ6wFAX8", "forum": "4g3dTrLnoF", "replyto": "4g3dTrLnoF", "signatures": ["ICLR.cc/2026/Conference/Submission16061/Reviewer_KzE9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16061/Reviewer_KzE9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902847983, "cdate": 1761902847983, "tmdate": 1762926251938, "mdate": 1762926251938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a practical distillation framework for training continuous diffusion language models for one-step sequence generation (DLM-One), without the need for iterative refinement during generation. The paper conducts three sequence-to-sequence (Seq2Seq) tasks, including question generation (QG), text simplification (TS), and paraphrasing (PP), to support its claims and demonstrate the effectiveness of the framework. However, the novelty of this paper is modest: most components (score distillation, adversarial stabilization, two-stage optimization) are transferred from the vision domain. The experiments are conducted on only three general datasets, which lack a comprehensive study to convince the reader. In addition, the paper is not well organized; for example, mixing the introduction with related work, which makes this paper a bit hard to follow."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a practical distillation framework for training continuous diffusion language models for one-step sequence generation (DLM-One), without the need for iterative refinement during generation.\n2. Three experiments on benchmarks (QQP, Quasar-T, Wiki-Auto) demonstrate competitive BLEU, ROUGE, and BERTScore compared to DiffuSeq, showing the effectiveness and validity of the framework.\n3. This method reduces inference cost—up to 500× speedup—without large quality degradation."}, "weaknesses": {"value": "1. Most of the components (score distillation, adversarial stabilization, two-stage optimization) are transferred from the vision domain, which limits the novelty of this paper.\n2. All experiments depend on one teacher model (DiffuSeq). The results may not generalize to other DLMs.\n3. Lack of experiments on classic generation tasks that require strict semantic evaluation, such as translation.\n4. Since degeneration is mentioned, there is limited qualitative or quantitative analysis of when or why the student diverges.\n5. In terms of writing, this paper is not well organized, such as mixing related work into the introduction, which makes it hard to follow and somewhat redundant."}, "questions": {"value": "1. Can this framework be used with other teacher models to show generalization?\n2. Can more benchmarks be used to explore the generalization of the framework, such as long-content Seq2Seq or translation benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xXMzrJM1uu", "forum": "4g3dTrLnoF", "replyto": "4g3dTrLnoF", "signatures": ["ICLR.cc/2026/Conference/Submission16061/Reviewer_kNgF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16061/Reviewer_kNgF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972600975, "cdate": 1761972600975, "tmdate": 1762926251411, "mdate": 1762926251411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a score-based distillation framework, DLM-One, for single-step sequence generation with continuous diffusion language models. By distilling the score function from a pre-trained teacher model into a student model, this approach eliminates the iterative refinement process required by traditional diffusion models, resulting in up to a 5x inference speedup while maintaining generation quality. The authors validate the method's effectiveness on three sequence-to-sequence tasks and demonstrate an approximately 500-fold speedup in text simplification without a significant degradation in quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method, DLM-One, introduces a novel approach to single-step sequence generation using continuous diffusion language models by leveraging score-based distillation techniques.\n\n2. The authors conduct experiments across three S2S tasks, demonstrating the effectiveness of DLM-One compared to baselines. They also provide thorough analysis of the trade-off between quality and diversity in single-step generation, highlighting potential areas for future improvements."}, "weaknesses": {"value": "1. Lack of Baselines. The paper only discusses one outdated baseline, DiffuSeq (2022), and does not discuss other acceleration techniques.\n\n2. The generation tasks are overly simplistic. The paper does not explicitly discuss the challenges associated with scaling the proposed method to larger datasets or more complex tasks."}, "questions": {"value": "NA."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J5BVBIRPzK", "forum": "4g3dTrLnoF", "replyto": "4g3dTrLnoF", "signatures": ["ICLR.cc/2026/Conference/Submission16061/Reviewer_dhub"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16061/Reviewer_dhub"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998942044, "cdate": 1761998942044, "tmdate": 1762926250984, "mdate": 1762926250984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}