{"id": "90mRxCvkmf", "number": 10147, "cdate": 1758162030180, "mdate": 1759897671161, "content": {"title": "Toward Undetectable AI Text: AIGT detection evasion with representation editing", "abstract": "With the growing popularity of large language models (LLMs), some concerns have been raised, such as misinformation, plagiarism, and deceptive reviews.\nBuilding an efficient and robust AI-generated text (AIGT) detection system has become an urgent demand. \nTo comprehensively assess the robustness of detectors prior to deployment, evasion methods gradually attract the attention of the research community.\nExisting evasion methods mainly fine-tuned LLMs to align their outputs with human-written text (HWT), which required substantial data and computational resources.\nMoreover, although leveraging model editing to directly modify the weights of LLMs can significantly reduce the training costs, the evasion performance is not significantly enhanced due to intrinsic limitation of the model-editing theory.\nTo address these limitations, we propose Representation Editing Attack (R-EAT), a training-free evasion method. \nR-EAT first constructs a difference space between AIGT and HWT.\nThen, it dynamically edits LLM hidden representation during generation by removing their projections onto this space, thereby encouraging the model to produce more human-like texts.\nThrough theoretical analysis, we demonstrate that R-EAT achieves superior performance by directly editing hidden states, thereby eliminating the inherent limitations of model editing while preserving its advantages in sample and time efficiency.\nExperimental results demonstrate that the R-EAT effectively reduces the average detection accuracy of 8 AIGT detectors across texts generated by two different LLMs.", "tldr": "", "keywords": ["AI generated text detection evasion", "large language model", "representation editing"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/11b7b914c04a1851d9950191ebca8ef7b39183d5.pdf", "supplementary_material": "/attachment/ce97ca0e887cf1254f3d7162571060dc7d25e18b.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces R-EAT (Representation Editing Attack), a training-free evasion method designed to help large language models (LLMs) generate text that is more difficult for AI-generated text (AIGT) detectors to recognize.\n\nR-EAT constructs a “difference space” between human-written text (HWT) and AI-generated text (AIGT) by comparing their hidden representations. During text generation, it removes the projection of LLM hidden states onto this difference space, thus steering the model’s internal representations toward more human-like patterns."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Proposes a novel training-free detection evasion method that constructs a difference space and dynamically edits hidden representations, which is fundamentally different from existing fine-tuning-based methods\n\nThe method is highly efficient, requiring only 500 samples to achieve effective evasion\n\nTime cost is significantly reduced (Figure 5 shows orders of magnitude improvement compared to baseline methods)\n\nIncludes human evaluation: Maintains good text quality (human evaluation results in Table 2)"}, "weaknesses": {"value": "1. The number of human evaluations is limited and could be increased\n\n2. The number of evaluated detectors is limited. Could add:\nFast-DetectGPT, ImBD (designed specifically for detection evasion), Text Fluoroscopy, Binoculars\n\n3. Evaluation scope: Experiments about LLMs to be detected are limited, and only consider two domains : OpenWebText and WritingPrompt"}, "questions": {"value": "1. Detection adaptation: Can detectors trained with adversarially augmented data resist R-EAT-type manipulations, like ImBD?\n\n2. Generalization: How robust is the learned difference space when applied to unseen domains or LLM architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5deRHri4w1", "forum": "90mRxCvkmf", "replyto": "90mRxCvkmf", "signatures": ["ICLR.cc/2026/Conference/Submission10147/Reviewer_26kA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10147/Reviewer_26kA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761580863510, "cdate": 1761580863510, "tmdate": 1762921518920, "mdate": 1762921518920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes training-free approach to make AI-generated text less detectable by AI-text detectors. R-EAT constructs a difference space between AI-generated and human-written texts, identifies discriminative features via singular value decomposition (SVD), and removes AI-like components from hidden states to generate more human-like text. This method avoids the computational cost of fine-tuning, improves sample and time efficiency, and reduces the detectability of AI text across multiple detectors while preserving text quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed idea is innovative.\n\n2. The paper is clearly structured and well written.\n\n3. The theoretical analysis is sound and well justified."}, "weaknesses": {"value": "1. R-EAT relies on a set of human-written and AI-generated texts to derive the difference space. Gathering representative samples of both HWT and AIGT are needed to compute the key representation differences. \n\n2. The attack must be implemented inside the model’s generation process, requiring internal access to the LLM’s hidden states at a particular layer. However, the frontier models are API models. How effective or useful this method will be is still in question.\n\n3. Could the method evade fast-detectgpt[R1]?\n\n[R1]. Bao, G., Zhao, Y., Teng, Z., Yang, L., & Zhang, Y. (2023). Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature. arXiv preprint arXiv:2310.05130."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jnSWtoduhm", "forum": "90mRxCvkmf", "replyto": "90mRxCvkmf", "signatures": ["ICLR.cc/2026/Conference/Submission10147/Reviewer_qYdx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10147/Reviewer_qYdx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776988038, "cdate": 1761776988038, "tmdate": 1762921518450, "mdate": 1762921518450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to evaluate the robustness of AI-generated text (AIGT) detectors and proposes a novel method for detection evasion. The authors argue that existing evasion methods have limitations: fine-tuning-based approaches (like DPO) demand significant computational and data resources , while model editing methods (which directly modify weights) suffer from limited performance due to the residual connections in the Transformer architecture.\n\nTo address these issues, the paper introduces R-EAT (Representation Editing Attack), a \"training-free\" evasion method. The core mechanism of R-EAT is:\n\n* Constructing a Difference Space: First, it collects hidden layer representations of AIGT and human-written text (HWT) at specific layers of the LLM to construct a \"difference space\" between them.\n* Dynamic Editing: During the text generation process, R-EAT dynamically modifies the LLM's hidden states by removing their projections onto this difference space . The authors claim this guides the model to produce more \"human-like\" text.\n\nThe main contributions of this paper include:\n* Proposing R-EAT, a training-free attack method based on representation editing.\n* Providing theoretical analysis arguing that R-EAT, by directly editing hidden states, avoids the inherent limitations of model editing methods while offering superior sample and time efficiency compared to DPO-based fine-tuning.\n* Presenting experimental results showing that R-EAT effectively reduces the average detection accuracy (AUROC) of eight AIGT detectors on texts generated by two LLMs (Llama2-13b and Qwen3-14b). The paper also reports advantages in time efficiency, sample requirements (using 500 samples in experiments), and preservation of text quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper presents a novel and efficient method for evading AI-generated text (AIGT) detection, supported by theoretical analysis and a comprehensive experimental setup.\n\n* Originality: The primary strength of this paper is its originality. The proposed method, R-EAT (Representation Editing Attack), introduces a new paradigm for AIGT detection evasion. While prior work has focused on post-processing , resource-intensive fine-tuning (e.g., DPO) , or modifying model weights , this is the first work, to my knowledge, to propose directly editing the hidden representations during the generation process to achieve evasion. This adaptation of representation editing principles to the AIGT evasion task is a creative and novel contribution.\n\n* Clarity: The paper is well-written and clearly organized. The core concept is introduced intuitively with helpful diagrams (especially Figure 1, which clearly contrasts the R-EAT paradigm with previous methods). The methodology in Section 3.2 is broken down into two distinct, easy-to-understand steps: \"Difference Space Construction\" and \"Representation Editing\". The mathematical notation is clear and consistent."}, "weaknesses": {"value": "While this paper proposes a novel method, it suffers from several critical flaws that undermine the reliability of its conclusions and the robustness of its contributions.\n\n* Serious Deficiencies in Literature Review: The \"Related Work\" section (Section 2) fails to properly situate R-EAT within the critical discourse on AIGT detection evasion. The authors have omitted foundational work that discusses the fundamental limits of detection and adversarial robustness. For instance, (Sadasivan et al., 2023, \"On the possibilities of AI-generated text detection\") and (Hu et al., 2023, \"RADAR: Robust AI-Text Detection via Adversarial Learning\") are core literature in this field.\n\n  * [1] makes the fundamental argument that as LLMs themselves improve, AIGT detection may become theoretically infeasible. The paper, while claiming successful evasion, does not engage with this core challenge.\n  * [2] specifically investigates hardening detectors against attacks via adversarial learning (e.g., RADAR). This paper proposes a new attack but completely fails to evaluate it against known robust defenses like RADAR.\n\n   This omission is severe. It is not just a missing citation; it indicates the authors may not have benchmarked their work against the most important theoretical and defensive research in the field, leading to an overstatement of the contribution's context and significance.\n\n\n**References**\n\n[1] Souradip Chakraborty, Amrit Singh Bedi, Sicheng Zhu, Bang An, Dinesh Manocha, and Furong\nHuang. On the possibilities of ai-generated text detection.\n\n[2] Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho. RADAR: Robust AI-Text Detection via Adversarial Learning."}, "questions": {"value": "Given that R-EAT is proposed as an attack method, its evaluation against standard detectors is insufficient for a complete robustness analysis. Can the authors provide additional experiments comparing R-EAT's effectiveness against detectors specifically designed to be robust to adversarial attacks, such as RADAR ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no concerns."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3iJY8yFKvH", "forum": "90mRxCvkmf", "replyto": "90mRxCvkmf", "signatures": ["ICLR.cc/2026/Conference/Submission10147/Reviewer_zLaZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10147/Reviewer_zLaZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801425621, "cdate": 1761801425621, "tmdate": 1762921518011, "mdate": 1762921518011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces R-EAT, a training-free approach for evading AI-generated text detectors. Distinct from previous finetuning- or model-editing-based evasion methods, R-EAT directly modifies hidden representations of large language models by removing projections onto discriminative directions that separate human-written and AI-generated text, constructed via SVD."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "R-EAT is a training-free approach that avoids the substantial computational and data costs seen in finetuning- and proxy-based evasion methods.\n\nThe theoretical section, especially Equations 11 and 12 in Section 3.3, details why direct representation editing is fundamentally more potent than model editing due to the architectural impact of residual connections."}, "weaknesses": {"value": "1. **Clarity of Mathematical Specification**: While the main formulas are present, Sec 3.2 contains ambiguous variable indexing and notational inconsistencies. For example, the definition of $\\bar{\\boldsymbol{h}}_j$ and the mean computation for the difference space would benefit from clean, explicit sample definitions especially wrt. set sizes $N^+$ vs $N$. The explanation around the centering step in the difference space construction is terse and glosses over rationale for removing mean direction, which could confuse readers less familiar with representation alignment methods.\n\n2. **Hyperparameter Sensitivity Not Fully Explored**: Although Fig 6–9 show R-EAT's behavior under changing $\\tau$, $\\alpha$, and layers, the explanations lack depth on why particular values perform best, and there is little practical guidance for practitioners. For example, Fig 7 shows a non-monotonic trend in AUROC versus $\\alpha$, but the text does not explain what types of distortion or artifacts arise at higher editing strengths.\n\n3.**Limited Discussion of Generalization and Transfer:** While R-EAT performs well on the presented Llama2-13b and Qwen3-14b models (Section 4.5), the method heavily relies on representation differences between AIGT and HWT at specific layers in these models. There is little discussion or empirical evaluation of how learned difference spaces transfer to unseen models or domains (other than an Appendix pointer); thus, practical applicability in real-world black-box or rapidly-evolving deployment settings is underexplored."}, "questions": {"value": "How robust is the learned difference space for R-EAT when used on models or text domains not represented in the preference dataset? For instance, can the same $\\boldsymbol{B}_j$ constructed on Llama2-13b/OpenWebText be used for Qwen3-14b or other text sources with minimal loss of efficacy?\n\nGiven that R-EAT is proposed as a research contribution, what steps are planned (if any) to control misuse, especially if code/models are released? Are there watermarking, detection countermeasures, or disclosure policies planned?"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "143TWc3GnA", "forum": "90mRxCvkmf", "replyto": "90mRxCvkmf", "signatures": ["ICLR.cc/2026/Conference/Submission10147/Reviewer_4Y1b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10147/Reviewer_4Y1b"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762759624072, "cdate": 1762759624072, "tmdate": 1762921516577, "mdate": 1762921516577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}