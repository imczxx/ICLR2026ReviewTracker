{"id": "ECl6HGrMQI", "number": 14283, "cdate": 1758231957274, "mdate": 1759897379051, "content": {"title": "LoRaQ: Optimized Low Rank Approximated Quantization Error for 4-bit Quantization", "abstract": "Post-training quantization (PTQ) is essential for deploying large diffusion-based transformers on resource-constrained hardware. However, aggressive 4-bit quantization introduces significant degradation in generative performance. While existing solutions mitigate quantization error through outlier smoothing or rotation techniques, low-rank approximation methods that add auxiliary linear branches to each quantized layer represent a promising new paradigm. Yet, these approaches suffer from computational overhead due to the data movement required by full-precision (W16A16) branches, limiting practical deployment. In addition, data-based calibration contributes to the complexity of the quantization process and involves risks such as potential accuracy degradation. We propose LoRaQ (low-rank approximated quantization), a data-free calibration approach to optimize quantization error compensation. This method can be used in composition with other PTQ models. LoRaQ further enables mixed-precision configurations by quantizing the low-rank branch itself, overcoming the limitations of prior work. While LoRaQ achieves superior quantization performance than state-of-the-art methods in their native W4A4 setting on PixArt-Sigma and SANA, it also allows for configurations such as W8A8, W6A6 and W4A8 for low-rank branch alongside a W4 main layer. This reduces data movement overhead and enables a fully quantized, hardware-efficient solution.", "tldr": "", "keywords": ["Post-Training Quantization", "Transformers", "Diffusion", "Image Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f10e23d8f7686360e9bd69632d403be351263a53.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a framework for low-rank quantization of diffusion models. Instead of performing standard weight quantization (W8A8, W4A8, etc.), the authors decompose each weight matrix into a low-rank factorization (LoRA-style), followed by quantization-aware optimization on the factorized matrices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Conceptual Integration of LoRA and Quantization.  The idea of combining LoRA and quantization into a single unified framework is logical and potentially impactful. It effectively exploits the redundancy in diffusion U-Nets and transformer-based blocks.\n\n2. Quantizing diffusion models is increasingly important for deployment on mobile or edge hardware. LoRaQ directly targets this problem, aligning with the trend of compute-efficient generative AI."}, "weaknesses": {"value": "1. The first picture appears to be a non-vectorial image. It is recommended to convert it to a vectorial image.\n\n2. The method is largely an engineering combination of existing paradigms, LoRA-style factorization and quantization-aware training, with a joint optimization loss. While useful, it lacks a novel theoretical component or formal analysis explaining why low-rank factorization improves quantization robustness."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1sTx5v4sNh", "forum": "ECl6HGrMQI", "replyto": "ECl6HGrMQI", "signatures": ["ICLR.cc/2026/Conference/Submission14283/Reviewer_YVaC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14283/Reviewer_YVaC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635850420, "cdate": 1761635850420, "tmdate": 1762924732484, "mdate": 1762924732484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LoRAQ, a data-free calibration approach to minimize the weight quantization error. The work extends from existing low-rank approximation method by proposing mixed-precision configuration with quantized low-rank branch. Quantization errors are further minimized by inserting a learned rotation matrix on the low-rank branch. The proposed method is further accelerated by system implementation optimizations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelty-wise, this paper moves away from the common data-dependent calibration of block reconstruction and explores a weight-only calibration method. Though the use of low-rank branch and the rotation matrix insertion are well-known ideas, the overall framework remains novel.\n2. The exploration on quantizing the low-rank branch opens up a new tradeoff between the rank and the quantization precision of the branch\n3. Both quantitative and qualitative results are provided for multiple diffusion models showing the proposed method outperforming SVDquant baseline.\n4. The paper has a clear presentation overall, easy to follow."}, "weaknesses": {"value": "1. For the efficiency evaluation, though the paper claims improved hardware support by removing floating scales and micro-scaling formats, no real runtime measurements are provided in the evaluation to demonstrate the improved efficiency. A latency or throughput comparison needs to be conducted to justify the improvement.\n2. The paper proposes multiple techniques, such as the new data-free calibration strategy, adding rotation to the low-rank branch, and performing different format of quantizations on the low-rank branch. However, ablation study is lacking to show the effect of each individual treatment. Ablation is especially needed to show the performance gain brought by the different calibration strategy and the rotation matrix.\n3. The paper claims a fiar comparison with SVDquant. However, the proposed method utilizes additional rotation matrix, which may add additional overhead to the inference."}, "questions": {"value": "Please provide additional results to tackle the three weaknesses mentioned in the previous section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tBNPATs7M1", "forum": "ECl6HGrMQI", "replyto": "ECl6HGrMQI", "signatures": ["ICLR.cc/2026/Conference/Submission14283/Reviewer_hyF4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14283/Reviewer_hyF4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761698511195, "cdate": 1761698511195, "tmdate": 1762924732078, "mdate": 1762924732078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "I am unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers on 14 Oct 2025."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "I am unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers on 14 Oct 2025."}, "weaknesses": {"value": "I am unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers on 14 Oct 2025."}, "questions": {"value": "I am unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers on 14 Oct 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "IsivDwsToC", "forum": "ECl6HGrMQI", "replyto": "ECl6HGrMQI", "signatures": ["ICLR.cc/2026/Conference/Submission14283/Reviewer_RNYk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14283/Reviewer_RNYk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960458918, "cdate": 1761960458918, "tmdate": 1762924731250, "mdate": 1762924731250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LoRaQ, a data-free calibration approach to optimize quantization error compensation. With W4A4 settings, LoRaQ achieves optimal experimental performance compared to existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The results show that the proposed method significantly improves the metrics compared to existing SVDQuant methods, demonstrating its advantages.\n2. The paper claims that they will release the PTQlibrary for transformer blocks, which will contribute to future research."}, "weaknesses": {"value": "1. The paper mentions that a major advantage of LoRaQ is its model independence, which eliminates the need to calibrate datasets to determine low-rank matrices. This significantly simplifies the quantization process. However, the authors do not explain the performance gains resulting from this simplification. Are there any metrics that can quantify the benefits brought by the model?\n2. The paper only conducted experiments on the PixArt-Î£ and SANA models. Is it generalizable for other models with different architectures or different numbers of parameters?"}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "esGbthYJB3", "forum": "ECl6HGrMQI", "replyto": "ECl6HGrMQI", "signatures": ["ICLR.cc/2026/Conference/Submission14283/Reviewer_7mZU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14283/Reviewer_7mZU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14283/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763448576863, "cdate": 1763448576863, "tmdate": 1763448576863, "mdate": 1763448576863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}