{"id": "0YVqQY1Udx", "number": 22845, "cdate": 1758336251138, "mdate": 1759896843236, "content": {"title": "Recursive Structure Discovery as an Inductive Bias for Symbolic Regression", "abstract": "Symbolic regression (SR) can recover analytic laws from data, but its search space is enormous. Many scientific targets are structurally simple, for example additively or multiplicatively separable, yet most SR pipelines do not exploit this. We introduce a recursive structure discovery step that tests for separability using accurate derivatives from a small neural model trained with second-order updates. The method decomposes $y=f(\\mathbf{x})$ into a hierarchy of simpler subfunctions, which we feed to SR as a structure prior. This plug-in reduces search complexity, improves interpretability, and can attach to any SR backend; here we pair it with a deep RL generator. On SRBench (Feynman, 120 equations), the structure-aware pipeline achieves state-of-the-art exact recovery, outperforming separability-only, pure RL, and prior hybrid baselines.", "tldr": "We recursively detect additive and multiplicative separabilities in data, decomposing it into simpler components. This inductive bias guides reinforcement learning–based symbolic regression, achieving state-of-the-art results on SRBench Feynman.", "keywords": ["Symbolic Regression", "Inductive Bias", "Functional Separability", "Recursive Decomposition", "Deep Reinforcement Learning", "Hierarchical Graph Structure", "Levenberg–Marquardt Optimization", "Scientific Machine Learning"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b533234cd8ceda45b62cf4e13b99956feece03f0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a two-stage framework: it first leverages derivative information to uncover the variable structure within expressions, then performs variable separation to decompose the overall problem into subproblems, and finally applies a reinforcement learning–based symbolic regression algorithm. The method achieves a high recovery rate on SRBench."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The ideas in the paper are sound, the language has no obvious issues, and the overall structure is complete."}, "weaknesses": {"value": "The literature review is insufficient. Derivative-based variable separation was first introduced into symbolic regression by AI Feynman, and AI Feynman 2.0 already handles variable separation for most separable cases, including multiplicative separability.\n\nUdrescu, S. M., Tan, A., Feng, J., Neto, O., Wu, T., & Tegmark, M. (2020). AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity. Advances in Neural Information Processing Systems, 33, 4860-4871.\n\nThe paper lacks novelty. The proposed method merely stitches together AI Feynman and DSR, and this combination has already been explored by other researchers, such as the UDSR mentioned in the paper.\n\nThe experimental baselines omit recent strong methods. The experiments lack reproducible and convincing empirical support; the reported performance of some baselines differs markedly from results in other papers and from those obtained using the corresponding open-source implementations."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WOyvnS2wRQ", "forum": "0YVqQY1Udx", "replyto": "0YVqQY1Udx", "signatures": ["ICLR.cc/2026/Conference/Submission22845/Reviewer_9hzL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22845/Reviewer_9hzL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815097780, "cdate": 1761815097780, "tmdate": 1762942409261, "mdate": 1762942409261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a recursive structure discovery framework that improves symbolic regression (SR) by uncovering how variables in scientific data combine through simple, interpretable structures. Instead of directly fitting equations, the method first trains a compact NestyNet with accurate estimated derivatives. These derivatives are used to detect additive, multiplicative, and nested nonlinear separabilities, building a hierarchical tree of sub-functions. This structure acts as a probabilistic prior guiding a deep reinforcement learning-based SR model to generate mathematically consistent expressions. The proposed method is demonstrate to outperform many existing SR methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a novel approach to first use powerful neural networks to fit the data and then detect key structures and inform symbolic regression. \n2. This paper proposes the key issue of compactness of the learned expression and the benefit of doing so, which may inspire later work.\n3. The empirical performance is convincing."}, "weaknesses": {"value": "1. The paper is a bit hard to read from time to time, possibly because there are many components in the method (NestyNet, finding structures, and incorporating it into symbolic regression). \n2. How the tree is built is not clear -- it might be helpful to elaborate on this. I understand that $f_i$ is a NestyNet, but it is unclear how the mapping $\\Phi$ is determined. The paragraph \"Composite model\" on page 5 is difficult to read.\n3. It's unclear how the threshold values are chosen in Table 1 (and there is a typo `treshold' above it). \n4. It is unclear why the method is paired with the RNN method."}, "questions": {"value": "1. Can you explain how the composite model on page 5 is trained?\n2. Why would you use a tree structure to combine the NestyNet layers? It is also common to use tree to represent expressions in symbolic regression, but it's unclear to me why using a tree makes sense here.\n3. How did you choose the threshold values in Table 1?\n4. How does the method compare with uDSR in Figure 5? Does uDSR use a similar RNN approach? The main question is that with the current presentation of the results, it is unclear which component leads to the margin of the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rsCOYYopGn", "forum": "0YVqQY1Udx", "replyto": "0YVqQY1Udx", "signatures": ["ICLR.cc/2026/Conference/Submission22845/Reviewer_zTXN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22845/Reviewer_zTXN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879223179, "cdate": 1761879223179, "tmdate": 1762942408668, "mdate": 1762942408668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-stage pipeline for symbolic regression (SR). First, it recursively discovers functional structure—additive/multiplicative separability and simple unary compositions—using accurate first/second-order derivatives from a compact neural “NestyNet” trained with Levenberg–Marquardt (LM). The resulting hierarchy is then used as a structural prior for an RL-based SR generator. On SRBench (Feynman, 120 equations), the method reports 72% exact recovery, outperforming separability-only, pure RL, and prior hybrid baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear problem framing & contributions. The paper argues convincingly that many scientific targets are modular and that exposing separability can shrink SR search while improving interpretability. \n\nMethodological neatness. The two-stage design is simple and broadly compatible with SR backends: (a) recursive separability discovery, (b) structure-guided RL generation. \n\nTechnical soundness. NestyNet has closed-form Jacobian/Hessian w.r.t. inputs, which enables reliable separability tests and LM optimization; formulas are explicit. \n\nConcrete, testable criteria. Additive and multiplicative separability tests are stated precisely with thresholds (ε_add=1e-4, ε_mul=1e-12, ε_βmad=1e-3). \n\nEmpirical performance. Strong results on SRBench with consistent hyperparameters; operators and evaluation limits are specified. \nInterpretability angle. The structural prior biases the generator toward symbolically faithful forms rather than merely accurate fits. \n\nInterpretability angle. The structural prior biases the generator toward symbolically faithful forms rather than merely accurate fits."}, "weaknesses": {"value": "Novelty boundary vs. prior separability work. The main leap is handling multiplicative separability and nested unary transforms and integrating them as soft priors, whereas prior AIF/uDSR emphasize additive separability. The paper should sharpen how much gain derives from each element (e.g., multiplicative test vs. nested transforms vs. RL prior shaping). \n\nReliance on a bespoke emulator. NestyNet is referenced as to be “fully described” elsewhere, which weakens reproducibility claims and makes it hard to benchmark against standard MLPs beyond one figure. \n\nSensitivity/robustness of the detectors. The separability decisions hinge on derivative quality and fixed thresholds; there is limited analysis of threshold sensitivity, failure modes (false positives/negatives), or uncertainty quantification during the recursive split. \n\nNoise robustness not yet validated. The Discussion acknowledges that formal noise testing is future work; given SRBench variations and real data, this is a notable gap. \n\nCompute profile. Pre-processing can average ~4 hours per case (GV100), and constant fitting is CPU-bound (~1 hour on 10-core Xeon). A deeper analysis of throughput and scaling to higher-dimensional inputs would help."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NO or VERY MINOR ethics concerns only"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qcBj5WP3Q9", "forum": "0YVqQY1Udx", "replyto": "0YVqQY1Udx", "signatures": ["ICLR.cc/2026/Conference/Submission22845/Reviewer_UpnE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22845/Reviewer_UpnE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883112302, "cdate": 1761883112302, "tmdate": 1762942408440, "mdate": 1762942408440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Recursive Structure Discovery (RSD) to mitigate the combinatorial explosion inherent in symbolic regression (SR).\nThe method aims to automatically detect additive and multiplicative separability of target functions and leverage such structure as a prior for symbolic regression.\n\nThe approach consists of two stages:\n\n1. Using a lightweight neural network called NestyNet to estimate high-precision derivatives and hierarchically detect separability;\n2. Using the detected structure as a structural prior in a reinforcement-learning-based SR framework. On the Feynman SR benchmark (SRBench, 120 formulas), the method achieves a 72 % exact-recovery rate, outperforming existing systems such as AI Feynman 2.0, uDSR, and PySR.\n\nThe technique can handle non-linear nested functional structures, showing that structural inductive bias can significantly enhance SR performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a novel and coherent formulation of structural inductive bias for symbolic regression.\n- The use of NestyNet for derivative-based structure detection provides an efficient reduction of the search space, compared with prior unstructured approaches.\n- The method can recursively detect additive and multiplicative separability, enabling the discovery of complex, nested physical relations.\n- Integration of the detected structure into the RL-based expression generator is conceptually natural and theoretically consistent."}, "weaknesses": {"value": "- The main concern lies in the lack of hyper-parameter sensitivity analysis, particularly regarding the thresholds for separability detection (ϵadd, ϵmul, ϵβmad).\n\n  These thresholds are empirically fixed, yet their effect on detection accuracy and mis-segmentation rates is not quantified.\n  A visualization of how structural decomposition changes with threshold variation would substantially strengthen the paper’s reliability.\n\n- Beyond these thresholds, other hyper-parameters—such as the LM damping factor, the hidden width h of NestyNet, and the entropy-regularization weight in the RL stage—are all kept fixed without discussion of robustness.\n\n  The method’s stability across parameter variations remains unclear.\n\n- No evaluation is provided on noisy or perturbed datasets. While the method is theoretically stable, empirical validation of noise robustness is missing.\n- Statistical uncertainty is not reported: Fig. 5 lacks error bars or significance testing, so the reliability of the performance gaps is uncertain."}, "questions": {"value": "1. Have you examined how sensitive the results are to the separability-detection thresholds (ϵadd, ϵmul, ϵβmad)?\n2. Would it be feasible to determine these thresholds adaptively—for instance, via BIC/MDL-based model selection?\n3. Could you provide stability analysis results for key hyper-parameters in both NestyNet and the RL stage?\n4. How does the structure-detection accuracy behave when moderate noise (e.g., 10 % Gaussian perturbation) is added to the data?\n5. Can you report statistical significance or variance estimates for the comparisons shown in Fig. 5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qWZGpO2eLS", "forum": "0YVqQY1Udx", "replyto": "0YVqQY1Udx", "signatures": ["ICLR.cc/2026/Conference/Submission22845/Reviewer_A1TF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22845/Reviewer_A1TF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762231782276, "cdate": 1762231782276, "tmdate": 1762942408146, "mdate": 1762942408146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}