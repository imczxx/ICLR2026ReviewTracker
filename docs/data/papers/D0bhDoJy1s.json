{"id": "D0bhDoJy1s", "number": 17476, "cdate": 1758276478254, "mdate": 1759897172936, "content": {"title": "GensKer: Generative Spectral Kernel with RKHS Expansion Guarantees", "abstract": "Deep spectral kernels are constructed by hierarchically stacking explicit spectral kernel mappings derived from the Fourier transform of the spectral density function. This family of kernels unifies the expressive power of hierarchical architectures with the ability of the spectral density in revealing essential patterns within data, helping to understand the underlying mechanisms of models. In this paper, we categorize most existing deep spectral kernel models into four classes based on the stationarity of spectral kernels and the compositional structure of their associated mappings. Building on this taxonomy, we rigorously investigate two questions concerning the general characterization of deep spectral kernels: (1) Does the deep spectral kernel retain the reproducing property during the stacking process? (2) In which class can the reproducing kernel Hilbert space (RKHS) induced by the deep spectral kernel expand with increasing depth? Specifically, the behavior of RKHS is related to its associated spectral density function. This means that we can implement the deep spectral kernel by directly resampling from an adaptive spectral density. These insights motivate us to propose the generative spectral kernel framework, which directly learns the adaptive spectral distribution by generative networks. This method, with the single-layer spectral kernel architecture, can: (1) generate an adaptive spectral density and achieve deep spectral kernel performance; (2) circumvent the optimization challenges introduced by multi-layer stacking. Experimental results on the synthetic data and several real-world time series datasets consistently validate our findings.", "tldr": "", "keywords": ["Spectral Kernel Learning", "Machine Learning", "Deep Spectral Kernel"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3fe7fa0dc90de4dc1be95b5a513b1a3e2dcf4c4b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a deep spectral kernel learning framework, called GensKer. The framework consists of two modules, spectral generative module and spectral kernel module. These modules are based on the spectral properties of kernels. They also theoretically show the expansion of the RKHS by considering the deep structure of the proposed kernels."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The proposed framework is based on the theoretical peoperties of the kernels. Also, they theoretically investigate the growth of the representation power by considering the deep structure of kernels and confirm it empirically. The topic is interesting and the theoretical investigations are solid."}, "weaknesses": {"value": "- In equations 6-9, the constants $C_{s,w/o}^{L-1}, C_{s,c}^{L-1}, C_{ns,w/o}^{L-1}, C_{ns,c}^{L-1}$ depend on $x$. Does this mean $C_{s,w/o}^{L-1}, C_{s,c}^{L-1}, C_{ns,w/o}^{L-1}, C_{ns,c}^{L-1}$ are functions of $x$, or do they depend on training samples? It was not clear for me the rigorous meaning of \"data-dependent spectral density\" in line 268. Could you clarify that? In addition, could you clarify why the proposed architecture enables GensKer to circumvent the optimization challenges posed by deep spectral kernels?\n\n- In section 3.2, the authors show that the RKHSs expands as the number $L$ of layers increases. Can you show the universality of the RKHS with sufficiently large $L$? Also, for the case of neural networks, the growth of the representation power is exponential in some sense (e.g. Cohen et al., JMLR, 2016). Can you evaluate the speed of the growth of the representation power for the proposed kernel?\n\n- The the references should be cited with parenthesis (use ```\\citep```).\n\n**Minor comments:**  \n- l 218: By and Lemma 3 shoud be By Lemma 3?"}, "questions": {"value": "Please see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Xp4aio42Ux", "forum": "D0bhDoJy1s", "replyto": "D0bhDoJy1s", "signatures": ["ICLR.cc/2026/Conference/Submission17476/Reviewer_pJsy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17476/Reviewer_pJsy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17476/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761093151163, "cdate": 1761093151163, "tmdate": 1762927358546, "mdate": 1762927358546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a theoretical analysis of deep spectral kernel methods for supervised learning and a new method following insights from the analysis. Deep spectral kernels arise from the composition of random Fourier feature maps through the stacking of multiple layers with a combination of trigonometric functions and frequency vectors sampled from an underlying spectral density. The analysis confirms the existence of a limiting reproducing kernel Hilbert space (RKHS) resulting from the compositional structure and shows that the stacking of layers leads to a sequence of RKHSs of potentially increasing complexity under certain conditions on the spectral kernel mappings. By observing the duality between the spectral representation and the kernel formulation, the authors then propose a new method (GensKer) to learn the spectral distribution by means of generative models, allowing one to learn the kernel from data. Experiments validating theory findings and comparing GensKer against spectral kernel learning baselines are presented, showing performance improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The presentation is mostly clear and relatively easy to follow for someone with a kernel methods background. The presented results bring novel insights into spectral kernel methods which should be useful to the kernel methods community. In addition, given the connection between kernel methods and neural networks in the infinite-width limit, I believe the results from this paper can be useful to shine new light onto how a neural network's depth affects its representation capacity via the resulting RKHSs, though that would require further investigations. The paper also presents a relatively extensive experimental validation comparing the proposed method against several state-of-the-art spectral kernel learning methods, which reveal further characteristics of these methods and their performance in practical settings."}, "weaknesses": {"value": "* It is not very clear from Definition 1 how the feature maps composition is realised. The initial feature map is mapping the domain $R^d$ to $R^M$, where $M$ denotes the corresponding number of features for the chosen feature map. However, it's not evident what's happening with the subsequent layers. Are each map in the subsequent layers mapping $R^M$ to $R^M$? If so, the domain is going to be of increasing dimensionality as the number of features goes to infinity, $M\\to\\infty$, and this does not seem to be properly addressed.\n* When only cosines are used, the original formulation by Rahimi and Recht (2007) included and additional random uniform bias term $b \\sim U[0, 2\\pi]$, compensating the effect of not explicitly accounting for the imaginary part with the sine functions. That term seems to have been neglected by the current analysis.\n* In the proof of Proposition 1, the effect of the finite-feature approximations seems to be silently ignored. In particular, I'm not 100% sure if the results of Proposition 1 and 2 hold for the finite-dimensional feature maps. What seems more obvious to me is that they should hold in the limit as $M \\to \\infty$. However, in that case, it could have been better to derive the results in that setting and then show/discuss what happens when using Monte Carlo approximations with a finite number of features. The order in which the limit is taken per layer might also affect the results.\n* Proposition 1 and 2 seem to only cover the Gaussian case. The results do not seem to be readily generalisable to compositions of other classes of Fourier features from stationary/non-stationary kernels. Yet, the paper reads as if its theoretical results were applicable to general deep spectral kernels.\n* Despite the claim of a progressive expansion, the proof that $H_{k^{L-1}} \\subseteq H_{k^L}$ does not ensure that the former is a strict subset of the latter. To confirm expansion, one would ideally have to show that there are elements in $H_{k^L}$ that cannot possibly exist in $H_{k^{L-1}}$, and that's not immediately obvious.\n* Line 250-251 claims that the deep spectral kernel is prone to \"local minima\", but it's not explicit that there is an optimisation component involved up to that point. So it's not clear what local minima are being referred to.\n* It see no discussion on the reason for the design choice of generating samples for $\\omega$ and $\\omega'$ via two independent pathways and then forcing $s(\\omega|\\omega')s(\\omega')$ and $s(\\omega'|\\omega)s(\\omega)$ to match, instead of simply sampling either $\\omega$ or $\\omega'$ independently and the sampling the other frequency vector conditional on the first one.\n* I've missed a discussion on limitations of the methodology and the analysis and avenues for future work.\n* Despite a few mentions throughout the text, I've missed a dedicated discussion on related work. In particular, it's not very clear how the proposed methodology contrasts with a few existing methods in the literature. For example, there are other distributional approaches of learning spectral frequencies from data, ranging from the basic learning of the maximum a posteriori (MAP) estimate via gradient descent, as in Lazaro-Gredilla et al. (2010), to a full Bayesian distribution learning, as, e.g., via Stein variational gradient descent [1, below]. \n\nMinor:\n* There are quite a few formatting issues with citations, especially in-text citations. For example, \"Rahimi and Recht Rahimi & Recht (2007)\", should be only \"Rahimi and Recht (2007)\". Proper citation formatting can be achieved using the `\\citet` and `\\citep` commands from `natbib` for in-text and parenthetical citations, respectively.\n* Line 128-129: \"identically and independently distributed\". The abbreviation \"i.i.d.\" usually stands for \"independently and identically distributed\". The term \"independently distributed\" makes less sense.\n* Line 130: The set of frequency pairs is missing symbols. I believe it should be $\\\\{(\\omega_i, \\omega_i') \\\\}_{i=1}^M$.\n* Claiming that Eq. 5 is a definition of the stacked deep spectral kernel could be somewhat misleading, as that's a result from the theoretical analysis, not necessarily a neutral definition.\n* Line 211-212: \"allow the inclusion relation ... can be transformed as...\" doesn't read well. Would it be \"... to be transformed as...\"?\n* Line 217-218: \"By and Lemma\" is missing something.\n* Line 53 (Appendix): When transforming the sum of exponentials into $\\cosh$, there is a factor of 2 that goes out. So it should be $\\frac{1}{2}$ multiplying $\\cosh$ in that line and subsequent equations with $\\cosh$.\n \nReferences:\n1. Warren, H., Oliveira, R., & Ramos, F. (2024). *Stein Random Feature Regression*. The 40th Conference on Uncertainty in Artificial Intelligence (UAI)."}, "questions": {"value": "Please, consider the issues raised above under \"Weaknesses\". In addition, I have the following questions.\n\n* What form was chosen for the distribution matching loss $\\ell_p$?\n* How could the results in Proposition 1 and 2 be generalised to deep spectral kernels with other forms of spectral densities, beyond Gaussians?\n* How do you ensure the resulting joint spectral density $s(\\omega, \\omega')$ produced by GensKer is positive definite?\n* Have other forms of generative models been considered for the learning of the spectral density? Can the method be generalised to arbitrary models that can produce samples from a valid spectral density?\n* Given the connection between random feature maps resulting from randomly initialised neural networks and random Fourier feature maps as explored in this paper, do you see a way of applying these results to general neural networks? Would perhaps this lead to a way of producing more powerful \"single-layer\" neural networks (though arguably not single layer, as there's potentially a multi-layer network in the generator) by adapting the layer's weights distribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fR2fwB75i0", "forum": "D0bhDoJy1s", "replyto": "D0bhDoJy1s", "signatures": ["ICLR.cc/2026/Conference/Submission17476/Reviewer_RCjB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17476/Reviewer_RCjB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17476/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635367956, "cdate": 1761635367956, "tmdate": 1762927358098, "mdate": 1762927358098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Spectral kernels seem to originate from Fourier transforms (Bochner's theorem for stationary kernels and Yaglom's theorem for non-stationary ones).\nConsidering four classes of spectral kernel mappings, this paper computes the resulting kernel functions obtained by stacking these mappings.\nThen, this paper proposes the generative spectral kernel (GensKer) framework to generate an adaptive spectral density and then constructs an associated spectral kernel.\nSome numerical experiments are provided to show the improvement of the proposed method.\nSeveral visualizations are also provided to illustrate the benefits of GensKer."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The idea of generative spectral kernel is interesting, which incorporates the representation theory of kernels to construct adaptive kernels. \nSome empirical improvements are observed in the experiments.\n* It is interesting to consider kernel constructed by stacking feature mappings and give analytical forms of the resulting kernels."}, "weaknesses": {"value": "* This presentation of this paper is not clear. Spectral kernel is not formally introduced and Definition 1 that introduces deep spectral kernel is also not clear. What does $k(x,x') \\approx \\langle \\phi(x), \\phi(x')\\rangle$ mean? Is it an equality or an approximation? Also, what are the difference between **deep spectral kernel** and general kernel?\n\n\n* The motivation of this paper is not very clear. The four classes of spectral kernel mappings are introduced but it is not clear why these four classes are chosen. The connection between deep spectral kernel and the generative spectral kernel framework is also not clear.\n* The theory is not solid. The statement seems to be ambiguous. For example, the \"constants\" $C$ in Proposition 1 and 2 depend on $x$, but they are treated as absolute constants in the discussion in Section 3.2. Also, $\\approx$ is used in the proof of the propositions without a formal treatment.\n* The numerical experiments are not sufficient to support the effectiveness of the proposed method. First, there is no detailed description of the experimental setup, such as the width, the initialization, etc. No code is provided. Second,  statistical significance is not reported. Moreover, while the number of parameters is compared across different methods, the computational cost is not compared."}, "questions": {"value": "1. What is the formal definition of spectral kernel? How is it related to general positive definite kernels in RKHS theory?\n2. What are the differences between the 3 plots in Figure 1? They seem to be the same.\n3. What are the results in Section 3.2? Can you provide some formal statements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tLiCRLkR28", "forum": "D0bhDoJy1s", "replyto": "D0bhDoJy1s", "signatures": ["ICLR.cc/2026/Conference/Submission17476/Reviewer_HXp9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17476/Reviewer_HXp9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17476/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761693239306, "cdate": 1761693239306, "tmdate": 1762927357590, "mdate": 1762927357590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses construction of deep spectral kernels. The paper first includes a theoretical part in which a few sin/cos-based feature map families are considered. It is argued that applying to them suitable stacking maps produces positive definite kernels with expanding RKHS. Then, a related generative spectral kernel framework is introduced. Finally, several sets of experiments with the proposed method are provided."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The results presented in the paper appear to be original. \n\nDetailed derivations of theoretical results are provided in the appendix. \n\nThe reported results of experiments suggest a good performance of the proposed method compared against several alternative spectral kernel methods (but the code does not seem to be provided, so these results are not verifiable)."}, "weaknesses": {"value": "Clarity\n--------\n\n**Poor mathematical exposition.** I found the paper quite hard to read. From the very beginning, the exposition suffers from various mathematical inaccuracies and confusing statements. \n\n- line 98: Yaglom's theorem is not carefully formulated. Theorem 2 states that a kernel is *positive definite* if and only if it is the Fourier transform of a *positive semi-definite bounded variation spectral density of a Lebesgue-Stiltjes measure*. To begin with, the paper never explains if \"positive definite\" means \"strictly positive definite\" or \"positive semi-definite\". Given that the semi-definiteness is explicitly mentioned, one would expect that positive definiteness is meant to be strict. However, the theorem is, of course, invalid in this case, because of the possibility $s(\\\\omega, \\\\omega)\\\\equiv 0$. Next, it is not explained what *bounded variation spectral density of a Lebesgue-Stiltjes measure* means precisely. The formulation of Bochner's (stationary) theorem is also not very careful: the non-negative measure $s(\\\\omega)d\\\\omega$ may have a singular component, not having a density.   \n\n- Moreover, it is not clear what is the point of stating Yaglom's theorem, since in the non-stationary case the spectral characterization is not transparent: the function $s(\\\\omega, \\\\omega)$ needs to be positive (semi-)definite like its Fourier image $k(x,x')$, so the description of kernels is not really simplified by Fourier transforming. (Of course, in the stationary case the situation is drastically different, since the spectral characterization in terms of non-negative measures is very transparent.)    \n\n- **(Most important)** The spectral function $s(\\\\omega, \\\\omega')$ appearing in Yaglom's theorem may be negative at $\\\\omega\\\\ne \\\\omega'$ - this does not contradict its positive definiteness. Positive definiteness of the function $s(\\\\omega, \\\\omega')$ and its positivity are two completely different notions (similarly to how the positive definiteness of a matrix is completely different from the positivity of its matrix elements). Then, I don't understand what is meant in definition 1 by the expectation $\\\\mathbb E_\\\\omega$ \"*under the spectral distribution $s(\\\\omega, \\\\omega)$.*\" In general, there is no probability distribution associated with the spectral function $s(\\\\omega, \\\\omega)$ if it is just positive-definite. \n\n- line 140: the lemma states that a \"reproducing kernel\" remains a reproducing kernel. It's unclear what would be a \"non-reproducing kernel\". It appears that the authors mean positive (semi-)definite kernels. \n\n- line 317: \"*The generated $s(\\\\omega,\\\\omega')$ is a probability density function, satisfying the positive definiteness condition\nof the spectral density in Theorem 2.*\" - Again, it looks like the authors confuse the positive definiteness and positivity of $s(\\\\omega,\\\\omega')$.  \n\n**Poor motivation.** In addition to mathematical issues, the paper does not properly motivate the specific addressed questions, e.g. why we should be interested in the iterated sine/cosine maps and \"removal\" and \"concatenations\" scenarios as in eqs. (3)-(4)).   \n\nContribution\n----------------\nI find the contribution of the paper to be very limited. The theoretical results such as Propositions 1 and 2 (assuming their correctness) are narrow and technical. Conclusions regarding the preservation of positive-definiteness or progressive expansion under stacking are more or less straightforward consequences of classical general theorems.  \n\nRegarding the experimental part, the authors don't seem to have open-sourced their code for independent verification."}, "questions": {"value": "This paper is poorly written and certainly requires a major revision."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ISb11vhnAq", "forum": "D0bhDoJy1s", "replyto": "D0bhDoJy1s", "signatures": ["ICLR.cc/2026/Conference/Submission17476/Reviewer_Ap4m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17476/Reviewer_Ap4m"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17476/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761778615207, "cdate": 1761778615207, "tmdate": 1762927356899, "mdate": 1762927356899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}