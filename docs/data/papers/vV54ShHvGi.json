{"id": "vV54ShHvGi", "number": 496, "cdate": 1756742694050, "mdate": 1759898257192, "content": {"title": "How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use", "abstract": "As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a “knowing–doing” gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles.", "tldr": "This work systematically studies LLMs in poker, uncovering heuristic, factual, and knowing–doing flaws, and introduces ToolPoker, a tool-integrated framework using external solvers to reach state-of-the-art gameplay and professional-level reasoning.", "keywords": ["Large Language Models", "Reinforcement Learning", "Imperfect Information Game", "Strategic Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/20a9d49a38454d57fde884ef7be1161046edf60b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles poker as a domain for strategic reasoning and analyzes the shortcomings of LLMs on poker. \nThe authors examine what features of reasoning correlate with poor performance, finding gaps between action and knowledge and flawed or heuristic reasoning. \nModels are compared to a range of baseline solvers.\nTo improve models, they propose a training approach consisting of behavior cloning on curated data followed by RL using PPO. They find that this improves performance over the untrained model, but that it still lags behind solvers like CFT. \nTo further improve the model, they propose ToolPoker, a method which enables LLMs to recruit external solvers and tools to improve.\nThey train ToolPoker also via a combination of BC and PPO and find that it further improves and closes much of the gap between models and CFP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper compares to standard baselines\n- games are run across multiple runs \n- both open and closed-source models are considered\n- qualitative analysis of reasoning is backed by quantitative results\n- The LLM judge is externally validated\n- BC+RL results show improvements, with ToolPoker showing further improvements"}, "weaknesses": {"value": "- ToolPoker improvements are not that surprising to me. Basically the gains here seems to boil down to externalizing the parts of the task that are more difficult for the model to external solvers. Implementationally, the training method might be useful, but I don't see what research question it is addressing that hasn't already been addressed by the BC+RL experiments. It seems like in the end, ToolPoker is largely introducing an engineered system that would be of limited use, since it offloads most of the strategic reasoning the model would do to other tools (so no longer getting at the motivating point of the paper on strategic reasoning) while also being costlier and less effective than baseline poker-playing solutions. \n- L083 the claim is made that ToolPoker is for imperfect-information games, but it is only evaluated on poker, and seems very explicitly engineered for poker with limited transfer to other games. \n- The reasoning analysis does not take into account that reasoning might not be faithful. Plenty of prior work has called into question whether reasoning traces from LLMs are faithful explanations of their behavior (see https://aigi.ox.ac.uk/wp-content/uploads/2025/07/Cot_Is_Not_Explainability.pdf for references). The analysis seems to hinge on reasoning being a causal explanation of the model's actions -- if that's not the case, it would explain the knowledge-action gap. \n- if ToolPoker uses CFR solver as a tool, why does it not outperform CFR in Table 5? L436 indicates it's a result of tool-calling errors, is there a way to reduce these? \n- The comparison of tool calling reasoning feels spurious. In this case, the tools are providing a lot of information that was not available to the model without tools."}, "questions": {"value": "Minor comments:\n- Tables 1, 3, 4 are unreadably small"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1bQrOSUZTQ", "forum": "vV54ShHvGi", "replyto": "vV54ShHvGi", "signatures": ["ICLR.cc/2026/Conference/Submission496/Reviewer_4eQy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission496/Reviewer_4eQy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761273552874, "cdate": 1761273552874, "tmdate": 1762915532062, "mdate": 1762915532062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the ability of LLMs to perform game-theoretic reasoning in the domain of two-player poker, a challenging incomplete-information game. The authors identify three fundamental reasoning flaws in current LLMs: reliance on shallow heuristics, factual misjudgments, and a marked \"knowing-doing\" gap between articulated reasoning and actions taken. Following attempts to mitigate these flaws using behavior cloning (BC) and regret-inspired reinforcement learning (RIRL), the authors propose ToolPoker in which LLMs interface with external solvers (e.g., CFR and equity calculators) via a unified tool API. ToolPoker combines imitation and RL to ensure both game-theoretic optimal (GTO) play and reasoning traces aligned with professional-level principles."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-written and well-organized. The proposed method is simple and easy to follow.\n2. This paper presents extensive experimental results with detailed analysis on the ablation study as well as limitations."}, "weaknesses": {"value": "1. The novelty of the paper is limited. It mainly applies reinforcement learning to a large language model using a classic game-theoretic solver (e.g., CFR+) as the reward signal or direct PPO, without introducing any fundamentally new algorithmic contributions or insights. Essentially, the work repackages standard solver outputs within an RL fine-tuning framework, resulting in incremental rather than conceptual advancement. Also, while the related work section is decent, it omits several works that would be natural inclusions, such as [1]\n\n2. While the core analysis and ToolPoker formulation are well-motivated by Leduc Hold'em and Limit Texas Hold'em, the current instantiation is restricted exclusively to these benchmarks. There is little empirical or conceptual discussion about scalability to larger, multi-player, or variable-rule settings. \n\n3. The system occasionally produces factual misunderstandings when tool outputs are unavailable or misinterpreted, revealing a continuing challenge for robust factual alignment.\n\n4. Most experiments are performed with synthetic datasets or CFR-solver-based action labels, raising questions about how well ToolPoker would generalize to noisy or inherently human gameplay traces.\n\nReference:\n\n[1] Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning"}, "questions": {"value": "1. Can the authors provide more details on how the LLM-as-a-Judge scores were calibrated and validated? Specifically, are there estimates of both inter-rater LLM agreement and alignment with human judges beyond the 20-trace reference set? How sensitive are reasoning scores in Tables 2 and 4 to the prompt, model, or domain?\n\n2. How does ToolPoker handle non-standard, noisy, or human-style inputs that diverge from solver-generated play? Has the method been evaluated on datasets that originate from human-expert games or crowd-sourced play, and how robust are the results in such settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eInVfE92ea", "forum": "vV54ShHvGi", "replyto": "vV54ShHvGi", "signatures": ["ICLR.cc/2026/Conference/Submission496/Reviewer_4Bxf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission496/Reviewer_4Bxf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762060429644, "cdate": 1762060429644, "tmdate": 1762915531922, "mdate": 1762915531922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the strategic reasoning capabilities of LLMs in the domain of imperfect-information games, using poker as a rigorous and interpretable benchmark. The authors systematically evaluate several LLMs across realistic poker environments, including Leduc Hold’em and Limit Texas Hold’em, assessing both gameplay outcomes and the quality of reasoning traces. The results show that existing LLMs exhibit three recurring flaws. To address these deficiencies, the authors first test a two-stage internal improvement pipeline—behavior cloning followed by reinforcement learning with step-level rewards. Although this approach yields more coherent, human-like reasoning, it remains insufficient for accurate game-theoretic play. Motivated by these limits, the paper introduces ToolPoker, a TIR framework that enables LLMs to call external poker solvers for GTO actions and quantitative support such as equity and hand ranges. ToolPoker unifies solver interfaces through a single API, constructs an expert-level reasoning dataset augmented with solver outputs, and trains models using a combination of supervised fine-tuning and PPO-based reinforcement learning"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The first systematic study analyzing LLM reasoning and action alignment in poker, identifying fundamental weaknesses in heuristic dependence, factual errors, and knowing–doing gaps.\n\n- A detailed investigation of whether behavior cloning and step-level RL can internally mitigate these flaws, revealing their limited capacity to achieve GTO-consistent reasoning.\n\n- ToolPoker integrates external solvers into LLM reasoning for imperfect-information games, with a unified API and solver-augmented training corpus."}, "weaknesses": {"value": "1. The composite reward (Eq. 4) combines R_answer, R_format, and R_tool with tunable weights.  How each component quantitatively contributes to tool-learning behavior. Providing ablation or sensitivity analyses—e.g., varying α_f and α_t, or visualizing reward trajectories—would improve transparency and reproducibility. Moreover, discussing how the model avoids reward hacking, e.g., overusing the solver or formatting cues without deeper reasoning, would strengthen the credibility of the RL setup.\n\n2. ToolPoker’s underlying design—LLMs invoking structured external APIs and fine-tuned with PPO—is conceptually close to frameworks like ReAct (Yao et al., 2023), Toolformer (Schick et al., 2023), and ReTool (Feng et al., 2025). The paper would benefit from explicitly contrasting how ToolPoker extends these approaches to imperfect-information and equilibrium-seeking contexts, possibly through a comparative discussion table or controlled ablation with ReTool baselines. Without this, the contribution risks being perceived as an application-specific adaptation rather than a fundamentally new paradigm. \n\n3. To reach the stated goal of establishing a “principled, general framework for tool-integrated strategic reasoning,” it would benefit from stronger theoretical justification. Why or when the hybrid architecture (LLM reasoning + solver calls) should converge toward a game-theoretic equilibrium, nor how tool-use uncertainty affects strategic optimality.  for instance, linking solver-invocation frequency to bounded rationality or expected regret"}, "questions": {"value": "1. Whether ToolPoker has any formal link to equilibrium convergence or regret minimization theory? Specifically, under what assumptions does the interaction between the LLM’s policy and the external solver guarantee or approximate GTO consistency?\n\n2. How sensitive is the model’s performance to the weighting of the three reward components-answer, format, tool execution? Were any instabilities observed when tuning these hyperparameters?\n\n3. Does ToolPoker include any mechanism to regulate when the solver should be called, or does it always invoke the solver at each decision step? If so, could you analyze cases where solver calls lead to redundant or conflicting information, and whether the model learns adaptive tool-use behavior over time?\n\n4. The paper cites ReTool (Feng et al., 2025) but does not present a direct comparison. Given the conceptual similarity—both integrate external APIs into LLM reasoning—could the authors discuss key differences in design philosophy or experimental setup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RS9N1UorP7", "forum": "vV54ShHvGi", "replyto": "vV54ShHvGi", "signatures": ["ICLR.cc/2026/Conference/Submission496/Reviewer_whik"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission496/Reviewer_whik"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762264328264, "cdate": 1762264328264, "tmdate": 1762915531780, "mdate": 1762915531780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper uses poker to investigate game-theoretic reasoning in LLMs. It first compares vanilla LLM performance against several traditional baselines for game-theoretic reasoning, finding that LLMs noticeably underperform these baselines. An approach for improving LLM performance on poker is then introduced, with two main components: (1) behavior cloning, which consists of fine-tuning the LLM on expert-level trajectories augmented with reasoning traces and (2) RL fine tuning. This approach fails on its own, but these two high-level components are incorporated into the paper's last main contribution, ToolPoker, which allows the LLM to incorporate calls to external poker solvers into its reasoning trace (and is trained on a similar combination of behavior cloning and RL fine-tuning)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is very clear in its presentation and generally high in quality: each newly introduced approach built on the last in a way that made the paper particularly easy to follow and made the motivation for the different components of the ToolPoker system apparent. The main points of significance and originality for the paper are that (1) it evaluates how LLMs reason about poker and presents qualitative and quantitative analyses of particular types of shortcomings in the reasoning process of vanilla LLMs and (2) applies a tool use framework to the poker setting."}, "weaknesses": {"value": "The primary weakness of this paper lies in the novelty of the approach: while the paper does a very good job analyzing LLM performance on poker and explaining why the ToolPoker approach was developed, it is not clear if there is anything that sets ToolPoker apart from other tool-use frameworks, other than the task setting. In particular, explicitly comparing the strengths of ToolPoker with other approaches like ReTool (mentioned in the paper) would be helpful in evaluating the approach."}, "questions": {"value": "1. What sets ToolPoker apart from other tool-use approaches?\n2. Can ToolPoker be generalized to other imperfect-information games? What changes might have to be made (other than the tools called)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CIzY0ILvW2", "forum": "vV54ShHvGi", "replyto": "vV54ShHvGi", "signatures": ["ICLR.cc/2026/Conference/Submission496/Reviewer_3gDV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission496/Reviewer_3gDV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762479480029, "cdate": 1762479480029, "tmdate": 1762915531535, "mdate": 1762915531535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}