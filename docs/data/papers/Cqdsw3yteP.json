{"id": "Cqdsw3yteP", "number": 9276, "cdate": 1758117179587, "mdate": 1759897733844, "content": {"title": "Entropy Regularizing Activation: Boosting Continuous Control, Large Language Models, and Image Classification with Activation as Entropy Constraints", "abstract": "We propose ERA, a new paradigm that constrains the sampling entropy above given thresholds by applying specially designed activations to the outputs of models. Our approach demonstrates broad effectiveness across different domains: 1) for large language models~(LLMs), boosting the AIME 2025 score for Qwen2.5-Math-7B by 37.4%; 2) for continuous control reinforcement learning agents, improving performance by more than 30% over strong baselines such as SAC on the challenging HumanoidBench; 3) for image classification, enhancing ImageNet top-1 accuracy by 0.69% for ResNet-50. These gains are achieved with a computational overhead of less than 7%. Our work validates output activation as a powerful tool for entropy control, opening a new direction for designing simpler and more robust algorithms.", "tldr": "We propose ERA, a new paradigm that constrains sampling entropy by applying specially designed activations to models' output. This approach demonstrates broad effectiveness across domains.", "keywords": ["Entropy", "Continuous Control", "Large Language Models", "Image Classification"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6ce994feefe5a75bba0f0bb7f32ce2b6d6c1aa1b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a novel way to enforce a minimal entropy for the final distribution returned by a neural network. This is typically required for reinforcement learning tasks: you want to learn policies that are not overly deterministic and preserve some stochasticity, which is both good for exploration and robustness. In prior works, such constraint was softly imposed by adding an entropy term to rewards or Q-functions, and thus by maximising a modified Q-function: you maximize both cumulative rewards and an entropy of the policy. Indeed, that was one of the features of the soft-actor-critic algorithm considered as a baseline in this paper. ERA proposed by the paper introduces a new post-activation function that imposes a minimal entropy in the hard way. This is achieved by modifying the final std of the Gaussian, variable $\\sigma$. This approach is further extended to classification and LLM fine-tuning with GRPO tasks. The final results show consistent improvements over many baselines over many continuous control tasks, math solving benchmarks and classic image classification.\n\nI suggest a weak accept for the paper as the idea of imposing entropy through an activation is original and very useful in practice, as shown in the experiments, however the paper can benefit from improved writing, more clear problem formalism and rigourous proofs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The idea of designing a special activation function that would enforce a minimal entropy is interesting and very useful in practice. With the help of many experiments, the authors showed that this activation function is sufficient to 1) keep entropy at a required level, 2) achieve superiour results compared to the trainings without ERA. I have appreciated that authors made a lot of experiments in different settings: for continuous control tasks they checked many difficult problems from Humanoidbench, DMC and Mujoco; for LLM  — finetuning 3 different sets of math competitions; for image classification — Cifar 10 and Imagenet. While improving results for many fixed architectures, ERA introduces only a slight time overhead."}, "weaknesses": {"value": "The paper sometimes lacks mathematical rigour and details. It is the most noticeable when the authors try to formalise the RL for both classic control tasks and LLM. The authors do not introduce what is $s$, $a$, $x$ and $y$, therefore it is also not clear what is $y^{1:K}$. While it is indeed clear what are states and actions in context of RL, for readers that are not familiar with RL for LLM, which is relatively recent, understanding what are $x$ and $y$ and how they are connected to state-action formalism can be challenging. Some equations are written in complicated way, while can be simplified: for example $\\exp(\\max(…, \\log(\\sigma))$ can be simplified by writing  $\\max(\\exp(…), \\sigma)$. Finally, while most theoretical results look sound, the proofs in some parts are not clear and maybe erroneous (see below)."}, "questions": {"value": "**What needs clarification**\n- Section 4.1: It should be more explicit at which level of the neural network ERA is applied. For example, in case of tanh transformation to get the final policy, is ERA applied before tanh or after?\n- Section 4.2.1: In the main paper, it is not clear what is the exact definition of $\\delta$ and how it is different from $\\hat \\delta$. Also it is not clear whether $\\mathcal{H}’_0 = \\mathcal{H}_0 + \\delta$ or $\\mathcal{H}’_0 = \\mathcal{H}_0 + \\hat \\delta$. It becomes more clear after reading the proof, but it should be more transparent in the main paper as well.\n- Figure 3: explain what IQM stands for.\n- Proposition 2, proof: the last inequality in eq.25 and first equality in eq.26 are not clear. I think eq.26 might be wrong for the current choice of $h(x) = \\ln(-x\\ln x)$, while it is trivial for $h(x) = -x \\exp(x)$.\n- Proposition 3, proof: I don’t understand how do you get eq.35, also the reasoning after eq.35 for obtaining sufficient condition. Furthermore, eq.33 and eq.34 might not be completely true: $\\Delta H = \\sum_a \\frac{\\partial H}{\\partial z_a} \\Delta z_a$, but as updates are performed with respect to $\\theta$ using new policy gradient formula, $\\Delta z_a$ may not be equal to $\\frac{\\partial J}{\\partial z_a}$. Check https://arxiv.org/pdf/2405.19816, Eq. 2 for further understanding.\n\n\n**Technical questions**\n- page 4: Preactivation mean $\\mu$ and std $\\hat \\sigma$ are denoted in different styles, can it be homogenized?\n- page 5: why traditional entropy methods are incompatible with the on-policy setting (because of entropy collapse?)\n- Eq. 16: what transformation do you perform when $A_t$ is negative, but $H_{resp}< \\omega_{low}$ or  $H_{resp}> \\omega_{high}$?\n- Figure 2: why do you compare FastTD3 vs Fast Sac-ERA in the third plot? With respect to what do you normalise the performances? \n- What final transformation do you use? Tanh or truncated Gaussian?\n- Table 2,3: I see some discrepancies in reported performances of base Qwen2.5-Math and Qwen2.5-Math-Instruct on Olympiad and Minerva benchmarks with respect to the original paper https://arxiv.org/pdf/2409.12122, can you please comment on why there are some differences?\n- Figure 15: On which tasks did you do time measurements? I expect time complexity to be $O(D)$, while humanoid bench has tasks with D=19 and D=61.\n- What kind of distributions are $\\tau$-divided distributions? Can you provide an example?\n\n\n**General questions**\n- Do you follow any principle when choosing target entropy? In the appendix, the choice seems to be problem-dependent.\n- Is ERA sensitive to the choice of the interval $[\\sigma_{min}, \\sigma_{max}]$? In general, how do you define these intervals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hSe40MUge9", "forum": "Cqdsw3yteP", "replyto": "Cqdsw3yteP", "signatures": ["ICLR.cc/2026/Conference/Submission9276/Reviewer_1XEa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9276/Reviewer_1XEa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760716164632, "cdate": 1760716164632, "tmdate": 1762920922934, "mdate": 1762920922934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a way to control of policy entropies by introducing ERA, a dedicated activation function for the final layer to clip entropies within bounds. The proposed framework can be applied to generic policy gradient settings and is experimented with continuous control, discrete classification and LLM-RL training although each setting requires dedicated instantiation to integrate the idea into its training. In experimental results, models trained with ERA outperform baselines across those three settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- ERA is a generic concept although this requires dedicated instantiations for each task and model.\n- Authors show strong performance improvements of each instantiation in through extensive experiments.\n-  For continuous control experiments, the authors show that ERA improves performance over different RL algorithms, which shows its generality."}, "weaknesses": {"value": "- It's not clear why SAC without the entropy term in targets is outperformed by ERA since Q-function is basically trained through the same objective between two algorithms in this case. Is this because ERA provides a hard way to constrain the entropy range? I believe that an analysis on behavior of entropy helps to clarify this.\n- This is a minor point. In the LLM-RL setting, ERA needs to modify not only the logits of tokens, but also advantages. This seems beyond just an activation layer although the paper claims it proposes activation functions.\n- Authors should provide how many trials they conducted each benchmark to get statistics not only for continuous control benchmarks, but also for the rest of benchmarks."}, "questions": {"value": "I understand that LLM-RL training takes time and consumes resources. So I leave some comments as questions, not requests.\n- How the number of `top 20%` was selected in the LLM-RL instantiation? Have you tested different values to see how it affects performance?\n- Have you tested ERA in LLM-RL without the advantage modification? If this works out, ERA for LLM-RL can be just the activation layer change."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DVg9Yg0ZGC", "forum": "Cqdsw3yteP", "replyto": "Cqdsw3yteP", "signatures": ["ICLR.cc/2026/Conference/Submission9276/Reviewer_8uvP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9276/Reviewer_8uvP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760877345887, "cdate": 1760877345887, "tmdate": 1762920922618, "mdate": 1762920922618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose in this paper a set of parameter transformations that map input distribution parameters (e.g. logits of a discrete distributions or mean/logstd of a Gaussian distribution) to parameters that are above an entropy threshold. The main motivation of the paper is that entropy is a key factor in RL as it helps with exploration, yet optimizing an entropy regularized objective can be challenging as it requires to carefully choose the entropy weight. Instead of using soft entropy regularization, the authors propose to introduce entropy regularization directly in parameter space, effectively restricting the search space to policies having an entropy above a given threshold. \n\nIn details, the authors propose three parameter transformations for i) discrete distributions ii) squashed (Tanh) Gaussian distributions and iii) truncated Gaussian distributions. The authors integrated the latter two transformations for continuous control RL tasks with bounded actions showing improved performance over baselines such as SAC or TD-MPC2, and the first transformation for the fine-tuning of LLMs on math problem datasets or for training a ResNet-50 on image classification tasks; in all cases showing improvements over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper extends prior work on differentiable parameter projections to new distributions and has an extensive empirical evaluation spanning an impressively large number of applications and baseline algorithms, with promising empirical results."}, "weaknesses": {"value": "- The paper does not discuss prior work even though there is existing literature extremely related to the submission. Namely, the paper [1] (*F. Otto et al.; Differentiable Trust Region Layers for Deep Reinforcement Learning; ICLR 2021*) proposes a set of parameter projections that enforce among other things KL and Wasserstein based trust regions. The paper [2] (*R. Akrour et al.; Projections for Approximate Policy Iteration Algorithms; ICML 2019*) proposes parameter transformations for KL and entropy constraints much like the current paper. [2] proposes a projection for Gaussian distributions and the current submission uses the same projection as a basis to extend the projection to Squashed and Truncated Gaussians. The contribution compared to [2] is thus clear (extending prior parameter transformations to new distributions) but I believe [1,2] should still be discussed in the related work. However, [2] also proposed a projection to enforce an entropy constraint for discrete distributions much like this submission, although the mechanisms seem at a glance a bit different: this paper scales the input logits while [2] mixes the input distribution with a uniform distribution. Since these two transformations serve exactly the same purpose (map an input discrete distribution to a distribution satisfying an entropy constraint), it would be good to at least discuss the conceptual differences between the two transformations, but ideally some additional empirical comparisons to gain insights on their practical differences would also be appreciated.   \n\n- The paper contains a few overclaims, i) first regarding SAC in l96 authors state that the variant of SAC that uses an entropy constraint with a dynamically adjusted temperature can lead to instability. I would appreciate some evidence supporting this claim. To the best of my knowledge this second version of SAC with a hard entropy constraint is the default SAC implementation in many deep RL code bases such as *stablebaselines*, and I have not heard or witnessed such instabilities before. ii) In a few places (e.g. l68 or l111) the authors state that their parameter transformations have provable guarantees, but from what I can see for squashed and truncated Guassians, the authors still need to adjust a parameter online, much like SAC as discussed in l1049, and the satisfaction of the entropy constraint is not guaranteed in for these distributions. \n\n-  For a paper whose main contribution is to provide a better control of the entropy, I was surprised to not see comparisons between the entropy control resulting from the author's method and from the dynamic tuning performed by SAC. Especially in light of the previous two (over)claims, it would be interesting to see how both algorithm manage to comply with their entropy constraints.  \n\n- The proof in appendix B.1 feel incomplete to me. The authors state that the $\\delta$ is a positive constant and although it seems trivially true for $\\delta_{\\text{tanh}}$, I would appreciate a proof for $\\delta_{\\text{TN}}$ \n\n- Wording regarding SAC-ERA's pseudocode is a bit confusing, authors talk of a soft Q-function (l933), but it seems to me that the algorithm is not in the entropy regularized RL paradigm and an unregularized Q-function is learned, making the implementation closer to TD3 than SAC.\n\n\n\nOverall, while I'm currently leaning towards a reject, I am willing to increase my score if the authors can better contextualize their work and provide some empirical insights regarding the entropy control their method proposed compared to existing methods."}, "questions": {"value": "- What is the difference between the proposed projection for discrete distributions and the one proposed by [2]? How do you expect conceptual differences to manifest in practice?\n- What is your evidence that SAC is unstable when tuning its entropy temperature dynamically and why do you expect the online tuning of $\\delta$ to be more stable?\n- What are the benefits of the parameter transformations compared to enforcing an entropy constraint by tuning the temperature online (as in SAC) for image classification or LLM fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cJTQUk3wow", "forum": "Cqdsw3yteP", "replyto": "Cqdsw3yteP", "signatures": ["ICLR.cc/2026/Conference/Submission9276/Reviewer_x7W9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9276/Reviewer_x7W9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921588216, "cdate": 1761921588216, "tmdate": 1762920922318, "mdate": 1762920922318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Entropy Regularizing Activation (ERA), a framework that enforces entropy constraints via custom output activations rather than through explicit loss terms. This allows entropy control without disturbing the main optimization objective. The authors demonstrate ERA in three settings, continuous control, image classification, and reinforcement learning for large language models, showing consistent performance improvements and theoretical guarantees for minimum entropy. The method is lightweight, general, and easy to integrate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**1. Simple yet general idea:** Treating entropy regularization as an architectural operation rather than a loss modification is elegant and broadly applicable. It bridges reinforcement learning, LLM training, and supervised classification under one principle.\n\n**2. Strong empirical validation:** The experiments are extensive and diverse, showing consistent improvements with minimal tuning or overhead (<7%). This cross-domain effectiveness strengthens the claim of universality."}, "weaknesses": {"value": "**1. Naming / framing:** Calling it “entropy as an activation” feels misleading; the object under control is the policy entropy. “Entropy-constrained policy\" or \"Entropy-constrained policy via output activation” would better reflect what’s actually enforced (and avoid implying entropy is a property of the activation itself).\n\n**2.Control-side doubts:** SAC’s state-conditional entropy is a feature, not a bug: high entropy in uncritical states, low in critical ones. A fixed or globally enforced entropy can bias learning toward suboptimal policies and blunt per-state adaptivity (e.g., see recent discussions on fixed-entropy pitfalls) [1]. This deserves explicit treatment in limitations and experiments probing failure modes when the target is mis-set.\n\n**3. Baseline realism in SAC:** Recent work/practice often uses very small or carefully scheduled entropy coefficients in SAC to avoid instability. If ERA is compared to SAC with suboptimal temperature tuning, improvements may be overstated [2],[3]. The paper should verify that gains hold under strong, modern SAC setups (small-α inits, robust target-entropy tuning/schedules).\n\n[1] When Maximum Entropy Misleads Policy Optimization, ICML'25.\n\n[2] https://araffin.github.io/post/sac-massive-sim/\n\n[3] Hyperspherical Normalization for Scalable Deep Reinforcement Learning, ICML'25."}, "questions": {"value": "Some suggestions following from Weakness\n\n**1. Clarify the naming and framing**: Consider revising the term “entropy as an activation” to something like “entropy-constrained policy via activation” to better capture the actual mechanism. The current phrasing may be conceptually misleading.\n\n**2. Acknowledge limitations of fixed entropy:** In the limitations section, explicitly discuss when ERA might underperform compared to standard SAC or entropy-maximized RL, especially in cases where adaptive, state-dependent entropy is important.\n\n**3. Evaluate under stronger SAC setups:** Re-run or compare against more standardized, high-performing SAC configurations (e.g., small initial entropy coefficients or temperature scheduling) to demonstrate that ERA’s improvements persist under robust, modern baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7vDYgJ3Ka9", "forum": "Cqdsw3yteP", "replyto": "Cqdsw3yteP", "signatures": ["ICLR.cc/2026/Conference/Submission9276/Reviewer_m4ks"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9276/Reviewer_m4ks"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951931521, "cdate": 1761951931521, "tmdate": 1762920921950, "mdate": 1762920921950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}