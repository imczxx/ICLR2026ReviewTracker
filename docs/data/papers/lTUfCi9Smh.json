{"id": "lTUfCi9Smh", "number": 17197, "cdate": 1758273345275, "mdate": 1759897191271, "content": {"title": "Defeating Cerberus: Concept-Guided Privacy-Leakage Mitigation in Multimodal Language Models", "abstract": "Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in processing and reasoning over diverse modalities, but their advanced abilities also raise significant privacy concerns, particularly regarding Personally Identifiable Information (PII) leakage. While relevant research has been conducted on single-modal language models to some extent, the vulnerabilities in the multimodal setting have yet to be fully investigated. In this work, we investigate these emerging risks with a focus on vision language models (VLMs), a representative subclass of MLLMs that covers the two modalities most relevant for PII leakage, vision and text. We introduce a concept-guided mitigation approach that identifies and modifies the model’s internal states associated with PII-related content. Our method guides VLMs to refuse PII-sensitive tasks effectively and efficiently, without requiring re-training or fine-tuning. We also address the current lack of multimodal PII datasets by constructing various ones that simulate real-world scenarios. Experimental results demonstrate that the method can achieve an average refusal rate of 93.3\\% for various PII-related tasks with minimal impact on unrelated model performances. We further examine the mitigation's performance under various conditions to show the adaptability of our proposed method.", "tldr": "", "keywords": ["Multi-modal", "privacy", "personal identifiable information"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/392ec8febf233fae39b395208a2416fdd809c9d6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes concept-guided privacy leakage mitigation method for MLLMs, specifically focusing on VLMs. The authors identify internal concept directions relatd to PII and steer model weights along these directions to reduce privacy leakage. They also construct two multimodal PII datasets (PII-Table and CelebA-Info) and report strong refusal rates without retraining."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and follows a standard empirical format.\n\n- The problem of privacy-violation in MLLMs is important and socially relevant.\n\n- The experiments are relatively comprehensive across multiple MLLMs."}, "weaknesses": {"value": "1. The inclusion of VHTest is appreciated, as it provides a sanity check on false refusals for non-PII tasks. However, VHTest is a small-scale, purpose-built dataset rather than a general multimodal benchmark. To convincingly demonstrate that the proposed steering does not harm general utlity, I strong recommend evaluating on one more widely-recognized multimodal benchmarks.\n\n2. The paper references activation-space steering methods such as Representation Engineering and Refusal is a Direction, which operate on hidden-state representations. However, the proposed method alters model weights instead, without any theoretical argument or empirical validation explaining why manipulating weights should preserve or even correspond to the same conceptual directions discovered in activation space. This conceptual leap—from activation-space intervention to weight-space modification—requires stronger justification. Without analysis of stability, representation drift, or equivalence between these spaces, the method feels ad-hoc rather than principled.\n\n3. The dataset contribution is marginal. Both PII-Table and CelebA-Info are synthetic repackaging of existing text-based PII datasets converted into image form, or face-text composites built from public datasets like CelebA. There is no new data collection, annotation, or benchmarking protocol. Comparable multimodl PII datasets already exist. Thus, the dataset component mainly serves as a format conversion utility rather than a research contribution."}, "questions": {"value": "1. Could the authors evaluate their steering method on more widely-recognised multimodal benchmarks (e.g., VQA, OCR, captioning, multi-task perception) rather than just their constructed PII-specific datasets? Including one or two general benchmarks would strengthen the claim that the mitigation preserves general model utility.\n\n2. Beyond privacy/PII leakage, does the proposed weight-steering intervention affect other safety behaviors (e.g., refusal consistency, toxicity, jailbreak vulnerability)? Have the authors tested whether the intervention inadvertently degrades other safety mechanisms or introduces new risks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2Fgxe03z9H", "forum": "lTUfCi9Smh", "replyto": "lTUfCi9Smh", "signatures": ["ICLR.cc/2026/Conference/Submission17197/Reviewer_p7pb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17197/Reviewer_p7pb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761308831344, "cdate": 1761308831344, "tmdate": 1762927171304, "mdate": 1762927171304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method to prevent the disclosure of personally identifiable information (PII) in responses generated by a VLM, given the input image containing PII. The authors construct a multimodal privacy dataset consisting of images either directly converted to a visual format or created by superimposing a relevant image onto the textual information. The authors then use this dataset for their internal concept steering method, which computes directional steering vectors using positive and negative examples (safe and unsafe examples) based on the concept of PII. These vectors are computed layer-wise and are specifically used to steer the last few layers towards understanding unsafe disclosures. The authors then test their method on a PII-heavy VQA task and compare it to prompting baselines, finding that their method performs better than these simple baselines across model families. Finally, the authors conduct various analyses, such as understanding the effect of the coefficient applied to the steering vector and determining what types of PII are most successfully protected."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors target an important problem of protecting the leakage of PII of multimodal LLMs. The authors are correct to note that most of the work in PII protection has focused on LLMs, not VLMs.\n\n2. The proposed method clearly and demonstrably beats the baseline prompting defenses without drastically hurting overall model performance.\n\n3. The authors propose an interesting concept steering method based on representation engineering to guide models towards refusing to disclose PII."}, "weaknesses": {"value": "1. The abstract claims that the method does not require fine-tuning or retraining, but weight updates are mentioned in Section 4.2. While I agree that this is not supervised fine-tuning, any weight updates should be considered training, and therefore, this claim about not requiring fine-tuning is quite misleading. \n\n2. From weakness 1, I think that many baselines are missing, including direct SFT, DPO on the positive and negative examples, few-shot prompting, and prompting with explicit guidelines about how the PII should look. Specifically, I think it would be necessary to show that you outperform SFT and DPO and/or show that your method is less computationally expensive.\n\n3. I looked in the appendix, but it seems like the prompt used for the prompting baselines is not provided. Could you please provide the prompt? I am worried the prompt is too generic, and so the baselines are weak.\n\n4. In Lines 311 - 314, you mention how you only measure whether the model refuses to respond to the query. However, it seems like such a task would be agnostic of the actual input image, i.e., you could run an LLM to refuse questions without looking at the image. Could you clarify this? Do you need the image context to make abstention decisions?\n\n5. The authors mention that sometimes models will respond with benign information, but not outright refuse, and that this is counted as a non-refusal. I think this isn't a fair analysis, and may be artificially pushing down your baseline performance.\n\n6. The introduction mentions that \"similar vulnerabilities in newer MLLMs are yet to be thoroughly investigated. However, there has been prior work on MLLM PII preservation, such as [1].\n\n[1] Chen, Yang, Ethan Mendes, Sauvik Das, Wei Xu, and Alan Ritter. \"Can language models be instructed to protect personal information?.\" arXiv preprint arXiv:2310.02224 (2023)."}, "questions": {"value": "1. How are refusals determined? Line 315 says \"typical phrases\" are used. However, there are no details about how this is done in the appendix. Is an LLM used?\n\n2. You mention in line 377 that you chose baselines comparable to yours in setup since they do not require computing resources. However, it seems like your weight update concept steering algorithm would require computational resources. Could you clarify this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VJprasGXJy", "forum": "lTUfCi9Smh", "replyto": "lTUfCi9Smh", "signatures": ["ICLR.cc/2026/Conference/Submission17197/Reviewer_QMQh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17197/Reviewer_QMQh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761778766456, "cdate": 1761778766456, "tmdate": 1762927171066, "mdate": 1762927171066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a lightweight, training-free method to prevent VLMs from answering PII-related prompt by steering internal LLM representations toward refusal. Using activation engineering, the proposed method extracts a “PII sensitivity” direction from text-only data and apply weight edits to make models refuse PII-extraction requests from VLMs prompts. The approach achieves 93.3% average refusal on PII tasks with claimed minimal impact on benign performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- First attempt to create realistic multimodal PII datasets\n\n- Clean application of LLM activation steering to a VLM problems\n\n- Strong refusal results with 93.3% average refusal across PII-Table, CelebA-Info, and scanned variants, while maintain low false positives on VHTest and non-PII DocVQA (<5%)."}, "weaknesses": {"value": "- Core technique (PCA on activation diffs → weight edit) is very similar to LLM activation steering [1]. There is almost no new theoretical or algorithmic contribution. Actually the whole concept of this paper is an application of LLM activation steering to PII problem (which could be seen as a problem of alignment research)\n\n- The utility evaluation is incomplete. While low non-refusal on non-PII datasets is reasonable to some extent, it can not accurately measure the trade-off in model utility. For example, by changing the weights, it still can answer the non-PII tasks but we are not sure about the quality of the answer. Additionally, the OCR performance shouldn't be excluded as stated in the paper. Can we test the performance on general VLM benchmarks before and after applying the steering.\n\n- The evaluation of PII datasets is mostly generated data. While this is somewhat fine as the first investigation, the real data should be majorly considered to realistically evaluate this problem. Can we evaluate the proposed method on more real samples?\n\n\n[1] Arditi, Andy, et al. \"Refusal in language models is mediated by a single direction\" NeurIPS 2024"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HGvg83gOrc", "forum": "lTUfCi9Smh", "replyto": "lTUfCi9Smh", "signatures": ["ICLR.cc/2026/Conference/Submission17197/Reviewer_cZm2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17197/Reviewer_cZm2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799634358, "cdate": 1761799634358, "tmdate": 1762927170693, "mdate": 1762927170693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Summary: This paper presents a concept-guided mitigation approach that identifies and modifies the model’s internal states associated with PII-related content to mitigate the PII leakage attacks in Vision Language Models, without fine-tuning the model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis paper introduces a dataset including PII with existing dataset sources for evaluating the security measure of VLMs under the proposed defense.\n2.\tIt proposed a mitigating technique against the PPI leakage attack by modifying the model’s internal state that triggers the model to reveal PIIs. It does not require further fine-tuning, which contributes to a lightweight solution."}, "weaknesses": {"value": "1.\tThe rationale behind the lines 262-264 is not clear enough. How does the model become less inclined to reveal the PII upon modifying the model’s weight in the direction V? What does this direction vector V mean?\n2.\tSince the proposed method modifies the model weights, what’s the difference of fine-tuning it? What are the benefits of it instead of fine-tuning? What’s the difference in PII refusal rate for the proposed method and fine-tuning?\n3.\tThe evaluation has been performed only on open-source small models. How would it be employed for the closed-sourced production models, such as GPT-5, Gemini-2.5?\n4.\tThis paper mentions several jailbreaking and backdoor attacks in lines 59-60. Against which specific PII leakage attack has this defense been proposed/evaluated?\n5.\tHow does the proposed defense impact the model’s performance/utility?"}, "questions": {"value": "Please follow the Weaknesses and respond to them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xazpjUINbi", "forum": "lTUfCi9Smh", "replyto": "lTUfCi9Smh", "signatures": ["ICLR.cc/2026/Conference/Submission17197/Reviewer_Vqu9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17197/Reviewer_Vqu9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939024568, "cdate": 1761939024568, "tmdate": 1762927170337, "mdate": 1762927170337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}