{"id": "V4YGa3V2QO", "number": 9060, "cdate": 1758108912012, "mdate": 1759897745899, "content": {"title": "SecTest-Eval: Can LLMs Verify Security Impacts of A Vulnerability?", "abstract": "Large language models (LLMs) have shown remarkable potential in cybersecurity applications, particularly in code vulnerability analysis for both defensive and offensive purposes. Hence, several benchmarks have emerged to evaluate LLMs' capabilities in verifying security impacts of a vulnerability. To achieve this, existing benchmarks challenge models with generating a test program for verifying a given security impact in the context of project-level code. While demonstrating the complexity of real-world scenarios, existing benchmark may lead to underestimate LLMs' comprehension on exploitation methods causing security impacts due to excessive context length and limited covered exploitation methods and security impacts. In this paper, we introduce SecTest-Eval, a benchmark that challenges LLMs to generate a unit test program for verifying security impacts of a vulnerable function. Task instances in SecTest-Eval covering vulnerable C/C++ functions in different lengths, across 14 weakness types and 3 security impacts covering confidentiality, integrity, and availability. Our evaluation of 5 state-of-the-art LLMs shows that LLMs are not yet capable of precisely verifying security impacts through test program generation, with success rates up to 56%, significantly lower than the requirements for practical security applications. The results underscore the need for further advancements in LLMs' vulnerability analysis capabilities.", "tldr": "", "keywords": ["Large Language Models (LLMs)", "Security Impact", "CIA Triad", "SecTestCase", "Vulnerable Functions"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a157a593ef0f4c37e53b08988735656b8bc46f7d.pdf", "supplementary_material": "/attachment/f673e2609cb73532ac71ef043d72194f4f5524b8.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a new benchmark, SecTest-Eval, for evaluating LLMs on the task of generating unit tests for verifying security impacts of vulnerable methods. \nThis subtly differs from the more common task of generating tests for vulnerable code snippets in that it requires explicit generation of \"sensitive statements\" that lead to security impacts.\nThe paper argues that existing benchmarks either focus on project-level test generation which might not isolate the vulnerability test generation capabilities of LLMs (SEC-Bench), or focus on limited security impacts (SEC-Bench, CyberEval).\nThe proposed benchmark focuses on function/method-level vulnerable code and three security impact categories: confidentiality (reading and printing sensitive information), integrity (modifying sensitive information), and availability (early termination or unresponsiveness).\nThe function-level vulnerable code snippets (C/C++) are obtained from existing vulnerability detection datasets (PRIMEVUL) which are then filtered using LLMs and manual inspection to 204 instances which contain sensitive statements (balanced by the three categories, 14 vulnerability types).\nFinally, the paper finds that 5 LLMs underperform on SecTest-Eval (~56% success rate by the best LLM) and lists the differences in terms of model type (general purpose vs. code LLMs), security impact category, function length, prompting technique, and vulnerability type."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- While there are benchmarks that evaluate detection of function-level vulnerabilities, the proposed benchmark is the first to look at the security impacts of these vulnerabilities. \n- The experiments section is well-written and easy to understand."}, "weaknesses": {"value": "- The models evaluated in the experiments are LLMs with two simple prompting strategies. The state-of-the-art methods on several Software Engineering tasks are agentic frameworks such as SWE-agent [1] and OpenHands [2] which are not evaluated here.\n- The experiments also use a temperature of 0 which does not necessarily guarantee determinism for modern LLMs. \n- There is no justification provided for the choice of security impacts. Is there a concrete definition or list of security impacts that can be cited here?\n- While I appreciate the list of insights from the experiments in Section 4.2, I do not understand how this guides (a) practioners who would like to use LLMs for this task and (b) future research in this area. The proposed future directions in Section 6 are applicable to the use of LLMs for most tasks, are there any pointers specific to the studied task?\n\nThere are some instances in the main text that should ideally include citations. For instance, line 136 mentions \"Recent researchers show that many labeled vulnerable functions...\" but this is not supported with a reference. Similarly, line 098 mentions that the benchmark \"covers the three main categories of security impacts\" but this isn't supported with a reference / justification.\n\nOverall, I think the paper would immensely benefit from including state-of-the-art LLM-based agents in the evaluation and listing actionable insights which can guide either practioners / researchers in this area. Further, supporting claims with references should improve the exposition of the text.\n\n\n[1] Yang, J., Jimenez, C. E., Wettig, A., Lieret, K., Yao, S., Narasimhan, K., & Press, O. (2024). Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37, 50528-50652.\n\n[2] Wang, X., Li, B., Song, Y., Xu, F. F., Tang, X., Zhuge, M., ... & Neubig, G. (2024). Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741."}, "questions": {"value": "I will summarize my questions from the weaknesses section above (please refer to that section for more details):\n1. Could you provide a list of actionable insights which can guide practioners / future researchers in this area?\n2. Could you comment on the exclusion of SotA LLM-based frameworks from the evaluation?\n3. How were the three security impacts chosen? \n4. How well does the temperature = 0 ensure determinism of outputs?\n\nAdditionally,\n\n5. How many samples are there in the benchmark? Line 218 mentions that SecTest-Eval includes 204 instances while Table 1 states a total of 203."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CxAtQCw1wI", "forum": "V4YGa3V2QO", "replyto": "V4YGa3V2QO", "signatures": ["ICLR.cc/2026/Conference/Submission9060/Reviewer_Q1vV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9060/Reviewer_Q1vV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870490838, "cdate": 1761870490838, "tmdate": 1762920769713, "mdate": 1762920769713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new benchmark, SecTest-Eval, to evaluate LLMs’ ability to verify the security impact caused by a weakness. Different from prior work, SEC-bench and CyberEval, SecTest-Eval include additional information to generate the proof of concept test program, such as an impact description. SecTest-Eval also covers 14 weakness types, which are more than some prior work. The paper presents some analysis of LLMs' performance on this new benchmark, indicating that room for improvement exists, and there are some patterns of failure were discussed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Security vulnerabilities are important weaknesses to study and detect. Having more benchmarks would allow better evaluation of detection and analysis techniques."}, "weaknesses": {"value": "- The evaluation is lacking a comparison with prior benchmarks. It is important to justify the motivation for another benchmark and what this new benchmark contributes to the existing landscape. And it is also very crucial to quantitatively show the uniqueness of this new benchmark compared to the existing ones.\n- The comparison with the prior benchmark is not complete; there is a need for a more detailed analysis of overlap and unique entries in terms of weakness types. Also, a separability and ranking agreement analysis between TestSec_Eval and prior benchmarks, such as Sec-Bench and CyberEval, would provide a better view of how the proposed benchmark contributes to the overall landscape of existing security benchmarks.\n- Lacks a proper justification for why the three primary security impacts chosen are the most important. Is there any evidence or prior study that supports this selection?"}, "questions": {"value": "1. LLMs only achieve an 18% success rate on SEC-Bench, meaning the existing benchmarks are still quite challenging. Why do we need yet another security benchmark? Why do we need to change the usage setting and provide a different set of inputs for LLMs?\n\n2. Since SecTest-Eval covers 14 weakness types, what is the overlap with vulnerability types of prior work (memory access and privacy issues)? \n\n3. Is there any evidence or prior study that supports the selection of three primary security impacts?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "It is not clear what license the benchmark will be in. However, it is important to have a license that satisfies the license constraints from all repositories from which the data was collected. The paper does not discuss this important point."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bsUPuOmELx", "forum": "V4YGa3V2QO", "replyto": "V4YGa3V2QO", "signatures": ["ICLR.cc/2026/Conference/Submission9060/Reviewer_mNdz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9060/Reviewer_mNdz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940255799, "cdate": 1761940255799, "tmdate": 1762920769345, "mdate": 1762920769345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SecTest-Eval, a function-level benchmark, evaluates whether LLM can generate a good PoC test for vulnerabilities that can verify security impacts for C/C++ programs. Basically, the input for LLM is a triplet ⟨vulnerable function fv, its CWE type, and a target security impact I⟩, the model should output a self-contained unit test format program T that can be directly tested by execution and trigger the problem. The benchmark construct 203 function-level tasks, which cover 14 CVE types with 3 main impact categories. They evaluate the whole benchmark with five SOTA LLMs under \"Direct\" and \"COT\" mode prompting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear and well-scoped formulation. The task is precisely defined ⟨vulnerable function fv, its CWE type, and a target security impact I⟩ -> T. The benchmark provides concrete, automatically verifiable oracles for each impact type—making evaluation reproducible and objective.\n2. Enough experiment workload, provide an evaluation of the SOTA models and check with two kinds of prompts(direct and COT) to show the diverse results."}, "weaknesses": {"value": "1. Limited realism of function-level scope. Generally, real vulnerabilities often span multiple functions or modules. Evaluating only isolated functions misses complex control/data-flow dependencies—meaning success on SecTest-Eval may not translate to real-world exploitation.\n2. Minor contribution and limited diversity. The dataset largely comes from previous work PRIMEVUL, with additional filtering and relabeling. While this makes the benchmark easier to construct, it limits originality. Moreover, the dataset only covers C/C++ code, which may introduce bias and prevent a more comprehensive evaluation of LLM capabilities across languages and ecosystems.\n3. Trivial evaluation method. The evaluation of generated PoCs mainly relies on executing the produced code and checking for explicit runtime signals (e.g., file modification or crash). This rule-based approach cannot capture more subtle or complex exploit behaviors.\n4.  Limited experimental design. The experiments focus only on direct model prompting and do not consider agentic or tool-augmented settings, which are a core capability of many state-of-the-art LLM. Without including agent-based reasoning or tool use, it remains unclear whether the observed weaknesses stem from model limitations or simply from the lack of external reasoning support."}, "questions": {"value": "1. It might be good to cite some secure code generation work as a shared related interest for LLM security.\n\n[1] Seccodeplt: A unified platform for evaluating the security of code genai\n\n[2] SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code\n\n[3] CodeLMSec benchmark: Systematically evaluating and finding security vulnerabilities in black-box code language models\n\n2. Do you consider testing with some SOTA code agent, like OpenHands, Claude-Code?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P1G4Rt7Mi0", "forum": "V4YGa3V2QO", "replyto": "V4YGa3V2QO", "signatures": ["ICLR.cc/2026/Conference/Submission9060/Reviewer_Hagu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9060/Reviewer_Hagu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958817550, "cdate": 1761958817550, "tmdate": 1762920768763, "mdate": 1762920768763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LLMs show promise in analyzing security vulnerabilities but current benchmarks may underestimate their true capabilities due to excessive context length and limited coverage of exploitation methods and security impacts. The paper introduces SecTest-Eval, a benchmark that tasks LLMs with generating PoCs, unit tests to verify security impacts of 203 vulnerable C/C++ functions across 14 weakness types and 3 impact categories. Evaluation of five state-of-the-art LLMs reveals modest success rates of up to 56%, highlighting the need for further improvement in LLMs’ ability to accurately assess security vulnerabilities."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses evaluating LLMs' capabilities for the problem of PoC generation, an essential and time-consuming step for assessing the security impact of any vulnerability. The problem is very challenging, it is timely to solve in the era of LLMs, and it is under-studied relative to its practical importance.\n\n2. Proposes a novel benchmark for generating PoCs in the form of programs containing unit tests that are specifically designed to validate the security impacts of a given vulnerable function.\n\n3. Curates a balanced dataset of 203 samples, each consisting of a C/C++ vulnerable function, the type of the contained weakness (across 14 types of CWEs), and 3 impact categories (Unauthorized Data Reading, Unauthorized Data Modification, and Denial of Service).\n\n4. Demonstrates headroom for improving LLM's abilities to generate PoCs, with only 56% overall accuracy for GPT-4.1."}, "weaknesses": {"value": "1. The benchmark is limited to function-level vulnerabilities. The authors consider this a strength since the latter can underestimate the true capabilities of LLMs due to excessive context length, but it is also a significant weakness to focus on single functions.\n\n2. The algorithm for how PoCs are generated is not presented. This is a lost opportunity since it would be essential to understanding important aspects of your approach such as its effectiveness (e.g. in terms of running time, LLM inference cost, and any guarantees it provides) and generality (e.g. in terms of extending it to support multiple functions, and different kinds of CWEs and security impacts). Ideally, the presentation of the algorithm should be in terms of the formal notation introduced in the problem formulation.\n\n3. The paper has other presentation issues. Most importantly, there is no example of a generated PoC. The appendix shows many examples of non-PoCs but not a single example of a valid PoC. I would expect at least one such example, ideally in the main body of the paper, and an illustration of how the method generates it.\n\n4. The paper claims generality as a strength over existing benchmarks in terms of aspects such as coverage of exploitation methods and security impacts. But it focusses somewhat narrowly on C/C++, and a few CWE types and impact categories.\n\nMinor comments:\n\n- Figure 3 is not referenced at all, making definitions 1-3 hard to understand.\n- I could not find statistics of valid PoCs such as lines of code. This would help in getting a sense of how complex PoCs your framework can generate.\n- A start of Section 3, you say 204 task instances, but Table 1 says 203."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4kH6Tub57T", "forum": "V4YGa3V2QO", "replyto": "V4YGa3V2QO", "signatures": ["ICLR.cc/2026/Conference/Submission9060/Reviewer_TATa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9060/Reviewer_TATa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762107469108, "cdate": 1762107469108, "tmdate": 1762920768364, "mdate": 1762920768364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}