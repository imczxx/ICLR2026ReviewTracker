{"id": "tIa69ILtVq", "number": 14736, "cdate": 1758242746901, "mdate": 1759897352166, "content": {"title": "Runtime Safety through Adaptive Shielding: From Hidden Parameter Inference to Provable Guarantees", "abstract": "Unseen shifts in environment dynamics, driven by hidden parameters such as friction or gravity, can trigger safety risks during deployment. We develop a runtime shielding mechanism for reinforcement learning, building on the formalism of constrained hidden-parameter Markov decision processes. Function encoders enable real-time inference of hidden parameters from observations, allowing the shield and the underlying policy to adapt online. To further promote safe policy learning, we introduce a safety-regularized objective that augments reward maximization with a bounded safety measure. This objective encourages the selection of actions that minimize long-term safety violations. The shield constrains the action space by forecasting future safety risks (such as obstacle proximity) and accounts for uncertainty via conformal prediction. We prove that the proposed mechanism satisfies probabilistic safety guarantees and yields optimal policies within safety-compliant policies. Experiments across diverse environments with varying hidden parameters show that our approach reduces safety violations while maintaining effective task-solving performance, and achieving robust out-of-distribution generalization.", "tldr": "", "keywords": ["Safe Reinforcement Learning", "Adaptive Shielding", "Hidden Parameters", "Constrained Markov Decision Process", "Conformal Prediction"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8d99584b89846fb82fda5da20cd69cd6137c1351.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper develops a generalized safe RL framework. At the core, authors leverages function encoders to encode the set of possible dynamic models for both safety integrated policy optimization and online conformal prediction based deployment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper in general is well-written with clear structure, novelty in online action shielding mechanism, and comprehensive experiments. Reviewer will consider raising score if below questions and concerns are addressed."}, "weaknesses": {"value": "1, Following two papers[1,2] are missing in the literature surveys, which also introduce safety guarantee during optimization process. \\\n2, The theoretical contribution of Proposition 1 is bit vague and trivial since the optimization happens with the assumption that within the zero-violation policy space? Is it possible to come up with some stronger theoretical results like convergence with newly defined safety-augmented Q function? Or performance bound/equivalence with CMDP approach like CPO[3], etc. ? And how does the safety regularized TRPO mentioned in the Appendix B related with the safety augmented optimization approach mentioned in the paper, particularly where is the KL policy constrained terms come from?\\\n3, The motivation of using function encoder as dynamic representation is not explained clearly in section 4.2 and introduction. Why other options such as latent state space model, ensemble models, etc. cannot work?\\\n4, Section 4.1 and 4.2 need more clarification for the whole RL training paradigm. \\\n5, The results in figure 1 do not support authors' argument that \"SRO or Shield-only\" approaches are less effective than the combined method.\\\n6, Reviewer would love to see using other dynamic model instead of function encoder in the ablation study section. Since in the shielding part function encoder seems to only be used to infer next state with lipschitz continuous assumptions, which are pretty common in other dynamic models. \n \n\n \n\n[1]: Choi, Jason, et al. \"Reinforcement learning for safety-critical control under model uncertainty, using control lyapunov functions and control barrier functions.\" arXiv preprint arXiv:2004.07584 (2020).\\\n[2]: Wang, Yixuan, et al. \"Enforcing hard constraints with soft barriers: Safe reinforcement learning in unknown stochastic environments.\" International Conference on Machine Learning. PMLR, 2023.\\\n[3]:Achiam, Joshua, et al. \"Constrained policy optimization.\" International conference on machine learning. PMLR, 2017.\\"}, "questions": {"value": "1, In line 243, how is $b_i$ inferred from the transition samples? \\\n2, In section 4.2, is it trained on-policy or off-policy with replay buffer? Where is the observed transition samples come from? At which stage of the training loop the underlying dynamics are trained? \\\n3, In line 257, what is the $\\nu$ function here? And is the lipschitz constant provided?\\\n4, Thm 1 seems to build on optimal policy regarding $J_R$ instead of $J_{aug}$?\\\n5, How is the ood environments implemented under the safety gym environments? By using different obstacles?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v2ouWt2USk", "forum": "tIa69ILtVq", "replyto": "tIa69ILtVq", "signatures": ["ICLR.cc/2026/Conference/Submission14736/Reviewer_Wk9s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14736/Reviewer_Wk9s"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761511908499, "cdate": 1761511908499, "tmdate": 1762925096161, "mdate": 1762925096161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an adaptive runtime safety framework for reinforcement learning under hidden environment parameters. The approach combines a safety-regularized CMDP objective, a function encoder that infers latent dynamics online, and an adaptive conformal prediction–based shield providing probabilistic safety guarantees. Experiments on Safe-Gym tasks demonstrate reduced safety violations and reasonable transfer to unseen dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The framework is clearly formulated with theoretical analysis.\n- The empirical results are consistent with the theoretical claims."}, "weaknesses": {"value": "- The overall contribution feels incremental, which is a straightforward combination of several techniques, CMDP training, latent parameter inference, and conformal shielding. The theoretical guarantees follow directly from existing conformal prediction results and standard Lipschitz assumptions, offering limited new insight.\n\n- The related work omits an important branch of control-theoretic safety research, such as control barrier function (CBF)–based safe RL, Lyapunov-based safe control, and Hamilton–Jacobi–Bellman (HJB) reachability methods. These frameworks also provide runtime safety filtering or invariant-set guarantees and are widely recognized in both ML and control literature [1,2,3]. A discussion contrasting these methods with the proposed statistical shielding would clarify the distinct contribution.\n\n- Only a small subset of Safety-Gym tasks (Point, Car, Button, Push) is considered. Recent safe-RL works typically evaluate on a wider variety of environments—including Inverted Pendulum, HalfCheetah-Safe, Hopper-Safe, and Walker2d-Safe—to test robustness under dynamic instability. As such, the empirical section feels limited in demonstrating scalability or generality.\n\n[1] Cheng, Yikun, Pan Zhao, and Naira Hovakimyan. \"Safe and efficient reinforcement learning using disturbance-observer-based control barrier functions.\" Learning for Dynamics and Control Conference. PMLR, 2023.\n\n[2] Ganai, Milan, et al. \"Iterative reachability estimation for safe reinforcement learning.\" Advances in Neural Information Processing Systems 36 (2023): 69764-69797.\n\n[3] Wang, Yixuan, et al. \"Enforcing hard constraints with soft barriers: Safe reinforcement learning in unknown stochastic environments.\" International Conference on Machine Learning. PMLR, 2023."}, "questions": {"value": "1. How is conformal calibration maintained over long episodes when hidden parameters drift gradually?\n\n2. Could the authors discuss connections to control-theoretic runtime safety frameworks (e.g., barrier or reachability-based methods)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U9iIbwJ9id", "forum": "tIa69ILtVq", "replyto": "tIa69ILtVq", "signatures": ["ICLR.cc/2026/Conference/Submission14736/Reviewer_S71N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14736/Reviewer_S71N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774008320, "cdate": 1761774008320, "tmdate": 1762925095650, "mdate": 1762925095650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents safety-augmented learning objective for RL agents in constrained MDP settings. It tackles an even more challenging problem of MDPs with fully unknown, varying/nonstationary dynamics. The paper leverages notions of linear combination of neural network basis functions, conformal prediction to handle uncertainty and generate uncertainty-aware actions, and a shield to ensure the selection of safe actions at run-time."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clear, concisely written and well organized. It combines related or disparate ideas (e.g. function encoders with zero-shot capability, adaptive CP, and \"standard-looking\" optimization objectives) into safe RL. The concepts are grounded in theory and then come with promising empirical results. In terms of empirical results, I appreciate the fact that the proposed method is handicapped vis a vis baseline methods and access to hidden dynamics information."}, "weaknesses": {"value": "The weaknesses are not technical. The main weakness is around algorithmic descriptions or approaches. The paper does well to describe SRO and Shielding, but then it is unclear how these are implemented in the results section. For example, one could imagine various policy gradient methods with SRO; but which one is used here? There are more details in the appendix but I believe this could be made more clear in the main body. Likewise with the shielding; which policy (what objective, what algorithmic framework, what networks, etc) is/are used to generate actions that are then shielded?\n\nThere is one editorial suggestion that will be rolled into a question below regarding function encoders, which is of central importance to the paper (or perhaps does not have to be, given that the main contribution is SRO and shielding, but this is somewhat obfuscated by the repeated mentioned of function encoders).\n\nThe discussion around equation (8) appears to be missing some details, for example telling the reader that the Cost function uses an indicator function, the definition of $L_\\nu$ and $\\Delta_{max}$. Also, the descriptions of $e(), E$ are hand-wavey.\n\nThis is putting way too fine a point on things, but the set definition in Proposition 1 refers to something that is technically not defined (I think?). The paper defines $J_R(\\pi)$ earlier but not $J_C(\\pi)$. In fact the augmented (safety-regularized objective) is formed as a combination of Q values, not a combination of objectives (or \"J\" functions). Again, not a huge deal but just noting this for posterity."}, "questions": {"value": "1. A conceptual question and an editorial suggestion rolled into one. The notion of inferring hidden parameters online, and the solution of using function encoders, is quite compelling. First, the question: is the number of basis functions / tunable parameters fixed a priori? And how are the basis functions themselves trained, i.e. the $g_j(x)$'s? My assumption is that these are trained offline in a supervised learning style setting, and then the $b_j$ are updated online. If this is the case, is there a concern about the policy (or policies) used to generate the training data? Related to this -- isn't the zero-shot performance still limited to (or by) whatever experience and performance the neural basis functions have?\nEditorial suggestion: perhaps the authors could expand on the intuition behind this and/or technical details to make this more self-contained...maybe a 1-pager in the appendix? I realize there is literature on this and it is cited well in the paper; but it is so central to the paper that it might deserve further treatment.\n\n2. The intuition behind upper and lower bounds of $Q^\\pi_{safe}$ is interesting and helpful, and this is a very nice metric. Is there concern about the second scenario, i.e. exploration? It makes sense mathematically and even intuitively, but it does not necessarily make sense in terms of safety. Yes, the likelihood of taking such an action is low, but conditioned on such a low probability action, this measure does not (cannot?) tell us anything about the safety of such an action.\n\n3. The design choice in equation (11) is interesting. Why this choice instead of just picking the action with the highest safety score? Or, why not use a different distribution than uniform and bias the sampling towards higher-safety actions? (e.g. using softmax over safe actions or something)\n\n4. After reading further, I believe the descriptions of \"Baselines\" (line 344 or so) and the bulleted list under it is slightly misleading. Yes, there is a comparison with six baselines, but is it not an assessment of these baselines with and without the proposed approach? That is, it's not like the paper's \"Method XYZ\" is compared directly to Methods 1 through 6, which is the case with many other papers. If my understanding is correct, this can be fixed with a slight modification of that text on line 344. To hopefully make my source of confusion more explicit, a bit later the text states, \"When evaluating our approach on top of each base algorithm\". Are you adding the SRO objective (or Shield) on top of all these baselines? My confusion is amplified by the results. SRO is standalone, Shield is standalone, and then there is SRO+Shield. But what is the actual algorithm at work in these? For example, PPO has an algorithmic framework, and then one can describe the models/architectures used to represent policy networks, critics, how they are trained in sequence or in parallel and with what data, and so forth. I am perhaps not making myself clear, but what is \"underneath the hood\" of SRO and Shield, how are they trained, etc?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5WD8d8Kuk4", "forum": "tIa69ILtVq", "replyto": "tIa69ILtVq", "signatures": ["ICLR.cc/2026/Conference/Submission14736/Reviewer_xCET"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14736/Reviewer_xCET"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939235920, "cdate": 1761939235920, "tmdate": 1762925095019, "mdate": 1762925095019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}