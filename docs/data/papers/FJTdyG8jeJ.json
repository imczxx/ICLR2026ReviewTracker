{"id": "FJTdyG8jeJ", "number": 5764, "cdate": 1757933122962, "mdate": 1759897955784, "content": {"title": "A Statistical Learning Perspective on Semi-dual Adversarial Neural Optimal Transport Solvers", "abstract": "Neural network-based optimal transport (OT) is a recent and fruitful direction in the generative modeling community. It finds its applications in various fields such as domain translation, image super-resolution, computational biology and others. Among the existing OT approaches, of considerable interest are adversarial minimax solvers based on semi-dual formulations of OT problems. While promising, these methods lack theoretical investigation from a statistical learning perspective. Our work fills this gap by establishing upper bounds on the generalization error of an approximate OT map recovered by the minimax quadratic OT solver. Importantly, the bounds we derive depend solely on some standard statistical and mathematical properties of the considered functional classes (neural nets). While our analysis focuses on the quadratic OT, we believe that similar bounds could be derived for general OT case, paving the promising direction for future research.", "tldr": "Statistical generalization bounds for (semi-dual) quadratic Neural Optimal Transport solvers", "keywords": ["optimal transport", "semi-dual optimal transport", "statistical learning theory", "approximation bounds"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bf5b51e4bb251d771d4605fefa8c0094c44a2ae1.pdf", "supplementary_material": "/attachment/6c7a9f074771df7579f461a86a560c4b5ffd03db.zip"}, "replies": [{"content": {"summary": {"value": "The authors study a particular semi-dual formulation of the quadratic Optimal Transport (OT) problem and derive statistical guarantee bounds for recovering the true OT map when using a solver that optimizes this formulation. More precisely, they adopt a generative modeling framework and assume that the input and target distributions are absolutely continuous probability measures given through samples. They consider the case where the solver relies on two neural networks optimized using the semi-dual objective, which is approximated via Monte Carlo estimation based on the empirical distributions of the measures. The authors provide an upper bound on the L2 distance between the true OT map and the best transport map that could be obtained by the considered solver. To achieve this, they first decompose this gap, referred to as the generalization error in the paper, into two terms: the estimation error, which accounts for the discretization of the semi-dual formulation, and the approximation error, which arises because the optimization is performed over a restricted class of neural networks rather than the entire space of functions. They bound the first term using the Rademacher complexity of the function class and show that the second term can be made arbitrarily small by choosing an appropriate class of neural networks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The theoretical results presented in this paper are interesting: the authors derive an upper bound on the number of samples required to approximate the true OT map using the proposed solver. Section 4, which focuses on deriving this upper bound, is well organized and clearly written, which permits to easily follow the main steps of the derivation."}, "weaknesses": {"value": "- I am a bit perplexed by the experimental setup. In Section 5.1, the authors assume that, because they use the same neural network architecture as the one corresponding to the ground-truth OT map, there is no gap in the approximation error, and thus the observed generalization error corresponds solely to the estimation error. This appears to be a strong assumption, as it implies that the optimization process finds the global minimum of the loss defined in Equation (8) and does not get trapped in local minima when training the ICNN.\n\n- Regarding the writing, Sections 2 and 3 could be improved. The theoretical background on OT with general cost functions is not necessary to understand the paper’s contributions. The authors should focus exclusively on the quadratic cost in Section 2. Similarly, the paragraph “Continuous OT solvers for quadratic cost” does not seem essential for understanding the main contributions.\n\n- Finally, the writing should be polished and carefully revised for clarity and precision (see, for example, lines 352–353, 405–406, and 410–411)."}, "questions": {"value": "- Does the class of neural networks described in Proposition 4.4 represent any $\\beta$-strongly convex function that is totally bounded with respect to the Lipschitz norm?\n\n- What does assuming that the optimal potential $\\phi^*$ is $\\beta$-strongly convex imply about the considered distributions $p$ and $q$ ? \n\n- In Section 5.1, you assume that, because you use the same neural network architecture as the one corresponding to the ground-truth OT map, there is no gap in the approximation error, and that the observed generalization error is, in fact, the estimation error. This seems to be a strong assumption, as it implies that the optimization process reaches the global minimum of the loss defined in Equation (8) and does not get trapped in local minima when optimizing the ICNN (a similar comment applies to Section 5.2). Could you comment on this? Do you observe that, for different initializations of the ICNN, the optimization consistently recovers approximately the same transport map? Or, did you try initializing the neural network with several random seeds and retaining the best result?\n\n- Your experiment in Section 5.1 seems ideal, as it suggests that the bound you derive is tight in dimension 4—is that correct? I am not a specialist in statistical bounds, but is it common to verify experimentally that such bounds are tight as you did? Could you elaborate on this?\n\n- During the experiments, did you constrain the ICNN as described in Proposition 4.4? Do you observe that imposing these constraints helps the optimization in practice?\n\n- Finally, there is a small typo: it should be $\\varepsilon / \\beta$ in Equation (20), although this does not affect the result."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no concerns"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5srI3gN77W", "forum": "FJTdyG8jeJ", "replyto": "FJTdyG8jeJ", "signatures": ["ICLR.cc/2026/Conference/Submission5764/Reviewer_1LsA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5764/Reviewer_1LsA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952696171, "cdate": 1761952696171, "tmdate": 1762918246426, "mdate": 1762918246426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a theoretical characterization of the generalization error for optimal transport mappings parameterized by input-convex neural networks, focusing on a class of continuous Wasserstein-2 distance solvers. The analysis decomposes the overall error into two components, estimation error and approximation error, each of which is independently derived and subsequently combined to yield the final generalization bound. The theoretical findings are supported by experimental validation on a simple, low-dimensional synthetic dataset, which confirm the consistency of the results with the proposed theoretical framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes an original theoretical contribution by deriving generalization error bounds for quadratic optimal transport (OT) solvers parameterized by neural networks, particularly within the class of input-convex models. This work provides valuable insights into the learnability and error behavior of neural OT mappings and offers practical guidance for their use in learning-based transport problems. The theoretical framework is rigorous, decomposing the overall generalization error into estimation and approximation components with clear mathematical justification.\n\nThe paper is clearly written, logically structured, and easy to follow, even when presenting technically complex material. Although the experiments are limited to simple synthetic datasets, they effectively validate the theoretical results. The significance of this work lies in establishing foundational learnability guarantees for neural OT solvers, which bridge the gap between theoretical understanding and practical implementation in modern OT-based learning frameworks."}, "weaknesses": {"value": "A key limitation of the paper is its exclusive reliance on low-dimensional synthetic datasets for empirical validation. While the theoretical analysis is sound, the absence of experiments on higher-dimensional or real-world datasets limits the assessment of the framework’s practical relevance and robustness. Extending the experiments to more complex domains would provide stronger empirical support for the theoretical claims and demonstrate the scalability of the proposed bounds.\n\nAdditionally, the paper assumes \\beta-strong convexity, a condition that is often difficult to guarantee in neural network parameterizations. While the authors acknowledge this assumption as restrictive, a deeper discussion of its implications and potential violations would be valuable.\n\nSome missing references, e.g. [proof ref.] in multiple theorems and propositions."}, "questions": {"value": "In Figure 3, the slope appears to increase as the dimensionality rises. Could the authors provide further insights into the underlying reasons for this behavior? Additionally, as the dimensionality continues to grow, do the authors expect the slope to remain below or equal to 0.5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pNSVbbSWoL", "forum": "FJTdyG8jeJ", "replyto": "FJTdyG8jeJ", "signatures": ["ICLR.cc/2026/Conference/Submission5764/Reviewer_aRUC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5764/Reviewer_aRUC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762180604536, "cdate": 1762180604536, "tmdate": 1762918245779, "mdate": 1762918245779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the approximation capabilities of a certain class of optimal transport problems for quadratic cost.  In particular, it focuses on the estimation of the *generalization* error: this stem from several factors, the approximation error (due to the dual potential being parametrized with a neural network) and the estimation error (stemming from finite sample).\n\nThis family of problems (and their solutions) is well studied, but the case of semi-dual solvers is less studied. Here, the semi-dual formulation allows to parametrize a potential using a convex class of functions, the ICNN. \n\nThe experimental part of the paper study a particular implementation of the approach using Input Convex Neural Network (ICNN).\n\nI did not check the proofs in appendix"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written, very clear, and accessible to people with familiarity in optimal transportation, learning theory and statistical consistency. Overall, the strategy to tackle the problem is well explained.  \n\nThe related work is well done, especially on semi-dual formulations, which are a getting popular, making the contribution timely.  \n\nThe methodology is standard, but the result is new. Only a subset of results actually depend on the approximation property of certain neural networks classes. Others are a classical decomposition of the error between generalization and approximation.\n\nWe can appreciate the diversity of tools and techniques used."}, "weaknesses": {"value": "### Missing related work\n\nThe recent work of Nietert and Goldfeld is relevant and could be added to the related work.\n\nNietert, S. and Goldfeld, Z., *Estimation of Stochastic Optimal Transport Maps*. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025.\n\n### Somewhat unconvincing experiments\n\nMy enthusiasm for the paper is somewhat tempered by the numerical results. \n\n**In Figure 3**, the logarithmic scale hides many phenomena. For example, I'd be curious to see what the experimental error looks like with the naive barycenter translation estimator: $T(x)=x+(\\mathbb{E}_Q[x]-\\mathbb{E}_P[x])$.\n\nI have similar concerns with Figure 4 (see questions).  \n\nOverall, since some experiments do not depend on neural networks, and since ICNN are notably hard to train, it could be good to plot results of other (possibly non-neural) parametrizations."}, "questions": {"value": "### Approximation power as function of depth\n\n**In Figure 4, Dim 2**, I spot a counter-intuitive behaviors. I see that the approximation error does not diminish with $\\max H_{\\phi}$. Do you know why? This somewhat contradicts Theorem 4.3.  \n\nCan you clarify what is the **x-axis**: why does it cover the 0-1 range? \n\nFinally, we sometimes see the error accumulating on special values (e.g Dim 2 maximum width, or Dim 4 small width). Can you double check that the network is not degenerating toward a trivial solution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pLcQzgw5tE", "forum": "FJTdyG8jeJ", "replyto": "FJTdyG8jeJ", "signatures": ["ICLR.cc/2026/Conference/Submission5764/Reviewer_BuZV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5764/Reviewer_BuZV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762294058567, "cdate": 1762294058567, "tmdate": 1762918245398, "mdate": 1762918245398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper develops a statistical learning framework for the proposed method, stating finite sample guarantees and excess risk bounds for the estimator. It highlights how regularity and beta-strong convexity assumptions drive stability and convergence rates.\nThe analysis connects optimization error and estimation error, with explicit dependence on sample size and ambient dimension. The results are positioned relative to statistical optimal transport, where dimension often controls rates."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Theoretical framing is clear, separating approximation, optimization, and estimation errors. The proofs use standard tools from empirical process theory and stability."}, "weaknesses": {"value": "The beta strong convexity condition is assumed rather than derived from the model or data-generating process. Please either prove it under verifiable conditions or provide a constructive check that practitioners can apply to certify it."}, "questions": {"value": "1. Under what concrete distributional conditions on the source and target does beta strong convexity hold? \n2. The estimation error exhibits explicit dimensional dependence that resembles known lower bounds in statistical optimal transport. Can the authors clarify whether structures such as low intrinsic dimension, manifold support, or spectral decay can reduce the exponent, and if so, how this would change the bounds and constants?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mu4jtWkgNw", "forum": "FJTdyG8jeJ", "replyto": "FJTdyG8jeJ", "signatures": ["ICLR.cc/2026/Conference/Submission5764/Reviewer_umCk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5764/Reviewer_umCk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762349947205, "cdate": 1762349947205, "tmdate": 1762918244827, "mdate": 1762918244827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}