{"id": "sL0NpgJRMs", "number": 6912, "cdate": 1758001351768, "mdate": 1762968589995, "content": {"title": "Advancing Sparse Attention in Spiking Neural Networks via Spike-Timing-Based Prioritization", "abstract": "Current Spiking Neural Networks (SNNs) underutilize the temporal dynamics inherent in spike-based processing, relying primarily on rate coding while overlooking precise timing information that provides rich computational cues. We address this by proposing \\textbf{SPARTA} (Spiking Priority Attention with Resource-Adaptive Temporal Allocation), which leverages heterogeneous neuron dynamics and spike-timing information to enable sparse attention mechanisms. SPARTA extracts temporal cues—including firing patterns, spike timing, and inter-spike intervals—to prioritize tokens for processing, achieving 65.4% sparsity through competitive gating. By selecting only the most salient tokens, SPARTA reduces attention complexity from (O(N^{2})) to (O(K^{2})), where (k!\\ll!n). Our approach achieves state-of-the-art accuracy on DVS-Gesture (98.78%) and competitive performance on CIFAR10-DVS (83.06%) and CIFAR-10 (95.3%), demonstrating that spike-timing utilization enables both computational efficiency and competitive accuracy.", "tldr": "SPARTA leverages biologically-inspired spike timing cues (firing rate, spike timing, intervals) to enable sparse attention in spiking transformers, achieving 65.4% sparsity and competitive accuracy while preserving neuromorphic principles.", "keywords": ["Spiking Neural Networks", "Neuromorphic Computing", "Transformer", "Energy Efficiency", "Sparse Attention", "Temporal Coding", "Event-based Vision"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/ff5db468324d52592abfa493d0d5f0a8559e7784.pdf", "supplementary_material": "/attachment/6f17246273919c3d333d7d4a8c4fae9713ad6024.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposed the Spiking Priority Attention with Resource Adaptive Temporal Allocation (SPARTA) to enable sparse attention mechanisms."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Clear motivation, but I think the paper would benefit from significant revision."}, "weaknesses": {"value": "The authors adopted a notably complex method to assess the temporal characteristics of spike trains, followed by another intricate approach for token selection. Based on the description in the Methods section and the provided code, it seems that the proposed “token selection” procedure is computationally more expensive than directly performing the spike calculation itself. However, the authors only take the sparsity/flops of the model as the effectiveness metric of their proposed method. \n\nWhile authors claim that SPARTA achieves better performance due to enhanced temporal diversity, results show that accuracy decreases with larger models and longer simulation steps. Furthermore, although the paper focuses on sparse attention, all evaluations are conducted on relatively small datasets.  This raises concerns about the generalizability of the proposed approach. It is also unclear why the authors characterize their model as “state-of-the-art” despite its inferior results on CIFAR10-DVS, CIFAR10, and CIFAR100 benchmarks.\n\nWhile “energy efficiency” and “sparsity” are emphasized as the core advantages of this work, Appendix A.3 shows that the proposed model exhibits substantially higher energy consumption compared with recent approaches evaluated on ImageNet [1]. In addition, many of the figures are difficult to interpret and do not meet the presentation quality expected for ICLR submissions. \n\nDue to the above concerns, I do not recommend acceptance at this stage.\n\n[1] Spiking Transformers Need High-Frequency Information. Neurips 2025."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qjFv2ZDbOs", "forum": "sL0NpgJRMs", "replyto": "sL0NpgJRMs", "signatures": ["ICLR.cc/2026/Conference/Submission6912/Reviewer_Gv7r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6912/Reviewer_Gv7r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761477026071, "cdate": 1761477026071, "tmdate": 1762919152549, "mdate": 1762919152549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "42Lv5oZmrP", "forum": "sL0NpgJRMs", "replyto": "sL0NpgJRMs", "signatures": ["ICLR.cc/2026/Conference/Submission6912/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6912/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762968589217, "cdate": 1762968589217, "tmdate": 1762968589217, "mdate": 1762968589217, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SPARTA, a biologically-inspired sparse attention framework for SNNs. It leverages precise spike-timing cues (first-spike time, inter-spike interval, and firing rate) to prioritize and selectively process tokens, aiming to improve both accuracy and efficiency. The method also incorporates heterogeneous neurons (HI-LIF) to enhance temporal feature representation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The core idea of using precise spike-timing information, beyond just firing rates, to guide the attention mechanism in SNNs is highly original and significant. It aligns well with the principles of neuromorphic computing and opens a new avenue for designing more brain-like attention.\n\n2. The method achieves state-of-the-art accuracy on the DVS-Gesture dataset, demonstrating that the proposed timing-based prioritization can be very effective for tasks with rich and distinct temporal dynamics."}, "weaknesses": {"value": "1. The proposed SPARTA framework is exceedingly complex. It introduces multiple specialized modules (STEN, MSP, STSG, SC) and a cascade of processing steps simply to select which tokens to attend to. The energy breakdown in the appendix reveals that the vast majority of energy (83.7%) is consumed by the front-end STEN module, suggesting that the feature extraction process itself is a major bottleneck. This complexity makes the model difficult to reproduce, analyze, and scale, and it goes against the goal of designing simple and efficient neural architectures. The lack of crucial metrics like inference latency further obscures a true assessment of its practical efficiency.\n\n2. Despite its focus on sparsity, the model's energy consumption is not low in absolute terms. The reported energy on DVS-Gesture is higher than recent SOTA SNN Transformers (e.g., Spiking Wavelet Transformer), which achieve better performance on more difficult tasks with only ~3.5 mJ. The paper's claim of \"computational efficiency\" is therefore misleading. While it improves upon some older models, it is not competitive on the current state-of-the-art efficiency frontier. \n\n3. The paper's experimental results are weak. Most importantly, it was not tested on ImageNet. On top of that, its performance on the smaller CIFAR datasets is not very good. It gets 95.3% on CIFAR-10, which is lower than other recent SNNs that also use less energy."}, "questions": {"value": "1. Can you provide the performance results for SPARTA on the ImageNet dataset? \n\n2. Could you provide a direct comparison and explain how SPARTA remains competitive in terms of the accuracy-efficiency trade-off against such state-of-the-art efficient spiking transformer architectures on the ImageNet dataset?\n\n3. To fully evaluate the practical efficiency of your method, could you report the inference latency and compare it to other baseline models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vlVd5JBVA3", "forum": "sL0NpgJRMs", "replyto": "sL0NpgJRMs", "signatures": ["ICLR.cc/2026/Conference/Submission6912/Reviewer_Rywp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6912/Reviewer_Rywp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761518593071, "cdate": 1761518593071, "tmdate": 1762919152176, "mdate": 1762919152176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the underutilization of precise temporal dynamics in Spiking Neural Networks (SNNs), which often rely primarily on rate coding. It proposes SPARTA (Spiking Priority Attention with Resource-Adaptive Temporal Allocation), a novel framework that leverages spike-timing information to enable an efficient sparse attention mechanism"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper clearly identifies a key limitation in the SNN field: an over-reliance on rate-based coding that overlooks rich temporal information. It proposes a novel and well-justified method that uses multiple bio-inspired temporal cues (e.g., first-spike timing, inter-spike intervals, and firing frequency) to guide a sparse attention mechanism."}, "weaknesses": {"value": "1. The paper's validation is limited to smaller-scale datasets (DVS-Gesture, CIFAR-10/100). It lacks experiments on large-scale benchmarks such as ImageNet, which is a standard requirement for validating the scalability and generalization capabilities of a novel architecture in the broader deep learning field.\n2. Insufficient Comparison to SOTA SNN Encoding Methods: The related work section and SOTA comparison (Table 5)  are incomplete. The paper fails to discuss or benchmark against other relevant and advanced SNN strategies, such as state-of-the-art \"direct coding\" SNNs [1] or \"GAC-SNN\" [2]  encoding methods. This omission makes it difficult to assess the true novelty and advantage of SPARTA compared to other advanced techniques addressing SNN efficiency and encoding.\n\n[1] Direct training for spiking neural networks: Faster, larger, better. AAAI 2021\n[2] Gated attention coding for training high-performance and efficient spiking neural networks. AAAI 2024"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lcyzrR8yQY", "forum": "sL0NpgJRMs", "replyto": "sL0NpgJRMs", "signatures": ["ICLR.cc/2026/Conference/Submission6912/Reviewer_kHbY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6912/Reviewer_kHbY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761672605873, "cdate": 1761672605873, "tmdate": 1762919151711, "mdate": 1762919151711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SPARTA (Spiking Priority Attention with Resource-Adaptive Temporal Allocation), a framework that leverages spike-timing information to enable sparse attention mechanisms in Spiking Neural Networks (SNNs). The key contributions include HI-LIF ( Heterogeneous Initialized Leaky Integrate-and-Fire) neurons, Spatio-Temporal Encoding Network (STEN) that extracts three temporal cues,  Sparse attention (from $O(N^2)$ to $O(K^2)$ by dynamically selecting top-K salient tokens based on biologically-inspired temporal priorities."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Biological motivation\n- The integration of three complementary temporal cues (early firing, short inter-spike intervals, high firing rate) is well-grounded in neuroscience literature and provides a principled approach to token prioritization.\n\n2. Comprehensive experimental analysis\n- Ablation studies on each component (HI-LIF heterogeneity, temporal weighting parameters)\n- Analysis across different temporal resolutions\n- Comparison of granularity levels (layer-wise, channel-wise, neuron-wise)\n\n3. Temporal variance analysis\n- The paper verifies that SPARTA maintains higher temporal diversity compared to other SNN architectures, validating the effectiveness of HI-LIF neurons."}, "weaknesses": {"value": "1. Hard to read\n- I couldn't follow the methodology section in conjunction with Figure 2. The figure appears disorganized, with substantial misalignment between the textual descriptions and visual representation. For example, in Section 4.2.1 (STEN), the authors mention 1×1 convolution and adaptive pooling, but these components are not identifiable in the figure. Additionally, it is unclear whether Section 4.2.3 (Patch Grouping) corresponds to STSG + SC or represents a separate component.\n\n\n2. Figure 1\n- Figure 1 does not appear to convey any core ideas or contributions essential to this paper. I cannot identify any empirical or theoretical results that demonstrate a meaningful correlation with the crossword puzzle analogy presented in Figure 1.\n\n\n3. Scalability & Limited dataset\n- This paper only experiments on small-scale datasets (CIFAR-10/100, DVS-Gesture) with a relatively small architecture (13.8M parameters). Current state-of-the-art Spiking Transformer works demonstrate scalability on large-scale datasets, with ImageNet serving as the minimum benchmark requirement.\n\n\n4. Energy Consumption\n- In the Appendix, the authors calculate energy consumption using $E_{AC}$ for all layers except the first convolutional layer. However, in Figure 2's Sparse Attention Layer, the inputs to linear layers and other modules do not appear to be binarized values. In this case, energy consumption should be calculated using $E_{MAC}$."}, "questions": {"value": "1. HI-LIF\n\n- Is there any computational or memory overhead introduced by using channel-wise $\\tau$ and $v_{th}$​? Additionally, regarding Table 2, I am somewhat confused about the channel-wise $\\tau$ and $v_{th}$​ configuration. Does the \"Adjusted (Combined)\" setting use constant values of 0.3 and 0.2 for $\\tau$ and $v_{th}$, respectively?\n\n2. Sparsity\n\n- The authors claim that SPARTA achieves high sparsity at 65.4%. However, to my knowledge, many existing spiking architectures, including spiking transformers, typically achieve sparsity levels in the range of 60-80%. Could you clarify what makes your sparsity achievement particularly significant or different from these existing methods?\n\n\n3. Computational overhead\n\n- Your architecture incorporates many additional modules compared to other spiking transformers. Could you provide quantitative comparisons of training time and inference overhead relative to baseline architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6A5ew3M0na", "forum": "sL0NpgJRMs", "replyto": "sL0NpgJRMs", "signatures": ["ICLR.cc/2026/Conference/Submission6912/Reviewer_Tvtr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6912/Reviewer_Tvtr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938336866, "cdate": 1761938336866, "tmdate": 1762919151195, "mdate": 1762919151195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}