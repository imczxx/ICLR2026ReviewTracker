{"id": "31CznLfRIS", "number": 22397, "cdate": 1758330542977, "mdate": 1759896868525, "content": {"title": "VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding", "abstract": "Precisely evaluating video understanding models remains challenging: commonly used metrics such as BLEU, ROUGE, and BERTScore fail to capture the fineness of human judgment, while obtaining such judgments through manual evaluation is costly. Recent work has explored using large language models (LLMs) or multimodal LLMs (MLLMs) as evaluators, but their extension to video understanding remains relatively unexplored. In this work, we introduce VideoJudge, a 3B and 7B-sized MLLM judge specialized to evaluate outputs from video understanding models (\\textit{i.e.}, text responses conditioned on videos). To train VideoJudge, our recipe builds on the interplay between a generator and an evaluator: the generator is prompted to produce responses conditioned on a target rating, and responses not matching the evaluator's rating are discarded. Across three out of four meta-evaluation benchmarks, VideoJudge-7B outperforms larger MLLM judge baselines such as Qwen2.5-VL-32B and Qwen2.5-VL-72B. Notably, we find that LLM judges (Qwen3) models perform worse than MLLM judges (Qwen2.5-VL) and that long chain-of-thought reasoning does not improve performance, indicating that providing video inputs is crucial for evaluation of video understanding tasks.", "tldr": "Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding", "keywords": ["Meta-evaluation", "llm-as-judge", "synthetic data", "self-refinement", "video understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b019901661df2266646c9ff31a18fc3bda34536.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes VideoJudge, a bootstrapped framework for training multimodal large language models to serve as automatic evaluators for video understanding tasks. By generating synthetic data through an iterative generator–evaluator process, the method enables scalable supervision without human labels. The resulting small models outperform larger MLLM judges in correlation with human judgments across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe generator-evaluator cycle to produce synthetic training data for the judge is meaningful and well-motivated. It reduces reliance on human annotations and enables larger scale.\n\n2.\tThe authors show that a relatively small model (3B) can outperform much larger baselines when trained appropriately, which is a strong practical result."}, "weaknesses": {"value": "1.\tThe bootstrapping relies on LLM-based generator and evaluator and that LLM may carry bias and errors. If the evaluator is weak, the whole pipeline could propagate flawed judgments. The paper could more deeply analyze potential bias or drift in this synthetic supervised signal.\n\n2.\tWhile the benchmarks used show strong correlation results, it is not entirely clear how robust the judge will be to entirely new video domains, tasks, or instruction types. The paper would benefit from a “domain shift” evaluation (unseen video types).\n\n3.\tWhile the correlation metrics are good, more detailed breakdowns of failure cases where the judge disagrees with humans would help understand the limitations. Are there types of errors the judge misses (e.g., subtle temporal reasoning, common sense)?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CzEwT16fQo", "forum": "31CznLfRIS", "replyto": "31CznLfRIS", "signatures": ["ICLR.cc/2026/Conference/Submission22397/Reviewer_zfmP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22397/Reviewer_zfmP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761300941476, "cdate": 1761300941476, "tmdate": 1762942200178, "mdate": 1762942200178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of scalable and reliable evaluation for video understanding tasks. The authors introduce VideoJudge, a 3B and 7B MLLM-based evaluator model trained via a bootstrapped generator-evaluator pipeline, where candidate responses at different quality levels are iteratively refined and filtered before being used as supervision. The system supports both pointwise rating and pairwise comparison, and the authors also construct meta-evaluation benchmarks to verify alignment with human preferences. Experiments show that VideoJudge models achieve comparable or even superior judgment quality to significantly larger MLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Overall, the paper is generally well-organized and clearly motivates the need for scalable video evaluation beyond standard reference-based metrics.\n2. The proposed bootstrapping framework is reasonable and avoids costly manual annotation, which is an important practical benefit.\n3. Experiments include multiple settings (pointwise, pairwise) and span several baseline models, showing consistent improvements."}, "weaknesses": {"value": "1. While the paper claims this is \"the first bootstrapped framework for training scalable MLLM-based evaluators across diverse video understanding tasks\", it would benefit from clarifying how it differs from existing MLLM-as-a-Judge work that also uses distillation from other evaluators (e.g., VideoScore and Prometheus-Vision). The novelty statement could be toned down to avoid overclaim.\n2. The performance gains over baseline on multiple datasets seem to be limited. According to Table 1, VideoJudge-7B performs worse than LLaVA-NeXT-7B (which shall be considered as a weak baseline given the weaker general video understanding capabilities than Qwen-2.5-VL-7B) under several metrics, yielding concerns regarding the actual effectiveness.\n3. Besides, Table 1 should include more MLLM-as-a-Judge works to clearly identify the potential effectiveness and significance of the proposed scheme."}, "questions": {"value": "Please refer to the weakness section for my questions. My major concerns are about the performance and the very limited baselines. Strong justifications shall be provided to demonstrate the significance of this submission.\n\nBesides, the authors claim that \"we have anonymously released the model checkpoints and datasets used for training and evaluation on HuggingFace\", but the links are not found."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yPMdF8bD5D", "forum": "31CznLfRIS", "replyto": "31CznLfRIS", "signatures": ["ICLR.cc/2026/Conference/Submission22397/Reviewer_Lbto"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22397/Reviewer_Lbto"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761412380085, "cdate": 1761412380085, "tmdate": 1762942199356, "mdate": 1762942199356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VideoJudge, a framework that uses multi-language large language models (MLLMs) as judges for video understanding tasks. It employs bootstrapping to iteratively improve the model’s performance by generating scalable supervision signals."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The use of bootstrapping to create scalable supervision for MLLM judges is novel. This method provides a unique solution to the challenge of limited labeled data in video understanding tasks.\n- The bootstrapping process is well-designed, with clear iterations and refinement steps that enhance model performance.\n- The paper is well-structured, with clear explanations of the methodology, experiments, and results."}, "weaknesses": {"value": "- The resulting VideoJudge model might learn to perfectly mimic the idiosyncrasies of evaluator model rather than learning a generalized, human-aligned evaluation function. This could create an artificial performance ceiling, preventing VideoJudge from truly exceeding the quality of the initial MLLM judge.\n- These general instruction-following datasets likely share a common distribution of tasks (e.g., simple QA, short-form captioning). This raises a question about the framework's performance when applied to video tasks with fundamentally different reasoning requirements, such as long-form temporal reasoning, dense video grounding,"}, "questions": {"value": "please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "29136GIHcI", "forum": "31CznLfRIS", "replyto": "31CznLfRIS", "signatures": ["ICLR.cc/2026/Conference/Submission22397/Reviewer_PNxS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22397/Reviewer_PNxS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967732660, "cdate": 1761967732660, "tmdate": 1762942198952, "mdate": 1762942198952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of evaluating video MLLMs, noting that traditional metrics (e.g., BLEU) are insufficient and human evaluation is costly. The authors propose VideoJudge, a specialized 3B/7B MLLM designed to act as an evaluator. The core method is a \"bootstrapping\" pipeline: A \"Generator\" model creates candidate answers for 1-5 star ratings, and an \"Evaluator\" model validates if these answers match the intended rating. This bootstrapped dataset (over 100k examples) is then used to fine-tune the small VideoJudge models. The central claim is that the fine-tuned VideoJudge-7B outperforms much larger models, such as Qwen2.5-VL-32B and 72B, on several meta-evaluation benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles the critical and unsolved problem of scalable, reliable evaluation for open-ended video MLLMs.\n- Good writing and easy to read."}, "weaknesses": {"value": "- Core Logical Flaw (Circular Evaluation): The paper's primary claim—that its 7B model \"outperforms\" a 72B model —is built on a foundation of circular logic. The training data for VideoJudge is created by a \"Generator-Evaluator\" (G-E) pipeline. The key meta-evaluation benchmarks used to prove this (e.g., VideoJudgeLLaVA-MetaEval, VideoJudgeVCG-MetaEval) were created using the exact same bootstrapping pipeline. This means the 7B model (the \"student\") is fine-tuned to mimic the preferences of the G-E pipeline (the \"teacher\"). The evaluation then tests this student against the teacher (in a zero-shot setting) on a test set created by the teacher itself. The student outperforming the zero-shot teacher on the teacher's own test is not surprising. It merely proves that fine-tuning on a specific data distribution works (i.e., distillation is effective). It does not prove the 7B model is a \"better\" or more reliable judge in a general sense. \n- Given the circular evaluation, the paper requires strong validation on a fully independent, human-annotated preference benchmark. While VideoAutoArena and a small 200+ sample set (VJ-H) are used, the main conclusions from Table 1, which support the paper's primary claim, are drawn from the self-generated benchmarks."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SxXL7OCIo0", "forum": "31CznLfRIS", "replyto": "31CznLfRIS", "signatures": ["ICLR.cc/2026/Conference/Submission22397/Reviewer_Q1YP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22397/Reviewer_Q1YP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972475295, "cdate": 1761972475295, "tmdate": 1762942198468, "mdate": 1762942198468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}