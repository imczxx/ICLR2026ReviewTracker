{"id": "IyV1QEc95F", "number": 24994, "cdate": 1758362907580, "mdate": 1759896738916, "content": {"title": "Model-Aware Tokenizer Transfer", "abstract": "Large Language Models (LLMs) are trained to support an increasing number of languages, yet their predefined tokenizers remain a bottleneck for adapting models to lower-resource or distinct-script languages. Existing tokenizer transfer methods typically rely on semantic heuristics to initialize new embeddings, ignoring higher-layer model dynamics and limiting transfer quality. We propose Model-Aware Tokenizer Transfer (MATT), a method that incorporates model internals into the tokenizer transfer process. MATT introduces an Attention Influence Modeling (AIM) objective that distills inter-token communication patterns from a source model into a target model with a new tokenizer, providing an efficient warm-up before standard language modeling. Unlike approaches that focus solely on embedding similarity, MATT leverages attention behavior to guide embedding initialization and adaptation. Experiments across diverse linguistic settings show that MATT recovers a large fraction of the original model’s performance within a few GPU hours, outperforming heuristic baselines. These results demonstrate that incorporating model-level signals offers a practical and effective path toward robust tokenizer transfer in multilingual LLMs.", "tldr": "This paper introduces Model-Aware Tokenizer Transfer, a method that leverages inter-token communication patterns in attention layers to efficiently adapt pretrained language models to new tokenizers and recover performance across diverse languages.", "keywords": ["Large Language Models", "Tokenizer transfer", "Embedding initialization", "Attention distillation", "Model-aware adaptation", "Multilingual NLP", "Vocabulary adaptation", "Low-resource languages", "Mid-resource languages", "Model-Aware Tokenizer Transfer", "Attention Influence Modeling", "Cross-Tokenizer Distillation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b47c846b9e3cb140ff026eb6dfcffe0937a423e9.pdf", "supplementary_material": "/attachment/a50063a233ac8f00fb4b70a0e760a29affed9cda.zip"}, "replies": [{"content": {"summary": {"value": "In this work, the authors take existings LLMs and swap their tokenizers for more efficient inference (and if desired finetuning) in a desired target language (e.g. Ukrainian). The novelty of their approach is to not only consider token embeddings on their own while reinitializing the embddings, but to also consider their perception by the model's attention layers. While considering the attention outputs does require undergoing a slight finetuning after embedding reinitialization, the efficiency of their propsed attention alignment objective is probably higher than traditional autoregressive finetuning. The authors show that their training methodology is lightweight, achieves better outcomes than embedding reinitialization alone, and bridges the gap between simpler and more advanced initializations."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Well-explained intuition and very reasonable assumptions.\n* Outstanding and well-summarized review of the state-of-the-art in tokenization trade-offs and embedding adaptation.\n* Numerous baselines and ablation studies, which bring confidence in the results, and offer many potential insights.\n* Very reasonable compute requirements for the achieved inference speed improvements achieved."}, "weaknesses": {"value": "* Unlike the selected baselines, MATT involves training time on a GPU. A fairer baseline would thus consider how much a FOCUS or Transtokenizers baseline would improve after seeing ~3h of embedding finetuning on a H100 (using a more traditional autoregressive loss). After reading the paper, I assume that the proposed approach would likely perform better at a small training regime, but it might saturate sooner as the signal is only an approximation of the real objective. This would be good to objectify.\n* Most LLMs do not use tied input/output embeddings, which reduce the effectiveness of the proposed method for these popular use cases. For the output embeddings, a more effective embedding transfer method and autoregresive finetuning might thus serve the use case better."}, "questions": {"value": "* Have you considered how performance improves after a more traditional finetuning of the model using its natural autoregressive finetuning?\n* Do you have an explanation for the reason why FOCUS and Transtokenizers perform about as well after MATT finetuning, but Transtokenizers perform much better alone? Are the correlations learnt by Transtokenizers' token alignment similar to the ones recovered by your approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pU0TruqEby", "forum": "IyV1QEc95F", "replyto": "IyV1QEc95F", "signatures": ["ICLR.cc/2026/Conference/Submission24994/Reviewer_2kWf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24994/Reviewer_2kWf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761472755814, "cdate": 1761472755814, "tmdate": 1762943276031, "mdate": 1762943276031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel method to initialize the embeddings of newly added tokens for cross-lingual transfer effectively. The proposed method aims to incorporate higher-layer model dynamics into the initialization of new embeddings by forcing the model with the target (adapted) tokenizer to generate output embeddings similar to those produced by the source tokenizer. This is achieved with the attention influence modeling objective and a relatively small amount of training. The experimental results on both Gemma3 and Qwen3 demonstrate that the proposed approach outperforms existing initialization baselines that do not go through training."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper focuses on an important problem of how to efficiently transfer vocabulary from source to target with a particular focus on vocabulary expansion settings. While investigated by many previous studies, this paper proposes a fundamentally new approach that seeks to incorporate attention-weight information between source and target tokenizers for transfer, accounting for higher-layer model dynamics. This aspect has been neglected in previous studies and can be seen as one of the major contributions of this study in terms of originality and significance.\n2. The paper is mostly well-written and easy to read up until Section 3. In particular, the intuition and motivation are pretty clear in the paper."}, "weaknesses": {"value": "1. The fundamental limitation of this work is that the method is only applicable to input embeddings. Some frontier models, like Qwen3, do not tie weights. In that case, the effectiveness of the proposed method substantially diminishes compared to models with weight tying (Table 3).\n\n2. The experiments and evaluation are neither extensive nor comprehensive. Specifically,  \n    * (i) The main adaptation experiments in 4.1 are only conducted in Ukrainian without justifications, which fails to demonstrate the applicability to different languages with different scripts.\n    * (ii) all target languages in the paper are officially supported in the tested model - Qwen3 (i.e., it lacks experiments on unseen languages or unsupported languages.) While Gemma3 does not provide a list of supported languages, it is highly likely to support the target languages considering the vocabulary size and multilingual performance reported in the technical paper. Therefore, it is not clear whether the proposed method exhibits a similar advantage of better downstream performance when applied to unseen or unsupported languages.\n    * (iii) The paper considers only English-to-target machine translation as a generation task, which lacks the diversity of evaluation.\n    * (iv) While the paper reports compression rates as the average number of characters, it fails to report the actual inference speedups (e.g., tokens/sec used in previous related papers - See Hong et al. (2024) and Yamaguchi et al. (2024, 2025). This prevents us from evaluating the actual efficiency impact of using the proposed method.\nhttps://arxiv.org/abs/2401.10660\nhttps://arxiv.org/abs/2406.11477\nhttps://arxiv.org/abs/2412.11704\n    * (v) The main experiments in 4.1 and the multilingual experiments in 4.2 use different models without justifications. Also, 4.2 lacks generation evaluation. This hinders comparison between monolingual and multilingual settings.\n    * (vi) The multilingual evaluation lacks in-depth analysis regarding observed differences between languages and models.\n\n3. The comparison against baselines seems unfair, making it impossible to understand the true gain from the proposed method. While the paper argues that evaluating only the initialized model is sufficient to compare MATT with existing baselines (L477-479), MATT indeed involves training. Given that the other baselines do not include training of the base model, the paper should continually pre-train all baselines using the same compute budget to make them comparable. Moreover, one can assume that the longer the continual pre-training, the smaller the difference between initialization methods. Therefore, it is also important to demonstrate how well the performance advantage of the proposed method holds when conducting continual pre-training.\n\n4. The discussion on the instruction-tuned model in L405-407 is entirely not reliable, given that (i) generation evaluation is conducted only on an English-to-target machine translation task and (ii) only Ukrainian is tested in the evaluation."}, "questions": {"value": "1. The attention-related definitions in Section 3 only consider a single-head scenario. Given that almost all transformer-based models use multi-head attentions, this aspect warrants clarification (at least by a footnote).\n\n2. How the paper chooses $n$ (i.e., how many layers are considered for the loss computation) is unclear in the main body of the paper. This should be clearly explained in the main text, as it is the core hyperparameter of the method.\n\n3. On Weakness 1, what happens when an LM head is tuned using MATT? Does it improve performance?\n\n4. On Weakness 2 (i), why does the paper consider only Ukrainian? The paper should expand its evaluation to different typological languages.\n\n5. On Weakness 2 (ii), the paper should consider languages that are not supported in Qwen3. For instance, Amharic is not on the list. There are many other languages that are not on the list but are included in FLORES, Belebele, and Global MMLU.\n\n6. On Weakness 2 (iii), the paper should consider another generation task like summarization. I believe most of the tested languages in this paper are included in XL-SUM.\n\n7. On Weakness 2 (iv), the paper should consider including results on actual inference speedups.\n\n8. On Weakness 2 (v), the paper should try to use the same set of tasks and models across the paper. If this is not possible, the paper should give justifications.\n\n9. On Weakness 2 (vi), the paper should include an additional discussion on the observed performance difference between languages and models in Section 4.2.\n\n10. On Weakness 3, the paper should conduct continual pre-training for all baselines using the same budget as MATT. Moreover, I would recommend including some full continual pre-training results for the proposed method and the best-performing baseline to examine the extent to which the advantage of the proposed method holds.\n\n11. On Weakness 4, the paper should expand the evaluation further to make the corresponding statement reliable. If this is not possible, the discussion requires revision."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "76giXKfTxr", "forum": "IyV1QEc95F", "replyto": "IyV1QEc95F", "signatures": ["ICLR.cc/2026/Conference/Submission24994/Reviewer_GENk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24994/Reviewer_GENk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649651312, "cdate": 1761649651312, "tmdate": 1762943274776, "mdate": 1762943274776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to tackle transfer of pre-trained language models on a source language to a certain target language. This is critical as the distribution of data availability is not uniform across languages and knowledge of a pre-trained model may enable more efficient transfer to obtain capable language models for target languages. To this end, the authors propose model-aware tokenizer transfer (MATT) which, contrary to existing methods, leverages the idea that a tokenizer for a target language should preserve the attention weight patterns of the source language to maximize performance. This is instantiated via Attention Influence Modeling (AIM) that aligns softmax weights between source and target tokenizers at overlapping string segments. MATT outperforms commonly used heuristics on various target languages demonstrating strong performance and improved compression rate."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well motivated and the idea is novel to the best of my knowledge\n- The results seem to be promising compared to other baselines and heuristics\n- The method enables efficient transfer from a source language to a variety of target languages."}, "weaknesses": {"value": "Generally, I am positive about the paper, however there are some clarifications required for me to fully comprehend the relevance and results reported in this work.\n\n**Baseline comparison**\n\nI am not convinced that Table 1 provides a fair comparison, as the authors mentioned, most of the competitors are mere heuristics that do not involve any optimization procedure whereas MATT involves an optimization process.\nThe comparison would be more fair if the amount of compute that was used for MATT optimization is invested in LM training for the heuristics, as this is what they usually rely on as a second step. This could be matched by the amount of compute that is needed for MATT and allocate the same amount of compute (like shown in Table 3) to tuning the embedding layer for heuristics on the language modeling task.\nEventually, this would add a new set of results that shows different compute budgets and a comparison for those of MATT to others, which would add strength and rigor to the paper.\n\n**Clarity**\n\nThe clarity of the experimental design could be improved. For me it was sometimes not clear whether the evaluation was done in the source or in the target task and it was not explicitly stated. \nFor example, Belebele spans 122 languages, was the evaluation in Table 1 only done on the Ukrainian subset? Same question for GlobalMMLU and LongFLORES.\n\nIn Table 3 there are some languages for which the base model is still better even after tokenizer transfer, like arabic or japanese. As far as I understand this provides negative results -- why would you do tokenizer transfer if the base model performs better on certain langauges anyways? Maybe I misunderstood this part, but I believe it is central to clarify the relevance of this work. If it is indeed a negative result, is there a way to find a proxy for when tokenizer transfer would work and when it doesn't?\n\nFinally, it is not entirely clear to me whether after tokenizer transfer during inference one still needs to sort of switch between tokenizers depending on the language one wants to predict for, i.e. do I need to know during inference whether I evaluate on a target language or not? In my view this knowledge is required, but from the way certain parts of the text are written it sounds like it is not, can the authors please elaborate on that?\n\n**Significance of results**\n\nThe performance difference between different components of MATT in Table 1 and Table 3 is sometimes marginal, therefore it is not clear what setup generally works best or should be used by default. I recommend to add error bars at least for the ablation studies to see whether different setups exhibit statistically significant performance.\n\n\n**Limitations**\n\nIf I understood correctly, another limitation that wasn't mentioned is that for collecting the targets for the optimization process you need an extra forward pass, once with the original tokenizer and once with the external tokenizer which is then used for backpropagation to obtain gradients for the embedding weights."}, "questions": {"value": "- Line 337: What does it mean to merge tokenizers? This is a bit confusing to me as it sounds that the training process is then agnostic to tokenizers, but for each input you still need to segment based on each tokenizers separately to obtain inputs to your loss function, right?\n- Line 361: Why is the MSE loss chosen to be on the 12th layer? I see in Table 5 all layers were tested, but were there also other single layers tested except layer 12? Could it be that some other layers provide more useful information? It might be interesting to look at earlier or intermediate layers as well."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P571X3VkOj", "forum": "IyV1QEc95F", "replyto": "IyV1QEc95F", "signatures": ["ICLR.cc/2026/Conference/Submission24994/Reviewer_4cGV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24994/Reviewer_4cGV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915353666, "cdate": 1761915353666, "tmdate": 1762943274357, "mdate": 1762943274357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to transfer the tokenizer of a pretrained LLM to a new tokenizer. It initializes embeddings for new tokens by optimizing a distillation objective based on matching segments of hidden states. The method is evaluated in language adaptation settings and compared against several other “heuristic” embedding initialization methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**S1:** The proposed method significantly outperforms (non-learning based) heuristic embedding initialization methods in the reported evaluations.\n\n**S2:** The method’s intuition on distilling inter-token attention patterns is well-motivated.\n\n**S3:** The paper is well-written and easy to follow, with good figures to explain the proposed method."}, "weaknesses": {"value": "**W1:** There are several important baselines missing, which are required to properly contextualize the strong results of the proposed method. In general, the current experimental evaluation is insufficient and forgoes comparison against stronger (non-heuristic) embedding initialization baselines. **It is not surprising** that directly optimizing new embeddings on 240 million (!) tokens outperforms heuristic initializations based on some mappings between embedding spaces without any further training. Therefore, I recommend adding comparisons against the following baselines in a future revision to back up the strong claims of the paper:\n- **W1.1:** The method compares only against “heuristic” embedding initialization methods which are not learning based. Apart from ZeTT (which amortizes the learning during its hyper-network pretraining), one crucial baseline is learning the new embeddings via the *classic next-token prediction objective* using the same setup as in the experiments for the proposed AIM objective (e.g. same data, computational budget, ..). **This baseline is necessary** and should have been provided in all experiments to evaluate the usefulness of the proposed (more complicated) objective vs. a simpler existing learning objective.\n - **W1.2:** As ZeTT (https://arxiv.org/abs/2405.07883) is very competitive, a comparison of the proposed method to ZeTT is desirable. Can the direct optimization of the embeddings via MATT beat the embeddings produced by ZeTT?\n- **W1.3:** Another non-heuristic baseline is the method proposed in https://arxiv.org/abs/2410.05864, which extracts new token representations from intermediate layers and maps them back to the embedding spaces. This method also outperforms heuristic-based initialization methods in their evaluations.\n\n**W2:** In l. 81ff:\n> Our contributions are: [...] Model-Aware Tokenizer Transfer (MATT): an efficient tokenizer-transfer method that exploits model dynamics instead of relying solely on semantic relationships, achieving state-of-the-art results with substantially lower computational cost than language modeling objectives.\n\n I could not find any comparison to language modeling objectives (i.e. next-token prediction with cross-entropy?) in the paper, could you point me to the correct Figure/Table?\n\n**W3:** In l. 137ff you write:  \n> Other work (Abagyan et al., 2025) shows that periodically resetting embeddings during pretraining makes models more robust to them, reducing the effort needed to learn new tokens afterwards. \n\nbut I cannot find support for this claim anywhere in the paper by Abagyan et al., 2025. Could you elaborate?\n\n\n\n\n**Score summary:** I recommend to **`reject`** the paper in its current state due to (1) missing baselines which are necessary to evaluate the strength of the proposed method compared to prior work / simpler methods (see W1), (2) claims for which I could not find support for in the paper/cited references (W2 & W3) and (3) questions regarding novelty compared to insufficiently discussed prior work (Q2). If these points are sufficiently addressed, I am open to raising my score."}, "questions": {"value": "- **Q1:** The training loss only optimizes the new embeddings in their role as input embeddings, as you discuss. How does this influence the model’s ability to actually generate the new tokens (in the case of tied embeddings)? \n- **Q2:** The proposed method’s intuition, setup, and loss objective seem (very) similar to the one proposed in https://arxiv.org/abs/2505.20133, could you discuss the differences of your work?\n- **Q3:** What target layer for the AIM/AIM* objectives is used in the main experiments?\n- **Q4:** How well does MATT work with varying degrees of presence of the target language(s) in the models original pretraining data? Does the model need to have already been trained on target languages?\n\n**(nit):** this does not influence my review of the paper, but could you explain the difference between the proposed AIM objective and the MATT method? Is MATT just the application of the AIM (or AIM*) objective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "l9nE0KWyLY", "forum": "IyV1QEc95F", "replyto": "IyV1QEc95F", "signatures": ["ICLR.cc/2026/Conference/Submission24994/Reviewer_emKC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24994/Reviewer_emKC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923621159, "cdate": 1761923621159, "tmdate": 1762943273949, "mdate": 1762943273949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}