{"id": "1Hxtv5kT5Y", "number": 2353, "cdate": 1757062195782, "mdate": 1763436410884, "content": {"title": "FastALM: Hierarchical Frame Q-Former for Effective Audio Modality Adaptation", "abstract": "Recent advances in large language models (LLMs) have demonstrated human-expert-level capabilities, driving significant interest in their potential for achieving artificial general intelligence (AGI). In particular, there is growing momentum in adapting LLMs to various modalities—including vision, video, and speech—through the development of multimodal LLMs (MLLMs). However, existing speech-language models (SLMs) research has largely overlooked cost-effective adaptation strategies for leveraging LLMs in the speech domain. In this paper, we propose FastSLM, a lightweight yet efficient SLM designed for effective understanding and reasoning over long-form speech. To address the challenge of aligning high-frame speech features with LLM, we introduce the hierarchical frame querying transformer (HFQ-Former), which compresses frame-level speech features while capturing both local and global context. Furthermore, we present a novel three-stage training strategy that enhances generalization across a wide range of speech-related tasks. Experimental results demonstrate that FastSLM achieves competitive performance compared to existing state-of-the-art (SOTA) models, despite operating with significantly lower FLOPs and parameter counts, while representing speech with only 1.67 tokens per second. The source code and model checkpoints are available at https://anonymous.4open.science/r/FastALM-1D6B.", "tldr": "", "keywords": ["Audio Language Model (ALM)", "Q-Former", "Multimodal"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bda9472e1144e59374a68f90cc792231191cdd4f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes FastALM, a speech-language model for efficient understanding of long-form spoken input. The method introduces a HFQ-Former that compresses high-rate acoustic frames into a compact set of queries for an LLM, and adopts a three-stage training strategy to adapt a pretrained LLM to the speech modality. The system is trained on a moderate-sized bilingual corpus in Korean and English and is evaluated on ASR, AST, spoken summarization, and spoken QA. The authors report competitive accuracy and faster end-to-end processing compared to speech-focused baselines under long audio inputs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Combines a hierarchical frame-to-query adapter with a simple three-stage adaptation schedule to reduce the acoustic-to-LLM interface cost for long speech.\n2. Evaluates across several speech tasks. The performance is competitive with high computational efficiency."}, "weaknesses": {"value": "The paper's biggest weakness is its overstatement of its scope.\n\nThe paper repeatedly refers to the model as an Audio-Language Model, yet all tasks and data are speech only. In current community usage, ALM typically denotes models that cover general auditory content that includes speech, environmental sounds, and music, not speech alone [1, 2, 3]. A more accurate term here is Speech-Language Model [4]. This affects positioning, related work, experiments, and claims. For example, statements like “address the challenge of aligning high-frame audio features with LLM” would be clearer as “aligning high-frame speech features with LLM.” Likewise, claims about enabling fast text generation from long audio should be stated as long speech. As a result, in experiments, it would be more appropriate to compare with more speech-focused models.\n\nOther weaknesses:\n\nPositioning and baselines:\nRelated work sections labeled only “Audio-Language Models” and “Q-formers for speech modality adaptation” should separate general-audio ALMs from speech-only SLMs, then justify how the proposed method advances the speech setting specifically. Experimental comparisons should include strong speech-centric baselines and report both accuracy and efficiency under matched conditions.\n\nMissing implementation detail:\nThe paper does not specify the LLM backbone used for LoRA adaptation. Since LoRA hyperparameters and achievable throughput depend on the base model, the manuscript should name the backbone and report key config details, including LoRA ranks, target modules, context lengths, tokenizer, and precision.\n\n[1] Tang, C., Yu, W., Sun, G., Chen, X., Tan, T., Li, W., ... & Zhang, C. (2023). Salmonn: Towards generic hearing abilities for large language models. arXiv preprint arXiv:2310.13289.\n[2] Kong, Z., Goel, A., Badlani, R., Ping, W., Valle, R., & Catanzaro, B. (2024). Audio flamingo: A novel audio language model with few-shot learning and dialogue abilities. arXiv preprint arXiv:2402.01831.\n[3] Chu, Y., Xu, J., Zhou, X., Yang, Q., Zhang, S., Yan, Z., ... & Zhou, J. (2023). Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919.\n[4] Cui, W., Yu, D., Jiao, X., Meng, Z., Zhang, G., Wang, Q., ... & King, I. (2024). Recent advances in speech language models: A survey. arXiv preprint arXiv:2410.03751."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YJP8vJvLDR", "forum": "1Hxtv5kT5Y", "replyto": "1Hxtv5kT5Y", "signatures": ["ICLR.cc/2026/Conference/Submission2353/Reviewer_Uz6B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2353/Reviewer_Uz6B"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761682754592, "cdate": 1761682754592, "tmdate": 1762916203641, "mdate": 1762916203641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FastALM, a lightweight audio–language model that integrates speech into large language models (LLMs) through a Hierarchical Frame Querying Transformer (HFQ-Former) and a three-stage training strategy. The HFQ-Former hierarchically compresses frame-level audio features from pre-trained encoders (e.g., Whisper) into compact representations (≈1.67 tokens/sec) for efficient LLM adaptation. The training pipeline includes short-form pre-training, long-form adaptation using ASR data, and instruction tuning on multi-task datasets (ASR, AST, SSUM, SQQA). Experiments show competitive performance compared with state-of-the-art ALMs, with notably lower FLOPs and token rates.\n\nWhile the paper is well-written and the system is practically effective, its novelty and conceptual contribution are limited. The proposed HFQ-Former mainly combines existing Q-Former designs with hierarchical down-sampling, and the training pipeline closely follows prior ALM works such as SALMONN, Audio-Flamingo, and Qwen2-Audio. The work thus reads more as an engineering optimization rather than a substantive methodological innovation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and systematically organized. The overall pipeline—from audio encoding to multimodal LLM alignment—is technically sound and reproducible.\n\n2. The hierarchical compression indeed reduces token rates and computational costs."}, "weaknesses": {"value": "1. The HFQ-Former is conceptually incremental—essentially a hierarchical stacking of existing Q-Former modules with down-sampling. No new mechanism or theoretical insight is introduced beyond standard attention-based compression.\n\n2. The performance improvement over previous Q-Formers is minor (e.g., WER reduced by only ~0.05 on LibriSpeech).\n\n3. The paper lacks ablation or visualization explaining why the hierarchical compression helps. There is no semantic or attention-level analysis of the compressed tokens, leaving the underlying mechanism unclear."}, "questions": {"value": "1. why donot maintain the same token rate to compare the performance between SQ-Former, WQ-Former?\n\n2. Why donot present the baseline with directly down-sampling? Such as the style of Qwen2-Audio. From my veiw, Q-former struture is not  the mainstream style for MLLM in vision domain. Many audio-LLMs also donot use the Q-former structure, beacause the Q-former also introduce the additional parameters, incleasing the training and inference cost."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9mWEkon5lQ", "forum": "1Hxtv5kT5Y", "replyto": "1Hxtv5kT5Y", "signatures": ["ICLR.cc/2026/Conference/Submission2353/Reviewer_eN8e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2353/Reviewer_eN8e"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840208353, "cdate": 1761840208353, "tmdate": 1762916203492, "mdate": 1762916203492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FastALM, a lightweight and efficient Audio-Language Model (ALM) designed to address the computational inefficiency of processing long-form audio in existing ALMs. \n\nThe core innovation is the Hierarchical Frame Querying Transformer (HFQ-Former). It compresses high-frame audio features into a compact representation while preserving local and global context, reducing LLM FLOPs and memory usage. FastALM also adopts a three-stage training strategy: (1) short-form ASR pre-training to align speech and text; (2) long-form audio adaptation to enhance long-audio processing capability; (3) instruction tuning for downstream tasks.\n\nKey contributions:\n1. A low-cost FastALM that enables fast text generation from long-form audio with minimal computational cost.\n2. A three-stage training pipeline that adapts pre-trained LLMs to speech modalities without costly end-to-end training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: it proposes HFQ-Former, a novel multi-stage hierarchical compression module that differs from prior Q-Formers by integrating downsampling to capture both local and global context, achieving a low-frame-rate representation.\n\nClarity: The paper is well-structured and clear, with a complete logical flow and appendices that include key details, all of which enhance readability and reproducibility.\n\nSignificance: The experiments strongly validated the significance of this work, which achieves higher or comparable performance on different audio tasks over SOTA approaches while reducing computational cost. It presents improvement to ALMs, especially for long-form audio."}, "weaknesses": {"value": "1. The novelty is limited.\n* The HFQ-Former is an incremental improvement of Q-Former, which leverages multi-scale modeling into the model. But \"multi-scale model can improve computing efficiency\" has been validated in other works, e.g. U-Net. The author should give a deeper insight into this idea to strengthen the novelty.\n* Similarly, multi-stage training has been a widely used strategy to improve training effectiveness and efficiency.\n\n2. The experiments are not convincing.\n*  Insufficient ablation studies for HFQ-Former.\n  * No analysis for the proposed model design. It makes me confused about why the module is designed in such a complicated form.\n  * No investigation on the impact of \"hierarchical\" (e.g. number of downsampling layers, downsampling scale, etc.)\n* Unvalidated fairness of the system comparison\n  * No details of baselines (model configuration, dataset, training details)"}, "questions": {"value": "As discussed in \"Weakness\", I have the following concerns:\n\n1. As you said, \"This design enables the model to effectively capture both local and global temporal information from audio features\". Can you give a deeper analysis of this claim? What does local and global temporal information include? Can a larger single-stage q-former extract local global temporal information effectively?\n2. Do you have ablation studies on the impact of \"number of stages\" and \"downsampling factors\"?\n3. Do S-former and W-former share the same dataset and training configuration as HFQ-former? Do they have the same embedding dimension?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y01Ex6Zquo", "forum": "1Hxtv5kT5Y", "replyto": "1Hxtv5kT5Y", "signatures": ["ICLR.cc/2026/Conference/Submission2353/Reviewer_dG35"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2353/Reviewer_dG35"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959185316, "cdate": 1761959185316, "tmdate": 1762916203258, "mdate": 1762916203258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}