{"id": "7kiL7mNrxG", "number": 13712, "cdate": 1758221339720, "mdate": 1759897417798, "content": {"title": "Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation", "abstract": "Radiology reports convey detailed clinical observations and capture diagnostic reasoning that evolves over time. However, existing evaluation methods are limited to single-report settings and rely on coarse metrics that fail to capture fine-grained clinical semantics and temporal dependencies. We introduce LUNGUAGE, a benchmark dataset for structured radiology report generation that supports both single-report evaluation and longitudinal patient-level assessment across multiple studies. It contains 1,473 annotated chest X-ray reports, each reviewed by experts, and 80 of them contain longitudinal annotations to capture disease progression and inter-study intervals, also reviewed by experts. Using this benchmark, we develop a two-stage structuring framework that transforms generated reports into fine-grained, schema-aligned structured reports, enabling longitudinal interpretation. We also propose LUNGUAGESCORE, an interpretable metric that compares structured outputs at the entity, relation, and attribute level while modeling temporal consistency across patient timelines. These contributions establish the first benchmark dataset, structuring framework, and evaluation metric for sequential radiology reporting, with empirical results demonstrating that LUNGUAGESCORE effectively supports structured report evaluation. Code and data are available at: https://anonymous.4open.science/r/lunguage", "tldr": "LUNGUAGE introduces the first benchmark, structuring framework, and metric for sequential radiology reporting, enabling fine-grained and temporally consistent evaluation of chest X-ray reports.", "keywords": ["Evaluation Benchmark", "Patient-level Chest X-ray Benchmark", "Structured Report", "Single Report Evaluation", "Longitudinal Report Evaluation", "Clinical NLP", "Structured Clinical Information", "Medical Benchmark", "Radiology Report Metric", "Report Evaluation", "Structured Report Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e7db10323aba551f61767f0eeeb3fd4e3c74b4a5.pdf", "supplementary_material": "/attachment/4ca0dac46fd7c47a681f8fb86e5d3fdaa3fbbd0e.zip"}, "replies": [{"content": {"summary": {"value": "A benchmark dataset of sequential radiology report is proposed, containing 1,473 reports from 230 patients, including 10 patients with sequential reports. A set of clinical entities and relationships are annotated. A new evaluation metric is proposed for generated report evaluation. \n\nThe dataset construction details are presented, but the size of the dataset is small."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "A benchmark dataset of sequential radiology report is proposed, where many kinds of clinical entities and relationships are annotated. Dataset construction is described with details."}, "weaknesses": {"value": "1. More experimental results are needed for testing the temporal-aware report generation models on the proposed dataset.\n\n2. The benchmark dataset is in small-scale. As the sequential interpretation claimed as the novelty or contributions, only 10 patients in the proposed dataset contains sequential reports (longitudinal trajectories)."}, "questions": {"value": "1. In the 3.1, there are several kinds of entities defined. Is this categorization referred from any medical textbook or handbook, or created by yourself? Any domain experts confirm that this category is reasonable? Can an entity be categorized into more than one kind?\n\n2. In the sequential report structure, can one clinical finding in the Day 10 report be associated with more than one clinical finding in the Day 90 report? In addition, is many-to-many association between clinical findings in different sequential reports possible? If possible, how can your proposed schema represent that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dx7SJMxsFA", "forum": "7kiL7mNrxG", "replyto": "7kiL7mNrxG", "signatures": ["ICLR.cc/2026/Conference/Submission13712/Reviewer_zLzN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13712/Reviewer_zLzN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13712/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761455659083, "cdate": 1761455659083, "tmdate": 1762924260706, "mdate": 1762924260706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a benchmark for structured and sequential chest x-ray interpretation. It addresses two major limitations in prior works, i.e., temporal reasoning across longitudinal reports (e.g., \"no change\" or \"worsening\" must be validated against prior findings) and \nfine-grained clinical accuracy, including attributes like location, size, and morphology, which are often lost in coarse metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper focuses on radiology report evaluation through the extraction of structured knowledge. In addition to single-report evaluation, the framework introduces a sequential evaluation paradigm, which has been largely underexplored in previous research.\n\n2. The paper provides a detailed pipeline for constructing an evaluation benchmark, which can serve as a valuable resource for future research in radiology report analysis.\n\n3. The proposed evaluation framework is comprehensive, covering a broad range of aspects, from lexical to semantic similarity. Moreover, the involvement of domain experts in annotating the structured radiology reports ensures the reliability and clinical validity of the dataset."}, "weaknesses": {"value": "1. Despite the comprehensive evaluation design, the structure extracted from radiology reports is primarily represented as triples in a knowledge graph. However, extensive research has already explored this representation. A higher-level view of structured reports should encompass not only triples but also observations, findings, and clinical statuses. Therefore, entity-level evaluation alone may be insufficient to capture the full complexity of radiology reports. ref: [1]\n\n2. The LUNGUAGESCORE combines both lexical and semantic similarity. However, before applying this metric, one must extract triples from reports using external tools (LLMs in this paper). While these models demonstrate good performance in triple extraction, it might be worth exploring an alternative approach, i.e., using LLM-as-a-judge for computing LUNGUAGESCORE. This approach is simple, effective, and relies only on prompting. Moreover, scoring semantic similarity through embeddings may not fully align with radiologists’ subjective preferences (as shown in Table 2).\n\n3. The structured evaluation closely resembles KG-based evaluation, yet its performance in single-report settings is comparable to that of RadGraph, suggesting limited improvement in this aspect.\n\n4. Regarding temporal evaluation at the semantic level, GREEN can also detect comparisons with prior findings (e.g., “(f) Omitting a comparison detailing a change from a prior study”). According to Table 2, the proposed metric appears to underperform GREEN in this regard.\n\n[1] Automated Structured Radiology Report Generation. https://arxiv.org/pdf/2505.24223"}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DRiBGDZeJJ", "forum": "7kiL7mNrxG", "replyto": "7kiL7mNrxG", "signatures": ["ICLR.cc/2026/Conference/Submission13712/Reviewer_u5tg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13712/Reviewer_u5tg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13712/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966850047, "cdate": 1761966850047, "tmdate": 1762924260049, "mdate": 1762924260049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LUNGUAGE, a fine-grained structured and longitudinal evaluation benchmark tailored for chest X-ray reports. The authors design and implement an automatic structuring framework aligned with the proposed schema, enabling entity and relation extraction and grouping across both single and longitudinal multi-report settings. The framework demonstrates relatively high agreement with human annotations. Furthermore, the paper proposes LUNGUAGESCORE, an interpretable evaluation metric that jointly assesses semantic fidelity, structural consistency, and temporal coherence, thereby establishing a systematic foundation for the evaluation of longitudinal radiology reports generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces LUNGUAGE, a longitudinal and structured benchmark for chest X-ray report generation. It offers fine-grained report structuring and pays specific attention to the evaluation of longitudinal reports. The dataset has been reviewed by radiologists, enhancing its clinical reliability.\n\n- The authors release their structuring algorithm, prompting templates, and the expert-validated vocabulary, which collectively facilitate standardized, automated, and reproducible extensions of this work.\n\n- The paper introduces LUNGUAGESCORE, integrates semantic, structural, and temporal dimensions into a unified scoring framework. This design enables more effective discrimination among models, especially in terms of their temporal consistency."}, "weaknesses": {"value": "- The overall dataset size in this work is relatively small. Although a key contribution is the focus on longitudinal report sequence evaluation, the number of such longitudinal sequences is limited, which makes the evaluation somewhat insufficient. While the authors acknowledge this limitation in the conclusion, I would strongly encourage them to expand the benchmark with more cases if feasible.\n\n- The study primarily uses the MIMIC-CXR dataset and its subsets, which exhibit a fairly homogeneous reporting style. The evaluation would be more convincing if it were extended to include an additional, distinct dataset with different reporting conventions.\n\n- The evaluation relies on the LLM’s performance. Minor errors in entity extraction, relation identification, or grouping can cascade, since MatchScore = Semantic × (Temporal if T > 1) × Structural, leading to unstable or unreliable results.\n\n- Several concerns regarding specific details, as outlined in **Questions**."}, "questions": {"value": "I fully appreciate the substantial effort behind this work and its significance for longitudinal report evaluation. However, a few details remain unclear to me:\n\n- In single-report evaluation, it is common for reports to already contain longitudinal comparison cues (e.g., “no change,” “unchanged,” etc.). How does the method interpret and evaluate such explicitly comparative attributes when assessing a single report in isolation?\n- The semantic similarity metric is somewhat confusing. Are you using the fully structured phrases (e.g., “opacity–left lung–nodular–slightly increased”) directly as input for embeddings, or are you using the original unstructured free-text sentences? If structured phrases, essentially sequences of discrete terms, are embedded directly, is the resulting semantic similarity truly meaningful? In contrast, structural similarity appears more reliable and interpretable in this context.\n- In the bipartite matching step, does the approach account for one-to-many alignments? For instance, a single phrase in the reference report might correspond to two or more phrases in the generated report. If such cases arise, how does the method handle them?\n- The paper mentions grouping certain entities. Do the authors account for view-specific groupings? Some findings are only observable in specific radiographic views (e.g., PA vs. lateral), and longitudinal sequences may occasionally miss certain views altogether. How does the proposed method handle and evaluate such view-dependent and potentially incomplete scenarios?\n- In longitudinal evaluation, some follow-up reports may be overly concise and omit entities previously mentioned (e.g., a finding described in an earlier report is absent in a later one without explicit negation). How does the method treat such omissions? Could this affect the final evaluation scores, especially if the absence is interpreted as a negative finding or simply ignored?\n\nSuggestions for Future Work:\n\n- While the current focus is on textual structuring, it would be a valuable extension to ground each structured phrase with corresponding bounding boxes in the chest X-ray images. This would enable more fine-grained, vision-language alignment and enhance clinical utility.\n- Given the benchmark’s small size, its main role is evaluation. It would be more impactful if the authors could leverage their automated structuring pipeline to scale up this dataset for training more robust chest X-ray report generation models. But I also have a concern, if without expert validation, automated expansion may introduce hallucinations. Although the paper notes high consistency when leveraging domain-specific lexicons, medical applications demand high reliability, and unverified synthetic labels could negatively impact downstream model training. It is another challenge that warrants careful consideration.\n\nOverall, I  hope authors will continue to expand and refine this work, which can benefit the communities."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "If the authors release their data and codes, I have no ethics concerns."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6urmlGyadP", "forum": "7kiL7mNrxG", "replyto": "7kiL7mNrxG", "signatures": ["ICLR.cc/2026/Conference/Submission13712/Reviewer_XQC1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13712/Reviewer_XQC1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13712/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762260225704, "cdate": 1762260225704, "tmdate": 1762924258835, "mdate": 1762924258835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents LUNGUAGE, a benchmark for structured and longitudinal radiology report generation. It includes 1,473 expert-annotated chest X-ray reports, with 80 capturing disease progression over time. The authors propose a two-stage framework for converting reports into schema-aligned representations and introduce LUNGUAGESCORE, a metric that evaluates entity, relation, and attribute consistency across time. This work enables fine-grained and temporally-aware assessment of radiology report generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper introduces LUNGUAGE, the benchmark specifically designed for both single-report and longitudinal structured radiology report evaluation. It addresses a key gap in temporal and schema-aligned medical language evaluation.\n* The proposed LUNGUAGESCORE enables structured, interpretable, and temporally aware evaluation of generated reports, addressing the limitations of coarse or purely lexical metrics."}, "weaknesses": {"value": "I have previously reviewed an earlier version of this work, and I acknowledge that the authors have addressed several prior concerns, particularly in terms of annotation quality and clinical relevance. However, one major concern still remains:\n\nWhile the annotation quality and clinical depth are commendable, the overall dataset scale—particularly the limited number of annotated reports and longitudinal cases—remains insufficient to support the benchmark’s claimed generalizability and broader impact. The small number of patients and annotated reports restricts the diversity of pathologies, temporal patterns, and linguistic variations, making it difficult to serve as a comprehensive resource for training or evaluating robust, general-purpose longitudinal models.\n\nGiven that this benchmark is positioned as a potential standard for evaluating structured report generation and temporal consistency, I would have expected either significantly broader coverage or stronger justification that the current scope is sufficient.\n\nI would also be interested in seeing how other reviewers weigh the dataset’s scale versus its clinical design, to better understand the consensus on its impact and readiness for adoption."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mioHKTgOyi", "forum": "7kiL7mNrxG", "replyto": "7kiL7mNrxG", "signatures": ["ICLR.cc/2026/Conference/Submission13712/Reviewer_mMcS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13712/Reviewer_mMcS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13712/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762285646145, "cdate": 1762285646145, "tmdate": 1762924258332, "mdate": 1762924258332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}