{"id": "pBb25np2bD", "number": 12670, "cdate": 1758209401505, "mdate": 1763488008841, "content": {"title": "How to Teach Large Multimodal Models New Skills", "abstract": "How can we teach large multimodal models (LMMs) new skills without erasing prior abilities? We study sequential fine‑tuning on five target skills while monitoring general ability on eight held‑out benchmarks across three model families. We observe that apparent “forgetting” on held‑out tasks after narrow fine‑tuning can partly recover at later stages. We trace this behavior to a measurable shift in the output token distribution, manifested through a simple counting‑bias probe that identifies the shift co‑varies with forgetting. Guided by this picture, we identify two simple, robust tuning recipes that learn strongly while limiting drift: (i) updating only the self‑attention projection layers, and (ii) updating only the MLP Gate\\&Up while freezing the Down projection. Across models and tasks, these choices deliver strong target gains while largely preserving held‑out performance.", "tldr": "", "keywords": ["Vision-Language Models", "Large Multimodal Models", "Continual Learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/93d587e6b8e275a62dad99db206dc92eee33586a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on the continual learning problem of LMMs. Through fine-tuning experiments and a counting-bias probe, it reveals that \"forgetting\" of held-out tasks post-fine-tuning is essentially recoverable output token distribution shift. It further proposes two fine-tuning schemes, updating only self-attention projection layers and updating only MLP’s Gate&Up layers while freezing Down layer, which ensure strong target-task learning with minimal forgetting across multiple models and tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Through fine-tuning experiments and a counting-bias probe, the paper first clarifies that the \"forgetting\" of held-out tasks in LMMs after fine-tuning is essentially a shift in the output token distribution. Moreover, this shift is partially recoverable via subsequent task fine-tuning, offering a crucial theoretical perspective on LMMs continual learning core issues.\n2. The paper proposes two concise fine-tuning methods that balance strong target-task learning and minimal original-capability forgetting, validated across 3 model families, 5 target task types and 8 held-out benchmarks for robust generalization and reliability."}, "weaknesses": {"value": "1. Regarding the question of whether the two fine-tuning methods can be combined to improve performance, Appendix F.1 notes simultaneous fine-tuning of SA Proj. and MLP (Gate&Up) offers no gain or even degrades performance, but lacks in-depth explanation of the underlying mechanism; alternative combinations (e.g., two-stage fine-tuning) are also unexplored.\n2. The paper only compares its strategy with traditional methods (LoRA, WiSE-FT, MoE) and excludes recent mainstream continual learning schemes, failing to clarify the strategy’s competitiveness.\n3. The \"forgotten knowledge recoverability\" claim lacks rigorous explanation/verification: no causal validation via experiments like \"adjusting distribution without training new tasks\", no quantification of distribution correction-recovery correlation, and unclear recovery triggers, reducing practical value.\n4. Focused on quantitative metrics, the paper does not compare fine-tuned models’ output differences for the same input or analyze intermediate feature changes before/after forgetting recovery, hindering intuitive understanding of the strategy’s effect."}, "questions": {"value": "1. The paper fails to quantify the training efficiency (parameter count, computation time, memory usage) of SA Proj. and MLP (Gate&Up), precluding efficiency comparison with full-model and LoRA fine-tuning. Can experimental data be supplemented to clarify applicability in resource-constrained scenarios?\n2. The paper innovatively uses a counting-bias probe to verify token distribution-forgetting correlation. Can similar probes (e.g., for medical VQA, clock reading) be tested to confirm if such probes can be generalized as a universal tool for measuring task-specific token distribution shifts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "alvbWNikYv", "forum": "pBb25np2bD", "replyto": "pBb25np2bD", "signatures": ["ICLR.cc/2026/Conference/Submission12670/Reviewer_s6ff"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12670/Reviewer_s6ff"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12670/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761053081336, "cdate": 1761053081336, "tmdate": 1762923508227, "mdate": 1762923508227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors compare various layer-level fine-tuning strategies and find that selectively tuning only the self-attention projection layers or the MLP’s up and gate components allows efficient learning of new tasks while retaining performance on previous ones. Using a counting-bias probe method, the paper further shows that forgetting mainly arises from output distribution drift rather than genuine knowledge loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The use of held-out benchmarks is highly valuable. Existing benchmarks typically measure forgetting only with respect to previously trained tasks within the same benchmark, without considering the preservation of the model’s intrinsic capabilities.\n- The discovery that fine-tuning the Self-Attention Projection (SA Proj.) or MLP Gate&Up layers can acquire new knowledge while greatly reducing forgetting of existing abilities is both effective and practically straightforward."}, "weaknesses": {"value": "- The conclusion that tuning SA Proj. and Gate&Up does not lead to significant forgetting has not been validated on other benchmarks. Therefore, it is difficult to rule out the possibility that this finding stems from dataset bias in the current experimental domain.\n- The paper lacks direct comparisons with recent SoTA methods (published in recent two years). Although this paper demonstrates that tuning SA Proj. and Gate&Up is effective, it remains unclear how effective this approach is relative to SoTA baselines.\n- The analysis of output distribution drift relies mainly on the counting-bias probe method, so it remains unclear whether the same conclusion holds for other cases where the task outputs are not primarily numeric."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eyH3UtmUpR", "forum": "pBb25np2bD", "replyto": "pBb25np2bD", "signatures": ["ICLR.cc/2026/Conference/Submission12670/Reviewer_xSMp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12670/Reviewer_xSMp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12670/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729984229, "cdate": 1761729984229, "tmdate": 1762923507871, "mdate": 1762923507871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies continual learning and forgetting in VLM training. The authors propose two simple but effective methods to mitigate the forgetting, that are, (1) only updating the attention and projection (Wq, Wk, Wv, Wo), and (2) updating only the MLP gate and up layers. run experiments on different tasks. The authors do test on 5 different tasks sequence, 8 held-out benchmarks, and 3 different VLM families to verify the effectiveness of the simple methods. There is also an interesting understanding part, trying to map the forgetting to the token-distribution shift."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) very clear writing: from my side, it is easy to understand\n(2) easy but effective method\n(3) experiments are relatively comprehensive on my side\n(4) i really appreciate the understanding part, where the authors dive deeper into the reason of forgetting, and map it into the token-distribution shift."}, "weaknesses": {"value": "The main contribution of the paper is to study which part of the parameters to update (to my understanding, correct me if I am wrong). While indeed the proposed methods already show signal, it is not clear if there is any logic/reasons behind selecting those parameters. There are many other confounding factors, which may make the conclusion change. for example\n(1) If the model is larger, are there any other rules for selecting the update parameters?\n(2) If the model is larger, will it be beneficial to use LoRA?\n(3) Will it be better to select the parameters based on the layer index, e.g., if it is better to update on a later layer than an earlier layer?\nI am afraid that in the end, it will just turn into an engineering problem, where you just run all the design choices and pick the best, it there are limited scientific guides.\n\nHowever, I still agree that the token distribution is interesting."}, "questions": {"value": "Please see the weakness part. my main questions are regarding these confounding factors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "55oYg2n4KR", "forum": "pBb25np2bD", "replyto": "pBb25np2bD", "signatures": ["ICLR.cc/2026/Conference/Submission12670/Reviewer_hD4Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12670/Reviewer_hD4Q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12670/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766454742, "cdate": 1761766454742, "tmdate": 1762923507470, "mdate": 1762923507470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies effective continual learning strategies in multimodal models. Major findings are: (a) Updating a selected set of parameters can reduce forgetting, and (b) forgetting can be traced in distribution shift in output tokens. The authors corroborate their study with extensive experiments on five target skills while monitoring general ability on eight held‑out benchmarks across three model families."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strength of the paper lies in its clear and simplistic presentation of the continual training experiments. By varying the parameters for updating in this sequence learning setting, they clearly show that selecting the right parameters to update can substantially reduce forgetting. Furthermore, through a mechanistic analysis, they connect forgetting behavior to mechanistic roles of attention vs. MLP layers. Through extensive experiments on wide variety of benchmarks and backbone models, the authors show that forgetting can be mitigated with a simple fine-tuning recipe."}, "weaknesses": {"value": "As such, I don't find any key weaknesses with the paper. I have some questions about the experiment setup:\n\na) All tasks are vision-language. Do the findings generalize if a text-only task was included in the sequence? If not, is it primarily the issue of the way the backbone LLM has been converted to the multimodal LLM? \n\nb) How do the results change if you vary the order of the tasks in sequence? The authors evaluate multiple task orders but do not report variance or confidence intervals (e.g. in table 1)."}, "questions": {"value": "Please see above for my questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2ZZHCvxYS1", "forum": "pBb25np2bD", "replyto": "pBb25np2bD", "signatures": ["ICLR.cc/2026/Conference/Submission12670/Reviewer_yr6z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12670/Reviewer_yr6z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12670/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886216965, "cdate": 1761886216965, "tmdate": 1762923507081, "mdate": 1762923507081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}