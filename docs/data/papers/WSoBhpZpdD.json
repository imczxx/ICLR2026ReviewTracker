{"id": "WSoBhpZpdD", "number": 2859, "cdate": 1757290055780, "mdate": 1763079406561, "content": {"title": "DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation", "abstract": "State-of-the-art text-to-image models excel at realism but collapse on multi-human prompts—duplicating faces, merging identities, and miscounting individuals. We introduce DisCo (Reinforcement with DiverSity Constraints), the first RL-based framework to directly optimize identity diversity in multi-human generation. DisCo fine-tunes flow-matching models via Group-Relative Policy Optimization (GRPO) with a compositional reward that (i) penalizes intra-image facial similarity, (ii) discourages cross-sample identity repetition, (iii) enforces accurate person counts, and (iv) preserves visual fidelity through human preference scores. A single-stage curriculum stabilizes training as complexity scales, requiring no extra annotations. On the DiverseHumans Testset, DisCo achieves 98.6% Unique Face Accuracy and near-perfect Global Identity Spread—surpassing both open-source and proprietary methods (e.g., Gemini, GPT-Image) while maintaining competitive perceptual quality. Our results establish DisCo as a scalable, annotation-free solution that resolves the long-standing identity crisis in generative models and sets a new benchmark for compositional multi-human generation.", "tldr": "", "keywords": ["Reinforcement Learning", "Multi-Human Generation", "Text-to-Image Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7807599fb86a61190e32d94eb1b3995264ca1ea5.pdf", "supplementary_material": "/attachment/b8cf950e1b9bb44754043310afa12a688a88fce3.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces DISCO, which utilizes RL to solve the problems like duplicating faces, merging identities, and miscounting individuals in prevailing image generation models. It features a group-wise diversity reward and a single-stage curriculum strategy. This method outperforms prevailing models on a self-built benchmark and an open-sourced benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The work represents a commendable effort to leverage reinforcement learning for image generation. Tackling controllability through an RL framework is a non-trivial and potentially valuable direction.\n- The manuscript is well-written and clearly structured. The exposition is straightforward, and the technical descriptions are easy to understand.\n- Empirical results suggest that the proposed method performs adequately for coarse-grained control tasks, such as specifying the number of human figures or preventing the id confusion of faces in generated images."}, "weaknesses": {"value": "- The methodological design appears relatively basic and does not introduce significant conceptual or technical novelty. In particular, the RL formulation follows standard practices without offering new insights into how RL can be better adapted to the nuances of image generation.\n- The success of the method stems primarily from the use of multiple external as reward sources, which limits the paper’s contribution.\n- As evidenced by both the method description and the test prompts in the appendix, the approach seems to lack fine-grained control over individual attributes (e.g., hairstyle, clothing style, facial expression). This limitation substantially restricts its practical utility in real-world applications where detailed customization is often required."}, "questions": {"value": "- How were the hyperparameters (e.g., $\\alpha,\\beta,\\gamma,t_\\textit{curriculum}$) selected? Was a systematic hyperparameter search performed?\n- How stable was the GRPO training process in practice? Did the authors observe frequent training instabilities or policy collapse?\n- Have the authors considered architectural or algorithmic extensions to enable more precise, attribute-level control?\n- What is the computational efficiency of the RL training ? How much additional training cost does this method incur compared to fine-tuning baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R1lQ9L3OT1", "forum": "WSoBhpZpdD", "replyto": "WSoBhpZpdD", "signatures": ["ICLR.cc/2026/Conference/Submission2859/Reviewer_zP3i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2859/Reviewer_zP3i"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760782734617, "cdate": 1760782734617, "tmdate": 1762916415300, "mdate": 1762916415300, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We appreciate the time and efforts the reviewers and AC invested in evaluating our submission. However, after careful consideration, we have decided to withdraw our paper from ICLR 2026.\n \n \nWhile we appreciate the feedback provided, we believe that certain key aspects of our work may not have been fully recognized during the review process. For instance, Table E.2 in the supplementary material already includes results from a hyperparameter search, which was specifically requested in the reviews. This suggests that parts of our submission have not been examined by the reviewers.\n \n \nMore importantly, we would like to reiterate the novelty of our contribution. Apart from the intra-image diversity reward, this is the first work to incorporate a reward based on \"group-level statistics\" for text-to-image models; it explicitly optimizes diversity across the whole generated distribution(this specifically enables ~99% unique faces across the ~5000 requested). Our compositional reward design goes beyond prior efforts by jointly addressing intra-image similarity, cross-sample identity repetition, count accuracy and text/perceptual alignment. Hence, it does not sacrifice perceptual quality, text control or requires additional annotations.\n \n \nFurthermore, our work is the first ever method to formalize and address a fundamental limitation present in all current text-to-image (T2I) models(including proprietary ones). This is the inability to maintain identity separation across multiple humans, both in the same image and across generated samples. While some reviewers perceived our approach as incremental, we respectfully disagree. One review referenced MultiCrafter, a concurrent ICLR 2026 submission that was not even publicly available before the submission deadline. Unlike reference-image-based generation frameworks (or “editing” models), our method focuses on improving base text-to-image generation without relying on reference images. We show that these long-standing issues can be resolved without compromising existing capabilities for human-level control. Hence, this approach is intended to act as a strong foundation for future work on reference-based generation. While we have upcoming submissions exploring that direction, it falls outside the scope of this paper.\n \n \nWe hope to refine and resubmit this research to a future venue."}}, "id": "kBgikvbIuj", "forum": "WSoBhpZpdD", "replyto": "WSoBhpZpdD", "signatures": ["ICLR.cc/2026/Conference/Submission2859/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2859/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763079405789, "cdate": 1763079405789, "tmdate": 1763079405789, "mdate": 1763079405789, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DISCO, a reinforcement learning framework aimed to improve text-to-image models’ ability to generate multiple distinct humans. Optimizing for facial diversity, correct person counts, and visual fidelity—without extra annotations—DISCO improves identity separation and sets a new benchmark for multi-human image generation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- multi-human prompt adherence is a failing point of text2image modelling. The method presented is a first attempt attacking this problem with reinforcement learning. The method is sensible and clearly improves on this issue quantitatively\n- experiment section includes ablation study of loss components showing clear contributions from each component"}, "weaknesses": {"value": "- penalizing intra-image facial similarity leads to id divergence might be restrictive at cases, e.g. family pictures, or pictures where we want the same person to appear twice\n- for count control the reward is agnostic of the degree of error, flat zero reward independent of the difference of generated vs requested\n- reproducibilityy is limited, the method is exposed in great detail, however, code and model weights are not provided\n- training data contain images with 2-7 people, does this limit model performance for generating a single person? Evals show results for 1-5 and 2-7 people separately which have great degree of overlap\n- More qualtilative results need to be added to the main body of the paper (currently 5 generations are presented) to showcase improvements and failures for specific cases, e.g. single person generation\n- Limitations should be included in the main paper in my opinion\n\n(Minor minor)\n- repeated \"crop crop\" in l.214"}, "questions": {"value": "- From 3.2 \"(ii) discourage reusing the same identity across samples of the same prompt\" can you explain what benefit this behaviour brings to the trained model? \n- why not try matching face similarities to target images instead of enforcing diversity?\n- does DISCO deteriorate model performance for single person generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Zg1wGXrU3H", "forum": "WSoBhpZpdD", "replyto": "WSoBhpZpdD", "signatures": ["ICLR.cc/2026/Conference/Submission2859/Reviewer_Cang"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2859/Reviewer_Cang"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814013564, "cdate": 1761814013564, "tmdate": 1762916414903, "mdate": 1762916414903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work directly applies GRPO to optimize identity diversity in multi-human generation tasks to resolve collapses such as duplicating faces and merging identities etc. The authors construct a compositional reward function to shape various aspects of multi-human generation and achieves SOTA performance. DisCo appears to be a simple but effective adaptation of RL (GRPO) to multi-human/identity image generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The effectiveness of the compositional rewards is promising, making DisCo outperform GPT. There are many reward terms, but from the reward curves in the Appendix they seem to harmonically improve the model together. Also, the rewards are \"annotation-free\", which is practical and makes the method scalable.\n\n2. The ablation studies with each reward component and the curriculum is meaningful. It shows that using curriculum for stability is indeed very helpful."}, "weaknesses": {"value": "1. The improvement is valuable but the novelty is incremental: the use of RL to enhance image generation [1], as well as a more specified multi-subject/identity/human generation [2] has been proposed before, and similar diversity rewards are designed in previous work. As the authors also discusses and leverages [2], the novelty of this work to me is the reward engineering, which brings predictable improvement. Concurrent work also adopts similar approach for more complex problems like video generation [3]. \n\n2. Then, it would add more value if the authors can provide further insights on analyzing RL algorithms for multi-subject generation, e.g., why GRPO, and comparing between a few PPO-variant RL methods to identify properties/designs that benefit similar tasks, enlightening future work. \n\n[1] Black, Kevin, et al. \"Training diffusion models with reinforcement learning.\" arXiv preprint arXiv:2305.13301 (2023).\n\n[2] Liu, Jie, et al. \"Flow-grpo: Training flow matching models via online rl.\" arXiv preprint arXiv:2505.05470 (2025).\n\n[3] Wu, Tao, et al. \"MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially Disentangled Attention and Identity-Aware Reinforcement Learning.\" arXiv preprint arXiv:2509.21953 (2025)."}, "questions": {"value": "Please see more details in Weaknesses. To me, the main concern is limited novelty, the reward engineering works well, the results on multi-human generation are great, but it has been proven that RL with fine-grained rewards can improve image generation tasks. There also lacks more in-depth analysis of RL in such tasks which could potentially draw new insights and observations, making this work incremental. Thus, I cannot recommend acceptance at this point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IZMTBzWZS2", "forum": "WSoBhpZpdD", "replyto": "WSoBhpZpdD", "signatures": ["ICLR.cc/2026/Conference/Submission2859/Reviewer_jrZd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2859/Reviewer_jrZd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965200939, "cdate": 1761965200939, "tmdate": 1762916414653, "mdate": 1762916414653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}