{"id": "z53s5p0qhf", "number": 9699, "cdate": 1758135260355, "mdate": 1759897703613, "content": {"title": "The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution", "abstract": "Real-world language agents must handle complex, multi-step workflows across diverse applications. For instance, an agent may manage emails by coordinating with calendars and file systems, or monitor a production database like BigQuery to detect anomalies and generate reports following a standard operating manual.  However, existing language agent benchmarks often focus on narrow domains or simplified tasks that lack the diversity, realism, and long-horizon complexity required to evaluate agents' real-world performance. \nTo address this gap, we introduce the Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering diverse applications and tools, realistic environment setup, and reliable execution-based evaluation. Toolathlon spans 32 software applications and 604 tools, ranging from everyday platforms such as Google Calendar and Notion to professional applications like WooCommerce, Kubernetes, and BigQuery.  Most of the tools are based on a high-quality set of Model Context Protocol (MCP) servers that we may have revised or implemented ourselves. Unlike prior works, which primarily ensure functional realism but offer limited environment state diversity, we provide realistic initial environment states from real software, such as multiple Canvas courses each with dozens of students or real-world financial spreadsheets. The Toolathlon benchmark includes 108 manually sourced or crafted tasks in total, requiring interacting with multiple applications over ~20 interaction turns on average to complete.  Each task is strictly verifiable through dedicated evaluation scripts. Comprehensive evaluation of state-of-the-art models highlights their significant shortcomings in performing real-world, long-horizon tasks: the best-performing model, Claude-4-Sonnet, achieves only a 29.9% success rate with 28 tool calling turns on average, while the top open-weights model DeepSeek-V3.1 reaches 13.9%. We expect Toolathlon to drive the development of more capable language agents for real-world, long-horizon task execution.", "tldr": "", "keywords": ["language agents", "tool use", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d24e6b1f00731594d596bb9b40bd31278f3aecb8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces TOOLATHLON, a benchmark designed to evaluate LLMs on complex, real-world tasks using MCP tools. It extends existing MCP benchmarks by incorporating 108 real-world tasks and 604 tools across 32 different software applications. The authors benchmark current LLMs on this dataset and find that state-of-the-art models still perform poorly: the best model, Claude-4-Sonnet, achieves only a 29.9% success rate. These results highlight that long-horizon, multi-application task execution remains a significant challenge for current AI agents."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The benchmark contains a substantially larger number of tasks than prior MCP benchmarks and evaluates models across realistic settings, including real-world state initialization and fuzzy instructions.\n\n2. Rigorous task validation and design make the benchmark reliable and valuable for MCP-agent research.\n\n3. The authors benchmark a wide range of state-of-the-art LLMs and provide detailed analysis, including case studies of failure modes and in-depth discussion of experimental results."}, "weaknesses": {"value": "The current evaluation setup for LLM agents with tools is relatively simple. The authors specify all tools for a task and provide the full tool list to the agent, which can significantly inflate the input context. More sophisticated tool-selection strategies could be explored—for example, using retrieval methods to surface relevant tools dynamically rather than supplying the entire tool inventory upfront."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5JyqRHdvcL", "forum": "z53s5p0qhf", "replyto": "z53s5p0qhf", "signatures": ["ICLR.cc/2026/Conference/Submission9699/Reviewer_EmBX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9699/Reviewer_EmBX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789631684, "cdate": 1761789631684, "tmdate": 1762921207440, "mdate": 1762921207440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Tool Decathlon (TOOLATHLON), a new benchmark designed to evaluate tool-using language agents on tasks that are diverse, realistic, and long-horizon. The authors argue that existing benchmarks are often limited in scope, focusing on narrow domains, single applications, or tasks that lack realistic complexity and environment states. To address this, TOOLATHLON spans 32 software applications (from everyday tools like Google Calendar to professional ones like Kubernetes and BigQuery) and 604 tools, mostly sourced from real-world Model Context Protocol (MCP) servers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A strong benchmark, heavy engineering behind \n- will be good use for the community"}, "weaknesses": {"value": "- Not really a weakness but the paper uses the average number of turns as a proxy for task difficulty. While reasonable, this is an outcome-based metric that can be influenced by the agent's (in)efficiency. A more intrinsic, task-defined complexity metric (e.g., based on the number of required applications, minimum number of steps in a ground-truth trajectory) could provide a slightly more objective measure of difficulty when analyzing performance across Easy/Medium/Hard tasks."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "frFezwLssc", "forum": "z53s5p0qhf", "replyto": "z53s5p0qhf", "signatures": ["ICLR.cc/2026/Conference/Submission9699/Reviewer_aKrQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9699/Reviewer_aKrQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762032929954, "cdate": 1762032929954, "tmdate": 1762921207134, "mdate": 1762921207134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TOOLATHLON (Tool Decathlon) — a benchmark designed to evaluate the performance of language agents on complex, long-horizon, and cross-application tasks. The benchmark covers 108 tasks, 32 applications and 604 tools, ranging from common productivity software (e.g., Notion, Google Sheets, WooCommerce) to domain-specific systems (e.g., Kubernetes, Snowflake). Each of the 108 tasks requires multi-step reasoning, realistic initial environment states, and verifiable execution via deterministic scripts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "+ TOOLATHLON spans a 32 real-world applications demonstrating tool and environment diversity.\n\n+ Execution-Based Evaluation: TOOLATHLON uses deterministic evaluation scripts that compare environment states, ensuring objectivity and reproducibility."}, "weaknesses": {"value": "Benchmark Labeling and Definitions: The categorization of existing benchmarks in Table 1 as “Real Tools” or “Not-Real Tools” appears inconsistent and potentially misleading. For example, τ-Bench is flagged as not-real despite subsets (“airline,” “retail”) interacting with actual databases. Similarly, BFCL is marked not-real, although it supports real execution in the “Execute” category,  with \"Crowd Sourced\" being community-contributed tools. LiveMCPBench is labeled not-real despite its claim to run live MCP tools, while TOOLATHLON’s own use of simulated components (e.g., local Poste.io for Gmail) blurs the same boundary. \n\nNovelty Ambiguity: While TOOLATHLON integrates many existing elements (real MCP servers, realistic states, cross-app tasks), the conceptual novelty beyond combining these components is modest. The work would benefit from a clearer articulation of methodological advances versus engineering scale-up."}, "questions": {"value": "Are the locally containerized applications (e.g., Poste.io for Gmail) treated as “real tools,” and if so, how do they differ philosophically from “simulated” counterparts in other benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LjSClrwTsK", "forum": "z53s5p0qhf", "replyto": "z53s5p0qhf", "signatures": ["ICLR.cc/2026/Conference/Submission9699/Reviewer_VZsp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9699/Reviewer_VZsp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762162254109, "cdate": 1762162254109, "tmdate": 1762921206772, "mdate": 1762921206772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}