{"id": "ulEcmq3IhC", "number": 9291, "cdate": 1758117742967, "mdate": 1759897733119, "content": {"title": "Training-Free Modality-Agnostic Concept Sliders: Fine-Grained Control via Diffusion Models of Images, Audio, and Video", "abstract": "Diffusion models have become state-of-the-art generative models for images, audio, and video, yet enabling *fine-grained controllable generation*, continuously steering specific concepts without disturbing unrelated content, remains challenging. Concept Sliders (CS) offer a promising direction by discovering semantic directions through textual contrasts, but they require per-concept training and architecture-specific fine-tuning (e.g., LoRA), limiting scalability to new modalities. In this work, we introduce a simple yet effective approach that is fully *training-free* and *modality-agnostic*, achieved by partially estimating the CS formula during inference. To support multimodal evaluation, we extend the CS benchmark to include both video and audio, establishing the first multimodal suite for fine-grained concept generation control. We further propose three modality-agnostic evaluation properties along with new metrics that more faithfully and broadly measures the desired properties. Finally, we identify the open problem of scale selection and non-linear traversals and introduce, a two-stage procedure that automatically detects saturation points and reparameterizes traversal for perceptually uniform, semantically meaningful edits. Extensive experiments demonstrate that our method enables plug-and-play, training-free concept control across modalities, improves over existing baselines, and establishes new tools for principled controllable generation.", "tldr": "We propose a training-free, architecture/modality-agnostic method that performs inference-time estimation to enable fine-grained concept control across images, video, and audio.", "keywords": ["Diffusion models", "Training-free methods", "Modality-agnostic", "Semantic control in generative models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7dc6d81526663ed865785ab995353bdbbb2043a5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The user propose a simple training-free and modality-agnostic method for concept slider task. Also, they provide three new metrics for concept slider (CS) for evaluating the range, smoothness and preservation of each method. Besides their inference time method, they also propose a method ASTD, which be used on top of other methods like CS, improve the performance."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Proposed inference-time method is simple, easy to follow, and achieves better scores than CS on the benchmarks proposed by the authors.\n- The evaluation principle (range, smoothness, preservation) for the CS task are reasonable, and using this idea to build a benchmark make sense.\n- Compositional concept slider shown by authors is interesting."}, "weaknesses": {"value": "- The author failed to adequately explore the proposed method in the paper. The authors simply present their method without any explanation or motivation. A simple and effective method is good, but could the authors provide some motivation and ablation for doing so and explain the underlying idea of the method? \n- The method seems trivial. It apply $\\epsilon_{\\theta}(x_t, c_{base}, t)$ at first k steps, and $\\epsilon_{\\theta}(x_t, c_{base}, c_{+}, c_{-}, t)$ at the remaining steps, which all operators are presented in CS except It does not require fine-tuning and use $c_{base}$ for inference at the first K steps. As mentioned above, without proper analysis and motivation, I cannot be convinced that this is better than a method that requires fine-tuning (besides computational cost).Can the author explain why fine-tuning in CS is inefficient, even less so than a inference-time method?\n- As models like clip/vclip often fails to assess fine-grained details about image/video, and sensitive to prompts. Could authors provide some user study about their comparison with other baselines? Also, the alignment between proposed metrics and human's preference should be assessed.\n- Some qualitative cases are not good applications for concept slider, like adding a eyeglasses for a people, it's somehow a yes or no problem, rather than a concept of variability. Could authors provide some explaination about the potential applications of these cases or replace them?"}, "questions": {"value": "- How do author choose K? Is this variable depending on the concept/model architecture/specific model, or is it a constant value?\n- Might be better to adjust the paper organization, move section 5 under method part would be better. Since it's placed after the experimental indicators section, it makes the paper seem somewhat disjointed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ifmCfAnmvM", "forum": "ulEcmq3IhC", "replyto": "ulEcmq3IhC", "signatures": ["ICLR.cc/2026/Conference/Submission9291/Reviewer_871i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9291/Reviewer_871i"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842285818, "cdate": 1761842285818, "tmdate": 1762920932819, "mdate": 1762920932819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a training-free, model-agnostic framework for applying Concept Sliders (CS) at inference time. The core idea is to partition the diffusion timesteps, using the base model for initial semantic generation and then applying a Classifier-Free Guidance (CFG)-like mechanism (positive-negative prediction difference) for the later, concept-forming steps. \nA significant contribution of this work is the proposal of three new metrics to evaluate CS quality, addressing the limitations of existing methods: Conceptual Range (CR), Semantic Preservation (SP), and Conceptual Smoothness (CSM). Building on these metrics, the authors also identify the non-linear nature of concept application and propose a two-stage Automatic Saturation and Traversal Detection (ASTD) method, which reportedly optimizes quality by investing additional computation at inference time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Simple and Intuitive Methodology: The core method for applying CS at inference time is simple and well-motivated. Adapting the CFG mechanism by partitioning timesteps (base-only vs. base+CS) is an elegant way to achieve training-free application.\n\n- Novel and Valuable Metrics: The proposal of the SP, CR, and CSM metrics is a strong contribution to the field. These metrics address clear limitations of existing evaluations (like $\\Delta$ CLIP) by attempting to quantify crucial quality aspects—such as identity preservation, range of concept expression, and smooth transitions—which are vital for usability but often overlooked.\n\n- Insightful Analysis (Non-linearity and ASTD): The paper insightfully identifies and demonstrates the non-linear behavior of concept sliders. Leveraging this observation to create the two-stage ASTD method is a clever extension that shows a deep understanding of the problem.\n\n- Comprehensive Experiments: The method's effectiveness is demonstrated across a comprehensive set of experiments, including image, video, and audio generation, suggesting broad applicability of the core ideas."}, "weaknesses": {"value": "While the contributions are notable, the paper has several weaknesses, primarily concerning practical trade-offs and the scope of validation, which position it as a borderline paper.\n\n1. Significant Inference Cost & Questionable Trade-off: The \"training-free\" claim comes at a very high price. The method requires at least three model evaluations (base, positive, negative) per guided step, introducing significant inference latency and memory overhead. This trade-off is not sufficiently discussed. More critically, the proposed ASTD method is noted to require even more computation than training a CS from scratch. This makes the practical utility of ASTD highly questionable. The paper needs a much stronger justification for why a user would accept this massive inference cost over a one-time training cost.\n\n2. Limited Conceptual Scope: The experiments are constrained to approximately ten concepts (e.g., 'age', 'smile'), which are well-established, common, and primarily relate to direct object manipulation or clear states. The paper lacks experiments on more abstract, nuanced, or rare concepts, which are often the main targets for creative exploration with sliders. This limits the demonstrated generalizability of the proposed metrics and methods.\n\n3. Limited Model-Agnosticism Claim: The experiments appear heavily focused on noise-prediction and v-prediction models. It is unclear if the method directly applies to other diffusion frameworks, such as the increasingly prevalent flow-matching models. This omission weakens the \"model-agnostic\" claim.\n\n4. Minor Issues:\n  - Clarity on Multi-Concept Sliders: The paper demonstrates two-concept sliders, but the mathematical formulation and operational details (e.g., sequential vs. parallel application, potential for order-dependency) are not explained.\n  - Presentation Inconsistencies: There are minor formatting issues, such as inconsistent references (e.g., 'Fig.' vs. 'figures' in Appendix C), missing experimental details (e.g., the exact sampler used), and unclear distinctions between which experiments used SD 1.4 vs. 1.5 (\"some\")."}, "questions": {"value": "I would appreciate it if the authors could clarify the following points in their rebuttal.\n\n### Questions\n1. Metric Formulation: Regarding the 'Overall Score' metric (CR / ($\\epsilon$ + SP) + (1 - CSM)): The choice of $\\epsilon$ = 1 seems disproportionately large compared to SP (which is < 1), potentially distorting the metric by heavily underweighting the Semantic Preservation term. Could the authors provide the rationale for this specific formulation and the value of epsilon? What intuition (e.g., related to saturation) guides this choice?\n\n2. Applicability to Flow Matching: Could the authors comment on the method's applicability to flow-matching models? Specifically, since SD 3 is mentioned (which is a rectified flow model), how was the CFG-like guidance equation (predict_noise(...) - predict_noise(...)) adapted for a framework that predicts velocity?\n\n3. Rationale for ASTD Cost: Given that ASTD is reportedly slower than training a new CS (and much slower than simple inference), what is the practical rationale for its use? In what specific scenario is this extreme trade-off (shifting a one-time training cost to every inference) justified?\n\n4. Multi-Concept Slider Implementation: When applying two sliders simultaneously, how are the positive and negative pairs defined and combined? Is the process order-dependent (e.g., does applying 'age' then 'smile' differ from 'smile' then 'age')? How are potential interferences between sliders managed?\n\n### LLM Disclosure\nI have used an LLM to assist with improving the grammar, clarity, and polishing of this review. The content, analysis, and final judgments are entirely my own."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ldwQuaqIO7", "forum": "ulEcmq3IhC", "replyto": "ulEcmq3IhC", "signatures": ["ICLR.cc/2026/Conference/Submission9291/Reviewer_EsD1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9291/Reviewer_EsD1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970949880, "cdate": 1761970949880, "tmdate": 1762920932482, "mdate": 1762920932482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training-free, modality-agnostic variant of Concept Sliders for fine-grained control in diffusion models. Instead of learning LoRA adapters per concept, the method computes the CS update at inference by doing three forward passes after an initial neutral phase, enabling plug-and-play control across images, video, and audio. The authors also extend Concept Slider benchmarking beyond images to a multi-modal suite, and propose ASTD to locate saturation points and reparameterize traversal for smoother, perceptually uniform edits. Experiments show competitive or better overall scores versus baselines, with modest extra inference cost and no training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea is simple and straightforward. The inference-time estimation of the CS update removes per-concept learning and is architecture- and modality-agnostic; it just needs extra forward passes. This makes the approach useful as models evolve.\n\n2. The authors extend the benchmark to video and audio, arguably the first unified multi-modal suite for fine-grained concept control."}, "weaknesses": {"value": "1. While the author claims a training-free approach for concept slider and showcase applications across different modalities on video and audio, such formulation is actually not new. A related paper [1] also proposes a training-free approach to scale concepts by adjusting the classifier-free guidance during test-time. [1] is also model agnostic as it targets the general noise prediction of diffusion models. Moreover, scaling the loudness of audio and other applications has already been explored by [1]. This paper hasn't incorporated any discussion or comparison experiments on [1], which should be justified in the rebuttal. Otherwise, the contribution wouldn't be as significant as claimed by the authors.\n\n[1] Scaling Concept With Text-Guided Diffusion Models\n\n2. CR and CSM rely on CLIP/ViCLIP/CLAP; metric validity inherits biases/sensitivity of these models. Limited discussion on robustness to the choice of aligner. This is raised briefly in the discussion but not systematically evaluated.\n\n3. For concepts like age, makeup, or waviness/loudness, perceptual smoothness and range are ultimately human-perceived. The paper lacks a human study to validate that lower CSM or higher CR correspond to better subjective control."}, "questions": {"value": "1. How does the choice of intervention step k and CFG scale affect range, smoothness, and preservation? Any simple heuristics for picking k per model/modality?\n\n2. Have you tried region- or attribute-specific preservation instead of pure global preservation?\n\n3. When composing multiple sliders, how do you prevent vector interference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L1P6fnTMgJ", "forum": "ulEcmq3IhC", "replyto": "ulEcmq3IhC", "signatures": ["ICLR.cc/2026/Conference/Submission9291/Reviewer_SUV9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9291/Reviewer_SUV9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975896780, "cdate": 1761975896780, "tmdate": 1762920931844, "mdate": 1762920931844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The original Concept Sliders are only applicable to images and require training LoRA models to learn semantic directions. In this work, the authors propose \"Training-Free Modality-Agnostic Concept Sliders\", a novel approach that directly computes concept directions during the inference phase by leveraging the noise sequences and conditional prompts generated by the base diffusion model, thereby eliminating the need for training. Furthermore, the authors extend this method to multi-modal tasks such as video and audio. The paper also introduces a unified evaluation metric, a reparameterization strategy, and empirically validates the effectiveness and composability of the method across different modalities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors propose a method that eliminates the need to retrain a LoRA model for each individual concept, enabling unified control across images, videos, and audio. This addresses the limitations of traditional concept sliders, which require separate training for each concept and cannot be quickly adapted to new modalities.\n2. To assess fine-grained concept control, the authors extend the original benchmark to include video and audio tasks. They also introduce three modality-agnostic evaluation criteria, facilitating fair comparisons in future research.\n3. The experimental setup is thorough, and the authors have considered the problem from multiple angles, enhancing the credibility of the results.\n4. The paper is well-written and easy to understand."}, "weaknesses": {"value": "1. Although the proposed method no longer requires training a LoRA model for each concept, constructing concept directions still involves multiple noise predictions by the base model. This leads to significant computational overhead, especially in video and audio tasks, where a full diffusion network must be executed at each sampling step.\n2. The dataset remains relatively limited, and the concept control tasks primarily focus on simple attributes. It remains to be validated through further experiments whether the concept sliders can maintain stability when dealing with complex actions or semantic variations.\n3. The ASTD method relies heavily on saturated point detection and traversal-based reparameterization, which appear to be based on empirical heuristics. This may limit its generalizability to different models or concepts."}, "questions": {"value": "1. The paper assumes that the noise difference between positive and negative concepts can effectively represent the concept direction, and that direct sampling and composition without additional training is sufficient. Is there any relevant mathematical theory to support the validity of this assumption?\n2. Can you experimentally demonstrate the method’s performance in scenarios involving multiple overlapping concepts? For instance, when simultaneously controlling attributes such as age, expression, and background, would there be potential interference among these factors?\n3. In terms of quantitative evaluation, could you supplement the results with scores generated by Vision-Language Models (e.g., Gemini or ChatGPT) to further validate the effectiveness of the proposed method?\n4. Is the method applicable and effective when the base model is a diffusion model based on a DiT architecture (e.g., FLUX or Stable Diffusion 3)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k2LAR07HiW", "forum": "ulEcmq3IhC", "replyto": "ulEcmq3IhC", "signatures": ["ICLR.cc/2026/Conference/Submission9291/Reviewer_e12K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9291/Reviewer_e12K"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989510579, "cdate": 1761989510579, "tmdate": 1762920931377, "mdate": 1762920931377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}