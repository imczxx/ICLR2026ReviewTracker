{"id": "GiaF5cFIpI", "number": 22138, "cdate": 1758326621313, "mdate": 1759896884333, "content": {"title": "Adaptive stimulation & response modeling of latent neural dynamics", "abstract": "Latent neural dynamics are a widely used model in neuroscience for describing the time evolution of collective neural activity. These models have been established as useful for neural decoding: for example, latent dynamical models of neural activity give state-of-the art predictions of ongoing kinematics in motor tasks. Despite their utility, the causal mechanisms behind the effectiveness of latent variable models remain poorly understood. To uncover how such latent variables causally encode behaviors, or how they change, would require methods for stimulating neural dynamics during an experiment. Algorithms to drive neural dynamics remain limited, however, due to the need to continually track and respond to changes in neural activity, to account for variation in neural responses under stimulation, and to select useful stimulations to apply from an extensive set of possibilities. Here, we develop a novel streaming method for stimulation-response modeling in affine latent spaces and an optimization framework for selecting high-dimensional stimulation patterns to drive low-dimensional dynamics. Our method integrates streaming latent space construction, an adaptive nonparametric model of the effects of stimulations, and projection maximization under feasibility constraints to determine stimuli that move dynamics along a desired vector. We demonstrate our approach on both simulated and real neural data (calcium fluorescence images, intracortial electrophysiological recordings). We evaluate our method across multiple latent space representations and multiple models of dynamics in parallel, and additionally provide a novel streaming estimator to determine which representation is most predictive of ongoing neural dynamics at any timepoint. This allows for direct comparison between different latent representations and the opportunity for adaptive selection of stimulations to best distinguish amongst neural subspace hypotheses. Finally, we demonstrate algorithm runtimes at faster than real-time speeds ($<$100 ms), making it compatible with future in vivo applications.", "tldr": "", "keywords": ["streaming dimensionality reduction", "neural stimulation", "adaptive experiments"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ebdf8113683f204dccd1bf9cd79569adbd03b2b8.pdf", "supplementary_material": "/attachment/da56bc99861015b4b2aeae0db0e783abd0d516a0.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a streaming method for stimulation-response modeling. To do that, they propose a new real-time subspace identification method building upon jPCA, a kernel-based approach to model stimulus-response effect, and an optimization problem to find the optimal stimulus to perturb the system along the desired direction. They show the proposed approach can indeed perturb the system in a set of feasible directions both in simulations and real datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The formulation of the sparse-input constraint is well-motivated and seamlessly integrated into the overall framework.\n- The proposed streaming subspace identification method achieves performance comparable to proSVD, demonstrating its accuracy and stability in real-time settings.\n- Beyond simulations, the approach is also evaluated on real-world datasets, underscoring its potential utility for practical, real-time applications."}, "weaknesses": {"value": "- Can the authors comment on the necessity of a new subspace identification method for their stimulus optimization approach? At each iteration, do they pick the subspace identification method that yields the best performance? How would the results change if they only use sjPCA or proSVD? \n- The proposed method is pretty much involved in terms of the required steps, such as subspace identification, dynamic model fitting, stimulus map estimation, prediction, and stimulus optimization (and doing all these iteratively in real time). I think a step-by-step, clear guide on the procedure (starting from a specific time t and perturbing the system in the desired direction at time T) would improve the readability of the paper significantly by helping the reader to connect the dots.  \n- Do the proposed stimulus optimization provide convergence to a specific location on the latent space, or just perturbation along a specific axis?\n- For my own understanding, is it correct that updating the $f_t$ and $S_t$ corresponds to fitting new models?\n- The target $v$ is selected based on the identified latent subspace, which does not need to carry a semantic meaning. Without a method to properly identify the map between the latent space and the desired stimulation effect, how can one design the stimulation target? Without such a map, what is the utility of the proposed approach?\n- If the sparsity and/or nonnegativity constraint is relaxed, can the authors perturb the system along infeasible directions?\n- Can you add a legend for Fig. 5 (in addition to mentioning them in the caption)?\n- While computing angles between $s_{obs}$ and $v$, do authors use the $s$ obtained at $t+1$ if the input is applied at $t$? \n- Can you add a legend for Fig. 5 (in addition to mentioning them in the caption)?\n- It looks like the proposed approach is limited to real-time subspace identification methods. With the increased interest in deep-learning-based foundation models, is it possible to combine the proposed stimulus optimization approach with a pretrained dynamical model? Is it possible to test this in a simulation setup with simple linear dynamical models?"}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vyJqGgPJ5K", "forum": "GiaF5cFIpI", "replyto": "GiaF5cFIpI", "signatures": ["ICLR.cc/2026/Conference/Submission22138/Reviewer_739W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22138/Reviewer_739W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760925488451, "cdate": 1760925488451, "tmdate": 1762942083696, "mdate": 1762942083696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Raw neural data is very high dimensional but actual neural activity patterns that occur during behavior tend to lie on a much smaller, low-dimensional surface within that space which is the neural manifold. The authors give an example of how selecting even just 30 neurons involves searching a space of over 10^45 combinations at least. \n\nThe authors explain that prior work has addressed isolated sections of this problem of tracking neural dynamics and designing neural stimulations. This work focuses on bringing down the high dimensionality neural data into low dimensionality latent space in real time (novel streaming method), and then trying to find stimulations that shift this low dimensional space in a particular direction. \n\nNovel streaming method:\nFirst getting the high dimensional data into low dimension through proSVD - the output is a low dimensional state that can update in real time. \njPCA is applied to low dimensional space to figure out the rotational dynamics. doing JPCA will give us the M matrix.\nM can drift or be unstable over time. They stabilize the latent subspace (eigenvectors of M) using the orthogonal procrustes step. \n\nThey compare their method (sjPCA) to existing streaming dimensionality reduction methods (proSVD and mmICA). All of them converge to similar representations as one computed offline. sjPCA performs on par with the existing methods (and also appears to be slightly faster). \n\nModeling:\napplying stimuli to neural responses is non-trivial and to effectively design stimuli in a real-time setting they want to determine the systems responses under multiple conditions. \nFor this they create 3 models, each more complex than the previous one. They start with an instantaneous response model assuming that the neural activity responds to the stimulus immediately in the next time step. Next is delayed response model which introduces delay d before the effect appears. Last, a third model that uses Kernel regression to model the effects of latent state, stimulus, and sample age to create a stimulus response mapping estimator.\n\nThe authors consider a large space of possible stimuli to search for feasible stimulations. They also acknowledge that the tradeoff with such a large space is that any possible solutions may be approximations. The goal is to find a vector u (stimulus) such that vector s (perturbation) aligns closely to vector v (goal). This is shown in figure 2a. \n\nThey try out their methods first on test data, and then real data obtained from calcium imaging with a miniscope. Their model adapts and recovers from externally added perturbation while the non-adaptive model’s error does not decrease."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors identify a potentially interesting problem of how to make experimental perturbations more efficient and better informed by theory."}, "weaknesses": {"value": "I have several major concerns.  The first is that the premise of this framework is built entirely on jPCA, which is a highly specialized method whose only utility is in finding rotational dynamics.  Contrary to the authors' claims, I am not aware of widespread use of jPCA beyond the original paper itself. Instead, the field has mostly adopted more flexible methods that do not only identify rotational dynamics. I don't understand the authors' motivation for starting with jPCA, as it seems they could have used a more general dynamical systems framework (i.e. no constraint on M) ."}, "questions": {"value": "1. Why is jPCA the right starting point, and why is this approach not possible without a constraint on the M matrix?\n2. In figure 1, sjPCA does not appear to be outperforming existing methods. Is this a just a matter of clarifying the presentation or is this an accurate assessment  of model performance?\n3. How does this method perform on synthetic data with ground truth that is not rotational?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fxxY0pYO7Y", "forum": "GiaF5cFIpI", "replyto": "GiaF5cFIpI", "signatures": ["ICLR.cc/2026/Conference/Submission22138/Reviewer_UUNK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22138/Reviewer_UUNK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876264630, "cdate": 1761876264630, "tmdate": 1762942083222, "mdate": 1762942083222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a real-time framework for adaptive stimulation and response modeling of latent neural dynamics. It introduces a streaming method for constructing latent neural manifolds, a nonparametric kernel-based model for stimulus-response mapping, and an optimization approach to design high-dimensional stimuli that drive low-dimensional neural dynamics in desired directions. The method is evaluated on both simulated and real neural datasets and achieves fast runtime (<100 ms), making it suitable for future closed-loop experiments."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The integration of streaming latent space estimation, stimulus-response modeling, and stimulus optimization is innovative.\n* The framework is comprehensive, covering multiple latent representations, dynamical models, and real-world constraints (e.g., non-negativity, sparsity).\n* The method is computationally efficient and demonstrated to run in real time, which is critical for in vivo applications.\n* Experiments and analyses are comprehensive, with both synthetic data and real-world neural data. The results are promising."}, "weaknesses": {"value": "* The presentation of the paper is not clear.\n* The impact of stimulations on behavior is not modeled or discussed, limiting the interpretability of stimulation effects in behavioral contexts."}, "questions": {"value": "* Is there any comparison regarding the time complexity of the three methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Wr0J8m9XMr", "forum": "GiaF5cFIpI", "replyto": "GiaF5cFIpI", "signatures": ["ICLR.cc/2026/Conference/Submission22138/Reviewer_ybhx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22138/Reviewer_ybhx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988202386, "cdate": 1761988202386, "tmdate": 1762942083004, "mdate": 1762942083004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a real-time stimulation framework targeting alterations in neural population / latent state space. The framework requires 1) an online system identification component, which the authors propose a novel, realtime variant of jPCA to test along online versions of SVD and ICA, 2) a state estimation / prediction component, such as Kalman Filter, 3) a learnable stimulation-response model, which is taken to be kernel regression, and 4) a optimization routine to find the best stimulation pattern given a goal perturbation direction in state space. The proposed method was tested on a synthetic rotational system, and two experiments in which simulated perturbations were added to offline recorded neural data."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- the problem of realtime perturbation in neural state space is clearly motivated, important, and systematically described\n- the proposed framework and its various modules is comprehensive and intuitively appealing\n- the paper is clearly and concisely written overall"}, "weaknesses": {"value": "- the modeled perturbation in both the synthetic system and real data as a independent increase and stationary decay seem too simplistic, and it was a bit confusing in the different experiments whether the simulated stimulation effect is applied to the data space or the latent space.\n- “real data” experiments also only consisted of simulated perturbations with simple decaying responses added onto real data, not actual perturbation experiments, which is not mentioned until 5 and a half pages in. Critically, it’s unclear how the natural evolution of neural populations in the data interacts with the intended / feasible directions in the stimulation experiments (e.g. lines 410-423).\n- the key contributions were diluted with the different novel components introduced. For example, it was unclear whether the streaming jPCA method / rotational latent model was necessary or performed better than the other two models, or if in the experiments the 3 models were always used, and if so, when the detected transitions were.\n- overall, I believe the current experimental design, in particular with the simulated perturbations on real data, does not demonstrate the validity nor value of the proposed framework, nor is it clear the key contributions of the various components and their respective importance for the whole framework."}, "questions": {"value": "- how stable is the online centering, i.e., mean/std estimation, for the various modalities of neural data?\n- Fig1c shows that the space most likely to give the best predictive probability (bright colors) coincide with low-magnitude parts of the flow field in 1b. Essentially, the parallel estimates of each system online gives the most likely system as the one with the least movement, which doesn’t seem ideal? Am I misinterpreting something here?\n- what happens if in the synthetic system the stimulation acts on multiple coordinates instead of just x3 (eq. 9)?\n- in section 4.2 / figure 4, what did the algorithm-designed stimulations look like? Given that the first PC (Q0) was the target, stimulating random individual / groups of neurons seems to be an inappropriate hypothesis. Intuitively, the naive hypothesis would be that stimulation patterns closely resembling the loading vectors would be maximally effective. Was this the case or tested?\n- line 417: it was found that some random directions were easy whereas others were not, did the “easy” directions coincide with the real data evolution at the time? In other words, was it easier to induce a movement in the direction that the data was naturally moving towards in the first place?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ekGmNEKtXY", "forum": "GiaF5cFIpI", "replyto": "GiaF5cFIpI", "signatures": ["ICLR.cc/2026/Conference/Submission22138/Reviewer_JwEk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22138/Reviewer_JwEk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762128113106, "cdate": 1762128113106, "tmdate": 1762942082741, "mdate": 1762942082741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}