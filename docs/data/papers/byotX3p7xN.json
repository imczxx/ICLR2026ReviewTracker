{"id": "byotX3p7xN", "number": 6715, "cdate": 1757993267779, "mdate": 1759897899177, "content": {"title": "Personalized and Temporal-aware Attention for Efficient Generative Recommendation", "abstract": "Recent discoveries of scaling laws in autoregressive and large generative recommendation models have attracted considerable attention in sequential recommendation. Larger models entail increased inference latency and training costs, while most recommendation applications have stringent latency constraints. Through empirical evaluations of recent self-attention-based recommendation models, we identified two critical issues that impair model performance while increasing computational costs: inadequate modeling of temporal information and KV cache redundancy. In this paper, we propose a novel method, termed Personalized and Temporal-aware Transformer for generative recommendation (PT-Recformer), to effectively mitigate these issues. On the one hand, we introduce a novel Rotary Temporal Encoding method to effectively model extreme temporal variations inherent in user contexts. On the other hand, we propose a personalized multi-head latent attention mechanism to enhance expressive power by integrating personalized and temporal features and significantly reduce KV cache costs. Comprehensive experiments conducted on several benchmark datasets demonstrate that PT-Recformer consistently outperforms state-of-the-art baselines with only 12\\% of the KV cache storage required by vanilla self-attention based models. Online A/B testing on a commercial platform has validated the effectiveness of the PT-Recformer in large-scale industrial settings, resulting in a 3.13\\% increase in effective Cost Per Mille. Our codes are available at \\url{https://anonymous.4open.science/r/GRec-B386}.", "tldr": "", "keywords": ["Sequential Recommendation", "Generative Recommendation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa9acc82ca4870eb8afd97326f145bb1fae262a5.pdf", "supplementary_material": "/attachment/9641df9ef82abbed25af612adc135020e14af90e.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces PT-Recformer, a personalized and temporal-aware Transformer model for generative recommendation, addressing inefficiencies in self-attention-based sequential recommendation (SR) systems. It identifies two critical issues: inadequate temporal information modeling and KV cache redundancy. The proposed solution includes Rotary Temporal Encoding (RoTE) for handling wide-ranging time intervals and a Personalized Multi-head Latent Attention (PMLA) mechanism to integrate user-specific features, reducing KV cache usage by 88%. Experiments on benchmark datasets (Amazon Books, Movies & TV, MovieLens-20M) and online A/B testing show superior performance and a 3.13% eCPM increase, but the methodology and claims require rigorous scrutiny."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Originality**: Attempts to tailor self-attention for SR with RoTE and PMLA, though heavily derivative.\n- **Quality**: Demonstrates some performance gains (e.g., 15.5% over HSTU-large on Books), but experimental design flaws temper reliability.\n- **Clarity**: Figures provide visual support, though annotations are inadequate.\n- **Significance**: Addresses practical efficiency concerns, but the solution’s novelty and scalability are questionable."}, "weaknesses": {"value": "- **Methodological Flaws**: The temporal interval analysis (e.g., 45-day gaps in Amazon Books) lacks statistical significance testing, and RoTE’s logarithmic bias (Equation 2) is arbitrarily parameterized without optimization studies.\n- **KV Cache Claims**: The 88% reduction is based on a fixed window size (w) without exploring dynamic adaptation, potentially sacrificing long-range context accuracy. No comparison with state-of-the-art memory optimization techniques (e.g., sparse attention) is provided.\n- **A/B Testing**: The 3.13% eCPM increase lacks transparency on sample size, test duration, or user segmentation, rendering it inconclusive.\n- **Reproducibility**: Hyperparameters (e.g., β, γ, w) are not systematically tuned or reported, and code availability (anonymous link) cannot be verified pre-publication.\n- **Oversight**: Ignores potential biases in dataset temporal distributions and their impact on model performance."}, "questions": {"value": "1. Can the authors provide statistical tests (e.g., t-tests) to validate the significance of temporal interval differences across datasets, and justify the choice of β in Equation 2?\n2. How does PT-Recformer’s accuracy degrade with varying window sizes (w) in PMLA, and why was no dynamic window adjustment tested against sparse attention methods?\n3. What was the A/B test’s sample size, duration, and control group design to support the 3.13% eCPM claim, and how were confounding factors (e.g., seasonality) mitigated?\n4. Why were hyperparameters (β, γ, w, τ) not optimized via grid search or reported with sensitivity analysis, and how does this affect reproducibility?\n5. Can the authors compare PT-Recformer’s KV cache efficiency with recent memory-optimized Transformers (e.g., Longformer, Performer) to substantiate the 88% reduction claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lEctuUzkUc", "forum": "byotX3p7xN", "replyto": "byotX3p7xN", "signatures": ["ICLR.cc/2026/Conference/Submission6715/Reviewer_tKAs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6715/Reviewer_tKAs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761371994990, "cdate": 1761371994990, "tmdate": 1762919005742, "mdate": 1762919005742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a personalized and temporal-aware transformer for generative recommendation. They introduce a rotary temporal encoding method to model the temporal variations inherent in user contexts, and a personalized multi-head latent attention mechanism to enhance the expressive power. They demonstrated its performance using both public benchmarks and a real-world online platform and compared it with other generative recommendation methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper considers the temporal information of each sequence in the sequence recommendation task and innovatively proposes rotary temporal encoding to model temporal information.\n\n2. This article found that only the closest items are focused on in self-attention from the perspective of KV cache cost, so personalized multi-head latent attention was designed.\n\n3. The authors tested PT-Recformer's performance and compared it with previous GR methods."}, "weaknesses": {"value": "1. While PT-Recformer claims to model temporal information, there is no actual visualization of Rotary Temporal Encoding (RoTE).\n\n2. There are several typos throughout the manuscript; ones I noticed include:\n      - γ is a scalar hyperparameter and should be expressed as “⊗” or simply “·” to indicate element-wise scaling.\n      - The dimension of $b_q$ in Eq. (7) should be $b_q \\in \\mathbb{R} ^{n_h d^R_h}$?\n      - The writing and certain claims need refinement.\n\n3.  Many modules in the paper appear to be incrementally stacked. For example, ROTE is a temporal extension of ROPE, and sliding window attention directly inherits Longformer. The multi-head attention mechanism does not seem to have any fundamental innovations. Can it really bring diversity to recommendations?"}, "questions": {"value": "1. HSTU and other structures can unify the **Retrieval** and **Ranking** tasks. How about PT-Recformer？\n2. Whether the dimension of $b_q$ in Eq. (7) should be $b_q \\in \\mathbb{R} ^{n_h d^R_h}$?\n3. Does Personalized Multi-head Latent Attention bring diversity to recommendations? Are there any experimental results to support this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7jIL24PRS0", "forum": "byotX3p7xN", "replyto": "byotX3p7xN", "signatures": ["ICLR.cc/2026/Conference/Submission6715/Reviewer_7C7b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6715/Reviewer_7C7b"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636215220, "cdate": 1761636215220, "tmdate": 1762919004468, "mdate": 1762919004468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper has focused on the sequential recommendation field. The authors found that inadequate modeling of temporal information and KV cache redundancy may impair the performance of the self-attention-based SRS model and lead to inefficiency challenges. To address these issues, this paper proposed a novel rotary encoding method to fully integrate temporal information and a personalized multi-head latent attention mechanism to relieve the inefficiency issue. Extensive experiments validated the effectiveness and efficiency of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ S1. This paper is well-organized and -written, making it easy to follow.\n+ S2. Extensive experiments have been conducted.\n+ S3. The code is released, making it easy to reproduce."}, "weaknesses": {"value": "- W1. The illustration that HSTU and Fuxi-$\\alpha$ fail to fully leverage the temporal information is arbitrary. More analysis is needed to validate such a conclusion.\n- W2. The term \"generative recommendation\" used in this paper does not seem accurate, which may confuse readers. In general, the generative recommendation refers to the model that aligns tokens of items with natural language and outputs the semantic ID of items, like TIGER [1]. The paper proposed in this paper belongs to discriminative recommendation.\n- W3. Though this paper has addressed the efficiency issue in sequential recommendation, many recent efficient sequential recommendation baselines have been ignored, such as.\n- W4. The experiments on training and inference time should include the baseline model Hydra, because it often achieves comparable performance.\n- W5. I found the training objective of this paper is InfoNCE, which is different from the objective of Fuxi-$\\alpha$ and HSTU. Different training objectives may lead to unfair comparison.\n\n\n\n[1].Rajput, Shashank, et al. \"Recommender systems with generative retrieval.\" *Advances in Neural Information Processing Systems* 36 (2023): 10299-10315.\n\n\n\n[2]. Li, Muyang, et al. \"MLP4Rec: A pure MLP architecture for sequential recommendations.\" *arXiv preprint [arXiv:2204.11510](https://arxiv.org/abs/2204.11510)* (2022)."}, "questions": {"value": "All my questions have been included in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MNCEmPkXDi", "forum": "byotX3p7xN", "replyto": "byotX3p7xN", "signatures": ["ICLR.cc/2026/Conference/Submission6715/Reviewer_iSR3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6715/Reviewer_iSR3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664470600, "cdate": 1761664470600, "tmdate": 1762919004076, "mdate": 1762919004076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PT-Recformer, a novel transformer-like architecture for sequential recommendation that addresses two inefficiencies in current self-attention based models: inadequate temporal modeling and KV cache redundancy. The authors introduce Rotary Temporal Encoding (RoTE) to handle temporal variations in user interaction sequences (ranging from seconds to years), and Personalized Multi-head Latent Attention (PMLA) that integrates user profile features while reducing KV cache costs through low-rank compression and sliding window attention. The method is evaluated on three public benchmarks and deployed in a commercial system, showing improvements over SOTA baselines with as little as 12% of the KV cache storage and 75% training time of vanilla self-attention models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "S1: Problem motivation: The paper provides empirical evidence (Figure 1) showing that self-attention models focus predominantly on the last few items, leading to massive KV cache redundancy.\n\nS2. Technical contributions: The paper makes the following technical contributions, including a/ RoTE with learnable frequencies, an elegant solution for handling extreme temporal ranges in recommendation data, b/ Integration of personalized features directly into attention mechanisms (rather than late fusion), and c/ Combination of low-rank compression with sliding window attention to reduce KV cache sizes\n\nS3. Comprehensive experimental validations: a/ The authors conduct extensive experiments on three public benchmarks with consistent improvements, with thorough ablation studies (eg Table 4). b/  The authors deployed the solution in real-world settings, with A/B testing showing 3.13% eCPM improvement.\n\nS4. Overall, the method seems like a strong improvement over current SotA, with as little as 12% KV cache, 75% training time, and faster inference, which is significant for practical deployment."}, "weaknesses": {"value": "W1: Limited novelty in individual components: While the combination is novel, individual components (sliding window attention, low-rank compression, RoPE) are well-established techniques. The primary contribution of this paper appears to be in their adaptation and combination for recommendation.\n\nW2. Design\n* Despite observation of \"self-attention models focusing predominantly on the last few items\", the optimal hyperparameter for ML-20M is 100, suggesting that sliding window may not be that effective.\n* While PMLA is a step forward on top of MLA, the usage of user profile tokens can introduce leakage.\n* The temporal relative attention bias mechanism in HSTU can be combined with PMLA too. What are the tradeoffs of (temporal aware) RAB vs RoPE on top of PMLA?\n\nW3. Experiment comparisons\n* Table 1 comparison is misleading; we should not compare sliding window style attention and full attention in the same table, as one can easily integrate sliding window attention into various baselines (eg SASRec, HSTU, etc).\n* Table 3: 0.37B model being almost as fast as 1B HSTU models suggest PT-Recformer implementation may not be sufficiently optimized?\n* While the solution is deployed in production, additional details for baseline/test would be helpful - eg scale of baseline model, distribution of sequence lengths, model refresh frequencies, whether dataset consists of news-like items with shorter timespans or retail with seasonal patterns, etc. \n\nW4. Writing\n- Incorrect citations. https://arxiv.org/abs/2111.11294 was the first work to study scaling laws in recommendations. Zhang et al. (2023)'s work came much later.\n- Figure 1 and Figure 2(b) are hard to read\n- Table 2 suggests that the results are accurate up to .00001 Recall@K but I doubt this is the case. Why not report variance given you've already conducted multiple runs?\n- Mathematical notation could be cleaner\n- Typos: Time interval resolution factor β is set 6.7 -- ??\n- Formatting in various Tables could be improved (eg 7169083s in Table 9 should be converted to hours)"}, "questions": {"value": "Please see above. Addressing issues in W2/W3/W4 would be especially helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dtSK3fMln4", "forum": "byotX3p7xN", "replyto": "byotX3p7xN", "signatures": ["ICLR.cc/2026/Conference/Submission6715/Reviewer_xc4X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6715/Reviewer_xc4X"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762653264497, "cdate": 1762653264497, "tmdate": 1762919003149, "mdate": 1762919003149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}