{"id": "jSt7oxzJxI", "number": 5349, "cdate": 1757903288731, "mdate": 1759897980333, "content": {"title": "Benchmarking and Enhancing Rational Preference Utilization for Personalized Assistants: A Pragmatic View", "abstract": "Large language model (LLM)-powered assistants have recently integrated memory mechanisms that record user preferences, leading to more personalized and user-aligned responses.\nHowever, the dual effects of personalization remain underexplored, and its adverse consequences are especially salient in real-world applications.\nTo address this gap, we propose Rational Personalization Acts, which reformulates memory utilization as a problem of pragmatic intent reasoning.\nBuilding on this perspective, we develop **RPEval**, a benchmark comprising a personalized intent reasoning dataset and a multi-granularity evaluation protocol.\nRPEval not only reveals the widespread phenomenon of irrational personalization in existing LLMs, but also, through a novel error pattern analysis, illustrates how irrational personalization can undermine user experience.\nFinally, we introduce RP-Reasoner, which treats memory utilization as a pragmatic reasoning process, enabling the selective integration of personalized information. Experimental results demonstrate that our method significantly outperforms carefully designed baselines on \\textsc{RPEval}, and resolves 80\\% of the bad cases observed in a large-scale commercial personalized assistant, highlighting the potential of pragmatic reasoning to mitigate irrational personalization. Our benchmark is publicly available at \\url{https://anonymous.4open.science/r/RPEval-E4B0}.", "tldr": "We propose RPA, a framework with a benchmark and method to analyze memory’s dual effects and enable rational personalization.", "keywords": ["LLM", "Personalization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/82b06332f351d93c95e8dff99b0bab6f05a75aa9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work addresses the question of how LLMs should incorporate user-specific memories, arguing that naively applying personalization can lead to poor outputs.  To study this problem, the authors introduce a synthetic benchmark of underspecified user queries labeled according to intent.  The dataset is constructed through multi-stage LLM generation, and evaluated primarily using an LLM judge.  To help the LLM address this query underspecification problem, the authors proposed a method, RP-Reasoner, a heuristic prompting scheme inspired by Rational Personalization Acts that combines a query likelihood term with an intent prior to decide the appropriate personalization level based on the inferred user intent.  Experiments suggest that this method outperforms simpler prompting baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-motivated and timely.  LLM personalization is an area of great interest to the community, and the authors rightly point out that existing approaches are largely naive, and better algorithms are needed for true contextual personalization."}, "weaknesses": {"value": "My major concern is that the paper is organized around the idea of RPAs, but I am not sure what new insights are gained from taking this viewpoint.  The core claim, that personalized systems should not blindly apply stored preferences, but instead should infer intent and use context to decide whether/what to personalize, is a long-standing theme in recommender systems and LLM personalization.  For example, here is a survey paper on context-aware recommender systems from 2011 with over 3K citations: https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2364.  I think L2 personalization is always taken for granted as the goal.\n\nAlso, I find that the description of “memory utilization” does not really align with the task or proposed method; this is more about properly conditioning on a persona.  There are many interesting questions around how to use a constantly evolving memory store of past user interactions (from this and other users), but this work does not grapple with that.\n\nI was excited when I read the first sentence of the motivation, that “This work centers on the duality of personalization, particularly the potential risks.”  However, I don’t think the issues addressed in this paper are truly risks, they’re just cases of bad personalization.  When I think of the risks of personalization, I think of addiction, sycophancy, and a range of other unhealthy feedback loops and phenomena.  It would have been interesting to see the paper focus on some of these real risks.\n\nI am unconvinced that the LLM judge has been thoroughly validated.  Figure 3c is underexplained, and from what I can tell agreement levels are not that high.  What check or significance test is done here?  I looked at examples in the appendix, and I actually disagreed with the filter bubble example (why can’t the parent want the child to have a small serving of protein with their vegetables?), so I am worried about how well evaluation might function here.\n\nI am also unconvinced that baselines are very competitive.  How was the CoT prompt optimized?  It seems like a carefully prompted reasoning model should be competitive at this.  Overall, it is hard to draw strong conclusions from the experiments; while the proposed method performs best, it is somewhat unsurprising given that it is bespoke to the unique benchmark created by the authors.  I am not sure of the broad applicability of this method."}, "questions": {"value": "- What significance testing was done w.r.t. agreement between LLM judge and human annotions?\n - How was the CoT prompt optimized?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GNfFyzmM7c", "forum": "jSt7oxzJxI", "replyto": "jSt7oxzJxI", "signatures": ["ICLR.cc/2026/Conference/Submission5349/Reviewer_Gvb6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5349/Reviewer_Gvb6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760727862795, "cdate": 1760727862795, "tmdate": 1762918018484, "mdate": 1762918018484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles over-personalization in LLM assistants—when stored preferences get applied even when they shouldn't be. The authors propose RPA (a pragmatic framework), RPEval (a benchmark with 8K samples), and RP-Reasoner (a Bayesian reasoning method). Results show current LLMs struggle badly at deciding when to ignore preferences, with RP-Reasoner bringing ~35% improvement."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Important problem: Over-personalization is a real issue that hasn't gotten enough attention. The sleep music example in Figure 1 perfectly illustrates why this matters.\n2. Well-designed benchmark: The preference inversion strategy is clever—generating queries first, then creating preferences that should/shouldn't apply. The error taxonomy (FB, RII, UPB, etc.) is also useful for understanding failure modes.\n3. Strong empirical results: 80% resolution on real commercial system bad cases is impressive and shows practical value.\n4. Interesting finding about model scale: The counterintuitive result that stronger models (GPT-5) can be worse at ignoring irrelevant preferences is worth highlighting."}, "weaknesses": {"value": "1. Heavy GPT-4 dependency: The whole dataset comes from GPT-4.1 generation. This feels circular when you're then evaluating GPT-4.1/GPT-5 on it. How do you know the benchmark doesn't just measure \"how well does model X mimic GPT-4's personalization decisions\"? Would've been better to ground this in real user interaction data.\n2. Subjectivity issue not fully resolved: The paper acknowledges when to apply preferences is subjective, but doesn't provide inter-annotator agreement scores. Also, only ~1K samples get human annotation—what about the other 7K?\n3. Still far from human performance: On Single.ALL, best result is 0.77 vs 0.95 for humans. What's causing this gap? The paper doesn't dig into what RP-Reasoner still gets wrong."}, "questions": {"value": "1. What's the inter-annotator agreement? How were disagreements handled?\n2. Computational cost: how much slower/expensive is RP-Reasoner vs baselines?\n3. The \"Ignore\" intent is hardest for models—why? Is there something fundamental about LLMs that makes them reluctant to discard context?\n4. How would this work with actual user behavior data instead of synthetic scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EiWFIRIubk", "forum": "jSt7oxzJxI", "replyto": "jSt7oxzJxI", "signatures": ["ICLR.cc/2026/Conference/Submission5349/Reviewer_tTCm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5349/Reviewer_tTCm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044679276, "cdate": 1762044679276, "tmdate": 1762918018180, "mdate": 1762918018180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Summary:**\n\nThe paper addresses the problem of *over-personalization* in LLM-based assistants and seeks a *rational equilibrium*, conceptually a Pareto-optimal balance, between personalization and generalization. It frames personalization as a **multi-objective reasoning task**, proposing the **Rational Personalization Acts (RPA)** framework, the **RPEVAL** benchmark, and the **RP-Reasoner** model, which performs pragmatic inference to decide *when and how* to use memory."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "### **Strengths**\n\n* **Timely and Well-Motivated Problem Setting.**\n  The paper addresses an important and underexplored challenge in LLM-based personalized assistants—how to balance personalization and generalization by reasoning about *when and how* to apply user memory. The framing of personalization as a multi-objective pragmatic reasoning problem is both novel and relevant to current trends in LLM alignment.\n\n* **High-Quality Benchmark (RPEVAL).**\n  The **RPEVAL** This dataset can serve as a reusable diagnostic tool for evaluating rational memory utilization across future LLMs.\n\n* **Effective and Practical Solution (RP-REASONER).**\n  The proposed **RP-REASONER** model demonstrates large and consistent gains over strong baselines—improving intent prediction accuracy by roughly **35%** and reducing error severity by **26%**. Moreover, the finding that it resolves **≈80% of bad cases** in a real commercial assistant underscores its potential practical value and real-world applicability."}, "weaknesses": {"value": "**Weaknesses:**\n\n1. **Oversimplification of intent categories**\n   The three-way scheme {Ignore, Support, Dominate} is a substantial simplification of real user needs. In practice, intentions are often multi-faceted, evolving, and context-dependent (e.g., conflicting or partially overlapping preferences).\n\n2. **Unclear baseline motivation (Vanilla, Reminder, CoT)**\n   The paper does not clearly justify why only Vanilla, Reminder, and CoT are used as baselines. It would strengthen the work to explain why **more advanced training-free approaches** and **stronger prompting ensembles** (e.g., self-consistency, self-refine/verify) are omitted. Without this rationale, it remains unclear whether the reported gains derive from the proposed pragmatic reasoning itself or from a limited baseline set.\n\n3. **Conceptual overlap with prior work**\n   The problem formulation is closely related to [1], which likewise argues that LLMs should not naively trust historical personalization and must continually detect and adapt to shifting user preferences. Both are **training-free, inference-time** frameworks that dynamically correct misalignment between user preferences and model behavior. The paper should explicitly compare and position its contribution relative to [1].\n\nReference\n[1] Unlearning Misalignment for Personalized LLM Adaptation via Instance-Response-Dependent Discrepancies (TMLR 2025)."}, "questions": {"value": "Question 1 (Baselines & Rationale)\nYour baselines focus on prompting (Vanilla/Reminder/CoT). Why were advanced training-free approaches like [1] not included, and how would RP-Reasoner compare to stronger prompting ensembles (e.g., self-consistency [2], self-refine [3])?\n\nQuestion 2 (Comparison & Positioning)\nRational Preference Utilization performs inference-time pragmatic reasoning in intent space to regulate memory usage. This is conceptually similar to [1], which performs training-free, inference-time discrepancy unlearning via probabilistic marginalization in response space.\nCould you include a comparison with [1] and discuss where RP-Reasoner is preferable (e.g., interpretability, latency, robustness to stale or contradictory memories) and where [1] is stronger?\n\nReferences\n\n[1] Unlearning Misalignment for Personalized LLM Adaptation via Instance-Response-Dependent Discrepancies (TMLR 2025).\n\n[2] Self-Consistency Improves Chain of Thought Reasoning in Language Models.\n\n[3] Self-Refine: Iterative Refinement with Self-Feedback."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0VRoWXNnpR", "forum": "jSt7oxzJxI", "replyto": "jSt7oxzJxI", "signatures": ["ICLR.cc/2026/Conference/Submission5349/Reviewer_vGZm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5349/Reviewer_vGZm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762120256324, "cdate": 1762120256324, "tmdate": 1762918017923, "mdate": 1762918017923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}