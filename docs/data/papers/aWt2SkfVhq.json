{"id": "aWt2SkfVhq", "number": 10617, "cdate": 1758177623512, "mdate": 1759897640314, "content": {"title": "Active Attacks: Red-teaming LLMs via Adaptive Environments", "abstract": "We address the challenge of generating diverse attack prompts for large language models (LLMs) that elicit harmful behaviors (e.g., insults, sexual content) and are used for safety fine-tuning. Rather than relying on manual prompt engineering, attacker LLMs can be trained with reinforcement learning (RL) to automatically generate such prompts using only a toxicity classifier as a reward. However, capturing a wide range of harmful behaviors is a significant challenge that requires explicit diversity objectives. Existing diversity-seeking RL methods often collapse to limited modes: once high-reward prompts are found, exploration of new regions is discouraged. Inspired by the active learning paradigm that encourages adaptive exploration, we introduce \\textbf{Active Attacks}, a novel RL-based red-teaming algorithm that adapts its attacks as the victim evolves. By periodically safety fine-tuning the victim LLM with collected attack prompts, rewards in exploited regions diminish, which forces the attacker to seek unexplored vulnerabilities. This process naturally induces an \\emph{easy-to-hard exploration curriculum}, where the attacker progresses beyond easy modes toward increasingly difficult ones. As a result, Active Attacks uncovers a wide range of local attack modes step by step, and their combination achieves wide coverage of the multi-mode distribution. Active Attacks, a simple plug-and-play module that seamlessly integrates into existing RL objectives, unexpectedly outperformed prior RL-based methods—including GFlowNets, PPO, and REINFORCE—by improving cross-attack success rates against GFlowNets, the previous state-of-the-art, from 0.07\\% to 31.28\\% (a relative gain greater than 400×) with only a 6\\% increase in computation.", "tldr": "RL-based red-teaming with evolving environments by safety fine-tuning victim LLM to promote easy-to-hard exploration.", "keywords": ["Large Language Models", "AI Safety", "Red-teaming", "Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ef355cbaca6682b27275e9fc190f5152aa7bfaa9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Active Attacks, a new method for automatically testing and improving the safety of LLMs. The key idea is to train an attacking model using reinforcement learning that adapts as victim model becomes safer over time. Unlike previous “static” methods where the attacker keeps exploiting the same weaknesses, here the victim is periodically fine-tuned on discovered attacks, which forces the attacker to look for new, harder-to-find issues. This creates a kind of natural “easy-to-hard” training process. The results show that Active Attacks discover more diverse and effective harmful prompts than earlier techniques like PPO or GFlowNets, while using only slightly more computation. It also improves the robustness of safety fine-tuning and maintains normal instruction-following ability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1.\tThe idea of making the environment adaptive in red-teaming is simple but clever. It brings active learning principles into RL-based safety testing, which hasn’t been explored much before.\n2.\tThe experiments are strong. The authors compare against several baselines and show clear improvements. The ablation and transfer results  make the method look reliable.\n3.\tThe paper is well organized and easy to follow. The figures help explain how the attacker and victim interact. Algorithm 1 clearly shows how Active Attacks is implemented.\n4.\tThis approach could make automated safety testing much more efficient and realistic. It’s easy to add to existing pipelines and doesn’t hurt general model performance."}, "weaknesses": {"value": "1.\tThe paper uses toxicity classification as the main measure of “harm,” which is narrow. It would be stronger to include other safety aspects such as misinformation or bias.\n2.\tThe success of Active Attacks depends heavily on how good the toxicity classifier is. If the classifier misses certain harms, the attacker might never explore them.\n3.\tWhile the paper shows good quantitative results, there are few deep analyses of what kinds of new vulnerabilities are discovered.\n4.\tIt’s unclear how scalable this method is for very large models in real-world settings. The experiments use models up to 7B parameters, but practical deployment may require testing on much larger systems."}, "questions": {"value": "1.\tHow sensitive is the method to the choice of fine-tuning frequency? Would a wrong schedule reduce diversity?\n2.\tCould this adaptive process cause “forgetting” of earlier attack types if the victim model is fine-tuned repeatedly?\n3.\tCan the method handle multi-turn or conversational jailbreaks, not just single-prompt attacks?\n4.\tWould the approach still work if the classifier has noise or bias?\n5.\tCould the authors discuss any ethical safeguards or limitations when using RL to generate harmful prompts？"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SqevJSxxrr", "forum": "aWt2SkfVhq", "replyto": "aWt2SkfVhq", "signatures": ["ICLR.cc/2026/Conference/Submission10617/Reviewer_gtj7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10617/Reviewer_gtj7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761401045721, "cdate": 1761401045721, "tmdate": 1762921877820, "mdate": 1762921877820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear reviewers, \n\nWe thank all reviewers for their thoughtful and constructive feedback. We have already responded individually to your feedback, and here we summarize the main changes in the paper, share additional experimental results, and clarify some possible misunderstandings.\n\n**Summary of main changes**\n- **Related work**: We added additional discussion on the difference between jailbreaks and automated red-teaming.\n- **Limitation**: We added additional discussion on the toxicity classifier we used for training the attacker LM.\n- **Ethical Statement**: We added additional discussion on ethical aspects related to using RL to generate harmful prompts.\n\n**New results**\n- **Appendix A.5**: In response to the feedback from T5ie, we conducted additional experiments and confirmed that our method does not suffer from over-refusal.\n- **Appendix A.6**: In response to the feedback from T5ie and uwwW, we conducted experiments on cross attack success rate between GCG and GFlowNet + Active Attacks to validate the advantages of automated red-teaming against jailbreaks.\n- **Appendix B.5**: In response to the feedback from uwwW, we included additional example responses from safety fine-tuned victim LLM to validate the defense of our method.\n\nWe have highlighted every revised section in purple.\n\n**Difference between jailbreaking and automated red-teaming**\n- **Jailbreaking** is the process of discovering a prompt or interaction pattern that reliably bypasses a target LLM’s safety and alignment mechanisms, causing it to produce content or behaviors that its policies are meant to restrict. A jailbreak is a unique adversarial exploit of the model’s learned behavior, and may or may not transfer across models.\n- **Automated red-teaming** seeks to discover a diverse set of attack prompts and interaction patterns that reveal a broad range of vulnerabilities in the target LLM. The objective is to build a systematic framework that explores a distribution of attack strategies, rather than focusing on a single exploit.\n\nThus, the two are orthogonal. Jailbreaking supplies concrete, high-yield exploits for specific attack patterns, while automated red-teaming provides a scalable procedure for exploring diverse attack strategies and generating prompts that can be used to improve robustness of the target LLM.\n\nThanks again, and please let us know if you have any questions.\n\nThe authors"}}, "id": "IAE9xxAw8q", "forum": "aWt2SkfVhq", "replyto": "aWt2SkfVhq", "signatures": ["ICLR.cc/2026/Conference/Submission10617/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10617/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10617/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763524682425, "cdate": 1763524682425, "tmdate": 1763524682425, "mdate": 1763524682425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Active Attacks, a novel RL-based red-teaming approach to address the lack of diversity in existing automatic red-teaming frameworks. It shows that reward-driven frameworks tend to settle on easy, high-reward prompts and suffer mode collapse, continuing to generate similar prompts. To mitigate this, the method adds a simple module that periodically fine-tunes the victim model and re-initializes the attacker LLM to enforce exploration of new modes. Evaluations demonstrate substantially improved diversity, and fine-tuning on these diverse prompts enhances robustness against attacks from other approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles critical issues in LLM red-teaming.\n- The presentation is clear, and the core idea is intuitive and easy to follow.\n- The evaluation is extensive, with promising results."}, "weaknesses": {"value": "- Limited comparison with state-of-the-art red-teaming methods, both RL and non-RL.\n- Insufficient assessment of utility preservation after safety fine-tuning.\n- Some terminology and experimental setups need clearer justification."}, "questions": {"value": "Overall, this is an interesting paper that offers an intuitive solution to diversity mode collapse in RL-driven red-teaming frameworks. However, several concerns in the current manuscript need further justification.\n\n1. The definition of “easy-mode” is unclear. Does it mean that, for a particular seed prompt, there are two variants, one easy for the attacker LLM to find and another harder? Or does it refer to two seed prompts with different semantics, one easier than the other? If the latter, is the diversity issue unique to RL-based red-teaming? Traditional approaches often start from a fixed set of harmful seed prompts and mutate them until the victim produces harmful content, implying diversity is bounded by the seed prompt. If we enforce the attacker LLM to start from predefined seeds, would diversity issues persist?\n\n2. LLM red-teaming has extensive baselines. While comparing to all is infeasible, it would help to include recent baselines from RL-based and typical non-RL methods (e.g., [1,2,3]) to strengthen the contribution.\n\n3. Line 453 evaluates instruction-following preservation on six tasks in the Open LLM leaderboard. I am particularly interested in over-refusal after safety fine-tuning, as aggressive tuning can induce overly conservative behavior where benign prompts resembling malicious ones are refused. Please include evaluations on benchmarks such as XSTest [4] or OR-Bench [5].\n\n----\n\nReference\n======\n\n[1] Lochab A, Yan L, Pynadath P, et al. VERA: Variational Inference Framework for Jailbreaking Large Language Models[J]. arXiv preprint arXiv:2506.22666, 2025.\n\n[2] Chen X, Nie Y, Guo W, et al. When llm meets drl: Advancing jailbreaking efficiency via drl-guided search[J]. Advances in Neural Information Processing Systems, 2024, 37: 26814-26845.\n\n[3] Zou A, Wang Z, Carlini N, et al. Universal and transferable adversarial attacks on aligned language models[J]. arXiv preprint arXiv:2307.15043, 2023.\n\n[4] Röttger P, Kirk H R, Vidgen B, et al. Xstest: A test suite for identifying exaggerated safety behaviours in large language models[J]. arXiv preprint arXiv:2308.01263, 2023.\n\n[5] Cui J, Chiang W L, Stoica I, et al. Or-bench: An over-refusal benchmark for large language models[J]. arXiv preprint arXiv:2405.20947, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z56RclKQLr", "forum": "aWt2SkfVhq", "replyto": "aWt2SkfVhq", "signatures": ["ICLR.cc/2026/Conference/Submission10617/Reviewer_T5ie"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10617/Reviewer_T5ie"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805608880, "cdate": 1761805608880, "tmdate": 1762921877404, "mdate": 1762921877404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a reinforcement learning-based jailbreaking approach, which can also be used to improve LLMs' robustness. Prior work uses an LLM attacker to generate jailbreaking data and trains the victim LLM on such data, which is a one-time process. The paper follows an adversarial training strategy by alternately training both the attacker model and the victim model. The evaluation is conducted on samples from two datasets. The results show that the proposed approach has better attack and defense performance compared to five baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Improving LLMs' robustness against jailbreaking is an important research problem.\n2. The experiments include multiple LLM-based baseline methods.\n3. The paper is generally easy to follow."}, "weaknesses": {"value": "1. The proposed approach is simply an application of adversarial training to the jailbreaking scenario. Although the experimental results seem to show its effectiveness, the paper does not illustrate the unique challenges of applying adversarial training to jailbreaking. The design described in Section 3.2 seems straightforward, such as fine-tuning the victim LLM with pre-collected attack prompts, reinitializing the attacker LLM for new iterations, etc. These are all standard procedures in traditional adversarial training. It is unclear what different problems exist for jailbreaking training.\n2. The baseline approaches evaluated in the paper are all LLM-based. However, there are many optimization-based jailbreak attacks, such as GCG and its follow-up works, that are not evaluated against the proposed defense to demonstrate its robustness. These techniques can also be used for adversarially training LLMs to improve robustness. It is suggested to evaluate their performance as well. Additionally, there are other jailbreak methods [1] that do not require crafting input prompts, which can further stress-test the performance of the proposed method.\n3. The metric for measuring the defense rate is quite confusing. The paper states that it is “the ratio of attacker prompts for which the victim LLM defended itself.” But how is this actually measured? Was it measured by a toxicity classifier or another LLM? If so, how does the paper determine whether the output is an actual refusal or a harmful response? Prior work has shown that it is very likely that an LLM may start with refusal sentences and then follow with a harmful response, or vice versa. How does the paper ensure the legitimacy of the reported results?\n4. The time cost of the proposed approach is reported to be much lower compared to the baseline. The paper explains that it does not require “sampling on-policy trajectories or a reward computation procedure.” However, as shown in Algorithm 1, the proposed approach also requires sampling on-policy trajectories at line 221 and computing the reward at line 224. Moreover, the proposed approach adds additional operations (lines 230–237) on top of the baseline. Shouldn’t the approach therefore require more computation compared to the baseline? It is unclear why the proposed method has a much lower GPU memory and time cost.\n\n\n[1] Zhang, Zhuo, et al. \"On large language models’ resilience to coercive interrogation.\" 2024 IEEE Symposium on Security and Privacy (SP). IEEE, 2024."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ipvs7XqDRa", "forum": "aWt2SkfVhq", "replyto": "aWt2SkfVhq", "signatures": ["ICLR.cc/2026/Conference/Submission10617/Reviewer_uwwW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10617/Reviewer_uwwW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946785056, "cdate": 1761946785056, "tmdate": 1762921876672, "mdate": 1762921876672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}