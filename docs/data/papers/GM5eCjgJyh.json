{"id": "GM5eCjgJyh", "number": 11745, "cdate": 1758203489371, "mdate": 1763693741150, "content": {"title": "M$^2$GenCO: Multi-task Meta Learning for Generative Combinatorial Optimization", "abstract": "Despite the fast progress, especially recent diffusion-based models for solving Combinatorial Optimization Problems (COPs) on graphs, existing neural solvers mainly learns a narrow task (e.g., uniform TSP) at a time and hardly handle instances with diverse distributions. To fill such gaps, this paper proposes M$^2$GenCO, a multi-task learning paradigm that pioneers the incorporation of generative CO solving into the meta-learning mechanism for graph-based COPs, first formulating \"tasks\" in meta-learning as distinct problem types instead of instances of the same problem. Additionally, a lightweight graph neural network with a hybrid of task-specific and shared encoding blocks is tailored to instantiate the framework, performing effective joint pre-training on a variety of problem types and efficient fine-tuning to adapt for out-of-distribution scenarios. Further, we establish a comprehensive benchmark comprising 5 classic graph-based COPs with varying scales and multiple distributions, forming 38 distinct test datasets that facilitate standard evaluation of generalizability and adaptability for neural CO solvers, which has not been well developed in literature. Empirically, M$^2$GenCO with only greedy decoders yields an overall 9.16% performance gain with an average 95.6$\\times$ acceleration for inference, and achieves concrete state-of-the-arts on all test sets with simple local searchers, maintaining superior solving time against previous neural methods. Meanwhile, the resource and time consumption for training are saved by up to 82\\% and 91\\%, respectively.", "tldr": "", "keywords": ["Multi-task Neural Combinatorial Optimization", "Meta Learning", "Generative Models"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d4f74fcade1a28d859fd16ceb3e1295bbb462e03.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes M²GenCO, claiming to be the first multi-task meta-learning framework for graph-based combinatorial optimization (CO). The core idea is to use MAML to pretrain across different CO problem types (TSP, ATSP, MIS, MCl, MCut), instantiated with a consistency model as the backbone. The model is pretrained on certain distributions (RB graphs for MIS/MCl, BA for MCut, uniform for TSP/ATSP), then finetuned on new distributions (ER, HK, WS, Gaussian, etc.) in a few-shot manner. The authors also contribute a benchmark with 38 test datasets across 5 COP types and multiple distributions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Benchmark contribution**: The proposed multi-distribution benchmark (38 datasets across 5 COPs) addresses a gap in the literature. Most prior work only evaluates on uniform/in-distribution instances, so having systematic out-of-distribution test sets is valuable for the community.\n\n2. **Strong reported performance**: According to the experimental results, the method shows significant improvements over SOTA baselines across the benchmark, along with substantial speedup. **If these results hold under fair comparison settings**, they would represent meaningful progress for the field."}, "weaknesses": {"value": "### Critical Issues\n\n#### 1. The \"meta-learning\" framing is fundamentally misleading\n\nThe paper claims to do meta-learning across different COP types, but there's a fatal flaw: **pretrain and test use completely different distributions**. \n\n- **Pretrain**: RB (MIS/MCl), BA (MCut), Uniform (TSP/ATSP)\n- **Test**: ER, HK, WS, Gaussian, Cluster, HCP, SAT, etc.\n\nThis violates the core assumption of meta-learning, where train and test tasks should come from the same task distribution $p(\\mathcal{T})$. What you're actually doing is **transfer learning** - pretraining on one set of distributions and adapting to completely different ones. This is a well-established paradigm (think ImageNet pretraining), not a novel meta-learning approach. The entire conceptual framing needs to be reconsidered.\n\n#### 2. Minimal algorithmic innovation\n\nLooking at the technical components:\n- MAML: Finn et al. 2017\n- Consistency models: Song et al. 2023\n- Multi-task learning: decades-old paradigm\n- GCN for COP: Joshi et al. 2019\n\nThe paper essentially combines existing techniques without introducing new algorithms or theoretical insights. The \"innovation\" of treating different COP types as meta-tasks is actually quite natural in the MTL literature - it's standard practice to have diverse task types, not a conceptual breakthrough.\n\n#### 3. Naive MAML implementation ignores known problems\n\nThe paper adopts vanilla MAML without addressing well-documented issues:\n\n- **Task conflict**: Different COPs (TSP needs connectivity, MIS needs sparsity, MCut needs balanced cuts) have fundamentally different structures. How do shared parameters simultaneously optimize for these conflicting objectives? No discussion.\n\n- **Negative transfer**: When tasks are very different, meta-learning can hurt rather than help. No analysis of when/why negative transfer might occur.\n\n- **One-step inner loop**: You do $\\theta' = \\theta - \\alpha \\nabla L$ (Eq. 1) for ONE step. But consistency models typically need thousands of steps to converge. How can one gradient step meaningfully adapt the model? This seems fundamentally incompatible with the CM backbone.\n\n- **Gradient pathology**: Second-order gradients in MAML are notoriously unstable, especially with deep networks (your 6-layer GCN). The L2 normalization (Eq. 3) is a band-aid, not a solution. Why not use ANIL, BOIL, or other improved meta-learning methods?\n\n#### 4. Training setup for baselines is completely opaque\n\nThis is extremely problematic. You detail your own training (Algorithm 1, full architecture spec) but say **nothing** about how baselines were trained:\n\n- Were baselines also pretrained on RB/BA/Uniform then finetuned on test distributions?\n- Or were they trained from scratch on your train or test distributions?\n- If the latter, you're comparing pretrained (yours) vs. non-pretrained (baselines) - fundamentally different settings\n\nWithout this information, I cannot assess whether the performance gains come from (a) the meta-learning mechanism, (b) multi-distribution pretraining, or (c) simply using more diverse training data. This is a critical omission.\n\n#### 5. Testing protocol appears biased\n\nThe statement \"We set batch_size=1 and use greedy decoders for all tests unless otherwise stated\" is concerning:\n\n- **Greedy decoding severely handicaps sampling-based methods**: diffusion models all rely on multiple samples or search algorithms. Forcing greedy can cause 5-10× performance drops for these methods.\n\n- **Double standard**: Table 2 shows M²GenCO with ‡ marks (using MCTS), but baselines don't get this. DIMES also uses MCTS in its original paper - why is it denied here?\n\n- **Vague definition**: What exactly does \"greedy\" mean? Does it prohibit sampling? Augmentation? MCTS? The ambiguity is suspicious.\n\nIf all methods must use greedy but only yours gets additional post-processing, that's not a fair comparison.\n\n### Major Issues\n\n#### 6. No statistical significance testing\n\nEvery single result in Tables 2-3 is a point estimate with no error bars, standard deviations, or significance tests. How many runs did you do? Is the 9.16% improvement statistically significant or within noise? This is unacceptable for an ML venue in 2025.\n\n#### 7. Code not available during review\n\nYou promise to release code \"upon publication\" but provide nothing for reviewers to verify. This makes it impossible to check:\n- Whether results are reproducible\n- How baselines were actually implemented\n- Whether there's cherry-picking\n- The exact definition of \"greedy decoder\"\n\n### Minor Issues\n\n#### 8. Incomplete ablations\n\nTable 5 shows with/without finetuning and Fig 2 shows with/without diffusion/meta-learning, but missing:\n- Effect of task pool size $m$ (what if $m=10$ instead of 2-3?)\n- Effect of inner learning rate $\\alpha$, outer learning rate $\\beta$\n- Effect of number of meta-tasks $k$ per iteration\n- Ablation of gradient normalization (Eq. 3)\n\n#### 9. Hyperparameter sensitivity not analyzed\n\nMAML is notoriously sensitive to learning rates, yet you don't show how performance varies with $\\alpha$ and $\\beta$. This makes it hard to know if results are robust or require careful tuning.\n\n**Minor note**: Table 1 has a reference error - MAB-MTL cites Liu et al., 2024b, which is a mismatch on this paper."}, "questions": {"value": "**Q1**: Can you clarify the baseline training protocol?\n- Exactly what distributions were each baseline trained on?\n- Were baselines pretrained on RB/BA/Uniform and then finetuned on test distributions, matching your setup?\n- If not, why not? You're comparing pretrained (yours) vs. non-pretrained (theirs), which seems unfair.\n\n**Q2**: What does \"greedy decoder\" mean precisely?\n- Does it prohibit sampling? How many samples?\n- Does it prohibit augmentation (like POMO's multiple starting points)?\n- Does it prohibit MCTS/local search?\n- If MCTS is allowed, why do only some methods (yours) have ‡ marks in Table 2?\n\n**Q3**: Can you provide statistical significance?\n- How many random seeds did you run?\n- What are the standard deviations?\n- Which results are statistically significant ($p < 0.05$)?\n\n**Q4**: Can you explain the distribution mismatch?\n- You pretrain on RB/BA/Uniform but test on ER/HK/Gaussian/etc.\n- This violates standard meta-learning assumptions (train/test tasks from same distribution)\n- Why is this still \"meta-learning\" rather than transfer learning?\n\n**Q5**: Have you tried the fair baselines?\n- **Baseline 1**: Multi-task pretraining WITHOUT meta-learning (no MAML, just standard MTL)\n- **Baseline 2**: Your method trained from scratch on test distributions (no pretraining)\n- This would isolate the contribution of the meta-learning mechanism\n\n**Q6**: How does one-step inner loop help?\n- Consistency models typically need thousands of training steps\n- How can $\\theta' = \\theta - \\alpha \\nabla L$ (one step) meaningfully adapt the model?\n- Did you try multi-step inner loops?\n\n**Q7**: Do you have evidence of negative transfer or task conflict?\n- What happens when you train on conflicting tasks (e.g., TSP + MIS)?\n- Is there ever negative transfer where multi-task hurts compared to single-task?\n\n**Q8**: Why are training costs lower?\n- CM training is slow, MAML adds overhead (multiple forward/backward passes)\n- Why is your training faster than single-task methods?\n- Are you sure baselines were trained to convergence?\n\n**Q9**: What's the performance with sampling?\n- If you allow sampling (e.g., 100 samples) for all methods including yours, what happens?\n- This would be a fairer comparison for diffusion/RL baselines"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cgRrGx3Tno", "forum": "GM5eCjgJyh", "replyto": "GM5eCjgJyh", "signatures": ["ICLR.cc/2026/Conference/Submission11745/Reviewer_Qhpp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11745/Reviewer_Qhpp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761468463393, "cdate": 1761468463393, "tmdate": 1762922775989, "mdate": 1762922775989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents “M2GenCO\" a Multi-Task Meta Learning for Generative Combinatorial Optimization. M2GenCO defines each problem type as a meta-task, enabling cross-problem generalization and few-shot adaptation to unseen distributions. The model employs a lightweight graph neural network with shared and task-specific layers and a supervised diffusion process to efficiently learn instance-wise solution distributions. The authors also introduce a new benchmark of 38 datasets for evaluating generalization across problem types and distributions. Experiments demonstrate that M2GenCO achieves state-of-the-art performance while reducing training cost."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper formulates meta-learning tasks across different combinatorial problem types (rather than instances of one), aiming to bridge diffusion-based generative solvers with multi-task meta-learning for broad generalization.\n- The authors construct a large-scale benchmark of 5 COP types and 38 datasets, enabling systematic assessment of out-of-distribution and cross-problem adaptability. The dataset can be of good value to the community.\n- The authors reported state-of-the-art results on all tested benchmarks."}, "weaknesses": {"value": "- The paper can be greatly improved in its structure and presentation. In its current state, it is very difficult to follow and understand even for expert readers.\n- The paper claimed as the first to (1) define meta-learning tasks across different COP types and (2) integrate a supervised diffusion backbone into a multi-task meta-solver, I feel this statement may be an over-claim, for example, unified frameworks e.g., GOAL, UniCO, MVMoE/MAB-MTL and diffusion-based CO solvers, are all relevant prior works, without clear differentiation, this claim is not grounded, and the contribution and novelty isn't justified. I suggest the authors to rephrase the claim and clearly articulate the novelty and contributions of their work in the context of existing approaches.\n- The proposed benchmark, though diverse in distributions, primarily uses synthetic datasets rather than real-world industrial or logistics data. Also although five classic COPs are covered, all are graph-based problems; extending to non-graph combinatorial domains may be needed.\n- The experiment section focuses heavily on quantitative metrics but provides little insight into what is the source of success"}, "questions": {"value": "- Would the code and dataset be released for public access\n- What're the sources of the gains? Would recommend the ablation study to include carefully designed experiments to provide understanding on this so future work can be built on top of such knowledge"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UDsxQ3NJQj", "forum": "GM5eCjgJyh", "replyto": "GM5eCjgJyh", "signatures": ["ICLR.cc/2026/Conference/Submission11745/Reviewer_rn8h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11745/Reviewer_rn8h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757259998, "cdate": 1761757259998, "tmdate": 1762922775404, "mdate": 1762922775404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes M2GenCO, a multi-task meta-learning paradigm that treats different COP types as meta-tasks. It couples a supervised diffusion (consistency) backbone and a lightweight GCN with hybrid task-specific / shared blocks to enable fast few-shot finetuning across graph-based COPs. The proposed method achieves strong empirical performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "S1 (strong performance): M2GenCO reports consistent gains across many OOD benchmarks and an overall improvement index while achieving dramatic inference speedups compared to previous generative solvers.\n\nS2 (novel framework): Treating problem types as the meta-task distribution and combining that with a diffusion/consistency generative backbone seems to be a new and well-motivated formulation that bridges CO and meta-learning.\n\nS3 (efficiency): The lightweight GCN design is suitable for multi-task sharing and enables the enables efficient adaptation compared to larger backbones.\n\nS4 (thorough analysis): The paper reports thorough cross-distribution, cross-scale and ablation studies."}, "weaknesses": {"value": "W1 (finetuning protocol): The proposed adaptation relies on offline few-shot finetuning using a support set. It is not fully clear how sensitive the method is to to distributional mismatch between support and query or to scenarios where no labeled support set is available.\n\nW2 (design choices): Many design choices such as the task-sequence length k, the gradient normalization in Eq. (3), and sensitivity to inner/outer learning rates lack deeper ablation studies or theoretical justifications.\n\nW3 (scope of tasks): The suite covers only 5 graph COPs, but the claim toward “general-purpose” neural solvers would be stronger if the method were tested for non-graph COPs (knapsack, integer programs, etc.) or if limitations were emphasized more in the limitations section."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zjwPwRpLDs", "forum": "GM5eCjgJyh", "replyto": "GM5eCjgJyh", "signatures": ["ICLR.cc/2026/Conference/Submission11745/Reviewer_n7kV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11745/Reviewer_n7kV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799477739, "cdate": 1761799477739, "tmdate": 1762922774936, "mdate": 1762922774936, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces M2GenCO, a multi-task meta-learning framework for generative combinatorial optimization on graphs. The key innovation is reformulating meta-learning tasks as different problem types (TSP, ATSP, MIS, MCl, MCut) rather than instances of the same problem, integrating this with diffusion-based generative modeling. The framework consists of multi-task meta-pretraining across problem types, followed by few-shot finetuning for specific distributions. Additionally, the authors construct a comprehensive benchmark with 38 datasets spanning 5 COPs across diverse distributions. Experiments demonstrate approximately 50% improvement in solution quality and up to 4× faster inference compared to prior SOTA, while achieving 82% reduction in computational resources and 91% reduction in training time."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "--Ambitious vision: The paper tackles a challenging problem - learning across structurally different COPs - which pushes the boundaries of current NCO methods\n\n\n--Comprehensive evaluation: The experimental evaluation is impressively thorough, covering 38 datasets with multiple baselines and extensive ablations\n\n\n--Valuable benchmark: The multi-distribution benchmark addresses a critical need in the NCO community for standardized OOD evaluation"}, "weaknesses": {"value": "-- Theoretical foundation needs development: While the empirical results are encouraging, the paper would benefit from deeper analysis of why cross-problem learning helps. Even an intuitive explanation based on shared graph structures or optimization patterns would strengthen the work.\n\n\n-- Task pool selection could be more systematic: The current selection appears somewhat arbitrary. An ablation study varying task combinations would provide valuable insights into which problems benefit from joint training.\n\n\n-- Integration could be tighter: The diffusion and meta-learning components, while both valuable, feel somewhat independent. Exploring their synergies more explicitly would strengthen the narrative.\n\n\n-- Baseline comparisons need clarification:\n\n\n  * It would be helpful to clarify whether Meta-EGN's sampling strategy difference affects fairness\n  * Comparing against standard multi-task learning (without meta-learning) would isolate the meta-learning benefit\n  * Training data amounts should be standardized where possible\n\n--Scalability discussion: The paper would benefit from more explicit discussion of scaling limits and potential solutions for larger instances (>1000 nodes)."}, "questions": {"value": "-- Shared structure hypothesis: Could you elaborate on what common structures or patterns across COPs might enable positive transfer? For instance, do all problems benefit from learning graph connectivity patterns?\n\n\n-- Task pool sensitivity: How robust is performance to different task combinations in pretraining? Does removing any single task significantly impact results?\n\n\n-- Ablation depth: Could you provide results for standard multi-task learning without the meta-learning machinery to isolate its contribution?\n\n\n-- Component necessity: Given Table 15's results, have you explored whether simpler architectures might achieve similar performance with less complexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zDpQCDInkB", "forum": "GM5eCjgJyh", "replyto": "GM5eCjgJyh", "signatures": ["ICLR.cc/2026/Conference/Submission11745/Reviewer_VuMe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11745/Reviewer_VuMe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155679347, "cdate": 1762155679347, "tmdate": 1762922774124, "mdate": 1762922774124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}