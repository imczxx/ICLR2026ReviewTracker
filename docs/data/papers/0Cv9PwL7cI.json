{"id": "0Cv9PwL7cI", "number": 272, "cdate": 1756733104796, "mdate": 1763761405142, "content": {"title": "Semantic-Aware Diffusion LLM Inference With Adaptive Block Size", "abstract": "Diffusion-based large language models (dLLMs) are gaining attention for their inherent capacity for parallel decoding, offering a compelling alternative to autoregressive LLMs. Among various decoding strategies, blockwise semi-autoregressive (semi-AR) approaches are widely adopted due to their natural support for KV caching and their favorable accuracy–speed trade-off. However, this paper identifies two fundamental limitations in the conventional semi-AR decoding approach that applies a fixed block size: i) late decoding overhead, where the unmasking of high-confidence tokens outside the current block is unnecessarily delayed; and ii) premature decoding error, where low-confidence tokens inside the current block are committed too early, leading to incorrect tokens. This paper presents the first systematic investigation challenging the fixed block size assumption in semi-AR decoding. Through a statistical analysis of confidence dynamics during the denoising process, we identify a volatility band (VB) region during dLLM decoding, which encodes local semantic structure and can be used to guide adaptive block sizing. Leveraging these insights, we introduce AdaBlock-dLLM, a training-free, plug-and-play scheduler that adaptively aligns block boundaries with semantic steps by adjusting block size during runtime. Extensive experiments across diverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy improvement under the same throughput budget. Beyond inference-time optimization, we hope our semantics-aware adaptive scheduling approach and confidence-based analysis will inspire future training strategies for dLLMs.", "tldr": "", "keywords": ["Diffusion Large Language Models", "Non-Autoregressive Decoding"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a392eba79e4a75cf55f71dd6a5b7f8f8bf5dc2c9.pdf", "supplementary_material": "/attachment/9f1d83373188a93be3c31dd3a48db5b6b39307e5.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the limitations of fixed block-size semi-autoregressive decoding in diffusion-based large language models (dLLMs). The authors identify two inefficiencies — Late Decoding Overhead and Premature Decoding Error — that arise when fixed-size blocks fail to align with semantic structure during decoding. To address this, they propose AdaBlock-dLLM, a training-free, plug-and-play adaptive scheduler that dynamically adjusts block size according to semantic cues and confidence scores during inference. The method leverages a novel concept called the Volatility Band (VB) — regions of fluctuating confidence that correspond to evolving semantic steps — to determine when to expand or contract blocks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Well motivated. The paper convincingly articulates the inefficiencies of fixed block-size decoding in diffusion LLMs.\n\n- The adaptive block-size scheduler is conceptually elegant, lightweight, and compatible with existing architectures.\n\n- The authors provide extensive results across multiple models and datasets. The method yields consistent accuracy improvements, particularly when combined with caching."}, "weaknesses": {"value": "- The experiments focus mainly on math  and code generation. Broader text generation tasks (e.g., summarization or translation) and harder math reasoning tasks such as AIME would strengthen claims of generality.\n\n- At larger block sizes, AdaBlock-dLLM shows reduced throughput compared to the baseline, raising concerns about its scalability for high-speed inference.\n\n- The delimiter threshold (τ_D) and delimiter set (D) are manually tuned per model family. A more principled or automated way to select these would increase robustness.\n\n- The paper lacks a discussion of situations when adaptive block size might fail."}, "questions": {"value": "Technical Concerns/Questions and Points to Address in Rebuttal:\n\n- Line 53 lacks evidence and most be supported through experiments or citations to prior work.\n\n- How is this work different from [1] ?\n\n- The concept of the Volatility Band (VB) appears intuitive—tokens near already decoded regions naturally exhibit higher confidence—so unless counterexamples or non-trivial cases are shown, the finding risks seeming self-evident rather than novel.\n\n\nReferences:\n\n[1] Wang, Xu, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, and Zhijie Deng. \"Diffusion llms can do faster-than-ar inference via discrete diffusion forcing.\" arXiv preprint arXiv:2508.09192 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ihVGh2vEjb", "forum": "0Cv9PwL7cI", "replyto": "0Cv9PwL7cI", "signatures": ["ICLR.cc/2026/Conference/Submission272/Reviewer_caBK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission272/Reviewer_caBK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761769974207, "cdate": 1761769974207, "tmdate": 1762915482657, "mdate": 1762915482657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method to adaptively select the left and right boundaries for the block in the semi-AR setting for dLLM. The main idea come from the analysis of the confidence dynamics, where inside a volatility band area, the confidence fluctuates dynamically, and the VB regions exhibit semantics structure. Then the author propose to collect indices whose predicted tokens fall in the delimiter set to determine the block size. Experiments show that the performance exceeds the one in DualCache."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a new method to solve the problem of how to adaptively decide the block size and also its position in dLLMs.\n2. The paper is well-written with sufficient analysis for the motivation.\n3. The proposed method is effective compared to the baseline."}, "weaknesses": {"value": "1. Adaptive block size is important, but the authors didn't show any other baseline for different ways to adaptively deciding the block size. They are other ways that can also achieve the adaptive block size, like some naive and straightforward ways you can just expanding the block size by using a sliding window and track the latest position of tokens that are not decoded as the left index. I think adding more analysis and comparisons with different ways to achieve adaptive block is needed.\n2. Why the performance for MBPP on Dream-Base is only 12. The results from the official report is ~55%. It's a large discrepancy.\n3. I am confusing about the result in Table 2. In Table 2, the TPS first increases and then decreases as you choose a larger bock size. In the analysis, the authors mentioned that it may because the increase of NFE, but the thing matters is the ratio between NFE and Block size. Can you provide more analysis on this.\n4. I think the contribution of this submission is not that meet the standard of ICLR. I fully agree that adaptive block is needed, but the contribution of the method is only to choose the block size by the delimiter set, and they are a lot of papers discussing the confidence dynamics in dllm decoding. The idea is interesting, but it's too simple and straightforward."}, "questions": {"value": "1. See Weaknesses.\n2. The way of selecting the delimiter set can be explored more. For traditional NLP tasks, there are a lot of ways (semantic parsing) to know different types to separate the semantics of a sentence, not only by the rough way of selecting \\n, [,], and [.]."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7v1A9hwxs3", "forum": "0Cv9PwL7cI", "replyto": "0Cv9PwL7cI", "signatures": ["ICLR.cc/2026/Conference/Submission272/Reviewer_e6SK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission272/Reviewer_e6SK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898306986, "cdate": 1761898306986, "tmdate": 1762915482445, "mdate": 1762915482445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies that fixed block sizes in semi-autoregressive (semi-AR) dLLM decoding cause \"late decoding overhead\" and \"premature decoding error\". It proposes AdaBlock-dLLM, a training-free scheduler that adaptively aligns block boundaries with semantic steps based on delimiter token confidence. This method improves accuracy by up to 5.3% without sacrificing throughput."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper systematically analyze and address the limitations of the fixed block size assumption in semi-AR decoding.\n\n- The core problems (late overhead, premature error) are clearly identified and illustrated. The proposed solution is simple, intuitive, and well-supported by experiments.\n\n- The method is practical (training-free, plug-and-play) and shows clear accuracy improvements on the accuracy-throughput frontier, especially when combined with KV caching."}, "weaknesses": {"value": "- The method introduces new hyperparameters that require model-specific tuning, slightly weakening the \"plug-and-play\" claim."}, "questions": {"value": "Given the marginal benefit of adding more delimiters (Table 6), did you consider a more automated or learned method to identify semantic boundaries instead of a predefined list?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sQUlurkjnd", "forum": "0Cv9PwL7cI", "replyto": "0Cv9PwL7cI", "signatures": ["ICLR.cc/2026/Conference/Submission272/Reviewer_c59M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission272/Reviewer_c59M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971633785, "cdate": 1761971633785, "tmdate": 1762915482310, "mdate": 1762915482310, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AdaBlock-dLLM, a semantic-aware adaptive block-size decoding method for diffusion-based large language models (dLLMs). By analyzing confidence dynamics during denoising, the authors propose AdaBlock-dLLM, which is a training-free, plug-and-play scheduler that dynamically adjusts block boundaries according to semantic step length. Extensive experiments on benchmarks show that the proposed approach maintains compatibility with caching mechanisms and improves both efficiency and semantic consistency in diffusion LLM inference."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes an adaptive, training-free approach that integrates seamlessly with existing diffusion LLM frameworks and improves accuracy without retraining.\n\n2. This paper provides a clear and well-motivated analysis of confidence dynamics and their connection to semantic structures in dLLM decoding, then uses the confidence score of delimiter to decide the block size adaptively."}, "weaknesses": {"value": "1. The heuristic nature of the delimiter-based semantic segmentation might not generalize well to less structured text other than math or coding problems. The authors should evaluate on more diverse test sets. For example, what if the output text is expected to be a long section and there's no '\\n' in the output?\n\n2.  Figure 6 can hardly be viewed as a pareto-frontier, as the throughput nearly does not change as the accuracy is increasing."}, "questions": {"value": "Please refer to weakness for more details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MIp4G97tqX", "forum": "0Cv9PwL7cI", "replyto": "0Cv9PwL7cI", "signatures": ["ICLR.cc/2026/Conference/Submission272/Reviewer_THAi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission272/Reviewer_THAi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762071187878, "cdate": 1762071187878, "tmdate": 1762915482055, "mdate": 1762915482055, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank the reviewer for their insightful suggestions. In the revised version, we highlight the modified part in **Orange** to make it easier to identify the changes. \n\nWe would be grateful for any acknowledgement or discussion from the reviewers. Please let us know if any questions remain, as we would be happy to provide further clarification."}}, "id": "eClHSYNy2f", "forum": "0Cv9PwL7cI", "replyto": "0Cv9PwL7cI", "signatures": ["ICLR.cc/2026/Conference/Submission272/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission272/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission272/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763764993820, "cdate": 1763764993820, "tmdate": 1763764993820, "mdate": 1763764993820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}