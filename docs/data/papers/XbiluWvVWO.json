{"id": "XbiluWvVWO", "number": 11820, "cdate": 1758204067755, "mdate": 1759897552570, "content": {"title": "TCMReasonSet: A Dataset for Explainable Medical Reasoning in Traditional Chinese Medicine", "abstract": "Large language models (LLMs) excel in structured tasks such as mathematics and programming but remain limited in knowledge-intensive domains like Traditional Chinese Medicine (TCM), which require complex reasoning.The primary bottleneck stems from the scarcity of high-quality training corpora that are well-structured and explicitly traceable in their reasoning pathways. To address this, we introduce TCMReasonSet, a high-quality dataset specifically designed for TCM clinical reasoning, aimed at enhancing the reliability and interpretability of LLMs in solving TCM-related problems. The construction of TCMReasonSet comprises three core components: (1) a proprietary TCM knowledge graph we developed — containing 52,000 entities and 1.38 million relations — serving as the foundation for dynamic retrieval and reasoning; (2) the generation of clinical question-answer pairs using LLMs, grounded in the aforementioned knowledge graph; and (3) building upon the knowledge graph and QA pairs, we propose the “TCM Tree-of-Thought” (TCM-ToT) methodology, which incorporates a dual-dimension scoring mechanism (logical consistency + factual accuracy) to evaluate clinical QA pairs and transform them into coherent, interpretable reasoning chains with explicit pathways. Through this pipeline, we ultimately generated 36,573 clinically interpretable reasoning samples. Experimental results demonstrate that fine-tuning models with TCMReasonSet significantly enhances medical problem-solving performance: the DeepSeek-Distill-8B model achieves an 8.9\\% accuracy gain, while our TCMReason-8B model surpasses the current state-of-the-art medical reasoning model by a 5.7\\% margin. Furthermore, expert evaluations further validate the reliability of our dataset in terms of factual accuracy and logical coherence.", "tldr": "", "keywords": ["Dataset", "Traditional Chinese Medicine", "Large Language Model Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d29d91e365b6be950f33a6f9c54407ad5dd5a6ed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces TCMReasonSet, a dataset tailored for explainable medical reasoning in Traditional Chinese Medicine. The dataset construction involves three core stages: (1) building a TCM knowledge graph with 52,000 entities and 1.38 million relations; (2) generating 50,000 knowledge-constrained QA pairs covering TCM theory, diagnosis, and pharmacology; (3) proposing the TCM Tree-of-Thought (TCM-ToT) methodology with a dual-dimensional scoring mechanism to transform QA pairs into interpretable reasoning chains. Evaluations further validate the dataset’s reliability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper try to adapt the ToT paradigm to TCM’s distinctive reasoning logic. The integration of a domain-specific knowledge graph into the reasoning pipeline also represents a thoughtful customization for TCM’s complex knowledge structure.\n2. The dataset construction exhibits multi-source validation and expert review. The experiments are comprehensive, covering multiple benchmarks and model architectures.\n3. The experimental results are clearly organized, enabling easy comparison between models."}, "weaknesses": {"value": "1. The paper relies on existing paradigms (knowledge graphs, ToT, dual-dimensional scoring) and primarily adapts them to TCM rather than proposing entirely new methodologies. For example, the TCM-ToT builds on the original ToT framework, and knowledge-constrained QA generation has been used in other medical subfields (e.g., Western medicine drug discovery).\n2. The paper focuses on benchmark performance but provides little insight into how TCMReasonSet or TCMReason-8B would perform in real clinical settings. \n3. Critical considerations like inter-expert variability in TCM syndrome differentiation, regulatory compliance for medical AI, and usability for TCM practitioners are not addressed.\n4. The experiments focus on model accuracy but lack analysis of reasoning interpretability beyond expert ratings. Additionally, the paper does not compare against non-Chinese TCM reasoning datasets or models, limiting the assessment of its global relevance.\n5. The TCM-KG’s entity and relation coverage is not evaluated against other existing TCM knowledge bases (e.g., TCM-SKG), making it hard to assess its comprehensiveness. \n6. The dual-dimensional scoring mechanism’s thresholds are not justified, and there is no analysis of how sensitive the results are to these parameters.\n7. Most importantly, this paper claims to be open source, but the current version does not include any open source URLs. We recommend that the revised version include an anonymized URL or dataset to further evaluate the dataset."}, "questions": {"value": "1. How did you resolve conflicting information from multi-source data  during TCM-KG construction? Providing specific examples and resolution strategies would strengthen the dataset’s credibility.\n2. Providing information about the annotation software or human review process would help improve the feasibility of the article.\n3. Currently, the dataset focuses on herbal formulas and syndrome differentiation, limiting its breadth. What is the future work of the current paper?\n4. Have you tested the proposed model in real clinical settings with TCM practitioners? If so, what feedback did you receive regarding the model’s reasoning interpretability and clinical utility? If not, what steps will you take to validate its real-world applicability?\n5. The dual-dimensional scoring mechanism uses HuatuoGPT-o1-72B as the scoring oracle. How does this choice impact the scoring consistency, and have you considered using multiple scoring models to mitigate potential biases from a single oracle?\n6. Why did you not compare TCMReasonSet against existing TCM knowledge graphs or reasoning datasets (e.g., TCMEval-SDT, SymMap-derived datasets)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wSPKXQpt82", "forum": "XbiluWvVWO", "replyto": "XbiluWvVWO", "signatures": ["ICLR.cc/2026/Conference/Submission11820/Reviewer_BnXL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11820/Reviewer_BnXL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760848920929, "cdate": 1760848920929, "tmdate": 1762922841404, "mdate": 1762922841404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TCMReasonSet, a reasoning-focused dataset for Traditional Chinese Medicine (TCM). TCMReasonSet is built via 1) a TCM knowledge graph, 2) LLM-generated, knowledge-constrained QA pairs, and 3) a TCM-specific Tree-of-Thought (TCM-ToT) pipeline scored on factual accuracy and global logical coherence. The constructed dataset is used to fine-tune several sub-10B models, which show higher performance gains across various TCM benchmarks when compared to models trained on Huatuo CoT dataset."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well motivated based on TCM’s non-linear diagnostic logic and proposes a ToT approach anchored in a KG.\n2. Local factuality (LocalScore) and global coherence (CohereScore) are proposed to ensure the data quality, with ablation showing their unique importance.\n3. Instruction-tuned and reasoning models fine-tuned on TCMReasonSet show consistent improvements across different TCM benchmarks."}, "weaknesses": {"value": "1. It is unclear why the LLM could not directly generate both the QA pairs and their corresponding reasoning steps simultaneously, given that the QA pairs are derived from paths within the knowledge graph.\n2. The paper lacks direct comparisons between TCMReasonSet and Huatuo-CoT on Qwen3-8B and DeepSeek-Distill-8B.\n3. Table 4 appears inconsistent with Table 1: the bolded \"w/\" scores in Table 4 seem to represent the original Qwen2.5-7B-Instruct performance without post-training. According to the table, removing any scoring mechanism leads to results significantly below the Huatuo-CoT-tuned baseline and even the untuned model. This discrepancy requires deeper analysis and clarification.\n\nMinor issues:\n- The first listed contribution in the Introduction states \"consisting of 30,000 ... \", while other parts of the paper refer to \"> 30,000\".\n- Some tokens in lines 199-202 look weird.\n- There is a broken citation for HuatuoGPT-o1-72B in line 303.\n- In-text citations with parentheses should be used wherever applicable."}, "questions": {"value": "- Why can't the LLM generate both the QA pairs and the corresponding reasoning steps simultaneously, given that the QA pairs are built from paths within the knowledge graph?\n- What is the performance of Huatuo-CoT on Qwen3-8B and DeepSeek-Distill-8B?\n- Why does Table 4 show results that appear inconsistent with Table 1, and why does the removal of any scoring mechanism cause significant performance to drop below even the untuned model? Could the authors provide further analyses or clarification for this discrepancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VHRyWrbLRV", "forum": "XbiluWvVWO", "replyto": "XbiluWvVWO", "signatures": ["ICLR.cc/2026/Conference/Submission11820/Reviewer_yJqs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11820/Reviewer_yJqs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802256255, "cdate": 1761802256255, "tmdate": 1762922840786, "mdate": 1762922840786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents TCMReasonSet, a large-scale dataset designed to enhance explainable reasoning for Traditional Chinese Medicine (TCM). It combines a newly constructed TCM knowledge graph (TCM-KG) with Tree-of-Thought (ToT) reasoning to create structured, interpretable reasoning chains. The work aims to improve the reliability and interpretability of medical large language models, addressing a practical need for transparent clinical reasoning in TCM diagnosis, treatment, and education."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well organized and clearly written, with intuitive figures and tables that make the overall pipeline easy to follow.\n\n2. The dataset construction process is well-designed, integrating multiple modules such as KG building, entity alignment, reasoning-chain generation, and multi-stage filtering. This modular pipeline reflects good engineering practice and provides a valuable resource for studying structured medical reasoning."}, "weaknesses": {"value": "1. The entire data construction pipeline relies heavily on large language models rather than natural data linkage or expert curation. Both the KG extraction and the question generation depend on LLM outputs, which could introduce unreliability and systemic bias.\n\n2. The data quality filtering and scoring rely primarily on HuatuoGPT-o1-72B, creating a self-referential setup where one model evaluates data generated by other models. This raises concerns about potential bias and lack of independent validation.\n\n3. The human expert evaluation is limited in scope. The number of evaluated samples is small, the scoring is subjective, and there is no report of inter-rater reliability or consistency, which weakens the credibility of the human assessment.\n\n4. The evaluation design does not fully align with the dataset’s stated goal. While the dataset emphasizes explainable reasoning, most experiments focus on multiple-choice benchmarks. Beyond subjective expert ratings and case studies, there are no objective process-level metrics—such as reasoning consistency, factual accuracy, or path correctness—to quantitatively validate improvements in interpretability."}, "questions": {"value": "1. Since both the TCM-KG construction and QA generation rely heavily on LLM outputs, how do the authors ensure the factual reliability and medical correctness of these automatically extracted or synthesized triples? Was there any human verification or error-rate estimation?\n\n2. How were ambiguous or conflicting knowledge entries handled when the same concept appeared with multiple relations or synonyms across different data sources?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EpwIMxr3TR", "forum": "XbiluWvVWO", "replyto": "XbiluWvVWO", "signatures": ["ICLR.cc/2026/Conference/Submission11820/Reviewer_SE2M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11820/Reviewer_SE2M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009759283, "cdate": 1762009759283, "tmdate": 1762922839989, "mdate": 1762922839989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}