{"id": "U3kxnDzuRz", "number": 24651, "cdate": 1758358992089, "mdate": 1759896757059, "content": {"title": "Deep-Cover Agents: Long-Horizon Prompt Injections on Production LLM Systems", "abstract": "Instruction-following LLM assistants that read untrusted data are susceptible to prompt injection, wherein a malicious actor injects a harmful request that the assistant naively complies with, to the user's detriment. We analyze the structure of tool-using LLM agents to create a descriptive framework for prompt injection attacks. By examining this framework, we find that certain attack modalities are understudied, and observe important trends in attack performance as we vary how prompt injection attacks are introduced and their token budget with practical takeaways. Importantly, previous work does not significantly explore the dimension of time, and we make the key finding that after being prompt-injected, many agents can behave benignly for 50+ conversation turns before taking a malicious action. Finally, we validate our work by executing sandboxed attacks against deployment systems such as Claude Code and Gemini-CLI. Our attacks readily succeed, and additionally reveal as-yet undocumented emergent behavior in these models' responses to prompt injection.", "tldr": "We analyze the structure of agents and what it means for prompt injection attacks. We create prompt injection attacks that match common use-patterns, and demonstrate that they work on production systems.", "keywords": ["agents; prompt injection; robustness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5696ce50c9098b0215353403e6c8bd6e8337f6ed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a framework for analyzing prompt injection attacks against tool-using LLM agents that process untrusted data. The authors identify understudied attack modalities and examine how attack performance varies based on injection method and token budget, yielding practical insights for both attackers and defenders. \n\nA key finding is the discovery of \"delayed\" prompt injection attacks, where compromised agents can behave normally for over 50 conversation turns before executing malicious actions—a temporal dimension largely unexplored in prior work. The research is validated through successful sandboxed attacks against real-world deployment systems including Claude Code and Gemini-CLI, which also reveal previously undocumented emergent behaviors in these models' responses to injection attempts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper makes an important and understudied contribution by investigating how prompt-injected agents can exhibit delayed malicious behavior. This temporal dimension has been largely overlooked in prior work and has implications for detecting and mitigating prompt injection attacks in real-world deployments, as it suggests that simple immediate monitoring may be insufficient.\n2. Prompt injection is a critical security concern for tool-integrated agents, and this paper identifies understudied attack modalities and examine how attack performance varies based on injection method and token budget, yielding practical insights for both attackers and defenders."}, "weaknesses": {"value": "I think the main problem is the limited novelty in threat model and the contribution is limited as well.\n\n1. The work appears to focus on direct prompt injection where attackers can modify system prompts, which is impractical. Real-world scenarios involve indirect prompt injections through untrusted data (emails, documents, web pages). The threat model may be too strong and less relevant than existing work on indirect attacks.\n2. The delayed/time-based attacks (behaving benignly for 50+ turns) seem more like an interesting observation than a critical security concern. If we cannot effectively defend against immediate prompt injections, delayed attacks are a secondary issue. The practical significance is unclear.\n3. The evaluation relies solely on ASR, while comprehensive benchmarks should also measure utility/benign task performance. This is critical for assessing the practical trade-offs of defenses and the true impact of attacks on system usability, for example agentdojo. \n\n4. Figure 1 is not referenced or explained in the text. Figure 3 (top left) lacks a legend making it difficult to interpret. Additionally, there are formatting issues with incorrect quotation mark usage (line 38 and 43)."}, "questions": {"value": "no"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "D7nAxt52Qc", "forum": "U3kxnDzuRz", "replyto": "U3kxnDzuRz", "signatures": ["ICLR.cc/2026/Conference/Submission24651/Reviewer_CGsz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24651/Reviewer_CGsz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761679279396, "cdate": 1761679279396, "tmdate": 1762943147252, "mdate": 1762943147252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework for analyzing prompt injection attacks on tool-using LLM agents. It characterizes the relevant factors an adversary should consider while crafting such attacks including Target, Stealth, Vector, Budget and Timing. It also proposes timing as a new characteristic of prompt injection attacks. It provides reasonable amount of experimental results to observe some trends for each of the factors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper takes an important step towards characterizing the relevant factors for the success of prompt injection attacks.\n\n2. The paper attempts to provide experimental results for each of the proposed factors.\n\n3. The paper presents attacks on real-world deployments including Claude Code and Gemini-CLI.\n\n4. The writing is clean and easy to follow."}, "weaknesses": {"value": "1. The experimental results have too much variation across models, it doesn't seem fair to draw any single conclusion from the average behaviors (refer Figure 3). Exploring the possible reasons for such huge fluctuations is an important task to claim anything about the transferability of the conclusions.\n\n2. The paper doesn't discuss about the stealth (one of the factors in framework) concretely in the experiments.\n\n3. The target for all the experiments is again fixed which might raise concerns about the transferability of conclusion (mentioned in the limitations as well).\n\n4. All these factors are not independent, studying the correlation between factors would be important to actually gauge their importance. For example, the experimental results for the timing implies that it is optimal to provide the trigger context and the opportunity to corrupt the response at the same time, is there any benefit in keeping them separate in terms of stealth ?"}, "questions": {"value": "1. Is there any analysis/conclusions for each LLM that may provide adaptive attacks and defenses tailored to each model ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2mVNnkCbNx", "forum": "U3kxnDzuRz", "replyto": "U3kxnDzuRz", "signatures": ["ICLR.cc/2026/Conference/Submission24651/Reviewer_vsBS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24651/Reviewer_vsBS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954540490, "cdate": 1761954540490, "tmdate": 1762943146952, "mdate": 1762943146952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies long-horizon prompt injection attacks on LLM-based agents, focusing on cases where malicious instructions injected into the context may activate dozens of turns later. The authors propose a five-axis framework (Target, Stealth, Vector, Budget, Timing) to describe prompt injection attacks, conduct controlled experiments on synthetic multi-turn dialogues, and test zero-shot transfer to real deployed systems such as Claude Code and Gemini-CLI. The results suggest that some commercial systems remain vulnerable even after many turns, and that reasoning depth can affect attack success rate."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The problem setting of studying long-horizon prompt injections is novel and intuitively important, especially as LLM agents increasingly operate in multi-turn environments."}, "weaknesses": {"value": "- Lack of formalization: The notion of “long-horizon prompt injection” is only loosely defined. The paper does not provide a clear formal threat model, trigger definition, or precise criteria for what constitutes a long-horizon attack versus a normal prompt injection. The attacks resemble backdoor-style conditional activation rather than classical prompt injection, but the paper does not clearly justify this distinction.\n\n- Evaluation methodology is non-standard and underspecified with limited diversity of the evaluation set: Only one attack type (inserting remote code execution vulnerabilities) is studied. The dataset lacks diversity across tasks, modalities, or realistic user contexts.\n\n- Writing and presentation issues: The paper’s structure and language are below top-conference standard."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n5hBdZc9JT", "forum": "U3kxnDzuRz", "replyto": "U3kxnDzuRz", "signatures": ["ICLR.cc/2026/Conference/Submission24651/Reviewer_NN78"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24651/Reviewer_NN78"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973863883, "cdate": 1761973863883, "tmdate": 1762943146281, "mdate": 1762943146281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present empirical analyses of long-horizon prompt injections, those in which the trigger and the attack opportunity are separated by a large number of conversation steps, and find that simple attacks are successful in this context; for certain models, this is true across system prompt, user message, and tool response attack vectors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors present a large number of useful experimental results, which indicate that a large number of commonly deployed language models are vulnerable to long-horizon attacks.\n- Results on real-world systems (e.g. Claude Code) are presented, and the examples of malicious behavior are useful."}, "weaknesses": {"value": "- The conclusions drawn from the results by the author are not fully substantiated and generally speculative. While \"testing a model's resistance to prompt injection at t=0 may not provide a strong indication..\" [307] is intuitive, the results in Figure 3 do not clearly justify this conclusion, and generally show that long-run ASRs fluctuate about the short-run values for the majority of values. Similarly, while strong effects are visible for certain LLMs, the effect of reasoning effort in Figure 4 appears to be weak for the majority of models. It is unclear whether variance impacts these results; the experiments should be conducted over multiple runs and standard hypothesis tests should be performed.\n- The results for ASR vs attack budget in Figure 3 actually measure the effects of summarization on the specific prompt injections used, not the adversary's budget, and should be described as such. Performance of a worst-case adversary should never decrease with budget.\n- The results are highly variable by model, and hence it is unclear to what exent these results generalize.\n- While the results presented here are a useful first analysis of long-horizon attacks, the notion of \"long horizon\" considered in this work is somewhat limited, and doesn't consider more subtle behavior, e.g. where the desired behavior is to delay defection and behave benignly initially."}, "questions": {"value": "In Figure 4, ASR is, as might be expected, by far the highest when the attack opportunity is immediately following the trigger. How does this vary over a shorter time horizon (e.g. for additional timesteps in the 40-50 range)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LjieH94HOR", "forum": "U3kxnDzuRz", "replyto": "U3kxnDzuRz", "signatures": ["ICLR.cc/2026/Conference/Submission24651/Reviewer_TZzu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24651/Reviewer_TZzu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762286501899, "cdate": 1762286501899, "tmdate": 1762943145978, "mdate": 1762943145978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}