{"id": "ctdnzPxDI3", "number": 14406, "cdate": 1758234583009, "mdate": 1759897372154, "content": {"title": "Efficient Regression-based Training of Normalizing Flows for Boltzmann Generators", "abstract": "Simulation-free training frameworks have been at the forefront of the generative modelling revolution in continuous spaces, leading to large-scale diffusion and flow matching models. However, such modern generative models suffer from expensive inference, inhibiting their use in numerous scientific applications like Boltzmann Generators (BGs) for molecular conformations that require fast likelihood\nevaluation. In this paper, we revisit classical normalizing flows in the context of BGs that offer efficient sampling and likelihoods, but whose training via maximum likelihood is often unstable and computationally challenging. We propose Regression Training of Normalizing Flows (RegFlow), a novel and scalable regression-based training objective that bypasses the numerical instability and computational challenge of conventional maximum likelihood training in favour of a simple $\\ell_2$-regression objective. Specifically, RegFlow maps prior samples under our flow to targets computed using optimal transport couplings or a pre-trained continuous normalizing flow (CNF). To enhance numerical stability, RegFlow employs effective regularization strategies such as a new forward-backward self-consistency loss that enjoys painless implementation. Empirically, we demonstrate that RegFlow unlocks a broader class of architectures that were previously intractable to train for BGs with maximum likelihood. We also show RegFlow exceeds the performance, computational cost, and stability of maximum likelihood training in equilibrium sampling in Cartesian coordinates of alanine dipeptide, tripeptide, and tetrapeptide, showcasing its potential in molecular systems.", "tldr": "A new regression-based approach to training normalizing flows", "keywords": ["Normalizing Flows", "Generative Models", "Optimal Transport", "Flow Matching", "AI for Science"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b4b2f965efbc9feeff2ca2fd7663812e9e5d9c76.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes to train normalizing flows by regression to alleviate the training instability in traditional MLE training to transform samples from a prior $q$ to a target distribution $p$, ie $x_0\\sim q$ and $x_1=T(x_0)\\sim p$. To enable regression-based training, it requires that there exists an invertible map between $x_0$ and $x_1$. Finding an exact solution is intractable, and therefore the author proposes two different approximations: 1. through coupling $(x_0, x_1)$ with optimal transport; 2. through coupling $(x_0, x_1)$ by training an additional CNF and letting $x_1$ be generated from $x_0$ through the trained CNF.\n\nThe author justifies the usage of their CNF-coupling by showing the error bound wrt wasserstein distance between the learned distribution and the target distribution. The experimental results also showcase the effectiveness of the proposed method, through various of peptide tasks.\n\nIn summary, the reviewer gives a weak acceptance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is simple and effective, which improves the training stability of NFs\n\n2. The author provides mathematical justification for using the CNF-coupling, by showing the wasserstein distance between trained model and target.\n\n3. Though the CNF-coupling sounds expensive at the first glance, the author shows that it requires much less computational overhead in table 4.\n\n4. The experimental results are good"}, "weaknesses": {"value": "1. The main concern lies in the invertibility of coupling, as both OT-coupling and CNF-coupling are approximations. It would be great if the author could elaborate on when this approximation would be broken and, in such a case, how the classic MLE objective would help. Intuitively speaking, if $p_0$ and $p_1$ are too separate, the true velocity might be less smooth and the (t-dependent) Lipschitz constants can be large, which means the Wasserstein bound can be very loose.\n\n2. The experiments in this paper focus on training NFs and then doing importance sampling to get equilibrium samples. However, in the plots, such as Figure 2, the author only shows the reweighted energy histogram but not the resampled Ramachandran plots. The reviewer thinks it is important as well to show the rama-plots with recalibrated samples."}, "questions": {"value": "1. Can the author also show the training time comparison analogous to table 4 on other benchmarks?\n\n2. Table 3 is a bit confusing. Why are the inference times of the same NF but different training methods (MLE/RegFlow) different?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lslsqBfI8I", "forum": "ctdnzPxDI3", "replyto": "ctdnzPxDI3", "signatures": ["ICLR.cc/2026/Conference/Submission14406/Reviewer_dEB8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14406/Reviewer_dEB8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760958199304, "cdate": 1760958199304, "tmdate": 1762924816768, "mdate": 1762924816768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript outlines a new approach to train one-step generative models, specifically normalizing flows, which allow exact sample likelihood evaluations. This work is important as likelihood evaluation is a bottleneck for applications of NF in the sciences, as most prominently outlined here: Boltzmann Generators. The idea outlined in this paper is simple and effective: train a regular NF against a pre-specified flow (e.g. either a pre-trained Continuous NF, or a pre-computed OT flow). This allows for faster and more stable training and more efficient sample likelihood evaluations in most cases.\n\nWhile the method seems like it still has some ways to go to be ready for prime-time, I find that the paper, overall, is an interesting conceptual step. Consequently, i am willing to increase my score if the concerns below are addressed."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Conceptually clear and well written manuscript. Numerous insightful comments about normalizing flows.\n- Well thought-out experiments and evaluations. All fairly standard in the field now, but still well done.\n- Clear performance gain, in terms of compute-time, over most of the included baselines."}, "weaknesses": {"value": "- Claims and attribution. The proposed TFEP method is closely related to the ambient thermodynamic interpolant approach by Moqvist et al https://arxiv.org/abs/2411.10075 \n- Sample quality and scaling. ESS remain fairly low. Scaling to tetra peptides is nice several other recent works e.g. https://arxiv.org/abs/2502.18462 demonstrate scaling to significantly larger systems. \n- Lack of error estimates on evaluation statistics."}, "questions": {"value": "- How do the authors envision this approach scale to larger systems? Comparing figures 7 and 8 it seems like the RegFlows miss important details that might be important for performance, and one would expect this to only increase with system size.\n- Are observables --- e.g. free energies --- computed under the DiT-based CNF meaningfully different from those under the reweighed values from the presented RegFlow approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WMVZiDdgX7", "forum": "ctdnzPxDI3", "replyto": "ctdnzPxDI3", "signatures": ["ICLR.cc/2026/Conference/Submission14406/Reviewer_fKR3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14406/Reviewer_fKR3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761516234626, "cdate": 1761516234626, "tmdate": 1762924815754, "mdate": 1762924815754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes REGFLOW, an approach for training a normalizing flow that performs better than traditional maximum likelihood training.  The approach regresses to predetermined flows, either from another model or precomputed optimal transport couplings. This approach results in substantial better normalizing flow models than traditional training."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper addresses a chronic problem with normalizing flows.  The described regression loss is intuitive an simple to implement (once couplings have been determined).  The approach results in dramatic improvements compared to MLE training using the same models and data.  Sensitivity to some parameters (e.g. regularization) is explored. I appreciate the evaluation of free energy.  The paper is well written and easy to follow."}, "weaknesses": {"value": "Although the improvement compared to NF models is extreme, the results aren't necessarily state-of-the-art compared to other models."}, "questions": {"value": "Why are NFs trained with REGFLOW substantially faster at computing likelihoods?\n\nHow does increasing the number of OT couplings improve performance? What if the OT is approximate?\n\nWhat is the basis for the statement that beyond a certain level of regularization that numerical invertability is guaranteed? Is this an empirical statement, or is there are proof (if the former, than perhaps an alternate wording would be more accurate)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "887A9L7Rce", "forum": "ctdnzPxDI3", "replyto": "ctdnzPxDI3", "signatures": ["ICLR.cc/2026/Conference/Submission14406/Reviewer_Upe7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14406/Reviewer_Upe7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973523657, "cdate": 1761973523657, "tmdate": 1762924815347, "mdate": 1762924815347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}