{"id": "oEtrDiFOFF", "number": 12797, "cdate": 1758210379884, "mdate": 1759897484685, "content": {"title": "Riemannian Federated Learning via Averaging Gradient Streams", "abstract": "Federated learning (FL) as a distributed learning paradigm has a significant advantage in addressing large-scale machine learning tasks. \nIn the Euclidean setting, FL algorithms have been extensively studied with both theoretical and empirical success. However, there exist few works that investigate federated learning algorithms in the Riemannian setting. In particular, critical challenges such as partial participation and data heterogeneity among agents are not explored in the Riemannian federated setting. This paper presents and analyzes a Riemannian FL algorithm, called RFedAGS, based on a new efficient server aggregation---averaging gradient streams, which can simultaneously handle partial participation and data heterogeneity. We theoretically show that the proposed RFedAGS has global convergence and sublinear convergence rate under decaying step sizes cases; and converges sublinearly/linearly to a neighborhood of a stationary point/solution under fixed step sizes cases.  These analyses are based on a vital and non-trivial assumption induced by partial participation, which is shown to hold with high probability. Extensive experiments conducted on synthetic and real-world data demonstrate the good performance of RFedAGS.", "tldr": "This paper presents and analyzes a federated learning algorithm that can handle generic manifold constraints, partial participation, and data heterogeneity.", "keywords": ["Riemannian federated learning", "Averaging gradient streams", "Partial participation", "Heterogeneity data", "Riemannian distributed optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2fc0ee7a3de720af69bfae93aceb2054fd49bc47.pdf", "supplementary_material": "/attachment/dbafa0c6854bc26a82045f19de510755edb00559.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose RFedAGS, a federated learning algorithm optimized for optimization on Riemannian manifolds under realistic scenarios of partial client participation and non-IID data. The method introduces a server aggregation strategy called Averaging Gradient Streams (AGS), which averages transported gradient information instead of local model parameters. This approach addresses the non-linearity of manifolds and avoids costly exponential map computations. The authors theoretically demonstrate global and sublinear convergence under decaying step sizes, as well as sublinear/linear convergence to a neighborhood under fixed step sizes. These results are supported by an assumption that holds with high probability as training progresses. Extensive experiments on both synthetic and real-world manifold-structured datasets showcase RFedAGS’s ability to achieve superior accuracy and faster convergence compared to existing Riemannian FL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces an aggregation mechanism, Averaging Gradient Streams (AGS), which avoids direct parameter averaging on curved manifolds by accumulating and transporting gradient updates in a common tangent space. This design elegantly resolves the geometric inconsistency inherent in prior Riemannian FL methods and extends federated optimization to non-Euclidean domains under partial client participation—a setting unexplored before.\n\n- The work demonstrates theoretical depth and rigor, providing global and sublinear convergence guarantees under decaying step sizes and linear convergence under Riemannian PL/strong convexity conditions. The proofs are comprehensive, grounded in standard FL and manifold optimization assumptions, and even include a non-trivial probabilistic justification (Assumption 3.8) for estimating client participation probabilities.\n\n- Despite the technical complexity, the paper is logically structured, moving smoothly from motivation to algorithm design, theory, and experiments. The authors provide sufficient mathematical background, clear notation, and consistent use of terminology. While additional intuition could help non-specialists, the exposition is coherent and reproducible, with algorithmic and experimental details clearly documented.\n\n- The proposed RFedAGS algorithm broadens the applicability of federated learning to manifold-constrained models (e.g., SPD matrices, hyperbolic embeddings, Stiefel subspaces), an area of high emerging interest."}, "weaknesses": {"value": "- While the paper is theoretically rigorous, its dense and mathematically demanding nature may hinder its accessibility to a broader audience at ICLR. Consequently, many critical ideas, such as the geometric intuition behind averaging gradient streams and how vector transport preserves consistency, are primarily presented in formal notation.\n\n- Although the paper claims to handle arbitrary participation and non-IID data, the experiments do not explicitly vary these factors to show robustness. It remains unclear how the algorithm performs under different participation rates or degrees of client heterogeneity.\n\n- The convergence proof relies on Assumption 3.8, which establishes a bound on the estimation error between the true and empirical participation probabilities. While this assumption is theoretically justified, the paper lacks a clear description of how these probabilities are computed in practice and how sensitive the algorithm is to their inaccuracies."}, "questions": {"value": "1. The convergence proofs rely heavily on Assumption 3.8, which bounds the deviation between estimated and true participation probabilities. However, it remains unclear how $q_{i,t}$ is actually computed during training.\n- Are these probabilities updated as empirical participation frequencies over rounds, or are they fixed a priori?\n- How sensitive is RFedAGS to inaccurate or time-varying participation estimates (e.g., if some clients drop out permanently)?\n\n2. While the theory emphasizes arbitrary participation and heterogeneous data, the experiments do not explicitly test these conditions.\n- Could the authors provide additional experiments that vary (a) the proportion of participating clients per round and (b) the degree of data heterogeneity across clients?\n- How does RFedAGS perform compared to baselines as participation becomes sparse or data distributions diverge?\n\n3. The proposed AGS framework involves transporting and averaging gradients in the manifold’s tangent space, which may introduce additional computational overhead compared to standard Riemannian FedAvg.\n- How does this affect runtime and communication efficiency when the number of clients or model dimensionality scales up?\n- Are there specific manifolds (e.g., Stiefel or SPD) where vector transport becomes a bottleneck?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GjQkJNoZ3A", "forum": "oEtrDiFOFF", "replyto": "oEtrDiFOFF", "signatures": ["ICLR.cc/2026/Conference/Submission12797/Reviewer_iW4m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12797/Reviewer_iW4m"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761682920092, "cdate": 1761682920092, "tmdate": 1762923606062, "mdate": 1762923606062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "They propose RFedAGS, a Riemannian Federated Learning algorithm that aggregates gradient streams instead of model parameters to handle manifold curvature and arbitrary partial participation. The method introduces a new aggregation correction AGS-AP, supported by convergence proofs under generalized retraction and bounded vector transport assumptions. Experimental results on several manifold-based tasks  show consistent improvement over prior Riemannian FL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The algorithm avoids the computational burden of exponential map inverses and parallel transport used in earlier Riemannian FL methods.\n\n2. It is the first Riemannian FL method proven to work under arbitrary partial participation."}, "weaknesses": {"value": "1. Theoretical clarity and novelty: While the proposed framework claims to generalize existing Riemannian FL methods by relaxing the requirements on retraction and vector transport, the theoretical advancement remains unclear. Specifically, the main difficulty in proving convergence under assumptions like 3.1, 3.2, and 3.5 is not explicitly articulated. The authors should clarify why convergence analysis becomes more challenging under generalized retraction and bounded vector transport, and in what way their proof techniques go beyond those established in prior works. In other words, the paper should highlight which parts of the analysis cannot be handled by the existing Riemannian FL theoretical tools and why this generalization is nontrivial.\n\n2. Significance of AGS-AP extension: The transition from AGS-RS to AGS-AP appears to be a relatively straightforward correction that compensates for non-uniform participation probabilities by reweighting expectations. While this adjustment enables handling arbitrary participation, it is not evident that it introduces fundamentally new theoretical challenges. The proposed fix seems more like an incremental adaptation rather than a substantial methodological contribution. The authors should therefore elaborate on why the treatment of partial participation in the Riemannian context poses unique analytical obstacles that cannot be addressed by simply adapting existing Euclidean analyses with weighted expectations.\n\nIn summary, the current formulation does not convincingly demonstrate that the proposed theoretical extensions constitute a significant leap beyond existing works. The authors are encouraged to explicitly contrast their convergence analysis with prior proofs, detailing where prior methods would fail or become inapplicable under their new setting."}, "questions": {"value": "1. Mislabels algorithm in line 438.\n\n2. In A.1.4 it seems like the effect of heterogeneity is almost unseen as the convergence improves consistently when K increases. Is it possible to show results for K > 10? Since the algorithm is not designed to mitigate heterogeneity, there should be a certain level of performance degradation observed with extremely large K."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fbEFfPmaVx", "forum": "oEtrDiFOFF", "replyto": "oEtrDiFOFF", "signatures": ["ICLR.cc/2026/Conference/Submission12797/Reviewer_mABS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12797/Reviewer_mABS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851142068, "cdate": 1761851142068, "tmdate": 1762923605541, "mdate": 1762923605541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RFedAGS, a new Riemannian federated learning algorithm that replaces parameter averaging with gradient-stream aggregation to preserve linearity in tangent spaces and improve computational efficiency. RFedAGS proposes an efficient retraction-based aggregation which eliminates the need for the computationally burdensome inverse exponential map or parallel transports required by prior Riemannian FL methods, and theoretically establishes global (and sublinear/linear neighborhood) convergence under various step size regimes. Extensive experimental results are provided across synthetic and real-world tasks, demonstrating RFedAGS’ advantages over competing Riemannian FL baselines under general participation and data heterogeneity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper proposes and analyzes RFedAGS, a Riemannian federated learning algorithm that introduces a new and efficient server aggregation scheme based on averaging gradient streams.\n\nThe method is designed to effectively handle both partial client participation and data heterogeneity. Theoretical analysis establishes that RFedAGS achieves global convergence and a sublinear convergence rate under decaying step sizes, and further converges sublinearly or linearly to a neighborhood of a stationary point or solution when using fixed step sizes. Extensive experiments on both synthetic and real-world datasets demonstrate the strong empirical performance and stability of the proposed approach."}, "weaknesses": {"value": "1. Limited novelty. The key idea—aggregating gradient flows in tangent space—is conceptually straightforward once the FedAvg update is projected to a manifold setting. \n2. The paper lacks a argument for why RFedAGS offers a distinct or superior geometric interpretation.\n3. Limited Scope of Baselines: Although several strong Riemannian FL baselines are included (RFedAvg, RFedSVRG, RFedProj) for targeted tasks, more recent algorithms are not considered, e.g., Wang et al., 2025 [1].\n4. Some results tied too closely to specific manifolds. Experiments and implementation notes (Appendix A.3) are mostly focused specific manifolds. Broader applicability to more exotic or high-dimensional manifolds remains an open question\n5. Although the authors claim computational efficiency (no need for exponential/logarithmic maps), no quantitative results support this.\n\n[1] Wang H, Pan Z, He C, et al. Federated Learning on Riemannian Manifolds: A Gradient-Free Projection-Based Approach[J]. arXiv preprint arXiv:2507.22855, 2025."}, "questions": {"value": "1. Can the authors provide empirical or theoretical discussion regarding the scalability of the method as the number of agents, local dataset size, or manifold dimension increases? \n2. Could the method be efficiently applied to other manifolds? Are there limitations?\n3. Could the method be compared with recent or advanced Riemannian federated learning algorithms (e.g., Wang et al., 2025 [1]) ?\n4. The paper claims computational efficiency due to the removal of exponential/logarithmic maps, yet provides no quantitative analysis.\nCould the authors offer detailed communication and computation cost metrics per round, beyond total CPU time, to support this claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U3keTa78D9", "forum": "oEtrDiFOFF", "replyto": "oEtrDiFOFF", "signatures": ["ICLR.cc/2026/Conference/Submission12797/Reviewer_mU3X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12797/Reviewer_mU3X"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948704698, "cdate": 1761948704698, "tmdate": 1762923604588, "mdate": 1762923604588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new federated optimization algorithm for learning over Riemannian manifolds. The proposed RFedAGS allows efficient and theoretically sound updates even under partial participation and non-IID data. The authors provide convergence guarantees and validate RFedAGS on synthetic and real datasets, showing consistent improvements over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The proposed aggregation mechanism is novel and easy to follow.\n* The paper provides comprehensive convergence analysis.\n* The main experiments effectively demonstrate the proposed method’s effectiveness."}, "weaknesses": {"value": "* While I understand the reasonableness of $G$, I am wondering what the value of $G$ would be when the true probabilities are not available to the server in the experiments.\n* How are the data partitioned across clients? How many total clients are included in the experiments, and what is the client participation ratio?\n* The ablation study is somewhat limited, and the sensitivity of several important parameters is missing—for example, different participation ratios, varying numbers of local steps, and comparisons between using approximate probabilities and true probabilities.\n* The assumption of Lipschitz continuity for each $f_i$ seems a bit strong, although it may be necessary for the Riemannian SGD convergence analysis. I am also curious whether this Lipschitz continuity can be empirically verified in the experiments."}, "questions": {"value": "See questions in the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BMszM2Agv4", "forum": "oEtrDiFOFF", "replyto": "oEtrDiFOFF", "signatures": ["ICLR.cc/2026/Conference/Submission12797/Reviewer_zBLo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12797/Reviewer_zBLo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981096106, "cdate": 1761981096106, "tmdate": 1762923603927, "mdate": 1762923603927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}