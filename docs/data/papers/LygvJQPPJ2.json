{"id": "LygvJQPPJ2", "number": 9891, "cdate": 1758146765150, "mdate": 1759897688692, "content": {"title": "Learning to Reason About Code Insecurity: Composite-Reinforcement Fine-Tuning for Cognitive Alignment", "abstract": "Automated vulnerability analysis increasingly relies on language models, yet even strong LLMs exhibit unstable security reasoning: they either over-flag benign code or miss critical flaws, particularly under cross-language shifts. We present ARGO--Composite-Reinforcement Fine-Tuning for Cognitive Alignment--a label-efficient training framework that explicitly optimizes a composite reward combining (i) label-based decision scoring via a strictly proper scoring rule on predicted probabilities, (ii) explanation grounding and consistency through structure- and code-referencing heuristics that do not use CWE labels or definitions, and (iii) output-format coherence through a strict schema validator. This moves the objective from bare classification toward deliberative, auditable analysis while explicitly acknowledging and isolating the supervised component in the reward. We cast each example as a short two-phase episode: first, the policy produces an explanation; then it deterministically emits a calibrated probability through a regression head. The binary decision is deterministically derived from the probability at inference (thresholding) rather than being sampled as a separate action. Policy updates are stabilized via batch-level affinity-weighted neighborhood smoothing over deterministic encoding and a KL trust term to a reference policy. Across BIGVUL, DIVERSEVUL, and CLEANVUL, ARGO consistently improves macro-F1 over strong baselines (e.g., up to 0.71 in-distribution; substantial gains under cross-language transfer). Compared to standard supervised fine-tuning, ARGO reduces catastrophic bias toward predicting the vulnerable class and improves recognition of benign code without relying on CWE supervision.", "tldr": "", "keywords": ["Cognitive Alignment", "Reinforcement Learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/75c6ffe5faad6ec50abced1fc1a063f93f0d760b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ARGO (Composite-Reinforcement Fine-Tuning for Cognitive Alignment), a training framework designed to make large language models (LLMs) both confidence-calibrated and explainable in the context of bug detection.\n\nARGO operates in two stages: first, the model analyzes the input code and generates an explanation describing the potential bug; then, it outputs a probability score indicating the confidence that a vulnerability exists.\n\nExperimental results show that ARGO achieves higher F1 scores and better confidence calibration than baseline models on bug detection datasets such as BigVal."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The problem addressed is meaningful. Bug detection is not a simple binary classification task—both the explanation and the model’s confidence are crucial. This paper aims to ensure that the model’s explanations are aligned with its detection confidence.\n- The evaluation introduces detailed metrics for assessing confidence calibration and carefully considers randomness and potential data contamination."}, "weaknesses": {"value": "- Although the paper aims to improve the quality of explanations in bug detection, the proposed reward only encourages the model to include *real code snippets* in its explanation. In many cases, even if the explanation refers to actual code entities, the underlying reasoning may still be incorrect.\n- This paper is not well-presented. The introduction introduces many concepts without sufficient clarification, and the logical flow is fragmented, making it easy for readers to get lost."}, "questions": {"value": "* Why does referencing real code entities in the explanation improve the model’s confidence?\n* When the model’s reasoning in the first (explanation) stage is incorrect, how can the system correct or compensate for it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "PxJhBvcoOL", "forum": "LygvJQPPJ2", "replyto": "LygvJQPPJ2", "signatures": ["ICLR.cc/2026/Conference/Submission9891/Reviewer_xwxp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9891/Reviewer_xwxp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704692459, "cdate": 1761704692459, "tmdate": 1762921354303, "mdate": 1762921354303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ARGO, a composite-reinforcement fine-tuning scheme for vulnerable-code detection that makes each sample a two-phase episode: the model first produces a structured rationale (Issue/Evidence/Mitigation) grounded in identifiers/spans, then outputs a single calibrated probability; the binary decision is a deterministic threshold on that probability. The training objective couples a proper-scoring term with label-free “grounding” and format signals (no CWE supervision or retrieval during training), aiming to reduce default-to-vulnerable bias and improve auditability and calibration. Evaluation on DIVERSEVUL, BIGVUL, and CLEANVUL compares ARGO primarily against zero-shot (±reasoning) and standard SFT under matched token/epoch budgets, reporting higher macro-F1, notably better benign recall, and improved calibration (ECE/Brier); the paper also highlights cross-language gains when training on C and testing on Java/Python/JavaScript."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Composite objective with concrete, label-free signals. Proper-scoring on the probability plus grounding/format checks computed without CWE supervision or retrieval. \n- Consistent empirical improvements on the reported setups. Figures/tables show higher macro-F1 and better benign recall vs. SFT/zero-shot on BigVul/DiverseVul/CleanVul, and stronger cross-language transfer when training on C (e.g., 0.51 vs. 0.25 on Java; 0.64 vs. 0.47 on Python)."}, "weaknesses": {"value": "- Baselines are limited to zero-shot (±reasoning) and standard SFT under matched budgets; stronger alternatives (e.g., other calibrated training or RL-from-rationales) are not included.\n- No in-depth analysis and ablation studies for designs.\n- They evaluate on DIVERSEVUL, BIGVUL, and CLEANVUL at snippet/function granularity (CLEANVUL is explicitly function-level), and the paper provides no per-CWE coverage tables.\n- Leveraging reasoning for vulnerable code is not very new.\n- Many terms are abused in this paper, such as the \"unsupervised\" in format coherence reward."}, "questions": {"value": "1. Can you provide experiment result of more baselines such as [1]?\n2. Can you carry out ablation studies to justify each design in ARGO?\n\n> [1] Weyssow, Martin, et al. \"R2Vul: Learning to Reason about Software Vulnerabilities with Reinforcement Learning and Structured Reasoning Distillation.\" arXiv preprint arXiv:2504.04699 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There are no specific ethics concerns in this paper."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DGNDEOjn4s", "forum": "LygvJQPPJ2", "replyto": "LygvJQPPJ2", "signatures": ["ICLR.cc/2026/Conference/Submission9891/Reviewer_A8w5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9891/Reviewer_A8w5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761780788267, "cdate": 1761780788267, "tmdate": 1762921353771, "mdate": 1762921353771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims at using LLM to detect vulnerability. It proposes an RL training algorithm with a novel set of rewards.\nWhile the paper is targeting at an important problem, I have significant concerns about its soundness and evaluation setups."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "It targets an important problem with creative training objective design."}, "weaknesses": {"value": "1. The experiment setup contradicts with consensus in the vulnerability detection domain. The experiment is conducted on individual functions. However, it is well established that the vulnerability of code snippet should consider coding contexts. It might not be meaningful to consider function separately when reasoning about its security.\n\n2. The unsupervised reward design lacks rationale.\nThe paper proposes three reward signals for evaluating whether a natural language explanation about the give code is plausible. First, it evaluates how many variables in the code are mentioned in the explanation (more means better).\nSecond, it evaluates whether the sentence matches code statements. Actually, I cannot understand the reward clearly from the writing. At line 199, the function S(x) and function match(u, s) were not defined. I don't know what those functions mean.\nThird, it evaluates whether the reasoning mentions \"Issue\", \"Evidence\", and \"Mitigation\" sections in the explanation.\n\nFrom my understanding, the rationale behind those reward signals are unclear. For example, it is unclear why an explanation that mentions more variables is a better explanation about certain vulnerability. Moreover, the reward design seems very vulnerable to reward hacking. An explanation that simply repeats the code line by line will result a high reward. It is not related to the final task (i.e., vulnerability detection) at all.\n\nThe unclear rationale behind reward design significantly undermines the paper's technical contribution.\n\n3. The RL algorithm lacks justification.\nThe paper proposes novel training losses to compute the advantages. It proposes a novel term named \"surprise statistic\". However, there is no ablation study to justify the effectiveness and improvement of the new algorithm. The contribution of such algorithm is unclear."}, "questions": {"value": "Please see the above discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3ULeBTIhJs", "forum": "LygvJQPPJ2", "replyto": "LygvJQPPJ2", "signatures": ["ICLR.cc/2026/Conference/Submission9891/Reviewer_hGKc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9891/Reviewer_hGKc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969258573, "cdate": 1761969258573, "tmdate": 1762921353357, "mdate": 1762921353357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}