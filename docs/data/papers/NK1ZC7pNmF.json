{"id": "NK1ZC7pNmF", "number": 16076, "cdate": 1758259477373, "mdate": 1759897263442, "content": {"title": "Benchmarking and Enhancing VLM for Compressed Image Understanding", "abstract": "With the rapid development of Vision-Language Models (VLMs) and the growing demand for their applications, efficient compression of the image inputs has become increasingly important. Existing VLMs predominantly digest and understand high-bitrate compressed images, while their ability to interpret low-bitrate compressed images has yet to be explored by far. In this paper, we introduce the first comprehensive benchmark to evaluate the ability of VLM against compressed images, varying existing widely used image codecs and diverse set of tasks, encompassing over one million compressed images in our benchmark. Next, we analyse the source of performance gap, by categorising the gap from a) the information loss during compression and b) generalisation failure of VLM. We visualize these gaps with concrete examples and identify that for compressed images, only the generalization gap can be mitigated. Finally, we propose a universal VLM adaptor to enhance model performance on images compressed by existing codecs. Consequently, we demonstrate that a single adaptor can improve VLM performance across images with varying codecs and bitrates by 10%-30%. We believe that our benchmark and enhancement method provide valuable insights and contribute toward bridging the gap between VLMs and compressed images.", "tldr": "Benchmarking and Enhancing VLM for Compressed Image Understanding.", "keywords": ["Benchmark", "Image Coding for Machines", "Vision-Language Model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a0fa49f4c8d387c453048b08be43a21a26893723.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a comprehensive benchmark to evaluate VLM performance on compressed images. It analyzes performance degradation using a novel \"Information Gap\" vs. \"Generalization Gap\" framework and proposes a lightweight adapter to close the latter, effectively improving the VLM's robustness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's primary strength lies in its extensive experiments conducted across a diverse range of MLLMs and codecs. \n\n2. This comprehensive testing provides valuable insights for the community into the impact of image compression on MLLM performance. Additionally, the paper is well-written, clear, and easy to follow."}, "weaknesses": {"value": "1. The Phrasing of Some Findings Lacks Rigor\n\nFinding 2: The claim that \"Stronger VLMs perform better on compressed images\" is not consistently supported. For instance, according to Table 2, the more powerful model, Qwen-VL-2.5, paradoxically exhibits a greater performance degradation on several metrics compared to the weaker Qwen-chat-7B.\n\nFinding 3: The statement \"Generative codecs offer better semantic reconstruction, benefiting VLM-oriented coding\" requires more nuance. As shown in Figure 4, generative codecs have significant limitations on the OCRBench task, performing far worse than conventional codecs. Furthermore, the superiority of generative codecs is often confined to low bitrates. At higher bitrates, they can introduce hallucinations, which is unacceptable for tasks that demand high-fidelity understanding.\n\nFinding 4: The paper's use of the term \"scaling law\" seems to refer to the observation that \"a larger model does not necessarily lead to a smaller performance drop.\" This deviates from the commonly understood definition of scaling laws, which describes how a model's absolute performance improves with scale. It is debatable whether the authors' observation should be framed as a \"scaling law.\"\n\n2. The paper decomposes VLM performance degradation into an \"Information Gap\" and a \"Generalization Gap,\" but this classification lacks sufficient theoretical grounding and can be ambiguous in practice.\nFor example, consider an OCR image where the letter \"O\" is compressed into what looks like a \"C\" with a slight opening. This is a form of information loss, intuitively an \"Information Gap.\" However, it is plausible that a fine-tuned model could correctly identify the letter as \"O\" by leveraging context. The paper's framework makes it difficult to classify this scenario. Is the model's ability to infer the correct character by context part of closing a \"Generalization Gap\"? This ambiguity blurs the lines between the two concepts.\n\n3. A significant body of work already exists under the umbrella of \"video/image coding for machine\" (VCM/ICM). MLLM-based understanding is a task that falls squarely within this category. The proposed method should be benchmarked against specialized approaches designed for machine vision tasks to provide a more comprehensive evaluation. Relevant works for comparison include:\n\n[1] \"TransTIC: Transferring Transformer-based Image Compression from Human Visualization to Machine Perception,\" ICCV 2023.\n\n[2] \"Image Compression for Machine and Human Vision With Spatial-Frequency Adaptation,\" ECCV 2024.\n\n4. The evaluation of the proposed adapter is limited, as its effectiveness was only demonstrated on the POPE and SEEDBench benchmarks. To substantiate its claims of being a general solution, the adapter's performance needs to be tested on a broader set of benchmarks, particularly on those where performance varies significantly across codec types, such as OCRBench and MME."}, "questions": {"value": "see weakness. The authors provide a lot of preliminary experiments, but need to reinterpret their findings and their theory and provide stronger validation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oqGTCOXvsz", "forum": "NK1ZC7pNmF", "replyto": "NK1ZC7pNmF", "signatures": ["ICLR.cc/2026/Conference/Submission16076/Reviewer_wXUs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16076/Reviewer_wXUs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761034230220, "cdate": 1761034230220, "tmdate": 1762926262995, "mdate": 1762926262995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a comprehensive benchmark for evaluating VLMs in the context of compressed image understanding. \nThe authors assess VLM performance on over one million compressed images across 11 different image codecs and various tasks.\nThe key findings is about the significant performance degradation when VLMs process compressed images. \nThey claim that it is due to two main factors: information loss during compression and a generalization gap where the model struggles to adapt to distorted images."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a large scale benchmark with over one million compressed images to evaluate VLMs. It covers a wide range of image codecs and tasks.\n2. It reasonably and effectively identifies and distinguishes the information loss and generalization gaps for VLM loss.\n3. The authors propose a lightweight VLM adaptor for the generalization gap, which improves performance by 10%-30% across different codecs and bitrates.\n4. The method generally offers potential for real-world applications where compressed images are prevalent."}, "weaknesses": {"value": "1. This benchmark is not tested with the latest VLM models, such as GPT-4o and Gemini 2.5.\n2. This paper focuces on addressing the generalization gap but does not provide extensive solutions for dealing with information loss due to compression.\n3. Since the benchmark is heavily based on specific tasks, the findings and experimental results might not generalize well to tasks that are not covered in the benchmark.\n4. The bitrate range is about 0.0-0.30, which is a quite low bitrate range. Experiments should be extended to high range of bitrates since we typically do not compress images that heavily in our daily usage."}, "questions": {"value": "1. What will happen when the method is applied to more complex or diverse real-world settings or reasoning process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VkSko6mJXA", "forum": "NK1ZC7pNmF", "replyto": "NK1ZC7pNmF", "signatures": ["ICLR.cc/2026/Conference/Submission16076/Reviewer_YY7A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16076/Reviewer_YY7A"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791698460, "cdate": 1761791698460, "tmdate": 1762926262635, "mdate": 1762926262635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluate Vision-Language Models (VLMs) on compressed images, covering over one million images across 11 codecs and multiple tasks. It systematically explores the performance degradation of VLMs under various compression methods and bitrates. The authors decompose the performance gap into an information gap and a generalization gap. To address the generalization gap, they propose a lightweight, universal VLM adaptor that improves performance by 10–30% across codecs without retraining the full model. The adaptor is trained via knowledge distillation on compressed images and incorporates codec-specific conditional embeddings. Experiments validate the adaptor’s effectiveness on both coarse- and fine-grained benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I think the paper has following strengths:\n\n1. It set up the first comprehensive benchmark for compressed images with VLMs, the author has experimented on major codecs and bitrates, which convinces me about the robustness of the benchmark.\n\n2. The proposed VLM adaptor is lightweight, codec-agnostic, and improves performance by 10–30% without requiring full model retraining.\n\n3. The author has clear motivation and problem definitions with an information gap (irreversible loss) and a generalization gap. I like the definition here."}, "weaknesses": {"value": "In general, I am satisfied with the paper, but small concerns remain:\n\n1. Though the author claims that the proposed adaptor can have good performance across different codecs and compression methods. I think there are some drawbacks:\n\nFor VLMs which share the same vision encoders (like Qwen-VL series, or other public VLMs which share CLIP vision encoders), I think the proposed method should also work, i.e., the proposed adaptor could be shared across different VLMs. But the author only uses QwenVL 3B to perform the experiments. I'm happy to see more VLM results here. \n\n2. Beyond the first drawback, I was wondering that, the adaptor could be shared between different vision encoders, i.e., the generalization gap may share across different models. The author could discuss it in the future."}, "questions": {"value": "I'm happy with the draft but please see the weaknesses section for my questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HyCJW3MRaq", "forum": "NK1ZC7pNmF", "replyto": "NK1ZC7pNmF", "signatures": ["ICLR.cc/2026/Conference/Submission16076/Reviewer_VCXK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16076/Reviewer_VCXK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938178672, "cdate": 1761938178672, "tmdate": 1762926262210, "mdate": 1762926262210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a comprehensive study on the performance of VLMs when understanding heavily compressed images. The paper systematically analyzes performance degradation across different codecs, VLMs, and evaluation tasks, providing valuable insights into the challenges of compressed image understanding. The paper identifies two key challenges for compressed images understanding: an information gap due to irreversible information loss, and a generalization gap caused by poor model generalization capacity on compression images. A universal VLM adaptor is then proposed and demonstrates promising results in mitigating generalization gap."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper focuses on compressed image understanding, which is highly relevant for real-world applications. It addresses a significant deployment gap in current VLM research.\n* The inclusion of diverse codecs across multiple bitrates provides a thorough evaluation framework. The coverage of distinct tasks ensures a holistic assessment of VLM capabilities.\n* The proposed VLM adaptor is lightweight and effective, demonstrating 10%-30% improvements across different codecs and bitrates without requiring full model fine-tuning."}, "weaknesses": {"value": "* As the adaptor is a key contribution of this work, more experimental validation should be provided. The experimental results only presents performance improvements on images compressed by the selected codecs during training, but lacks analysis on the generalization capability of the adaptor on images compressed by other unselected codecs. \n* It’s better to provide a performance comparison between the proposed adaptor and other existing methods for enhancing VLMs for compressed images understanding, if available.\n* The benchmark mainly focuses on question-answering and recognition tasks. It would be more convincing if incorporating evaluation on more tasks such as image captioning and text generation, which are also important VLM capabilities.\n* There are some spelling and detail errors that require careful correction.\n(1) Line 755: “dataset” instead of “dastaset”. \n(2) The header of Appendix C.1, C.2 and C.3: “RESULTS” instead of “RESUTLS”.\n(3) Figure 2 is not referenced or discussed in the main text.\n(4) The captions for several figures (e.g., Figure 4, Figure 5) should be more detailed. Specifically, the Y-axis labels lack clarification and need to be defined for better comprehension."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9Ax56yk4sZ", "forum": "NK1ZC7pNmF", "replyto": "NK1ZC7pNmF", "signatures": ["ICLR.cc/2026/Conference/Submission16076/Reviewer_RZtc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16076/Reviewer_RZtc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982181309, "cdate": 1761982181309, "tmdate": 1762926261709, "mdate": 1762926261709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}