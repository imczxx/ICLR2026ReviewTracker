{"id": "sGsZaw88DM", "number": 10344, "cdate": 1758167754302, "mdate": 1759897657137, "content": {"title": "GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models", "abstract": "Unlearning in large language models (LLMs) is becoming increasingly important due to regulatory compliance, copyright protection, and privacy concerns. However, a key challenge in LLM unlearning is unintended forgetting, where the removal of specific data inadvertently impairs the utility of the model and its retention of valuable, desired information. While prior work has primarily focused on architectural innovations, the influence of data-level factors on unlearning performance remains underexplored. As a result, existing methods often suffer from degraded retention when forgetting high-impact data.\nTo address this problem, we propose GUARD — a novel framework for Guided Unlearning And Retention via Data attribution. At its core, GUARD introduces a lightweight proxy data attribution metric tailored for LLM unlearning, which quantifies the ''alignment'' between the Forget and Retain sets while remaining computationally efficient. Building on this, we design a novel unlearning objective that assigns adaptive, nonuniform unlearning weights to samples, inversely proportional to their proxy attribution scores. Through such a reallocation of unlearning power, GUARD mitigates unintended retention loss. We also provide rigorous theoretical guarantees that GUARD significantly improves retention while maintaining forgetting metrics comparable to prior methods. Extensive experiments on the TOFU and MUSE benchmarks across multiple LLM architectures demonstrate that GUARD reduces utility sacrifice on the TOFU Retain Set by up to 194.92\\% in terms of Truth Ratio when forgetting 10\\% of the training data, and improves knowledge retention on the MUSE NEWS Retain Set by 16.20\\%, with comparable or very moderate increases in privacy loss compared to state-of-the-art methods.", "tldr": "", "keywords": ["Large Language Models Unlearning", "Data Attribution"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/31deebfede314b8bf2721a890237ca028349eb2e.pdf", "supplementary_material": "/attachment/4e32ea81950856e57ae57408f75e88a266f6867d.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses a critical challenge in machine unlearning for large language models (LLMs)—the trade-off between effective forgetting of targeted data and retention of useful knowledge. The proposed method, GUARD (Guided Unlearning and Retention via Data Attribution), introduces a data attribution–based framework that adaptively assigns unlearning weights inversely proportional to each sample’s influence on the retained data. Results demonstrate substantial improvements in knowledge retention while maintaining similar forgetting performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The attribution-driven unlearning method is underexplored in the literature and the proposed method is simple.\n\n2. The experiments show that the method is effective compared to baselines.\n\n3. The authors provide proofs supporting improvements in retention efficiency."}, "weaknesses": {"value": "1. The writing could be improved. For example the introduction is lengthy and reduant to some extent, which reduces clarity and readability.\n\n2. No other data attribution methods are tested. It would strengthen the paper to explore different attribution measures or analyze sensitivity to gradient noise."}, "questions": {"value": "See weaknesses.\n\nSince the proposed method uses the inner product of gradients as the proxy, I'm wondering why not just use the similarity of embeddings rather than the gradients? What would be the results compared to the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9aEVbGilwJ", "forum": "sGsZaw88DM", "replyto": "sGsZaw88DM", "signatures": ["ICLR.cc/2026/Conference/Submission10344/Reviewer_1Uua"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10344/Reviewer_1Uua"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878059867, "cdate": 1761878059867, "tmdate": 1762921676613, "mdate": 1762921676613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "GUARD is a retention-aware unlearning method for LLMs. It scores each forget sample by the inner product between its gradient and the average retain gradient, then applies temperature-controlled inverse weights to focus unlearning where it least harms retention. Theory shows reduced retain loss, comparable forgetting, and a better retention–forget tradeoff. Experiments on LLaMA 2 7B across several objectives consistently improve retain performance with similar or better forgetting, and the approach is simple and efficient to plug into existing pipelines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Recasts unlearning as retention aware weighting using a gradient alignment score, directly aligned with the goal of preserving retained knowledge.\n2. GUARD pipeline shows clear properties and theory that guarantees lower retain loss, comparable forgetting, and a better retention–forget tradeoff.\n3. Proposed GUARD pipeline gains across strong baselines and metrics, with large improvements on Retain while keeping Forget performance on target, also quite easy to plug into existing procedures.\n4. The description of the whole paper is quite clear."}, "weaknesses": {"value": "1. The core score is the inner product between a forget sample gradient and the average retain gradient. So I am curious about the effect of the inverse weighting by deriving it from an explicit objective, for example, maximizing loss on the forget set subject to a first-order constraint on the retain set, and present the solution through a Lagrangian or projection analysis.\n2. In the experiment section, the comparison is mainly focused on the current MU methods and with GUARD pipeline, and since the authors mentioned the novelty of GUARD pipeline with the current data attribution methods, why not add the comparison between these methods and GUARD in the experiment section.\n3. The experiment misses the sensitivity ablation of some parameters of GUARD training. In addition, I am curious about whether introducing GUARD shifts the optimal settings of the underlying unlearning methods. For example, does the preferred NPO $\\beta$ change with GUARD, and how do key parameters for other unlearning algorithms behave before vs. after applying GUARD?\n4. Prior work suggests RMU is a more stable unlearning algorithm than NPO. Including RMU as an additional baseline would strengthen the empirical validity of the proposed method."}, "questions": {"value": "Please see the weakness section, and I will consider raising the score if the authors address the problems clearly"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3TpkPdUNx9", "forum": "sGsZaw88DM", "replyto": "sGsZaw88DM", "signatures": ["ICLR.cc/2026/Conference/Submission10344/Reviewer_u1aC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10344/Reviewer_u1aC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965362781, "cdate": 1761965362781, "tmdate": 1762921676215, "mdate": 1762921676215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates LLM unlearning from a data attribution perspective. It proposes a data attribution algorithm GUARD, which first uses gradient information to estimate the influence of each data sample in the retain/forget data. Then performs unlearning training using the gradient-information weighted loss on each sample. The paper presents a theoretical guarantee to prove its effectiveness, and shows better performance than other baselines in the expeirments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed weighting method can be applied to many different LLM unlearning loss, and it seems to bring benefit to multiple method in the experiment.\n* The paper presents a theoratical gurantee about the weighting's effectiveness for unlearning training."}, "weaknesses": {"value": "* Potential unreasonable unlearning setting. Equation (2) computation on the weighting depends on original model $\\theta_0$ gradient on the $D_r$ dataset and forget set example. This does not look reasonable to me, since typical unlearning does not assume access to the pre-trained model weight before fine-tuning on the knowledge data.\n* Potential unreasonable data assumption. Assumption 1 (line 291) mentions condition 1, with $<\\bar{g_r}, \\bar{g_f}>0$, which seems abrupt and lacks sufficient motivation why is this case. Authors should provide more justification on why this holds.\n* Potential robustness concern on retain data selection. Since the attribution relies on both the forget data (user request) and retain data (may be arbitrary), this paper lacks discussion on the robustness of retain data selection, and how the retain data should be constructed.\n* Non-standard unlearning hyper-parameter selection. Appendix Page 26 presents the forgetting training hyper-parameter only involving 1 epoch, and batch size 1, which is not the common practice in previous work, that mostly involving more epoch training and best checkpoint selection like those in [1, 2].\n\n[1] Fan, Chongyu, et al. \"Simplicity prevails: Rethinking negative preference optimization for llm unlearning.\" arXiv preprint arXiv:2410.07163 (2024).\n\n[2] Maini, Pratyush, et al. \"Tofu: A task of fictitious unlearning for llms.\" arXiv preprint arXiv:2401.06121 (2024)."}, "questions": {"value": "* Appendix computation cost compares normal training and GUARD-guided weighting unlearning training, however, the main computation cost in obtaining the gradient information on all data sample."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C8b7Elm3Y3", "forum": "sGsZaw88DM", "replyto": "sGsZaw88DM", "signatures": ["ICLR.cc/2026/Conference/Submission10344/Reviewer_sr5u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10344/Reviewer_sr5u"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967478412, "cdate": 1761967478412, "tmdate": 1762921675293, "mdate": 1762921675293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GUARD, a method for LLM unlearning based on data attribution scores. Specifically, instead of using uniform weights on all data samples, the authors calculate the attribution score for a particular forget sample as the inner product of its gradient and the average gradient over the retain set. Then the weight for each forget sample is assigned to be inversely proportional to its attribution score, so that samples that might lead to a large performance drop on the retain set are assigned smaller weights. The authors further present theoretical results showing that GUARD can improve the sacrifice rate over the baseline gradient ascent method. Experiments on two datasets show that GUARD outperforms strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is simple intuitively, and experiments show that it outperforms strong baselines, especially on the retain performance.\n2. The results show that using the proposed sample weights consistently outperforms the uniform weights when applied to different unlearning methods, models, and datasets.\n3. The proposed method is theoretically grounded."}, "weaknesses": {"value": "1. The method assumes access to the retain set in order to calculate the attribution scores. However, what if we don't know what questions are in the retain set? Does the method still work if we use a general corpus, such as a subset of the pre-training corpus, to calculate the attribution score? Are there any ways to obtain some surrogates for the retain set?\n2. The method needs to first calculate gradients over the retain and forget set, which is more expensive than baselines. How much additional cost does this bring?"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VhvnJIDPzb", "forum": "sGsZaw88DM", "replyto": "sGsZaw88DM", "signatures": ["ICLR.cc/2026/Conference/Submission10344/Reviewer_9Kbs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10344/Reviewer_9Kbs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968598805, "cdate": 1761968598805, "tmdate": 1762921674516, "mdate": 1762921674516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}