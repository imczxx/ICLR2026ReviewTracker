{"id": "ZMQAGxMmE2", "number": 12155, "cdate": 1758206022391, "mdate": 1763559550349, "content": {"title": "Being More Lightweight and Practical: Mini-sized Contrastive Learning Pre-trained Models for Fine-grained Traffic Tasks", "abstract": "Fine-grained traffic prediction is critically important for mitigating traffic congestion in key urban areas and for providing lane-change guidance in autonomous vehicles and navigation systems. However, task-specific models are not efficient enough, city-scale pre-trained models often overlook fine-grained requirements, and the demand for extensive computational resources hinders practical deployment. To address this issue, we developed a lightweight pre-training framework, MiniTraffic. This framework leverages abundant road-level data to address lane-level data scarcity through a frequency domain stability augmentation module and captures road-lane correlations via contrastive clustering to construct small-scale graph structures, significantly reducing model parameters. Fine-tuning with minimal target data provides a unified and efficient solution for fine-grained traffic prediction. In multi-granularity traffic prediction tasks across six fine-grained datasets, MiniTraffic demonstrated superior performance compared to all existing baseline models. The MiniTraffic-related code, datasets, and pre-trained models are available at https://anonymous.4open.science/r/MiniTraffic-ICLR26/.", "tldr": "We propose a mini-sized pre-training model for fine-grained traffic. The model provides an efficient and accurate solution for fine-grained traffic with limited computational resources and data.", "keywords": ["Fine-grained traffic prediction", "Spatio-temporal modeling", "Lightweight models"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7fda04a18691c9fbc48e3b44cacddfa5a551091f.pdf", "supplementary_material": "/attachment/166aaa0ade1e60af1106cc8eb79ea264b6f931d3.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes MiniTraffic, a strategy to build mixed-granularity (both road level and lane-level) traffic forecasting models. MiniTraffic is developed with a pre-training and fine-tuning paradigm, which is first pre-trained over road-level datasets and fine-tuned with both road-level and lane-level datasets to ensure versatility. MiniTraffic is motivated by the fact that lane-level datasets are rare, but they share significant similarity with road-level datasets. Therefore, MiniTraffic pre-trains over road-level data and fine-tunes over a mixture of road and lane-level data. Extensive designs are made to enhance the pre-training process, such as the frequency domain perturbation, masked pre-training, localized attention, and contrastive learning between time steps and road segments. Experiments over mixed-granularity datasets are performed where MiniTraffic achieves improvements over existing task-specific and pre-trained multi-task models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear motivation and corresponding technical solution. This paper faces the problem that data for lane-level forecasting is rare, and proposes a corresponding method to borrow knowledge from road-level datasets. The methodology of pre-training and fine-tuning is indeed useful in this scenario, making the flow of the paper coherent and sound. \n2. Good organization and presentation. The paper is generally well-written and easy to follow and understand. \n3. Experiments involve lots of baselines and diverse real-world datasets. The results are good and showcase the improvements of MiniTraffic."}, "weaknesses": {"value": "1. Missing relevant methods to discuss/compare. First, as this paper focuses on pre-training and fine-tuning of spatio-temporal models, especially traffic forecasting, there are several important baseline methods that can be (and should be) compared to enhance the soundness of the paper, such as OpenCity (Li et al. 2024, https://arxiv.org/abs/2408.10269), STEP (Shao et al. 2022, https://arxiv.org/abs/2206.09113) and MetaST (Yao et al. 2019，https://arxiv.org/abs/1901.08518). All these methods aim to pre-train spatio-temporal models for better task adaptation, and more importantly, MetaST and OpenCity pre-train over diverse tasks and cities. Therefore, it is important that the authors should at least discuss or even compare some of them. Moreover, regarding transfer learning for spatio-temporal data, there are also several related work such as TransGTR (Jin et al. 2023, KDD 2023), and the authors may want to discuss some of them. \n2. Missing details of model architecture. The authors mention \"lightweight\" as a key motivation and contribution of this paper, and indeed, MiniTraffic is more efficient than other compared methods (e.g. Figure 7). However, the model structure of MiniTraffic is not clearly described, making it hard for readers to understand how the model is designed to achieve such light-weight property. The authors only mention a localized graph attention network as a model component, leaving a lot to be explained, e.g. any temporal modules like LSTM or attention, how many layers of GAT, etc.   \n3. Section 5.3 is confusing. The authors say that two pre-training mechanisms are compared, one using lane-level data sources, and the other using a single dataset. However, in Section 5.1, the authors use METR-LA, PeMS-BAY, PeMS-Road, Huanan-Road for pre-training, which are solely road-level datasets. Therefore, this section is confusing --- if pre-training with lane-level datasets yields better performances, why did the authors only mention road-level pre-training in Section 5.1? \n4. Some design choices can be better justified. For example, if MiniTraffic is light-weight, would it be feasible to use full-fine-tuning, instead of freezing the MiniTraffic backbone? Also, I am interested in the design choice of why the authors replace the  FDA head with extension for road-level fine-tuning, and retain FDA for lane-level fine-tuning. \n5. The uniqueness of lane-level traffic forecasting is not well-justified. It may be that I am missing something, but I am not fully convinced that lane-level traffic data is very different from road-level ones. The reasons are (1)Figure 1 shows that they are similar in terms of trend, and (2) The problem formulations are both graph-based forecasting. Therefore, would a more unified approach (i.e. without distinguishing road-level and lane-level data) achieve better results?"}, "questions": {"value": "Please refer to the Weakness section, especially 2-5."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LFRKbqNJpA", "forum": "ZMQAGxMmE2", "replyto": "ZMQAGxMmE2", "signatures": ["ICLR.cc/2026/Conference/Submission12155/Reviewer_UMuj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12155/Reviewer_UMuj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760761878228, "cdate": 1760761878228, "tmdate": 1762923110527, "mdate": 1762923110527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MiniTraffic, a lightweight pre-trained model designed for fine-grained traffic prediction, including both road-level and lane-level forecasting. The authors address key challenges in this domain: data imbalance between road and lane levels, lack of unified modeling frameworks, and the computational burden of large-scale models. MiniTraffic leverages frequency domain augmentation, contrastive clustering, and granularity-aware fine-tuning to transfer knowledge from abundant road-level data to scarce lane-level tasks. The model is evaluated on several datasets and outperforms the baselines while maintaining a minimal parameter count."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Originality: To the best of my knowledge, this is the first mini-scale pre-trained model specifically designed for multi-granularity traffic prediction. The idea of using frequency domain augmentation to simulate lane-level variability from road-level data is novel and well-motivated. The use of contrastive clustering to build sparse, semantic-aware graph structures is also a creative and efficient alternative to full-graph attention.\n\n- Quality: The paper is technically sound, with rigorous modeling, detailed ablation studies, and comprehensive experiments. The theoretical analysis of the FDA module, including perturbation bounds and generalization implications, adds depth. The experimental setup is robust, with comparisons across 29 baselines and 6 datasets, including both regular and irregular lane structures.\n\n- Clarity: The paper is well-structured and clearly written. Complex modules like FDA and contrastive clustering are explained with sufficient mathematical detail and intuitive justifications. Figures (e.g., Fig. 1, Fig. 3) effectively illustrate key concepts.\n\n- Significance: The work addresses a real-world, practical problem—the lack of lane-level data and the inefficiency of large models in edge deployment. MiniTraffic’s lightweight design and strong few-shot generalization make it highly relevant for smart city applications, especially in resource-constrained environments."}, "weaknesses": {"value": "- Limited Ablation on Frequency Domain Parameters: While the FDA module is central to the model, the paper only briefly analyzes the impact of the perturbation parameter λ. A more systematic sensitivity analysis on both λ and τ (the frequency masking threshold) would strengthen the understanding of their roles and robustness across datasets.\n\n- Scalability to Larger Urban Networks: All experiments are conducted on relatively small road networks. It is unclear how MiniTraffic scales to city-level networks with thousands of nodes. A discussion or experiment on scalability (e.g., training time, memory usage, performance degradation) would be valuable.\n\n- Transferability Across Cities: The model is pre-trained on California (PeMS) and Guangzhou (HuaNan) data. However, there is no cross-city evaluation (e.g., train on PeMS, test on HuaNan). Given the emphasis on domain transfer, evaluating generalization across cities with different traffic patterns would strengthen the claim of practical applicability.\n\n- Limited Discussion of Temporal Horizon Sensitivity: While the model is tested over horizons of 3, 6, and 12 steps, there is no analysis of error growth over time or discussion of long-term forecasting challenges (e.g., error accumulation and model divergence). This limits understanding of the model’s applicability in long-horizon planning tasks."}, "questions": {"value": "1. Cross-city generalization: How does MiniTraffic perform when pre-trained on one city (e.g., PeMS) and fine-tuned on another (e.g., HuaNan)? Does the frequency domain augmentation still help under distribution shift?\n\n2. Scalability: What is the computational complexity of MiniTraffic when applied to large-scale graphs (e.g., 1,000+ road segments)? How does the contrastive clustering step perform in terms of time and memory?\n\n3. FDA robustness: How sensitive is the model to the choice of λ and τ across different datasets? Is there an adaptive mechanism to tune these parameters automatically based on data characteristics?\n\n4. Long-horizon forecasting: How does the model behave for prediction horizons beyond 12 steps (e.g., 24 or 48)? Is there a degradation pattern, and if so, how does it compare to other baselines?\n\n5. Real-world deployment: Has the model been tested in online or streaming settings? How does it handle concept drift or missing data in real-time traffic systems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "E5Gq7Omo7B", "forum": "ZMQAGxMmE2", "replyto": "ZMQAGxMmE2", "signatures": ["ICLR.cc/2026/Conference/Submission12155/Reviewer_dsBq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12155/Reviewer_dsBq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908124132, "cdate": 1761908124132, "tmdate": 1762923109962, "mdate": 1762923109962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MiniTraffic, a lightweight pre-trained model specifically designed for fine-grained traffic prediction. The model leverages abundant road-level data for pre-training and transfers knowledge to scarce lane-level prediction tasks. The core methodology includes: (1) a frequency domain stability augmentation module to simulate lane-level variations, and (2) contrastive clustering to construct small-scale graph structures, compressing the model to only 100K parameters. Experimental results on six fine-grained datasets demonstrate that MiniTraffic outperforms all baseline models at both road-level and lane-level granularities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The lightweight pre-trained model design has substantial implications for real-world deployment in resource-constrained traffic systems.\n\n2. The proposed approach is reasonably novel, and the experimental comparison against 29 baseline models across multiple categories is thorough and convincing."}, "weaknesses": {"value": "1. The paper focuses on forecasting future trends, but provides limited explanation of how temporal dynamics are handled. How does the model capture temporal dependencies?\n\n2. Can road-level predictions be obtained by simply averaging lane-level predictions? How would this baseline compare to the proposed method? This comparison would strengthen the justification for the bidirectional modeling strategy.\n\n3. The training datasets are relatively small, raising concerns about generalization to larger networks. The paper should explicitly discuss the applicable scenarios, advantages, and potential limitations of the proposed method."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cdSkI86ufJ", "forum": "ZMQAGxMmE2", "replyto": "ZMQAGxMmE2", "signatures": ["ICLR.cc/2026/Conference/Submission12155/Reviewer_9r52"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12155/Reviewer_9r52"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989875679, "cdate": 1761989875679, "tmdate": 1762923109484, "mdate": 1762923109484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}