{"id": "l0QAezg8HX", "number": 17427, "cdate": 1758275912721, "mdate": 1763729766176, "content": {"title": "Preserving Gradient Harmony: A Rotation-Based Gradient Balancing for Multi-Task Conflict Remedy", "abstract": "Multi-task learning (MTL) enables knowledge sharing across tasks but often suffers from gradient conflicts, leading to performance imbalances among tasks. Existing weighting-based methods attempt to balance the directional conflicts by striving for the optimal weights computed from gradient or loss information. However, those indirect weighting operations face a limited balancing effect, as the gradient's per-dimensional sensitivities are omitted. Alternatively, gradient manipulation methods such as PCGrad, GradDrop, etc., directly control the task gradients to eliminate opposing gradient directions, but their over-aggressive operations potentially harm the gradient properties, leading to suboptimal updates. They are associated with the issues of over-correction, order dependence, and poor scalability in high-dimensional task settings. To overcome these limitations, we propose the Rotation-Based Gradient Balancing (RGB), a novel algorithm that rotates normalized task gradients toward a consensus direction using independently optimized per-task angle corrections. Unlike projections, rotations provide fine-grained control that preserves beneficial gradient components, reduces global conflicts holistically, and implicitly incorporates loss change information for balanced optimization. Empirical results demonstrate the effectiveness and consistency of RGB, achieving state-of-the-art performance in various datasets, where RGB is the first method on the QM9 dataset with 11 tasks to surpass single-task baselines on average, and its performance is consistent across various benchmarks ranging from 3–40 tasks. Moreover, we propose the concept of multi-task equilibrium relationship that is supported by our empirical experiment and inferring the phenomenon of miss-correction angular error. We also provide the theoretical global convergence of RGB to Pareto stationary under standard smoothness assumptions.", "tldr": "Gradient manipulation methods often over-correct task gradients; we introduce RGB to balance multi-task conflicts by optimally rotating them toward consensus, yielding state-of-the-art results.", "keywords": ["Multi-task Learning", "Optimization", "Deep Learning", "Gradient Conflicts", "Pareto Optimality"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/76c8c095cbc74fa7268f4e71d93e93da3f868cfa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the Rotation-Based Gradient Balancing (RGB) algorithm, which rotates normalized task gradients toward a consensus direction by independently optimizing angle corrections for each task. The RGB algorithm offers fine-grained control that preserves beneficial gradient components, comprehensively reduces global gradient conflicts, and implicitly integrates loss change information to achieve balanced optimization. Empirical evaluations across four datasets show its effectiveness. However, there are still some aspects in this paper that require improvement or further clarification."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1: The RGB algorithm offers precise control mechanisms that effectively preserve advantageous gradient components, systematically mitigate global gradient conflicts, and seamlessly integrate loss change information to facilitate balanced optimization.\nS2: Empirical evaluations conducted across multiple datasets convincingly demonstrate the efficacy of the RGB algorithm."}, "weaknesses": {"value": "W1: The innovation of the gradient rotation method is limited. Essentially, gradient rotation amounts to multiplying the original gradient by a rotated coordinate system. What is the fundamental difference between this approach and traditional gradient constraint or game-theoretic methods (such as CAGrad and Nash-MTL)?\nW2: In practical training scenarios, gradients may be unstable or contain noise. How does the gradient rotation method address the issues of unstable gradients or noise?\nW3: There is a lack of analysis on time and space complexity. A large number of gradient operations may pose challenges in terms of computational overhead when dealing with more tasks or larger models.\nW4: The demonstrated effectiveness is not entirely robust. For instance, on the NYUv2 dataset, the gradient rotation algorithm only achieves state-of-the-art performance on the Segmentation task.\nW5: There is a lack of visual analysis for visual tasks. For example, for the NYUv2 and Cityscapes datasets, it is recommended to provide visual results.\nW6: There are doubts about the authenticity of the results. In Figure 6 of the appendix, why is the optimization trajectory plot of the Nash-MTL method labeled as \"Ours\"? Moreover, the optimization trajectory plots of the RGB method and the Nash-MTL method appear to be very similar. Are the presented results genuine?\nW7: There are numerous formatting and layout issues. For example, the font size in Table 5 is too large, while the font sizes in Figures 4-5 are too small."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NONE"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "csac84mecH", "forum": "l0QAezg8HX", "replyto": "l0QAezg8HX", "signatures": ["ICLR.cc/2026/Conference/Submission17427/Reviewer_vAwe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17427/Reviewer_vAwe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747423625, "cdate": 1761747423625, "tmdate": 1762927321003, "mdate": 1762927321003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates multi-task learning (MTL) from the perspective of gradient manipulation. It argues that previous approaches often tend to over- or under-correct task gradients, thereby compromising task-specific gradient information. To address this issue, the paper proposes a rotation-based strategy that introduces an objective function designed to simultaneously minimize gradient conflicts and regularize gradient deviations. Extensive experiments conducted on four public datasets demonstrate the competitive performance of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of de-conflicting task gradients through a rotation-based perspective appears to be novel in the context of MTL.\n\n2. The proposed method achieves competitive performance across multiple mainstream MTL benchmarks.\n\n3. The paper provides some theoretical insights that help support and motivate the proposed approach."}, "weaknesses": {"value": "1. Figure 1 provides a conceptual illustration of a phenomenon. To move beyond a purely illustrative claim, the manuscript would be significantly strengthened by either empirical evidence or a formal theoretical analysis to verify and substantiate this depiction.\n2. In Figure 1, how to derive such an optimal gradient direction? \n3. Can you provide some insights on why RGB is extremely effective on QM9? \n4. Why only report across a single seed on CelebA? And why is $\\Delta m$ \\% here different from previously reported in other literature [1]?\n5. The notations $f_z(x)$ and $F(x)$ are employed without prior definition.\n\nReference:\n\n[1] Fair resource allocation in multi-task learning. ICML 2024."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kQqxXGvD17", "forum": "l0QAezg8HX", "replyto": "l0QAezg8HX", "signatures": ["ICLR.cc/2026/Conference/Submission17427/Reviewer_xkvf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17427/Reviewer_xkvf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833844717, "cdate": 1761833844717, "tmdate": 1762927320282, "mdate": 1762927320282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "RGB is a rotation-based gradient balancing method for multi-task learning: each task’s unit gradient is rotated toward an EMA reference via a global alignment-plus-proximity objective, then averaged. The paper proves convergence to Pareto-stationary points and reports consistent gains on NYUv2, Cityscapes, CelebA and QM9."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Principled, globally coordinated reconciliation. Uses per-task scalar rotations toward a shared reference to reduce conflict globally, avoiding PCGrad-style pairwise projections while preserving task-specific signal via a proximity term.\n\n2.Theoretical grounding to Pareto stationarity. Provides existence of the inner minimizer and convergence to Pareto-stationary solutions under standard smoothness/stepsize assumptions, tying the geometric construction to multi-objective optimality rather than a heuristic."}, "weaknesses": {"value": "1.Mathematical clarity in the rotation operator.\nThe text states that $r_i(\\tilde\\alpha_i)=d_t$ when $\\tilde\\alpha_i=\\pi/2$, but by construction $r_i(\\pi/2)=w_i$ (the component orthogonal to $\\bar g_i$). Hitting $d_t$ requires $\\tilde\\alpha_i=\\angle(\\bar g_i,d_t)$.\n\n2.Unquantified compute/memory overhead and scalability.\nThe method introduces an EMA reference direction and an inner loop for angle optimization, but the paper does not report peak memory, extra FLOPs, or wall-clock overhead vs. baselines, nor scaling with the number of tasks $T$.\n\n3.Missing ablation on the proximity weight $\\lambda$.\n$\\lambda$ governs the trade-off between conflict reduction and fidelity to original gradients, effectively bounding the rotation. There is no systematic sweep . A grid over $\\lambda$ with curves for scores is needed to validate robustness and to show the method does not hinge on a narrow setting.\n\n4.Missing ablation on the EMA coefficient $\\mu$ and reference-direction design.\nBecause $d_t$ defines the feasible rotation plane, its construction is critical to stability. The paper lacks sensitivity analyses over $\\mu$ and comparisons to alternatives.\n\n5.Incomplete positioning relative to prior rotation methods (RotoGrad)[1].\nThe paper does not cite or empirically compare to RotoGrad, a closely related line that also uses rotation to harmonize multi-task gradients. A proper literature positioning and a head-to-head comparison under the same backbone/budget are needed.\n\n[1] Javaloy, A. & Valera, I. RotoGrad: Gradient Homogenization in Multitask Learning. ICLR 2022."}, "questions": {"value": "See the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YQ4QoCLCz3", "forum": "l0QAezg8HX", "replyto": "l0QAezg8HX", "signatures": ["ICLR.cc/2026/Conference/Submission17427/Reviewer_nN2S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17427/Reviewer_nN2S"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875024735, "cdate": 1761875024735, "tmdate": 1762927319599, "mdate": 1762927319599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}