{"id": "EbjbESm8MD", "number": 3319, "cdate": 1757400845123, "mdate": 1763641450525, "content": {"title": "Revisit Visual Prompt Tuning: The Expressiveness of Prompt Experts", "abstract": "Visual Prompt Tuning (VPT) has proven effective for parameter-efficient adaptation of pre-trained vision models to downstream tasks by inserting task-specific learnable prompt tokens. Despite its empirical success, a comprehensive theoretical understanding of VPT remains an active area of research. Building on the recently established connection between Mixture of Experts (MoE) and prompt-based methods, wherein each attention head can be conceptualized as a composition of multiple MoE models, we reinterpret VPT as the introduction of new *prompt experts* into these MoE structures. We identify a key limitation in existing VPT frameworks: the *restricted functional expressiveness* of prompt experts, which remain static and thus limited in their adaptability. To address this, we propose **Visual Adaptive Prompt Tuning (VAPT)**, a novel method that endows prompt experts with enhanced expressiveness while preserving parameter efficiency. Empirical evaluations on VTAB-1K and FGVC demonstrate that VAPT achieves *substantial performance improvements*, surpassing fully fine-tuned baselines by **7.34%** and **1.04%**, respectively. Moreover, VAPT consistently outperforms VPT while *requiring fewer additional parameters*. Furthermore, our theoretical analysis indicates that VAPT achieves optimal sample efficiency. Collectively, these results underscore the theoretical grounding and empirical advantages of our approach.", "tldr": "", "keywords": ["mixture of experts", "visual prompt tuning", "theory", "parameter-efficient fine-tuning", "pre-trained model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0f386e8d7995899e4e2cbc299c6a31b2da331034.pdf", "supplementary_material": "/attachment/00b14b3acceaf3ec61cbfad488de7f0ccd05e904.zip"}, "replies": [{"content": {"summary": {"value": "The paper reinterprets VPT as the introduction of new prompt experts into these MoE structures, solving the current limitation in existing VPT frameworks: the restricted functional expressiveness of prompt experts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The experiment settings are sound with sufficient numbers of ablation studies in the Appendix. \n2. The paper is easy to follow, and the motivation for introducing MoE to prompt tuning is reasonable."}, "weaknesses": {"value": "1. The baselines provided in this paper are not new. More recent prompt tuning and other PEFT methods [1-4] should be included for completeness. \n\n2. A critical problem of this paper is its novelty; [5] has proposed MoE prompt tuning as a manifold mapper, indicating that MoE design on prompt tuning can bring stronger expressivity. This work is highly related to the proposed research, although it has not been discussed. \n\n3. Inconsistent experimental report. Table 1 includes E2VPT, while Table 2 does not. Similar for GateVPT.\n\n4. The claim on the low-data regime in the introduction does not have further showcases. Also, the noticeable performance gap might be an outline for VPT. The reason is, as shown in [6], VPT generally brings good few-shot performance, when the training repeats for several times (to avoid bad samples for the training). \n\n[1] Visual Fourier Prompt Tuning\n\n[2] Visual instance-aware prompt tuning\n\n[3] Apla: A simple adaptation method for vision transformers\n\n[4] DS2VP: Dynamically-Selected Spatially Visual Prompting\n\n[5] MEPT: Mixture of Expert Prompt Tuning as a Manifold Mapper\n\n[6] Facing the Elephant in the Room: Visual Prompt Tuning or Full Finetuning?"}, "questions": {"value": "The major concern is the novelty and some claims in this paper. The most relevant paper on MoE prompt tuning is not discussed in this paper. Also, the paper sounds like the direct integration of MoE. Even without a similar approach, the novelty itself is questionable. The authors do not clearly separate their method from traditional MoE attempts.\n\nAnother problem is that some claims might be misleading, though the authors formulate the MoE and the proposed method's training objective; the core idea is intuitive and simple. I think some equations are unnecessary and further complex the understanding of the basic concept of this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HreZGdesyP", "forum": "EbjbESm8MD", "replyto": "EbjbESm8MD", "signatures": ["ICLR.cc/2026/Conference/Submission3319/Reviewer_sjhP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3319/Reviewer_sjhP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760675997953, "cdate": 1760675997953, "tmdate": 1762916667736, "mdate": 1762916667736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to Reviewers' Comments"}, "comment": {"value": "Dear Area Chairs and Reviewers,\n\nWe would like to express our sincere gratitude for the time, effort, and constructive feedback you have invested in evaluating our submission. Your insightful comments have significantly helped us improve the clarity and completeness of the paper. We are especially encouraged by the positive assessments, which highlight the following aspects of our work:\n\n- **Contributions**:\n    - Strong motivation (Reviewer DQQA).\n    - A clear theoretical reinterpretation of VPT through the lens of MoE, providing both conceptual insight and mathematical grounding for understanding prompt tuning behavior (Reviewer YSjb).\n    - The proposed Visual Adaptive Prompt Tuning (VAPT) effectively enhances the expressiveness of prompt experts by introducing input-dependent adaptive prompts while **maintaining parameter efficiency** (Reviewer YSjb).\n- **Soundness**: The paper presents a variety of experiments, with results on FGVC, VTAB-1K, and both supervised and self-supervised pretrained backbones, demonstrating the robustness of the proposed method. In addition, the ablation studies in the appendix are very helpful for understanding the proposed method (Reviewer DQQA, Reviewer sjhP).\n- **Presentation**: The paper is easy to follow (Reviewer DQQA, Reviewer YSjb, Reviewer sjhP).\n\nIn what follows, we would like to clarify the novelty of our contribution and explain why we believe the paper is substantively original along the following dimensions:\n\n- **Foundational Motivation**: As noted by Reviewer DQQA, our work is driven by a strong motivation: we connect VPT to MoE and identify a key limitation in the expressiveness of prompt experts. While pre-trained experts $f_j(X) = {W_m^V}^\\top E_jX$ adapt to the input, standard prompt experts in VPT $f_{N + j'}(X) = {W^V_m}^\\top p_{j'}$ function as static, input-independent experts. This discrepancy naturally raises the question of whether more expressive, input-dependent prompt experts could offer substantial benefits. **Crucially, our approach leverages the existing MoE structure within attention heads rather than imposing an external MoE architecture, as is common in prior work.** This perspective allows us to design VAPT as **a simple and practical implementation while still using MoE theory to rigorously analyze its behavior.**\n\n- **Expressiveness and Efficiency, Without Trade-off**: **A common assumption is that increasing functional expressiveness necessarily comes at the cost of efficiency**. Our design challenges this assumption by demonstrating that both objectives can be achieved simultaneously. VAPT enhances the expressiveness of prompt experts through adaptive, input-dependent prompts while maintaining parameter efficiency (as highlighted by Reviewer YSjb). Moreover, our results show that the benefits extend beyond parameter count: VAPT also exhibits strong data efficiency, **supported by both experimental evidence and theoretical justification**. This dual improvement underscores that expressiveness and efficiency need not be mutually exclusive.\n\n- **Novel Theoretical Insights and Analytical Tractability**: A central contribution of our work is to show that VAPT achieves an optimal sample efficiency rate. **Prior works often rely on more complex architectures and heuristic mechanisms to vary prompts across inputs.**  While these designs can be empirically effective, **their functional complexity often precludes meaningful theoretical analysis**. In contrast, **we deliberately designed VAPT to retain a simple yet expressive functional form**, which leads to the clean expert formulations in Equations (13) and (14). These formulations **enable a thorough theoretical analysis without sacrificing empirical performance, an aspect largely missing in prior literature. The resulting balance between empirical effectiveness and theoretical guarantees addresses a significant gap in the field**: VAPT not only performs well in practice but also offers provable robustness and generalization behavior, thereby adding substantive scholarly value.\n\nWe deeply appreciate the reviewers' positive feedback and thoughtful suggestions. We will address each reviewer's specific concerns in our detailed responses below and describe the corresponding revisions made based on their valuable recommendations.\n\nBest regards,\n\nAuthors"}}, "id": "Apnen8X5Gv", "forum": "EbjbESm8MD", "replyto": "EbjbESm8MD", "signatures": ["ICLR.cc/2026/Conference/Submission3319/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3319/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3319/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763634462531, "cdate": 1763634462531, "tmdate": 1763634462531, "mdate": 1763634462531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits Visual Prompt Tuning through the lens of Mixture of Experts and identifies a fundamental limitation: conventional prompt tokens are static and input-invariant, thus lacking expressive power. To address this, the authors propose Visual Adaptive Prompt Tuning, which generates input-dependent adaptive prompt experts using lightweight token-wise projectors, channel-wise convolutions, and a shared feature projector. This design enhances the functional expressiveness of prompts while maintaining parameter efficiency. Theoretically, VAPT achieves optimal sample efficiency under the MoE framework, and empirically, it outperforms both fully fine-tuned and VPT baselines across VTAB-1K and FGVC benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper offers a clear theoretical reinterpretation of VPT through the lens of MoE, providing both conceptual insight and mathematical grounding for understanding prompt tuning behavior.\n\n2. The proposed Visual Adaptive Prompt Tuning (VAPT) effectively enhances the expressiveness of prompt experts by introducing input-dependent adaptive prompts while maintaining parameter efficiency.\n\n3. Overall writing is clear and easy to follow,"}, "weaknesses": {"value": "1. Conceptually, VAPT’s “input-adaptive prompt experts” is similar to prompt-pool-based approaches [R1. R2]. These methods also condition prompt selection or generation on input features. Especially, [R2] generates tokens based on visual prompts based on the input. If authors could provide comparison between proposed method and existing  prompt-pool-based approaches, it strengthens the novelty of  works.\n\n[R1] Wang, Zifeng, et al. \"Learning to prompt for continual learning.\" CVPR 2022.\n\n[R2] Kim, Youngeun, et al. \"Open-world dynamic prompt and continual visual representation learning.\" ECCV 2024.\n\n2. The proposed approach introduces multiple small components (channel-wise conv, token-wise projector, shared MLP). While lightweight in current ViT-B/16 settings, their scalability to larger backbones (ViT-L/14, ViT-H/14) or higher-resolution inputs is not discussed. The added modules could potentially become computational bottlenecks or require additional tuning.\n\n3. Although the paper argues that VAPT enhances functional expressiveness, there are no visual analyses (e.g., attention maps, learned prompt diversity, or feature attribution) to substantiate this claim. Qualitative results could help illustrate how adaptive prompts differ in behavior from static ones.\n\n4. (optional) All experiments are performed on classification and segmentation benchmarks. It would strengthen the contribution to show that VAPT generalizes to non-classification visual tasks, such as detection or vision-language retrieval, especially since prompt-tuning is often used in multimodal settings."}, "questions": {"value": "Please address questions in Weakness section. Thank you."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lw7qMjBOmU", "forum": "EbjbESm8MD", "replyto": "EbjbESm8MD", "signatures": ["ICLR.cc/2026/Conference/Submission3319/Reviewer_YSjb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3319/Reviewer_YSjb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887856749, "cdate": 1761887856749, "tmdate": 1762916666935, "mdate": 1762916666935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reinterprets Visual Prompt Tuning (VPT) through the lens of Mixture of Experts (MoE) and identifies a key limitation: conventional VPT uses static, input-invariant prompt tokens, which restricts expressiveness. Building on this observation, the authors propose Visual Adaptive Prompt Tuning (VAPT)—a parameter-efficient extension that generates input-dependent prompts. The authors empirically demonstrate consistent gains over VPT and other PEFT methods across VTAB-1K and FGVC benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The main motivation of this paper can provide a mathematically grounded analysis\n\n- The paper is easy to follow.\n\n- The authors provide a variety of experiments, with results on FGVC, VTAB-1K, and supervised and self-supervised pretrained backbones, showing the robustness of the proposed method. In addition, ablation studies in the Appendix are very helpful to understand the proposed method."}, "weaknesses": {"value": "- My major concern is the novelty.\nFor adaptive visual prompt tuning, there are many visual prompt tuning works (e.g., CVPT, CoCoOp, ViaPT, V2APT) already exploring visually adaptive or instance-aware prompts. Hence, the contribution in adaptivity itself is incremental rather than fundamentally new. In addition, MoE Interpretation is also heavily motivated by Le et al, who already framed attention and prompting under MoE theory. \n\n\n[CVPT] CVPT: Cross-Attention help Visual Prompt Tuning adapt visual task, NeurIPS 2024\n\n[CoCoOp] Conditional Prompt Learning for Vision-Language Models, CVPR 2022\n\n[ViaPT] Visual Instance-aware Prompt Tuning, MM 2025\n\n[V2APT] Visual Variational Autoencoder Prompt Tuning, arXiv 2025\n\n\n- The paper lacks sufficient comparison with other recent variants of Visual Prompt Tuning. There has been a surge of follow-up works for VPT, yet these are not adequately discussed or compared. Especially, input-dependent or adaptive prompting methods should be carefully compared and discussed.\n\n- There are some fairness issues, where the Tuned/Total ratio differs substantially across methods. For instance, Gated VPT (in Table 2) uses only about 0.05 % of trainable parameters, while VAPT tunes 0.27 % – 0.28 % of the model. Such discrepancy can partly explain the performance gap, making the comparison less fair. A more rigorous evaluation should control for the number of trainable parameters."}, "questions": {"value": "- Can the proposed visual adaptive prompt method be applied on top of recent VPT variants?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9h1Ae5sPQE", "forum": "EbjbESm8MD", "replyto": "EbjbESm8MD", "signatures": ["ICLR.cc/2026/Conference/Submission3319/Reviewer_DQQA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3319/Reviewer_DQQA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001675920, "cdate": 1762001675920, "tmdate": 1762916666105, "mdate": 1762916666105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}