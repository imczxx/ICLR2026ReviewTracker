{"id": "osxP6FafPZ", "number": 3525, "cdate": 1757462230252, "mdate": 1759898083197, "content": {"title": "SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms", "abstract": "Rigorous testing of autonomous robots, such as self-driving vehicles, is essential to ensure their safety in real-world deployments. This requires building high-fidelity simulators to test scenarios beyond those that can be safely or exhaustively collected in the real-world. Existing neural rendering methods based on NeRF and 3DGS hold promise but suffer from low rendering speeds or can only render pinhole camera models, hindering their suitability to applications that commonly require high-distortion lenses and LiDAR data. Multi-sensor simulation poses additional challenges as existing methods handle cross-sensor inconsistencies by favoring the quality of one modality at the expense of others. To overcome these limitations, we propose SimULi, the first method capable of rendering arbitrary camera models and LiDAR data in real-time. Our method extends 3DGUT, which natively supports complex camera models, with LiDAR support, via an automated tiling strategy for arbitrary spinning LiDAR models and ray-based culling. To address cross-sensor inconsistencies, we design a factorized 3D Gaussian representation and anchoring strategy that reduces mean camera and depth error by up to 40% compared to existing methods. SimULi renders 10-20$\\times$ faster than ray tracing approaches and 1.5-14$\\times$ faster than prior rasterization-based work (and handles a wider range of camera models). When evaluated on two widely benchmarked autonomous driving datasets, SimULi matches or exceeds the fidelity of existing state-of-the-art methods across numerous camera and LiDAR metrics.", "tldr": "We design a factorized 3d gaussian representation that improves multi-sensor reconstruction and strategies to accelerate LiDAR rendering by 10x.", "keywords": ["neural rendering", "3d gaussians", "3d vision"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6d3f09e2997d1ba622892b978887b4e288d92035.pdf", "supplementary_material": "/attachment/93d03a3b9b47d552ab04994453b6985143ffb78e.zip"}, "replies": [{"content": {"summary": {"value": "SimULi extends 3DGUT by incorporating LiDAR support, enabling simultaneous modeling of multimodal sensors in autonomous driving scenarios. The system achieves performance on both camera and LiDAR modalities that matches or surpasses the current state of the art."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Achieves joint modeling of multimodal sensors (camera and LiDAR) in autonomous driving scenes, with each modality outperforming existing single-modality methods.\n\n - Incorporates modeling of fisheye cameras and rolling-shutter effects, demonstrating a highly comprehensive design.\n\n - Significantly improves rendering speed."}, "weaknesses": {"value": "- The interaction between camera and LiDAR Gaussian primitives relies solely on the anchor loss, which may lead to redundant Gaussian points.\n\n - The system demonstrates strong engineering merit, but the novelty is relatively limited."}, "questions": {"value": "- How is LiDAR integrated into 3DGUT? Are the seven points projected and weighted following the LiDAR’s ray model? If so, could the authors provide a visualization similar to Fig. 7 in 2DGS to illustrate the validity of this approximation? As nonlinearity of LiDAR projection is typically stronger than that of camera projection.\n\n - Is the interaction between camera and LiDAR Gaussian primitives only achieved through the anchor loss? Would such a weak coupling lead to redundant Gaussian points?\n\n - Could the proposed approach be combined with recent generative-aided reconstruction methods (e.g., StreetCrafter, DriveX)?\n\n - Which component contributes most to the significant improvement in rendering speed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cLT1vso6Dw", "forum": "osxP6FafPZ", "replyto": "osxP6FafPZ", "signatures": ["ICLR.cc/2026/Conference/Submission3525/Reviewer_HQsC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3525/Reviewer_HQsC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719171419, "cdate": 1761719171419, "tmdate": 1762916787444, "mdate": 1762916787444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SimULi describes a framework for real-time simulation of LiDAR and camera for autonomous driving scenes. The focus of this work is to improve the rendering speed and support arbitrary LiDAR and camera configurations. To do so, this work builds on 3DGUT to support arbitrary spinning LiDAR configuration. Furthermore, a factorized 3D Gaussian representation and anchoring strategy was proposed to address discrepancies between simulated LiDAR and camera data. The proposed method was benchmarked on two datasets to showcase the fidelity and efficiency in relation to existing works."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper is well-written and easy to follow. The limitations of prior works and their relation to the proposed work are clearly highlighted.\n2.\tThe proposed method demonstrates strong rendering fidelity and significant boosts in speed compared to state-of-the-art methods."}, "weaknesses": {"value": "1.\tThe proposed method has been evaluated on a relatively limited set of datasets. Common benchmarks used in prior works, such as nuScenes and Argoverse 2, would help demonstrate the robustness of the method across datasets and sensor setups.\n2.\tSplatAD encodes all sensor information into the same Gaussian set. SimULi proposes to encode each sensor into its own particle set to address the inconsistencies between LiDAR and camera. The impact of this on the training time and potentially memory requirements is not discussed."}, "questions": {"value": "1.\tHow does factorizing the Gaussian set impact training speed? How does the training time of the proposed work compare against existing works?\n2.\tFor the anchoring loss, the choice of 50 nearest neighbors and updating assignments every 1000 iterations are hyperparameters. How does varying the number of nearest neighbors and update frequency impact convergence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FuPzwWL3k9", "forum": "osxP6FafPZ", "replyto": "osxP6FafPZ", "signatures": ["ICLR.cc/2026/Conference/Submission3525/Reviewer_DQih"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3525/Reviewer_DQih"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002136856, "cdate": 1762002136856, "tmdate": 1762916786215, "mdate": 1762916786215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "this paper introduces a simulator method for autonomous driving that can render both complex camera models and LiDAR data in real time. It builds on 3DGUT and improves cross-sensor consistency, making it more accurate and faster than existing methods. The paper conducted experiments on two public datasets and show state-of-the-art performances."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. The proposed method consistently outperforms baselines according to both visualizations and tables. \n3. The proposed method achieves significantly faster rendering speed than baselines.\n4. The  factorized representation is interesting and innovative, which improves both the camera and lidar rendering accuracy."}, "weaknesses": {"value": "would the factorized representation largely increase the total number of Gaussians in the scene? Does this lead to significantly higher memory usage compared to unified representations?"}, "questions": {"value": "1. From what I understand, the authors equalize the elevation tiling using a 1D CDF of elevation angles, and then reuse the same azimuth tiling across the whole scan. Wouldn’t that implicitly assume that the LiDAR point distribution is separable between elevation and azimuth, and also static over time? In practice, the point density may vary a lot with azimuth (depending on the scene or motion), so a fixed azimuth tiling might lead to load imbalance — some tiles being overloaded and others almost empty. Did you observe this issue in your experiments, or did you use any mechanism to adapt the azimuth tiling dynamically?\n2. Since each camera Gaussian is softly constrained to stay close to its nearest LiDAR neighbor, how sensitive is the method to that K-nearest-neighbor choice (K = 50)? Also, do you ever observe issues where the anchoring loss pulls camera Gaussians toward noisy or missing LiDAR points, especially around thin structures or reflective surfaces?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N73ENvFJyJ", "forum": "osxP6FafPZ", "replyto": "osxP6FafPZ", "signatures": ["ICLR.cc/2026/Conference/Submission3525/Reviewer_u66L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3525/Reviewer_u66L"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013078979, "cdate": 1762013078979, "tmdate": 1762916785797, "mdate": 1762916785797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a 3DGS-based simulator for real-time camera and LiDAR rendering in autonomous driving. The main contributions include a non-equidistant tiling strategy that efficiently handles arbitrary spinning LiDAR sensors, and a factorized 3D Gaussian representation that mitigates cross-sensor inconsistencies between camera and LiDAR modalities, thereby improving rendering realism. The experiments are comprehensive, with extensive comparisons on the Waymo and PandaSet datasets, demonstrating state-of-the-art performance and realism."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well written and clearly motivated.\n* Experiments are thorough and cover multiple datasets and baselines. The proposed method achieves strong quantitative and qualitative performance, demonstrating competitive or superior rendering realism and efficiency."}, "weaknesses": {"value": "* The novelty is limited. The method feels like a natural extension of 3DGUT, and the way LiDAR rendering is supported is conceptually similar to SplatAD.\n* The improvement in handling the camera–LiDAR accuracy tradeoff mainly comes from the decoupled representation, but the deeper issue—imperfect sensor modeling (e.g., motion blur, rolling shutter, or calibration)—is not really addressed. Prior work such as NeuRAD has shown that explicitly modeling these effects can significantly improve reconstruction quality.\n* This paper also omits related efforts such as AlignMiF, which also tackle multimodal alignment in autonomous driving simulation. A discussion or comparison with AlignMiF would make the contribution clearer and better positioned."}, "questions": {"value": "* The paper claims improved cross-sensor consistency, but how does the method perform when modeling more accurate physical effects such as rolling shutter, motion blur, or calibration errors? This could also be evaluated under controlled conditions, for example using CARLA.\n* Can the proposed LiDAR tiling strategy generalize to non-spinning LiDAR sensors, such as solid-state LiDARs, where the sampling pattern is different?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dsBhAlOUVr", "forum": "osxP6FafPZ", "replyto": "osxP6FafPZ", "signatures": ["ICLR.cc/2026/Conference/Submission3525/Reviewer_6a2S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3525/Reviewer_6a2S"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762318562469, "cdate": 1762318562469, "tmdate": 1762916785611, "mdate": 1762916785611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}