{"id": "osxP6FafPZ", "number": 3525, "cdate": 1757462230252, "mdate": 1763716969540, "content": {"title": "SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms", "abstract": "Rigorous testing of autonomous robots, such as self-driving vehicles, is essential to ensure their safety in real-world deployments. This requires building high-fidelity simulators to test scenarios beyond those that can be safely or exhaustively collected in the real-world. Existing neural rendering methods based on NeRF and 3DGS hold promise but suffer from low rendering speeds or can only render pinhole camera models, hindering their suitability to applications that commonly require high-distortion lenses and LiDAR data. Multi-sensor simulation poses additional challenges as existing methods handle cross-sensor inconsistencies by favoring the quality of one modality at the expense of others. To overcome these limitations, we propose SimULi, the first method capable of rendering arbitrary camera models and LiDAR data in real-time. Our method extends 3DGUT, which natively supports complex camera models, with LiDAR support, via an automated tiling strategy for arbitrary spinning LiDAR models and ray-based culling. To address cross-sensor inconsistencies, we design a factorized 3D Gaussian representation and anchoring strategy that reduces mean camera and depth error by up to 40% compared to existing methods. SimULi renders 10-20$\\times$ faster than ray tracing approaches and 1.5-14$\\times$ faster than prior rasterization-based work (and handles a wider range of camera models). When evaluated on two widely benchmarked autonomous driving datasets, SimULi matches or exceeds the fidelity of existing state-of-the-art methods across numerous camera and LiDAR metrics.", "tldr": "We design a factorized 3d gaussian representation that improves multi-sensor reconstruction and strategies to accelerate LiDAR rendering by 10x.", "keywords": ["neural rendering", "3d gaussians", "3d vision"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4ca625d1a680fabed7d92d68687d9bffcf84f872.pdf", "supplementary_material": "/attachment/93d03a3b9b47d552ab04994453b6985143ffb78e.zip"}, "replies": [{"content": {"summary": {"value": "SimULi extends 3DGUT by incorporating LiDAR support, enabling simultaneous modeling of multimodal sensors in autonomous driving scenarios. The system achieves performance on both camera and LiDAR modalities that matches or surpasses the current state of the art."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Achieves joint modeling of multimodal sensors (camera and LiDAR) in autonomous driving scenes, with each modality outperforming existing single-modality methods.\n\n - Incorporates modeling of fisheye cameras and rolling-shutter effects, demonstrating a highly comprehensive design.\n\n - Significantly improves rendering speed."}, "weaknesses": {"value": "- The interaction between camera and LiDAR Gaussian primitives relies solely on the anchor loss, which may lead to redundant Gaussian points.\n\n - The system demonstrates strong engineering merit, but the novelty is relatively limited."}, "questions": {"value": "- How is LiDAR integrated into 3DGUT? Are the seven points projected and weighted following the LiDAR’s ray model? If so, could the authors provide a visualization similar to Fig. 7 in 2DGS to illustrate the validity of this approximation? As nonlinearity of LiDAR projection is typically stronger than that of camera projection.\n\n - Is the interaction between camera and LiDAR Gaussian primitives only achieved through the anchor loss? Would such a weak coupling lead to redundant Gaussian points?\n\n - Could the proposed approach be combined with recent generative-aided reconstruction methods (e.g., StreetCrafter, DriveX)?\n\n - Which component contributes most to the significant improvement in rendering speed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cLT1vso6Dw", "forum": "osxP6FafPZ", "replyto": "osxP6FafPZ", "signatures": ["ICLR.cc/2026/Conference/Submission3525/Reviewer_HQsC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3525/Reviewer_HQsC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719171419, "cdate": 1761719171419, "tmdate": 1762916787444, "mdate": 1762916787444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SimULi describes a framework for real-time simulation of LiDAR and camera for autonomous driving scenes. The focus of this work is to improve the rendering speed and support arbitrary LiDAR and camera configurations. To do so, this work builds on 3DGUT to support arbitrary spinning LiDAR configuration. Furthermore, a factorized 3D Gaussian representation and anchoring strategy was proposed to address discrepancies between simulated LiDAR and camera data. The proposed method was benchmarked on two datasets to showcase the fidelity and efficiency in relation to existing works."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper is well-written and easy to follow. The limitations of prior works and their relation to the proposed work are clearly highlighted.\n2.\tThe proposed method demonstrates strong rendering fidelity and significant boosts in speed compared to state-of-the-art methods."}, "weaknesses": {"value": "1.\tThe proposed method has been evaluated on a relatively limited set of datasets. Common benchmarks used in prior works, such as nuScenes and Argoverse 2, would help demonstrate the robustness of the method across datasets and sensor setups.\n2.\tSplatAD encodes all sensor information into the same Gaussian set. SimULi proposes to encode each sensor into its own particle set to address the inconsistencies between LiDAR and camera. The impact of this on the training time and potentially memory requirements is not discussed."}, "questions": {"value": "1.\tHow does factorizing the Gaussian set impact training speed? How does the training time of the proposed work compare against existing works?\n2.\tFor the anchoring loss, the choice of 50 nearest neighbors and updating assignments every 1000 iterations are hyperparameters. How does varying the number of nearest neighbors and update frequency impact convergence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FuPzwWL3k9", "forum": "osxP6FafPZ", "replyto": "osxP6FafPZ", "signatures": ["ICLR.cc/2026/Conference/Submission3525/Reviewer_DQih"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3525/Reviewer_DQih"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002136856, "cdate": 1762002136856, "tmdate": 1762916786215, "mdate": 1762916786215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "this paper introduces a simulator method for autonomous driving that can render both complex camera models and LiDAR data in real time. It builds on 3DGUT and improves cross-sensor consistency, making it more accurate and faster than existing methods. The paper conducted experiments on two public datasets and show state-of-the-art performances."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. The proposed method consistently outperforms baselines according to both visualizations and tables. \n3. The proposed method achieves significantly faster rendering speed than baselines.\n4. The  factorized representation is interesting and innovative, which improves both the camera and lidar rendering accuracy."}, "weaknesses": {"value": "would the factorized representation largely increase the total number of Gaussians in the scene? Does this lead to significantly higher memory usage compared to unified representations?"}, "questions": {"value": "1. From what I understand, the authors equalize the elevation tiling using a 1D CDF of elevation angles, and then reuse the same azimuth tiling across the whole scan. Wouldn’t that implicitly assume that the LiDAR point distribution is separable between elevation and azimuth, and also static over time? In practice, the point density may vary a lot with azimuth (depending on the scene or motion), so a fixed azimuth tiling might lead to load imbalance — some tiles being overloaded and others almost empty. Did you observe this issue in your experiments, or did you use any mechanism to adapt the azimuth tiling dynamically?\n2. Since each camera Gaussian is softly constrained to stay close to its nearest LiDAR neighbor, how sensitive is the method to that K-nearest-neighbor choice (K = 50)? Also, do you ever observe issues where the anchoring loss pulls camera Gaussians toward noisy or missing LiDAR points, especially around thin structures or reflective surfaces?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N73ENvFJyJ", "forum": "osxP6FafPZ", "replyto": "osxP6FafPZ", "signatures": ["ICLR.cc/2026/Conference/Submission3525/Reviewer_u66L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3525/Reviewer_u66L"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013078979, "cdate": 1762013078979, "tmdate": 1762916785797, "mdate": 1762916785797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a 3DGS-based simulator for real-time camera and LiDAR rendering in autonomous driving. The main contributions include a non-equidistant tiling strategy that efficiently handles arbitrary spinning LiDAR sensors, and a factorized 3D Gaussian representation that mitigates cross-sensor inconsistencies between camera and LiDAR modalities, thereby improving rendering realism. The experiments are comprehensive, with extensive comparisons on the Waymo and PandaSet datasets, demonstrating state-of-the-art performance and realism."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well written and clearly motivated.\n* Experiments are thorough and cover multiple datasets and baselines. The proposed method achieves strong quantitative and qualitative performance, demonstrating competitive or superior rendering realism and efficiency."}, "weaknesses": {"value": "* The novelty is limited. The method feels like a natural extension of 3DGUT, and the way LiDAR rendering is supported is conceptually similar to SplatAD.\n* The improvement in handling the camera–LiDAR accuracy tradeoff mainly comes from the decoupled representation, but the deeper issue—imperfect sensor modeling (e.g., motion blur, rolling shutter, or calibration)—is not really addressed. Prior work such as NeuRAD has shown that explicitly modeling these effects can significantly improve reconstruction quality.\n* This paper also omits related efforts such as AlignMiF, which also tackle multimodal alignment in autonomous driving simulation. A discussion or comparison with AlignMiF would make the contribution clearer and better positioned."}, "questions": {"value": "* The paper claims improved cross-sensor consistency, but how does the method perform when modeling more accurate physical effects such as rolling shutter, motion blur, or calibration errors? This could also be evaluated under controlled conditions, for example using CARLA.\n* Can the proposed LiDAR tiling strategy generalize to non-spinning LiDAR sensors, such as solid-state LiDARs, where the sampling pattern is different?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dsBhAlOUVr", "forum": "osxP6FafPZ", "replyto": "osxP6FafPZ", "signatures": ["ICLR.cc/2026/Conference/Submission3525/Reviewer_6a2S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3525/Reviewer_6a2S"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762318562469, "cdate": 1762318562469, "tmdate": 1762916785611, "mdate": 1762916785611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We appreciate our reviewers' thoughtful feedback and are glad they universally concur that our paper is \"well-written and easy to follow\" (6a2S, DQih). They agree that our method demonstrates \"strong quantitative and qualitative performance\" (6a2S) and \"matches or surpasses the current state of the art\" (HQsC), that our \"experiments are thorough and cover multiple datasets and baselines\" (6a2S), and that our \"factorized representation is interesting and innovative\" (u66L).\n\nA common concern shared by reviewers (u66L, DQih, HQsC) is whether our factorized representation increases the total number of Gaussians in the scene (affecting memory and training time). In our experiments we cap the overall number of Gaussians across both sensor types to 4 million, which is slightly less than the baseline closest to ours (SplatAD, which uses a maximum of 5 million). We hypothesize that factorizing actually improves our ability to encode the scene efficiently instead of being a hindrance, as (1) we avoid allocating Gaussians as \"floaters\" to explain cross-sensor inconsistencies and (2) each Gaussian only stores in the information needed to represent its modality (a naive unified representation would store two separate sets of spherical harmonics for camera and LiDAR within each Gaussian). Concretely, our Gaussians serialize to about 900 MB (inline with other 3DGS methods) and could be further optimized via complementary 3DGS compression methods such as vector quantization [1, 2] and visibility filtering [3].\n\nAnother common question was how the choice of K used in the K-nearest neighbor anchoring loss (u66L, DQiH) and the frequency of neighbor assignments (DQiH) affects convergence and training speed. Although we set K=50 as a conservative best guess in our experiments (and update the assignments every N=1000 iterations), varying K between 20 and 100 and N between 500 and 2000 results in negligible differences (<0.05 db in PSNR). As to training time, applying the anchoring loss with K=50 takes 13ms per iteration (6.5 minutes over 30k iterations) and the full K-NN assignment takes 15 seconds every N=1000 iterations (7.5 minutes total). Choosing K=20 reduces the anchoring loss overhead to 6ms per iteration and 6 seconds per assignment (6 minutes total). The cost is relatively minor in both cases, and our training speed is otherwise comparable to other 3DGS-based pipelines. We have updated the latest version of our submission to note these considerations, along with other feedback that we address in reviewer-specific comments.\n\nReferences:\n\n[1] K L Navaneet et al. CompGS: Smaller and Faster Gaussian Splatting with Vector Quantization. In ECCV 2024.\n\n[2] Zhiwen Fan, Kevin Wang, et al. LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS. In NeurIPS 2024.\n\n[3] Michael Niemeyer et al. RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS. In 3DV 2025."}}, "id": "sFl6WvyXB8", "forum": "osxP6FafPZ", "replyto": "osxP6FafPZ", "signatures": ["ICLR.cc/2026/Conference/Submission3525/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3525/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3525/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763715687844, "cdate": 1763715687844, "tmdate": 1763715687844, "mdate": 1763715687844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}