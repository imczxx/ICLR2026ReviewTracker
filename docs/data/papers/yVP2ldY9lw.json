{"id": "yVP2ldY9lw", "number": 15862, "cdate": 1758256266924, "mdate": 1759897277077, "content": {"title": "UltraCUA: Scaling Computer Use Agent through GUI and Programmatic Control", "abstract": "Multi-modal agents for computer use rely exclusively on primitive actions (click, type, scroll) that require accurate visual grounding and lengthy execution chains, leading to cascading failures and performance bottlenecks. While other agents leverage rich programmatic interfaces (APIs, MCP servers, tools), computer-use agents (CUAs) remain isolated from these capabilities. We present UltraCUA, a foundation model that bridges this gap through hybrid control—seamlessly integrating GUI primitives with high-level programmatic tool calls. To achieve this, our approach comprises four key components: (1) an automated pipeline that scales programmatic tools from software documentation, open-source repositories, and code generation; (2) a synthetic data engine producing 17,000+ verifiable tasks spanning real-world computer-use scenarios; (3) a multi-agent system generating high-quality hybrid control trajectories with both low-level GUI actions and high-level programmatic tool calls; and (4) a two-stage training pipeline combining supervised fine-tuning with online reinforcement learning, enabling strategic alternation between low-level and high-level actions. Experiments with our 7B and 32B models demonstrate substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA models achieve an average 27% relative improvement over base models, while being 11% faster in terms of steps. Out-of-domain evaluation on WindowsAgentArena shows our model reaches 21.7% success rate, outperforming baselines trained on Windows data. The hybrid control mechanism proves critical, reducing error propagation while maintaining execution efficiency. This work establishes a scalable paradigm that bridges primitive GUI interactions and programmatic intelligence for stronger and unified computer use.", "tldr": "", "keywords": ["GUI Agents", "Computer-Use Agents", "Multi-Modal Agents"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d4fd7d4e19ca8ccb76d03eb71b8d21a212bef1f5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents UltraCUA, a scalable computer-use agent that unifies low-level GUI actions with high-level programmatic tool invocation via a hybrid control mechanism. To enable this capability, the authors develop an automated pipeline for extracting and expanding programmatic tools from software documentation, open-source code, and agent-generated scripts. Additionally, they introduce a large-scale synthetic data engine that provides more than 17,000 verifiable tasks across diverse real-world computing scenarios. UltraCUA is trained using a two-stage process, supervised fine-tuning followed by online reinforcement learning, which substantially improves tool-use strategies and execution efficiency. Experiments demonstrate strong performance gains on OSWorld benchmarks and significant cross-platform generalization to Windows environments without domain-specific tuning. Overall, this work provides a systematic and scalable paradigm for bridging primitive GUI interactions and programmatic intelligence, advancing the capability of computer-use agents toward robust and efficient automation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The hybrid setup provides clear usability benefits over purely GUI-driven agents, especially in reducing long action chains and improving execution efficiency.\n- The automated pipeline for constructing verifiable hybrid-control tasks (17K+) is well executed and helps address the data bottleneck in computer-use agent training.\n- The paper includes strong empirical evaluation across multiple environments, showing consistent improvements and demonstrating that the system works reliably in practice."}, "weaknesses": {"value": "- The paper argues that GUI actions are low-level while programmatic tools are high-level, yet it does not clearly justify why both must coexist within a unified control framework. Prior programmatic agents (e.g., Voyager, CodeActAgent) already show that LLMs can synthesize execution scripts that effectively serve as high-level action sequences for computer manipulation. This raises the question of whether hybrid control offers conceptual necessity beyond what script-based control can already achieve.\n- The RL objective adds a positive bonus for trajectories that contain tool calls (R_tool = 0.3 when succeed), which intrinsically biases the learned policy toward tools. This makes it hard to attribute the reported gains to an emergent hybrid strategy rather than to reward shaping that prefers tool usage. The paper also removes format rewards, which weakens constraints on syntactic correctness and may cause brittle tool calling.\n- The paper describes an automated pipeline to harvest tools, but provides no quantitative coverage (how many APIs/tools per app), no failure/precision metrics, and no robustness to software updates—so the claimed scalability is not empirically substantiated. (Contribution described in abstract; missing robustness analyses elsewhere.)\n- The paper cites both “17,000+” verifiable tasks and “16,000+” verified tasks in different sections, with limited breakdowns of task types or difficulty alignment with claimed hybrid benefits. This inconsistency complicates reproducibility and makes it hard to assess how much of the gain is due to data scale vs. hybrid design."}, "questions": {"value": "- Given that OSWorld tasks are deterministic with rule-based evaluators and the Windows evaluation is treated as OOD primarily due to an OS change, can you evaluate on stochastic or previously unseen UIs and quantify UI/layout variability to substantiate real-world generalization claims?\n- On WindowsAgentArena the absolute SR is 21.7% and results are reported at 15-step budgets; could you report wall-clock time, crash/timeout rates, and performance under larger step budgets to demonstrate end-to-end practicality, and clarify how your chosen metrics (SR/Pass@4/steps) relate to real latency and throughput?\n- Since trajectory collection uses a powerful planner (o3) and a SOTA grounder (GTA1-7B), can you disentangle how much improvement comes from these components versus the proposed hybrid formulation (e.g., by swapping/removing them)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jlOR3RxxIJ", "forum": "yVP2ldY9lw", "replyto": "yVP2ldY9lw", "signatures": ["ICLR.cc/2026/Conference/Submission15862/Reviewer_yDXL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15862/Reviewer_yDXL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723626428, "cdate": 1761723626428, "tmdate": 1762926082867, "mdate": 1762926082867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents UltraCUA, a novel approach to enhancing computer-use agents by integrating graphical GUI actions with high-level programmatic tool calls. This hybrid control model addresses the limitations of traditional CUAs that rely solely on primitive actions like clicking and typing, which often lead to cascading errors and inefficiencies. UltraCUA introduces four key components: an automated tool collection pipeline, a synthetic data engine for task generation, a hybrid control trajectory collection, and a two-stage training pipeline combining supervised fine-tuning with reinforcement learning. The experiments demonstrate significant improvements over state-of-the-art models, with UltraCUA achieving a 27% relative improvement in success rate on the OSWorld benchmark and a 21.7% success rate in out-of-domain evaluations on WindowsAgentArena."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The integration of GUI actions with programmatic tool calls is a significant advancement, allowing for more efficient and reliable task execution. This approach effectively bridges the gap between GUI-based generality and programmatic efficiency.\n\n2. The paper outlines a robust methodology, including an automated tool collection pipeline and a dual-pipeline synthetic data engine, which ensures the generation of diverse and verifiable tasks. This comprehensive approach supports the development of a scalable and adaptable agent.\n\n3. UltraCUA demonstrates substantial improvements over existing models, both in terms of success rates and execution efficiency. The model's ability to generalize across different platforms, as evidenced by its performance on WindowsAgentArena, highlights its versatility and robustness."}, "weaknesses": {"value": "1. The manuscript does not adequately discuss the limitations of the proposed approach. A thorough examination of potential drawbacks or areas for improvement would provide a more balanced view and help guide future research\n\n2. The paper does not clearly differentiate its contributions from previous works, which may lead readers to perceive it as merely a combination of existing methods rather than a novel advancement. \n\n3. The technique contribution could be RL, but which is a little bit simple of the hyper-parameters in the reward setting, which could give a further ablation study and discussion.\nThere is a concern that the reward system could be hacked by using tools that do not affect the environment but still contribute to the reward.\n\n4. The necessity of hybrid control should be further discussed, e.g., what if using a router model to decide between GUI primitive actions and programmatic tool calls? \n\n5. There are some typos, like line 126 ”tool”."}, "questions": {"value": "see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wTLhJZew7w", "forum": "yVP2ldY9lw", "replyto": "yVP2ldY9lw", "signatures": ["ICLR.cc/2026/Conference/Submission15862/Reviewer_jRhg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15862/Reviewer_jRhg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926667740, "cdate": 1761926667740, "tmdate": 1762926082353, "mdate": 1762926082353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UltraCUA, a large-scale computer-use agent framework that combines low-level GUI actions with high-level programmatic tool calls in a unified hybrid control paradigm. UltraCUA encompasses an automated pipeline for collecting programmatic tools from documentation, open-source, and code generation; a synthetic data engine generating over 17,000 verifiable computer-use tasks; a multi-agent demonstration collection strategy; and a two-stage training pipeline (supervised fine-tuning plus online RL). UltraCUA models (7B and 32B scale) exhibit consistent state-of-the-art results on challenging benchmarks (OSWorld, WindowsAgentArena), demonstrating both improved success rates and execution efficiency versus GUI-only or tool-only baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Hybrid Control Paradigm:** The central innovation—explicitly supporting both GUI primitives and programmatic tool calls—addresses critical failure modes in current computer-use agents, markedly reducing error propagation and improving efficiency.\n\n**Automated Programmatic Tool Collection:** The paper presents a scalable pipeline that extracts and organizes 880+ actionable programmatic tools from diverse sources (documentation, open-source, code generation), extending beyond most previous hand-curated or ad hoc approaches. Appendix Table 7 quantifies this breadth and granularity.\n\n**Robust Synthetic Data Engine:** The construction of a dual-pipeline synthetic engine (evaluator-first and instruction-first) for generating 17,000+ verifiable tasks—tabulated in Appendix Tables 6 and 8—is a significant asset. This ensures both diversity and automatic reward signals for RL, addressing a key bottleneck in scalable training of computer-use agents.\n\n**Comprehensive Empirical Evaluation:** The experiments (Table 1, Table 2) are thorough, comparing UltraCUA against strong commercial, agentic, and multi-agent baselines under strict regimes (fixed step budgets, open/closed-source splits, and out-of-domain tasks). UltraCUA-32B, for example, exceeds all baselines by nontrivial margins on OSWorld.\n\n**Insightful Ablations and Analysis:** Section 3.3 (Table 3, Table 4, Figure 3) and qualitative case studies (Figures 5, 6, 7) provide sharp analysis of the contributions of hybrid control, memory, and RL. Figure 1(c) in particular concisely illustrates the conceptual efficiency gain from UltraCUA's approach.\n\n**Demonstrated Transferability:** The positive results on WindowsAgentArena (Table 2) using only Ubuntu-based training highlight promising cross-platform and tool generalization properties, a core advantage claimed for the hybrid control mechanism.\n\n**Clarity and Organization:** The exposition is generally clear, with good use of figures like Figure 2 (system overview) and concrete qualitative cases that increase accessibility."}, "weaknesses": {"value": "**Marginal Empirical Gains and Questionable Scaling Hypothesis:** The paper's central claim is that its scaling methodology (tools, tasks) yields significant benefits. However, the empirical evidence for this is weak. The paper highlights a \"23.9% relative\" improvement (Table 3), but this framing is misleading as it masks a very modest **1.9% absolute gain** (25.1% to 27.0%) from adding the entire hybrid toolset. Similarly, the complex online RL stage, trained on thousands of synthetic tasks, contributes only another **1.9% absolute gain** (27.0% to 28.9%). These small gains call into question the value of the entire, complex infrastructure for automated scaling. The results do not present a clear scaling trend, and it is highly questionable if scaling this further (e.g., to 50,000 tasks or 2,000 tools) would yield any meaningful improvement, suggesting a potential performance ceiling for this approach.\n\n**Positioning vs. Most Directly Related Work:** There is a lack of direct discussion of the most contemporaneous or directly parallel efforts (e.g., foundational work or concurrent preprints focused on hybrid action spaces for computer-use agents). This undercuts the claim of innovation and makes it harder to situate the precise advancement UltraCUA represents, especially given the fast-moving nature of this research area.\n\n**Empirical Comparisons Are Not Uniformly Comprehensive:** On some benchmarks, not all contemporary multi-agent or hybrid-control systems are directly compared (see Table 1 and Table 2). For instance, hybrid/goal-oriented interface baselines or agents equipped with declarative tool APIs (as highlighted in recent concurrent works) are not included or discussed, nor is there careful analysis of why certain models perform as they do relative to UltraCUA. This limits the interpretability of the comparative improvement claims.\n\n**Hybrid Action Modality Switching—Granular Justification:** While the hybrid control concept is appealing, the methodological description of *action selection* (how/when to call GUI vs. tool) is still somewhat black-boxed in both the SFT and RL stages. For example, in Section 2.3 and the corresponding agent rollouts, there is insufficiently detailed discussion (with math or algorithmic specification) of the state encoding and policy outputs that determine action type. Are there separate output heads? What failure modes (e.g., tool misuse) persist, and how is hybrid selection supervised beyond outcome reward? This lack of detail makes it difficult to assess if the policy is truly learning a generalizable scaling strategy or simply memorizing heuristics from the 17,000 synthetic tasks.\n\n**Math and Reward Formulations:** The RL reward structure (Equation for $R(\\tau)$) is only marginally specified. There is no detailed ablation on the sensitivity to the value of tool-use reward (0.3), nor rigor in justifying why this value avoids degenerate solutions (e.g., spamming tool calls or eschewing GUI when needed). Given the minimal performance uplift from the entire RL stage (1.9% absolute), a more rigorous analysis is required to justify its inclusion and complexity.\n\n**Figure 3 Analysis—Insufficient Granularity:** While Figure 3 shows positive outcome and format reward trends and improvements in \"tool-call pattern,\" it does not offer domain/task-type breakdowns, nor examples where RL leads to suboptimal strategies (tool overuse/underuse). The shaded error bars are not explained (variance or confidence interval?), and the precise number of steps/increments between rollouts is missing.\n\n**Long-Term Generalization/Failure Analysis:** Despite strong zero-shot and out-of-domain results, there is little in-depth analysis of tasks where UltraCUA fails, or ablation of failure types (e.g., cross-application workflows with conflicting action modalities, tool staleness, context window overflow in tool lists). Figure 4 alludes to tool usage scaling, but the transition dynamics for model size/capability could be more rigorously explored.\n\n**Ethical and Deployment Discussion:** While the system release is promised, there is only a high-level mention of responsible practices. Potential risks (e.g., adversarial tool use, privacy issues in general desktop automation) could merit deeper engagement, given the powerful cross-domain control this agent enables.\n\n**Ablation/Qualitative Examples—Scope:** Figures 5, 6, and 7 are strong but limited in their domain and do not cover \"hard cases\" (e.g., conflicting GUI and programmatic actions, ambiguous instructions, adversarial UIs). Supplementing these with explicit failure cases or hybrid action errors would sharpen the case for UltraCUA's robustness.\n\n**Notation and Algorithmic Exposition:** Some key technical mechanisms (e.g., memory tag handling, programmatic tool argument parsing, trajectory sampling procedures in SFT and RL) are described mostly in prose. There are opportunities to formalize these for reproducibility and clarity, as the current description might not support faithful replication.\n\n**Loosely Defined Baselines:** For Table 1, the baseline \"multi-agent frameworks\" and \"general models\" categories group together fundamentally different models; more careful stratification and breakdown could illuminate *where* UltraCUA offers most gain.\n\nPotentially Missing Related Work\n\n- Yang, Y., Yang, Z., Dou, Z.-Y. (2025): *UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action* — This foundational work appears highly relevant and must be explicitly discussed in Related Work and methodology positioning, especially regarding its own hybrid action mechanism.\n- Wang, Y., Li, M., Chen, H. (2025): *A Case for Declarative LLM-friendly Interfaces for Improved Efficiency of Computer-Use Agents* — Should be cited around Section 2.1 and in comparison to the programmatic tool pipeline as it proposes a novel interface paradigm relevant to UltraCUA's efficiency claims.\n- Sager, P. J., Meyer, B., Yan, P. (2025): *AI Agents for Computer Use: A Review of Instruction-based Computer Control, GUI Automation, and Operator Assistants* — Particularly useful for providing broader context in the Related Work section and comparative analysis.\n- Zhang, Y., Wu, J., Li, X. (2025): *AXIS: Efficient Human-Agent-Computer Interaction with* — Related for its focus on effective computer interaction, relevant both to Hybrid Control and evaluation metrics.\n- Gou, L., Wu, J., Kil, J. (2025): *GUI Agents: A Survey* — Should be cited to position UltraCUA within the development trajectory of GUI hybrid agents; discuss in Related Work.\n- Mei, K., Zhu, X., Gao, H. (2025): *LiteCUA: Computer as MCP Server for Computer-Use Agent on AIOS* — Discuss in the context of augmenting agents with an MCP-like abstraction; compare to UltraCUA's dual-pipeline tool strategy.\n- Yeh, T., Chang, T.-H., Miller, R. C. (2009): *Sikuli: Using GUI Screenshots for Search and Automation* — Classic reference for visual GUI automation; discuss in tool repertoire/seamlessly integrating visual actions."}, "questions": {"value": "**Justification of the Scaling Premise:** Given the marginal absolute performance gains from both the scaled toolset (1.9%) and the scaled RL task-set (1.9%), what evidence suggests that this scaling approach has further potential? Is it possible that the current benchmarks are saturated, or that this hybrid-control approach hits a performance ceiling quickly, making further scaling of tools and tasks an inefficient endeavor?\n\n**Limitations of Automated Evaluation:** The OSWorld benchmark relies on automated, rule-based evaluators. Could these evaluators be \"gamed\" by programmatic tool calls that satisfy the rule but fail in a general, human-centric sense? How do you ensure the 17,000 synthetic tasks, particularly the 'evaluator-first' ones, are robust and not just teaching the agent to satisfy a narrow set of programmatic checks?\n\n**Action Selection Modality:** Please elaborate on the mechanism by which UltraCUA policies select between GUI primitives and programmatic tool calls at every decision step. Are there dedicated selectors, separate output heads, or integrated token-level outputs? How is supervision provided beyond reward (e.g., via imitation, auxiliary losses)?\n\n**Tool-Use Reward Sensitivity:** What is the sensitivity of UltraCUA's performance to the chosen value of the tool-use reward (0.3)? Have you qualitatively analyzed any failure cases where this reward unduly encouraged or discouraged hybrid action?\n\n**Failure Analysis—Failure Modes:** Can you provide a breakdown of where UltraCUA fails relative to GUI-only or tool-only baselines in OSWorld or WindowsAgentArena (e.g., cross-application, ambiguous tool names, tool argument errors)?\n\n**Ethical Safeguards:** Are there any built-in mechanisms or guidelines for restricting tool use, preventing sensitive operations, or handling user privacy in deployment scenarios?\n\n**Reproducibility—Implementation Details:** Would you be able to release scripts for programmatic tool extraction and synthetic data generation? Is the memory management/tagging mechanism easily extensible to custom or third-party applications?\n\n**Scaling to Long-Horizon/Real Desktop Scenarios:** How does UltraCUA's performance degrade in long-horizon tasks or under context-window overflow (for tool lists)? Any plans for hierarchical memory or tool prioritization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i1dAcEZV8o", "forum": "yVP2ldY9lw", "replyto": "yVP2ldY9lw", "signatures": ["ICLR.cc/2026/Conference/Submission15862/Reviewer_a2H9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15862/Reviewer_a2H9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986610696, "cdate": 1761986610696, "tmdate": 1762926081676, "mdate": 1762926081676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes UltraCUA, a computer-use agent that operates with hybrid control, combining low-level GUI actions with high-level programmatic tool calls. The authors build an automated pipeline to collect tools from documentation, open-source repos, and code-generated utilities, synthesize 17k+ verifiable tasks, gather 26.8k hybrid control trajectories using a planner-grounder setup, and train 7B and 32B models via SFT followed by online RL. On OSWorld, UltraCUA improves over base models and shows some cross-OS generalization to WindowsAgentArena."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear problem motivation: GUI-only agents suffer from cascading errors and long action chains; hybrid control is a reasonable and timely direction.\n\nComprehensive system design: Figure 2 presents an integrated pipeline that includes tool mining, task synthesis, trajectory collection, and a two-stage SFT+RL training loop. The components fit together coherently.\n\nStrong headline results: Table 1 reports notable gains on OSWorld, particularly for the 32B model (39.0% at 15 steps), and Figure 1a visually reinforces these differences. Table 2 shows the 7B model’s out-of-domain performance on WindowsAgentArena."}, "weaknesses": {"value": "Empirical Scope on Real-World Generalization: While out-of-domain generalization is tested (WindowsAgentArena), the real-world applicability remains partially open. The synthetic task engine generates diverse and realistic tasks, but there is only limited evidence that UltraCUA can robustly handle genuinely unseen, messy, or poorly documented real-world desktop environments and apps outside established benchmarks.\n\nSynthetic Data Limitations and Bias: The bulk of training data is synthetic (Section 2.2, Appendix A.4, Table 6/8). There’s inadequate discussion or empirical assessment regarding potential biases or coverage gaps—e.g., are critical real user workflows or less common edge-case behaviors captured? The risk of overfitting to synthetic artifacts versus natural UI heterogeneity is underexplored."}, "questions": {"value": "Can the authors provide more detailed analysis of real-world failure cases? Where does the hybrid agent most frequently fail in practice—are there common themes in tool misfire versus GUI confusion versus state management breakdown?\n\nHow well does UltraCUA handle applications and operating systems not seen in either the training or out-of-domain benchmarks (e.g., MacOS, obscure Linux distros, proprietary business software)? Any qualitative/quantitative results or prospective extension plans?\n\nFor the RL pipeline, can the authors formalize the policy update rule in more detail? Is it possible to provide pseudocode or more explicit algorithmic structure for the hybrid control loop?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IDyKsKQaED", "forum": "yVP2ldY9lw", "replyto": "yVP2ldY9lw", "signatures": ["ICLR.cc/2026/Conference/Submission15862/Reviewer_uv6L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15862/Reviewer_uv6L"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762697667303, "cdate": 1762697667303, "tmdate": 1762926081316, "mdate": 1762926081316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}