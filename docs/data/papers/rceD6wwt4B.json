{"id": "rceD6wwt4B", "number": 6803, "cdate": 1757996385543, "mdate": 1759897892592, "content": {"title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents", "abstract": "With advances in decision-making and reasoning capabilities, multimodal agents show strong potential in computer application scenarios. Past evaluations have mainly assessed GUI interaction skills, while tool invocation abilities, such as those enabled by the Model Context Protocol (MCP), have been largely overlooked. Comparing agents with integrated tool invocation to those evaluated only on GUI interaction is inherently unfair.\nWe present OSWorld-MCP, the first comprehensive and fair benchmark for assessing computer-use agents’ tool invocation, GUI operation, and decision-making abilities in a real-world environment. We design a novel automated code-generation pipeline to create tools and combine them with a curated selection from existing tools. Rigorous manual validation yields 158 high-quality tools (covering 7 common applications), each verified for correct functionality, practical applicability, and versatility.\nExtensive evaluations of state-of-the-art multimodal agents on OSWorld-MCP show that MCP tools generally improve task success rates (e.g., from 11.6\\% to 22.1\\% for OpenAI\no3 at 15 steps, from 40.1\\% to 43.8\\% for Claude 4 Sonnet at 50 steps), underscoring the importance of assessing tool invocation capabilities. However, even the strongest models have relatively low tool invocation rates, Only 37.4\\%, indicating room for improvement and highlighting the benchmark’s challenge.\nBy explicitly measuring MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents and sets a new standard for evaluating performance in complex, tool-assisted environments. We will release all code and data to the community.", "tldr": "", "keywords": ["MCP", "Computer-use Agent", "LMM"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/acaff39db05997f18f979450d294cbcc8f88e111.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents OSWorld-MCP, a novel and comprehensive benchmark framework to evaluate computer-use agents within the OSWorld benchmark. This framework incorporates tool use (via the Model Context Protocol, or MCP) into the standard GUI benchmark. The authors show that by leveraging the MCP tools, agents achieve improved task success in OSWorld tasks.\nThis paper also introduces an end-to-end pipeline to curate MCP tools, leveraging both LLM tool generation and human verification. The paper presents 158 high-quality, manually-validated MCP tools, allowing agents to autonomously choose between GUI operations and tool calls."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is a solid, well-written paper. The experimental design is very detailed, and the authors do a great job discussing the results, which are supported by clear visualizations. I especially appreciated the ablation studies on the number of tools, the RAG module, and the order of tools in the prompt—these were very interesting. The detailed case studies in the appendix are also a helpful addition.\n\nThe core idea of combining MCP tools with a GUI agent is novel. This is a strong piece of work that should be a valuable contribution to research on both MCP and GUI agents."}, "weaknesses": {"value": "I would like to see more detail on the RAG module's design for the MCP tool selection. The paper doesn't specify what retrieval method was used or its retrieval accuracy, which makes it hard to fully understand that component of the experiment. Additionally, the tools were generated by \"producing code-based solutions for every task in OSWorld\", which may cause an unfair comparison. Since the tools were created using the same tasks they were evaluated on, there's a risk of task information leakage. It would be better to generate the tools using one set of tasks and then evaluate their performance on a separate, held-out validation set."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S8nshRHXm1", "forum": "rceD6wwt4B", "replyto": "rceD6wwt4B", "signatures": ["ICLR.cc/2026/Conference/Submission6803/Reviewer_DEYi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6803/Reviewer_DEYi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761614767753, "cdate": 1761614767753, "tmdate": 1762919074190, "mdate": 1762919074190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces OSWorld-MCP, a benchmark designed to fairly and comprehensively evaluate multimodal computer-use agents that combine GUI interaction with Model Context Protocol (MCP)–based tool invocation. It extends OSWorld by integrating 158 manually verified MCP tools covering 7 common applications (LibreOffice, VS Code, Chrome, VLC, OS utilities, etc.)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The automated pipeline (generation, filtering, wrapping) for tool creation is well-structured and uses both LLM-based and manual verification steps.\n\n- Covers multiple proprietary and open models under consistent settings, providing valuable comparative insights."}, "weaknesses": {"value": "- While benchmark creation idea is strong, analysis mostly confirms intuitive findings (e.g., higher TIR means higher accuracy)\n\n- The paper’s main contribution (augmenting OSWorld with MCP tool invocation) is meaningful, but I’m concerned that OSWorld’s task design predates MCP and may not naturally align with tool-based workflows. The authors partly mitigate this by revalidating 361 tasks, identifying 250 “tool-beneficial” cases, and manually curating 158 tools, which shows care in adaptation. However, the benchmark still fundamentally inherits OSWorld’s task definitions, rather than introducing new ones designed from the ground up for MCP-enabled agents. This limits its novelty and may constrain how representative it is of modern tool-use scenarios\n\n- Some results (Tables 1–2) lack significance testing or variance measures; performance improvements could benefit from confidence intervals.\n\n- While clear overall, the draft is verbose in places and could improve figure readability and section organization. E.g. Figure 1 contained red wavy dash, need fix."}, "questions": {"value": "1. How was task difficulty balanced between GUI-only and MCP-enhanced versions to ensure fairness?\n\n2. Could tool invocation failures be categorized (e.g., recognition vs. reasoning vs. execution errors)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wEMc0Zc2PC", "forum": "rceD6wwt4B", "replyto": "rceD6wwt4B", "signatures": ["ICLR.cc/2026/Conference/Submission6803/Reviewer_SQsy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6803/Reviewer_SQsy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963726232, "cdate": 1761963726232, "tmdate": 1762919073761, "mdate": 1762919073761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OSWorld-MCP, a new benchmark for evaluating AI agents' ability to use both GUI and external tools via the Model Context Protocol (MCP). The authors developed a set of 158 high-quality tools and found that using them generally improves agent performance on various computer tasks. However, the results also show that even the most advanced models still struggle with effectively invoking these tools, indicating significant room for improvement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Creation of a High-Quality and Rigorously Validated Toolset**: The paper's primary contribution is the construction of a novel benchmark centered around a \"curated collection of 158 high-quality tools\". The reliability of this toolset is strongly supported by a meticulous creation process, which combines a \"novel automated code-generation pipeline\" with \"rigorous manual validation\". This validation was not superficial; each tool was independently assessed by at least two experienced reviewers to ensure its \"correct functionality, practical applicability, and versatility\", establishing a robust and trustworthy foundation for future agent evaluation.\n2. **Introduction of More Granular and Insightful Evaluation Metrics**: Moving beyond simple task accuracy, the paper introduces two new metrics—Tool Invocation Rate (TIR) and Average Completion Steps (ACS)—to enable a \"more nuanced evaluation\" of agent capabilities. TIR offers crucial insights into an agent's \"tool utilization propensity,\" while ACS directly \"quantifies task completion efficiency\". \n3. **A Unified Benchmark Bridging GUI and Tool-Use (MCP)**: A significant strength is the benchmark's design, which integrates GUI operations with MCP tool invocation in a single, dynamic environment. This approach directly addresses the \"gap between pure-GUI and text-based tool-use evaluations\", creating the first \"comprehensive and fair benchmark\" of its kind. By forcing agents to autonomously choose between GUI actions and MCP tools at each step, OSWorld-MCP provides a \"holistic and realistic assessment\" of an agent's hybrid decision-making skills in complex, real-world computer tasks."}, "weaknesses": {"value": "1. **Incremental Novelty Built on an Existing Framework**: While the work is a significant engineering effort, its core contribution is presented as an extension to the pre-existing OSWorld benchmark. The paper states it is \"Built upon a widely used... environment OSWorld\" [cite: 077-078]. Consequently, the foundational novelty could be viewed as somewhat limited, as it enhances an established platform rather than introducing a completely new paradigm for agent evaluation. \n2. **Insufficient Depth in Analyzing the Hybrid GUI-MCP Interaction**: The paper's central premise is that evaluating the choice between GUI and MCP is more realistic. However, the analysis could delve deeper into the nuances of this hybrid decision-making. While the results in Table 1 show that adding MCP tools improves outcomes, the paper does not offer a detailed qualitative analysis of the agents' decision-making process itself. For example, it lacks a thorough exploration of common failure modes, such as when an agent illogically prefers a complex GUI sequence over a simple tool, which would offer richer insights into the current reasoning limitations of models.\n3. **Current Limitations in Application and Task Diversity**: The benchmark's scope is currently constrained, as the authors acknowledge that \"MCP servers were not developed for GIMP and Thunderbird\". This gap limits the benchmark's ability to assess agent performance across a truly comprehensive range of real-world activities and narrows the generalizability of its conclusions."}, "questions": {"value": "1. Could you please provide more details on the Retrieval-Augmented Generation (RAG) system? Specifically, what retriever model was used, and how was the query formed at each step to filter the 158 tools?\n2. What was the exact prompt template used to integrate the visual information (screenshot), the task instruction, and the descriptions of the RAG-filtered MCP tools for the LMM to make a decision at each step?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wy4EUNJsj6", "forum": "rceD6wwt4B", "replyto": "rceD6wwt4B", "signatures": ["ICLR.cc/2026/Conference/Submission6803/Reviewer_cXcb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6803/Reviewer_cXcb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984168118, "cdate": 1761984168118, "tmdate": 1762919073383, "mdate": 1762919073383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces OSWorld-MCP, a new benchmark built on OSWorld for evaluating multimodal computer-use agents that perform both GUI operations and Model Context Protocol (MCP) tool invocations. It extends OSWorld by adding 158 validated MCP tools across seven common applications and supplements existing metrics with two new ones—Tool Invocation Rate (TIR) and Average Completion Steps (ACS)—to measure tool-use propensity and efficiency. An automated pipeline using OpenAI o3 is proposed to generate and curate high-quality tools. Experiments on several state-of-the-art multimodal models demonstrate that MCP tools substantially improve task performance, though multi-tool composition remains challenging. Overall, OSWorld-MCP offers a fair and realistic framework for benchmarking tool-augmented computer-use agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is original in extending OSWorld to evaluate MCP-based tool invocation, a capability overlooked in prior benchmarks. Its methodology is solid, combining automated tool generation with manual validation and extensive experiments on several state-of-the-art models. The presentation is clear and well-structured. The significance is high—OSWorld-MCP offers a realistic, open benchmark that will benefit research on tool-using multimodal agents. Its usefulness will further grow as more GUI applications adopt MCP-compatible tools."}, "weaknesses": {"value": "The benchmark’s scope is somewhat limited—its 158 tools cover mainly seven desktop apps, leaving out broader web or cross-platform settings. The tool-generation pipeline lacks quantitative evaluation of success and failure cases. Some tasks remain similar to the original OSWorld, reducing novelty in task design. While TIR and ACS are useful, their interpretation could be clearer. Finally, the benchmark’s long-term value depends on wider MCP adoption across GUI applications."}, "questions": {"value": "1.Tool Description Impact: How does the phrasing or length of tool descriptions influence model performance? Have the authors observed any sensitivity to wording or order beyond the shuffle experiment?\n\n2.Failure Case Analysis: Can the authors provide an analysis of common failure cases—specifically, whether models fail by selecting the wrong tool or by invoking the correct tool with incorrect arguments?\n\n3.Metric Interpretation: How should Tool Invocation Rate (TIR) be interpreted when models invoke tools unnecessarily or inefficiently—does a higher TIR always indicate better decision-making quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hD6NeYuoX1", "forum": "rceD6wwt4B", "replyto": "rceD6wwt4B", "signatures": ["ICLR.cc/2026/Conference/Submission6803/Reviewer_8XpA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6803/Reviewer_8XpA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045860912, "cdate": 1762045860912, "tmdate": 1762919073092, "mdate": 1762919073092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}