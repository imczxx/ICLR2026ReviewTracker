{"id": "1oB2Ks57Og", "number": 14238, "cdate": 1758230966437, "mdate": 1759897382003, "content": {"title": "Fast RoPE Attention: Combining the Polynomial Method and Fast Fourier Transform", "abstract": "The transformer architecture has been widely applied to many machine learning tasks. A main bottleneck in the time to perform transformer computations is a task called attention computation. [Alman and Song, NeurIPS 2023] have shown that in the bounded entry regime, there is an almost linear time algorithm to approximate the attention computation. They also proved that the bounded entry assumption is necessary for a fast algorithm assuming the popular Strong Exponential Time Hypothesis.\n\nA new version of transformer which uses position embeddings has recently been very successful. At a high level, position embedding enables the model to capture the correlations between tokens while taking into account their position in the sequence. Perhaps the most popular and effective version is Rotary Position Embedding (RoPE), which was proposed by [Su, Lu, Pan, Murtadha, Wen, and Liu, Neurocomputing 2024]. \n\nA main downside of RoPE is that it complicates the attention computation problem, so that previous techniques for designing almost linear time algorithms no longer seem to work. In this paper, we show how to overcome this issue, and give a new algorithm to compute the RoPE attention in almost linear time in the bounded entry regime. (Again, known lower bounds imply that bounded entries are necessary.) Our new algorithm combines two techniques in a novel way: the polynomial method, which was used in prior fast attention algorithms, and the Fast Fourier Transform.", "tldr": "", "keywords": ["computational complexity", "fine-grained complexity", "FFT", "polynomial method"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/193457098c56795a68260c58620512bbec78cf6c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a fast, almost linear-time algorithm for approximating RoPE-based attention. The main technique of the proposed method is a combination of the polynomial method and the Fast Fourier Transform (FFT) to handle the structure of RoPE. The paper also provides a computational hardness result."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "RoPE is a cornerstone of many state-of-the-art LLMs (Llama, Claude, etc.), and developing faster algorithms for it is of significant practical interest."}, "weaknesses": {"value": "1). The technical novelty is limited. The paper uses FFT to handle Toeplitz-like structures in positional encodings, which is also a known approach in existing models [1]. The primary contribution is the specific application of this technique to RoPE and combining it with the polynomial method which is yet another known method. While this is a valid contribution, the paper fails to articulate more generalizable algorithmic insight beyond this direct combination. \n\n[1] Qin, Zhen, et al. \"Toeplitz Neural Network for Sequence Modeling.\" The Eleventh International Conference on Learning Representations.\n\n2). The assumptions in the theorems, $B=o(\\sqrt{\\log n})$ and $d= O(\\log n)$, are not validated. In particular I doubt if $d= O(\\log n)$ is true in practice. For current LLMs, the embedding size is very large. For example, the embedding size of DeepSeek-R1 is 7168, which is unlikely to be of $\\log n$ scale.\n\n3). Although the research problem is driven by practical applications, the paper is purely theoretical. Without experiments, it is impossible to assess:\n  - The practical speed-up over standard, hardware-accelerated attention.\n  - The actual trade-offs between speed and approximation error.\n  - How the method compares to other approximate attention mechanisms in terms of quality and performance.\n  - The overhead introduced by the FFT and polynomial coefficient computations.\nAs a result, the paper's claims of efficiency cannot be translated into practical innovations.\n\n4).The related work section is weirdly long and diverges from the main topic. For example, it is not clear why accelerating diffusion models or GNNs is relevant to this work."}, "questions": {"value": "The paper mentions that Claude uses RoPE. How do you know that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6qxN6TYAwm", "forum": "1oB2Ks57Og", "replyto": "1oB2Ks57Og", "signatures": ["ICLR.cc/2026/Conference/Submission14238/Reviewer_3Gw1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14238/Reviewer_3Gw1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760672605931, "cdate": 1760672605931, "tmdate": 1762924692820, "mdate": 1762924692820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the first almost linear-time algorithm for RoPE attention under bounded entries, matching known lower bounds. It introduces a generalized RoPE attention problem (ARAttC), proves an upper bound via a novel combo of the polynomial method with FFT on sums of rescaled Toeplitz matrices, and a SETH-based lower bound."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The motivation of this paper is clear. The paper explains why classic polynomial-method low-rank arguments break under RoPE (Toeplitz-like structure rather than low rank) and why FFT is the right technique.\n\n(2) The method incorporates the polynomial approximation and fast computation of FFTs."}, "weaknesses": {"value": "(1) The theorems are asymptotic; it would help to expose the exact dependence on the polynomial degree and the number of rescaled-Toeplitz summands t after approximation. Here, n is the sequence length. In real applications, will it be approaching \\infty? I think the real LLMs have a sliding window, and n is not very large, right?\n\n(2) The paper does not conduct any experiments to show the improvement of the computation efficiency. I suggest including some experiments (even small synthetic ones) to compare the proposed method with Flashattention, etc."}, "questions": {"value": "(1) Is the polynomial approximation to the exp function stable? Especially when we are using higher-order polynomials. \n\n(2) How does the algorithm extend across multi-head attention and batching? \n\n(3) How does the approach interact with causal masking and sliding-window attention often used with RoPE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tV6OYioAlE", "forum": "1oB2Ks57Og", "replyto": "1oB2Ks57Og", "signatures": ["ICLR.cc/2026/Conference/Submission14238/Reviewer_6Kug"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14238/Reviewer_6Kug"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798444181, "cdate": 1761798444181, "tmdate": 1762924692382, "mdate": 1762924692382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a polynomial methods for efficient computation of attention, with **Rotary Position Embeddings** (RoPE).\nWhile prior works achieved near-linear algorithms for standard attention(under bounded entry regimes), these methods do not extend to the increasingly popular RoPE variant.\nThe authors develop a new algorithm that achieves near-linear time for RoPE attention computation, combining the polynomial method (for low-rank approximations) with the Fast Fourier Transform\nusing rescaled Toeplitz formulation.\n They prove that the nearly linear regime's theoretical thresholds extend to RoPE and provide the first such provable algorithms for this popular class of attention mechanisms."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Clear Motivation & Relevance: The paper highlights the importance of efficient attention mechanisms, especially as RoPE becomes standard in large LLMs (Llama, Claude, Gemini, Apple, etc.).\n\nOriginality: The combination of the polynomial method with FFT for rescaled Toeplitz matrices is novel, and the authors identify why previous techniques fail in the RoPE case.\n\nTheoretical Rigor: Strong upper and lower bounds are established. The authors are meticulous in showing tightness of their results, connecting them with SETH and prior complexity theory for attention.\n\nExposition: Section structure is logical and easy to follow. Notation, background, and step-wise algorithmic development are mostly clear. \nThe proofs and more technical details are referenced for reproducibility."}, "weaknesses": {"value": "Clarity: Some sections (esp. regarding structured matrix manipulations) assume a degree of background with FFT applications and polynomial approximations in algorithms.\n Additional diagrams or simplified intuition would make the work more accessible to a wider ML/AI audience.\n\nRelated Work Scope: The related work is comprehensive regarding theoretical literature, but more discussion about current practical/engineering solutions for fast attention \n(e.g., FlashAttention variants, hardware-accelerated solutions) might provide context for potential synergy or limitations.\nAlso, it is not clear if the RoPE issue is also present in Linear attention methods, where the attention is represented by matrix multiplication of (Mercer's) kernel functions. \n\nGenerality: The main result holds under bounded norm assumptions and for certain embedding sizes relative to sequence length (O(log n)). While justified, practical consequences\n and possible relaxations/tighter practical bounds could be discussed more. Can you point in which cases this bound breaks? is it model/ data dependent?\n\nExperimental Results Are Missing: Up to page 9 (end of main content), the paper is entirely theoretical.\n There is no validation of the algorithm's practical performance on real-world attention problems or large models (e.g., LLMs using RoPE in practice).\n While the theory is strong, empirical evidence for efficiency, accuracy, and scalability trade-offs is necessary for broader impact."}, "questions": {"value": "$Q_1$. Precision/Accuracy Robustness: How does the practical choice of polynomial approximation error $\\epsilon$ and norm bound $B$ affect downstream accuracy and speed?\n Are there scenarios (e.g., quantized or noisy activations) where theoretical advantages may not materialize?\n\n$Q_2$. Scalability: while the paper highlights regime restrictions (O(logn)), is it possible to extend your approach to non-logarithmic scenarios in practice? or will a break in theoretical guarantees?\n\n$Q_3$. Hardware implications: Given the rising importance of custom hw accelerators, is your method amenable to efficient hardware implementation/composability with existing frameworks?\n\n$Q_4$. Empirical Validation: Do you plan to provide experiments on LLM inference/training with RoPE using your algorithm?\n How does the runtime and accuracy compare to current best practical methods (e.g., FlashAttention) on models like Llama-2/3?\n \n$Q_5$. Linear attention extention: A vast class of fast attention methods are linear attention. Does the RoPE issue present also in this class of attentions? Does your method applicable to this case (e.g., Nystrom, Performer, etc.)? \n\n$Q_6$. Low-dregree polynomial approximation: In lines 181-190 the paragraph explains that exp function can be approximated by a polynomial function.\n However, softmax defintion is diffferent and will have a different approximation error when using low degree polynomial to apprixmate. \n You should clarify the consequences of low-degree approximation of softmax rather than exp. \n\n$Q_7$. Minor comments : \n - definition in line 63, is repeating again in line 68.\n - in line 60 \"this lower bound\" refers to ? \n - equation (1) is per head (should be clarified) \n - line 134, \"changing the many parameters\" - > \"changing many parameters\"\n - line 138, norm |S| should be defined (or referenced). \n - line 151, typo ==> 1/sqrt(d) (this sqrt{d} term should appear also in eq (1))"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "bCWwky80J2", "forum": "1oB2Ks57Og", "replyto": "1oB2Ks57Og", "signatures": ["ICLR.cc/2026/Conference/Submission14238/Reviewer_cCXb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14238/Reviewer_cCXb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844444608, "cdate": 1761844444608, "tmdate": 1762924691944, "mdate": 1762924691944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}