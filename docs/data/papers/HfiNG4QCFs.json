{"id": "HfiNG4QCFs", "number": 25026, "cdate": 1758363343710, "mdate": 1759896737759, "content": {"title": "Asymmetric Effects of Self-Corrective Learning on Chain-of-Thought Reasoning for Efficient Policy Adaptation", "abstract": "Recent advances in language model (LM)-powered agents have demonstrated the potential to tackle complex embodied tasks by grounding the models’ commonsense world knowledge in the interactive physical environments in which the agents operate. However, these LM-based agents' adaptation to a stream of diverse tasks over time remains challenging, particularly under limited supervision and resource constraints. In this paper, we present BiCL, an embodied task adaptation framework that addresses the problem of continual LM finetuning across diverse tasks and adaptation stages using only a small dataset per task and a small LM (i.e., with 0.5B parameters). We devise bidirectional CoT learning, which jointly optimizes chain-of-thought (CoT) reasoning and reflexive reasoning through per-task bidirectional supervision: few-shot CoT guidance and rationale-wise correction. The latter enables the model to revise its prior rationale trajectories for new tasks, while the former strengthens multi-step task-specific reasoning through minimal demonstrations. This dual optimization allows the agent to adapt more efficiently through forward knowledge transfer over time, ultimately yielding asymmetric effects by fostering robust CoT reasoning at inference without requiring explicit reflection. Furthermore, we implement rationale-wise test-time scaling, a mechanism that dynamically adjusts the depth of CoT reasoning based on the model’s confidence in actions inferred from its own rationales. Through extensive experiments on VirtualHome and ALFWorld, we demonstrate performance superiority over other LM-based planning and continual task adaptation approaches, while achieving strong efficiency in computation, data usage and model parameters.", "tldr": "", "keywords": ["embodied agent", "task adaptaton"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a14d24a6b85dad429d9b69e79458fe35273f3e86.pdf", "supplementary_material": "/attachment/a6ed70894f881c08db1a963b2bea73e44e40fb78.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses LM continual learning across task variations by proposing a method that jointly optimizes CoT reasoning and reflexive reasoning using few-shot expert demonstrations. To improve inference efficiency, the proposed method is further equipped with rationale-wise test-time scaling to dynamically adjust the depth of CoT reasoning, which is achieved by evaluating model's logits on ground-truth actions conditioned on generatated rationales.\n\nExperiments on VirtualHome and ALFWorld shows the effectiveness of proposed method (with test-time scaling) compared to several continual task adaptation approaches using small LMs, though it still underperforms LLMs (GPT-4o) prompted with demonstrations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Simultenously fine-tuning CoT and reflexive reasoning in LMs instead of treating them as separate capabilities is insighful, and the findings about the asymmetric effects of reflexive reasoning on CoT reasoning is promising: reflecive reasoning can enhance CoT reasoning through LM fine-tuning but it does not require explicit reflection during inference.\n2. Dynamically adjust the depth of CoT reasoning during inference is crutial to computation efficiency, though the paper relies on ground-truth actions to achieve this.\n3. The paper is well-written and extensive experiments have been conducted to demonstrate the effectiveness of proposed method in different aspects, including inference efficiency and training data efficiency."}, "weaknesses": {"value": "1. **Heavily relies on expert demonstrations.**\nEach component of BiCL including the rationale-wise TTS relies heavily on expert demonstrations consisting of CoT rationales and ground-truth actions, which is impractical in many realistic embodied tasks, especially those involving open-ended environments without any ground-truth labels. \n2. **Concerns about the usefulness of the method in practice.**\nIn the paper, the rationales in demonstrations are annotated by GPT-4o-mini, which still relies on LLMs despite that the paper assums a resource-constrained setting. Even so, fine-tuning a small LM (qwen-0.5b) via LoRA with the distilled CoT (from GPT-4o-mini) still underperforms GPT-4o, raising concerns about the significance of using such a method in practice.\n3. **Assumed resource-constrained settings does not aligned with the experiments.** \nBenchmarks used in the experiments, VirtualHome and ALFWorld, does not specifically aim for resource-constrained settings in embodied tasks, so it is not aligned with the paper's assumption that only small LMs (0.5b, 1.5b, 3b) can be available. It is noted that a qwen2.5-7b model trained with RL has achieved nearly perfect performance in ALFWorld [1].\n4. **Lack of ablation studies to validate each component of BiCL.** \nThough the paper has provided some ablation results, but they are separate, making it hard to find what component contributes most and least to BiCL.\n5. Better to provide ethics and reproducibility statement.\n\nReferences:\n\n[1] Feng, Lang et al. “Group-in-Group Policy Optimization for LLM Agent Training.” NeurIPS 2025."}, "questions": {"value": "1. What does \"generate rationales segment by segment\" in line 208 mean?\n2. Please elaborate how to acquire the feedback $f (z_k, z^′_k)$ in details.\n3. Please elaborate the notion of CoT step, action execution step, and the task index as welll as their relationships. For example, within a task $i$, does a trajectory contain several action steps and before each action step, $k$ CoT steps are generated? How to define a CoT step?\n4. Please elaborate more on **BiCL w/o base**. If it does not leverage the base reasoning-policy in any form, then what policy does it use?\n5. How to calculate the percentage of the rationale $z_k$ at which reasoning is terminated in Table 6?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gkRPrgAoJU", "forum": "HfiNG4QCFs", "replyto": "HfiNG4QCFs", "signatures": ["ICLR.cc/2026/Conference/Submission25026/Reviewer_6u9M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25026/Reviewer_6u9M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723419055, "cdate": 1761723419055, "tmdate": 1762943287763, "mdate": 1762943287763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses continual task adaptation for LM-based embodied agents under limited supervision and small model capacity. It proposes BiCL, a framework which jointly optimizes CoT reasoning and reflexive reasoning via few-shot demonstrations and rationale-wise correction of prior policie. Moreover, it introduces rationale-wise test-time scaling to dynamically adjust reasoning depth. Experiments on VirtualHome and ALFWorld show performance gains over baselines in data, computation, and parameter efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is generally well-written and easy to follow.\n\n- I think the idea of Introducing reflexive reasoning as correction of prior policy rationales, thus enabling forward knowledge transfer without inference-time reflection, is novel and interesting.\n\n- Experimental results on VirtualHome and ALFWorld show consistent performance improvement over previous baselines in data, computation, and parameter efficiency."}, "weaknesses": {"value": "- Token reduction is reported for BiCL, but no wall-clock time or FLOPs are provided.\n\n- Task sequences are fixed to 4 stages; scalability to longer streams is not assessed.\n\n- No ablation is provided to test whether the categorical feedback is necessary for reflection—e.g., whether using binary feedback (correct/incorrect) or the original design (minor/moderate/major revision) is better."}, "questions": {"value": "Please see the weaknesses for the major concerns. I recommend for weak acceptance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iJR8zobgnE", "forum": "HfiNG4QCFs", "replyto": "HfiNG4QCFs", "signatures": ["ICLR.cc/2026/Conference/Submission25026/Reviewer_LD91"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25026/Reviewer_LD91"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927850597, "cdate": 1761927850597, "tmdate": 1762943287442, "mdate": 1762943287442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BiCL, a framework for the continual adaptation of small language model-based embodied agents. The goal is to efficiently adapt to new tasks using only few-shot demonstrations and a small LM. The method consists of two main components: (1) \"bidirectional CoT learning,\" which jointly trains the model to generate CoT rationales and to reflexivel correct rationales from a previously learned policy, and (2) \"rationale-wise test-time scaling\", a mechanism to dynamically stop the reasoning process at inference. The authors claim this bidirectional training creates an \"asymmetric effect,\" where the model internalizes self-correction capabilities, leading to more robust single-pass inference without needing an explicit reflection step."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a significant and practical challenge: how to continually adapt LM agents in resource-constrained environments where standard \"pretrain-then-finetune\" or large-scale prompting is not feasible.\n- Within its chosen setting, the BiCL framework demonstrates consistently superior performance over relevant continual learning baselines, such as sequential finetuning (SeqFT-Distill) and adapter-based methods (TAIL-Distill)."}, "weaknesses": {"value": "- BiCL is not a general-purpose post-training technique. It is a complex system with many moving parts, each tailored to this specific embodied agent setup. This complexity makes the method difficult to reproduce and adapt to other domains, feeling less like a principled learning framework and more like a collection of specialized heuristics.\n- The TTS mechanism relies on a threshold $\\delta_k$ computed via a hyperparameter $\\lambda$. The paper reports using $\\lambda = -0.524$ for VirtualHome and $\\lambda = 0.0$ for ALFWorld. A mechanism that requires environment-specific hyperparameter tuning is not robust. Furthermore, the sensitivity analysis shows that performance is indeed highly sensitive to this threshold, making it a fragile component rather than a stable efficiency gain.\n- The paper treats reasoning (generation) and reflection (correction) as two distinct tasks. However, more general and powerful reasoning frameworks tend to integrate these capabilities. Effective reasoning often involves a single, unified process where the model generates thoughts, identifies potential flaws, and self-corrects within one continuous \"long CoT\" generation."}, "questions": {"value": "Please refer to my weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Jokfseg6pi", "forum": "HfiNG4QCFs", "replyto": "HfiNG4QCFs", "signatures": ["ICLR.cc/2026/Conference/Submission25026/Reviewer_zaRL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25026/Reviewer_zaRL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961522449, "cdate": 1761961522449, "tmdate": 1762943287225, "mdate": 1762943287225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "### Summary: Asymmetric Effects of Self-Corrective Learning\n\n#### Research Problem\nThe paper addresses the challenge of **efficiently adapting Large Language Model (LLM)-powered embodied agents** to a stream of diverse, sequential tasks, especially under limited supervision (few-shot or zero-shot scenarios) and resource constraints. The goal is to maximize **forward transfer** (improving future tasks) while minimizing **catastrophic forgetting** (losing knowledge of past tasks).\n\n#### Methodology (BiCL)\nThe authors propose **BiCL (Bidirectional Contrastive Learning)**, an embodied task adaptation framework that leverages Chain-of-Thought (CoT) reasoning.\n\n1.  **Self-Corrective Learning:** BiCL uses a self-corrective mechanism to refine **both the CoT rationale and the corresponding action policy**. This is achieved by generating high-quality self-correction samples for both components.\n2.  **Asymmetric Contrastive Learning (ACL):** ACL is introduced to guide the adaptation process:\n    * **Forward Transfer (CoT Rationale):** ACL maximizes the **similarity** between the current task's rationale and those of *future* tasks.\n    * **Catastrophic Forgetting (Action Policy):** ACL maximizes the **dissimilarity** between the current task's policy and those of *past* tasks. This asymmetric objective ensures the CoT is broad and transferable, while the policy remains discriminative and task-specific.\n3.  **Rationale-Wise Test-Time Scaling:** A mechanism is used during inference to adapt the CoT's influence based on its confidence, promoting a **balanced trade-off** between reasoning and action selection.\n\n#### Key Experiments\nBiCL is evaluated on two complex embodied continual learning benchmarks:\n* **ALFWorld** (textual environment).\n* **VirtualHome Beh-IL** (visual environment with hierarchical actions).\n\n**Results** show that BiCL significantly **outperforms state-of-the-art baselines** in both seen and unseen task categories, demonstrating superior **bidirectional adaptation** by achieving high forward transfer and minimal catastrophic forgetting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "### Strengths of the Paper (Asymmetric Effects of Self-Corrective Learning)\n\n1.  **Novel Asymmetric Contrastive Learning (ACL) for Continual Adaptation:** The core strength is the **BiCL** framework's use of Asymmetric Contrastive Learning. It cleverly separates the learning objectives for the reasoning component (CoT rationale) and the action component (policy). It promotes **forward transfer** by making the CoT rationale broadly similar to future tasks, while simultaneously mitigating **catastrophic forgetting** by making the current policy discriminatively dissimilar from past policies. This bidirectional optimization is highly effective for continual learning.\n\n2.  **Integrated Self-Correction and Dual Refinement:** The methodology incorporates a robust **self-corrective learning** mechanism that refines *both* the Chain-of-Thought (CoT) rationale and the *action policy* based on generated high-quality samples. This dual refinement ensures that the agent's internal reasoning process and its external physical actions are consistently optimized, leading to more reliable and accurate policy adaptation under limited supervision."}, "weaknesses": {"value": "### Potential Weaknesses of the Paper (Asymmetric Effects of Self-Corrective Learning)\n\n1.  **Complexity and Computational Cost of BiCL:** The proposed **BiCL** framework is inherently complex, involving three interconnected components: Chain-of-Thought (CoT) generation, self-correction for both rationale and policy, and Asymmetric Contrastive Learning (ACL). This complexity requires multiple forward and backward passes, likely leading to **significant computational overhead** compared to simpler fine-tuning or rehearsal methods, which may restrict its scalability or deployment in resource-constrained settings.\n\n2.  **Sensitivity to Hyperparameters (Rationale-Wise Scaling):** The paper introduces a **Rationale-Wise Test-Time Scaling** mechanism, which relies on a threshold ($\\delta_k$) to decide when to stop CoT reasoning. The results indicate that the performance is highly **sensitive to this threshold**, with both setting it too low or too high causing significant performance drops. This hyperparameter requires careful tuning per task or domain, potentially limiting its robustness and automatic deployment.\n\n3.  **Dependence on High-Quality Self-Correction Samples:** The effectiveness of the entire approach hinges on the ability of the self-corrective mechanism to generate **high-quality, accurate self-correction samples** for both the CoT rationale and the action policy. If the initial LLM-powered agent generates poor self-corrections (e.g., due to insufficient initial knowledge or ambiguous environments), the learning process could be misguided, leading to error amplification rather than improvement.\n\n4.  **Simulated Environment Bias:** While evaluated on two distinct benchmarks (**ALFWorld** and **VirtualHome Beh-IL**), both are **simulated environments**. The observed benefits in mitigating catastrophic forgetting and maximizing forward transfer might not translate seamlessly to the complexity, noise, and unpredictability of **real-world physical robot tasks**, where state observation is often partial and actions are subject to physical uncertainty."}, "questions": {"value": "### Related Questions and Suggestions for the Paper\n\n#### 1. Experimental Setup and Baselines (ACL vs. Self-Correction)\n\n**Question:** Given that the BiCL framework combines **Self-Correction** and **Asymmetric Contrastive Learning (ACL)**, have the individual contributions and interplay of these two complex mechanisms been sufficiently isolated and analyzed?\n\n**Suggestion/Query:**\n* Could the authors provide a more detailed **Ablation Study** to compare the effects of: (a) Self-Corrective Learning only (without ACL), versus (b) ACL only (without self-correction)?\n* This would help validate the specific role of ACL in promoting **forward transfer** and mitigating **catastrophic forgetting**, and confirm the method's superiority is not predominantly driven by the robust self-correction mechanism alone.\n\n#### 2. Methodological Rationale (Asymmetry Justification)\n\n**Question:** What is the fundamental **theoretical rationale** for the asymmetric design where the **Chain-of-Thought (CoT) rationale** aims to **maximize similarity** to future tasks, while the **Action Policy** aims to **maximize dissimilarity** from past tasks?\n\n**Suggestion/Query:**\n* Has the paper explored a **symmetric baseline** (e.g., maximizing similarity for both components) to rigorously demonstrate the necessity of this asymmetry?\n* Furthermore, how does this mechanism reliably handle tasks with highly similar CoT steps (e.g., planning to move to the same location) but requiring distinct physical policies (e.g., \"pick up\" vs. \"put down\") without the maximizing dissimilarity objective causing destructive interference?\n\n#### 3. Additional Experimental Content (Scale and Efficiency)\n\n**Question:** Since BiCL targets **embodied continual learning**, do the current experiments fully cover the challenges of long-term learning and real-world deployment complexity?\n\n**Suggestion/Query:**\n* **Scale and Duration:** Can the **length of the task stream** and the **diversity of tasks** be significantly scaled up (e.g., by an order of magnitude) to provide a more stringent test of the model's robustness and long-term **forgetting rate**?\n* **Efficiency Metrics:** As BiCL is computationally complex, it would be beneficial to include a detailed analysis of the **training time and inference latency** overhead introduced by the self-correction and ACL components, establishing the method's **efficiency-performance trade-off** compared to simpler baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3jKoEPfe1C", "forum": "HfiNG4QCFs", "replyto": "HfiNG4QCFs", "signatures": ["ICLR.cc/2026/Conference/Submission25026/Reviewer_K8gT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25026/Reviewer_K8gT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987089746, "cdate": 1761987089746, "tmdate": 1762943286992, "mdate": 1762943286992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}