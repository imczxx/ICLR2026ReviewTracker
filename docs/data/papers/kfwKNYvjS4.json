{"id": "kfwKNYvjS4", "number": 4009, "cdate": 1757583558908, "mdate": 1759898058191, "content": {"title": "Trading Carbon for Physics:  On the Resource Efficiency of Machine Learning for Spatio-Temporal Forecasting", "abstract": "Development of modern deep learning methods has been driven primarily by the push for improving model efficacy (accuracy metrics). This sole focus on efficacy has steered development of large-scale models that require massive resources, and results in considerable carbon footprint across the model life-cycle. In this work, we explore how physics inductive biases can offer useful trade-offs between model efficacy and model efficiency (compute, energy, and carbon). We study a variety of models for spatio-temporal forecasting, a task governed by physical laws and well-suited for exploring different levels of physics inductive bias. We show that embedding physics inductive biases into the model design can yield substantial efficiency gains while retaining or even improving efficacy for the tasks under consideration. In addition to using standard physics-informed spatio-temporal models, we demonstrate the usefulness of more recent models like flow matching as a general purpose method for spatio-temporal forecasting. Our experiments show that incorporating physics inductive biases offer a principled way to improve the efficiency and reduce the carbon footprint of machine learning models. We argue that model efficiency, along with model efficacy, should become a core consideration driving machine learning model development and deployment.", "tldr": "We promote evaluation practices that consider both efficacy and efficiency, and characterise the trade-offs through physics-inductive spatio-temporal forecasting models.", "keywords": ["physics informed machine learning", "energy consumption", "carbon footprint", "flow matching"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03fc05a5f58e13e2ed4a153d9b69ccb5a5396409.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores the paradigm of Physics informed Machine Learning in the context of tradeoff between efficiency and accuracy. The authors deep dive into how introducing physics biases into pure data-driven methods can reduce carbon footprint of the computational heavy deep learning models. The authors provide in-depth empirical results for the simplest physics problem (oscillation) to more complex physical phenomenon (navier-stokes). The experimental setup shows during training pure data-driven methods like UNet variants perform better in terms of accuracy and strong physics methods like FNO perform better in terms of efficiency with lowest carbon footprint. However, during inference FNO variants and Flow Matching models costs higher than pure data-driven models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper explores an important subject of carbon footprint of pure data-driven methods (deep learning models).\n2. In-depth experiments are performed targeting simplest physics to more complex physics phenomena. \n3. The paper presents both efficacy and efficiency metrics at both training and inference times.\n4. The discussion section of the paper is clearly and well-written highlighting how physics alone is not the answer of reducing carbon footprint."}, "weaknesses": {"value": "1. The authors emphasise on the trade-off for carbon footprint and accuracy, however as the problem gets complex the trade-off seems to disappear as Unet based variants do perform better at inference times as well as have a lower carbon footprint, while FM which is supposed to be a mid point for physics and data-driven has the highest carbon footprint at inference time.\n2. The study is solely done on synthetic dataset, it would be good to see some examples, efficacy and efficiency metrics for some real world datasets.\n3. In my point of view the study is largely experimental and I couldn't pinpoint the innovative aspect of it. Therefore, I think it needs to be backed up by a lot more experiments convincing that there indeed exists a accuracy–carbon trade-off in introducing strong and weak physics inductive bias."}, "questions": {"value": "1. An experimental setup on real world dataset would definitely strengthen the study.\n2. Additional experiments on real world dataset exploring the costs at training and inference times \n3. More insights into why the carbon footprint landscape changes during training and inference time. for example can authors provide an insight into why FM inference cost is so high, does it solely have to do with the increase in roll-out steps?\n4. Figure 8 a shows the predictive performance vs carbon footprint at training time, could authors include the same figure or results for inference time as well for longer rollouts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OJGFugluFn", "forum": "kfwKNYvjS4", "replyto": "kfwKNYvjS4", "signatures": ["ICLR.cc/2026/Conference/Submission4009/Reviewer_Y12R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4009/Reviewer_Y12R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760487968827, "cdate": 1760487968827, "tmdate": 1762917133904, "mdate": 1762917133904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper advocates for elevating resource efficiency (e.g., carbon footprint) to a primary consideration alongside efficacy in machine learning model development. Through a series of experiments on spatio-temporal forecasting tasks, the work systematically investigates how physics-inductive biases affect the trade-off between model performance and carbon cost. The authors compare a spectrum of models, from purely data-driven U-Nets to models with strong physical priors like PINNs and Fourier Neural Operators (FNOs). The study finds that incorporating physics biases generally leads to significant reductions in training costs, though this advantage may diminish or even reverse during inference."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Important Topic: The paper tackles the sustainability of machine learning, a topic of growing importance and immense practical relevance to the community. It makes a strong case for shifting from a performance-only paradigm to one that balances efficacy and efficiency.\n- Comprehensive Empirical Study: The systematic comparison across a spectrum of models with varying degrees of physics bias is rare and valuable. The detailed carbon footprint data provides an excellent baseline for future researchers.\n- Nuanced Analysis: The paper's sharp distinction between training and inference costs is an often-overlooked but critical detail. This finding deepens the understanding of a model's lifecycle cost."}, "weaknesses": {"value": "- Limited Novelty: The main weakness is the originality of the central thesis. The paper validates and quantifies what is largely \"common knowledge\" in the Scientific ML community: that physics constraints can reduce the reliance on brute-force computation. While the validation itself is valuable, it does not constitute a fundamental new discovery or method.\n- Lack of Hyperparameter Tuning: The authors acknowledge that all models were trained without extensive hyperparameter optimization. This is a significant limitation. Different architectures may have different sensitivities to hyperparameters, and the reported trade-offs might be artifacts of specific (and potentially suboptimal) configurations rather than intrinsic properties of the model families. This could affect the robustness of the observed Pareto fronts.\n- Generalizability of Conclusions: The study is focused exclusively on PDE-based spatio-temporal forecasting. While an excellent testbed, the paper does not deeply explore the generalizability of its findings to other domains where inductive biases are critical (e.g., equivariance in computer vision, structural biases in NLP)."}, "questions": {"value": "- How do the authors distinguish their contribution from the established consensus in the Scientific ML community that physics biases improve efficiency? Beyond providing quantitative evidence, does this work offer a new conceptual understanding?\n- The lack of hyperparameter tuning could systematically affect the results. For example, is it possible that a more complex architecture like a U-Net could benefit disproportionately more from tuning than an FNO, potentially altering the cost-performance trade-offs?\n- The Flow Matching model stands out as an outlier with high inference costs. Could the authors elaborate on the fundamental algorithmic or architectural reasons for this? Is this inefficiency inherent to the method or an artifact of the specific implementation used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FR8VWIoyip", "forum": "kfwKNYvjS4", "replyto": "kfwKNYvjS4", "signatures": ["ICLR.cc/2026/Conference/Submission4009/Reviewer_JQDf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4009/Reviewer_JQDf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880681728, "cdate": 1761880681728, "tmdate": 1762917133658, "mdate": 1762917133658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper claims that physics priors improve the carbon–accuracy balance for spatio-temporal forecasting. The experiments are a harmonic oscillator, viscous Burgers, and a small 2D periodic shear-flow subset. Carbon is computed with CarbonTracker using a fixed global grid intensity of 445 gCO2/kWh and a PUE factor; training is single-run; one-step inference is repeated three times. The main comparison is U-Nets, three FNOs, and a low-resolution flow-matching model that integrates 10 Euler steps per forecast.\n\nMy main concerns include the fact that the design fixes intensity and PUE, so reported CO2 is simply a constant multiple of energy/runtime. The \"carbon\" plots therefore do not add information beyond kWh and wall-time, so it is essentially a scalar multiple of what can be computed from existing descriptions of training runs and simulations. Moreover, it only computes this for their small-scale simulations, so likely does not accurately represent the tradeoff in real-world large-scale simulations. I think it would have been more accurately to simply perform a meta survey of existing simulations and models, rather than build them from scratch - because the observations are actually less realistic. In addition, the paper wholly misrepresents PINNs, and conflates them with amortised forecasters. In reality a PINN needs to be retrained for each new problem, and thus the author conclusions based upon them for forecasting tasks are invalid."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It is helpful to report both training and inference footprints on the same page.\n- The configs are stated clearly."}, "weaknesses": {"value": "- The paper's framing of PINNs within a \"forecasting\" context is misguided. A PINN is essentially a solver that learns one specific PDE _solution_ via gradient descent, not an amortised forecaster. Retraining per instance means it cannot generalise to new initial conditions or rollouts. Plotting a PINN on the same accuracy–carbon Pareto axis as data-driven forecasters conflates fundamentally different paradigms. This therefore complicates their claim about physics-informed models, since the PINN was used as an example at one side of the spectrum, when in reality, it is a completely different approach. In addition, the PINN (marked as the most physics-informed model) is actually much more computationally expensive than data-driven forecasters, since it needs to be retrained for each new problem, which ends up providing evidence in the complete opposite direction of the main thesis of the paper. (Note that I don't doubt that more appropriate priors reduce computational expense - I am simply pointing out that a PINN is not a good example to use for making this claim)\n- Because carbon intensity and PUE are fixed, CO2 is a scalar multiple of measured energy. The carbon plots therefore convey the same ranking as energy, though the authors do correctly measure real power usage (CarbonTracker) and note that runtime and energy are not strictly proportional. The carbon axis could be replaced by energy without loss of information. And the energy is closely tied to the GPU hours, which are already reported in many papers.\n- The benchmarks are didactic and not representative. The Harmonic Oscillator and Burgers equations are teaching tools, not serious simulations. But you could have easily computed the carbon usage of large-scale simulations that had been run previously (since it is standard to report compute costs), which would give you a more realistic scenario. The shear-flow case is small, 2D, periodic. If the goal is to compare ML against simulation on carbon, use an established turbulence dataset (see, e.g., Johns Hopkins Turbulence Database) with known solver cost and compare solver vs surrogate at matched skill, rather than re-running toys and drawing conclusions based on a completely different size class.\n- The FM setup is not matched to baselines. It operates at 1/16 resolution and performs 10 numerical steps per forecast, while others are full-resolution single-pass models. Any \"bias vs carbon\" statement gets confounded by resolution and integrator costs.\n- The metrics do not test physics. Pearson r (plus VRMSE) says little about incompressibility, spectra, conservation, or error growth. This does not support a claim about “trading carbon for physics”."}, "questions": {"value": "No questions. My concerns are fundamental rather than clarificatory. Please feel free to correct any mistakes in my understanding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XFbYZfK2ol", "forum": "kfwKNYvjS4", "replyto": "kfwKNYvjS4", "signatures": ["ICLR.cc/2026/Conference/Submission4009/Reviewer_4zZT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4009/Reviewer_4zZT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240362677, "cdate": 1762240362677, "tmdate": 1762917133422, "mdate": 1762917133422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}