{"id": "gwi2T2CsfK", "number": 801, "cdate": 1756818704621, "mdate": 1763236723650, "content": {"title": "FoeGlass: When Simple In-Context Learning Is Enough for Red Teaming Audio Deepfake Detectors", "abstract": "Audio deepfake detection (ADD) models are critical for countering the malicious use of text-to-speech (TTS) models. Evaluating and strengthening ADD models requires developing datasets that span the space of generated audio and highlight high-error regions. Existing dataset development strategies face two challenges: (i) manual collection, and (ii) inefficient discovery of blind spots in the ADD models. To address these challenges, we propose FoeGlass, the first black-box automated red-teaming method for ADDs, which effectively discovers ADD failure modes in the space of generated audio underexplored by state-of-the-art deepfake benchmarks. FoeGlass uses the in-context learning capabilities of an LLM to explore the input space of a TTS model, generating audio samples that fool the target ADD using only black-box access to all components. By using a carefully designed context based on diversity measurements, FoeGlass mitigates the common problem of mode collapse in automated red-teaming systems. Empirical evaluations on several open-source ADD and TTS models demonstrate that data generated from FoeGlass substantially improves the false negative rates over unconditional sampling baselines and recent spoofing datasets by up to 94%, while requiring no manual supervision. Furthermore, we show that the attacks generated by FoeGlass are transferable across different target ADDs, demonstrating its broad applicability and ease of use for the automated red teaming of ADD systems.", "tldr": "We introduce FoeGlass, a black-box method that uses an LLM to automatically find and generate audio deepfakes that fool detection systems without manual effort.", "keywords": ["Red-Teaming", "Audio Deepfake Detection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b6f236e32a551a13646f21bf2a189547a81b285a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method that controls large language model with chain-of-thought prompting on generating new audio Deepfake attacks to hack the Deepfake detectors, making text-to-speech systems generating difficult outputs, compensating the heavy labor inputs. The real attacker becomes then the LLM itself. A text-based feedback module prior to the detector is set , in order to provide the feedback for the context designer module. The full prompt attempt has been given in the Appendix as examples."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is very complete and clear in terms of presentation, objective an motivation. It has clear research objective, which is also quite concerning in modern audio Deepfake detection. \n2. In terms of originality, this study leverages the LLMs to control the TTS generator, with reward-like feedback scheme. It provides good level of insights to generate novel spoofing attacks, as well as training the defending models.\n3. Also thanks to the high transparency and simplicity of the related methods and analysis, this paper potentially has high level of re-producibility, despite the acquisition of LLMs, which can be a varying factor for generating random outputs via inference."}, "weaknesses": {"value": "1. In the related work of research, the author shall cite more about applying audio LLMs for security concern and more specifically, Deepfake detection.\n2. The experimental comparison between ASVspoof 5 and the proposed method might be unfair, due to the natural difference between deterministic and non-deterministic approaches. ASVspoof 5 data collection leverages a wide range of collaborators, but each team only contribute for a fixed number of samples within given context. The authors shall compare its work with more in-the-wild fake datasets such as In-the-Wild [1] and Deepfake-Eval-2024 [2], which contains more variations and fake samples. Of course, since we only concerns FNR here, the authors are advised to fetch only fake samples from those datasets, and see whether we can have consistency outperforming results.\n3. The absence of real samples. The current modern audio Deepfake detectors mainly has been trained on both real and fake samples, and acquiring solely fake samples may be biased when comparing across different models. This is not that relevant to the main novelty of the work, but including some real samples (e.g. from VCTK) and construct the evaluation set for computing equal error rates can help - the TTS models themselves sometimes will cast bias, and EERs can sidenote this or help finding such issue.\n4. The authors are not clear about the prompts themselves in this study - what part of the related prompts are annotated and written by the authors? What are automatically generated from the LLMs, with the instruction from the authors? The authors need to be crustal clear on this, to make the work extendable and more insightful.\n\n[1] Chandra, Nuria Alina et al. “Deepfake-Eval-2024: A Multi-Modal In-the-Wild Benchmark of Deepfakes Circulated in 2024.” ArXiv abs/2503.02857 (2025).\n[2] Müller, N., Czempin, P., Diekmann, F., Froghyar, A., Böttinger, K. (2022) Does Audio Deepfake Detection Generalize? Proc. Interspeech 2022, 2783-2787, doi: 10.21437/Interspeech.2022-108"}, "questions": {"value": "The questions and concerns from the reviewer has been outlined in the weakness section. Please answer or address the concerns in that section."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "This research has ethical concerns which span as below.\n\n1. Using text-to-speech methods for malicious purposes has been severe concern for years. The authors have only made a general promise on condemning the use and proposing related methods. However, for such framework and methods, licensing and disclaimers are what are actually needed, which is not what the authors have addressed throughout the whole paper.\n2. In terms of data licensing, if the author is going to release sample data, the licensing can be a issue to discuss and needs confirm."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eaJTpMEMTx", "forum": "gwi2T2CsfK", "replyto": "gwi2T2CsfK", "signatures": ["ICLR.cc/2026/Conference/Submission801/Reviewer_rpfF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission801/Reviewer_rpfF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761460757439, "cdate": 1761460757439, "tmdate": 1762915607949, "mdate": 1762915607949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FoeGlass, a black-box adversarial attack method targeting audio deepfake detection (ADD) systems. FoeGlass explores the input space of text-to-speech (TTS) models through large language model (LLM)-based sampling guided by multiple feedback signals. The effectiveness and transferability of FoeGlass are verified across several TTS–ADD model pairs. This work demonstrates that FoeGlass can identify blind spots in the latent space of ADD systems and cause transcript-level detection failures."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The attacker LLM is well-designed. Its prompt incorporates the history of failed and successful attacks as well as diversity-oriented feedback. The effectiveness of this design is demonstrated in the ablation studies.\n- The experiments are comprehensive, showing that FoeGlass successfully discovers blind regions that remain challenging for ADD systems.\n- The paper is generally well-written and logically consistent."}, "weaknesses": {"value": "- The paper lacks citations for closely related prior works. Although the authors claim to be the first to design such a transcript-level adversarial method for ADD, I found two highly related papers presenting very similar ideas [1, 2]. The authors should discuss differences and clearly position their contribution relative to prior works. Furthermore, compared to FoeGlass, [1] adopts a greedy optimization approach to generate textual perturbations for important words while preserving semantics. This strategy arguably represents a more realistic and practically feasible attack scenario.\n- The evaluation focuses primarily on ADD models that lack pretrained self-supervised learning (SSL) components. These models always perform worse than SSL-based ADD models. I am curious about how FoeGlass would perform on SSL-based ADD models. In addition, as a comparison with [1], evaluation results on commercial detectors are needed.\n\n[1] Nguyen, Binh, et al. \"What You Read Isn't What You Hear: Linguistic Sensitivity in Deepfake Speech Detection.\" arXiv preprint arXiv:2505.17513 (2025). \n\n[2] Turing's Echo: Investigating Linguistic Sensitivity of Deepfake Voice Detection via Gamification."}, "questions": {"value": "- I am concerned about the amount of LLM prompt usage required to generate a successful adversarial audio sample. I believe such an evaluation metric would be important for assessing real-world practicality.\n- The visualization of ADD blind spots is persuasive, but this aspect is not further explored. I am interested in what specific factors within FoeGlass-generated samples cause these blind spots. It would be helpful if the authors could suggest strategies for generating audio samples that better cover the overall latent space."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SbYTVNgA4q", "forum": "gwi2T2CsfK", "replyto": "gwi2T2CsfK", "signatures": ["ICLR.cc/2026/Conference/Submission801/Reviewer_6vBc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission801/Reviewer_6vBc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555522382, "cdate": 1761555522382, "tmdate": 1762915607805, "mdate": 1762915607805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FoeGlass, an automated red-teaming framework for Audio Deepfake Detection systems. FoeGlass uses in-context learning from a black-box reasoning LLM to generate natural adversarial audio samples by only optimizing input transcripts to TTS models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. FoeGlass introduces an automated red-teaming pipeline that leverages reasoning LLMs and iterative feedback to synthesize natural adversarial audio. This is a useful direction because it avoids hand-crafted perturbations and does not require labeled attack examples.\n\n2. The writing is good and easy to follow."}, "weaknesses": {"value": "1. The citation format/style throughout the paper is inconsistent and should be revised. For example, the very first sentence has an incorrect citation format, and similar issues appear in several other places.\n2. The audio deepfake detection models evaluated in the paper are not state-of-the-art. There are stronger and more recent models available on [Hugging Face’s Speech-DF-Arena](https://huggingface.co/spaces/Speech-Arena-2025/Speech-DF-Arena). Given this, I’m not sure how well the proposed method would perform on more advanced models.\n3. Also as shown in Table 2, it is interesting to see that, for the AST model, the FNR on ASVspoof5 is even higher than that on the FoeGlass-generated xTTS-v2 data. This further raises concerns about the generalizability of the proposed attack across different ADD models.\n4. It’s unclear how the paper defines and calculates the attack success rate. In audio deepfake detection, Equal Error Rate (EER) is a more standard and interpretable metric, as it identifies the point where FNR equals FPR. What threshold is used in the evaluation? If the authors used a fixed threshold of 0.5 to distinguish real and fake audio, please justify why. In practice, different models trained on different datasets typically require threshold tuning, so using a uniform threshold may not yield fair or comparable results. Clarifying this would make the evaluation more convincing."}, "questions": {"value": "1. The paper would benefit from a deeper discussion of why models such as VIT-Melspectrogram and ASV-Spoof5 exhibit better robustness to the proposed attack. Understanding the specific failure cases of FoeGlass could provide more insights valuable insights.\n2. It’s not entirely clear how diversity scores and context lengths influence the attack’s performance. Have the authors conducted any ablation studies or sensitivity analyses? \n3. In addition, since TTS models such as xTTS-v2 support multiple voices. Did the authors use different voices during red-teaming? Additionally, was there any observable difference in ASR performance across male/female voices, various accents, or languages? I don't see the details of how the attack generates the adversarial audio.\n4. While the results may not seem sound to me, the proposed red-teaming framework could be a potential adversarial component to help generate challenging fake audio samples to train the detectors. Have the authors explored if FoeGlass could improve the detection performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zEc5pucwAP", "forum": "gwi2T2CsfK", "replyto": "gwi2T2CsfK", "signatures": ["ICLR.cc/2026/Conference/Submission801/Reviewer_uoRF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission801/Reviewer_uoRF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897810600, "cdate": 1761897810600, "tmdate": 1762915607710, "mdate": 1762915607710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Request for Further Reviewer Engagement"}, "comment": {"value": "Dear Reviewers,\n\nThank you for your feedback on our paper.\n\nOur detailed rebuttal was released **5 days ago**, featuring new experimental results and clarifications that directly address the points raised. You may also find our responses to the other reviewers helpful.\n\nWe kindly encourage you to share any remaining questions or thoughts, as it would help to make a more informed decision on the paper.\n\nIf our rebuttal and revisions have resolved your initial concerns, we would be very grateful if you would consider updating your evaluation.\n\nThank you again for your time."}}, "id": "cSD7SckIt1", "forum": "gwi2T2CsfK", "replyto": "gwi2T2CsfK", "signatures": ["ICLR.cc/2026/Conference/Submission801/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission801/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission801/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763680244998, "cdate": 1763680244998, "tmdate": 1763680244998, "mdate": 1763680244998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}