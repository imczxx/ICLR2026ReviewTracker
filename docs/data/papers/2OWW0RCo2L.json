{"id": "2OWW0RCo2L", "number": 18978, "cdate": 1758292512480, "mdate": 1759897069414, "content": {"title": "Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented Generation with Flexible User Control", "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to mitigate large language model (LLM) hallucinations by incorporating external knowledge retrieval. However, existing RAG frameworks often apply retrieval indiscriminately, leading to inefficiencies---over-retrieving when unnecessary or failing to retrieve iteratively when required for complex reasoning. Although recent retrieval strategies can adaptively navigate among alternative retrieval strategies, they make their selection based solely on query complexity and incorporate no mechanism for prioritizing speed over accuracy or vice versa. This lack of user-defined control makes their use infeasible for diverse user application needs. In this paper, we introduce a novel user-controllable RAG framework that enables dynamic adjustment of the accuracy-cost trade-off. Our approach leverages two classifiers: one trained to prioritize accuracy and another to prioritize retrieval efficiency. Via an interpretable control parameter $\\alpha$, users can seamlessly navigate between minimal-cost retrieval and high-accuracy retrieval depending on their specific requirements. We empirically demonstrate that our approach effectively balances accuracy, retrieval cost, and user controllability \\footnote{Code is available at anonymous github \\url{https://anonymous.4open.science/r/Flare-RAG-Anonymous-D6A2/}.}, making it a practical and adaptable solution for real-world applications.", "tldr": "", "keywords": ["Retrieval Augmented Generation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/93cdea4c67348a535430dc25eaf137212191b29d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces **Flare-Aug**, a user-controllable Retrieval-Augmented Generation (RAG) framework that balances accuracy and computational cost through an interpretable parameter $\\alpha$. Flare-Aug enables users to dynamically adjust retrieval strategies according to their specific needs. The framework employs two classifiers: a Cost-Optimized Classifier (trained to select the cheapest correct retrieval strategy for a specific LLM) and a Reliability-Optimized Classifier (trained on dataset-level labels for LLM-agnostic stability). Users control the interpolation between these classifiers via $\\alpha \\in [0, 1]$. Evaluated on 6 QA datasets with Flan-T5 and GPT-4o, Flare-Aug demonstrates good accuracy-cost trade-offs compared to Adaptive-RAG and static baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "**Novel user controllability**: enabling explicit, interpretable control over accuracy-cost trade-offs via a single parameter"}, "weaknesses": {"value": "1. **LLM Usage Disclosure**.\n   - The manuscript contains extensive use of em dashes (—) appearing 19 times, which may indicate significant LLM involvement in paper writing. \n   - The conference guidelines explicitly state: \"Not disclosing significant LLM usage can lead to desk rejection.\" The authors should clarify the extent of LLM usage in preparing this manuscript to comply with submission policies.\n\n2. **Evaluation Metric Issues**\n   - Metric Definition: The accuracy metric is vaguely defined as \"whether the predicted answer contains the ground-truth answer\" (lines 328-329). \n   - This should specify either Exact Match (EM) or token-level F1 score following standard QA evaluation metrics.\n\n3. **Reproducibility Concerns**\n   - The authors randomly sample 500 queries from each dataset rather than using standard test splits. This deviates from established benchmarks and prevents reproducibility.\n   - The paper should either use official test sets or provide exact query IDs for the sampled subsets.\n\n4. **Insufficient Baseline Comparisons**\n   - The baselines are limited to Adaptive-RAG and static strategies, ignoring recent agentic RAG systems that autonomously determine retrieval necessity and stopping conditions. \n   - The paper should compare against agentic systems: AutoRAG, Search-o1, Search-R1, R1-Searcher, ReasonRAG, etc. Meanwhile, modern adaptive methods with planning capabilities should be compared.\n   - These systems naturally handle the search decision-making that Flare-Aug addresses, making them critical comparisons.\n\n5. **Outdated Model Choices**. Authors use Flan-T5 (XL/XXL) released in 2022, as answer generation models. This raises concerns about generalizability. The method should be validated on modern LLMs (e.g.,Llama-3,  Qwen2.5,  Qwen3) widely used in 2024-2025.\n\n6. **Weak Baseline Hypothesis**: In Figure 3, multi-step retrieval sometimes underperforms Flare-Aug, suggesting the answer model may be too weak to leverage retrieved information effectively. This questions whether observed gains stem from the proposed method or model limitations.\n\n7. **Classifier Training Overhead**. While authors claim computational efficiency (~640 seconds), they understate the data preparation burden:\n   - Query sampling across datasets\n   - Running LLM inference with all three strategies for labeling\n   - Manual verification of correctness\n\n   Moreover, the classifier supports only 3 decision classes. In contrast, modern LLMs possess strong built-in planning capabilities and can perform retrieval decisions in a training-free manner. The added complexity of maintaining separate classifiers seems unnecessary given advances in LLM reasoning.\n\n8. **Retrieval Strategy Limitations**\n   - BM25 Dependency: The exclusive use of BM25 is outdated. The claim that \"BM25 continues to outperform many dense retrievers\" (citing Ram et al., 2023) does not hold in 2025.\n   - Modern embedding models (BGE-M3, Qwen2.5-Embedding, Voyage-3) offer substantially superior retrieval quality. The method should be evaluated with state-of-the-art retrievers.\n\n\n9. **Multi-Step Retrieval Details Missing**. The paper lacks crucial implementation details:\n   - Query decomposition: How are sub-queries generated for multi-step retrieval?\n   - Knowledge gap identification: What determines missing information?\n   - Stopping criteria: When does multi-step retrieval terminate?\n\n   Without these details, the multi-step baseline cannot be reproduced, and its fairness as a comparison is questionable.\n\n8. **Minor Issues**\n   - Line 67-68: \"Flare-Agu\" should be \"Flare-Aug\"\n   - Figure quality: Figures 3-5 could benefit from larger fonts and axis rescalng for readability"}, "questions": {"value": "Recommendations for Revision\n- Add disclosure of any LLM usage in manuscript preparation\n- Clarify evaluation metrics (EM vs. F1) and use standard test sets\n- Include modern baselines: Self-RAG, agentic RAG systems with full results\n- Validate on current LLMs: Qwen2.5, Llama-3.1, Qwen3\n- Provide multi-step retrieval details: query decomposition, stopping criteria, implementation\n- Compare with modern retrievers: BGE-M3, Qwen3-embedding-0.6B or similar dense models\n- Justify classifier necessity given modern LLM planning capabilities"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UTzfaiWeYV", "forum": "2OWW0RCo2L", "replyto": "2OWW0RCo2L", "signatures": ["ICLR.cc/2026/Conference/Submission18978/Reviewer_VzJ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18978/Reviewer_VzJ8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18978/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632824144, "cdate": 1761632824144, "tmdate": 1762931030125, "mdate": 1762931030125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Flare-Aug, a user-controllable RAG framework featuring two externally trained routing modules, a Cost-Optimized Classifier (LLM-specific, selecting the least costly strategy that still yields a correct answer) and a Reliability-Optimized Classifier (LLM-agnostic, trained on single-hop vs. multi-hop supervision). Experiments on a mixed subsets of SQuAD, NQ, TriviaQA, MuSiQue, HotpotQA, and 2WikiMultiHopQA demonstrate smooth, monotonic accuracy–cost trade-offs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The single user-exposed alpha to navigate accuracy–cost trade-offs is intuitive; the author also shows accuracy and retrieval steps increasing monotonically with alpha choices.\n- The cost-oriented router is LLM-specific; the reliability router is LLM-agnostic, which is a reasonable decomposition for deployment.\n- Experiments across single-hop and multi-hop QA senarios show its effectiveness and strengthes."}, "weaknesses": {"value": "- This work interpolates model parameters of two independently trained classifiers with a linear combination (ref. formula in Line 303-304), but there is no justification that a linear blend in parameter space yields calibrated probabilities or coherent decision boundaries, especially their training objectives and label spaces are different. This control design seems ad-hoc rather than theoretically grounded.\n- A follow up question based on the first concerns: Why linear parameter interpolation rather than other strategies like score-space ensembling or a learned gate? Is there any calibration or decision-quality checks across alpha?\n- The LLM-agnostic classifier is labeled entirely by dataset identity, i.e., single-hop --> single-step and multi-hop --> multi-step. It is noisy and leverages dataset bias rather than ground-truth retrieval needs. This weak supervision risks over-retrieval on many \"easy\" queries and under-retrieval on \"hard single-hop\" ones.\n- Only BM25 is used for retrieval, however, dense/hybrid retrievers are also common settings. Switching to these settings may change the trade-off surface.\n- Dataset scale is relatively small, which limits statistical power and stress testing."}, "questions": {"value": "Please refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jCLxlaLKke", "forum": "2OWW0RCo2L", "replyto": "2OWW0RCo2L", "signatures": ["ICLR.cc/2026/Conference/Submission18978/Reviewer_rhyR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18978/Reviewer_rhyR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18978/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917686028, "cdate": 1761917686028, "tmdate": 1762931029356, "mdate": 1762931029356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Flare-Aug, a user-controllable RAG framework that enables dynamic accuracy-cost trade-offs via a tunable parameter α. The system trains two classifiers—Cost-Optimized (minimizes retrieval steps) and Reliability-Optimized (ensures answer quality)—then interpolates between them based on user preferences. Evaluated on 6 QA datasets with 4 LLMs, the method achieves competitive performance while providing flexibility lacking in prior work like Adaptive-RAG."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses Real User Need\nIdentifies genuine limitation in existing adaptive RAG: inability to adjust retrieval strategy based on application constraints. Medical diagnosis vs. customer chatbots have fundamentally different accuracy-latency requirements.2. Simple, Interpretable Design\nThe α parameter provides intuitive control: α=0 prioritizes speed, α=1 prioritizes accuracy. Monotonic relationships (Figures 3-5) make system behavior predictable and easy to tune.3. Solid Experimental Methodology\n\nTests 4 LLMs (Flan-T5-XL/XXL, GPT-4o/mini) showing generalizability\nMixed dataset (3 single-hop + 3 multi-hop) simulates query diversity\nExtensive ablations on classifier size, training epochs, α values\nLow training cost (~640 seconds for both classifiers)\n4. Practical Deployment Guidance\nOffers concrete α-tuning strategies (incremental adjustment, validation-based estimation) with empirical validation that validation set trends transfer to test set (Appendix C.2)."}, "weaknesses": {"value": "1. Limited Technical Novelty\nCore contribution is linear classifier interpolation: W = (1-α)W_cost + αW_reliability. This is standard multi-objective optimization used widely in ensemble methods and multi-task learning—not a novel algorithmic insight.2. Weak Theoretical Foundation\nReliability Classifier assumes single-hop datasets → single-step retrieval, multi-hop datasets → multi-step retrieval. This dataset-level labeling:\n\nConflates dataset construction bias with actual query complexity\nRemains empirically unvalidated (acknowledged in A.4 but dismissed)\nMay misclassify queries (e.g., simple questions in multi-hop datasets forced into expensive retrieval)\n3. Oversimplified Cost Model\nUses only retrieval step count as cost proxy, ignoring:\n\nLLM inference costs (GPT-4 >> Flan-T5 per token)\nRetrieval latency variations (BM25 vs. dense retrievers)\nContext length impact on GPU memory\nReal deployment costs = API fees + wall-clock time\n4. Catastrophic Failure with Unanswerable Queries\nTable 2 shows adding \"unanswerable\" class causes performance collapse. Cost Classifier loses ability to discriminate retrieval strategies, instead overfitting to answerable vs. unanswerable classification. No solution provided.5. Unfair Baseline Comparison\nClaims Adaptive-RAG lacks flexibility, but:\n\nAdaptive-RAG can retrain for different cost-accuracy preferences\nFlare-Aug also requires validation tuning of α (not truly \"online\")\nReal difference: one-time training + parameter vs. multiple training runs—a practical but not paradigmatic advantage\n6. Strong Baseline Buried in Appendix\nTable 5 shows direct prompting (asking LLM to decide retrieval strategy) achieves Acc=0.381, Steps=0.19—competitive with full system at lower complexity. Relegating this to appendix rather than engaging substantively raises questions about necessity of dual classifiers."}, "questions": {"value": "1. Have you tested on ANY out-of-distribution data? (e.g., customer service logs, medical records, code repositories, legal documents).\n\nIn particular:\nAcademic Query (your test set):\n\"Who invented the telephone?\" \n→ Single-step retrieval works perfectly\n\nReal-World Query (day 1 of deployment):\n\"When will my order #12345 ship?\"\n→ Your classifier predicts single-step BM25 retrieval\n→ Actually needs SQL database query (no text retrieval!)\n→ System fails completely\n\n2. Table 5 shows direct prompting (Acc=0.381, Steps=0.19) nearly matches your method (Acc=0.388, Steps=1.3) at 85% lower cost and zero training. Why not simply prompt GPT-4: \"Given this query and user priority (speed/accuracy), should I retrieve?\" This naturally handles distribution shift without dataset-specific classifiers.\n\n3. Your Cost Classifier labels are LLM-specific (training on Flan-T5-XL's successes/failures), but users may deploy on different LLMs. When a company switches from Flan-T5 to GPT-4, doesn't this require complete retraining, negating the \"reusability\" claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rACVYy6VhG", "forum": "2OWW0RCo2L", "replyto": "2OWW0RCo2L", "signatures": ["ICLR.cc/2026/Conference/Submission18978/Reviewer_8mHd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18978/Reviewer_8mHd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18978/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962829870, "cdate": 1761962829870, "tmdate": 1762931028761, "mdate": 1762931028761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a parametric classifier that determines the retrieval strategy for each individual query in RAG (Retrieval-Augmented Generation). The retrieval strategy for a query can be one of the three choices: no retrieval, single-step retrieval, and multi-step retrieval. The classifier was trained a T5 model. In experiments, a RAG system with the BM 25 retrieval model was setup for evaluation. The answering models in the experiments include Flan-T5-XL, Flan-T5-XXL, GPT-4o-mini, and GPT-4o. Experimental results show the addition of the classifier improves the RAG performance for the smaller models, while the more capable LLM, GPT-4o, performs slightly better with multi-step retrieval. However, the addition of the classifier can greatly reduce the retrieval steps."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Multi-step RAG is a practical and challenging issue. \n\n* The proposed method can be generally applied to many existing RAG systems and LLMs."}, "weaknesses": {"value": "* The training data and the test data were sampled from the same datasets. It is not clear if the test data were held-out from the training data. \n\n* In addition, it is also unclear how well the proposed classifier is applied to the test data from a new domain. \n\n* The addition of the classifier, which is a T5 model, adds computational cost and latency to the RAG pipeline. However, this paper did not provide an analysis of this overhead. A comparison between GPT-4o + the proposed classifier and GPT-4o + multi-step retrieval in terms of runtime could be added to clarify the benefit of the proposed method."}, "questions": {"value": "* Did you hold out the test data from the training data? \n\n* In addition to the T5 model, did you consider an even lightweight backbone model for the classifier?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5ZNYRJ9Uca", "forum": "2OWW0RCo2L", "replyto": "2OWW0RCo2L", "signatures": ["ICLR.cc/2026/Conference/Submission18978/Reviewer_ruAa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18978/Reviewer_ruAa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18978/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762337610350, "cdate": 1762337610350, "tmdate": 1762931028196, "mdate": 1762931028196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}