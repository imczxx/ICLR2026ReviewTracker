{"id": "Yjlsd2Ueox", "number": 7470, "cdate": 1758023615299, "mdate": 1759897851024, "content": {"title": "RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation", "abstract": "The safety and reliability of embodied agents rely on accurate and unbiased visual perception. However, existing benchmarks mainly emphasize generalization and robustness under perturbations, while systematic quantification of visual bias remains scarce. This gap limits a deeper understanding of how perception influences decision-making stability. To address this issue, we propose RoboView-Bias, the first benchmark specifically designed to systematically quantify visual bias in robotic manipulation, following a principle of factor isolation. Leveraging a structured variant-generation framework and a perceptual-fairness validation protocol, we create 2,127 task instances that enable robust measurement of biases induced by individual visual factors and their interactions. Using this benchmark, we systematically evaluate three representative embodied agents across two prevailing paradigms and report three key findings: (i) all agents exhibit significant visual biases, with camera viewpoint being the most critical factor; (ii) agents achieve their highest success rates on highly saturated colors, indicating inherited visual preferences from underlying VLMs; and (iii) visual biases show strong, asymmetric coupling, with viewpoint strongly amplifying color-related bias. Finally, we demonstrate that a mitigation strategy based on a semantic grounding layer substantially reduces visual bias by approximately 54.5\\% on MOKA. Our results highlight that systematic analysis of visual bias is a prerequisite for developing safe and reliable general-purpose embodied agents. Our code is available at [https://anonymous.4open.science/r/Roboview-Bias-CCFD-ee/](https://anonymous.4open.science/r/Roboview-Bias-CCFD-ee/).", "tldr": "", "keywords": ["benchmark", "embodied agents", "robotic manipulation", "visual bias"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c4b713cd32f936cdae8796dd262c70982bded7f1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces RoboView-Bias, a benchmark for analyzing visual bias in embodied agents built on foundation models. It isolates factors such as color, camera pose, and distance to evaluate how visual variation affects task performance. The authors propose metrics to quantify bias and show that color and viewpoint strongly influence results. They evaluate three embodied agents, two VLM-driven and one VLA model, and find that all exhibit strong visual biases, particularly sensitivity to camera viewpoint and color saturation. A case study on the MOKA agent identifies bias arising from mismatched language and vision modules. They address this by refining ambiguous instructions and reducing color bias."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear. The paper tackles visual bias in (foundation-model-based) embodied agents, distinct from the more common notion of ‚Äúrobustness under perturbation‚Äù. As a result, I am sufficiently convinced that analyzing behavioural bias from pre-training data, such as camera angles, distances, and colors, matters for reliability in embodied perception, apart from just robustness testing.\n2. Benchmark design. The factor isolation principle is well thought out. Varying one visual factor at a time while holding others constant gives interpretable results. The structured variant-generation framework (SVGF) is more principled than the random domain randomization used in prior works.\n3. Explicit bias metrics. The paper introduces additional metrics (Œº‚Çõ·µ£, CV‚Çõ·µ£, IEC) to measure both individual and interaction effects. Despite the overcomplicated notation, the idea is meaningful: measure not just mean performance, but stability under visual variations.\n4. Findings. The asymmetry between color and viewpoint bias is interesting. Also, the insight that viewpoint bias amplifies color bias (but not vice versa) is credible and well interpretable.\n5. The case study on MOKA‚Äôs color bias is a useful point. It goes beyond descriptive benchmarking by identifying how semantic inconsistencies in the planner (VLM) and perceptual deviations in the grounding module (DINO) compound to produce systematic color bias."}, "weaknesses": {"value": "1. The paper is very difficult to follow due to heavy LLM use for writing. The text is saturated with tautologies and filler phrasing such as *‚Äúwe systematically quantify,‚Äù ‚Äúwe comprehensively evaluated,‚Äù ‚Äúprinciple of factor isolation,‚Äù* and *‚Äústructured variant-generation framework.‚Äù* Such repeated, formulaic expressions rather inflate simple statements than clarify them. The writing also relies heavily on generic claim templates like *‚Äúsystematically quantify visual bias‚Äù* and *‚Äúrobust measurement of bias,‚Äù* which are reiterated with minor wording changes (*‚Äúsystematic measurement,‚Äù ‚Äúfactor isolation principle,‚Äù ‚Äúrobust generalization,‚Äù ‚Äúa rigorous Perceptual Fairness Validation pipeline‚Äù*). Such circular phrasing gives the appearance of methodological depth without adding substantive explanation. Additionally, there is inconsistency in terminology: the SGL method is called *‚ÄúSemantic Grounding and Perceptual Calibration,‚Äù ‚ÄúSemantic Grounding Layer,‚Äù* and then *‚ÄúSemantic Anchoring Layer.‚Äù* Pick one and stick to it. Finally, some sentences, e.g., *‚ÄúWe execute pre-training alignment instructions and visible evidence‚Äù,* read as syntactically correct but semantically incoherent. This non-exhaustive list of examples is a hallmark of LLM-generated text. One of the goals of a research paper is to clearly convey its ideas to the readers, and this work falls short on that front.\n2. Section 5 tries too hard to sound formal, which makes the notation look heavier than it needs to be. It‚Äôs written in a way rather to impress than help readers understand. \n    1. Sets and tuples are mixed: G = (g‚ÇÅ, ‚Ä¶, g‚Çò) is treated as a tuple, but then union subspaces C_gen‚Çñ that are sets of tuples. It‚Äôs better to explicitly say each C_gen‚Çñ ‚äÇ D‚ÇÅ √ó ‚Ä¶ √ó D‚Çò.\n    2. Equations (1) and (2) define  ùê∂_gen·µè and ùê∑_context overengineer the notation for ‚Äúwe vary one context dimension at a time while keeping others fixed.‚Äù Writing two equations for this adds nothing but pseudo-rigor.\n    3. The introduction of both CV(V·µ¢‚à£c) and CV‚Çõ·µ£(V·µ¢) is redundant. CV‚Çõ·µ£(V·µ¢) is merely the averaged coefficient of variation, yet it‚Äôs treated as a distinct ‚ÄúBias Coefficient.‚Äù Readers have to mentally juggle CV, CV‚Çõ·µ£, CCV, all defined within a few lines.\n    4. The authors define ùëá(V·µ¢) = V·µ¢ √ó ùê∂_gen(V·µ¢) = { (v, c) | v ‚àà V·µ¢, c ‚àà ùê∂_gen(V·µ¢) }. This is just a Cartesian product between visual values and context configs. The notation looks formal but contributes no new insight. The same meaning could be stated in English without math symbols. Also, ‚ÄúTask Subspace‚Äù is a misleading name. It‚Äôs not a subspace in any vector-space sense, just a subset.\n    5. The Generalization Context Space is defined as ùê∂_gen(V·µ¢). Later, in Eq. 6, the expectation over ‚Äúcontext‚Äù uses ùê∂_gen(V·µ¢, V‚±º) without prior definition. The reader has to guess that it means the context space for both variables varied together. A single sentence and a simple correlation metric would have communicated this more clearly.\n    6. Within almost a single-page span, the authors define ùê∑_context, ùê∂_gen, ùê∂_gen·µè, ùëá(V·µ¢), ùêµ, ùêµ‚Åª‚Å±, Œº‚Çõ·µ£, CV‚Çõ·µ£, and IEC. All for what amounts to ‚Äúvary one thing, keep others fixed, measure mean and variance.‚Äù\n3. The ‚Äúmulti-stage fairness validation‚Äù is weaker than claimed. It relies on a VLM to flag inconsistent renderings, inheriting the VLM‚Äôs own perceptual biases. The human reviewers only validate a small subset of cases. A more reliable solution would be to implement programmatic visibility checks. Segmentation and depth maps could be used to verify that all target objects are fully visible, unoccluded, and correctly colored.\n4. Although the paper reports 2,127 valid environments, this figure is largely inflated by fine-grained color enumeration within a single underlying manipulation task. The benchmark covers only one type of task, and while camera pose and distance variations contribute some genuine diversity, most of the 141 color variants occupy nearly identical regions in perceptual color space. Moreover, the evaluation does not explore the full Cartesian product of factors: each dimension is varied independently, resulting in many near-duplicate scenes. Figure 2 already bins the 141 colors into 11 groups, which appears far more realistic, as only those broader distinctions meaningfully affect model behavior. Nowhere in the results is there evidence that fine-grained color distinctions, such as between magenta, violet, and orchid, lead to any measurable difference in performance. These hues all effectively map to a ‚Äúpurple-like‚Äù region for the models. With roughly 11 effective colors and correlated camera‚Äìdistance settings, the number of genuinely distinct and valid environments would likely shrink to only a few dozen. In effect, the task instance diversity is much narrower than claimed.\n5. The evaluation includes only three agents, two of which are variants of the same VLM-driven setup and share substantial perception components. Even the supposedly distinct paradigms (VLM-driven vs VLA) depend on similar pretrained visual encoders. As a result, the claim of discovering visual biases across paradigms is not as general as claimed. It‚Äôs unclear whether the observed patterns stem from architecture-level biases or simply from shared visual foundations.\n6. The reported color perception bias is not as ‚Äú*systematic*‚Äù as the authors claim. While Orange and Pink result in some of the lowest performance for SimpleAgent, they yield the highest success rates for œÄ‚ÇÄ.\n7. The proposed SGL is an interesting attempt to reduce ambiguity between language and perception, but it comes across as somewhat artificial. It relies on privileged scene information to detect ambiguities and rewrite instructions, effectively giving the system access to ground-truth object attributes. Even setting that aside, the approach is not very generalizable. It depends on hand-crafted rules and requires manual effort to define relevant attributes and disambiguation logic. As such, it fits neatly into the controlled cube-stacking setup used in this paper but would not extend naturally to other embodied tasks or less modular architectures.\n\n### Minor points\n\n1. The title of Section 5.3 is glued to the text above it, with no spacing left in between\n2. The CV‚Çõ·µ£ metric assumes SR distributions are unimodal and roughly continuous, which is shaky for discrete categories (like colors).\n3. The IEC metric can be very noisy in practice. Compounding variances on variances amplifies randomness. Without many samples, it‚Äôs statistically fragile.\n4. Vertical lines in Table 1\n5. The text in Figures 2 and 4 is difficult to read.\n6. In Figure 3, the dots are connected by lines, which visually suggests a continuous relationship between discrete visual categories (camera poses). Since these factors are categorical, connecting the points makes little sense. They should be plotted as unconnected markers to avoid implying continuity.\n\n### Typos\n\n1. Line 36 channelLiu et al & agentsMa et al\n2. Line 70 enables systematically quantification ‚Üí enables systematic quantification\n3. Line 189 a ‚Äî> an\n4. Line 301 5 times.We\n5. Line 305 configuration. and each ‚Äî> configuration, and each / configuration. Each\n6. line 386 œÄo ‚Äî> œÄ0\n\nI am happy to raise my score if the points above are addressed. However, in its current form, the paper is not yet up to par with the standards of a top-tier venue like ICLR."}, "questions": {"value": "1. Why are the colors reported in Figure 4 different than the ones used in Figure 2?\n2. Instead of only measuring variance, why not quantify directional bias (e.g., which color is favored)? If, say, SimpleAgent and MOKA both prefer red, but œÄ‚ÇÄ prefers blue, their bias directions are not aligned. This could be done, for instance, by computing signed deviations from the overall mean and ranking these differences. Figure 2 already qualitatively shows these trends, but it would be interesting to quantify this.\n3. Why are the bottom half of the colors in Figure 2 in a gray box?\n4. How could the SGL approach be adapted to other settings or methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GjIMGgDPu3", "forum": "Yjlsd2Ueox", "replyto": "Yjlsd2Ueox", "signatures": ["ICLR.cc/2026/Conference/Submission7470/Reviewer_RXBV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7470/Reviewer_RXBV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761703691072, "cdate": 1761703691072, "tmdate": 1762919585770, "mdate": 1762919585770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a benchmark to measure visual biases in visual language models used for robotics. The paper particularly focuses on biases arising due to colors of the objects and camera poses used to capture environmental observations. They first create a combinatorial framework to vary individual components and measure the impact of change in a certain variable on the success rate of a task. For measurement of the bias, the paper uses conditional coefficient of variation and to measure the coupling between two different biases it measures interaction effect. The experimental analysis involves 3 different agents. The main results show that MOKA agent has low camera pose bias, but high color bias with both Qwen-VL and GPT-4o backbones. Further, the authors conduct a case study on the color bias of MOKA agent where they attribute this bias to biases in the underlying visual models. To ameliorate these biases, the paper proposes semantic grounding where they detect ambiguity in the scene description and modify the attributes that would lead to poor performance. Later it is shown that this approach reduces the coefficient of variation in MOKA by 54.5%."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper shows that there is significant bias in visual language models with respect to colors of entities. This is insightful as the models are usually considered powerful enough to be robust against innocuous changes in colors.\n2) The methodology is straightforward and allows to remove various confounding factors in the analysis. I especially like the choice of individual modification the component and performing analysis on the single manipulation task.\n3) The semantic grounding feels promising given its simplicity and reduction in the bias in the case of MOKA agents.\n4) I like the presentation quality in the various plots and tables. In general, the writing quality is also maintained high throughout."}, "weaknesses": {"value": "**Major (my reasons for not providing higher score):**\n\n1. Lack of motivation behind use of particular metrics for bias calculation. Why do you use coefficient of variation? This metric would unnecessarily aggravate the impact of success rate variance in harder tasks. Personally, it would be more desirable to see how performance changes in the tasks where the success rate was already high. This would show that the tasks that were easy enough for the model become very hard if color or camera pose changes. So, an actionable change would be to introduce other metrics that disentangle average performance from the variance in the success. One such metric would be standard deviation. \n2. Similarly, the interaction effect coefficient can be complemented with measurements of the covariance in the success rate as two entities change.\n3. Figure 3:  What to interpret here? This plot can certainly be improved. Maybe classify these poses into certain broad name categories. Then identify the pattern where a pose would lead to failure. Contrast the patterns where different models \n4.  Figure 4:  The font size of the x and y labels here is too small. The caption does not mention how to interpret the figure.\n5. Issue in bias reduction in other models: The proposed bias reduction method does not lead to reduction in 2/3 models. For these agents, there is almost no change in the bias.\n\n**Minor:**\n1. It would be a good idea to describe any special model referred in the paper (e.g. MOKA) before using it to assert claims. \n2. Section 3 is essentially about how to create combinations for analysis. It could have been explained in simpler language."}, "questions": {"value": "1.  Of the different camera poses considered, how many are standard poses in VLM-based robotics policies? Among those typical poses, which ones exhibit significant bias? Are the VLM agents in general able to perform well under these typical poses?\n2. You mention that ‚ÄúA key finding is that all agents have specific viewpoints that lead to complete task failure.‚Äù Can the authors comment on what kind of viewpoints are leading to such task failures?\n3. Out of curiosity, how realistic is it that camera pose would change on a robot? Aren't cameras some of the best guarded equipments whose pose will not change drastically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4MAvEe15Zy", "forum": "Yjlsd2Ueox", "replyto": "Yjlsd2Ueox", "signatures": ["ICLR.cc/2026/Conference/Submission7470/Reviewer_dtBd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7470/Reviewer_dtBd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930939811, "cdate": 1761930939811, "tmdate": 1762919584777, "mdate": 1762919584777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents RoboView-Bias, a benchmark for assessing visual bias in robotic manipulation agents. The benchmark is developed using a structured variant-generation framework, including visual perturbation factors (color, camera euler, camera poses, scale) and task context perturbation factors (initial position, geometric shapes and task instructions). The paper evaluates three agents, SimpleAgent (based on BadRobot), MOKA with two alternative LLMs (GPT4-o and Gwen-VL-Max) and pi zero. Results show that all agents are affected by color, while there are viewpoints that lead to task failure. There are also interaction effects between camera and pose. The authors also explore the idea of a Semantic Grounding Layer, which improves the bias of MOKA, but not other models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper investigates a timely issue of high relevance and importance, and does so using a systematic approach that investigates both the effect of individual parameters (visual and contextual factors), as well as their interaction.\n\n - The \"perceptual fairness\" validation step is interesting and supports the overall rigorous approach followed in the design of the benchmark.\n\n - The paper offers novel findings regarding the biases of the evaluated models, including color bias as well as interactions between color and camera pose."}, "weaknesses": {"value": "- The benchmark concerns a single task and synthetic / simulated data. This raises the question on whether the results and observation also transfer to real-world data and/or other tasks.\n\n- Additional factors would be expected for visual bias assessment, such as texture / patterns / objects and, more generally, complex visual elements that are closer to the real world.  \n\n - The evaluation performed using the benchmark could be more extensive. The number of backbones and model types evaluated is limited. Furthermore, the analysis reports averages of metrics, without reporting metrics of statistical significance. For example for the $\\pi_0$ model, could the observed differences for different colors be due to chance? Which of the pairwise differences are significant?\n\n - The SGL approach is based on heuristics, and does not offer a general-purpose  method for addressing this problem. Moreover, it does not lead to improvements in the Simple and $\\pi_0$ models\n\n  - More generally, it seems that the benchmark is in the right direction of addressing an important issue, but it seems too limited in scope to be useful in practical applications"}, "questions": {"value": "- Why doesn't SGL help with Simple and $\\pi_0$? \n- Do you have ways to disentangle the source of bias (architecture, underlying VLMs)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LKfaSeCsdA", "forum": "Yjlsd2Ueox", "replyto": "Yjlsd2Ueox", "signatures": ["ICLR.cc/2026/Conference/Submission7470/Reviewer_57MG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7470/Reviewer_57MG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933617010, "cdate": 1761933617010, "tmdate": 1762919584300, "mdate": 1762919584300, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RoboView-Bias - a benchmark designed to evaluate visual biases in robotic policies by factorizing various forms of visual biases and doing controlled experiments. The paper identifies camera viewpoint and bias towards high-saturation colors as biases in the methods evaluated."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper finds some important ways in which VLAs are biased. For example, the effect of low-saturation colors and the fact that most methods have some sort of a preferred camera pose are helpful observations. \n2. The two-stage validation process in Perceptual Validation Pipeline is instructive in how evaluation pipelines should be tested and confounders should be handled before being deployed. \n3. The case study of VLM-biases in high-level planning for MOKA is interesting - especially, the emphasis on how success rate is affected by the inconsistency of the labels used for describing the same object, even if they are all somewhat accurate."}, "weaknesses": {"value": "1. The most important weakness of this paper, for me, is that it claims that the study of visual biases on robotic policies has not been done before and that RoboView-Bias is the first benchmark that aims to do so. However, there are quite a few papers that have done it. Gao et. al. introduced the Star-Gen framework that actually evaluates on almost all the metrics discussed in this paper. In particular, the factorized analysis is also presented in Star-Gen. Similar Wang et. al.'s VLATest is another paper to take a look at. \n2. The types of visual bias considered here are quite narrow. There are several other important factors, such as lighting and distractor objects, that should be studied as well. \n3. The choice of agents that are evaluated is not well-justified. There are four instantiations of VLM-driven agents (two for SIMPLE and two for MOKA) whereas only one (i.e. $\\pi_0$) VLA policy being evaluated, even though VLAs, like openVLA, are known to be more reliable than methods like MOKA. It would be helpful to see another VLA being evaluated in order to also validate the observations from line 319 to 323. \n4. The study of SGL needs to be significantly more detailed and ablated. It is not clear to me why the approach does not perform too well with $\\pi_0$ and SimpleAgent. Without this being addressed, it is unclear what the contribution of SGL is to the paper from just observing its effect on MOKA. Similarly, Figure 7 is unclear in terms of what visual perturbation dimension the bias coefficient is corresponding to. Modularizing that would not just be helpful for clarity but also in helping us understand what SGL contributes. Finally, doing an ablation over multiple VLMs (say, between Qwen-VL-Max and GPT-4o) is necessary to understand the general strengths of SGL, because, as of now, it is unclear whether this result in Figure 7 is overfitted to a specific choice of a VLM. \n\nMinor:\n1. Please follow the citation protocol described in the latex template (using \\citep{...} versus \\citet{...}) - there are issues with spacing between citations and the rest of a line (for e.g. the first sentence of the introduction). \n\n[1] Jensen Gao, Suneel Belkhale, Sudeep Dasari, Ashwin Balakrishna, Dhruv Shah, Dorsa Sadigh. A Taxonomy for Evaluating Generalist Robot Policies. \n[2] Zhijie Wang, Zhehua Zhou, Jiayang Song, Yuheng Huang, Zhan Shu, Lei Ma. VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation."}, "questions": {"value": "I am curious about how the two-stage validation pipeline worked in practice. How many iterations of the refinement were needed in stage 1 and stage 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AgArOSJmyr", "forum": "Yjlsd2Ueox", "replyto": "Yjlsd2Ueox", "signatures": ["ICLR.cc/2026/Conference/Submission7470/Reviewer_ZJTK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7470/Reviewer_ZJTK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983136564, "cdate": 1761983136564, "tmdate": 1762919583900, "mdate": 1762919583900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RoboView-Bias, a benchmark systematically quantify visual bias in embodied agents for robotic manipulation. The authors propose a structured variant-generation framework (SVGF) that isolates visual perturbation factors (color, camera viewpoint, scale) from task context factors to enable controlled bias measurement. They evaluate three representative agents across two paradigms (VLM-driven and VLA models) and report significant visual biases, with camera viewpoint being the most critical factor. The paper also proposes a Semantic Grounding Layer (SGL) as a mitigation strategy. The evaluation reveals strong asymmetric coupling between color and viewpoint biases, with viewpoint changes amplifying color-related bias more than vice versa."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides interesting insights in understanding visual bias in embodied manipulation agents.\n- The factor isolation principle through SVGF is well-motivated and enables attributable bias measurement."}, "weaknesses": {"value": "- The benchmark only evaluates a single, simple grasping task where the robot picks up one object. It does not consider other skills such as pushing, sliding, insertion, assembly, and does not deal with more complex objects such as multi object and articulated object manipulation, deformable object handling. In the current simple task, precise object geometry, contact points, and spatial relationships are less critical. More complex tasks requiring fine-grained visual understanding (e.g., peg-in-hole, cable routing, multi-object assembly) may reveal entirely different bias patterns. \n- All experiments are simulation-only, there is no real world evaluation, however, visual bias in simulation may not correlate with real-world failures.\n- The semantic grounding layer works well only for MOKA but has minimal improvements for SimpleAgent and $\\pi_0$. This further reduces the technical contribution of this work."}, "questions": {"value": "- Why does SGL fail for SimpleAgent and œÄ‚ÇÄ? The explanation that tasks are \"too simplistic\" seems contradictory. Can you provide quantitative analysis of when/why SGL works?\n- Is there evidence that reducing bias (as measured by CV) actually improves overall task performance and safety? Could you show that low-bias agents generalize better to new scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q0tjdfdjcM", "forum": "Yjlsd2Ueox", "replyto": "Yjlsd2Ueox", "signatures": ["ICLR.cc/2026/Conference/Submission7470/Reviewer_ARmd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7470/Reviewer_ARmd"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762145327425, "cdate": 1762145327425, "tmdate": 1762919583575, "mdate": 1762919583575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}