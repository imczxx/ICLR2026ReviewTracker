{"id": "3RpaTizIdH", "number": 20789, "cdate": 1758310177039, "mdate": 1759896958620, "content": {"title": "Aligning Video Models with Human Social Judgments via Behavior-Guided Fine-Tuning", "abstract": "Humans intuitively perceive complex social signals in visual scenes, yet it remains unclear whether state-of-the-art AI models encode the same similarity structure. We study (Q1) whether modern video and language models capture human-perceived similarity in social videos, and (Q2) how to instill this structure into models using human behavioral data. To address this, we introduce a new benchmark of over 49,000 odd-one-out similarity judgments on 250 three-second video clips of social interactions, and discover a modality gap: despite the task being visual, caption-based language embeddings align better with human similarity than any pretrained video model. We close this gap by fine-tuning a TimeSformer video model on these human judgments with our novel hybrid triplet–RSA objective using low-rank adaptation (LoRA), aligning pairwise distances to human similarity. This fine-tuning protocol yields significantly improved alignment with human perceptions on held-out videos in terms of both explained variance and odd-one-out triplet accuracy. Variance partitioning shows that the fine-tuned video model increases shared variance with language embeddings and explains additional unique variance not captured by the language model. Finally, we test transfer via linear probes and find that human-similarity fine-tuning strengthens the encoding of social-affective attributes (intimacy, valence, dominance, communication) relative to the pretrained baseline. Overall, our findings highlight a gap in pretrained video models’ social recognition and demonstrate that behavior-guided fine-tuning shapes video representations toward human social perception.", "tldr": "We introduce a dataset of 49k human video similarity judgments and use a novel hybrid triplet-RSA objective to fine-tune a TimeSformer, boosting alignment with human social perception.", "keywords": ["representation learning", "video understanding", "social perception", "human alignment", "similarity judgments", "odd‑one‑out", "triplet loss", "representational similarity analysis (RSA)", "multimodal", "variance partitioning", "linear probes", "TimeSformer", "datasets and benchmarks"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ead5920af8eb89c39788aa0b993be0aca3dc2b7b.pdf", "supplementary_material": "/attachment/aac0a88a4047e31bd4c9e7601e6668df797f4670.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a compelling investigation into the alignment of video model representations with human social perception. The authors identify a significant and counter-intuitive \"modality gap\": embeddings from language models applied to video captions are better predictors of human similarity judgments for social videos than embeddings from purpose-built, pretrained video models. To address this misalignment, the work introduces a novel \"behavior-guided fine-tuning\" methodology. The core of this approach is an innovative hybrid loss function that combines local, triplet-based constraints with a global alignment objective based on Representational Similarity Analysis (RSA), a technique repurposed from an analysis tool into a differentiable training signal."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is exceptionally well-motivated. It tackles the timely and critical problem of aligning AI systems with human perception, which is essential for developing more intuitive, interpretable, and trustworthy AI, particularly as these systems are deployed in human-centric environments. The central finding that motivates the work—the \"modality gap\"—is both surprising and insightful.\n\nThe creation and planned release of the social video similarity judgment dataset is a substantial contribution in its own right. Collecting large-scale human behavioral data is a difficult and resource-intensive endeavor, and such datasets are invaluable for the research community. The dataset, with its ~49,000 odd-one-out judgments, provides a rich and dense source of relational supervision that captures the latent structure of human perception far more effectively than discrete categorical labels.\n\nThe core technical contribution—the hybrid loss function—is both novel and well-justified. While triplet loss is a well-established method for metric learning, its combination with a differentiable RSA objective is a significant innovation.\n\nThe empirical validation of the proposed method is thorough, convincing, and multifaceted. The authors employ a well-designed set of experiments that provide strong support for their claims.\n\nThe paper provides nuanced analyses that deepen the reader's understanding. The variance partitioning analysis in Figure 3 is a prime example. It clearly visualizes how fine-tuning not only increases the shared variance between the video and language models but also significantly boosts the unique variance explained by the video model. \n\nThe use of Low-Rank Adaptation (LoRA) for fine-tuning is a pragmatic and well-justified choice. By updating less than 2% of the model's parameters, the authors make their approach computationally efficient, scalable, and accessible. This aligns with the broader trend in the field toward parameter-efficient fine-tuning (PEFT) as a sustainable way to adapt large foundation models, making the proposed method highly practical for other researchers to adopt and build upon."}, "weaknesses": {"value": "(W1) The primary concern is the generalizability of the findings, stemming from the limited size of the video stimulus set. While the number of human judgments is large, these judgments are all made on a set of only 250 video clips, which are themselves sourced from a subset of the Moments in Time dataset. This raises the possibility that the model has learned a similarity function that is highly specific to the content, style, and inherent biases of this particular video corpus. The paper acknowledges this limitation but could more deeply consider its implications.   \n\n(W2) Furthermore, the paper does not discuss the cultural or demographic properties of either the human annotators or the individuals depicted in the videos. Social perception is known to be culturally situated; what constitutes a \"similar\" interaction can vary significantly across different cultural contexts. A model aligned with the judgments of one specific annotator population may fail to align with—or even perform poorly for—others. This is a critical consideration under the ICLR Code of Ethics principle to \"Be Fair and Take Action not to Discriminate,\" as there is a risk of encoding a culturally specific viewpoint as a universal ground truth.\n\n(W3) The paper uses TimeSformer, an influential transformer-based video model from 2021, as its experimental backbone. While a reasonable choice, the field of video representation learning has seen significant advances since its publication, particularly in the area of self-supervised learning. The authors themselves note the promise of modern architectures like V-JEPA. The magnitude of the initial \"modality gap\" could be partially an artifact of the specific limitations of the chosen baseline. It is plausible that a more recent self-supervised model, trained to learn richer and more generalizable representations of the world, might exhibit a smaller initial gap with human judgments. Without testing the fine-tuning method on a more contemporary backbone, it is difficult to know if the reported gains are specific to improving an older architecture or if they represent a more fundamental benefit that would also apply to state-of-the-art models.\n\n(W4) The implementation details of the hybrid loss function could be specified more clearly. The paper states that the weighting parameter $\\alpha$ is fixed at 0.7, while $\\beta$ is linearly ramped from 0.3 to 0.7 over the course of training.1 However, the rationale for these specific values and the ramping schedule is not provided. It is unclear whether these are the result of a systematic hyperparameter search or an ad-hoc choice.\n\n(W5) The strength of the language model baseline is a cornerstone of the paper's motivation, yet its validity hinges on the quality of the video captions. The paper describes them as \"brief descriptive captions\" but provides no examples or details on their generation process. If the captions are highly abstract and semantically rich (e.g., \"two friends playfully competing\" vs. \"two people boxing\"), they might contain information that is not easily inferred from the visuals alone, giving the language model a significant advantage. Providing a few examples of the captions would be crucial for readers to properly interpret the modality gap and contextualize the performance of all models."}, "questions": {"value": "1. Regarding the Stimulus Set: Could you provide more detail on the diversity of the 250 video clips, perhaps with a breakdown of the types of social interactions represented? Furthermore, could you comment on the demographic background of both the human annotators and the individuals in the videos, and discuss how this might shape the \"ground truth\" human similarity structure you are aligning to?\n\n2. Regarding the Choice of Backbone: The selection of TimeSformer is understandable. However, how do you anticipate the initial \"modality gap\" and the relative gains from your fine-tuning method would change if applied to a more recent, self-supervised video model like V-JEPA or VideoMAE?\n\n3. Regarding the Hybrid Loss Formulation: Could you elaborate on the process for selecting the loss weights ($\\alpha=0.7$) and the ramping schedule for $\\beta$? Were these values determined through a hyperparameter search, and how sensitive is the model's final performance to these specific choices?\n\n4. Regarding the Language Baseline: To help the reader better interpret the \"modality gap,\" would it be possible to provide a few representative examples of the video captions that were used to generate the language model embeddings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n5nbIf4Rks", "forum": "3RpaTizIdH", "replyto": "3RpaTizIdH", "signatures": ["ICLR.cc/2026/Conference/Submission20789/Reviewer_TRb2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20789/Reviewer_TRb2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760731108224, "cdate": 1760731108224, "tmdate": 1762935029745, "mdate": 1762935029745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel benchmark of human similarity judgments on short social video clips and a behavior-guided fine-tuning method that aligns video model representations with human social perception. The authors introduce a hybrid loss combining triplet and RSA objectives and evaluate its effectivenes. Results show improved alignment between model and human similarity judgments as well as enhanced representation of social-affective attributes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Interesting Motivation: The idea of aligning model representations with human social perception is an important and timely topic, especially as video-based AI systems become increasingly prevalent in social and interactive contexts.\n2. New Dataset Contribution: The benchmark of 49k human odd-one-out judgments on social videos provides valuable behavioral data that could support future studies on human-model alignment.\n3. Novel Fine-Tuning Approach: The hybrid triplet–RSA loss is methodologically sound and creatively combines local and global representational alignment.\n4. Clear Presentation: The paper is clearly written, provides reproducibility details, and the methods are well-documented."}, "weaknesses": {"value": "1. Cultural Diversity Ignored – The work does not address cross-cultural or contextual differences in social judgment. Since human similarity judgments are deeply culture-dependent, the benchmark risks encoding culturally specific biases. This limitation should be discussed in more depth and ideally evaluated empirically.\n2. Weak Baselines and Model Choices –\n    * The use of TimeSformer pretrained on Kinetics-400 as the main video backbone is a significant limitation. Kinetics-400 is small by current standards and biased toward action recognition, not social understanding.\n    * Given the state of the field, stronger and more modern baselines (e.g., Qwen3-VL, InternVL-3.5, SigLIP-v2, DINOv3, CLIP) should be included. Many of these outperform older video models and are standard in current video vision-language research.\n    * It is also well known that video encoders underperform image encoders in general visual understanding due to differences in training data and supervision. Testing the best image-based encoders with frame averaging would be an important baseline.\n3. Limited Model Generalization – The authors draw conclusions based on experiments using a model (TimeSformer on Kinetics-400) that is not representative of current top-performing video architectures. Thus, the results and claims about “closing the modality gap” are not convincing in the broader context of modern video-LLMs.\n4. Lack of Qualitative Examples – The paper provides no qualitative examples or visualizations of the dataset or model outputs. This omission weakens the reader’s understanding of what constitutes “social similarity” and how the model captures it.\n5. Section Naming and Style – Minor: Section 3 should be titled “Method” instead of “Methods” for consistency with standard paper structure.\n6. Misaligned with Field Progress – The motivation, while conceptually interesting, does not sufficiently engage with the current landscape of multimodal vision-language research, which already investigates human alignment via large-scale preference learning, contrastive learning, and cross-modal supervision. The contribution therefore feels incremental relative to recent progress."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0ySnwC2hSa", "forum": "3RpaTizIdH", "replyto": "3RpaTizIdH", "signatures": ["ICLR.cc/2026/Conference/Submission20789/Reviewer_WHgJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20789/Reviewer_WHgJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603547216, "cdate": 1761603547216, "tmdate": 1762934998233, "mdate": 1762934998233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduced a new dataset of approximately 49,000 human \"odd-one-out\" similarity judgments on 250 short social video clips. Using the similarity dataset, the authors demonstrated that embeddings from caption-based language models align better with human visual judgments than pretrained video models. Furthermore, the authors fine-tuned a TimeSformer video model with a novel hybrid triplet-RSA loss. This new model achieves superior human alignment and demonstrates an improved ability to encode social-affective attributes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- this is a valuable and timely investigation into the alignment of video model representations with human social perception, especially on action understanding / social judgment tasks\n- the identified modality gap is interesting.  It clearly motivates the need for new approaches to align video models with human-like representations\n- the behavior-guided fine-tuning improves the encoding of social-affective attributes, which is a surprising result"}, "weaknesses": {"value": "While the results seem interesting, I have a few concerns:\n- Depending on the nature of video captions, the modality gap of similarity alignment may or may not be that interesting. Generally, captions capture information at a higher semantic level than visual features, which may be why language models align well with human judgment. For example, language embeddings for captions like “basketball” and “tennis” might be reasonably close since they are both ball sports, but the visual scene for these two sports are vastly different. So it is almost expected that the language embedding performs closer to human behavior, as the vision task could be more difficult due to the much more noisy information.\n- While the authors evaluated an extensive list of models , they are generally pre-trained on smaller datasets e.g., Kinetics-400. There lacks an evaluation of large-scale foundational vision models like CLIP and DINO.\n- While not a decisive weakness as it's not really within the scope of this paper, the action recognition accuracy did not improve after behavior-guided fine-tuning . This could imply limited generalizability of the fine-tuned model to other, broader visual tasks."}, "questions": {"value": "- Could the authors provide qualitative examples on dataset triplets? In particular, it would be illustrative to see triplets where language embeddings are aligned with human judgments but the video embeddings are not.\n- Could the authors evaluate larger-scale foundational vision models like CLIP and DINO? A CLIP-style model would be particularly interesting since it explicitly enforces vision-language alignment. In these models, is there still the \"modality gap\" on similarity alignment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3lSapIvcho", "forum": "3RpaTizIdH", "replyto": "3RpaTizIdH", "signatures": ["ICLR.cc/2026/Conference/Submission20789/Reviewer_BVfZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20789/Reviewer_BVfZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949030109, "cdate": 1761949030109, "tmdate": 1762934938914, "mdate": 1762934938914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}