{"id": "YUOxux1lIu", "number": 6185, "cdate": 1757957038684, "mdate": 1763716138650, "content": {"title": "PointARSim: Point-cloud–enhanced Generative Auto-Regressive Simulation for Closed-loop End-to-end Autonomous Driving Evaluation", "abstract": "End-to-end autonomous driving (E2E-AD) has garnered increasing attention in recent years. However, exist evaluation fails to simultaneously meet the requirements for both interactivity (e.g., CARLA) and realism (e.g., nuScenes). In this work, we introduce \\textbf{\\emph{PointARSim}}, an autoregressive generative simulation framework that leverages point cloud skeleton representation accumulated from offline datasets as conditions for diffusion models to synthesize multi-view images. Specifically, we separately aggregate foreground-background point cloud, facilitating utilization of visual geometry. To enable accurate control of the generated driving scene, we design a multi-conditioning ControlNet that integrates point cloud priors, previous-frame condition and optional backgrounds injection, \n guided by a geometry-aware cycle loss. \n Furthermore, to assess generative models’ fidelity when the ego-vehicle’s trajectory differs greatly from recorded offline logs, we introduce a new evaluation protocol based on the nuPlan dataset, called \\textbf{\\emph{nuPlan-SimGenEval}} benchmark.\n Extensive experiments on both nuScenes and nuPlan-SimGenEval demonstrate that the proposed approach significantly outperforms existing methods, highlighting its potential for next-generation closed-loop autonomous driving simulation.", "tldr": "", "keywords": ["generative simulator", "autonomous driving"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/631d3c37ceed87bd186595bf62b02d4fc49857a0.pdf", "supplementary_material": "/attachment/16b764951067f429b86bddd673ad7240e569ccca.zip"}, "replies": [{"content": {"summary": {"value": "This paper focuses on camera view simulation in driving scenes using generative models. Building upon prior work FreeVS, the contributions of this work include: 1. Introducing additional control conditions beyond Color Map (such as depth map and noisy previous latent to compensate for the limitations of Color Map; 2. Proposing augmentation and supervision like Color Actor Vibration, degenerating previous latents, and Geometry-Aware Cycle Loss; 3. Proposing the use of segmentation performance under novel trajectory (Interactive Divergence) track to evaluate the effectiveness of view simulation. This work extends the FreeVS pipeline based on WOD to nuscenes/nuplan. Experiments demonstrate that the proposed method achieves high image quality (FID) and contour fidelity (Seg IoU)."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The analysis of FreeVS's limitations is well-reasoned. FreeVS's heavy reliance on color maps leads to poor robustness when dealing with partial or sparse point clouds, and it fails to model content beyond LiDAR coverage (e.g., elevated or distant areas). This shortcoming is likely more obvious on the nuScenes/nuPlan datasets, where point clouds are generally sparser. Additionally, FreeVS does not incorporate previous frame outputs as conditions, which limits its temporal consistency.\n\nTo address these issues, this work proposes several well-founded designs: introducing a depth map condition, using a clustered actor points template to complete object contours in the depth map, and adding previous frame latent conditions. The work also extends the framework to support multi-view synthesis.\n\nThe proposed segmentation performance metric can better evaluate the geometric fidelity of view results."}, "weaknesses": {"value": "My concerns primarily focus on two points:\n\n1. I think the design of Color Actor Vibration is not well-founded. The idea of augmenting (degenerating) the color map to force the model to rely on it for texture and on the depth map for geometry/contours is reasonable. However, augmenting by shifting object locations seems unjustified. This not only compromises the geometric reliability of the color map (core to FreeVS) but also significantly harms the generative model's precise control over object appearance and location.\nIn Fig. 5, the color of the generated vehicle (top-left) is not controlled by the color map.\nIn Fig. 8/16, the positions of the generated vehicles also show clear misalignment with the color map.\nIf the limitation of the color map stems from partial/sparse point clouds, why did the authors choose to augment by moving object locations (which does not match any real-world degradation) instead of applying point cloud downsampling? This choice is puzzling; its negative impacts seem cannot be fully counteracted even with an extra Geometry-Aware Cycle Loss.\n\n2. Why is the method implemented based on SD rather than a video generation model? This design choice seems to result in notably poor cross-frame consistency in the outputs."}, "questions": {"value": "Check the weakness. I believe an ablation study on the effect of Color Actor Vibration (+ a comparison against point cloud downsampling) is necessary. Metrics like PSNR/SSIM on the original trajectory should be used to validate the controllability of the generated content for this abl exp. I will raise my rating if this concern is addressed.\n\nTypo: \"Noisy* Previous Latent\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NeuwYYP5Yb", "forum": "YUOxux1lIu", "replyto": "YUOxux1lIu", "signatures": ["ICLR.cc/2026/Conference/Submission6185/Reviewer_zTZt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6185/Reviewer_zTZt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760608037512, "cdate": 1760608037512, "tmdate": 1762918526925, "mdate": 1762918526925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PointARSim, an autoregressive generative simulation framework for closed-loop end-to-end autonomous driving evaluation, which leverages point cloud skeleton representation (decoupling foreground-background) and a multi-conditioning ControlNet. It also proposes the nuPlan-SimGenEval benchmark with Log-Replay and Interactive Divergence tracks to assess generative fidelity when the ego-vehicle’s trajectory deviates from offline logs. Experiments on nuScenes and nuPlan show PointARSim outperforms existing methods, achieving improvement in perception under Interactive Divergence."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Employs a point cloud skeleton system that decouples and accumulates foreground-background point clouds (with color-depth dual streams) to provide geometric guidance for generative simulation, addressing the issue of incomplete or inconsistent visual cues when the ego-vehicle deviates from recorded trajectories.\n2. Designs a multi-module generator optimization (Color Actor Vibration, Noise Previous Latent, Geometry-Aware Cycle Loss) to balance conflicting conditioning signals (point cloud priors, previous frames) and mitigate autoregressive error accumulation.\n3. Proposes the nuPlan-SimGenEval benchmark with an Interactive Divergence track, specifically evaluating generative fidelity under drastically divergent ego-vehicle trajectories (lacking ground-truth references) and using point cloud skeleton-derived segmentation labels for fair perception assessment."}, "weaknesses": {"value": "1. Fails to address near-field scene distortion: When vehicles are extremely close to the ego-car, point cloud/depth map incompleteness and projection errors lead to distorted generated actors, with no proposed solutions to fix this inherent limitation of its point cloud skeleton-based design.\n2. Limited dynamic interaction granularity: It only ensures basic ID consistency of surrounding agents via point cloud tracking but lacks fine-grained control over dynamic behaviors (e.g., natural acceleration/deceleration of vehicles, pedestrian gait coherence), failing to solve complex dynamic interaction challenges.\n3. Narrow generalization of the nuPlan-SimGenEval benchmark: The benchmark only covers lane deviation as an extreme scenario, excluding more complex cases (e.g., sudden pedestrian crossing, multi-vehicle parallel lane changes), and is heavily dependent on nuPlan’s dataset characteristics, making migration to other datasets difficult.\n4. Unclear key implementation details: Critical parameters (e.g., voxel downsampling size selection basis, noise injection variance in Noise Previous Latent, background injection’s N value) and optimizer configurations (e.g., weight decay coefficient) are unspecified, hindering result reproducibility."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3dpl39IFwj", "forum": "YUOxux1lIu", "replyto": "YUOxux1lIu", "signatures": ["ICLR.cc/2026/Conference/Submission6185/Reviewer_nbji"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6185/Reviewer_nbji"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761365825070, "cdate": 1761365825070, "tmdate": 1762918526378, "mdate": 1762918526378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a autoregressive generation framework for autonomous driving multi-view images and sensor data draws from previous frame sensor data and noised images to inform next frame generation. PointARSim utilizes a point cloud skeleton, accumulated from offline datasets, to condition diffusion models for synthesizing multi-view images, separating foreground and background for visual geometry. The method allows for interactive editing of previously static data from nuScenes and nuPlan while preserving realism. The paper also introduces the nuPlan-SimGenEval benchmark, which evaluates simulator generation fidelity when trajectories diverge significantly from the GT ego trajectories."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The submission targets a valid problem in Autonomous Driving research: the need for realistic closed loop simulators. The proposed system augments nuScenes and nuPlan with divergent track data which adheres to the original data's color maps\n- The work demonstrates notable improvements in FID and FVD over previous generative works based off nuScenes\n- The paper enables better divergent track generation based solely off a ControlNet architecture without the need for additional abstractions like Gaussian Splats or a NeRF representation."}, "weaknesses": {"value": "W1. The submission lacks novelty. While the paper identifies a key problem with FreeVS being it's inability to have large trajectory deviations from the original data, foreground/background segmentation for point clouds using 3D bounding boxes is not new. Nor is noising some inputs to ControlNets in order to avoid conditioning too heavily on previous frames.\n\nW2. While the Ablation Study for FID, FVD, and Seg in Table 4 is appreciated, the paper doesn't speculate or explain why even just the LO+Prev row achieves significantly better FID than related works. An FID of 7.09 on nuScenes might even indicate too much adherence to the original nuScenes data distribution. So while the method allows for more coherent divergent track generation, the actual diversity of the generated data is poor.\n\nW3. The Interactive Divergence Track is custom made to evaluate this specific task, data where tracks diverge heavily from original data. However, it's not clear that vehicle segmentation alone along with FID/FVD alone is sufficient to argue an improvement over previous methods in terms of generalizability. The supplemental videos themselves show evidence of agents changing color between frames, despite the emphasis on the color map perturbations.\n\nW4. Various grammatical errors throughout the error which affect the readability beyond the extent that simple typos would. (Lines 54-55, 106, 188-189, 298-300, etc.). There are also less problematic typos on lines 319, 379, 432, etc."}, "questions": {"value": "Additional questions:\n\nQ1. Building on W2, can the authors provide some insight into why they observed an improvement in FID with even a small portion of their methods? The overly low FID may be an indication of overfitting.\n\nQ2. ControlNets can be conditioned on text prompts to edit things like weather and the style of the images explicitly. Did the authors experiment with augmenting nuScenes/nuPlan data to achieve visual diversity in addition to ego-track diversity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D6QM72hFUU", "forum": "YUOxux1lIu", "replyto": "YUOxux1lIu", "signatures": ["ICLR.cc/2026/Conference/Submission6185/Reviewer_HR55"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6185/Reviewer_HR55"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761696543345, "cdate": 1761696543345, "tmdate": 1762918525657, "mdate": 1762918525657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PointARSim, an autoregressive generative simulation framework designed for closed-loop evaluation of end-to-end autonomous driving systems. The key idea is to leverage a point cloud skeleton—constructed by accumulating and decoupling foreground (dynamic agents) and background (static scene) point clouds from offline datasets—to guide a diffusion-based image generator. The framework supports interactive simulation by updating the point cloud skeleton based on the ego vehicle's actions and using it to synthesize multi-view images from arbitrary viewpoints."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The point cloud skeleton effectively bridges geometric and visual domains, enabling robust novel-view synthesis and dynamic agent control.\n-The Interactive Divergence track in nuPlan-SimGenEval addresses a critical gap in existing benchmarks by testing generative models under significant trajectory deviations."}, "weaknesses": {"value": "- The method struggles when vehicles are extremely close to the ego car, leading to distorted or incomplete actor generation due to projection limitations.\n- Although some out-of-distribution examples are shown, systematic evaluation across diverse weather, lighting, or geographic conditions is lacking."}, "questions": {"value": "- When the point cloud prior for a novel viewpoint is insufficient, could the generated data be adversely affected?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qnM3i6OGtd", "forum": "YUOxux1lIu", "replyto": "YUOxux1lIu", "signatures": ["ICLR.cc/2026/Conference/Submission6185/Reviewer_r6ZQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6185/Reviewer_r6ZQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005987510, "cdate": 1762005987510, "tmdate": 1762918524511, "mdate": 1762918524511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}