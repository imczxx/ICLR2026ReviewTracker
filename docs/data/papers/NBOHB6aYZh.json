{"id": "NBOHB6aYZh", "number": 1308, "cdate": 1756869719637, "mdate": 1759898216158, "content": {"title": "Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools", "abstract": "Multimodal large language models (MLLMs) have demonstrated remarkable potential in bridging visual and textual reasoning, yet their reliance on text-centric priors often limits their ability to disentangle semantically similar actions in open-vocabulary scenarios. To address this, we propose Video-STAR, a framework that harmonizes contextual sub-motion decomposition with tool-augmented reinforcement learning for open-vocabulary action recognition (OVAR). Unlike prior methods that treat actions as monolithic entities, our approach innovatively decomposes actions into discriminative sub-motions for fine-grained matching while dynamically invokes domain-specific tools for cross-modal interleaving, thereby enabling category-specific reasoning capacity and reducing cross-modal hallucination. Moreover, by designing a hierarchical reward that balances tool-usage efficiency, sub-motion relevance, and structural coherence in reasoning, our method autonomously leverages external tools to prioritize sub-motion patterns without explicit supervision, transmitting from text-centric reasoning to visually grounded inference. Extensive evaluations on HMDB-51, UCF-101, SSv2, Kinetics-400, and Kinetics-600 datasets demonstrate our state-of-the-art performance, outperforming existing methods in distinguishing fine-grained actions and handling cross-modal hallucination, while maintaining computational efficiency.", "tldr": "", "keywords": ["multimodal reasoning", "vision-language model", "action recognition"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ddf01d6f387b3e442203aa4f378488ce9b65755d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Video-STAR, a framework for open-vocabulary action recognition that integrates contextual sub-motion decomposition with tool-augmented reinforcement learning on top of MLLMs. Actions are no longer treated as atomic labels; instead, they are decomposed into discriminative motion primitives, while external tools are invoked to reduce cross-modal hallucinations and enable category-specific reasoning. A hierarchical reward is designed to jointly optimize tool-usage efficiency, structural coherence, and sub-motion relevance. Extensive experiments demonstrate substantial gains over CLIP-based baselines and vanilla MLLMs across multiple settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The combination of sub-motion decomposition and tool-augmented RL constitutes a meaningful departure from static cross-modal alignment pipelines commonly used in OVAR.\n\nThe method delivers consistent and large improvements across diverse benchmarks and evaluation settings."}, "weaknesses": {"value": "1. While the integration is well-designed, the core building blocks (tool-augmented CoT, RL-based post-training, and sub-action decomposition) are all known paradigms; the novelty is primarily at the system-level composition rather than at the level of a fundamentally new principle.\n\n2. The approach depends on specific external tools, yet the paper does not analyze robustness to tool inaccuracies or the portability of the method under alternative tool choices.\n\n3. The computational overhead of repeated tool invocation and multi-round RL inference is not reported nor compared against CLIP-based or purely SFT-based OVAR pipelines. Furthermore, the training cost of GRPO fine-tuning is also not quantified or compared with prior methods.\n\n4. Although standard CLIP-based OVAR methods are included, the paper does not benchmark against recent LLM-augmented CLIP paradigms that explicitly incorporate generative priors for action understanding, such as [1–3].\n\n5. The evaluation is conducted primarily against mid-scale or earlier-generation MLLMs (e.g., Qwen2.5-VL), without comparison to state-of-the-art frontier models (e.g., Qwen3-VL, GPT-5, Claude, Gemini), many of which already demonstrate strong video reasoning capabilities.\n\n[1] Building a Multi-modal Spatiotemporal Expert for Zero-shot Action Recognition with CLIP\n\n[2] Generating Action-Conditioned Prompts for Open-Vocabulary Video Action Recognition\n\n[3] VTD-CLIP: Video-to-Text Discretization via Prompting CLIP"}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ywC3L02ey2", "forum": "NBOHB6aYZh", "replyto": "NBOHB6aYZh", "signatures": ["ICLR.cc/2026/Conference/Submission1308/Reviewer_zTP9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1308/Reviewer_zTP9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726438004, "cdate": 1761726438004, "tmdate": 1762915731333, "mdate": 1762915731333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes, implements and investigates a novel strategy and framework, i.e., Tools Augmented CoT reasoning for zero-shot fine-grained human action recognition. The framework employs VFMs, VLMs and Visual RAG to extract sub-concept vision representations such as human, pose, and video explanation, and generated a combined prompt on proposed format for CoT to MLLM for final stage prediction. The core innovations are the format definition and implementation of sub-action decomposition, candidate selection, and matching scoring for CoT and reinforcement learning. Concrete evaluations are performed on five formal benchmarks and the results shown significant improvements over the SOTA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "A novel Tool Augmented CoT framework and implementation approach for open-vocabulary action recognition (OVAR), concrete evaluations of 5 benchmarks and new SOTA performance."}, "weaknesses": {"value": "There are still some uncertain issues. (1) the experiments on only one base MLLM model, Qwen2.5-VL, are reported, where the training prompts of reasoning chain for CoT are generated by Qwen2.5-VL-72B, and then fine-tune the small-size model Qwen2.5-VL-3B and Qwen2.5-VL-7B for experiments, is teacher-student knowledge distillation on the proposed reasoning chain format able to achieve similar effectiveness? May be better to add more results on other leading frontier MLLMs such as InternVL2.5, Gemini-2.5-Pro, Llama, etc. (2) is it applicable to professional actions such as FineGym, Diving? Where expertise sub-action concepts might not well be learned for AGI models. (3) On lines 356-360, are the protocols defined for previous benchmarks? Please cited them. If novel classes Y_N are completed unknown in MLLM, how to generate the novel nouns of the new classes? So that the base VLM and MLLM have been trained on related concepts, and may be not strictly zero-shot performance. (4) As the training sub-concepts and reasoning chains are generated by VFM, VLM, and MLLM, are there hallucinated chains which lead to final correct answers on ground truth? Maybe the discussion on the Visual Grounded CoT is helpful."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zgdfYfdvgU", "forum": "NBOHB6aYZh", "replyto": "NBOHB6aYZh", "signatures": ["ICLR.cc/2026/Conference/Submission1308/Reviewer_vdeG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1308/Reviewer_vdeG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895564795, "cdate": 1761895564795, "tmdate": 1762915731076, "mdate": 1762915731076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper discusses this idea: don’t let the model guess in one shot. Make it first break the action into sub-motions (arms, torso, legs, contact), then match those to candidate actions, and finally score them — and let the model call tools (YOLO human detection, pose estimation, Qwen-based action/video explanation) when it thinks vision cues are not enough."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper names two real OVAR pain points, cross-modal hallucination and similar-action confusion, and every design choice (tools, sub-motions, hierarchical reward) points back to those, making a coherent motivation.\n2. Treating “shoot ball” as “bend → jump → arm extend → release” matches how actions are actually separable in video; it’s more plausible than pure text-CoT on top of global video tokens.\n3. Tools aren’t a fixed pipeline — the model decides whether to call pose / human / RAG /video description, and the reward penalizes useless tool calls. That’s better than many “agentic VLM” papers that just always run pose."}, "weaknesses": {"value": "1. They say open-vocab, but the system leans on online RAG / Qwen API to pull category-specific definitions at inference. That narrows the search space. It’s closer to “recognition with external label dictionary + video grounding” than to “truly open” recognition.\n2. YOLO 11 for human + pose, Qwen API for explanation / video description — that’s a very specific tool stack.\n3. First round: “which tool(s)?” Second round: “do sub-motion reasoning.” Plus GRPO sampling 4–6 responses. That might not be cheap for real-time video, and the paper doesn’t talk about latency / streaming."}, "questions": {"value": "The questions are the same as weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K1SyTDadzU", "forum": "NBOHB6aYZh", "replyto": "NBOHB6aYZh", "signatures": ["ICLR.cc/2026/Conference/Submission1308/Reviewer_Hc9X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1308/Reviewer_Hc9X"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986570709, "cdate": 1761986570709, "tmdate": 1762915730897, "mdate": 1762915730897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses two fundamental limitations of current multimodal large language models (MLLMs) for open-vocabulary action recognition:\n(1) the over-reliance on textual priors that neglect domain-specific visual cues, and\n(2) the inability to distinguish semantically ambiguous actions in open-vocabulary settings.\nThese issues are indeed of great importance and widely exist across multiple MLLM-based video understanding tasks. The proposed Video-STAR framework attempts to mitigate these problems by constructing multimodal Chain-of-Thought (CoT) data and introducing tool-augmented reasoning with reinforcement learning optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper is motivated by a well-defined and practically significant problem—bridging the gap between text-centric reasoning and visually grounded inference.\n\n2.The use of a multimodal CoT to train MLLMs for structured, tool-guided reasoning is a valuable direction.\n\n3.The framework integrates detection, pose estimation, and semantic reasoning tools with a hierarchical reward design, which is novel and effectively demonstrated across multiple benchmarks."}, "weaknesses": {"value": "Major Comments\n\n1.During data construction, the model is trained using decomposed motion sequences (sub-actions).\nHow does the framework behave when encountering previously unseen actions in the open-vocabulary test set?\nIs there any mechanism ensuring compositional generalization beyond the motion patterns observed during training?\nWithout such a mechanism, the model might overfit to the seen sub-motion combinations.\n\n\n2.How exactly is each action decomposed into sub-actions?\nIs the decomposition manually annotated, automatically generated, or derived from an existing motion ontology?\nMoreover, how do the authors ensure that the set of sub-actions can comprehensively cover unseen action categories during testing?\nSince this decomposition is central to the model’s reasoning ability, more transparency on this process is necessary for reproducibility and understanding its generalization scope.\n\n3.Video-STAR performs task-specific supervised fine-tuning (SFT) and reinforcement learning (RL), while most baselines (e.g., Qwen2.5-VL,) are evaluated without any fine-tuning.\nThis introduces a fairness issue: the superior performance of Video-STAR may partly result from extra supervision rather than the proposed method itself.\nA fairer comparison would include a fine-tuned Qwen2.5 baseline trained on the same dataset but without tool usage and sub-motion decomposition, to isolate the true contribution of the proposed framework.\n\n4.The ablation studies only compare the presence vs. absence of tool usage but do not analyze which tool or combination contributes most.\nHow is the tool selected in practice?\nWhat is the performance when all tools are used simultaneously (pose, detection, action explanation, and video description)?\nA more detailed comparison of tool selection strategies would clarify whether the proposed policy is optimal or if simpler combinations yield similar gains.\n\n5.Since Video-STAR relies on multiple external tools, the inference pipeline likely introduces additional computational overhead.\nThe paper should report both the overall inference latency and the module-wise cost (e.g., tool invocation vs. model reasoning).\nComparing the efficiency of Video-STAR with standard MLLMs would help quantify the trade-off between accuracy improvement and computational expense.\n\n6. Line 213: The symbol T_r appears for the first time without explicit definition. Please clarify its meaning and source."}, "questions": {"value": "1. The paper claims to mitigate “text-centric reasoning,” but the explanation of how this is achieved is somewhat abstract.\nPlease explicitly describe the mechanism by which visual grounding is enforced during training and inference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WgXVKdwH9k", "forum": "NBOHB6aYZh", "replyto": "NBOHB6aYZh", "signatures": ["ICLR.cc/2026/Conference/Submission1308/Reviewer_mh4G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1308/Reviewer_mh4G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992551918, "cdate": 1761992551918, "tmdate": 1762915730595, "mdate": 1762915730595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}