{"id": "rTte3iUsXV", "number": 20397, "cdate": 1758305493499, "mdate": 1759896979578, "content": {"title": "Displacement-Resistant Extensions of DPO with Nonconvex $f$-Divergences", "abstract": "DPO and related algorithms align language models by directly optimizing the RLHF objective: find a policy that maximizes the Bradley-Terry reward while staying close to a reference policy through a KL divergence penalty. Previous work showed that this approach could be further generalized: the original problem remains tractable even if the KL divergence is replaced by a family of $f$-divergence with a convex generating function $f$. Our first contribution is to show that convexity of $f$ is not essential. Instead, we identify a more general condition, referred to as DPO-inducing, that precisely characterizes when the RLHF problem remains tractable. Our next contribution is to establish a second condition on $f$ that is necessary to prevent probability displacement, a known empirical phenomenon in which the probabilities of the winner and the loser responses approach zero. We refer to any $f$ that satisfies this condition as displacement-resistant. We finally focus on a specific DPO-inducing and displacement-resistant $f$, leading to our novel SquaredPO loss. Compared to DPO, this new loss offers stronger theoretical guarantees while performing competitively in practice.", "tldr": "We characterize a broad class of functions f enabling DPO with f-divergence, and prove a simple condition on f that ensures displacement resistance.", "keywords": ["Alignment", "Direct Preference Optimization", "Reinforcement Learning", "f-divergences"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fe4c72a72544685e21828b679cb36bd9c7426962.pdf", "supplementary_material": "/attachment/ebed25f88b9d65d2a7df89a417400fd19ef33ccf.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies a generalization of $f$-DPO to non-convex $f$ generator functions, and identifies a simple condition to enforce on the generator function $f$ to be more resistant to the phenomenon of \"likelihood displacement\" affecting DPO, by which the probabilities of chosen responses in the preference dataset tend to 0 as the training progresses. The identified condition is that the minimum of $f(t)$ should occur at $t \\geq 1$. They call such functions \"displacement-resistant\".  The paper proposes a specific generator function that they term SquaredPO and compares it against DPO on TL;DR."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- S1) The paper discusses novel ideas on the realm of using $f$ divergences as penalty terms in KL-regularized RL, specifically in the context of $f$-DPO.\n- S2) It comes with an extensive theoretical analysis of the discussed algorithms"}, "weaknesses": {"value": "- W1) The paper, while clear and well-written, was also a bit terse to read for me. This is highly subjective, and I don't weigh this weakness strongly in my evaluation.\n- W2) The usage of a non-convex $f$ felt quite arbitrary, and there is no comparison with a convex displacement-resistant function. In a sense, the generalization to non-convex $f$ might be interesting, but it's unclear from the content of this paper what can we gain from it.\n- W3) More in general, the comparison of the proposed technique is very limited as it only considers one baseline (DPO) and one dataset (TL;DR) , as noted by the authors in the limitations section.  While the authors claim that they leave further comparisons for future work, I don't think this is an aspect that can be relegated to future work."}, "questions": {"value": "- Q1) Your \"displacement-resistant\" condition is a necessary, but is it sufficient to avoid likelihood displacement? If not, isn't the name \"displacement-resistant\" a bit misleading?\n- Q2) Your displacement-resistant is quite simple. Wouldn't it predict that a simple change of variables, say $t' = t - (1- e^{-1})$ for the reverse KL, work just as well?\n- Q3) I think you might want to clarify early on that the usage of $f$-divergences here concerns the generalization of the regularization penalty term only, to distinguish it from other usages in LLM post-training where they have been used as the loss to minimize in a distribution matching objective (https://arxiv.org/abs/2302.08215). Also, not sure if this is relevant, but I also note that there was another contemporary paper that studied the $f$-divergence generalization of the penalty term in the context of RLVR: https://arxiv.org/abs/2509.07430."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gtWoizxqVc", "forum": "rTte3iUsXV", "replyto": "rTte3iUsXV", "signatures": ["ICLR.cc/2026/Conference/Submission20397/Reviewer_Y1Xo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20397/Reviewer_Y1Xo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761321781301, "cdate": 1761321781301, "tmdate": 1762933845975, "mdate": 1762933845975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Likelihood displacement has been know as one of the problems of DPO. This paper tries to solve this likelihood displacement issue by changing the KL divergenge regularization term in RLHF objective to nonconvex f-divergences. It theoretically shows that a convexity is not a necessary condition. From this theoretical finding, this paper suggests square of log function. It is both displacement resistant and DPO inducing. Empirical results on TL;DR and MT-Bench show that SQUAREDPO alleviates displacement while maintaining alignment performance comparable to standard DPO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Rigorous theoretical analysis.\n- Easy to adapt loss (only regularization term is changed yet the validity is proved)"}, "weaknesses": {"value": "- Experiment is narrow.(model is only llama, the only baseline is naiveDPO)\n- In experiment, the performance gain is marginal or even similar to naive DPO. Specifically, if the same performance for squareDPO be achieved with epoch 4 as DPO with epoch 1(Table1), why we should use SquareDPO?\n- No performance analysis without LORA"}, "questions": {"value": "- How much displacement is mitigated? \nHow it improved the model performance empirically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XOS5sc6Rt5", "forum": "rTte3iUsXV", "replyto": "rTte3iUsXV", "signatures": ["ICLR.cc/2026/Conference/Submission20397/Reviewer_iniP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20397/Reviewer_iniP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755163836, "cdate": 1761755163836, "tmdate": 1762933845614, "mdate": 1762933845614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes exploring DPO variants that are based on a generalization of f-divergences that does not require convexity of f. They also analyze how these variants such as SquaredPO prevent probability displacement. They derive a set of functions that are DPO-inducing and a set of functions that are displacement resistant. They show that SquaredPO in particular is more robust to over-optimization and mitigates displacement while maintaining comparable performance to DPO."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Analysis - The paper provides a thorough analysis of the properties of different objectives and characterizes a wide range of possible objective functions. They also provide direct empirical comparisons of changes in likelihood as well as win rate and benchmark performance. \n\nClarity - The paper provides a clear presentation of ideas with detailed explanations. The theoretical definitions and interpretations walk through the key ideas and the experimental setup is well described and provides support for the claims."}, "weaknesses": {"value": "Contributions - While the analysis is thorough and claims made are well supported, there is a lack of comparison to other methods that aim to achieve the same goal and it is unclear whether the convexity constraint is an issue. Figure 1 shows that there are multiple convex functions which are DPO-inducing and displacement-resistant, many of which have already been explored and have successfully mitigated over-optimization of displacement. As a result, without further comparison to these existing methods or a strong justification as to why the convexity constraint is limiting, it is unclear whether the paper provides a method or insights that goes beyond existing methods. The key ideas involved are also primarily extensions of existing ideas in f-DPO and the cited paper for Lemma 2."}, "questions": {"value": "- Could you provide justification as to why the convexity of f may be a concern?\n- Could you provide comparisons to existing methods such as f-DPO or Chi-PO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OxYAfdiQ97", "forum": "rTte3iUsXV", "replyto": "rTte3iUsXV", "signatures": ["ICLR.cc/2026/Conference/Submission20397/Reviewer_YofB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20397/Reviewer_YofB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951622926, "cdate": 1761951622926, "tmdate": 1762933845108, "mdate": 1762933845108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work dives deeper into DPO with f-divergences ($f$-DPO): (1) Proves more relaxed DPO-inducing condition, i.e., a sufficient condition that yields the $f$-DPO loss; (2) Proves a displacement-resistant condition, i.e., a necessary condition for $f$-DPO to avoid probability displacement issue; (3) proposes SQUAREDPO, a special case of $f$-DPO that satisfies the DPO-inducing and displacement-resistant conditions, and is empirically demonstrated to have better performance and mitigated displacement issue compared with vanilla DPO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The perspective of looking at $f$-DPO is novel, with non-trivial theoretical results, which also yields a new DPO variant that is theoretically and empirically better than vanilla DPO. Many benchmarks are used in the experiments. There seem to be sufficient details to reproduce the experiments."}, "weaknesses": {"value": "Figure 4 does not show significant performance difference between DPO and the proposed SQUAREDPO. \n\nThe paper could be better if \n\n(1) Sufficient conditions for displacement-resistancy could be provided. \n\n(2) $f$-DPO with more choices of $f$ could be empirically compared, such as $\\chi^2$."}, "questions": {"value": "(1) What is $\\mathbb{R} _ {++}$ in Corollary 1? \n\n(2) Both $\\ln$ and $\\log$ are used in the paper. Do they mean the same? \n\n(3) Is there any sufficient condition to prevent probability displacement? \n\n(4) What metrics are used in Figure 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "DAydijriuU", "forum": "rTte3iUsXV", "replyto": "rTte3iUsXV", "signatures": ["ICLR.cc/2026/Conference/Submission20397/Reviewer_Zbuk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20397/Reviewer_Zbuk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967387139, "cdate": 1761967387139, "tmdate": 1762933844197, "mdate": 1762933844197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}