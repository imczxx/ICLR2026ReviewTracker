{"id": "DZcpnudp7f", "number": 10931, "cdate": 1758184967864, "mdate": 1759897620247, "content": {"title": "MoCa: Modeling Object Consistency for 3D Camera Control in Video Generation", "abstract": "Camera control is important in text-to-video generation for achieving realistic scene navigation and view synthesis. This control is defined by parameters that describe movement through 3D space, thereby introducing a 3D consistency into the generation process. A core challenge for existing methods is achieving 3D consistency within the 2D pixel domain.  Strategies that directly integrate camera conditions into text-to-video models often produce artifacts, while those relying on explicit 3D supervision face generalization issues. Both limitations originate from the gap between the 2D pixel space and the underlying 3D world. The key insight is that the projection of a smooth 3D camera movement produces consistency in object view, appearance, and motion across 2D frames. Inspired by this insight, we propose MoCa, a dual-branch framework that bridges this gap by modeling object consistency to implicitly learn 3D relationships between camera and scene. To ensure view consistency, we design a Spatial-Temporal Camera encoder with Plücker embedding, which encodes camera trajectories into a geometrically grounded latent representation. For appearance consistency, we introduce a semantic guidance strategy that leverages persistent vision-language features to maintain object identity and texture across frames. To address motion consistency, we propose an object-aware motion disentanglement mechanism that separates object dynamics from global camera movement, ensuring precise camera control and natural object motion. Experiments show that MoCa achieves accurate camera control while preserving video quality, offering a practical and effective solution for camera-controllable video synthesis.", "tldr": "We propose MoCa, a framework that enables precise camera control in text-to-video generation by modeling object view, appearance, and motion consistency to bridge 2D pixels and 3D scenes without explicit 3D supervision.", "keywords": ["Text-To-Video", "Camera-Control", "Video Generation", "Generative Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/68ecefc42600a27f56080b9945df27a6f4a1943b.pdf", "supplementary_material": "/attachment/a773bb00fb0f09d64617868861cdaf944d804411.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes MoCa, a dual-branch diffusion-transformer framework for camera-controllable text-to-video generation. The framework targets to maintain consistency in terms of view, appearance and motion. Concretely: (1) a Spatial-Temporal Camera Encoder (ST-Encoder) encodes per-pixel camera rays using Plücker coordinates and fuses them via cross-attention into the DiT to improve view consistency; (2) a semantic guidance path (ReferenceNet) injects frozen vision-language features to stabilize appearance; (3) an object-aware motion disentanglement uses a high-frequency (2D-DWT) mask over VL features to separate local object motion from global camera motion for motion consistency. The model is fine-tuned from CogVideoX on RealEstate10K and evaluated on RealEstate10K (mostly static) and VidGen (dynamic): MoCa shows better or competitive camera controllability and object/background consistency (e.g., RotErr/OC/BC), and ablations support each component."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+Clear factorization of “consistency\": Framing camera control around view/appearance/motion consistency is intuitive and aligns well with qualitative failures of prior work \n\n+Appearance stabilization via VL features. The ReferenceNet “semantic guidance” improves object identity/texture stability;\n\n+The high-frequency, object-aware mask improves OC/BC and motion plausibility when the camera is moving;\n\n+Details are provided for reproducing the method: The paper lists training details and links to code/resources."}, "weaknesses": {"value": "-Novelty vs. prior camera-conditioning is incremental.\n\nThe paper adopts Plücker coordinates for camera rays (as in CameraCtrl) and fuses conditions into DiT blocks (as in AC3D/VD3D). What is new in the ST-Encoder beyond adding spatial/temporal convs before cross-attention, and how does it differ from CameraCtrl’s/AC3D’s conditioning path are not addressed.\n\n-Missing evaluation for proposed component for view consistency\n\nIt’s plausible that per-pixel rays + temporal convs should help view (and maybe motion) consistency, but the paper doesn’t directly measure “consistency” improvements from the ST-Encoder alone (e.g., removing temporal convs, or comparing addition vs. attention per consistency metric). \n\n-Object-aware mask lacks important details/evaluation.\n\nThe mask comes from a 2D-DWT over foundation features, but key questions remain:\n* How about multiple moving objects? How are several instance regions separated if VL features are class-level and not instance-level? \n\n* Static vs moving distinction. How does the method decide that an object is static vs moving. Are static objects still routed through the disentangling branch; if so, any degradation?\n\n* User-specified object motion. Can users specify independent object motion (e.g., “a bear walks left-to-right while the camera dollies forward”), or is motion always unsupervised/implicit? Motion-control scope and ability of the proposed disentanglement method are needed, given that it is one of the major contribution. \n\n-Complexity/overhead not fully discussed.\nThe paper should report latency overhead from dual-branch fusion, and the cost/benefit of 2D-DWT during inference. Training uses 16×H200; inference speed vs. baselines would be helpful for practical adoption."}, "questions": {"value": "Please see the Weakness section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CmK4kXtpPc", "forum": "DZcpnudp7f", "replyto": "DZcpnudp7f", "signatures": ["ICLR.cc/2026/Conference/Submission10931/Reviewer_uDhp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10931/Reviewer_uDhp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849973940, "cdate": 1761849973940, "tmdate": 1762922128571, "mdate": 1762922128571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles camera-controllable text-to-video (T2V) generation. The core claim is that smooth camera motion in 3D should manifest as consistent object view, appearance, and motion in 2D frames. MoCa proposes a dual-branch framework with:\n(1) a Spatial-Temporal Camera Encoder using Plücker ray embeddings to inject geometry-aware camera signals for view consistency\n(2) a semantic guidance path that feeds vision-language features from ReferenceNet into the denoiser to stabilize appearance\n(3) an object-aware motion disentanglementto separate object dynamics from global camera motion for motion consistency\n\nExperiments on RealEstate10K (static) and VidGen (dynamic) show improved camera control and object stability versus MotionCtrl, CameraCtrl, and AC3D, with ablations for each design choice."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Paper is clearly written and easy to understand\n- Motion disentanglement through DWT is neat. Additional discussions in the appendix help strengthen the authors' claims.\n- Under a uniform 16-frame protocol, the method achieves top-rank or second-best mixes on RealEstate10K, and on VidGen improves key control/consistency metrics (RotErr, OC, CLIPSIM).\n- Ablations explicitly test fusion choice (cross-attention vs addition) and discuss alternatives in the appendix, helping attribute gains to the proposed architecture rather than incidental training details. This strengthens causal claims about the design."}, "weaknesses": {"value": "- Coverage of recent SOTA baselines is limited. While AC3D is included, other very recent transformer-based or geometry-aware camera-control methods cited in the text (e.g., VD3D, CamCo, ViewCrafter, CameraCtrl II) are not in the main comparison tables; this weakens claims of state-of-the-art across the latest literature.\n- Camera accuracy evaluation relies on Mega-SAM reconstructions. It would help to quantify Mega-SAM failure rates or uncertainty propagation.\n- The 16-frame evaluation setting seems to be a bit short in 2025. Recent models have shown to handle longer clips well; reporting an additional 32 to 48 frame setting (even on a subset) would better reflect practical camera control usage."}, "questions": {"value": "In addition to the weakness section above:\n\n- How sensitive is MoCa to camera path magnitude and frame count?\n\n- What fraction of scenes yield Mega-SAM tracking failures, and how do you handle them? \n\n- What is the runtime speed compared to the baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dw2hYaTdML", "forum": "DZcpnudp7f", "replyto": "DZcpnudp7f", "signatures": ["ICLR.cc/2026/Conference/Submission10931/Reviewer_fpxS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10931/Reviewer_fpxS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894835755, "cdate": 1761894835755, "tmdate": 1762922128169, "mdate": 1762922128169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MoCa, a framework for camera-controlled video generation that improves object stability by focusing on maintaining consistency in view, appearance, and motion. The model uses a dual-branch architecture with semantic guidance to preserve object identity and a disentanglement mechanism to separate object dynamics from camera movement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The video examples shown in the paper supp are strong. The generated videos look much more stable and realistic than the comparison methods.\n2.  The main idea of framing the problem around \"object consistency\" (view, appearance, and motion) is a smart way to tackle the challenge. It breaks down a complex 3D problem into more manageable 2D properties that we can observe in the final video. \n3.  The method for separating object motion from camera motion is clever. Using a 2D Discrete Wavelet Transform (2D-DWT) to create a \"high-frequency object-aware mask\" is an interesting technical contribution."}, "weaknesses": {"value": "1. The paper relies on Object Consistency (OC) and Background Consistency (BC) scores from VBench to prove its main contribution. However, as the VBench paper itself explains, these metrics just measure feature similarity (using DINO and CLIP) across frames. This means they mainly check if an object is consistently present, not if its motion is natural or if it's free from distortion. A video with a \"frozen\" object sliding unnaturally across the screen could still get a high OC score, which doesn't really support the claim of improved motion consistency.\n2. The method is built by fine-tuning CogVideoX, a very large (~5B) foundation model. While this helps achieve impressive results, it makes it hard to judge how much of the performance comes from the new MoCa architecture versus the power of the base model, especially considering the nature of randomness in the generation.\n3. The paper could use another round of proofreading. There are several places where the citation formatting is incorrect (e.g., in Section 4.1, should be a proper \\citep command). These small errors, along with some sections that are a bit dense, can make the paper harder to read and feel less polished, such the following part:\n\n   - The semantic guidance strategy (Section 3.2) uses a ReferenceNet to maintain object identity. The paper says it uses \"reference video frames\" (line 228), but it's not clear what this means in practice. Is the input just the first frame of the video, or a specific set of keyframes? This detail is crucial for understanding how the model gets its \"identity guidance.\"\n   - The high-frequency object-aware mask is a key part of the motion disentanglement. Could you provide more detail on how this mask is actually used in the \"Hybrid Condition Fusion\" step (line 263)? For example, is it used as a soft attention map to guide the DenoisingNet, or is it combined with other features in a different way? A clearer explanation of this mechanism would be very helpful."}, "questions": {"value": "- The semantic guidance strategy (Section 3.2) uses a ReferenceNet to maintain object identity. The paper says it uses \"reference video frames\" (line 228), but it's not clear what this means in practice. Is the input just the first frame of the video, or a specific set of keyframes? This detail is crucial for understanding how the model gets its \"identity guidance.\"\n   - The high-frequency object-aware mask is a key part of the motion disentanglement. Could you provide more detail on how this mask is actually used in the \"Hybrid Condition Fusion\" step (line 263)? For example, is it used as a soft attention map to guide the DenoisingNet, or is it combined with other features in a different way? A clearer explanation of this mechanism would be very helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aegM7SqlfU", "forum": "DZcpnudp7f", "replyto": "DZcpnudp7f", "signatures": ["ICLR.cc/2026/Conference/Submission10931/Reviewer_QNWA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10931/Reviewer_QNWA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957653464, "cdate": 1761957653464, "tmdate": 1762922127787, "mdate": 1762922127787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}