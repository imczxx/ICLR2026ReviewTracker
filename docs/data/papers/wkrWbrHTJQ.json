{"id": "wkrWbrHTJQ", "number": 12568, "cdate": 1758208661737, "mdate": 1763641463361, "content": {"title": "Cascadia: An Efficient Cascade Serving System for Large Language Models", "abstract": "Recent advances in large language models (LLMs) have intensified the need to deliver both rapid responses and high-quality outputs. More powerful models yield better results but incur higher inference latency, whereas smaller models are faster yet less capable. Recent work proposes balancing this latency‚Äìquality trade-off using model cascades, which route simpler queries to smaller models and more complex ones to larger models. However, enabling efficient cascade serving remains challenging. Current frameworks lack effective mechanisms for handling (i) the huge and varying resource demands of different LLMs, (ii) the inherent heterogeneity of LLM workloads, and (iii) the co-optimization of system deployment and routing strategy.\n\nMotivated by these observations, we introduce Cascadia, a novel cascade serving framework designed explicitly to schedule request routing and deploy model cascades for fast, quality-preserving LLM serving. Cascadia employs a bi-level optimization method: at the deployment level, it uses a mixed-integer linear program to select resource allocations and parallelism strategies based on LLM information and workload characteristics; at the routing level, it applies a Chebyshev-guided method to iteratively co-optimize the routing strategy and the system deployment produced by the deployment level. Our extensive evaluation on diverse workload traces and different model cascades (DeepSeek and the Llama series) demonstrates that Cascadia significantly outperforms both single-model deployments and the state-of-the-art cascade serving baseline, achieving up to 4$\\times$ (2.3$\\times$ on average) tighter latency SLOs and up to 5$\\times$ (2.4$\\times$ on average) higher throughput while maintaining target answer quality.", "tldr": "", "keywords": ["Distributed", "Parallel", "and Cluster Computing"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b0bbc3cf3bf78c18ca4b494ac590e90d6af80654.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces CASCADIA, a cascade-serving system for LLMs that jointly optimizes (i) GPU/resource deployment of multiple models (including DP/TP/PP choices) and (ii) threshold-based routing across the cascade to meet a target latency‚Äìquality trade-off. It uses a bi-level optimization loop: a MILP picks per-model allocations/parallelism given a routing pattern, and a Chebyshev-guided routing solver adjusts thresholds to satisfy a user quality floor. On DeepSeek and Llama cascades, CASCADIA outperforms single-model serving and CascadeServe, reporting up to 4√ó tighter SLOs and 5√ó higher throughput, with ablations showing that both resource allocation and parallelism search are necessary."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Real problem, 2026-relevant: serving fleets of heterogeneous LLMs under SLO and quality constraints is exactly what people are doing.\n- The bi-level loop (MILP for deployment + Chebyshev for routing) is a clean way to expose the coupling between ‚Äúhow many requests go to 70B‚Äù and ‚Äúhow many GPUs the 70B should get.‚Äù\n- Strong empirical numbers: up to 4√ó lower latency SLOs and up to 5√ó higher throughput than single-model; 1.7‚Äì2.5√ó over CascadeServe; wins also hold for Llama cascades.\n- Removing parallelism search or using uniform GPU allocation degrades performance notably, so the system is not just ‚Äúa nicer scheduler,‚Äù it‚Äôs actually using its degrees of freedom.\n- Online re-scheduling: handling trace1 ‚Üí trace2 ‚Üí trace3 shifts and still beating baselines makes the system more believable for production."}, "weaknesses": {"value": "- Routing quality is based on GPT-4o-as-a-judge. That‚Äôs fine for the paper but less fine for on-prem/air-gapped setups; we don‚Äôt see how robust the method is to weaker judges.\n- Incremental vs existing cascade work: CascadeServe, AutoMix, and even 2025 routing+speculative papers are moving in the same direction.\n- No cost/energy accounting, they motivate with resource efficiency and even cite energy papers, but do not report $/req or some energy unit /req, that‚Äôs what operators would care about.\n- If the quality distributions used in the routing solver are mis-estimated (domain shift, very hard math/coding workloads), the Chebyshev objective could over-route to large models and wipe out the latency gains."}, "questions": {"value": "1. The routing quality depends on GPT-4o as the judge. If we swap in a weaker/open-source judge (e.g. Llama-3-70B with an arena prompt), does CASCADIA still satisfy the same ùëû min, or does it start over-routing to bigger models? A small sensitivity table would help.\n2. Rescheduling policy. How exactly do you detect a ‚Äúsignificant‚Äù workload shift before re-running the scheduler, and do you use any hysteresis to avoid oscillating between plans?\n3. Can the deployment MILP be warm-started from the previous solution to keep solve time low when only the routing thresholds change slightly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t170YLBN02", "forum": "wkrWbrHTJQ", "replyto": "wkrWbrHTJQ", "signatures": ["ICLR.cc/2026/Conference/Submission12568/Reviewer_m6H1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12568/Reviewer_m6H1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959809606, "cdate": 1761959809606, "tmdate": 1762923422818, "mdate": 1762923422818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Cascadia, a novel cascade serving framework for efficient & effective LLM serving. Cascadia employs a bi-level optimization strategy to compute the optimal serving strategy which includes the deployment strategy (i.e., the resource allocation and parallelism strategies) as well as the routing strategy (i.e., which user requests should be consumed by which models). Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper studies efficient and effective LLM serving, which is critical problem for a wide range of real-world applications.\n2. This paper introduces Cascadia with rigorous and solid technical developments.\n3. Cascadia achieves up to 4x lower latency deadlines and 5x higher system throughputs, which are impressive."}, "weaknesses": {"value": "1. In Algorithm 1, Cascadia relies on iteratively optimizing both deployment and routing strategies. It remains unclear if the bi-level optimization is theoretically optimal or not. Such guarantees could be critical in practical scenarios.\n2. LLM routing is another well-studied technique aiming for efficient & effective LLM serving, which is under-discussed in this paper. Authors may want to discuss and compare to this line of work to better position the contribution of this paper. Several example references are as follows,\n\n[1] Ong, Isaac, et al. \"RouteLLM: Learning to Route LLMs from Preference Data.\" The Thirteenth International Conference on Learning Representations.  \n[2] Ding, Dujian, et al. \"BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute.\" Forty-second International Conference on Machine Learning."}, "questions": {"value": "What is the typical/expected number of iterations required to achieve stable solutions? If the number of iteration tends to be huge, it can lead to non-negligible overheads and compromise the efficiency gains."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pr2PCralpM", "forum": "wkrWbrHTJQ", "replyto": "wkrWbrHTJQ", "signatures": ["ICLR.cc/2026/Conference/Submission12568/Reviewer_jBdC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12568/Reviewer_jBdC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998801963, "cdate": 1761998801963, "tmdate": 1762923422066, "mdate": 1762923422066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CASCADIA, a novel cascade serving framework designed explicitly to schedule request routing and deploy model cascades for fast, quality-preserving LLM serving. CASCADIA employs a bi-level optimization method: at the deployment level, it uses a mixedinteger linear program to select resource allocations and parallelism strategies based on LLM information and workload characteristics; at the routing level, it applies a Chebyshev-guided method to iteratively co-optimize the routing strategy and the system deployment produced by the deployment level. \n\nIt uses mixed-integer linear programming (MILP) to determine the optimal deployment plan given a routing strategy. It balances response latency and output quality. Within each cascade stage, CASCADIA supports various parallelism strategies (e.g., tensor and pipeline parallelism), which allows it to automatically select the optimal strategy based on model size, incoming workload, and routing decisions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Stengths:\n\n1. Efficient serving multiple LLM to balance accuracy and latency is an important topic.\n\n2. The proposed cascading method intuitively can help the multi-model serving system.\n\n3. Extensive experiments show the performance."}, "weaknesses": {"value": "1. The main concern is on the real-time efficiency and cost. LLM serving is an online process. If using GPT-4 to judge the small model response, runs GPT-4 takes a few seconds and the cost is expensive. \n\n2. Time to first token is also very long. For simple prompt, it also needs to wait until GPT-4 finishes the judge.\n\n3. The baselines are insufficient. BERT-based router [1, 2, 3] that directly routes prompt to multiple LLMs can be compared.\n\n\n[1] https://github.com/vllm-project/semantic-router\n\n[2] Tensoropera router: A multi-model router for efficient llm inference\n\n[3] RouteLLM: Learning to Route LLMs with Preference Data"}, "questions": {"value": "1. Maybe instead of using GPT-4, fine tune a BERT for cascading?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "32VpwIWmgt", "forum": "wkrWbrHTJQ", "replyto": "wkrWbrHTJQ", "signatures": ["ICLR.cc/2026/Conference/Submission12568/Reviewer_11UR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12568/Reviewer_11UR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149764917, "cdate": 1762149764917, "tmdate": 1762923421775, "mdate": 1762923421775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents CASCADIA, an efficient cascade serving system for large language models that jointly optimizes resource allocation and request routing across hierarchies of model sizes, enabling fast, high-quality, and cost-effective LLM inference. By formulating cascade serving as a bi-level optimization problem and dynamically adapting deployment strategies, CASCADIA achieves significantly lower latency and higher throughput compared to single-model and existing cascade baselines, while maintaining answer quality across diverse workloads."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The joint formulation of resource allocation and adaptive inference via model cascades is a novel approach to cost-efficient LLM inference and is a very important problem that needs to be solved before cascades can be deployed in real world systems. The paper also considers heterogeneity in model and workload characteristics which are important considerations in real settings.\n\n2. The paper proposes a viable solution to the problem via bi-level optimization that helps to find an appropriate deployment and routing strategy and shows clear improvements over baselines in experiments"}, "weaknesses": {"value": "1. Some of the details of the approach are not clearly explained. I have added several questions below around points that were not clear to me.\n\n2. While the bi-level optimization itself doesn't seem to be taking too long to solve in online settings (Section 4.4), the latency of re-allocating the models/changing the parallelization may be high.\n\n3. The approach does not consider prefix caching even though the traces used in the experiments do contain multi-turn conversations and prefix caching in such settings and may affect the latencies of both the baselines and Cascadia. Even if it is difficult to incorporate prefix caching in the optimization formulation, I believe when running the traces prefix caching should be enabled to see if it causes the results to deviate significantly from what is expected after solving the optimization problem."}, "questions": {"value": "1. Why do you try to minimize the maximum latency across models, L, in the MILP, when the latency of a query will be the sum of the latencies of the models that it passes through?\n\n2. Why do you consider separate thresholds for each model when determining the routing strategy even though the same input is passed through the models until a satisfactory output is obtained (if the input is the same then the threshold on the output score should also be the same for all models)?\n\n3. $L(\\theta)$ in line 263 appears to be non-differentiable. If that is indeed the case, please clarify how $\\theta$ is updated to converge to the minima of the optimization problem.\n\n4. Please provide a citation for the baseline CascadeServe in Section 4.\n\n5. How is query complexity measured when looking for a shift in workload characteristics in Section 4.4? \n\n6. How does CascadeServe handle distribution shifts? Does it not make any changes under distribution shift?\n\n7. Can you quantify the scheduling overhead (line 476) in terms of additional latency (and not just throughput) when re-scheduling under online workloads? For e.g. if there is a spike in the P95 latency during the rescheduling window then that would not be a good thing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KuzJ26MULf", "forum": "wkrWbrHTJQ", "replyto": "wkrWbrHTJQ", "signatures": ["ICLR.cc/2026/Conference/Submission12568/Reviewer_HDzY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12568/Reviewer_HDzY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762192872238, "cdate": 1762192872238, "tmdate": 1762923421494, "mdate": 1762923421494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}