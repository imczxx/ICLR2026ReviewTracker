{"id": "s5XJibyXlZ", "number": 6051, "cdate": 1757951570842, "mdate": 1759897937824, "content": {"title": "Improving Online Reinforcement Learning via Behavior Prior Distillation", "abstract": "Existing behavior prior reinforcement learning (BPRL) algorithms predominantly rely on offline pre-training, where a behavior cloning model is learned from offline datasets, and policy priors are used to guide the online fine-tuning of the agent. However, the limited quality of offline datasets often hinders the ability to provide high-value policies that can effectively guide policy updates. The absence of expert trajectories significantly impairs online policy learning, leading to low sample efficiency and suboptimal performance. To address these challenges, we depart from conventional behavior prior approaches and propose a Bidirectional Behavior Prior Distillation (B2PD) algorithm. B2PD leverages action-value priors to guide a conditional variational autoencoder (CVAE) in generating a high-value behavior support set. The resulting expert behavior priors are further distilled into the agent, effectively reducing inefficient exploration and enabling stable policy optimization, while establishing a bidirectional knowledge flow mechanism. Experimental results on across both state- and pixel-based environments demonstrate that B2PD significantly improves both sample efficiency and overall performance.", "tldr": "We propose a Bidirectional Behavior Prior Distillation (B2PD) algorit", "keywords": ["online policy learning", "behavior prior distillation", "bidirectional knowledge flow"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f4890b5d13845fa4481621c25316714e3f123af5.pdf", "supplementary_material": "/attachment/a43e94017df6f3b0bdcd0a688f54884a787b2406.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes an off-policy actor critic algorithm for online reinforcement learning. Departing from the standard approach of distilling a prior from offline data, the method relies on a behavior prior distilled from the data collected online, and guided by the critic. In practice, the algorithm seems to combine different techniques and ideas:\n- entropy regularization as in SAC\n- a multi-modal behavior prior (CVAE)\n- a random-shooting procedure for computing action targets while distilling the prior\n- noise injection over actions as in TD3\n\nThe combination of these techniques produces the proposed algorithm, B2PD, which is evaluated extensively in state-based and visual online RL (across standard Mujoco and Pybullet continuous control environments). The submission is concluded by an ablation visualizing entropy curves on a single task, and by a detailed Appendix."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The goal of designing a sample efficient online RL algorithm remains relevant.\n- The experimental evaluation is sufficiently broad and includes several relevant baselines.\n- Empirically, the method seems to perform rather well, at a modest computational overhead."}, "weaknesses": {"value": "- The core idea of the method is not justified formally, to the best of my understanding. Despite a few propositions, it's unclear why one should train both a policy and a multi-modal prior on the same data. Behavioral priors are almost exclusively deployed in asymmetric settings, in which the prior and the policy are exposed to different data sources and objectives: in the most common setting, the prior is trained on offline, task-agnostic data, while the policy is trained online on a different downstream task. In this case, the benefits of a prior which was exposed to more data is clear. In this submission, both policy and prior are trained on the same data, with similar objectives (maximizing Q-values, with some regularization). Can the authors provide a formal justification of why a prior is necessary in these settings? If the issue is simply multimodality, can we directly train a multi-modal policy? \n- Aside from the core contribution, the method and its components appear not to be principled, despite formal arguments:\n\n(i) The method is presented in the standard max-entropy framework, but it is not clear why. In fact, the method is also applied on top of TD3 in experiments. Is there any fundamental synergy between the proposed method and the max-entropy framework? If not, what is the significance of Section 3.2?\n\n(ii) Standard (linear) online RL admits an optimal deterministic policy. In this case, the benefit of a multi-modal action distribution is unclear. This is of course not the case for entropy-regularized RL, but given that the algorithm is also combined with TD3, the question still stands.\n\n(iii) Proposition 3.3 appears to be wrong in general MDPs (i.e., consider an MDP with a constant rewards, or rewards matching the log-prob of a Gaussian over actions in the max-entropy case).\n\n(iv) To the best of my understanding, Propositions 3.4-3.6 are the standard soft policy iteration results from SAC. Their relevance to this method is not clear, and the original results are not referenced.\n\n(v) The SDA noise scheduling mechanism appears to produce actions from the sum of two Gaussians, which is in turn Gaussian. As the variances of the two are connected by Eq. 8, this seems equivalent to scaling the variance specifically for the action used for computing value targets, instead of a TD3-style noise injection.\n\n- The experiments do not help disentangling which of the proposed components (summary) is inducing the algorithm's performance. Ideally, each of the component which deviates from SAC should be ablated independently."}, "questions": {"value": "## Minor issues and questions:\n- Line 31 blames the issues of off-policy algorithm on the Bellman equation, which \"leads to ineffective exploration\". I think this is a strong mischaracterization: exploration is a fundamental problem in RL, and the Bellman equation does not induce it in any particular way. Can the authors further comment on this?\n- Section 3.1 directly begins with an Assumption, which should perhaps be introduced in text.\n- Assumption 3.1: $G$ and $G_\\omega$ are used interchangeably.\n- Why is Proposition 3.3 is surrounded by round brackets?\n- Line 208: is Fujimoto 2019 the right reference for CVAEs?\n- Section 4.1 lacks references for nearly all baselines.\n\n## Conclusion\nIn my opinion, while the method performs rather well, it is also overcomplicated and not principled. Formal results are not directly relevant to explaining the method's performance, and several components seem to be redundant (e.g. injecting noise on top of a max-entropy policy). For these reasons, I currently recommend rejection. Disentangling why the method works, either empirically or formally, would constitute an important priority in my opinion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Jpql9DOtDZ", "forum": "s5XJibyXlZ", "replyto": "s5XJibyXlZ", "signatures": ["ICLR.cc/2026/Conference/Submission6051/Reviewer_eZE4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6051/Reviewer_eZE4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761509418314, "cdate": 1761509418314, "tmdate": 1762918432782, "mdate": 1762918432782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Paper summary**: This work studies how to apply a pre-collected dataset to online RL performance improvement. Specifically, they focus on behavior prior RL (BPRL), which trains a behavioral cloning policy from a pre-collected dataset, thereby distilling it into an online RL policy. Like offline RL, they suffer from chronic limitation, which heavily relies on the quality of the pre-collected dataset. To alleviate this issue, this paper introduces a bidirectional behavior prior distillation (B2PD) algorithm. The main idea is simple: 1) train the CVAE policy with Q guidance and 2) distill it into an online RL policy with a simple RL objective. They show the justification of their algorithmic choice using a Toy example and ablation study. Additionally, B2PD outperforms the selected baselines over both the state- and pixel-based environments, including seven MuJoCo, four PyBullet, and four DMControl tasks.\n\n---\n**Summary of review**: Overall, the paper is clearly written and relatively easy to follow. However, the motivation and methodological exposition lack clarity, and several parts of the paper contain elements that reduce its overall polish and completeness.\nFrom a technical standpoint, using a CVAE to model a multimodal policy distribution and distill it into the actor is interesting, yet the approach still relies on rather strong assumptions and feels somewhat limited in conceptual scope. In practice, comparisons with more modern generative modeling techniques such as flow matching or diffusion steering would strengthen the argument for its novelty and effectiveness. Furthermore, the theoretical analyses appear to be moderate extensions of existing results rather than introducing fundamentally new insights. In summary, the contribution is closer to an engineering refinement than a conceptual breakthrough. To sum up, I therefore assign an initial score of 4, while leaving room for possible adjustment pending clarifications and additional justifications in the author response."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "**Writing**\n- The paper is well-structured and relatively easy to follow.\n- The authors clearly articulate the fundamental limitation of behavior-prior RL methods and justify why addressing this issue is both timely and important for improving online RL performance.\n- Key assumptions are clearly stated, helping to delineate the theoretical scope and strengthen the paper’s transparency.\n\n**Methodology**\n- The proposed solution is conceptually simple, but it shows powerful performance; in addition, some parts could be easily integrated into standard RL frameworks.\n-  The inclusion of prior distillation and SDA presents a good engineering design aimed at stabilizing optimization while reducing inefficient exploration.\n\n**Experiments**\n- This work includes diverse experiments, spanning $7$ MuJoCo, $4$ PyBullet, and $4$ DMControl tasks, with consistent hyperparameters and 10 random seeds per environment.\n- Both state-based and pixel-based settings are tested.\n- The appendix includes sensitivity analyses for key hyperparameters and thorough ablation studies, reinforcing the reproducibility of results.\n\n**Theoretical support**\n- The theoretical analysis provides a reasonable degree of mathematical justification for convergence and Q-value smoothness.\n- These analyses help to confirm the soundness and stability of the optimization dynamics."}, "weaknesses": {"value": "**Writing**: weak motivation and framing\n-  While the problem setup is valid, the justification for transitioning to a purely online RL paradigm is not entirely convincing. The paper may overstate the generality of online-only applicability without fully addressing hybrid or offline-to-online alternatives.\n- The discussion lacks a comparative reflection on when offline priors are still beneficial or how the proposed method complements existing pretraining pipelines\n\n**Methodology**\n- The proposed SDA noise scheduling is designed with two constants (0.2 and 1.5) whose motivation or sensitivity is not well explained.\n- Compared to baselines, the authors do not analyze computational overhead quantitatively. \n- Behavior-prior generation heavily relies on the assumption of a well-trained Q-function with broad coverage. Theoretical guarantees may fail under biased or sparse datasets; this limitation should be acknowledged more explicitly.\n\n**Theoretical depth**\n- Propositions 3.4–3.6 mainly restate established RL principles with incremental extensions.\n- The analysis is sound but does not introduce fundamentally new theoretical insights. The reviewer thinks that moving some incremental theorems to the appendix and expanding more empirical reasoning would improve focus.\n\n**Experiments**\n- The reviewer thinks that the toy example illustrates behavior qualitatively but does not convincingly link trajectory patterns to sample efficiency or exploration coverage. Quantitative measures of state-space visitation or gradient signal analysis would better support the claims.\n- The authors ask, `Why does behavior prior distillation outperform entropy-driven exploration?', but provide only empirical evidence rather than a logical explanation.\n- The baselines are somewhat outdated. More recent algorithms, such as TD7 [1], Mr.Q [2], or other modern behavior-prior representation methods, should be considered.\n- There is no comparison to recent or alternative exploration strategies, for example, intrinsic reward [3], diversity-driven [4], distributional RL [5], or other RL on prior data (RLPD) [6]. Similarly, while the related-work section cites [7-9], these works are not included in experimental comparisons or deeper discussions.\n- There is no main table to grasp overall performance across all benchmarks and tasks. The reviewer thinks that it would be better to provide the main summary table, consolidating all benchmark results and averaged performance across tasks.\n\n**Miscellaneous**\n- Formatting and typesetting issues appear, e.g., garbled characters in Figure 1 and Figure C.1.\n- Notational inconsistencies:\n   - The discount factor $\\gamma$ is stated as $\\gamma \\in (0,1)$, through theoretically it can be $\\leq 1$.\n   - Some variables (\\theta, \\phi, \\tau) are not introduced clearly on first use. \n   - Section organization could be smoother. In the experimental section, there is a solved research question by a toy example.\n   \n**References**\n\n[1] S. Fujimoto, et al. For SALE: State-Action Representation Learning for Deep Reinforcement Learning. NeurIPS 2023.\n\n[2] S. Fujimoto, et al. Towards General-Purpose Model-Free Reinforcement Learning. ICLR 2025.\n\n[3] N. Chentanz, et al. Intrinsically motivated reinforcement learning. NeurIPS 2004 \n\n[4] D. Pathak, et al. Curiosity-driven exploration by self-supervised prediction. ICML 2017.\n\n[5] W. Dabney, et al. Distributional Reinforcement Learning with Quantile Regression. AAAI 2018.\n\n[6] P. Ball, et al. Efficient Online Reinforcement Learning with Offline Data. ICML 2023.\n\n[7] H. Zang, et al. Behavior Prior Representation learning for Offline Reinforcement Learning. ICLR 2023.\n\n[8] G. Spigler. Proximal Policy Distillation. arXiv 2024.\n\n[9] M. Nakamoto, et al. Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning. NeurIPS 2023."}, "questions": {"value": "- What is the computational cost of training and sampling from the CVAE, particularly as the number of sampled priors $H$ increases? Please discuss its bottleneck or limitations that might arise when scaling to an image-based environment.\n- Related to hyperparameter sensitivity ablation, are there practical tuning heuristics or observed failure modes when these parameters are mis-set?\n- Could the authors provide a direct comparison or analytical discussion with recent approaches, as mentioned in the weakness section?\n- Table E.1 and Figure E.1 compare KL-divergence vs. MSE objectives for distillation. Do the authors have principled guidelines for choosing between them depending on the policy’s stochasticity or parameterization?\n- To what extent can the CVAE generate actions outside the replay buffer support? Does it meaningfully encourage exploration beyond previously seen behaviors, or mainly reinforce high-value regions already represented in the buffer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LobxRVRnK2", "forum": "s5XJibyXlZ", "replyto": "s5XJibyXlZ", "signatures": ["ICLR.cc/2026/Conference/Submission6051/Reviewer_vq3Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6051/Reviewer_vq3Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578725430, "cdate": 1761578725430, "tmdate": 1762918432460, "mdate": 1762918432460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes B2PD: Bidirectional Behavior Prior Distillation for online RL. Rather than relying on offline pretraining, B2PD builds a behavior prior online using a CVAE and establishes a two-way knowledge flow:\n(1) Action-Value Prior Distillation (AVPD) trains the CVAE with guidance from a learned $Q$ so that it samples a high-value support set;\n(2) Behavior Prior Distillation transfers the best actions from that set back to the actor via a KL-style anchor loss.\nThe method also introduces a Standard Deviation Aware (SDA) noise schedule to stabilize soft $Q$ targets and provides tabular convergence for policy evaluation, improvement, and iteration. On MuJoCo, PyBullet, and DMControl (including pixel-based DrQ-v2 settings), B2PD improves sample efficiency and final returns over TD3/SAC and prior-guided baselines, with ablations and a toy study that visualize reduced inefficient exploration."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Online priors without offline data. A generative prior creates diverse, high-value anchors and distills them into the actor.\n\nBidirectional design. AVPD guides the CVAE with $Q$; prior anchors regularize the actor, reducing aimless entropy-driven exploration.\n\nStability add-on. SDA smooths value targets and reduces variance.\n\nBroad evaluation. Consistent gains across 16 tasks, including pixel-based settings.\n\nAblations and toy study. Each module’s effect is isolated; exploration becomes more targeted over training."}, "weaknesses": {"value": "Dependence on value quality. AVPD uses a learned $Q$ to steer the CVAE. Failure modes under biased $Q$ or sparse rewards are not deeply diagnosed.\n\nAnchor selection sensitivity. The $\\arg\\max$ over $H$ sampled actions may be sensitive to $Q$ noise; the compute vs. robustness trade-off for $H$ is underexplored.\n\nTheory scope. Convergence is shown in tabular settings, not with function approximation or under distribution shift.\n\nPixel baselines. Visual experiments mainly use DrQ-v2; more backbones would strengthen claims of modality robustness."}, "questions": {"value": "$Q$ uncertainty. Have you tried ensembles or variance-aware filters to avoid over-optimistic AVPD targets, especially early or in sparse-reward tasks\n\nAnchor budget $H$. What is the trade-off between $H$, wall-clock time, and final return Do you observe diminishing returns beyond $H=10$\n\nSDA schedule. How sensitive are results to the constants in the SDA equation and to excluding noise from the entropy term Could per-dimension scaling collapse exploration\n\nCVAE underfit. If the CVAE misses high-value modes, how quickly can B2PD recover Any diagnostics to detect support gaps\n\nPixel-based generality. Does B2PD transfer to other visual backbones (for example DrQ-v3, RAD, PI-SAC) without retuning"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "byfF1VsrMX", "forum": "s5XJibyXlZ", "replyto": "s5XJibyXlZ", "signatures": ["ICLR.cc/2026/Conference/Submission6051/Reviewer_H77H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6051/Reviewer_H77H"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898743181, "cdate": 1761898743181, "tmdate": 1762918432187, "mdate": 1762918432187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the sample inefficiency of online RL by proposing Bidirectional Behavior Prior Distillation (B2PD), which replaces conventional offline behavior priors with dynamically generated high-value policy guidance. B2PD trains a CVAE using Q-value-guided gradients to produce a diverse support set of actions. The highest-value actions from this set are distilled into the agent policy, reducing inefficient exploration. ). Experiments on continuous control tasks (MuJoCo, PyBullet, DMControl) demonstrate improved sample efficiency and performance over baselines like SAC and TD3."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Novel way for combining generative modeling and RL.\n- Theoretically grounded contributions\n- Comprehensive and rigorous evaluation"}, "weaknesses": {"value": "- In Figure 2, B2PD(w/o AVPD) converges faster (at 100K steps) to the states with high reward than B2PD (at 150K steps). This result seems to show that the AVPD module is even harmful for effective state exploration.\n- Learning under value-guided offline policy distribution has been widely researched in offline RL before. For example, the advantage-weight CVAE model [1], and the advantage-conditioned CVAE model [2]. But None of them are referenced in the main text or compared in experiments. \n- Actually, the main contribution is the Q-value prior distillation loss in Eq. 10, where the CVAE decoder is constrained to decode actions with high Q value. This method is quite similar to LAPO [1] mentioned above, which also encourages the CVAE to consider the state-action pair with high advantage. Thus, I think the novelty is quite limited.\n- Meanwhile, considering the ablation study of weight $\\xi$ presented in Figure F.2, the effectiveness of Q-value prior distillation loss seems unclear and unstable, where sometimes the returns are even worse than without this loss."}, "questions": {"value": "- Can the author provide a detailed pseudo-code in the appendix? Is both the CVAE and critic model trained in offline stage?\n- Instead of retraining a new policy from scratch under the offline RL–trained or BC–trained policy, why not directly fine-tune the offline RL–trained policy in the online stage using Offline-to-Online (O2O) algorithms? I believe that O2O algorithms are more effective technique.\n- More studies about the effectiveness of Q-value prior distillation loss should be conducted. For example, how will the policy perform under the CVAE trained by [1][2]?\n\n[1] LAPO: Latent-Variable Advantage-Weighted Policy Optimization for Offline Reinforcement Learning. NeurIPS 2022\n\n[2] A2PO: Towards Effective Offline Reinforcement Learning from an Advantage-aware Perspective. NeurIPS 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pm0U7OfgZK", "forum": "s5XJibyXlZ", "replyto": "s5XJibyXlZ", "signatures": ["ICLR.cc/2026/Conference/Submission6051/Reviewer_NHmE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6051/Reviewer_NHmE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762172284060, "cdate": 1762172284060, "tmdate": 1762918431920, "mdate": 1762918431920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}