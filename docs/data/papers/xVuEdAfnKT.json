{"id": "xVuEdAfnKT", "number": 22840, "cdate": 1758336147682, "mdate": 1759896843437, "content": {"title": "GroundAttack: Mitigating Easy-Options Bias for Visual Question Answering", "abstract": "In this early study, we observe an Easy-Opitions Bias (EOB) issue in several multiple-choice Visual Question Answering (VQA) benchmarks, including MMStar, RealWorldQA, SEED-Bench, NeXT-QA, STAR benchmark, and Video-MME. This bias allows vision-language models (VLMs) to select the correct answer using only the vision ($\\boldsymbol{V}$) and options ($\\boldsymbol{O}$) as inputs, without the need for the question ($\\boldsymbol{Q}$). Through grounding experiments, we attribute the bias to an imbalance in visual relevance: the correct answer typically aligns more closely with the visual contents than the negative options in feature space, creating a shortcut for VLMs to infer the answer via simply vision-option similarity matching. To mitigate this, we introduce GroundAttack, an agentical method that automatically generates hard negative options as visually plausible as the correct answer. \nWe apply it to the NeXT-QA and MMStar datasets, creating new EOB-free annotations. On these EOB-free annotations, current VLMs approach random accuracies under ($\\boldsymbol{V}$+$\\boldsymbol{O}$) settings, and drop to non-saturated accuracies under ($\\boldsymbol{V}$+$\\boldsymbol{Q}$+$\\boldsymbol{O}$) settings, providing a more realistic evaluation of VLMs' QA ability.", "tldr": "We reveal and address an Easy-Options Bias in VQA benchmarks by introducing GroundAttack, which generates hard negative options to enable fairer evaluation of vision-language models.", "keywords": ["Visual Question Answering", "Shortcut learning", "Vision Languame Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5aa54b0e130d71285a92406b337d69c211105980.pdf", "supplementary_material": "/attachment/9ced97144b8108f7088b62c1fb8ddcb18bb549ed.pdf"}, "replies": [{"content": {"summary": {"value": "This paper highlights a caveat of certain multiple-choice Visual Question Answering (VQA) benchmarks used to asses the performance of Vision-Language Models (VLMs). Authors indeed present the “Easy-Options Bias” (EOB), where the correct option (ground-truth answer to the question) is much more aligned with the content of the input image than other options. In this case, considering the question is not necessary to answer it. Such an alignment is empirically validated by showing that the ground-truth option is often closer to the image in CLIP embedding space. To mitigate this limitation, they propose a method to generate hard negatives, i.e. alternative options to the ground-truth answer which are more aligned with the input image, to require any model to process the question to successfully answer it. Such method is composed of 3 modules: (i) a visual captioner generates a visual description of the image, fed to (ii) a distractor producing a set of negative candidate options that (iii) a selector will filter to keep the best ones. The existence of EOB, along with the impact their correction method, are validated empirically across different multiple-choice VQA benchmarks and VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- S1: High-quality benchmarks are primordial to properly and fairly assess the progress of a research field, so identifying and correcting potential flaws are important contributions.\n- S2: The addressed limitation of VQA benchmarks is clearly explained, motivated and empirically validated.\n- S3: The proposed method leads to a significant reduction of the detected bias in the considered VQA benchmarks.\n- S4: It is also appreciated that authors present a comprehensive list of limitations for their work."}, "weaknesses": {"value": "- W1: [Major] As acknowledged by authors, a comparison with other distractor generation methods would be important to evaluate the gain from the proposed method. From the current paper, we know that their method succeeds in making the VQA benchmarks more challenging, but maybe alternative methods could be even better.\n\n- W2: [Major] Authors show that their method makes VQA benchmarks more challenging, but do they verify no hard negative is as good of an answer to the question as the ground-truth one, which would make the question ill-posed? Related to this question, are the generated options double-checked by human annotators?\n\n- W3: [Major] Hard negatives are generated by models that could also be biased in a different way. Could authors elaborate on such limitations of using LLMs to automatically generate options?\n\n- W4: [Major] It seems that the “CLIP-Selector” baseline is quite competitive, with a performance close to “GroundAttack” while being much simpler. Could authors discuss how each component of their method is really necessary?"}, "questions": {"value": "- Q1: [Related to W1] Could authors compare their method to other distractor generation approaches (e.g. the ones they present in their paper)?\n\n- Q2: [Related to W2] Do authors verify no hard negative is as good of an answer to the question as the ground-truth one, which would make the question ill-posed? Related to this question, are the generated options double-checked by human annotators?\n\n- Q3: [Related to W3] Could authors elaborate on such limitations of using LLMs to automatically generate options?\n\n- Q4 [Related to W4] Could authors discuss how each component of their method is really necessary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E6tVNYu3nq", "forum": "xVuEdAfnKT", "replyto": "xVuEdAfnKT", "signatures": ["ICLR.cc/2026/Conference/Submission22840/Reviewer_ibiF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22840/Reviewer_ibiF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918189756, "cdate": 1761918189756, "tmdate": 1762942408060, "mdate": 1762942408060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies and formalizes Easy-Options Bias (EOB) in multiple-choice VQA: models can often answer correctly using only vision and options, without the question. The authors propose GroundAttack, an agentic pipeline that replaces only the negative options with visually groundable, hard distractors, and re-annotate MMStar and NExT-QA accordingly. Empirically, accuracies drop markedly under (V,Q,O) and approach chance under (V,O), indicating reduced shortcutting and a more realistic evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper shows EOB is widespread across several VQA benchmarks and defines EOB, offering a principled lens on a subtle evaluation flaw."}, "weaknesses": {"value": "1. Method is primarily evaluative/incomplete. The work focuses on test-time re-annotation; it does not study whether training on GroundAttack-augmented data improves model robustness or transfer—leaving the algorithmic impact on learning untested.\n\n2. Limited dataset coverage. Despite diagnosing EOB broadly, the mitigation is validated only on MMStar and NExT-QA; generality to other listed benchmarks remains unverified. \n\n3. Quality control of generated negatives. Beyond aggregate accuracy drops, the paper lacks human studies or fine-grained error audits to verify that generated negatives are semantically plausible yet definitively incorrect and not introducing new annotation noise.\n\nOverall, this is a timely and important problem with a practical evaluation tool; however, the method feels incomplete without training-time studies and broader validation."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NKaxPkqwPh", "forum": "xVuEdAfnKT", "replyto": "xVuEdAfnKT", "signatures": ["ICLR.cc/2026/Conference/Submission22840/Reviewer_zzQa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22840/Reviewer_zzQa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927679422, "cdate": 1761927679422, "tmdate": 1762942407769, "mdate": 1762942407769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies and investigates a new form of dataset bias in multiple-choice Visual Question Answering (VQA) benchmarks, termed Easy-Options Bias (EOB). The authors demonstrate that state-of-the-art vision-language models (VLMs) can often predict the correct answer using only the vision and options inputs, without requiring the question. This undermines the validity of existing benchmarks as tests of multimodal reasoning. To mitigate this, the paper proposes GroundAttack, an automated method that generates visually plausible and semantically coherent hard negative options using a three-agent system: (1) a Captioner that produces image descriptions, (2) a Distractor that generates negative candidates, and (3) a Selector that chooses the most confusing distractors via CLIP similarity and clustering. Experiments demonstrate that GroundAttack substantially reduces EOB, lowering VLM accuracies under (V,O) settings to near-random performance and yielding more realistic evaluations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The discovery of Easy-Options Bias is original and highly relevant. The study evaluates six benchmarks and multiple VLM families, providing convincing evidence that EOB is widespread and non-trivial.\n2. GroundAttack is a pragmatic contribution, requiring minimal manual intervention while effectively generating high-quality hard negatives. Results show that EOB is mitigated and that model accuracies decrease to near-random levels when the question is omitted."}, "weaknesses": {"value": "1. While the Easy-Options Bias (EOB) phenomenon indeed exists, the authors’ interpretation may be overly strong. When a model is given only the image and must select the answer without the question, it is natural and even intelligent for it to choose the option most visually aligned with the image. This behavior does not necessarily imply shallow reasoning. If the newly introduced confusing negative options were truly negative and the model accuracy dropped significantly (see 2), that could support the claim of shallow reasoning. However, even in that case, expecting model performance to approach random chance may not be a reasonable criterion.。\n2. The paper uses VLM to generate confusing negative options, but these are not always semantically incorrect. For instance, in Figure 5, the original dataset’s negatives (“a basketball,” “a volleyball,” “a soccer ball”) are clearly wrong, while GroundAttack produces alternatives like “a sporting good,” “a ball for experienced players,” or “a clean ball,” which are not factually incorrect but rather vague or underspecified. In this example, the correct answer (“a tennis ball”) is indeed more appropriate than these generated alternatives, but the difference lies in degree rather than categorical correctness. However, the generated “hard negatives” may sometimes represent alternative but equally reasonable expressions of the correct answer—so subtle that even humans might struggle to distinguish them—making the correct answer non-unique or even ambiguous. This undermines the reliability of the paper’s claims regarding EOB reduction.\n3. As acknowledged in the Limitations section, the models themselves contain inherent biases. Since multiple VLM are used to generate different components of the negative options, the consistency and reliability of these outputs are difficult to trust. The resulting negatives may reflect correlated model biases rather than genuine adversarial diversity.\n4. The paper introduces three separate modules (Captioner, Distractor, Selector), but provides limited empirical or conceptual justification for this design choice. Why must these be distinct components? Why can’t a single model directly generate the final adversarial negatives? The necessity of the Captioner, as well as the two-stage “generate candidates then select” process, is not convincingly demonstrated. More ablation or efficiency analysis is needed to support this design.\n5. As noted by the authors themselves, there is no comparison with alternative distractor generation methods, nor any experiment showing whether training on GroundAttack-augmented data improves generalization or robustness. Without such evaluations, the effectiveness of GroundAttack remains insufficiently validated and its broader utility uncertain."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UITublonWs", "forum": "xVuEdAfnKT", "replyto": "xVuEdAfnKT", "signatures": ["ICLR.cc/2026/Conference/Submission22840/Reviewer_te8u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22840/Reviewer_te8u"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136434071, "cdate": 1762136434071, "tmdate": 1762942407463, "mdate": 1762942407463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work explores a previously unexplored failure mode in VQA benchmarks, where the models can answer a question with only the image and the option pair without needing the question. It then proposes a new method called GroundAttack in order to generate visually and semantically plausible hard negatives to remove the bias of the correct option towards the image in the feature space. Their experiment on NExT-QA and MMStar shows that the performance of VLMs drop close to random baseline when only image and option is given as input."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The identification of Easy-Options Bias provides a interesting perspective of current VQA benchmarks.\n* GroundAttack is modular, easy to implement, and agent-based, potentially generalizable to other multimodal tasks."}, "weaknesses": {"value": "* Lemma 1 shows no proof, and the result is not very obvious.\n* Tables 2 and 3: The paper reports number of smaller family of models (capped within 7B). It doesn’t show how the bigger parameter models perform in this setting.\n* All experiments shows single run number. No variance reporting is done.\n* The number of datasets studied is limited in scope.\n* The goal of hard negatives is to make it hard for the model reach to the correct answer using any shortcuts. From Figure 5. Some options are not just hard but based on the question context are candidate correct answer. Figure5: Example 1 - The clean ball can be a correct answer as well. Example 2- The misty fields can be a correct answer as well. The way these distractors are generated and using embedding based selection, it seems some of the generated “hard negative options” are just rephrasing of the correct answer or plausible answers.\n* For the crucial steps of the proposed methods, this work uses small family of models like Gemma-3n-E48. This can hinder the quality of the generated samples.\n* The paper's initial diagnosis shows that the model performance is still quite high with only (V,O) pairs but it shows no analysis as to why.\n* Definition 1 seems redundant."}, "questions": {"value": "* Section 3.3: As per the claim of the paper, the options of a VQA triplet are not completely unbiased. The correct option is biased in the feature space towards the images, thereby leading the model to claim that as the correct answer instead of the negative ones. The paper shows no insights into how the Total EOB as per Table 1, correlated to the performance drop in Table 2 and Table 3. If the total EOB of MMStar is 13.6%, how is the average drop in performance across all the models only 9.7 % in Table 1?\n* Since the hard negatives are selected using top-m embeddings, what is the comparative difference in similarity between the generated negative options and the original negative options with the correct answer?\n* As there is no threshold given while selecting the negative options, what stops this method from selecting options with 0.99 similarity with the correct option?\n* In B.1 Prompt for generating distractors for Image VQA: In the in-context example, the image description shows no relation to a black dog as mentioned in the question, correct options and the generated negative. Can you share any of the reasoning traces of generative negative options candidates?\n* The claim that the VLM finds shortcut in answer a question is simply based on the accuracy metrics. Is it possible to show any analysis to back up this claim?\n* Was there any human evaluation conducted to measure distractor plausibility?\n* Can you clarify or empirically validate the probabilistic assumptions in Lemma 1?\n* Can you verify that there is a consensus between a large model like GPT-4o and the model you used in this work, for generating the negative options?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oCDq8Rmb5H", "forum": "xVuEdAfnKT", "replyto": "xVuEdAfnKT", "signatures": ["ICLR.cc/2026/Conference/Submission22840/Reviewer_Eo8V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22840/Reviewer_Eo8V"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762431205697, "cdate": 1762431205697, "tmdate": 1762942407227, "mdate": 1762942407227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}