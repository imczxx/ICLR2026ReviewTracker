{"id": "XzXKnazRBF", "number": 1396, "cdate": 1756879471907, "mdate": 1759898210987, "content": {"title": "GarmentGPT: Compositional Garment Pattern Generation via Discrete Latent Tokenization", "abstract": "Apparel is a fundamental component of human appearance, making garment digitalization critical for digital human creation. However, sewing pattern creation traditionally relies on the intuition and extensive experience of skilled artisans. This manual bottleneck significantly hinders the scalability of digital garment creation. Existing generative approaches either operate as data replicators without intrinsic understanding of garment construction principles (e.g., diffusion models), or struggle with low-level regression of raw floating-point coordinates (e.g., Vision-Language Models).\nWe present GarmentGPT, the first framework to operationalize latent space generation for sewing patterns. Our approach introduces a novel pipeline where a Residual Vector Quantizer Variational AutoEncoder (RVQ-VAE) tokenizes continuous pattern boundary curves into discrete codebook indices. A fine-tuned Vision-Language Model then autoregressively predicts these discrete token sequences instead of regressing coordinates, enabling high-level compositional reasoning. This paradigm shift aligns generation with the knowledge-driven, symbolic reasoning capabilities of large language models.\nTo address the data bottleneck for real-world applications, we develop a Data Curation Pipeline that synthesizes over one million photorealistic images paired with GarmentCode, and establish the Real-Garments Benchmark for comprehensive evaluation. Experiments demonstrate that GarmentGPT significantly outperforms existing methods on structured datasets (95.62\\% Panel Accuracy, 81.84\\% Stitch Accuracy), validating our discrete compositional paradigm's advantages.", "tldr": "", "keywords": ["Garment Generation", "vision language models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/46896ce34b72fd42a97fbbb38ceee027b41b6c36.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel framework called GarmentGPT for generating 2D sewing patterns from multi-modal inputs (such as images and text). GarmentGPT introduces a Residual Vector Quantizer Variational AutoEncoder (RVQ-VAE) to transform the pattern generation problem from a continuous coordinate regression task into a discrete, compositional sequence generation task. Furthermore, to address the scarcity of photo-to-pattern pairs for real-world applications , the paper also contributes a Data Curation Pipeline for synthesizing over one million \"photorealistic\" images paired with their corresponding \"GarmentCode\""}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper's greatest strength lies in its core concept. Instead of forcing VLMs to do what they are not good at (regressing thousands of floating-point coordinates) , the authors use an RVQ-VAE to create a geometric \"vocabulary\". The VLM then \"composes\" these geometric primitives, much like \"writing\" a sequence . This is an elegant and powerful solution.\n- The method achieves outstanding performance on the structured GarmentCode dataset, outperforming the regression-based SOTA model Alpparel by a large margin (e.g., +16.7% Panel Accuracy, +25.3% Stitch Accuracy).\n- The proposed Data Curation Pipeline , the synthesized 1M+ dataset (RealGarment-IM) , and the Real-Garments Benchmark  are important resources that can drive future research in this field."}, "weaknesses": {"value": "1. The proposed \"Data Curation Pipeline\"  does not use real photos. Instead, it uses an image editing model (Qwen-image-edit) to convert rendered images into \"photorealistic\" images. This is a simulation process. The results in Table 3 show that the model's performance drops sharply when transferring from structured data (Table 2) to this new \"real\" benchmark (Table 3) (e.g., Panel Accuracy drops from 95.62% to 47.81%) . This indicates that the data pipeline fails to effectively bridge the sim-to-real gap. Therefore, the paper's claim of being the \"first [model] trained on large-scale real photos\"  is misleading.\n\n2. The paper reports poor performance on the Real-Garments Benchmark (Table 3)  but provides no in-depth analysis. Why is the performance degradation so significant? Is it a failure of the VLM's perception, or poor generalization of the RVQ-VAE tokens to real images? What are the most common error types (e.g., incorrect panel count? incorrect curve shapes?)? The lack of this analysis is a missed opportunity.\n\n3. Lack of clarity and fairness in the experimental comparison. The paper states in Section 5 \"Framework\" that the VLM was fine-tuned on \"our curated dataset\" , which is defined in Section 4 as the newly created RealGarment-IM dataset. However, the SOTA comparison experiment against baselines like Alpparel (Table 2) was conducted on the GarmentCode Dataset . This makes it difficult to determine whether the performance improvement comes from the data or the innovative model architecture."}, "questions": {"value": "1. The SOTA comparison (Table 2) is conducted on the GarmentCode Dataset , but the paper states the model was trained on the newly produced Real-Garments dataset (RealGarment-IM). Does this not create an unfair comparison? Please clarify the exact training set used for the SOTA comparison in Table 2 to ensure the comparison was fair.\n\n\n2. Given that the model was trained on the Real-Garments dataset, yet its performance on this benchmark shows a sharp decline compared to the structured dataset , can the authors analyze the main reasons for this performance drop? Does it stem from limitations in the model or in the data pipeline?\n\n3. Could you provide a qualitative or quantitative analysis of the main error types on the Real-Garments Benchmark? For example, what percentage of errors are high-level structural errors (e.g., incorrect number of panels) versus low-level geometric errors (e.g., inaccurate curve indices)?\n\n4. Inconsistency in Table 3: In the text of Section 5.3, the authors state the model used for the real-world benchmark is \"GarmentGPT (LLaVA-1.5 with Image+Text input)\". However, the caption for Table 3 reads \"Performance of GarmentGPT (Qwen-7B)\". Which model actually produced the results in Table 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yGyfF5FaKE", "forum": "XzXKnazRBF", "replyto": "XzXKnazRBF", "signatures": ["ICLR.cc/2026/Conference/Submission1396/Reviewer_qZMb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1396/Reviewer_qZMb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761393905569, "cdate": 1761393905569, "tmdate": 1762915761092, "mdate": 1762915761092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors fine tune a LLM to produces a structured description of a garment (panels, shapes, stitching)  using tokens learned from a Residual VQVAE.  They train it on a novel 1M sample dataset of garments - the dataset is constructed using modern image generators to make them appear more realistic."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Ambitious attempt to bridge SMPL renders and realistic imagery.\nIntroduces automatic data curation pipeline for garment datasets.\nLarge-scale dataset construction (1M+ samples) is potentially impactful.\nMentions residual quantization for compression–quality balance."}, "weaknesses": {"value": "Abstract: “Paradigm shift” reads as add-speak / promotional, not scientific.\n\n057: RCQ-VAE insufficiently defined; unclear whether novel or existing concept. Missing citations for residual or hierarchical quantization methods. (significant)\n189: “Such as” vague; replace with “including … as shown in experiments.”  They used specifically both later.  (minor)\n200: “Ours” should be “Our” or omitted for clarity. (minor)\n253–254: Unclear comparison procedure for arcs; method not described (significant)\n253–254: Commitment loss mentioned without definition or formula. (this is more severe)\n280: SMPL acronym unexplained at first mention (minor)\n291: Claim of “ensuring garment structure consistency” unsubstantiated by evidence. (significant)\n298: “Real” oversells results; “realistic” or “photorealistic” more accurate. (easily addressable)\nFig. 3: Visible segmentation errors and unrealistic texture blending undermine claims.(somewhat significant - would help to show the same garments)"}, "questions": {"value": "Can it handle the jean shorts shown in figure 3?  How does the segmentation error shown impact the data?\nWhich papers or frameworks support the residual quantization claim?  Which aspects of it are novel?\nHow are arcs compared—pointwise, via curvature metrics, or other geometry loss?\nWhat exactly is “commitment loss”? Why is it not described elsewhere?\nHow do authors verify “garment structure consistency”—visual inspection or metric-based?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concern here."}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Guka4DdcJK", "forum": "XzXKnazRBF", "replyto": "XzXKnazRBF", "signatures": ["ICLR.cc/2026/Conference/Submission1396/Reviewer_e7jR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1396/Reviewer_e7jR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761496338913, "cdate": 1761496338913, "tmdate": 1762915760794, "mdate": 1762915760794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GarmentGPT to address the challenge of generating structured garment sewing patterns from inputs like text or images. It proposes a two-stage framework that first tokenizes garment geometry into discrete latent codes using a Residual Vector Quantized VAE and then trains a VLM to sample or edit these token sequences auto regressively. A hierarchical token schema is proposed (garment > panel > edge > stitch) to ensure that the generated output respects garment topology. Edge curves and pose are encoded into separate codebooks to disentangle geometric factors. The contributions are a novel discrete token representation of garment structures that enables compositional reasoning and editing, the GarmentGPT model for generation and modification of sewing patterns, and a large-scale dataset to evaluate structured garment generation from real photos."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The idea of encoding garment structure as a hierarchical sequence of discrete tokens innovative and aligns with current research directions in llms to enable structured reasoning over geometrical data compositions. The model design is conceptually consistent and technically well-motivated.\n- The framework supports not only garment generation from text or images but also editing and refinement of existing GarmentCode sequences, making it more practical for real design applications.\n- The introduction of a large-scale, multimodal dataset and benchmark is a strength.\n- The approach demonstrates improvements over existing, regression-based method."}, "weaknesses": {"value": "1. I find the motivation for the proposed contribution difficult to understand. Does the method solely work for creating garments for digital avatars? What about creating patterns for real-world garments? \n2. To the best of my knowledge, real patterns are much more complex than the examples shown in the paper as they often exhibit fine-grained textile features for seams or laminated strips. It appears to me that the method does not capture this complexity. At the very least, this is difficult to assess without substantial qualitative examples.\n3. There are almost no qualitative results shown in the main paper or the appendix. Highlighting visual results on both the GarmentCode and Real-Garments datasets would be essential to assess the contribution and current capabilities of the proposed method. It would help the understanding in multiple ways as it a) shows how practically applicable the method is by showing what complexity of garment designs can be extracted/generated from the inputs,  b) visualizes the setting and difficulty of tasks in the benchmarks, c) allows to draw qualitative comparisons between the proposed method and baselines.\n4. The quantitative results on the proposed real-garments benchmarks make the contribution somewhat questionable. I acknowledge the authors disclose the performance drop and pose this as an open challenge for future research, but doesn’t this show that the method not really works for the intended application? I would also like to see the metrics for the other baseline methods on the real-garments benchmark.\n5. The authors do not provide any information about the cost (necessary GPU resources) of their method. This information would be important to assess how this be scaled and improved, e.g. to capture more complex patterns and finer details. \n\nMinor remarks:\n1. Figure 1 has a fairly low resolution, making it hard to read when zoomed in. Please consider embedding it as pdf or svg for better quality."}, "questions": {"value": "1. Referring to Weakness 1: Why do I need to create parametric garment-codes for digital human avatars? Is this a necessary contribution to the field of digital avatars?\n2. Referring to Weakness 2: For real-world applications, i.e. extracting the garment codes from photographs, do the method-generated patterns capture the actual complexity of garment designs? What about different types of seams that are required for stitching together certain parts of a garment? \n3. I find the order of subchapters in chapter 5 a bit unconventional. Why do the ablations come before the evaluation of the experiments? I would move the ablations to the end of the chapter. This would also highlight the importance of current Table 2 (the main quantitative results) over current Table 1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j62VwekbwY", "forum": "XzXKnazRBF", "replyto": "XzXKnazRBF", "signatures": ["ICLR.cc/2026/Conference/Submission1396/Reviewer_HvT3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1396/Reviewer_HvT3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835271940, "cdate": 1761835271940, "tmdate": 1762915760181, "mdate": 1762915760181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the task of generating sewing patterns conditioned on an image of a person wearing a garment. To this end, a novel Real Garments Benchmark is established which re-purposes existing GarmentCode dataset to create pairs of realistic garment images and accompanying patterns. The garment images are obtained by taking GarmentCode models, rearranging them to different poses and augmenting the garments with textures obtained from real-world images and texture generators.\n\nAs for the method that generates sewing pattern descriptions based on images, a pipeline based on residual vector quantization and vision-language models is proposed. The sewing pattern is encoded by two resnet based networks: (i) the first resnet encodes points collected along individual panel edges, (ii) the second network encodes rotation and translation parameters of individual panels that are concatenated. These encodings are discretized with residual quantization. A decoder network reconstructs the panels based on the quantized representations.\n\nThe patterns may be represented with tokens: each garment/panel/edge is defined by the starting and ending tokens, and discretized representation allows for the physical properties to also turn into tokens. This representation enables training of vision-language models, where sewing pattern prediction becomes a prediction of the correct garment token sequence conditioned on the input image."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "[S1] The effort of creating a large scale dataset is appreciated, especially as good benchmarks drive development of better models.\n\n[S2] The idea of reformulating pattern descriptions as a sequence of tokens that gives a hierarchical representation of the garment is interesting."}, "weaknesses": {"value": "[W1] I am still a bit uncertain about the use case of generating a garment pattern from a single image of a person wearing the pattern. If there is a person wearing an item of clothing, that means that the pattern already exists. Also, it is impossible to predict panels on unseen regions of garments (e.g. the back). The success of the proposed method suggests an overfitting to the training dataset.\n\n[W2] Somewhat connected to the previous point, the paper would benefit from additional experiments stress testing the method and connecting it to potential real-world uses. E.g. what happens when the method is applied to real images of people in images captured in the wild? Are the produced patterns reasonable? Are the patterns rasonable for an identical garment worn by people of different sizes? How far can the edge of the panel be warped before it cannot be represented by the codebook? Will an edge be well represented if it is not in a training data (e.g. a model is trained on only straight edges, but a curvy edge appears in the test data)?\n\n[W3] The paper would benefit from qualitative results. I am especially not clear on what garment editing is or how it is evaluated.\n\n[W4] I find the dataset itself the biggest contribution of this paper, but it is very badly described (see questions)."}, "questions": {"value": "[Q1] Explaining some of the elements of the problem would make it more friendly for readers not in the domain: e.g. what is A-pose, rotation and translation parameters of the code and why they are important etc.\n\n[Q2] Data curation pipeline is badly described. The citations are not separated from the text (lines 283 - 300) and this should be fixed. I am still not completely clear on the way that the dataset was generated. For now it seems that these are synthetic images generated by a 3D simulation where the texture of the model and the garment may be controlled. I am not sure I would call this real human images, though they are probably better at capturing true garment behaviour. \n\n[Q3] Is there a theoretical limit to the edge variablity that may be expressed by the quantization approach?\n\n[Q4] Is this approach sensitive to the order of the panels during training? What indicates which patterns should be sewn together and how is that encoded?\n\n[Q5] Could the dataset be extended to include e-commerce image of the garment? That would make the dataset valuable for the further development of tasks such as Virtual Try-On and Virtual Try-Off."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vvWexquyBc", "forum": "XzXKnazRBF", "replyto": "XzXKnazRBF", "signatures": ["ICLR.cc/2026/Conference/Submission1396/Reviewer_EWew"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1396/Reviewer_EWew"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762255312956, "cdate": 1762255312956, "tmdate": 1762915760050, "mdate": 1762915760050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}