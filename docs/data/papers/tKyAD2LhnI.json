{"id": "tKyAD2LhnI", "number": 10281, "cdate": 1758165889474, "mdate": 1759897661277, "content": {"title": "SIGMark: Scalable In-Generation Watermark with Blind Extraction for Video Diffusion", "abstract": "Artificial Intelligence Generated Content (AIGC), particularly video generation with diffusion models, has been advanced rapidly. \nInvisible watermarking is a key technology for protecting AI-generated videos and tracing harmful content, and thus plays a crucial role in AI safety.\nBeyond post-processing watermarks which inevitably degrade video quality, recent studies have proposed distortion-free in-generation watermarking for video diffusion models.\nHowever, existing in-generation approaches are non-blind: they require maintaining all the message-key pairs and performing template-based matching during extraction, which incurs prohibitive computational costs at scale.\nMoreover, when applied to modern video diffusion models with causal 3D Variational Autoencoders (VAEs), their robustness against temporal disturbance becomes extremely weak.\nTo overcome these challenges, we propose SIGMark, a Scalable In-Generation watermarking framework with blind extraction for video diffusion.\nTo achieve blind-extraction, we propose to generate watermarked initial noise using a Global set of Frame-wise PseudoRandom Coding keys (GF-PRC), reducing the cost of storing large-scale information while preserving noise distribution and diversity for distortion-free watermarking.\nTo enhance robustness, we further design a Segment Group-Ordering module (SGO) tailored to causal 3D VAEs, ensuring robust watermark inversion during extraction under temporal disturbance.\nComprehensive experiments on modern diffusion models show that SIGMark achieves very high bit-accuracy during extraction under both temporal and spatial disturbances with minimal overhead, demonstrating its scalability and robustness.", "tldr": "", "keywords": ["watermarking", "video generation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7354410fdce6f539b4816ff885bd48da0dbc812d.pdf", "supplementary_material": "/attachment/1707e9014e92d6e37bf64db4270771b192338c05.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes SIGMark, a watermarking method for diffusion video generation. The key contribution is its \"blind\" feature that doesn't require storing the original watermark message for template matching during extraction. This is in contrast to existing methods such as VideoShield. To achieve this target, the authors build their solution based on the recently proposed PRC (pseudorandom code). Basically, PRC couples the message with some 'testbits' (part of the key) during encoding, and then compare testbits during decoding to verify whether decoded message is correct or not. The authors also propose a SGO module (optical flow segmentation + sliding window detection) to handle the attacks specific to video, e.g., frame adding, dropping, etc."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ I like the \"blind\" feature they add to diffusion-based video watermarking domain. It simplifies the watermarking deployment pipeline. PRC is a cryptographically strong primitive that enables encoding/decoding different messages with a single global key, in contrast to traditional encryption methods used in other watermarking methods that require storing different keying material for different messages.\n\n+ The proposed SGO module is effective in handling various frame-level attacks in videos.\n\n+ The experimental results are good. Although the robustness is not consistently better than existing methods, they have the unique advantage of being blind."}, "weaknesses": {"value": "- The technical contribution for watermark extraction should be articulated with more methodological comparison. (See my suggestion below)\n\n- Experimental settings are not very clear."}, "questions": {"value": "+ In Section 3.4, you should compare your method (optical flow + sliding window) to previous solutions (VideoShield, VideoMark) in a methodological way, in order to better understand your technical contribution. E.g., do they use any method to partition video to continuous subsequences? How? How do they detect the starting of a group (or do they need to detect this)? What's the main advantage of your solution?\n\n+ Line 345: symbol \\hat is at wrong place.\n\n+ In Table 2, we need more details on the setting of drop, insert, and clip. What's the drop ratio? What are inserted frames and insertion ratio?\n\n+ Running time overhead breakdown should be provided for the various steps in their system, e.g., diffusion, PRC encode/decode, optical flow segmentation, sliding window detection, etc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Pbem63MmMw", "forum": "tKyAD2LhnI", "replyto": "tKyAD2LhnI", "signatures": ["ICLR.cc/2026/Conference/Submission10281/Reviewer_TYUV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10281/Reviewer_TYUV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761024121019, "cdate": 1761024121019, "tmdate": 1762921634199, "mdate": 1762921634199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Video generation with diffusion models is advancing rapidly, and invisible watermarking is essential for provenance and IP protection. Existing in-generation schemes typically require storing message–key (or key–template) pairs and performing template-based matching at inference, which can become costly at scale and tend to be fragile under temporal disturbances. The paper proposes SIGMark, aiming for blind extraction via Global Frame-wise Pseudorandom-Coding keys (GF-PRC) and improved robustness via a Segment Group-Ordering (SGO) module tailored to causal 3D-VAEs, enabling reliable inversion under temporal perturbations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper identifies a practical limitation of many video in-generation watermarking systems: they are not truly blind yet require storing large message–key/template tables, which raises efficiency and storage concerns at scale.\n\n2.\tThe proposed design addresses both the blindness/scalability issue (via GF-PRC) and the temporal robustness issue (via SGO).\n\n3.\tThe paper is generally well organized with clear figures, which makes it easy to follow."}, "weaknesses": {"value": "1.\tLimited novelty (GF-PRC). The GF-PRC component mainly builds on the original PRC method. Moreover, introducing PRC (Appendix E) negatively affects robustness. How to balance the impact of PRC, or improve PRC specifically for watermarking robustness requirements remains an open problem.\n\n2.\tScalability evidence. The paper claims that non-blind approaches incur prohibitive computational costs at scale and raise efficiency/storage issues, but this discussion remains at the level of Appendix B (Computation Overhead). The authors should provide experiments to demonstrate that this is a serious practical problem and to show the operational performance of the proposed method.\n\n3.\tRobustness gap. Although the method claims robustness to temporal disturbance, its performance under frame drop is far below VideoShield, and it is overall worse than VideoShield under spatial disturbances. The paper attributes this to PRC, suggesting that the framework’s robustness is still not fully resolved."}, "questions": {"value": "1.\tSGO procedure clarity. In Figure 4, for z_1 there appear to be two candidates: [4, pad, pad, pad] or [pad, pad, 6, 7]. Which one is used for decoding? Or should it be [4, pad, 6, 7]? Please separately explain the frame-drop and clip cases so readers can better understand the SGO workflow.\n\n2.\tMeaning of “w/o” in Table 2. Does “w/o” denote a clean setting with no time disturbance? Please state this explicitly in the caption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LEZ74iR0il", "forum": "tKyAD2LhnI", "replyto": "tKyAD2LhnI", "signatures": ["ICLR.cc/2026/Conference/Submission10281/Reviewer_PfP7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10281/Reviewer_PfP7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761025402207, "cdate": 1761025402207, "tmdate": 1762921633722, "mdate": 1762921633722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SIGMark, a training-free, in-generation watermarking framework for video diffusion models. It addresses two issues of prior approaches: (i) non-blind extraction that scales not well because it requires storing message, and (ii) lacking temporal robustness when modern causal 3D VAEs are disturbed by frame edits. Experiments on HunyuanVideo and Wan-2.2 across T2V and I2V pipelines with a 400-video subset of VBench-2.0 show high accuracy with limited quality impact. Under both spatial and temporal perturbations, SIGMark achieves competitive with non-blind baselines, and outperforms prior non-blind in-generation methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-organized and clearly writtern. \n- The inituion of the paper is soundness. \n- Experimental results show the effectivness of the proposed method."}, "weaknesses": {"value": "- Missing references: \n[1] Huang, Huayang et al. “ROBIN: Robust and Invisible Watermarks for Diffusion Models with Adversarial Optimization.” ArXiv abs/2411.03862 (2024): n. pag.\n\nPlease also refer to the \"Questions\" section."}, "questions": {"value": "- What is the false-positive rate when decoding regenerations (video-to-video) from different diffusion models?\n- The paper claims O(1) extraction complexity and “near–real-time” segmentation. The appendix provides only a cursory analysis. Could you add end-to-end timings with per-component breakdowns, scaling with (d_t) and payload size, and comparisons to baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4qSthcAVI3", "forum": "tKyAD2LhnI", "replyto": "tKyAD2LhnI", "signatures": ["ICLR.cc/2026/Conference/Submission10281/Reviewer_1pSm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10281/Reviewer_1pSm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979731381, "cdate": 1761979731381, "tmdate": 1762921633290, "mdate": 1762921633290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}