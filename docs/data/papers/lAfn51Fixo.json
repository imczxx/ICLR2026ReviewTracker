{"id": "lAfn51Fixo", "number": 3199, "cdate": 1757361302631, "mdate": 1759898102419, "content": {"title": "Chimera: Compositional Image Generation  using Part-based Concepting", "abstract": "Personalized image generative models are highly proficient at synthesizing images from text or a single image, yet they lack explicit control for composing objects from specific parts of multiple source images without user specified masks or annotations. To address this, we introduce Chimera, a personalized image generation model that generates novel objects by combining specified parts from different source images according to textual instructions.\nTo train our model, we first construct a dataset from a taxonomy built on 464 unique <part,subject> pairs, which we term semantic atoms. From this, we generate 37k prompts and synthesize the corresponding images with a high-fidelity text-to-image model.\nWe train a custom diffusion prior model with part-conditional guidance, which steers the image-conditioning features to enforce both semantic identity and spatial layout. We also introduce an objective metric PartEval to assess the fidelity and compositional accuracy of generation pipelines. Human evaluations and our proposed metric show that Chimera outperforms other baselines by 14% in part alignment and compositional accuracy and 21% in  visual quality.", "tldr": "", "keywords": ["Image personalization", "Text-to-Image generation", "Part-based generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0090f8c3d709c62acede0325256e4dca822324ba.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Chimera, which is a framework for compositional generation based on parts. The authors propose a technique based on learning of IP-Adapters for each part, output of which is then composed to get the required image. The authors create a dataset for training of HiDream-11 model to generate the dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Paper is clearly written and is understandable.\n\nThe problem is interesting to solve."}, "weaknesses": {"value": "Results are Unnatural and contain artifacts: In Fig. 1, the results in rows 1,2, and 4 look quite unnatural; further, there are artifacts in the head region of Row 1.\n Novelty: I find the proposed approach to be very similar to the approach in Piece-It-Together. Hence, the novelty of the proposed method is unclear.\n\nSoundness: I find that the dataset for learning itself contains artifacts (Fig. 1, showing the aeroplane and sunflower). So unclear to me how it can be used to train a model to do a part composition task with bad supervision?\n\nThe performance of the Omni-Gen2 and Chimera is very similar. Hence, the exact advantage of the method needs to be clarified."}, "questions": {"value": "The part identity of the parts is not similar to the input images provided (Fig. 6). Could authors provide a reason for that?\n\nSome recent works like [21, 11, 15] are not considered as baselines for comparison. Could the authors provide reasoning for that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NAaijno7l6", "forum": "lAfn51Fixo", "replyto": "lAfn51Fixo", "signatures": ["ICLR.cc/2026/Conference/Submission3199/Reviewer_RB71"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3199/Reviewer_RB71"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761482478968, "cdate": 1761482478968, "tmdate": 1762916598328, "mdate": 1762916598328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Chimera, a pipeline for compositional image generation that aims to combine specified parts from different source images based on text prompts. The core methodology relies on three key components: (1) SAMv2 for identifying parts, (2) Diffusion Prior (IP-Prior) trained on a large, synthetic corpus of hybrid concepts, and (3) a decoder. Also, the authors propose a metric PartEval, using Gemini-Flash for evaluating semantic compositional quality of the generated images."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. By intelligently integrating a strong segmentation model (SAMv2), the approach eliminates the primary usability bottleneck for previous part-based generation models.\n2. Part-Eval provides a novel metrics that is essential for measuring the success of compositional tasks. This moves the field beyond general image quality metrics (like FID) which are insufficient for judging correct part-blending.\n3. Its interesting to see how the proposed approach successfully focuses the difficult learning task (compositional blending) onto a small, dedicated model (the IP-Prior) while leveraging the decoding power of a much larger, pre-trained model."}, "weaknesses": {"value": "1. An important step in validating the PartEval metric is missing. The paper does not report running PartEval against the HiDream-11 ground-truth images used for training. This omits the essential check required to verify that the Gemini pipeline accurately assigns a perfect score to the ideal compositional result (which is being used as ground truth during the training).\n2. SAMv2 obtained masks play a critical role in the practical utility of the proposed method. The authors should perform analysis demonstrating the IP-Prior's robustness to segmentation errors.\n3. The training structure suggests the model may be relying on memorized structural templates (slots for head, body, wheels, etc.) derived from the limited taxonomy, rather than learning general blending logic. The authors should potentially discuss its ability to extrapolate to novel or conflicting spatial configurations in order to prove the robustness of the approach.\n4. In the fig 6. qualitative results., some results aren't that good: e.g, in the first column, the generated image does seem to have impact from zebra's body (see the thicker black tail) and not just zebra's head. I would appreciate if the authors can provide an explanation to the same."}, "questions": {"value": "1. I wonder how reliable HiDream-11 generations would be? For the examples shown in figure 3, for the prompt mentioning a plant with petals of rose and leaves of sunflower the generated image also has the yellow petals from sunflower which may not be desirable.\n2. Is the IP-prior sensitive to perturbations/minor errors in the SAMv2 masks?\n3. I would appreciate if the authors can provide some intuition for how IP-prior would resolve potential spatial conflicts in a potential such scenarios when you have multiple input parts to be used in the composition. For instance, if the input had a cow and a horse and the user wants both of them to play a role in how the head of the compositionally generated object looks like, would the model perform some blending, or pick one of the two?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x5J0oM3XYI", "forum": "lAfn51Fixo", "replyto": "lAfn51Fixo", "signatures": ["ICLR.cc/2026/Conference/Submission3199/Reviewer_aNSa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3199/Reviewer_aNSa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883363637, "cdate": 1761883363637, "tmdate": 1762916595154, "mdate": 1762916595154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Chimera — a personalized image generation model that creates novel hybrid objects by combining parts from different input images. The main contribution of this paper is designing a part taxonomy that contains 464 unique <part, subject> semantic atoms. This taxonomy enables the generation of a large amount of part combination data. However, the paper lacks innovation in aspects such as model structure and experimental setup."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed ⟨part, subject⟩ taxonomy covers 6 semantic domains and 464 semantic atoms, avoiding the limitations of baselines (e.g., PartCraft is restricted to \"birds/dogs,\" and PiT focuses on \"toy creatures\"). \n- The paper proposes a new metric that leverages MLLMs to evaluate the part-influenced generation."}, "weaknesses": {"value": "- The second innovation claimed in the paper—\"The model does not require masks\"—is meaningless. This is because the authors utilize Grounded SAM for image segmentation, which offers no novelty whatsoever.\n- The paper provides insufficiently clear descriptions of the training details.\n- From a qualitative results perspective, the consistency between the parts of the images generated by the model and the input parts is not good."}, "questions": {"value": "- During model training, are the input part images segmented from ground truth images using Grounded SAM? If so, how to ensure the model's generalization ability?\n- Qualitatively, the generated images show very weak adherence to the input parts and text prompts in the 4-part compositions setting. This is particularly evident in the numerous cases presented in Figure 11, where the generated content barely captures the input features. A further explanation on this issue is expected."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CWvquBB79O", "forum": "lAfn51Fixo", "replyto": "lAfn51Fixo", "signatures": ["ICLR.cc/2026/Conference/Submission3199/Reviewer_uVkz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3199/Reviewer_uVkz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929113756, "cdate": 1761929113756, "tmdate": 1762916594220, "mdate": 1762916594220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a personalized image generation model that generates new objects by combining specified parts from different\nsource images according to textual instructions. A training dataset is built with <part, subject> pairs. A metric PartEval pipeline is introducted to assess the fidelity and compositional accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The task of compositional generatation of new objects by combining specified parts from different source images according to textual instructions is investigated.\n- The experiments are reported and analysis."}, "weaknesses": {"value": "- Qualitative results comparisons is insufficient. \n- Failure cases are not provided. \n- The number of compositional objects is small, restricting generalization to complex, real-world scenes with diverse element combinations. \n- The code is not provided,  hindering reproducibility and further validation of the proposed approach."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N25EIFqoyp", "forum": "lAfn51Fixo", "replyto": "lAfn51Fixo", "signatures": ["ICLR.cc/2026/Conference/Submission3199/Reviewer_JDyS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3199/Reviewer_JDyS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993116860, "cdate": 1761993116860, "tmdate": 1762916593958, "mdate": 1762916593958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}