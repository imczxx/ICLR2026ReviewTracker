{"id": "XG8wZ31oUE", "number": 16020, "cdate": 1758258690743, "mdate": 1759897267126, "content": {"title": "Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation", "abstract": "Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle with information-intensive images that densely interleave textual annotations with fine-grained graphical elements. \nThe main challenges lie in precisely localizing critical cues in dense layouts and multi-hop reasoning to integrate dispersed evidence.\nWe propose Speculative Verdict (SV), a training-free framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model. \nIn the draft stage, small VLMs act as draft experts to generate reasoning paths that provide diverse localization candidates; in the verdict stage, a strong VLM synthesizes these paths to produce the final answer, minimizing computational cost while recovering correct answers.\nTo further improve both efficiency and accuracy, SV introduces a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict.\nEmpirically, SV achieves consistent gains on challenging information-intensive and high-resolution visual question answering benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K. By synthesizing correct insights from partially accurate reasoning paths, SV achieves both error correction and cost-efficiency compared to large proprietary models or training pipelines.", "tldr": "Speculative Verdict is a training-free framework that combines lightweight draft experts with a strong verdict model to achieve accurate and efficient reasoning over information-intensive images.", "keywords": ["multimodal reasoning", "visual question answering", "vision-language model", "information-intensive images", "speculative decoding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6277c8c0ce206135fc6093427af8af11b7b277b1.pdf", "supplementary_material": "/attachment/565af04507bfbe02732564d9789886ccf76e5097.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SV, a training-free framework that combines multiple lightweight draft experts with a large verdict model. They use small VLMs to generate reasoning paths as candidates, and use strong VLMs to produce the final answer. They claim SV achieves consistent gains over many benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Enhancing visual reasoning ability is a highly important and actively studied research problem.\n\n2. The paper is easy to follow, the figures are clear, and the code is open-sourced.\n\n3. The experiments are detailed, and the ablation study in Section 4.4 is very comprehensive."}, "weaknesses": {"value": "1. My main concern with this work lies in the evaluation. If this issue can be properly addressed, I would be willing to raise my score. Currently, Table 1 compares the performance of several VLMs, and SV claims to outperform all of them. However, I would like to see a cost analysis. Although Appendix B provides some numerical results, it lacks a comparison of the computational cost between SV and the baseline methods. How much additional cost does SV introduce compared to the baselines? Under the same computational cost, would using only the verdict yield a similar effect? Likewise, under the same cost, would conducting debates among multiple draft models achieve comparable results?\n\n2. The literature review is missing several important works. In the area of vision-language model reasoning, this paper is not limited to tool-related methods; therefore, it should also include more general VLM reasoning studies and cite some representative works in that domain."}, "questions": {"value": "1. Address the issues mentioned in the weaknesses.\n\n2. The formatting on page 17 could be improved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gufh6dx6ff", "forum": "XG8wZ31oUE", "replyto": "XG8wZ31oUE", "signatures": ["ICLR.cc/2026/Conference/Submission16020/Reviewer_E8fc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16020/Reviewer_E8fc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761702552036, "cdate": 1761702552036, "tmdate": 1762926225734, "mdate": 1762926225734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the difficulty of VLMs in reasoning over information-intensive images that combine dense text and fine-grained graphics (e.g., infographics, charts), and introduces Speculative Verdict (SV), a training-free framework for information-intensive visual reasoning tasks. Inspired by speculative decoding, SV operates in two stages. The first is a draft stage where multiple lightweight VLMs generate diverse reasoning paths; and the second is a verdict stage where a large VLM synthesizes these paths to produce the final answer. The authors introduce a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict model. SV is evaluated on several benchmarks (InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K) and demonstrates consistent improvements over strong baselines while maintaining cost efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Originality: The paper presents a novel adaptation of speculative decoding for visual reasoning quality improvement rather than its original purpose of inference acceleration. \n\n- Quality: \n  1. The proposed approach is effective. \n  2. The experimental evaluation is comprehensive, covering multiple benchmarks and comparing against strong baselines. \n  3. The ablation studies provide insights into the importance of different components.\n\n- Clarity: \n  1. The paper is well-structured and clearly written. \n  2. The figures effectively illustrate key concepts and results. (BTW, I like the animal icons in Figure 2.)\n  3. The methodology is described in sufficient detail.\n\n- Significance: The paper addresses a significant challenge in multimodal AI with a cost-effective solution that outperforms more expensive alternatives, which has practical implications for deploying such systems."}, "weaknesses": {"value": "1. The tables lack clarity: The tables lack direct comparison between baseline and SV, such as directly providing the increment from GPT-4o (line 332) to SV+4o (line 341), and also the increment on Qwen2.5VL-72B, so that readers can easily compare the performance gain regarding different base models, instead of refering to other places in the paper (such as refering to line 377). Althought I like the figures and plots, the tables really need improving. \n\n2. Limited analysis of computational efficiency: This paper claimis that SV is more cost-efficient, but doesn't provide detailed metrics comparing computational costs (e.g., FLOPs, inference time, memory usage), and the budget cost.\n\n3. Restricted Draft Model Pool: The evaluation is restricted to a fixed draft model pool, limiting understanding of how SV would perform with a more diverse set of draft models.\n\n4. Insufficient Analysis of Failure Cases: The paper would benefit from a more detailed analysis of cases where SV fails to understand the approach's limitations.\n\n5. Limited Comparison to Advanced Ensemble Methods: The paper compares SV to majority voting but doesn't compare to more advanced ensemble methods that could be applied to this problem.\n\n6. Potential Overfitting to Specific Benchmarks: While evaluating on multiple benchmarks, they all focus on similar types of information-intensive visual reasoning, making generalizability unclear."}, "questions": {"value": "1. Why SV improves more dramatically with GPT-4o (11.9/6.6/11.4/4) compared to Qwen 72B (2.5/7.5/2.3/2.5), especially for InfographicVQA and ChartQAPro? \n\n2. Could you provide more detailed metrics on the computational efficiency of SV compared to baselines? For example, inference time, FLOPs, or memory usage would help quantify the efficiency claims.\n\n3. How sensitive is SV to the choice of draft models? Have you experimented with draft models of different sizes and architectures beyond the ones reported?\n\n4. In what types of cases does SV fail? A more detailed analysis of failure modes would help understand the limitations.\n\n5. How does SV compare to more advanced ensemble methods beyond majority voting? For example, methods that learn to weight or combine the outputs of multiple models.\n\n6.How well does SV generalize to other types of visual reasoning tasks beyond the information-intensive benchmarks evaluated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zEmKV9hSRV", "forum": "XG8wZ31oUE", "replyto": "XG8wZ31oUE", "signatures": ["ICLR.cc/2026/Conference/Submission16020/Reviewer_656R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16020/Reviewer_656R"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904766272, "cdate": 1761904766272, "tmdate": 1762926225156, "mdate": 1762926225156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses Large Vision-Language Models’ struggles with information-intensive images—difficulty localizing critical cues in dense layouts and multi-hop reasoning. It proposes Speculative Verdict (SV), a training-free framework combining lightweight draft experts and a large verdict model. Small VLMs generate diverse reasoning paths in the draft stage; a strong VLM synthesizes these for final answers in the verdict stage. SV adds a consensus expert selection to forward only high-agreement paths. Experiments show SV gains on benchmarks like InfographicVQA, achieving error correction and cost-efficiency vs. large models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) This paper accurately pinpoints VLMs’ core flaws in information-intensive images—poor dense cue localization and error-prone multi-hop reasoning—and clarifies limitations of existing solutions, ensuring relevance.\n2) The “Draft-Verdict” two-stage structure (lightweight experts for coverage + large VLM for synthesis) and consensus selection balance accuracy and efficiency, with clear alignment to solving target challenges.\n3) Experiments on diverse benchmarks (InfographicVQA, HR-Bench 4K) and comparisons with various baselines, plus error correction data (47-53%), fully validate performance and cost-efficiency."}, "weaknesses": {"value": "1) How about the comparison of the proposed method with specialized models? \n2) The inference speed is not presented in the experiments section. Does it add much computation cost to the baseline method thus slow down the inference1 speed, and if yes, could you give the speed?\n3) On HR-Bench 4K, SV w/ GPT-4o Verdict performs worse than SV w/ Qwen2.5-VL-72B-Instruct Verdict, and even worse than several Open-source VLMs, please explain why?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PlfNpOmZ5M", "forum": "XG8wZ31oUE", "replyto": "XG8wZ31oUE", "signatures": ["ICLR.cc/2026/Conference/Submission16020/Reviewer_Cnwz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16020/Reviewer_Cnwz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972633234, "cdate": 1761972633234, "tmdate": 1762926224089, "mdate": 1762926224089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a prompting framework for high-resolution image reasoning tasks, where small LVLMs generate draft reasoning trajectories and answers, and a large verdict model incorporates all reasoning and produces the final answers. The reasoning trajectories are selected based on the consensus score (i.e., the absolute difference between the model's own answer and the answer generated by the other model).  Experimental results show the framework achieves better results on high-resolution/dense-layout benchmarks such as InfoVQA and HRBench."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The investigated problem of solving dense-layout image reasoning tasks using ensemble learning is of great practical value.\n- The performance is promising, surpassing tool-based methods such as DeepEyes."}, "weaknesses": {"value": "- Unclear Connection to Speculative Decoding\nThe paper's framing as \"speculative decoding\" is confusing. Traditional speculative decoding aims at inference acceleration, whereas this work operates more as an LLM-as-a-Judge paradigm where candidate answers are evaluated by a verdict model. The paper lacks discussion and comparison with existing judging frameworks (e.g., [1, 2]), which weakens its positioning within the literature.\n\n- Limited Technical Contribution:\nViewing this work through the LLM-as-a-Judge lens, the technical novelty appears limited beyond modifying the aggregation process with consensus scores. Despite the ablation in Figure 7, several critical aspects remain underexplored:\n  - What is the distribution of agreement/disagreement among models?\n  - How does normalization affect the results? Could overconfident models skew the consensus?\n  - Since answers are generated with reasoning trajectories, does estimating NLL on answers alone introduce inaccuracy due to off-policy estimation?\n\n- Insufficient Motivation and Analysis:\nThe paper would benefit from deeper investigation to strengthen its claims:\n  - A detailed analysis of reasoning trajectory patterns across different models\n  - Exploration of whether using a smaller model as the verdict would yield similar performance gains\n  - Token efficiency comparison against standard LLM-as-a-Judge frameworks, since both approaches can leverage prefilling\n\n\n\n[1] LLaVA-Critic: Learning to Evaluate Multimodal Models, CVPR 2025\n[2] VL-RewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models, CVPR 2025"}, "questions": {"value": "## Questions\n\n- The reasoning trajectory analysis is interesting in Figure 1 and Section 3.2. I am wondering if there is any quantitative analysis of reasoning type distribution between draft models and their influence, e.g., which models are more complementary?\n\n- Why is the ablation study about the verdict scale only conducted on a subset of InfoVQA?\n\n- Will some of the small reasoning models produce a long reasoning trajectory, and then the final prefill stage exceeds the verdict model's length limit? \n## Format\n- Table 7 -> Figure 7."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7hfRzO6xAw", "forum": "XG8wZ31oUE", "replyto": "XG8wZ31oUE", "signatures": ["ICLR.cc/2026/Conference/Submission16020/Reviewer_65ys"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16020/Reviewer_65ys"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762077019154, "cdate": 1762077019154, "tmdate": 1762926223703, "mdate": 1762926223703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}