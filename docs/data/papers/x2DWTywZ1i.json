{"id": "x2DWTywZ1i", "number": 4664, "cdate": 1757739203076, "mdate": 1763677852172, "content": {"title": "SIGMA-GEN: STRUCTURE AND IDENTITY GUIDED MULTI-SUBJECT ASSEMBLY FOR IMAGE GENERATION", "abstract": "We present SIGMA-GEN, a unified framework for multi-identity preserving image generation. Unlike prior approaches, SIGMA-GEN is the first to enable single-pass multi-subject identity-preserved generation guided by both structural and spatial constraints. A key strength of our method is its ability to support user guidance at various levels of precision — from coarse 2D or 3D boxes to pixel-level segmentations and depth — with a single model. To enable this, we introduce SIGMA-SET27K, a novel synthetic dataset that provides identity, structure, and spatial information for over 100k unique subjects across 27k images. Through extensive evaluation we demonstrate that SIGMA-GEN achieves state-of-the-art performance in identity preservation, image generation quality, and speed.", "tldr": "SIGMA-GEN is a method for controllable, identity-preserving text-to-image generation with multiple subjects in one diffusion loop.", "keywords": ["image generation", "identity preservation", "controllable generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/49c753bbb53c517dd3c72e313c37e2d5fe595745.pdf", "supplementary_material": "/attachment/017541e0d440ebe048b60f56deab4b5c2136e8cf.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents SIGMA-GEN, a unified framework for multi-subject, identity-preserving image generation under flexible spatial and structural controls. The method allows users to specify both (a) subject identities using exemplar RGB images, and (b) structural arrangements through controls of varying granularity—from 2D bounding boxes to pixel-level depth maps and 3D layouts."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper elegantly bridges subject personalization and structural control, previously treated as separate problems.\n\n2. The routing–structure control representation is compact and adaptable to multiple input modalities (2D/3D/depth).\n\n3. The authors benchmark across single- and multi-subject settings, include runtime analyses, and evaluate both fidelity and identity preservation.\n\n4. SIGMA-SET27K is systematically generated with aligned modalities, providing a valuable resource for future research.\n\n5. The framework aligns well with creative workflows (scene layout, compositing, virtual try-on), making it broadly relevant beyond academic interest."}, "weaknesses": {"value": "1. The synthetic dataset may not reflect real-world visual variability; generalization to real photos is untested.\n\n2. All quantitative metrics are automated (DINO, SigLIP, MUSIQ); perceptual user studies would strengthen the claims.\n\n3. Some baselines (e.g., Insert Anything*) are inference-level adaptations, not retrained for fairness—potentially favoring SIGMA-GEN.\n\n4. The paper ablates over control granularity but not over architecture (e.g., without bidirectional compositing or without routing tokens)."}, "questions": {"value": "1. How does the model behave when trained on real datasets with authentic multi-person scenes (e.g., MS-COCO or Visual Genome)?\n\n2. Can the routing–structure image representation generalize to video or temporal settings?\n\n3. Are there failure cases when subject identities overlap or occlude one another heavily?\n\n4. How does the method handle domain shift between synthetic and real identities?\n\n5. Could the authors release data-generation scripts separately from trained models for transparency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A0CMI8rndy", "forum": "x2DWTywZ1i", "replyto": "x2DWTywZ1i", "signatures": ["ICLR.cc/2026/Conference/Submission4664/Reviewer_i8jB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4664/Reviewer_i8jB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760454168145, "cdate": 1760454168145, "tmdate": 1762917499309, "mdate": 1762917499309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Addressing reviewers' comments and invitation for discussion"}, "comment": {"value": "We would like to thank the reviewers for their valuable suggestions. The reviewers appreciated our novel data generation pipeline, the importance of the task being tackled, and the robustness and flexibility of SIGMA-Gen. We have now appended our previous supplementary material to the main pdf and also added new sections to the main pdf based on the suggestions by the reviewers. These new sections start from A.10. (page 21) and have the text “POST REVIEWS” prepended to the section names. We would like to address the most frequent concerns below:\n\n\n- **Novelty.**\nSIGMA-Gen is the first model that enables multi-subject identity preserved image generation while supporting diverse granularity levels of control. While our architectural changes are minimal, our novel and simple way of providing routing control to the model is highly effective. We also propose SIGMA-Set27k - a first of its kind dataset that includes up to 10 identities per image along with their references, structure and position information. \n\n- **Performance on real references.**\nWe would like to point to Figures 9,11,13 that we originally provided where we tested SIGMA-Gen for various scenarios using DreamBooth identities. We also included a new Figure 20 (Section A.11) where we include additional qualitative examples on DreamBooth and DeepFashion datasets’ real world reference images for multi-subject generation. While these highlight qualitative aspects, we conduct evaluation on the DreamBooth dataset for single-subject generation and discuss results in Section A.10. Both the qualitative and quantitative results point to SIGMA-Gen’s robustness to real references. We would also like to reiterate that we use the real domain AnyInsertion dataset for single subject generation as a part of our training data as we had previously stated in Section A.1.\n\n\n- **User study.**\nWe have designed and started conducting a human study for the precise masks and depth scenario to compare SIGMA-Gen with the baseline Insert-Anything*. We show our human study setup in Section A.16 (Figure 22). We will update the main pdf with the results of the study as soon as it is completed."}}, "id": "BbNnSPQnJy", "forum": "x2DWTywZ1i", "replyto": "x2DWTywZ1i", "signatures": ["ICLR.cc/2026/Conference/Submission4664/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4664/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4664/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763678451892, "cdate": 1763678451892, "tmdate": 1763678451892, "mdate": 1763678451892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce SIGMA-GEN, a unified framework designed for generating images containing multiple, specific identities. This method is presented as the first to achieve single-pass, multi-subject generation that preserves identity while adhering to both structural and spatial constraints provided by the user. A significant feature is the model's flexibility, allowing user guidance at various levels of precision (from simple bounding boxes to detailed segmentation maps) within one model. To accomplish this, the authors also created a large-scale synthetic dataset, SIGMA-SET27K, providing rich identity and spatial information. The paper claims state-of-the-art performance in identity preservation, image quality, and speed."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The primary strength is its reported ability to handle multi-subject identity preservation in a single pass while simultaneously respecting complex structural and spatial constraints. This addresses a major limitation in existing generative models.\n- The model offers significant practical utility by accepting a wide spectrum of user guidance—from coarse 2D/3D boxes to precise pixel-level maps—within a single, unified framework. This versatility makes it accessible for different use cases without needing specialized models.\n- The creation of the SIGMA-SET27K dataset is a valuable contribution in its own right. A large-scale synthetic dataset with comprehensive annotations for identity, structure, and spatial information is a key enabler for this type of complex, multi-modal training and will likely benefit the wider research community."}, "weaknesses": {"value": "- Lack of Clarity in Methodology and Reporting: The paper's presentation is not as clear as it could be. Specifically, the pipeline for constructing the SIGMA-SET27K dataset is underspecified; critical details about the tools and processing steps used are omitted, making the dataset's creation difficult to reproduce or fully evaluate. \n- Insufficient Experimental Baselines: The experimental comparison, while showing strong results against some methods, is not comprehensive. To truly validate the \"state-of-the-art\" claim, the evaluation needs to be broadened to include more recent and relevant open-source methods. A comparative analysis against top-tier commercial models (such as those from Google, e.g., gemini-2.5-flash-image-preview, or JiMengAI) is also necessary to properly contextualize the model's performance."}, "questions": {"value": "I am interested in the model's performance on more fine-grained, real-world customization tasks. Specifically, how effectively does this framework handle high-fidelity, instance-level customization, such as preserving the exact facial identity of a specific, real-world person provided by a user."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tDicaK7n5p", "forum": "x2DWTywZ1i", "replyto": "x2DWTywZ1i", "signatures": ["ICLR.cc/2026/Conference/Submission4664/Reviewer_UzUG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4664/Reviewer_UzUG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813203990, "cdate": 1761813203990, "tmdate": 1762917498601, "mdate": 1762917498601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SIGMA-GEN for multi-subject identity-preserving image generation with structural control at various granularities. The authors introduce SIGMA-SET27K, a synthetic dataset with 27k images containing up to 10 subjects, and demonstrate improvements over baselines especially in multi-subject scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Jointly controlling multiple subject identities and spatial layout at various granularities (2D boxes to pixel-level depth) with a single model addresses a practical need in creative workflows.\n\n- Strong quantitative results with 31 points improvement in fidelity and 4x speedup for 5+ subjects, with consistent gains across metrics.\n\n- Well-designed automatic data pipeline and comprehensive ablations provide good insights into component contributions.\n\n- Demonstrated versatility through applications like insertion, reposing, and mixed-granularity control."}, "weaknesses": {"value": "- Limited technical novelty since the core architecture directly adopts OminiControl's [1] unified attention mechanism without significant modification. The main contribution is dataset engineering rather than methodological innovation.\n\n- Training and evaluating entirely on synthetic data is problematic. You're essentially fitting to outputs from Flux Kontext [2] and other models [3,4], then testing on the same distribution. This doesn't demonstrate real-world generalization, and you should validate on existing benchmarks like DreamBooth [5] with real photographs. Errors from the generation pipeline also propagate into your model.\n\n- Critical technical details are underspecified. How does the model map routing mask intensities (10, 20, 30...) to identity image blocks - is this explicitly supervised or purely learned? The three-stage curriculum suggests fundamental scalability issues rather than unified learning. Why can't you train end-to-end?\n\n- Incomplete baseline comparisons. You cite MultiBooth [6] and other recent multi-subject methods but only compare against MSDiffusion [7]. Also adapting MSDiffusion to use filled masks instead of coordinate embeddings may unfairly disadvantage it. No user studies validate whether your automatic metrics correlate with human perception.\n\n- Training with only subject depths when Table 3 shows full depth improves results (SigLIP-T: 17.73→18.08) seems arbitrary. The bidirectional compositing for occlusions feels like a heuristic workaround rather than principled depth reasoning like InstanceDiffusion [8].\n\n### References\n\n[1] Tan et al., \"OminiControl: Minimal and Universal Control for Diffusion Transformer,\" arXiv 2024\n\n[2] BFLabs et al., \"Flux.1 Kontext: Flow Matching for In-Context Image Generation,\" arXiv 2025\n\n[3] Ren et al., \"Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks,\" arXiv 2024\n\n[4] Wang et al., \"MoGe-2: Accurate Monocular Geometry with Metric Scale,\" arXiv 2025\n\n[5] Ruiz et al., \"DreamBooth: Fine Tuning Text-to-Image Diffusion Models,\" CVPR 2023\n\n[6] Zhu et al., \"MultiBooth: Towards Generating All Your Concepts in an Image,\" AAAI 2025\n\n[7] Wang et al., \"MS-Diffusion: Multi-Subject Zero-Shot Image Personalization,\" arXiv 2024\n\n[8] Wang et al., \"InstanceDiffusion: Instance-Level Control for Image Generation,\" CVPR 2024"}, "questions": {"value": "- Can you evaluate on real-world data with actual photographs as identity images to validate generalization beyond the synthetic training distribution? How does performance compare when the identity images and test scenarios come from real captures rather than model-generated content?\n\n- Please clarify the identity routing mechanism mathematically and show whether the three-stage curriculum is necessary with ablations. Why not compare to other recent multi-subject methods you cite like MultiBooth, and can you provide user studies validating your metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "O6crfAZkkW", "forum": "x2DWTywZ1i", "replyto": "x2DWTywZ1i", "signatures": ["ICLR.cc/2026/Conference/Submission4664/Reviewer_cKMR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4664/Reviewer_cKMR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814240739, "cdate": 1761814240739, "tmdate": 1762917498321, "mdate": 1762917498321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The method focuses on the problem of fine-grained control in text-image generation. Existing methods support the function of controlling the single-subject structure or identity, but cannot handle the multi-subject structure and consistent identity with the accurate spatial layout. The proposed SIGMA-GEN proposed several modules to tackle the balance between accuracy and efficiency in multi-subject generation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strength:\n-\tThe paper is well-motivated with several technical challenges;\n-\tThe method proposed seems sound and correct.\n-\tThe new evaluation sub-benchmark is proposed with a pipeline for the dataset generation."}, "weaknesses": {"value": "Weakness: \n-\tIt seems confusing that which exact module corresponds to tackle the problem of multi-subject structure and identity. It seems that the proposed modules are not unique to this specific challenge. \n-\tThe novelty of the pipeline is limited. It seems integration of existing modules into a pipeline. Please directly compare with existing baseline methods and show the novelty of each module."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dRmU33xXnZ", "forum": "x2DWTywZ1i", "replyto": "x2DWTywZ1i", "signatures": ["ICLR.cc/2026/Conference/Submission4664/Reviewer_ETmR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4664/Reviewer_ETmR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884926325, "cdate": 1761884926325, "tmdate": 1762917497980, "mdate": 1762917497980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}