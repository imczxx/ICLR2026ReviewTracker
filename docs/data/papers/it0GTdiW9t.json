{"id": "it0GTdiW9t", "number": 9706, "cdate": 1758135703843, "mdate": 1759897703166, "content": {"title": "Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation", "abstract": "Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model’s role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.", "tldr": "Adaptive, geometry-aware diffusion for image translation, keeping cross-modal translations on-manifold and high-fidelity with far fewer steps.", "keywords": ["Generative Learning", "Imaging"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0044f09e1d12fa26a413a66db44bb39cc63d1d15.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a new diffusion-based framework (CDTSDE) for cross-modality image translation. The core idea is to embed the domain shift directly into the generative process, instead of relying on a global linear blend between source and target images. The authors introduce a spatial-channel varying weight matrix for combining source and target paired data, which serves as the condition in diffusion models. The forward and reverse sampling processes are developed. Extensive experiments across several applications are conducted."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-  The paper gives a convincing geometric argument for why globally linear / fixed schedules send trajectories off-manifold, creating large correction burdens and semantic drift, motivating the method. \n\n- Instead of a single scalar $\\eta_t$, $\\Lambda_t$ is a learnable pixelwise, channelwise field with monotonic constraints and boundary clamping. This is novel and intuitively matches heterogeneous cross-modal shifts (texture, contrast, anatomy), and is also grounded by the theoretical analysis.\n\n- Building on the pixelwise, channelwise weight $\\Lambda_t$, the authors developed the corresponding forward and reverse processes, enabling the idea to be realistic.\n\n- Experiments span three regimes of difficulty: (i) relatively mild contrast change (MRI T1↔T2), (ii) strong cross-sensor shift (SAR→optical), and (iii) extremely semantic mapping (electroluminescence image → defect mask). The proposed method shows convincing results across different tasks."}, "weaknesses": {"value": "- The core formulation assumes access to paired (source, target) images during training. However, many cross-modality settings (especially SAR↔optical, certain medical domains) are weakly paired or unpaired in practice. The method’s dependence on paired data limits its applicability, but this is not deeply analyzed. \n\n- The presentation is mathematically dense. Some key definitions (e.g. how $\\Lambda_t$ is actually predicted by the network in practice, how constraints like monotonicity in $t$ are enforced during training, and how the logistic squashing with $\\epsilon$ is implemented during backprop) are split across sections and the appendix. This may make reproduction harder for non-experts, despite the promise of releasing code later. Meanwhile, more explanation on the definition equations of $\\Lambda_t$ should be included to strengthen readability."}, "questions": {"value": "- Your method conditions directly on paired $(\\hat{x}_0, x_0)$ and uses $x_0$ explicitly in the adaptive mixture at training time. How sensitive is CDTSDE to imperfect pairing or slight misregistration, especially for SAR→optical and PSCDE where pixel alignment can be noisy? Have you tried training with synthetically perturbed / misaligned pairs to evaluate robustness? \n\n- Many real cross-modality scenarios do not provide paired data (e.g. historical SAR ↔ optical, multi-scanner MRI). Can your formulation be adapted to unpaired or weakly paired data, or is the approach fundamentally limited to paired supervision? Please clarify what breaks if $x_0$ is not available at training time, at least in discussion.\n\n- You compare a global linear schedule $\\eta_t$ vs. your spatially varying $\\Lambda_t$ and report gains (e.g. Dice 0.46→0.49, Hausdorff 59.5→39.8 on PSCDE). Could you also report an intermediate baseline: a per-channel but spatially uniform schedule (i.e. $\\Lambda_t$ depends on channel and $t$, but not $(p)$)? This would help isolate whether most of the benefit comes from spatial adaptivity or just from deviating from a single scalar $\\eta_t$.\n\n- Theorem 1 proves that allowing pixelwise $\\Lambda(t)$ yields strictly lower path energy $E[d]$ than any global schedule under heterogeneity assumptions. How should we interpret this physically?   Is lower path energy empirically correlated with perceptual realism / fewer artifacts?  Do you ever observe cases where $\\Lambda_t$ lowers energy but produces locally inconsistent textures (i.e. visually implausible mixing of source and target in the same region)?\n\n- In which regimes does CDTSDE not help? For IXI (milder contrast shift), you mention improvements are “marginal.” Can you show qualitative counterexamples where CDTSDE produces artifacts (hallucinated structures, topology breaks, etc.) so we understand the limits of the method? \n\nI look forward to the response from the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wV5cRa1UuC", "forum": "it0GTdiW9t", "replyto": "it0GTdiW9t", "signatures": ["ICLR.cc/2026/Conference/Submission9706/Reviewer_jiJC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9706/Reviewer_jiJC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9706/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829331903, "cdate": 1761829331903, "tmdate": 1762921212130, "mdate": 1762921212130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Cross-Domain Translation SDE (CDTSDE) for Cross-modal image-to-image translation. CDTSDE embeds an adaptive, spatially varying mixing field into the diffusion process. The forward marginal is centered on a per-pixel blend of source and target, and the reverse dynamics add a restoration drift aimed at keeping large steps near the data manifold. Reported results show improved PSNR/SSIM and PSCDE on several datasets."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "– Across three modalities/datasets, the method reports better SSIM/PSNR and strong PSCDE structure metrics. \n\n– Concept of putting domain-shift adaptation inside the dynamics (rather than as external guidance) is principled."}, "weaknesses": {"value": "– The paper is difficult to follow: many equations and notations are introduced without clear motivation or explanation of each component’s role, making the method hard to reconstruct; it focuses more on how things are done than why they are needed.\n\n– The propose solution is fairly incremental since the core contribution is to propose an adaptive interpolation of $\\hat{x}^{src}_0$ and $x_0$, replacing time-varying interpolation of source and target [1]\n\n– Efficiency claims are supported largely by empirical numbers; there is little analysis explaining why the proposed dynamics/sampler should be more efficient.\n\n– The setting effectively reduces to paired translation. A substantial line of prior work on diffusion bridge models already targets this regime and reports SOTA results (e.g., [2,3,4,5]), but these are neither discussed nor compared.\n\n– Fixing a number of training steps and concluding CDTSDE is more efficient may be unfair: different methods have different designs/optimizers. The paper should provide training curves (and, ideally, wall-clock) to show faster convergence."}, "questions": {"value": "– Which aspects of the mixing field matter most (spatial vs. channel, monotonicity, positional encoding)? Please include ablations. \n\n– How does the method compare directly to recent DBM baselines on the same splits? \n\n– What drives the reported efficiency—the restoration drift, the step schedule, or early stopping? \n\n– Please provide training curves for the method and baselines.\n\n[1] Cui, et al. \"Taming Diffusion Prior for Image Super-Resolution\nwith Domain Shift SDEs\", NeurIPS 2024\n\n[2] Zhou, et al. \"Denoising Diffusion Bridge Models\", ICLR 2024\n\n[3] He, Guande, et al. \"Consistency diffusion bridge models.\" NeurIPS 2024\n\n[4] Liu, et al. “I$^{2}$SB: Image-to-Image Schrödinger Bridge”, ICML 2023\n\n[5] Zheng, et al. “Diffusion Bridge Implicit Models”, ICLR 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ta2cGB0DPq", "forum": "it0GTdiW9t", "replyto": "it0GTdiW9t", "signatures": ["ICLR.cc/2026/Conference/Submission9706/Reviewer_dYd8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9706/Reviewer_dYd8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9706/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892152064, "cdate": 1761892152064, "tmdate": 1762921211905, "mdate": 1762921211905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “Adaptive Domain Shift in Diffusion Models via Latent-Space Normalization” proposes a training framework to mitigate the performance degradation of diffusion models under domain shift scenarios. The authors argue that existing fine-tuning or adapter-based methods fail to preserve generative quality when target-domain data are scarce or stylistically distinct. To address this, they introduce an adaptive latent-space normalization (ALSN) module that aligns latent statistics between source and target domains through learned normalization parameters, dynamically adjusted during training. Experiments on multiple domain adaptation benchmarks (Art→Photo, Synthetic→Real) show modest improvements in FID and CLIP similarity compared to baseline fine-tuning or LoRA-based adaptation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Domain adaptation for diffusion models is a growing and challenging topic, and addressing domain shift in generative tasks is an important research direction, especially as diffusion models become widely deployed across diverse domains.\n+ The proposed ALSN approach is computationally efficient and easy to integrate into existing diffusion pipelines, making it potentially attractive for practitioners seeking domain-robust generative models."}, "weaknesses": {"value": "- The core idea, i.e., adjusting latent normalization statistics to align source and target distributions, closely resembles well-known techniques in domain adaptation. The paper primarily recontextualizes these ideas within diffusion models without offering substantial theoretical or methodological innovation. This limits the paper’s conceptual contribution.\n- While results are shown, the paper does not convincingly explain why ALSN improves performance or how it interacts with diffusion timesteps and denoising dynamics. There is no analysis of latent trajectory behavior, feature drift, or stability to justify its effectiveness.\n- Improvements in metrics such as FID or LPIPS are small (often within variance range), raising doubts about the real impact. The method’s simplicity is a strength, but it also highlights how incremental the advance is."}, "questions": {"value": "Have you evaluated whether ALSN affects the diversity or mode coverage of generated samples, especially when applied across multiple distinct domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h2NDrhaJNb", "forum": "it0GTdiW9t", "replyto": "it0GTdiW9t", "signatures": ["ICLR.cc/2026/Conference/Submission9706/Reviewer_Tz66"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9706/Reviewer_Tz66"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9706/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980534495, "cdate": 1761980534495, "tmdate": 1762921211543, "mdate": 1762921211543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CDTSDE (Cross-Domain Translation SDE), a novel diffusion-based framework for cross-modality image translation. The key idea is to embed adaptive domain-shift dynamics directly into the diffusion process via a spatially varying mixing field that evolves throughout reverse-time sampling. This dynamic field replaces the conventional fixed linear interpolation between source and target domains, allowing geometry-aware, low-energy paths that stay closer to the data manifold. Extensive experiments on three benchmarks show consistent gains in various matrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-motivated and conceptually clear. It generalizes traditional linear domain-shift formulations to a nonlinear, manifold-aware framework, supported by solid theoretical analysis and proofs.\n\n2. The method is technically sound and demonstrates strong generality, making it straightforward to apply across different diffusion architectures and cross-modality image translation tasks.\n\n3. Extensive experiments on three benchmarks IXI, Sentinel, and PSCDE show consistent gains in various metrics, outperforming GANs (Pix2Pix) and diffusion baselines (BBDM, DOSSR)."}, "weaknesses": {"value": "1. The method reduces to a linear domain-shift scheme when only a single diffusion step is used, implying that it still depends on multiple denoising steps for stable performance. This reliance could pose an efficiency limitation and hinder direct adaptation to flow-matching or single-step generative methods.\n\n2. While the method shows clear conceptual advances, its quantitative improvements over DOSSR on the Sentinel and IXI datasets are relatively minor, indicating that the advantages may be limited in tasks with larger modality discrepancies."}, "questions": {"value": "Please refer to weakenesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZLYf4jyXYI", "forum": "it0GTdiW9t", "replyto": "it0GTdiW9t", "signatures": ["ICLR.cc/2026/Conference/Submission9706/Reviewer_4o8z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9706/Reviewer_4o8z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9706/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982503968, "cdate": 1761982503968, "tmdate": 1762921211303, "mdate": 1762921211303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}