{"id": "bMrH2PFMsi", "number": 3880, "cdate": 1757559832289, "mdate": 1759898064885, "content": {"title": "CoNavBench: Collaborative Long-Horizon Vision-Language Navigation Benchmark", "abstract": "Vision-and-Language Navigation (VLN) primarily focuses on a single-agent-centric approach that executes human instructions step-by-step. In real environments with high demand or parallel workflows, collaboration VLN offers distinct benefits including shorter makespan and greater robustness through parallelism and role specialization. Collaboration VLN also brings new challenges including congestion, handoff errors, and rendezvous timing, which single-agent formulations overlook. Current datasets and protocols remain single-agent centered, which hides opportunities for assistance and ignores inter-robot interference. We fill this gap with Collaborative Long-Horizon VLN benchmark (\\textbf{CoNavBench}), consisting of 4048 single and collaborative episodes with graph-level annotations and a collaboration type taxonomy that controls handoff styles and rendezvous patterns. To generate and evaluate at scale, we build \\textbf{NavCraft}, an automated graph-grounded data generation platform. A two-stage hierarchical agent first produces a long-horizon base mission for the primary robot and then instantiates helper robots, allocates subgoals, and specifies validated handoffs and rendezvous. The agents operate with a scene graph in the loop derived from Habitat-Sim, which enables reachability checks, travel time, and interference assessment, and iterative schedule repair via an efficiency tool library. As a reference, we provide a collaborative baseline based on a finetuned Qwen2.5-VL-3B with a memory-aware mechanism. Trained with CoNavBench, collaborative policies reduce makespan and improve reliability over strong single robot counterparts, yielding \\textbf{18.11\\%} step level success. Anonymous Website: https://navcraft.github.io.", "tldr": "", "keywords": ["collaborative vision-and-language navigation; LLM agents; benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa53a2ee785cb959a58a10718c31598d298efc3b.pdf", "supplementary_material": "/attachment/869b5ce14d8ce028f8e908d98d71423c5425ff4c.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces CoNavBench, a large-scale benchmark designed to evaluate collaborative vision-and-language navigation (VLN) under multi-agent, long-horizon settings. The authors further propose NavCraft, a graph-grounded data generation and validation platform that constructs semantically annotated scene graphs, generates single-agent base tasks (NavCraft-S), and lifts them into collaborative multi-robot schedules (NavCraft-C). The experiments use Qwen2.5-VL-3B/7B models fine-tuned on the dataset, demonstrating improved performance over single-robot baselines and validating the benefit of collaboration in task success and path efficiency. Overall, this work establishes an interesting benchmark for multi-agent embodied navigation by unifying data generation, simulation, and evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper convincingly identifies a major gap in the VLN landscape: the lack of standardized evaluation for multi-agent, cooperative scenarios, and fills it with a well-structured benchmark and taxonomy.\nNavCraft’s design (scene-graph generation, task validation, and efficiency tools) is technically detailed, modular, and reproducible within Habitat-Sim, enabling scalable and context-aware task generation.\nThe framework successfully merges semantic graph annotation, spatial reasoning, and LLM-driven language prompts to produce feasible, diverse, and verifiable tasks.\nFinetuned Qwen2.5-VL models show consistent improvements in SR, SPL, and CSR across both single- and multi-agent settings, confirming the utility of CoNavBench for training collaborative policies."}, "weaknesses": {"value": "*Limited collaboration scope*\n\nOnly two fixed handoff types (A1, A2) are explored, which may not fully capture the diversity of real collaborative behaviors (e.g., concurrent exploration, dynamic task reassignment).\n\n*Data generation*\n\nThe dependence on GPT-4o-mini and closed APIs limits long-term reproducibility; no quantitative analysis is provided on the variability or bias of generated instructions.\n\n*Lack of detailed time-efficiency evaluation*\n\nAlthough collaboration is claimed to reduce makespan, this paper needs more clarification on collaboration acceleration and time-based metrics.\n\n*Incomplete ablations for important modules*\n\nComponents such as the memory-aware mechanism, efficiency tool library, and profile-conditioned sampling are presented but not individually quantified in their contributions."}, "questions": {"value": "Besides the weakness mentioned above, the reviewer has some extra questions below:\n\nHow does the benchmark handle multi-agent interference and collision during simulation? Are these events explicitly annotated or only indirectly measured by task failure?\n\nCould the authors provide a quantitative comparison between NavCraft-generated and manually designed tasks to justify realism and linguistic fidelity?\n\nWhy does the Qwen2.5-VL-3B outperform 7B in several collaborative tasks? Does this stem from optimization saturation or data scarcity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uZTkShLjBY", "forum": "bMrH2PFMsi", "replyto": "bMrH2PFMsi", "signatures": ["ICLR.cc/2026/Conference/Submission3880/Reviewer_wsr9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3880/Reviewer_wsr9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3880/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761560086739, "cdate": 1761560086739, "tmdate": 1762917080026, "mdate": 1762917080026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. CoNavBench Benchmark: It includes 4048 single-robot and multi-robot collaborative tasks, designed to test how robot collaboration can optimize task completion time and improve efficiency in long-horizon tasks.\n\n2. NavCraft Platform: An automated data generation platform for collaborative tasks that uses the scene graph from Habitat-Sim to create semantically rich environmental layouts and generate collaborative tasks. It includes two stages of task generation: NavCraft-S (for generating single-robot tasks) and NavCraft-C (for generating collaborative tasks).\n\n3. Collaborative Models and Performance Evaluation: Experiments using the Qwen2.5-VL-3B model show that the collaborative model improves step-level success by 18.11% and reduces task completion time compared to the single-robot model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "By introducing multi-robot collaboration, the CoNavBench benchmark optimizes the completion time of long-horizon tasks. Compared to single-robot tasks, collaborative robots can perform multiple subtasks simultaneously, effectively reducing task delays and idle time, significantly improving overall task efficiency. The CoNavBench benchmark includes 4048 single-robot and multi-robot collaborative tasks, providing a comprehensive evaluation of collaborative navigation systems in complex tasks. It tests how robots coordinate and distribute tasks in scenarios where multiple robots work together."}, "weaknesses": {"value": "1. The inference speed of large models is very slow; will there be a significant delay in task completion?\n2. Will the two agents collide in the environment?\n3. There are many VLN methods based on large models [1]; why were they not compared with these methods?\n4. Are the initial positions of the two agents the same, or is one agent directly summoned?\n\n\n\n\n[1] Cheng, An-Chieh, et al. \"Navila: Legged robot vision-language-action model for navigation.\" arXiv preprint arXiv:2412.04453 (2024)."}, "questions": {"value": "same as weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tIpwxv50jo", "forum": "bMrH2PFMsi", "replyto": "bMrH2PFMsi", "signatures": ["ICLR.cc/2026/Conference/Submission3880/Reviewer_UuGT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3880/Reviewer_UuGT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3880/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725697271, "cdate": 1761725697271, "tmdate": 1762917079841, "mdate": 1762917079841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CoNavBench, a collaborative long-horizon Vision-and-Language Navigation (VLN) benchmark designed to study multi-robot cooperation under language-guided tasks. It extends prior single-agent datasets by supporting multi-agent task decomposition, handoff scheduling, and rendezvous-based cooperation. To generate the data, the authors build NavCraft, a graph-grounded generation platform with semantic scene graphs, hierarchical planning agents (NavCraft-S / NavCraft-C), and an on-graph efficiency tool library for validation and optimization. Experiments demonstrate reduced makespan and improved step-level success over single-agent baselines using Qwen2.5-VL policies."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. CoNavBench provides a large-scale benchmark (4048 episodes) covering both single and multi-robot navigation with rich annotations for collaboration types and performance metrics.\n2. The proposed NavCraft system establishes a structured data-generation pipeline grounded in semantic graphs, incorporating task synthesis, collaboration lifting, and efficiency validation, which enhances data consistency and scalability.\n3. The framework introduces a useful utility for checking reachability, congestion, and timing constraints directly on scene graphs, which is a practical contribution for large-scale synthetic task design.\n4. The manuscript provides dataset statistics, training configurations, and baselines clearly, allowing others to replicate and evaluate under consistent protocols."}, "weaknesses": {"value": "1. Visual clarity is not acceptable at all, for example, Figure 9 is difficult to interpret and provides almost no new information beyond stating that the training loss decreases. It lacks analytical insight and does not meaningfully support the main claims. The visual examples fail to illustrate collaborative interaction or path optimization clearly. Comparisons are blurry or incomplete. An example is the visualization of collaborative cases, the authors didn't provide any useful information in these cases to support the claim.\n2. While the data-generation pipeline heavily relies on prompt engineering for NavCraft, there is no systematic or theoretical analysis of its design choices, prompt sensitivity, or ablation, leaving uncertainty about robustness.\n3. The entire framework and experiments are conducted in simulation (e.g., Habitat-3). Although acceptable at this stage, no real-world experiments or transfer evaluations are included to verify deployability or generalization."}, "questions": {"value": "1. Could authors provide more clear visualization results to clarify the claims, for example, how Figure 9 contributes analytically, whether it reflects convergence stability, mode collapse, or loss behavior differences between different models?\n2. Are there plans to improve qualitative visualizations to more clearly demonstrate cooperative efficiency or multi-agent coordination?\n3. Have you analyzed how different prompt templates or role-conditioning strategies affect the generated tasks’ diversity or validity?\n4. Is there any plan to evaluate NavCraft or CoNavBench tasks on real robots or real-world scans to assess practical applicability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There is no ethics concern from the reviewer."}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vyirJddCbK", "forum": "bMrH2PFMsi", "replyto": "bMrH2PFMsi", "signatures": ["ICLR.cc/2026/Conference/Submission3880/Reviewer_7PLf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3880/Reviewer_7PLf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3880/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958524414, "cdate": 1761958524414, "tmdate": 1762917079340, "mdate": 1762917079340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce CoNavBench, a new benchmark for \"Collaborative Long-Horizon Vision-and-Language Navigation\" where multiple robot agents collaborate to follow instructions inside 3D simulated homes (using Habitat-Sim). They introduce Navcraft, an algorithm to generate these multi-robot cooperation scenes where agents share and hand off subtasks. It contains 4048 episodes annotated with scene graphs, but more can be generated by Navcraft. Navcraft uses a two-stage hierarchy, Navcraft-S and NavCraft-C, to first generate a single-robot task, then break it down into a two-robot collaborative task, with graph-based scene understanding and validation. The authors fine-tune Qwen2.5-VL models on this benchmark, reporting improvements over single-agent baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is a novel benchmark for robotic cooperation in 3D home environments. The proposed mehtod can generate an arbitrary number of scenarios. The paper presents a clear and interesting pipeline (NavCraft-S and NavCraft-C) for task creation, graph annotation and validation. It is overall clear with illustrative figures. The authors use standard metrics for measuring performance on the benchmark, and provide a thorough baseline and ablation analysis with various LLM APIs."}, "weaknesses": {"value": "- The benchmark, while innovative, is still limited to two agents and specific relay-style tasks (one robot carries something from A to B, hands it off to a second robot to carry it from B to C). Dynamic collaborations are not explored. \n- NavCraft relies on proprietary LLM APis for data generation, which can lead to issues for reproducibility of the benchmarking. This limitation is addressed by the authors. \n- Some parts are dense in formulas and could benefit from a high-level intuitive explanation beforehand."}, "questions": {"value": "- Can the author expand on how do the results vary based on the chosen collaboration type (A1 vs A2) ? \n- How does this approach (generate single-agent task then split) compare against directly generating a two-agent task? \n- Authors report using two robots for the task instead of a single one improves performance. This can be explained by each robot being able to learn its task more efficiently. Do the authors believe this difference would disappear with SOTA large language models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "geyTAwZxx4", "forum": "bMrH2PFMsi", "replyto": "bMrH2PFMsi", "signatures": ["ICLR.cc/2026/Conference/Submission3880/Reviewer_cERd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3880/Reviewer_cERd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3880/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984732052, "cdate": 1761984732052, "tmdate": 1762917079163, "mdate": 1762917079163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}