{"id": "vwqnUTh2eU", "number": 14748, "cdate": 1758242989282, "mdate": 1762976721840, "content": {"title": "VTBench: Evaluating Visual Tokenizer for Autoregressive Image Generation", "abstract": "Autoregressive (AR) models have recently shown strong performance in image generation, where a critical component is the visual tokenizer (VT) that maps continuous pixel inputs to discrete token sequences. The quality of the VT largely defines the upper bound of AR model performance. However, current discrete VTs fall significantly behind continuous variational autoencoders (VAEs), leading to degraded image reconstructions and poor preservation of details and text. Existing benchmarks focus on end-to-end generation quality, without isolating VT performance. To address this gap, we introduce VTBench, a comprehensive benchmark that systematically evaluates VTs across three core tasks: Image Reconstruction, Detail Preservation, and Text Preservation, and covers a diverse range of evaluation scenarios. We systematically assess state-of-the-art VTs using a set of metrics to evaluate the quality of reconstructed images. Our findings reveal that continuous VAEs produce superior visual representations compared to discrete VTs, particularly in retaining spatial structure and semantic detail. In contrast, the degraded representations produced by discrete VTs often lead to distorted reconstructions, loss of fine-grained textures, and failures in preserving text and object integrity. Furthermore, we conduct experiments on GPT-4o image generation and discuss its potential AR nature, offering new insights into the role of visual tokenization. We release our benchmark and codebase publicly to support further research and call on the community to develop strong, general-purpose open-source VTs.", "tldr": "We introduce VTBench, a comprehensive benchmark designed to evaluate VTs across multiple dimensions, including three core tasks: Image Reconstruction, Detail Preservation, and Text Preservation, and covers a diverse range of evaluation scenarios.", "keywords": ["Visual Tokenizer", "Autoencoder", "Autoregressive modeling", "Image Generation", "VQVAE", "VAE", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/0b05cfbee23c393454f531e86b5327bce8554c95.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper posits that the visual tokenizer (VT) is a primary performance bottleneck for autoregressive image generation models. The authors construct a new benchmark, VTBench, to systematically evaluate VTs in isolation, a task not covered by existing end-to-end generation benchmarks. VTBench comprises three tasks—Image Reconstruction, Detail Preservation, and Text Preservation—designed to stress-test VTs under diverse conditions (e.g., high/varying resolution, multilingual text). The experimental results, covering a wide range of VTs from recent models, demonstrate a consistent and significant performance gap between discrete tokenizers and continuous VAEs or the proprietary GPT-4o system. The work concludes by releasing the benchmark to spur community effort in developing improved open-source VTs."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Problem Formulation: The paper identifies and formalizes a critical, yet overlooked, evaluation gap in the literature. Isolating the VT component for analysis is a necessary step for principled progress in AR generative models, and this paper provides the first comprehensive framework to do so.\n2. Benchmark Rigor: The proposed VTBench is thorough. Its multi-task structure, especially the inclusion of challenging sub-tasks like high-density academic text and non-Latin scripts, goes beyond standard ImageNet-based reconstruction and provides a much more demanding and realistic testbed for modern multimodal requirements.\n3. Empirical Contribution: The extensive empirical study provides a clear and valuable snapshot of the current state of visual tokenization. The finding that virtually all open-source discrete VTs fail significantly on complex tasks is a strong, sobering result for the community.\n4. Resource Provision: The public release of the benchmark, dataset, and codebase is a significant contribution that will lower the barrier for future research and enable standardized comparisons."}, "weaknesses": {"value": "1. Analysis is Diagnostic, not Prescriptive: The paper provides an excellent diagnosis of the problem but offers limited insight into why certain designs fail and what specific architectural principles lead to success. The analysis primarily ranks models but does not deeply correlate architectural choices (e.g., quantization method, downsampling factor, use of hierarchy) with specific outcomes in VTBench. This leaves the reader understanding the problem but with little concrete guidance on how to build a better VT.\n2. Potential for Dataset Bias: The use of GPT-4o to synthesize several of the more challenging evaluation sets (e.g., for detail and text preservation) introduces a potential methodological confounder. As the internal architecture of GPT-4o is unknown but hypothesized to be AR-based, the data it generates may have an inherent distributional bias that favors certain types of architectures over others. A discussion of this limitation is warranted. The evaluation tasks, particularly for text and detail, are performed in a \"clean room\" environment. The text is computer-rendered, and the images are either high-quality photographs or pristine synthetic generations. This overlooks the primary challenge of real-world vision: handling noise, degradation, and unpredictable variations (e.g., motion blur, compression artifacts, handwritten text, low lighting). A VT optimized for perfect reconstruction on clean data might be brittle and fail catastrophically on noisy, real-world inputs. \n3. Lack of Human-in-the-Loop Evaluation: The paper relies exclusively on automated metrics. For tasks like Detail Preservation, metrics such as LPIPS are known to be imperfect proxies for human perceptual judgment. While understandable for a large-scale benchmark, a small-scale user study to validate whether the metric-based rankings align with human perception would have made the conclusions on reconstruction quality more robust.\n4. Attribution of GPT-4o's Performance: The paper attributes GPT-4o's strong performance to a superior VT. While plausible, this is an inference. The performance could also stem from a powerful diffusion-based decoder that is exceptionally good at correcting tokenization artifacts, or a combination of both. The paper could be more precise by framing this as a hypothesis and discussing alternative explanations."}, "questions": {"value": "1. From your extensive analysis, can you distill any specific design principles? For instance, does residual quantization (as in VAR/Infinity) consistently outperform other methods across all tasks, or does it excel in detail preservation at the cost of, say, text fidelity? A more fine-grained \"if you need X, prioritize Y\" analysis would be highly valuable.\n2. Regarding the GPT-4o generated data: Could you discuss the potential for dataset bias? Have you considered validating the key findings from these sub-tasks on a small, curated set of real-world images (e.g., from technical manuals or artistic photography) to ensure the conclusions generalize beyond GPT-4o's output distribution?\n3. You hypothesize that GPT-4o may use an RVAE-like tokenizer. If an open-source model were to adopt such an architecture, what, in your view, would be the most significant remaining challenge to closing the performance gap? Is it purely a matter of scale, or are there other fundamental architectural or training subtleties that VTBench might not capture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EEbCPvtNSz", "forum": "vwqnUTh2eU", "replyto": "vwqnUTh2eU", "signatures": ["ICLR.cc/2026/Conference/Submission14748/Reviewer_WWhR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14748/Reviewer_WWhR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760883714945, "cdate": 1760883714945, "tmdate": 1762925107587, "mdate": 1762925107587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "ogZtmSdLQ6", "forum": "vwqnUTh2eU", "replyto": "vwqnUTh2eU", "signatures": ["ICLR.cc/2026/Conference/Submission14748/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14748/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762976721085, "cdate": 1762976721085, "tmdate": 1762976721085, "mdate": 1762976721085, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VTBench, a benchmark designed to evaluate the reconstruction accuracy of visual tokenizers used in autoregressive models. VTBench consists of three tasks: Image Reconstruction, Detail Preservation, and Text Preservation. Two evaluation metrics CER (Character Error Rate) and WER (Word Error Rate) are introduced to measure text reconstruction ability of visual tokenizers. Experimental results show that the reconstruction performance of publicly available visual tokenizers for autoregressive models remains inferior to that of continuous visual tokenizers used in diffusion models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to follow.\n- The proposed metrics CER and WER can effectively measure the text reconstruction performance of visual tokenizers.\n- Experiments covered a wide range of visual tokenizers used in autoregressive image generation."}, "weaknesses": {"value": "- While detail preservation is an important aspect of visual tokenizers as highlighted in the paper, the evaluation metrics used for the detail preservation task in Table 2 remain traditional ones such as PSNR, SSIM, LPIPS, and FID. These metrics may not effectively capture the nuanced differences between visual tokenizers in their ability to preserve fine details.\n- The dataset construction heavily relies on images generated by GPT-4o, including high-resolution images for Task 1, images for Task 2, and movie posters for Task 3. Although GPT-4o is a powerful image generation model, its outputs may not match the quality of real-world images, particularly in detailed structures and textual content. This reliance on generated data could introduce biases into the benchmark and potentially affect the validity of subsequent evaluations and developments.\n- The technical contribution of the paper appears to be limited. The introduced Reconstruction task (Task 1) overlaps with existing benchmarks, and no new evaluation metrics are proposed for the newly introduced Detail Preservation task.\n- While reconstruction accuracy is an important ability of visual tokenizers which sets the upperbound of generation models, differences in latent space structure can also significantly influence downstream generation quality and thus deserve further attention."}, "questions": {"value": "Please refer to the weakness session."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ryW6UgbbJo", "forum": "vwqnUTh2eU", "replyto": "vwqnUTh2eU", "signatures": ["ICLR.cc/2026/Conference/Submission14748/Reviewer_tgCE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14748/Reviewer_tgCE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636463412, "cdate": 1761636463412, "tmdate": 1762925107031, "mdate": 1762925107031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VTBench, a benchmark for evaluating Visual Tokenizers (VTs) in Autoregressive (AR) image generation. It isolates VT performance through three tasks Image Reconstruction, Detail Preservation, and Text Preservation covering scenarios such as high/variable resolutions, text-rich layouts, and multilingual scripts. Experiments on various SOTA VTs show that discrete VTs lag far behind continuous VAEs in fidelity, detail, and text accuracy, especially under complex conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a clear gap in AR image generation by introducing VTBench, the first dedicated benchmark for evaluating visual tokenizers. The benchmark design is systematic and covers diverse, realistic scenarios, including high-resolution, variable-resolution, and multilingual text inputs. The experimental evaluation is thorough, uses appropriate quantitative metrics (e.g., CER, WER), and provides insights into the limitations of current discrete VTs."}, "weaknesses": {"value": "While VTBench is valuable, the paper scope is mostly limited to reconstruction metrics and high-level qualitative analyses; less emphasis is placed on why certain VT architectures fail and how they could be improved beyond generic suggestions. The benchmark depends heavily on GPT-4o-generated datasets, which may introduce biases and limit applicability in other domains."}, "questions": {"value": "Since VTBench relies heavily on GPT-4o generated data, how do you assess potential bias in high-resolution or multilingual scenarios, and would results differ with real-world images? Could the dataset and evaluation be expanded with a more fine-grained categorization, for example, sub-categories within each task (e.g., different types of textures, object classes, or text styles)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MZwbxDDtYD", "forum": "vwqnUTh2eU", "replyto": "vwqnUTh2eU", "signatures": ["ICLR.cc/2026/Conference/Submission14748/Reviewer_Jsih"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14748/Reviewer_Jsih"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762066209235, "cdate": 1762066209235, "tmdate": 1762925106566, "mdate": 1762925106566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Problem.** Autoregressive (AR) image generators hinge on the **visual tokenizer (VT)**, yet current VTs may lose structure, fine detail, and text, and most benchmarks don’t isolate VT quality. **Method.** The paper proposes **VTBench**, isolating VT performance via three tasks—Image Reconstruction, Detail Preservation, and Text Preservation—spanning ImageNet, 1024×1024 “high-res,” varying resolutions, and multilingual text; metrics include PSNR/SSIM/LPIPS/FID and OCR-based CER/WER (Fig. 3). **Key innovations.** A VT-only evaluation protocol with diverse conditions, a curated high-frequency detail set, and text-heavy (English/multilingual) suites. **Main results.** Across Tables 1–3 and qualitative figs., discrete VTs lag far behind continuous VAEs (e.g., FLUX.1-dev/SD3.5L) in reconstruction/detail/text; many discrete VTs fail on arbitrary resolutions. **Significance.** If adopted broadly, VTBench could clarify VT bottlenecks for AR pipelines and catalyze stronger, reusable open VTs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Clear scope separation.** Evaluates the VT in isolation with well-defined tasks/metrics (Fig. 3; §3).\n- **Comprehensive scenarios.** Includes 1024×1024 and varying resolutions; multilingual and dense-text images (Tables 1–3; Figs. 4–5).\n- **Consistent, interpretable findings.** Continuous VAEs strongly outperform discrete VTs in fidelity and text preservation (Tables 1–3)."}, "weaknesses": {"value": "1) **Fairness across VT capacity/bit-rate is unclear.**  \nTable 1 mixes tokenizers with very different parameter counts and latent granularities (e.g., 54 M→946 M; multiple quantizers/codebooks) but does not **bit-rate-match** or control for tokens-per-image. Without rate control (bits/pixel) or matched FLOPs/latents, conclusions like “continuous VAEs are better” risk capacity confounds. Please add **bit-rate-controlled** comparisons and per-model tokens-per-image/FLOPs/latency. (Table 1; §4.1).\n\n2) **High-resolution benchmark uses GPT-4o-generated images as “ground truth.”**  \nYou synthesize 100× 1024² images with GPT-4o for Tasks 1–2 detail sets (and posters/abstracts in Task 3), but GPT-4o’s pipeline is undisclosed and may introduce **style/anti-aliasing biases** that favor continuous decoders. Please justify this choice with an **independent high-res source** (e.g., DIV2K/DF2K) and report sensitivity of results to the source generator. (Task definitions; §3.1–3.3).\n\n3) **Resolution robustness conflates *acceptance windows* with quality.**  \nTable 1 marks many discrete VTs as “unsupported” at 1024 or variable size; Fig. 4 reports semantic failures when forced. This blends **architectural constraints** (fixed input grid) with **reconstruction quality**. Please separate: (a) acceptance (can the VT ingest the size?) vs. (b) quality conditional on legal sizes. Provide per-VT legal-size baselines and an **oracle resize** control. (Fig. 4; §4.1).\n\n4) **Text preservation pipeline depends on a single OCR backend.**  \nCER/WER rely on a single OCR; while an appendix claims robustness, the main text lacks head-to-head with alternate OCRs or **human adjudication**. Please add PaddleOCR/Tesseract/TrOCR ablations and inter-annotator agreement for a subset; report κ/CI. (Table 3; §4.3).\n\n5) **Metric choice may penalize slight misalignments.**  \nFID on reconstructions (not generations) is sensitive to resizing/codec choices; PSNR/SSIM are brittle to tiny shifts. You include LPIPS, which helps, but **DISTS/LPIPS-Alex/VGG variants** and **precision/recall** for texture content would strengthen claims. Please add these, plus **per-class** breakdowns for patterned/detail images. (Task 2; Table 2).\n\n6) **Lack of variance and statistical testing.**  \nCore tables report single numbers; improvements are large for VAEs but smaller among discrete VTs (e.g., MaskBiT 16→18 bit, Titok variants). Add **μ±σ across ≥3 seeds**, paired tests, and **effect sizes**; include CIs for CER/WER. (Tables 1–3).\n\n7) **Causality between VTBench and end-to-end AR quality is asserted, not shown.**  \nIntroduction claims VTs “define the upper bound” of AR performance, but there’s no **correlation study** linking VTBench scores to downstream AR generation metrics across a fixed AR model. Please provide correlation (Spearman/Pearson) between VTBench and end-to-end GenEval/T2I-compbench on the same AR backbones. (Abstract/Intro; Fig. 1).\n\n8) **Heterogeneous preprocessing & resizing policies.**  \nTask 1 mixes center-crop to model-specific size, fixed 1024², and “native” varying resolutions; different VTs may use different antialias filters. Please tabulate **exact resize/antialias** per VT and add a **shared canonical preproc** control. (Task 1; §3.1; Table 1).\n\n9) **Use-case coverage is narrow for documents/UI.**  \nText-heavy sets use posters/abstracts; real-world documents (scans, forms, receipts, UI screenshots) differ in fonts/noise. Please add at least one **document/UI** corpus and report CER/WER by font size and stroke width. (Task 3; Table 3).\n\n10) **Reproducibility details need hard artifacts.**  \nYou promise an anonymized repo and eventual open-sourcing; for review, please release **frozen configs, seeds, exact prompts, generated image hashes**, and scripts to re-mint the synthetic sets. (Ethics/Reproducibility).\n\n11) **Interpretation of GPT-4o results is speculative.**  \nThe paper hypothesizes AR aspects of GPT-4o; while interesting, this belongs in Appendix with clearer caveats and should not anchor the benchmark narrative. Please move speculation and add **ablation removing GPT-4o-sourced content** from Tasks 2–3 to test sensitivity. (Intro/Discussion)."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GfjA4ENtGp", "forum": "vwqnUTh2eU", "replyto": "vwqnUTh2eU", "signatures": ["ICLR.cc/2026/Conference/Submission14748/Reviewer_T18B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14748/Reviewer_T18B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762078683764, "cdate": 1762078683764, "tmdate": 1762925106116, "mdate": 1762925106116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}