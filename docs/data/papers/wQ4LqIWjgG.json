{"id": "wQ4LqIWjgG", "number": 24331, "cdate": 1758355526267, "mdate": 1759896771055, "content": {"title": "The Quiet Prompt: Erasing Ineffable Styles from Diffusion via Concept Embedding", "abstract": "Recent text-to-image diffusion models (DMs) depend on vast, uncurated datasets that often include copyrighted or personal images, risking the generation of unwanted content. Fully curating these datasets is costly, and retraining the entire DMs from scratch is impractical. To address this, concept erasure methods have emerged to suppress undesirable outputs in pre-trained models without additional training. However, because these methods solely rely on text prompts for erasure, they are largely limited to concepts that can be explicitly described in words. This raises an important question: how can we erase highly personalized or visually nuanced concepts that are difficult to articulate verbally? To tackle this issue, we leverage fewer than ten reference images to derive a unified concept embedding that seamlessly captures even ineffable styles or content. During inference, the concept embedding serves as negative guidance—facilitating both the precise removal of complex visual moods and comprehensive erasure of target concepts without verbose prompts. Our method consists of two stages: (1) Concept embedding generation, which compresses the color, texture, and morphology of an unwanted style into a single latent vector; and (2) Concept‐aware negative guidance, which uses the extracted concept embedding as a semantic anchor to steer the diffusion process away from the undesirable concept without requiring model retraining. Building on these components, we propose Quiet Prompt (QuP), a method that simultaneously delivers consistent style removal—effectively erasing complex styles without verbose text—and comprehensive concept removal, allowing broad specification of target concepts without enumerating multiple synonyms or descriptions. Extensive evaluations on idiosyncratic and historical art styles, as well as object removal tasks, demonstrate that our method achieves superior performance in both unwanted style suppression—measured by a novel style score—and content preservation, outperforming state-of-the-art text-based baselines.", "tldr": "We propose a novel method that effectively removes a target style using only a few reference images.", "keywords": ["Text-to-image generation", "Generative AI", "Diffusion model", "Concept erasure", "Style removal"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/438ad41e472e7537c87470d57dff564cf922369a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Quiet Prompt (QuP), a reference image-based concept erasure method designed to address the ineffectiveness of text-based negative prompts in removing complex and \"ineffable\" styles. The method learns a target concept embedding, $v^*$, by performing a process similar to Textual Inversion (termed Concept Embedding Generation, CEG) on a small set of reference images. Subsequently, during the inference stage, this embedding is incorporated as negative guidance (Concept-aware Negative Guidance, CNG) into a modified Classifier-Free Guidance (CFG) formulation. The authors demonstrate its qualitative advantages on various style and concept removal tasks and introduce a \"style score\" for evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.  The method is novel. By using image references, it bypasses the limitations of text-based prompts, enabling the capture and suppression of complex style details that are difficult to describe accurately with language.\n2.  The proposed method demonstrates superior performance in eliminating style residues and artifacts, with experimental results showing significant improvements over text-based approaches.\n3.  The paper introduces two new metrics, $S_{sty}$ and $S_{obj}$, to measure the effectiveness of concept erasure."}, "weaknesses": {"value": "1.  The paper's claim of being \"training\\-free\" (line 102) is potentially misleading, as the process of generating the concept embedding vector ($v^\\*$) from a few reference images is not entirely training\\-independent.\n2.  It is unclear if the model, once deployed, can only operate with pre\\-learned $v^\\*$ embeddings. In other words, to erase a new, previously unseen concept, is it necessary to collect corresponding reference images and retrain (or fine\\-tune) a new $v^\\*$ ?\n3.  The method's robustness to **non\\-target attributes** (e.g., watermarks, background textures, or irrelevant objects) within the reference images is a concern. If such details are inadvertently encoded into $v^\\*$, how does the method ensure the embedding captures only the target concept and not these coincidental features? During inference, such a \"contaminated\" $v^\\*$ might lead the model to erase both the target style and irrelevant background or objects, causing an over-erasure problem.\n4.  The newly proposed metrics, $S_{sty}$ and $S_{obj}$, appear to be primarily based on empirical design. The paper lacks sufficient theoretical derivation or statistical justification to fully support their validity and interpretability."}, "questions": {"value": "See Strengths and Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jvbkxKYxiR", "forum": "wQ4LqIWjgG", "replyto": "wQ4LqIWjgG", "signatures": ["ICLR.cc/2026/Conference/Submission24331/Reviewer_NwdU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24331/Reviewer_NwdU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761413162870, "cdate": 1761413162870, "tmdate": 1762943045727, "mdate": 1762943045727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Quiet Prompt (QuP), a technique for concept erasure in diffusion models that avoids full retraining. QuP introduces two components: Concept Embedding Generation (CEG), which learns a latent embedding representing the concept to erase via Textual Inversion, and Concept-aware Negative Guidance (CNG), which uses this embedding as a negative semantic direction during diffusion sampling. The authors evaluate QuP on style, object, and NSFW content removal tasks and report improvements over existing unlearning baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method integrates seamlessly with existing diffusion pipelines.\n2. Enables selective removal of copyrighted or unsafe styles.\n3. Demonstrates quantitative and qualitative gains over SLD, ESD, and AdvUnlearn."}, "weaknesses": {"value": "1. \"Training-free\" claim is misleading: The paper repeatedly emphasizes that the proposed approach is “training-free.” However, the Concept Embedding Generation (CEG) step clearly involves gradient-based optimization to learn new embedding vectors, which constitutes a form of training—even if limited in scope. This characterization risks misleading readers. Furthermore, the paper omits any discussion of computational cost, convergence behavior, or sensitivity to hyperparameters (e.g., learning rate, number of iterations).\n\n2. Lack of theoretical grounding: The method assumes that concept embeddings learned through textual inversion reside in the same semantic space as pre-trained text embeddings, but this assumption is not substantiated. There is no theoretical justification or empirical validation (e.g., t-SNE visualization, cosine similarity analysis, or linear probing) demonstrating that these learned embeddings meaningfully align with the textual semantic manifold.\n\n3. Limited generalization across diffusion models. All experiments are conducted exclusively on Stable Diffusion v1.4, which limits the generality of the claims. There is no evidence that the method transfers to more recent or structurally different text-to-image models (e.g., SDXL, DeepFloyd, or non-CLIP-based backbones).\n\n4. Poor robustness and scalability in multi-concept and adversarial scenarios. The method shows substantial degradation in multi-concept removal tasks (Table 16), suggesting limited scalability beyond single-concept editing. In addition, robustness to prompt evasion and adversarial reformulations (e.g., synonym substitution, blended or indirect prompts) is not evaluated. This omission raises concerns about the reliability of the proposed approach in real-world or safety-critical contexts."}, "questions": {"value": "1. How consistent are CEG embeddings across seeds or reference sets?\n2. Is negative guidance applied dynamically per-step or statically precomputed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3DrAFeKzVa", "forum": "wQ4LqIWjgG", "replyto": "wQ4LqIWjgG", "signatures": ["ICLR.cc/2026/Conference/Submission24331/Reviewer_iRPL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24331/Reviewer_iRPL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761446809225, "cdate": 1761446809225, "tmdate": 1762943045336, "mdate": 1762943045336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper raises an important question: how to erase personalized or purely visual concepts that lack clear textual descriptions. To tackle this challenge, the authors propose a new erasure framework capable of handling visually grounded and implicit concepts. Their method enables targeted removal of visual attributes while maintaining the model’s generative quality. \nThe proposed method consists of two stages: (1) Concept embedding generation, which compresses the color, texture, and morphology of an unwanted style into a single latent vector; and (2) Concept-aware negative guidance, which uses the extracted concept embedding as a semantic anchor to steer the diffusion process away from the undesirable concept without requiring model retraining.\nExperiments demonstrate that the proposed approach achieves more precise and controllable erasure compared to existing text-based methods. The work provides a practical and scalable solution to improve the safety of text-to-image diffusion models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-structured and easy to follow.\n\n2. The proposed embedding-based concept removal approach appears novel.\n\n3. Extensive experimental results demonstrate that the method achieves superior performance."}, "weaknesses": {"value": "1. First, I believe the proposed method does not clearly demonstrate its necessity in the context of style removal. As shown in Figures 3, 4, 6, and 7, style removal often leads to a loss of contextual fidelity. If removing the style also alters the image content, then the proposed method becomes less meaningful, since one could simply use a text prompt to regenerate a style-free image instead.\n\n2. The paper does not compare or discuss existing works on style–content disentanglement, which are directly relevant to this task.\n\n    [1] Uncovering the disentanglement capability in text-to-image diffusion models.\n\n    [2] Less is More: Masking Elements in Image Condition Features Avoids Content Leakages in Style Transfer Diffusion Models.\n\n    [3] Stylediffusion: Controllable disentangled style transfer via diffusion models.\n\n    [4] Not only generative art: Stable diffusion for content-style disentanglement in art analysis.\n\n\n3. The proposed approach relies on additional style reference images. It remains unclear how the method can effectively perform style removal when no explicit style reference is available.\n\n4. In Line 102, the authors claim the method is training-free, but it is not explained how $v_{\\star}$ is obtained.\n\n5. Can the proposed method simultaneously perform both style removal and object removal?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mbY9qEGCtv", "forum": "wQ4LqIWjgG", "replyto": "wQ4LqIWjgG", "signatures": ["ICLR.cc/2026/Conference/Submission24331/Reviewer_rnM8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24331/Reviewer_rnM8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718302050, "cdate": 1761718302050, "tmdate": 1762943044965, "mdate": 1762943044965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for removing specific styles or concepts from pre-trained diffusion models. The key feature of the proposed method is that it leverages multiple images containing the target concept, rather than relying on text. The method consists of two main components: (1) Concept Embedding Generation, which embeds the target concept as soft tokens derived from multiple images, and (2) Concept-aware Negative Guidance, which prevents the generation of the target concept by subtracting the output associated with the soft tokens. Experimental results demonstrate that, compared with existing concept removal methods, the proposed method more effectively removes the target concept while better preserving other information."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors propose a style removal method that utilizes multiple images containing the target concept rather than text. While text-based removal methods such as Negative Prompting and Safe Latent Diffusion struggle with precise style specification, the proposed method enables detailed style removal by employing images.\n\n2. Another advantage of concept removal using images is robustness against rephrasing. Concept removal using text may generate the intended image if synonyms of the target text are input. In contrast, the proposed method uses images, allowing it to avoid generating the image even when synonyms are input.\n\n3. Experiments demonstrate that the proposed method removes style and concept more efficiently than existing methods. Style and concept removal techniques are crucial technologies for copyright protection and avoiding inappropriate content, indicating significant impact."}, "weaknesses": {"value": "1. The difference between the proposed \"Concept embedding generation\" and Textual Inversion (Gal et al., 2022) is unclear. The Textual Inversion paper demonstrates the ability to convert concepts and styles from multiple images into token embeddings. The authors should clarify this distinction.\n\n2. The difference between the proposed \"Concept-aware negative guidance\" and the methods \"Negative Prompt\" and \"Safe Latent Diffusion\" (Schramowski et al., 2023) is unclear. Both Negative Prompt and Safe Latent Diffusion, like the proposed method, aim to prevent the generation of specific concepts by negatively applying classifier-free guidance. The authors should clarify how their approach differs from these existing methods.\n\n3. Whether removing a specific style influences other styles has not yet been verified. For instance, [1] investigates the effect of removing the \"Van Gogh\" style on the \"Picasso\" and \"Monet\" styles, and reports that Negative Prompt and Safe Latent Diffusion negatively impact the \"Picasso\" and \"Monet\" styles. Similar experiments should be performed to ensure that the proposed method does not unintentionally degrade other styles.\n\n[1] Wang, Yuan, et al. \"Precise, fast, and low-cost concept erasure in value space: Orthogonal complement matters.\" 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2025."}, "questions": {"value": "Please add explanations regarding the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xuoAN8xyN8", "forum": "wQ4LqIWjgG", "replyto": "wQ4LqIWjgG", "signatures": ["ICLR.cc/2026/Conference/Submission24331/Reviewer_8PGp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24331/Reviewer_8PGp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727885264, "cdate": 1761727885264, "tmdate": 1762943044504, "mdate": 1762943044504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Quiet Prompt (QuP), a reference-based concept-erasure method for text-to-image diffusion. From fewer than ~10 reference images, it learns a compact concept embedding via textual inversion and then uses a concept-aware negative guidance during sampling to suppress that concept without retraining the base model. QuP targets “ineffable” styles (hard to describe in text) as well as objects and NSFW categories, and evaluates across style/object/NSFW erasure with quantitative metrics and user studies."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Clear, practical mechanism (few-shot embedding + negative guidance) that’s easy to plug into existing T2I pipelines; strong qualitative illustrations.\n- Broad evaluations: idiosyncratic & art styles, objects, and NSFW; combines quantitative metrics (FID/CLIP, counts) and user preference.\n- Competitive numbers vs text-based baselines (e.g., SD-Neg/SLD) and useful ablations on data efficiency, robustness to reference sets/seeds, and cross-backbone transfer to SD2.1/SDXL."}, "weaknesses": {"value": "- Novelty positioning needs sharper contrast. The core recipe—learn a concept embedding with textual inversion, then use it as a negative condition—sits close to prior personalization and prompt-based erasure. Please provide a crisper head-to-head against text-only negatives and image-guided negatives to establish distinctiveness.\n  - [1] https://arxiv.org/abs/2211.12572\n  - It represents the strongest “image-guided” alternative to your reference-learned negative. If QuP can erase the style/object as well as or better than PnP while preserving unrelated content (and with lower inference overhead), that cleanly establishes distinctiveness.\n- Style-score validation. The proposed style metric is sensible but would benefit from correlation to human preference and a comparison to alternative style-distance measures (e.g., Gram/feature stats, Style-Aligned).\n- Scope beyond SD1.4. Cross-model results (SD2.1/SDXL) live mostly in the appendix; consider elevating at least one row to the main tables to support generality claims."}, "questions": {"value": "- The claims regarding the comment\n  - It represents the strongest “image-guided” alternative to your reference-learned negative. If QuP can erase the style/object as well as or better than PnP while preserving unrelated content (and with lower inference overhead), that cleanly establishes distinctiveness.\n- Why does a reference-learned concept embedding outperform (i) text-only negative prompts and (ii) image-guided negatives (e.g., CLIP-image embedding as negative)? Please add direct controls on the same splits as Table 1.\n- Can you report correlation between your style distance and user preference (Fig. 14), and compare the distance to Gram/feature-statistic or Style-Aligned alternatives?\n- For object erasure, can you show attention/attribution maps indicating suppression stems from the negative concept embedding rather than prompt drift (ties to Tables 2 & 8)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RAGHow6R26", "forum": "wQ4LqIWjgG", "replyto": "wQ4LqIWjgG", "signatures": ["ICLR.cc/2026/Conference/Submission24331/Reviewer_yDAR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24331/Reviewer_yDAR"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission24331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933847400, "cdate": 1761933847400, "tmdate": 1762943043964, "mdate": 1762943043964, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method for removing hard-to-describe or abstract concepts from text-to-image diffusion models using a few reference images. It first learns a textual embedding through textual inversion from these images, then applies negative guidance with the learned embedding to suppress the target concept during generation. The approach also generalizes to standard objects, styles, and NSFW content. For evaluation, the paper fine-tunes a DreamBooth LoRA on the same reference images to learn the target style or concept, and then tests the proposed removal method on it"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Being able to remove abstract concepts is indeed useful and practical. It’s often hard to describe concepts just via text and using techniques like textual inversion to first learn a concept embedding is both intuitive and shown to be effective by the paper. \n2. The paper is well written and easy to understand"}, "weaknesses": {"value": "1. Negative guidance for concept removal has been explored (e.g., SD-Neg, SLD, Dynamic Negative Guidance). The main addition here is combining it with textual inversion. It would be great if the paper explores some unique advantages that using reference images provides—e.g., finer disentanglement of attributes (stroke style vs. color palette, content vs. texture). Demonstrating user-mentioned partial removals would make the contribution stronger. \n2. The qualitative and quantitative evaluation can be improved. Figure 4 only shows the qualitative comparison with SLD and SD-Neg and not other fine-tuning-based methods. Also, adding comparison to more recent baselines, such as EraseAnything or MACE, in Table 2 would be great. Prior works (e.g., CA [Reverse KL divergence, Sec. 4.3]) also demonstrate concept removal using real reference images; adding a comparison to that would clarify the value of using real reference images vs. negative guidance for concept removal. \n3. Does the method still work for guidance distilled models like FLUX.1-dev or few-step distilled models? \n\nMinor points: \n1. The paper first uses DreamBooth LoRA to train a model on abstract concepts and then uses their method to remove it. However, it would be more convincing to show the efficacy of the method on abstract styles that can be generated by the model itself. \n2. It would be great to discuss the choice to measure KL divergence (Eq. 5) in VAE feature space rather than CLIP/DINO feature space. \n3. Does the choice of reference images have any effect on the final performance, e.g., using a different set of reference images for DreamBooth LoRA training and the textual inversion part, and a different set of prompts for NSFW removal than I2P prompts?\n \n[1] Dynamic Negative Guidance of Diffusion Models [https://openreview.net/pdf?id=vvBAZJh2nQ]   \n[2] EraseAnything: Enabling Concept Erasure in Rectified Flow Transformers"}, "questions": {"value": "Please look at the weakness section, particularly regarding any unique advantage that the use of textual inversion with real reference images provides in the proposed method"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vsv1XrEFDQ", "forum": "wQ4LqIWjgG", "replyto": "wQ4LqIWjgG", "signatures": ["ICLR.cc/2026/Conference/Submission24331/Reviewer_xvUR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24331/Reviewer_xvUR"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission24331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762069318888, "cdate": 1762069318888, "tmdate": 1762943043747, "mdate": 1762943043747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}