{"id": "vpKXTmMtBQ", "number": 18627, "cdate": 1758289594759, "mdate": 1759897090939, "content": {"title": "Model Merging Scaling Laws in Large Language Models", "abstract": "We study empirical scaling laws for language model merging measured by cross-entropy. \nDespite its wide practical use, merging lacks a quantitative rule that predicts returns as we add experts or scale the model size. \nWe identify a compact power law that links model size and expert number: the size-dependent floor decreases with model capacity, while the merging tail exhibits clear diminishing returns in the number of experts. \nThe law holds in-domain and cross-domain, tightly fits measured curves across diverse architectures and methods (Average, TA, TIES, DARE), and explains two robust regularities: most gains arrive early, and variability shrinks as more experts are included. \nBuilding on this, we present a simple theory that explains why gains fall roughly as \\(1/k\\) and links the floor and tail to properties of the base model and the diversity across domains. This law enables \\emph{predictive planning}: estimate how many experts are needed to reach a target loss, decide when to stop adding experts, and trade off scaling the base model versus adding experts under a fixed budget—turning merging from heuristic practice into a computationally efficient, planable alternative to multitask training.", "tldr": "", "keywords": ["Scaling Laws", "Model Merge", "LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d805b4fcfc5d9481c790841aeee99d53c4a2567d.pdf", "supplementary_material": "/attachment/986e27ad2990ddd4dccc97ddaeab2a6db72e72f2.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a compact and predictive scaling law for large language model merging. The authors propose that the cross-entropy loss of a merged model can be accurately predicted by a \"floor + tail\" formula that depends on the base model size (N) and the number of merged experts (k). The law states that a size-dependent \"floor\" loss decreases as a power law of model size, while a \"merging tail\" shows diminishing returns proportional to 1/(k+b). Through extensive experiments on over 10,000 merged models, the authors demonstrate that this law holds robustly across different model architectures, merging methods, and domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper addresses an important but rarely investigated area. While model merging is a popular technique, it has largely been guided by empirical trial-and-error. This work provides a framework for understanding and planning merging experiments.\n\n2. The empirical evidence presented is strong. The authors validate their law on 10,506 merged models, spanning sizes from 0.5B to 72B, across nine distinct domains, four representative merging methods (Average, TA, TIES, DARE), and multiple model backbones (Qwen, LLaMA).\n\n3. This paper offers clear and interesting insights for practitioners."}, "weaknesses": {"value": "1. The experts are trained and evaluated on specialized datasets for math, science, and code (Mixture-of-Thoughts, OpenScience). While this provides a controlled environment, it differs from many real-world use cases where models are fine-tuned on more diverse tasks like dialogue, translation, or creative writing. It remains an open question whether the precise exponents of the scaling law would hold for merging experts trained on such broad, general-domain text.\n\n2. For most fine-tuned models, especially for SFT training, the CE loss is not an reliable evaluation metric. Although some works have shown that the downstream task performance if related to CE loss, but these works focus on based models, not chat-models (SFT-ed models). The paper does not explore whether the observed scaling trends translate to predictable improvements on these more practical, end-user-facing evaluations.\n\n3. While the paper provides a theoretical proof for the 1/k shape of the merging tail, the power-law parameterization for the model size N (i.e., L∞(N) and A(N)) is presented as an empirical finding. The theory in Section 3.2 does not derive why the floor and tail amplitude should scale with N in this specific power-law form. This mirrors the approach of early pretraining scaling laws, where the form was observed empirically and then validated. A deeper theoretical justification for this N-dependence would further strengthen the work.\n\n4. The theoretical analysis and much of the empirical work assume equal weighting of task vectors. While this is a common and simple approach, more advanced merging techniques often employ adaptive or learned weighting schemes to resolve conflicts between experts more effectively. The current law does not account for these variations, limiting its direct applicability to more complex merging strategies."}, "questions": {"value": "Do you use Qwen-base model and Qwen-instruct model as your \"base model\"?\n\nWhy don't fine turn on more general corpus such as CC/fineweb?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3nTjM2PSb4", "forum": "vpKXTmMtBQ", "replyto": "vpKXTmMtBQ", "signatures": ["ICLR.cc/2026/Conference/Submission18627/Reviewer_fZCR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18627/Reviewer_fZCR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835240533, "cdate": 1761835240533, "tmdate": 1762928340494, "mdate": 1762928340494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies an empirical scaling law that governs how model merging performance changes with both model size and the number of merged experts. The authors show that the expected loss follows a compact \"floor + tail\" power law: larger base models reduce the size-dependent loss floor, while the marginal benefit of adding more experts diminishes roughly as 1/(k + b). Through over ten thousand experiments across domains, architectures, and merging methods, they confirm that most improvements occur with the first few experts, variance contracts as experts increase, and differences between merging methods shrink at scale. The authors derive a simple theoretical explanation for the law, validate it on multiple backbones, and demonstrate that three measurements can predict the full merging curve. Overall, the work transforms model merging from a heuristic procedure into a predictable, budget-aware alternative to multitask fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper uncovers a simple and consistent empirical law that explains merging behavior across scales, transforming model merging from an empirical practice into a predictable process.\n\n- The study is supported by a large and systematic experimental effort spanning over ten thousand runs, which gives the findings strong empirical credibility.\n\n- The proposed \"floor + tail\" formulation provides a compact and interpretable description of performance dynamics, offering both theoretical and practical insight.\n\n- The three-measurement prediction method is elegant and useful, allowing practitioners to estimate merging outcomes with minimal computation.\n\n- The analysis reveals unifying trends across merging methods and model sizes, suggesting a deeper structural regularity that could guide efficient model composition strategies."}, "weaknesses": {"value": "- The theoretical explanation for the proposed scaling law is largely heuristic and lacks formal derivation or error bounds, relying on empirical fits rather than principled analysis (Section 4.2, Figure 7).\n\n- The experiments, though extensive, are dominated by language models from a single family (Qwen2.5), which limits the claim of universality across architectures and modalities (Table 1).\n\n- The analysis does not explore failure cases or outliers where merging violates the predicted scaling, leaving unclear how robust the law is under data imbalance or domain mismatch.\n\n- The practical implications for real-world deployment are underdeveloped -- the paper does not quantify how the scaling law translates into compute, memory, or latency trade-offs in large-scale systems.\n\n- The evaluation focuses on aggregate loss metrics, but omits downstream or task-specific performance, which makes it difficult to assess whether predictable scaling aligns with actual quality gains."}, "questions": {"value": "Does the fitted exponent remain stable when the merged models differ substantially in training data or domain?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eGDENCEYFl", "forum": "vpKXTmMtBQ", "replyto": "vpKXTmMtBQ", "signatures": ["ICLR.cc/2026/Conference/Submission18627/Reviewer_H4kd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18627/Reviewer_H4kd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979043516, "cdate": 1761979043516, "tmdate": 1762928339773, "mdate": 1762928339773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper establishes an empirical scaling law for language model merging. The law shows that adding more experts (k) yields diminishing returns with most gains achieved by k~5-6. The law, validated across 10,506 merged models (0.5B-72B parameters, 9 domains, 4 methods), enables predictive planning: a three-point fit forecasts the full loss curve and recommends optimal expert counts, making merging a computationally efficient alternative to multitask training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A new scaling law for model merging field, which could be a good reference for practicers in this community.\n2. This paper provides a theoretical explanation for the scaling law."}, "weaknesses": {"value": "1. Heterogeneous Model Sizes in Merging. What is the appropriate value of N when merging models with different parameter counts? The paper only considers merging experts derived from the same base model (same N). Your theoretical framework assumes fixed N throughout (Section 2.2, Appendix B), and all experiments use homogeneous models. Does your scaling law extend to heterogeneous merging scenarios?\n2. Your law predicts monotonic improvement with k, but real-world merging frequently shows performance degradation, especially when:\n(1) Task vectors have large parameter differences (2) Domains are highly incompatible (e.g., code + biology)\nYour theory assumes benign conditions (positive-definite Hessian) but doesn't address task interference or negative transfer. I find it puzzling that the paper appears to completely overlook scenarios where merging multiple models could actually lead to worse performance. This is a very common occurrence in practice. The formula presented in the paper directly suggests that loss decreases as the number of models increases, which contradicts both my intuition and my experimental findings (as well as those of many prior studies).\n3. Similarly, what occurs when merging a strong expert with a weak one (or a extensively-trained model with a minimally-trained one)? Based on my intuition and experience, this typically yields a mediocre model or even complete failure (given the large discrepancy between experts). Has your scaling law considered this case?\n4. Alternative formula fitting approaches: Beyond the 1/k formula proposed in the paper, have other potential formulas been fitted? In fact, with only 9 data points on the curve, it seems that various formulas could easily fit the data, such as a power law relationship. Have you ever considered using AIC/BIC to compute complexity of your model?"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uysHKfjppQ", "forum": "vpKXTmMtBQ", "replyto": "vpKXTmMtBQ", "signatures": ["ICLR.cc/2026/Conference/Submission18627/Reviewer_MDxx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18627/Reviewer_MDxx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009491091, "cdate": 1762009491091, "tmdate": 1762928339146, "mdate": 1762928339146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new study on the scaling laws of model merging. The principal final outcome of the study is to obtain a characterisation expression which predicts quantitatively what we get from augmenting/reducing the number of models (K) and their parametric size (N), let's say the general hyperparameters that scale or do not scale the problem. The study is based on LLMs with a size between 0.5B and 72B parameters and up to 1300h of GPU computing for the largest multi-task model tested. The general development of the scaling laws and the adjustment of coefficients is built on top of Kaplan et al. (2020) and their procedure. While having a heavy empirical load, the paper provides the formal derivation of the law, in the Appendix."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Characterising the model merging problem in terms of scaling laws is definitely interesting. While I still identify some issues related to reproducibility, clarity, and mostly lack of explicit reasoning on the general derivation (in the manuscript), I do think the floor+tail way of viewing this problem is useful and insightful.\n- Nothing really to complain about on the experimental side, I do think that it is great how models are tested, and in general, how results are presented. There is a clear will of clarity and showing empirical evidence, which can be perceived at first sight from the extensive experiments. No doubts about this, and a clear strength from my point of view.\n- Despite some too-big statements for my taste and some other unclear approximations (I will add details on these later on), I do think the paper sends a clear message, presents clear contributions, and in general does a great job communicating the high-level advantages of doing this sort of characterisation. While I am more skeptical about the scaling laws on other applications, here, the advantage of knowing the consequences of prior merging decisions is undeniable."}, "weaknesses": {"value": "- I see a very limited review of previous methods and related work references, like, it's not clear to me if this piece of work aligns well with the rest of the model merging approaches, SOTA methods, and baselines in general. From another point of view, the LLM-centrism of the paper penalizes the scientific quality as it quickly shows some too-big statements about discoveries and the generality of a method only applied to LLM while ignoring the rest of ML/AI models and a lot of previous work on merging approaches (since the 00s at least). With this last reference, I am mostly referring to transfer learning, mixtures of experts, products of experts, committee machines, etc. (If someone makes a big statement, at least a good review or demonstration of bibliographic knowledge should be included)\n- The structure and presentation of the work do not help a lot to follow what is going on. I honestly didn't even understand how Eq. 1 came out in the first place in pp. 1, and I found all the formulation in the main manuscript repetitive and looping around the same equation without too much complexity or new ideas involved. This last part was my conclusion after finally deciphering why things were done in this way. For instance, zero intuitions or scientific insights are given to the reader for managing Eq. 5 --- I am familiar with such a sort of operators, but section 3.2 looks like it pretends to be of minimum size and clarity...\n- Additionally, I found myself in the uncomfortable situation as a reviewer and reader of having to decipher for myself how the scaling law where derived, obtained, and in general how I could reproduce the procedure for my own problems or challenges. In that sense, the work is somewhat obscure, and just a little derivation is included in the Appendix, not with a lot of additional clarity. From my perspective, not all scaling laws should be in the form of floor+tail fitting, as in Kaplan et al. (2020). This should be explained and the reader guided through the main motivations for it. In some sense, that seminal reference does it better. \n- Last but not least, the big conclusion on the dependence on the number of parameters (size of models) and the number of models to merge seems a bit small to me. In the sense that such a response is somehow evident from other perspectives and types of models. I am maybe more surprised by the experiments in section 3.3.1 on the fact that larger models are easier to merge, but still, there are a lot of open questions left behind. One of these is also on the dominance or similarity between models, which is not considered in depth."}, "questions": {"value": "- Which one is the Hessian approximation in Eq. 5?\n- Just as a curiosity: could the authors make an estimation of the C02 equivalent footprint of all the experiments and the financial resources needed based on the computing hours declared in the overview of Figure 2 and the GPU infrastructure?\n- Are there limitations on the scaling law fitting if we consider LLM or just models with more diverse tasks? \n- Is the alignment of logits/labels considered in the general model merging method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EnTF5NT8yd", "forum": "vpKXTmMtBQ", "replyto": "vpKXTmMtBQ", "signatures": ["ICLR.cc/2026/Conference/Submission18627/Reviewer_RTHq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18627/Reviewer_RTHq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762294846205, "cdate": 1762294846205, "tmdate": 1762928338634, "mdate": 1762928338634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}