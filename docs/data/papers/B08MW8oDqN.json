{"id": "B08MW8oDqN", "number": 6301, "cdate": 1757965772292, "mdate": 1759897923793, "content": {"title": "Can Transformers Really Do It All? On the Compatibility of Inductive Biases Across Tasks", "abstract": "Transformers are remarkably versatile and their design is largely consistent across a variety of applications. But are they optimal for any given task or dataset? The answer may be key for pushing AI beyond the mere scaling of current designs.\n\n*Method.* We present a method to optimize a transformer architecture for a given dataset, which we use as a tool to study optimal task-specific inductive biases. The method replaces the most important non-linearities (GeLUs, softmax) with components optimized on held out data. We then use each resulting new architecture\nwith other datasets as a way to evaluate the compatibility between pairs of tasks.\n\n**Findings.** On a range of popular algorithmic tasks, our method identifies new architectures with dramatic improvements in learning speed, generalization, and stability across seeds. These designs prove very task-specific, which means that the tasks require inductive biases very different from those of standard transformers. On a range of code and language modeling datasets, we also find architectures with consistent, yet smaller improvements. These designs now transfer much better across datasets, domains (English vs. computer code), and tokenizations.\n\n**Implications.** These results show that standard transformers are rarely a local optimum in the space of architectures. We show that alternative designs can perform better, but they often sacrifice universality. This calls for future work on architectures that could serve multiple objectives such as fluency and robust reasoning.", "tldr": "We optimize transformers by replacing GeLUs/softmaxes with parametrized splines optimized on held-out data. This tools reveals when task-specific architectures can be better.", "keywords": ["Transformers", "language models", "inductive biases", "length generalization", "activation functions"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3c472a63775b4c8accc6fe6e5dcbccc539399406.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The work investigates the question of whether transformers have the optimal architecture for all tasks. To investigate this, the authors replace the static nonlinearities of softmax in the attention block and the GeLU in the MLP block with trainable activations, and show that a 2 stage approach where we first train on a dataset to get the optimal shape of activation (the optimal architecture), and then in the second stage we fix the activation shape and then train on the dataset again from scratch, performs better than simply training a standard transformer architecture with standard activations. The benefits are in the form of convergence speed, stability across training seeds, and the final performance on held out test sets, as measured on algorithmic tasks as well as language modeling tasks. It also finds that usually the learnt activations that work best for one dataset usually are not the best choice for another datasets, suggesting dataset-specific variations in optimality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* Good coverage of datasets for evaluation of the proposed hypothesis including both language modeling tasks and algorithmic tasks.\n* The idea of using learnt splines for representing activation functions instead of fixed activations is creative. \n* The presentation of the paper is good."}, "weaknesses": {"value": "* Regarding the primary research question answered in the paper “Are transformers a unique and optimal solution endowed with generic inductive biases?” it is well known that one single architecture cannot be the best at learning all datasets and tasks, and there have been works in the past that have demostrated it too with respect to transformers (e.g. “Are Transformers Effective for Time Series Forecasting?” Zeng et al. AAAI 2023)\n* The value of transformers’ unified architecture is that they can be pretrained to perform well across multiple tasks and domains. In this view the utility of having task specific architectures is diminished.\n* The proposed approach uses 2-stage training where the first stage is used to learn activation function shape via parameterized splines and then training is restarted by training it from scratch but using the fixed learnt spline activation. This leads to faster convergence as shown in the paper. However, I am not sure if there is a fair comparison here because the first stage of activation learning will also be needed to be run on every dataset on which you want to train, and so the steps needed for the activation learning phase also need to be accounted for."}, "questions": {"value": "* In section 3.1 and figure 1, the claim is that the model converges faster. Is the convergence steps measured by counting the training steps in stage2? This is relevant because this would mean that there is some information from stage1 training (the learnt spline parameters) that are re-used in stage2 and can thus lead to faster convergence.\n* If one were to run the stage1 twice, do you get similar splines? I am wondering how much can we pinpoint to the learnt architecture to be optimal, given that there can be other splines that are just as good."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WWROEjycAY", "forum": "B08MW8oDqN", "replyto": "B08MW8oDqN", "signatures": ["ICLR.cc/2026/Conference/Submission6301/Reviewer_zxSg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6301/Reviewer_zxSg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761595519311, "cdate": 1761595519311, "tmdate": 1762918603814, "mdate": 1762918603814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work intends to answer the question: is the standard transformer architecture optimal for all tasks? This question is important because the transformer architecture is often ported, without modification, to a wide variety of tasks. This work addresses this question by presenting a novel method for tuning the inductive biases in the transformer architecture to a particular dataset, and then evaluating performance of these optimized architectures on other datasets. This work demonstrates that the optimal inductive biases for algorithmic tasks are quite heterogeneous, often resulting in only mild improvements over the baseline architecture when used for other algorithmic tasks. However, the standard architecture is much better suited for language modeling tasks, with only mild improvements conferred to small models by optimizing the architecture."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "This work addresses an interesting and important outstanding question.\n\nThis work uses a variety of tasks, both synthetic and naturalistic, to demonstrate that the standard transformer architecture is not a globally optimal architecture (or even a locally optimal architecture; see GeLU initialization results).\n\nThe results regarding the effect of scale are interesting and situate these results with the field — they acknowledge that scale might render inductive biases less relevant, but also demonstrate that proper inductive biases might enable smaller models. Though, see weakness section for concerns/questions."}, "weaknesses": {"value": "The results for the optimized architectures on the language datasets do not seem significantly different than the baseline for the subword tokenized results. These same results could be used to argue strongly for the optimality of the baseline transformer architecture for language modeling. Only by using the (fairly nonstandard) metric of token accuracy do differences present themselves. This is not a problem, and is indeed interesting! It is worth making this point more explicitly in the text, rather than emphasizing small differences between the optimized and baseline architecture.\n\nThe results in Fig 7 seem to contradict the point made regarding character-level transformers in the text. In text, it is argued that smaller models (referring to char-level transformers) are better served by optimizing nonlinearities. However, this is inconsistent with the results on the tinystories dataset, where larger models consistently benefit more from optimizing the nonlinearities. How do you interpret this apparent contradiction?\n\nIn footnote 2 on page 3, the authors mention that the nonlinearities are better viewed as pre-tuned hyperparameters rather than extra model capacity, as they are frozen for stage 2 training. It would be very helpful to make this point empirically. For example, it would be great to have a baseline where the model is trained on all data in Stage 1, and performance is recorded from this model. If performance is higher in this setting than in the reported results, then one can feel more confident that the two stage training process is not merely adding extra capacity to the model.\n\nIt would be very interesting to compare modalities. In particular, characterizing how the optimal architecture for encoder-only models in language and vision differs would be very impactful."}, "questions": {"value": "For Figure 4, is the caption mixing up the colors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mZCQrlcWeK", "forum": "B08MW8oDqN", "replyto": "B08MW8oDqN", "signatures": ["ICLR.cc/2026/Conference/Submission6301/Reviewer_A5zP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6301/Reviewer_A5zP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951785405, "cdate": 1761951785405, "tmdate": 1762918603500, "mdate": 1762918603500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uses learnable non-linearities of linear splines in transformers, and optimizes the transformers for a variety of tasks, both small algorithmic tasks and language/code modelling. They find that optimized models converge faster and more consistently on the datasets that they are trained on, but that transfer is variable, meaning that there is no global configuration that is best for all types of data (and that transformers are not the local optimum for any one modelling task)"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "I am not very familiar with the complete literature around supervised architecture search, but with my knowledge I would say that this is an original and interesting method. \n\nBoth the framing (around finding inductive biases and how they compare between tasks), and the results, are interesting contributions, and I feel that I have learned something from this paper. \n\nThe analyses are thorough and well-done — for example I liked the extra analysis of Appendix D looking in to whether cleaner wavelets are better."}, "weaknesses": {"value": "I think this is a strong and interesting paper, and any weaknesses are relatively minor:\n\nW1: I would have also liked to see a bit more of an analysis of what the fact that we are restricting the search space to linear splines might mean. I understand that it is unreasonable to expect a paper to have a larger search space, so I think it’s perfectly reasonable to restrict to linear splines. However, though the authors do state it as a limitation, they do not provide much evidence or discussion about what the effects of this limitation might be, and what this can tell us about training LMs. Are there any pilot experiments you might do on one or two settings with a different function search space, to demonstrate the different effects it might have?\n\nW2: I think that one of the strongest aspects of this paper  are the ideas around inductive bias and learnability, but the discussion is not as nuanced as it could be. I think it would be useful to mention some of the more cognitive-science-leaning studies on learnability and inductive bias such as (in no particular order, and no need to cite all I just want to give a sense of what I mean)\n\nFutrell, Richard, and Kyle Mahowald. \"How linguistics learned to stop worrying and love the language models.”\n\nMueller, Aaron, and Tal Linzen. \"How to plant trees in language models: Data and architectural effects on the emergence of syntactic inductive biases.”\n\nPapadimitriou, Isabel, and Dan Jurafsky. \"Injecting structural hints: Using language models to study inductive biases in language learning.”\n\nHooker, Sara. \"The hardware lottery.”\n\nWarstadt, Alex, and Samuel R. Bowman. \"What artificial neural networks can tell us about human language acquisition.\"\n\nMillière, Raphaël. \"Language models as models of language.”\n\nKallini, Julie, et al. \"Mission: Impossible language models.”\n\nYang, Xiulin, et al. \"Anything Goes? A Crosslinguistic Study of (Im) possible Language Learning in LMs.”"}, "questions": {"value": "Is there any analysis, or hypothesis to be tested, that you think comes out of seeing how the optimized splines look? It’s quite interesting that the optimized splines  don’t look much like the nonlinearities that we usually use, and I think that might be a fertile avenue for further analysis that can come out of this paper. Does it have to do with the initialization, or is there something about those structures that is better? Is it misguided of us as a field that we tend to use more monotonic nonlinearities? I really appreciate the analysis in Appendix D"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rQ70H1VZx7", "forum": "B08MW8oDqN", "replyto": "B08MW8oDqN", "signatures": ["ICLR.cc/2026/Conference/Submission6301/Reviewer_2nXb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6301/Reviewer_2nXb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762193702915, "cdate": 1762193702915, "tmdate": 1762918603127, "mdate": 1762918603127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to parameterize the activation functions of transformers (including MLPs, Attention, and GLU) using splines and analyze how changing the inductive biases (by parameterizing the activation functions) of transformers can influence the final performance of the model along with its training stability and convergence time. They observe that in algorithmic tasks parametrizing the activation functions help in faster convergence along with improved training stability. However, the parameterized activation functions are very task specific and do not generalize well across other tasks unless they are very similar. They further scale their method on larger datasets including tiny stories and observe that they achieve improved performance over using the standard activation functions like Relu and Gelu. However, in this case the improvements are smaller but the transferability is also better. In summary, this work tries to analyze how modifying the inductive biases (via changing the activation functions) can influence the learning process in transformers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Developing a better understanding about the inductive biases in neural networks is very important in order to understand generalization. And authors investigate an important problem by analyzing the inductive biases induced due to network activations. In this regard, I really liked the approach adopted by the authors to parameterize the activation functions with splines.\n* The empirical results on algorithmic tasks are interesting and clearly demonstrate the importance of inductive biases induced by the activation functions. \n* Parameterizing the activation functions using splines is interesting because it provides flexibility to the model to adapt as per the task complexity. Intuitively, a simple task would learn simpler activation functions and the complex ones would learn complex activation functions. Therefore parameterizing the activations (if done properly) should help the model adapt to the desired complexity. This would help in improved convergence and performance as shown by this work on simple tasks."}, "weaknesses": {"value": "* Many of the contributions in this paper are quite similar to [1]. For instance, [1] also shows that neural networks when trained on modulo addition converge faster as shown by this paper. Similarly, [1] also parameterizes activation functions using b-splines as done in this work. It would be very helpful if the authors can share more about how their work is different. If the authors are trying to demonstrate that the main differentiating part is to show the effectiveness of parameterizing activation functions on transformers, then it would be very helpful if more evidence on larger models (upto 6-10 layers) and multiple heads is shown. One of my concerns is that scaling this method might lead to overfitting to specific tasks, and thus the generalizability of this method wont be observed.\n* Although the current experiments are interesting, I think more evidence demonstrating the importance of parameterizing the activation functions is needed for larger models. It would be helpful if the authors can scale up their experiments. Currently most of the experiments are limited to using relu and gelu activation functions, but in practice we use a few others also like tanh, swish. It would be interesting to have them as baselines. I think the paper would benefit from having evaluations on different architecture choices like rnns, mamba, etc.\n* As a side note, at some places the authors should try to tone down their claims, if possible. For instance, in lines 430-431, demonstrating their method being close to the activation functions used in practice, doesn't necessarily mean that current architectures are close to optimal ones.\n\n[1] Do We Always Need the Simplicity Bias? Looking for Optimal Inductive Biases in the Wild (https://arxiv.org/abs/2503.10065)"}, "questions": {"value": "* It is not completely clear on how the activation functions should be parameterized. Why is using splines one of the best choices? And if possible, can the authors share more details on how they choose the basis function of splines? My understanding is that they are using linear basis functions? Is there any reason for this? And why did the authors decide to choose 122 points equally spaced between [-20,20]? Is there some reason to come up with these design choices? Just out of curiosity, is it possible to parameterize the spline spacing hyper-parameter?\n* An alternate way to learn the activation functions is to use the method proposed in this work [1]. Is there any reason why authors did not use this method? Does it not work in their setup?\n* Did authors observe any overfitting? I would expect that parameterizing the activation functions would make the models more vulnerable to overfitting the training dataset and thus hamper generalization. This overfitting could become worse for larger models.\n* Simplicity bias says that neural networks prefer learning simpler functions. However, in most cases the activation function learned by authors seems to have a large complexity (at least in fourrier basis). Even though a simpler explanation (say by learning relu) could give high likelihood on training dataset, the authors observe that simpler functions are not learned. Is it showing that simplicity is not always preferred? Or is it that with more data points, the activation functions will asymptotically converge towards relu or a simpler function? Is there some explanation that can justify this?\n\n[1] Do We Always Need the Simplicity Bias? Looking for Optimal Inductive Biases in the Wild (https://arxiv.org/abs/2503.10065)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gfkDcRYFW4", "forum": "B08MW8oDqN", "replyto": "B08MW8oDqN", "signatures": ["ICLR.cc/2026/Conference/Submission6301/Reviewer_ZLpT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6301/Reviewer_ZLpT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762207457057, "cdate": 1762207457057, "tmdate": 1762918602772, "mdate": 1762918602772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}