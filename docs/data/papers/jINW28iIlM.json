{"id": "jINW28iIlM", "number": 10249, "cdate": 1758165092013, "mdate": 1759897663217, "content": {"title": "FAST THINKING FOR LARGE LANGUAGE MODELS", "abstract": "Reasoning-oriented Large Language Models (LLMs) often rely on generating explicit tokens step by step, and their effectiveness typically hinges on large-scale supervised fine-tuning or reinforcement learning. \nWhile Chain-of-Thought (CoT) techniques substantially enhance performance on complex reasoning tasks, they remain inefficient, requiring long reasoning traces that increase latency and token usage. \nIn this work, we introduce \\emph{Latent Codebooks for Fast Thinking}, a framework that uses concise CoT sketches only during training to learn a codebook of discrete strategy priors. \nAt inference, the model conditions on a handful of continuous thinking vectors distilled from the codebook in a single pass, enabling strategy-level guidance without producing explicit reasoning tokens. \nTo complement this design, we propose GainRouter, a lightweight routing mechanism that adaptively switches between fast codebook-guided inference and slow explicit reasoning, thereby suppressing overthinking and reducing unnecessary token generation. \nExperiments across multiple reasoning benchmarks show that our approach achieves competitive or superior accuracy while substantially lowering inference cost, offering a practical path toward efficient and controllable reasoning in large language models.", "tldr": "LC-FT learns a latent codebook of strategy priors from concise CoT, then uses a few continuous hint vectors to enable single-pass “fast thinking.”", "keywords": ["Large Language Models", "Efficient Reasoning", "Adaptive Routing"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bef9e961fda5d906a39dcbed94a0af574f288d42.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Latent Codebooks for Fast Thinking, a novel framework designed to improve the efficiency of reasoning in large language models (LLMs). While reasoning-oriented LLMs typically depend on lengthy Chain-of-Thought (CoT) traces during inference—leading to high latency and excessive token consumption—our approach leverages concise CoT sketches only in training to learn a discrete codebook of strategy priors. During inference, the model uses a small set of continuous thinking vectors, distilled from the codebook, to guide reasoning in a single forward pass, eliminating the need for generating explicit reasoning tokens. To further enhance efficiency, we propose GAINROUTER, a lightweight routing mechanism that dynamically chooses between fast, codebook-based inference and slower, explicit reasoning based on task complexity, thereby preventing overthinking. Extensive experiments on multiple reasoning benchmarks demonstrate that our method achieves competitive or better accuracy while significantly reducing inference cost, providing a practical solution for fast, controllable, and efficient reasoning in LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed Latent Codebooks for Fast Thinking (LC-FT) framework significantly reduces the inference cost by using concise CoT sketches during training to learn a codebook of discrete strategy priors.\n\n2. The GainRouter mechanism allows the model to adaptively switch between fast codebook-guided inference and slow explicit reasoning based on the predicted failure risk."}, "weaknesses": {"value": "1. The major drawback of this paper is the lack of comparison with many relevant baseline methods. These baselines are neither discussed in the related work section nor included in the experimental comparisons.\n\n[1] Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning\n\n[2] Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging\n\n[3] L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning\n\n[4] Adaptive Group Policy Optimization: Towards Stable Training and Token-Efficient Reasoning\n\n[5] Concise Reasoning via Reinforcement Learning\n\n[6] SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning\n\n[7] Plan and Budget: Effective and Efficient Test-Time Scaling on Large Language Model Reasoning\n\n[8] Not All Tokens Are What You Need In Thinking\n\n[9] Stable Reinforcement Learning for Efficient Reasoning\n\n[10] Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning\n\n[11] Optimizing Anytime Reasoning via Budget Relative Policy Optimization\n\n[12] Making Small Language Models Efficient Reasoners: Intervention, Supervision, Reinforcement\n\n2. The paper does not evaluate the generalization of the proposed method’s reasoning capabilities, for instance, on STEM-focused datasets such as GPQA.\n\n3. The LC-FT framework comprises multiple components, including the latent codebook, GainRouter, and a two-stage training process. This complexity may pose practical challenges for implementation and likely requires substantial computational resources and technical expertise to set up and fine-tune effectively."}, "questions": {"value": "1. How does the LC-FT framework handle the trade-off between accuracy and efficiency in more complex reasoning tasks where the risk of failure is higher? For instance, in highly complex mathematical problems or real-world scenarios with high variability, can the GainRouter mechanism effectively balance the need for reasoning accuracy with the goal of computational efficiency?\n\n2. The ablation study suggests that removing individual components does not lead to significant performance degradation. Could the authors provide a more in-depth analysis to explain this observation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qaNPNK3RUX", "forum": "jINW28iIlM", "replyto": "jINW28iIlM", "signatures": ["ICLR.cc/2026/Conference/Submission10249/Reviewer_G29z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10249/Reviewer_G29z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10249/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760583956207, "cdate": 1760583956207, "tmdate": 1762921607180, "mdate": 1762921607180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uses distillation and a codebook mechanism to enable concise reasoning, along with a router so that it can have longer thinking for more difficult questions."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation is plausible, as adaptive thinking based on question difficulty is promising.\n- Simple routing actually increases the performance on math and code."}, "weaknesses": {"value": "- There are many papers on efficient reasoning other than just routing papers (e.g., rejection fine-tuning, concise reinforcement learning with length penalty, etc). This paper totally ignores all of these.\n- Fundamentally, this is distillation, as the model is trained to imitate the concise reasoning of teacher models. The codebook is just an abstract representation to use. Also, the codebook is not generally applicable, as the codebook and LLM are trained simultaneously. I think the introduction and overall paper fail to outline the core idea.\n- Also, why don’t you compare with full fine-tuning of distillation training, as that would make a fair comparison?\n- Also, the router is too naive and heuristic, raising concerns about applicability and generalization. Moreover, the gains mostly come from the router, so I am highly skeptical of why that complex codebook training is required.\n- The method also requires two separate models, causing memory issues.\n- Confusing terms and claims:\n    - Fast and Slow Thinking\n    - What it means by “Strong multi-step reasoning in practice often relies on either process supervision or preference-based policy optimization,” as recent reasoning models often rely only on outcome-based reward.\n    - “Latent prior learning” is somewhat ambiguous."}, "questions": {"value": "- What exactly is a hint r in Section 3.1?\n- Please clarify what is “CoT Model” in Figure 2.\n- For baseline “lora”, what data do you use?\n- Why are there some tokens before “Answer” in Answer tokens?\n- Some math operators look weird, like *raw_logit*."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iGBKheWK8y", "forum": "jINW28iIlM", "replyto": "jINW28iIlM", "signatures": ["ICLR.cc/2026/Conference/Submission10249/Reviewer_RYHy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10249/Reviewer_RYHy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10249/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761534798898, "cdate": 1761534798898, "tmdate": 1762921606714, "mdate": 1762921606714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose \"Latent Codebooks for Fast Thinking\", a learned latent codebook of discrete, reusable priors. \n\nTo train this approach, the authors first build a dataset of concise CoTs, by prompting a teacher model to generate \"hints\" from $x$ and $y$. This hint is accepted i.f.f. no answer leakage is detected, and conditioning on the hint leads the student to a successful answer (these hints seem analogous to a strategy for solving the problem at hand). \n\nThe codebook is integrated via cross attention, via a learnable query matrix that's shared across inputs. \nAt a given layer, the input is augmented on the sequence length with $K$ thinking tokens, concatenated to the right, so they can attend the previous input. A separate \"Refiner\" MLP additionally processes the thinking tokens. \n\nThe actual training is in two phases : in the alignment phase, a frozen LLM processes $(x, r)$, where $r$'s representations are mean-pooled at layers $l >L$, which will serve as alignment targets for the mean-pooled thinking token slots of the same layer. This can be seen as distilling the rationales inside the thinking tokens. In the Supervised Fine-Tuning phase, the teacher model and the rationales are dropped, and the new architecture is trained to predict directly from the input and the thinking tokens the answer.  \n\nTo account for settings where a slower, lengthier generation would be better, the authors propose the GainRouter,  a lightweight router switching between this \"condensed\" mode of reasoning, and \"standard\" reasoning. Its input is a function for both the pooled query tokens and the thinking tokens, trained with BCE loss on whether only the slow model succeeds. \n\nExperiments are conducted with the Qwen base model, and compared against small set of baselines on math and coding benchmarks. Finally the authors provide an ablation study on several components of their proposed methods, showing the effectiveness of the codebook."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors tackle an important question of efficiency in reasoning that is plaguing LLMs at test-time. The latent embedding approach is novel and interesting. \n\n2. The authors provide an extensive ablation on some of the different components of their approach."}, "weaknesses": {"value": "1. The proposed approach is somewhat convoluted; a separate data creation step, a learned codebook, additional cross-attention, two-phase training, an added refiner MLP, several losses, and an additional router. After reading the paper, I am unsure I truly have enough of the implementation details to  exactly implement the approach. Given that the authors have shared code, I am ok with overlooking the last point (that being said the link doesn't work so please fix it). That being said, overall given the complexity of the approach, it's unclear to me whether the small but somewhat consistent gains makes this a viable option for practical use cases. \n\n2. The results (specifically figure 4 left) for trading off performance would be better represented as Pareto curves; the authors could simply change the threshold in the GainRouter to use or not full reasoning; this way we could see how performance is \"interpolated\" across Slow and Fast Training."}, "questions": {"value": "Questions : \n1. \"More\u0002over, strong multi-step reasoning in practice often relies on either process supervision or preference\u0002based policy optimization which raises data and engineering barriers for large-scale deploy\u0002ment Schulman et al. (2017)\" \nAfter reading the paper it's unclear how your approach bypasses this; to generate your training data you are still going through the process of generating multiple rollouts and keeping the successful ones, which is the de-facto approach for CoT training. \n\n2. Figure 2 is not clear; is the bottom left the generated hint ? is CoT model == Teacher Model ? \n3. Figure 3 is a quite dense; what exactly is $L_q$ ? What is $S$ is Layer $S-1$ ? Why does it say alignment loss $L_{LM}$ on the answer tokens in the middle figure, are you calling the next-word prediction objective an alignment loss, or is this something else ?\n\n4. The authors phrase the training dataset as containing \"Concise CoT\", but this seems somewhat poorly named, given that the CoT don't contain the actual chain of though, but rather \"hints\", that could be incomplete. For example, if the base model can already solve the problem at hand,  $\\hat{r}$ could be empty, hence pass verification, but is not a CoT. \n\n4. If you are using cross-attention (and not self-attention), why do you need to pad your input ? Why not just process without padding for the first L layers, and then concatenate the thinking tokens ? This should be doable if your sequences are left padded. \n\nComments : \n1. please use `\\citep{}` in latex when the citation is not part of the sentence's syntax or meaning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FuBjjUA8xV", "forum": "jINW28iIlM", "replyto": "jINW28iIlM", "signatures": ["ICLR.cc/2026/Conference/Submission10249/Reviewer_eamC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10249/Reviewer_eamC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10249/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761610527575, "cdate": 1761610527575, "tmdate": 1762921606273, "mdate": 1762921606273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce Latent Codebooks for Fast Thinking (LC-FT), a framework that uses concise CoT sketches during training to learn a latent codebook of reusable strategy priors.\nDuring inference, LC-FT prepend K continuous “thinking vectors” generated from the codebook before CoT generation.\nIt also has a GainRouter module that adaptively decides when to use fast (codebook-guided) vs. slow (explicit CoT) reasoning, balancing accuracy and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of storing soft representation of reasoning strategies for retrieval at inference time is novel.\n2. The use of codebook seems to improve model performance compared to vanilla SFT as shown in table 1 and 2.\n3. The router significantly improves the performance, while also significantly increase generation length."}, "weaknesses": {"value": "1. The writing is a bit unclear, especially the method section. It took me a while to understand that the K soft thinking vectors are prepend to the CoT instead of replacing the CoT. Maybe make that clear at the begining.\n2. The paper is about \"fast thinking\", but the thinking speed up seem to purely coming from the teacher generated concise CoT. The method itself actually adds K soft tokens in front of the CoT. As shown in Table 1 and 2, the generation length of LC-FT is roughly the same as SFT on the concise CoTs. The generation is even significantly longer than base model when using a router on top of LC-FT. To me the paper is closer to a parameter-efficient training method for reasoning tasks instead of \"fast thinking\".\n3. The two training sets are quite small (1k+) and only four testing sets are considered. I wonder how generalizable is the proposed method. Would the learned codebook only cover a specific set of problems and strategies so the method can only be applied to one narrow domain?\n4. typos: mis-formated equations on line 254"}, "questions": {"value": "See weeknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BrXv5vv3ZA", "forum": "jINW28iIlM", "replyto": "jINW28iIlM", "signatures": ["ICLR.cc/2026/Conference/Submission10249/Reviewer_6hoU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10249/Reviewer_6hoU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10249/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023691823, "cdate": 1762023691823, "tmdate": 1762921605781, "mdate": 1762921605781, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}