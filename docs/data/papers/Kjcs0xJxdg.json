{"id": "Kjcs0xJxdg", "number": 11174, "cdate": 1758192075031, "mdate": 1763644079511, "content": {"title": "Scalable and Adaptive Trust-Region Learning via Projection Convex Hull", "abstract": "Learning compact and reliable convex hulls from data is a fundamental yet challenging problem with broad applications in classification, constraint learning, and decision optimization. We propose Projection Convex Hull (PCH), a scalable framework for learning polyhedral trust regions in high-dimensional spaces. Starting from an exact MINLP formulation, we derive an unconstrained surrogate objective and show that, under suitable weight assignments, the optimal hyperplanes of the MINLP are recovered as stationary points of the surrogate. Building on this theoretical foundation, PCH adaptively constructs and refines hyperplanes by subregion partition, strategic weight assignment, and gradient-based updates, yielding convex hulls that tightly enclose the positive class while excluding negatives. The learned polyhedra can serve as geometric trust regions to enhance selective classification and constraint learning. Extensive experiments on synthetic and real-world datasets demonstrate that PCH achieves strong performance in accuracy, scalability, and model compactness, outperforming classical geometric algorithms and recent optimization-based approaches, especially in high-dimensional and large-scale settings. These results confirm the value of PCH as a theoretically grounded and practically effective framework for trust-region learning.", "tldr": "We introduce Projection Convex Hull (PCH), a scalable framework that constructs boundary-tight convex hulls, enhancing downstream tasks such as selective classification and constraint learning.", "keywords": ["Convex hull learning", "boundary-tight separation", "scalable polyhedral separation", "constraint learning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/22cefa9e97c6da2febc7f55a9e7f0469ed9e0594.pdf", "supplementary_material": ""}, "replies": [{"content": {"title": {"value": "Response Summary"}, "comment": {"value": "We thank all reviewers for their careful evaluations and constructive suggestions. We are encouraged that the reviews consistently recognize the strengths of our submission. Reviewers describe the paper as “very well written” (hxmb), “strong in both theory and practice” (hxmb), “novel” with a “rigorous theoretical decomposition” (hxmb), and “a gradient-based surrogate formulation that bridges discrete MINLPs with differentiable optimization” (5jY7). They highlight the method’s “scalability, adaptivity, and boundary tightness” (c9H4) and note that PCH enables “practical training in high-dimensional settings” (5jY7). Reviewer G8zX also emphasizes the “sound theoretical foundation” and the ability of PCH to “achieve high accuracy on synthetic and real-world datasets” (G8zX), reinforcing both the theoretical contributions and empirical strength of the framework.\n\nBuilding on these encouraging assessments, we summarize below the main concerns raised by the reviewers and the clarifications and improvements made in the rebuttal and revised manuscript.\n\n**1. Relation between theory and algorithm.**  \nSeveral reviewers asked how the theoretical results connect to the practical procedure. We clarified that Theorem 1 formalizes the stationary condition linking the MINLP structure to a feasible weight assignment, which directly motivates the projected weight update in Algorithm 1. Sections 4–5 have been revised to make this connection explicit and to show how the surrogate optimization approximates the stationary configuration suggested by the theory.\n\n**2. Convergence behavior and local optima.**  \nReviewers also asked about convergence and the possibility of local optima. We provided a theoretical explanation showing that the alternating updates consistently improve the same separation geometry. Combined with empirical trajectories, where both the hyperplanes and weight profiles stabilize quickly across random initializations, this indicates that the optimization behaves reliably in practice.\n\n**3. Diverse and challenging experiments.**  \nTo address requests for more challenging datasets, we added results on MNIST-35, FMNIST-24, CIFAR-35, and Adult Income, which involve high dimensionality, nonlinear, and imbalanced data manifolds. PCH consistently achieves the highest trust-region accuracy and boundary tightness among scalable baselines. We also included noisy polyhedral experiments, in which PCH remains stable and achieves high separation accuracy. The iteration curves show that PCH typically converges within a few rounds, whereas classical geometric and MILP-based approaches become impractical at these scales.\n\n**4. Algorithm update process visualization.**  \nFollowing the request for a clearer illustration of the update process, we added a 2D example showing how the hyperplanes and their corresponding weight assignments evolve during training. This provides a step-by-step visualization of how the surrogate optimization proceeds.\n\n**5. Model capacity and boundary approximation**  \nWe clarified how the hyperplane budget affects boundary quality and added a 2D circular example to illustrate capacity scaling. We also explained why nonconvex positive regions do not pose issues: any trust-region-learning method necessarily returns an outer approximation and includes interior negatives, and PCH follows this established principle while maximizing separation from external negatives.\n\nWe thank all reviewers again for their insightful feedback, which helped us improve the paper's presentation, theoretical clarity, and experimental coverage."}}, "id": "PJEoyCMqB3", "forum": "Kjcs0xJxdg", "replyto": "Kjcs0xJxdg", "signatures": ["ICLR.cc/2026/Conference/Submission11174/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11174/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11174/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763642001641, "cdate": 1763642001641, "tmdate": 1763642001641, "mdate": 1763642001641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Projection Convex Hull (PCH) method, a scalable framework for learning polyhedral trust regions in high-dimensional spaces. The link between the intractable MINLP for tight convex hull learning and an unconstrained surrogate objective is established. The PCH framework incorporates (i) partition-based subregion assignment, (ii) a surrogate objective to align hyperplanes with class boundaries,and (iii) adaptive hyperplane pruning and addition. Experiments on synthetic and real-world datasets show that\n PCH achieves high accuracy and superior scalability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. PCH is proposed with sound theoretical foundation.\n2. PCH scales to high-dimensional and large-scale problems.\n3. Besides convex hull learning, downstream tasks including selective classification and constraint learning are considered.\n4. Figure 1 and Figure 2 intuitively illustrates the main steps of PCH ."}, "weaknesses": {"value": "1. Whether trapping into local minima is possible during the optimization is not discussed.  \n2. How is the robustness of PCH in the presence of data noise?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cFUDvJ8ziz", "forum": "Kjcs0xJxdg", "replyto": "Kjcs0xJxdg", "signatures": ["ICLR.cc/2026/Conference/Submission11174/Reviewer_G8zX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11174/Reviewer_G8zX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829159529, "cdate": 1761829159529, "tmdate": 1762922332451, "mdate": 1762922332451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an algorithm for learning convex hulls (polyhedral trust regions) from labeled data, ensuring that all positive samples are enclosed while excluding negatives as much as possible. Traditional geometric or MILP approaches fail to scale in high dimensions and lack flexibility, and it aims to design a scalable, theoretically grounded, and adaptive framework to learn boundary-tight convex hulls efficiently. The main contribution is that, the authors propose a divide-and-conquer learning framework that links an exact MINLP problem to an unconstrained surrogate objective, making gradient-based optimization feasible."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a novel gradient-based surrogate formulation for convex hull learning, bridging discrete MINLPs with differentiable optimization. Moreover, its adaptive structure and surrogate update scheme enable practical training in high-dimensional settings, as demonstrated empirically."}, "weaknesses": {"value": "1. It is unclear how the main theoretical result (Theorem 1) relates to the practical algorithm. The theorem establishes the existence of a certain weight assignment, but it is not evident whether this weight can be computed or approximated by Algorithm 1 or the procedure described in Section 5.2. Although there is some discussion in lines 354–356, the connection remains ambiguous, especially since Equation 3 does not explicitly include a weight term.\n\n2. In addition, no convergence analysis or guarantee (e.g., monotonic improvement) is provided for the surrogate updates.\n\n\n\n3. Since the algorithm is somewhat unintuitive, it would be helpful to include a simple 2D toy example illustrating the evolution of the hyperplane and weight updates over iterations. This visualization would greatly enhance readers’ understanding of the surrogate optimization process."}, "questions": {"value": "1. A main contribution concerns the algorithm’s computational complexity. However, it appears to depend on the number of iterations—what is the typical order or empirical scaling of this term?\n\n2. The authors should discuss potential failure cases, such as when the positive region is nonconvex or when the hyperplane budget is insufficient to capture the desired boundary.\n\n3. Is that any small example showing that the surrogate formulation indeed approxiamte the discrete MINLP problem?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "iaMU8VUiHD", "forum": "Kjcs0xJxdg", "replyto": "Kjcs0xJxdg", "signatures": ["ICLR.cc/2026/Conference/Submission11174/Reviewer_5jY7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11174/Reviewer_5jY7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949606132, "cdate": 1761949606132, "tmdate": 1762922332114, "mdate": 1762922332114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Motivated by the observation that complex and nonlinear classification boundaries often exhibit regional linearity and can be locally approximated by hyperplanes with small classification error, the paper constructs a convex hull that is scalable,\nstructurally adaptive, and geometrically tight. Specifically, from the MINLP problem formulation, the work establishes its connection to a family of unconstrained surrogate objectives. This enables gradient-based learning of convex hulls that are both compact and theoretically motivated."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper effectively addresses key limitations of prior works in terms of scalability, adaptivity, and boundary tightness, and provides theoretical guarantees to support its claims. Given the intrinsic complexity of the MINLP formulation, it is particularly commendable that the authors decompose the original problem into a series of per-hyperplane subproblems with unconstrained surrogate objectives, making the optimization process more tractable. \nThe final solution is obtained through gradient-based optimization, which further enhances computational efficiency. Moreover, the proposed adaptive structural adjustment introduces principled hyperplane addition and pruning criteria, enabling the model to maintain scalability without sacrificing geometric tightness."}, "weaknesses": {"value": "The experimental evaluation appears somewhat limited. While the results on the BreastCancer, Spambase, Bace, and HIV datasets demonstrate the feasibility of the proposed approach, these datasets are relatively simple and well-studied binary classification benchmarks. To better assess the generalization ability and robustness of the proposed method, it would be valuable to include experiments on more challenging and diverse datasets, such as those with higher-dimensional features, class imbalance, or more complex data manifolds.\n\nAdditionally, it is not entirely clear whether the optimization process is guaranteed to converge. Since the proposed formulation involves nonconvex components and relies on surrogate objectives and iterative weight updates, a brief theoretical or empirical discussion of the convergence behavior (e.g., monotonicity of the objective, stability of fixed points, or stopping criteria) would strengthen the work and improve the reader’s understanding of the algorithm’s reliability."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "AxjVdMSDqM", "forum": "Kjcs0xJxdg", "replyto": "Kjcs0xJxdg", "signatures": ["ICLR.cc/2026/Conference/Submission11174/Reviewer_c9H4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11174/Reviewer_c9H4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987185822, "cdate": 1761987185822, "tmdate": 1762922331776, "mdate": 1762922331776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Projection Convex Hull (PCH), a scalable and adaptive framework for learning compact, polyhedral trust regions from data consisting of positive and negative points. The fundamental problem of computing the tight convex hull is actually a Mixed-Integer Nonlinear Program (MINLP). Classical geometric methods suffer from complexity that grows exponentially with dimension, and existing optimization-based approaches lack practical scalability.\n\nThe authors overcome this challenge through a rigorous theoretical decomposition. They first formally express the convex hull problem as a MINLP that explicitly accounts for model compactness. Then, they reduce the problem into multiple subproblems that handle one hyperplane at once. The authors prove that the solution for finding the optimal hyperplanes can be recovered as a stationary point of an unconstrained surrogate objective function under suitable weights. This surrogate loss function is minimized using gradient steps. The weight for each negative point is carefully designed and updated during this process to ensure that minimizing the weighted surrogate function is equivalent to maximizing the intended separation margin of the original MINLP.\n\nIn practice, the PCH framework achieves structural adaptivity: it dynamically manages the model complexity by pruning redundant hyperplanes and adding new ones if the current hull still incorrectly encloses negative samples. This ensures the final model is as compact and accurate as possible."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is very well written. It solves the problem gradually by breaking it down into simpler, smaller subproblems, makes the algorithm easy to follow.\n- The method is strong in both theory and practice. PCH maintained high accuracy and low running time and its usage benefits the real-world downstream tasks."}, "weaknesses": {"value": "- More detailed comparison to related gradient-based works are needed.\n- see the question below"}, "questions": {"value": "- Lemma 1 does not ensure that the collection of all optimal solutions of the subproblem is the optimal solution of the original problem. How does the proposed algorithm ensure its global optimality?\n- The adaptive strategy adds hyperplane when contains negative points and removes when encounter redundancy. Then how to decide the number S in practice? And what happened when the positive points are not convex (where the convex hull containing negative points are inevitable) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Jyt8HJrBiN", "forum": "Kjcs0xJxdg", "replyto": "Kjcs0xJxdg", "signatures": ["ICLR.cc/2026/Conference/Submission11174/Reviewer_hxmb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11174/Reviewer_hxmb"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission11174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995663648, "cdate": 1761995663648, "tmdate": 1762922331329, "mdate": 1762922331329, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}