{"id": "BTmc8Llsuz", "number": 18822, "cdate": 1758291144854, "mdate": 1759897079449, "content": {"title": "Pseudo-GT Driven Region-Constrained Black-Box Attack on Semantic Models for Autonomous Driving", "abstract": "Semantic segmentation models have been widely adopted in various domains, including safety-critical applications such as autonomous driving, where they play a pivotal role in enabling accurate scene understanding and decision-making. However, despite their utility and prevalence, these models remain vulnerable to targeted adversarial perturbations, which can compromise their reliability in real-world deployments. To highlight this challenge, we propose a two-stage attack framework that crafts structured perturbations confined to the central vehicle region, inducing misclassifications while preserving background semantics. First, we generate a pseudo-ground-truth segmentation by inpainting the detected vehicle mask within the central third of the image, enabling the attacker to anticipate the model’s response if the target class were absent. Second, we optimize an ℓ∞-bounded perturbation via a hybrid loss combining mean-squared error to the pseudo-ground-truth, total variation regularization for spatial coherence, and a class-wise IoU loss to degrade segmentation across all non-target classes. Finally, we refine the attack using region-specific cross-entropy losses to simultaneously mislead vehicle pixels toward surrounding classes and maintain background consistency. Evaluated on the Cityscapes dataset, our attack achieves over 92% predicted mask accuracy for the target zone and over 93% background preservation elsewhere with Segformer model. These results demonstrate that spatial constraints cannot prevent powerful region-based attacks, underscoring the urgent need for robust defense strategies.", "tldr": "", "keywords": ["Adversarial Attacks & Robustness", "Segmentation", "Vision for Robotics & Autonomous Driving", "Adversarial Learning & Robustness"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dbe88067979681c845c3989f3a4defbe6ac60c63.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper constrains the adversarial perturbation to the central region of the input image. Using inpainting, the pixels of the vehicle within the central region are erased and filled with pixels of the surrounding environment (such as the road and sky), whose semantic segmentation result is then used to supervise the optimization of the central region perturbation."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper enhances the attack stealthiness by constraining the adversarial perturbation to the central region and maintaining the prediction results of other regions. \n\n2. The authors evaluate across three datasets (Cityscapes, KITTI, GTAV) and three different segmentation models (SegFormer, DeepLabV3+, HRNet), and they report multiple metrics"}, "weaknesses": {"value": "1. This article considers the autonomous driving (AD) scenario, but the proposed attack is still digital and cannot be deployed in the real world. Therefore, it cannot threaten the actual application of AD.\n\n2. The optimized perturbation is input-specific and not universal. This means that perturbations need to be generated in real time as the vehicle moves. However, this paper did not discuss the run time. In fact, the method requires ~420 queries per image. Such a query budget would be impractical for attacking a running AD system.  \n\n3. The proposed method is largely a repackaging of well-established components—region masking, inpainting, PGD-style iterative optimization, and standard IoU/TV regularizers. While these are assembled into a pipeline, the conceptual advance beyond prior localized or stealthy adversarial attacks is minimal. The contribution feels more like an incremental engineering variation rather than a substantive new idea.\n\n4. There are currently a large number of global attacks and local attacks, but this paper does not consider baselines for comparison in the experiment.\n\n5. Figure 2 is confusing: Its design initially misled me into thinking that the paper was addressing a poisoning or backdoor attack, rather than an adversarial attack.\n\n6. There is no visualization of the crafted perturbations in experiments."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "A7xLcq8mIN", "forum": "BTmc8Llsuz", "replyto": "BTmc8Llsuz", "signatures": ["ICLR.cc/2026/Conference/Submission18822/Reviewer_f5hn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18822/Reviewer_f5hn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18822/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760524715185, "cdate": 1760524715185, "tmdate": 1762930788116, "mdate": 1762930788116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a two-stage adversarial attack on semantic segmentation for autonomous driving. The method crafts localized perturbations on the central vehicle region using hybrid and region-specific losses to mislead predictions while preserving the background. Experiments on Cityscapes with SegFormer show high attack success, revealing that spatial constraints do not guarantee robustness."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "N/A"}, "weaknesses": {"value": "The paper is not well-written and lacks sufficient comparisons with existing works in the field. \nFor instance, examples of relevant prior works include:\n[A] Pietrosanti et al., “Benchmarking the Spatial Robustness of DNNs via Natural and Adversarial Localized Corruptions,” Pattern Recognition, 2025.\n[B] Rossolini et al., “On the Real-World Adversarial Robustness of Real-Time Semantic Segmentation Models for Autonomous Driving,” IEEE TNNLS, 2023.\n[C] K. K. Nakka and M. Salzmann, “Indirect Local Attacks for Context-Aware Semantic Segmentation Networks,” ECCV, 2020.\n[D] A. Arnab et al., “On the Robustness of Semantic Segmentation Models to Adversarial Attacks,” CVPR, 2018.\n\nOther weaknesses:\n- The introduction is written from a very general perspective, starting by reintroducing the concept of adversarial examples from scratch rather than focusing on the specific technical problem. It lacks depth and technical clarity.\n- In Figure 1, the paper should include a clear visual representation of the input perturbation. This would help the reader understand, from the beginning, the distinction between stealthy and non-stealthy attacks.\n- The related work section completely omits key references, such as those mentioned above, and these works are not discussed anywhere in the main text.\n- The paper is poorly structured, lacks proper experimental comparisons, and provides an insufficient discussion of defense mechanisms. Moreover, the authors do not seem to have evaluated these defenses in depth."}, "questions": {"value": "- Did the authors test any defense mechanisms against the proposed attacks? If not, why were they mentioned in the related work section?\n\n- How is this work relevant compared to previous studies on adversarial attacks for semantic segmentation in driving?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y7BbgtyxPQ", "forum": "BTmc8Llsuz", "replyto": "BTmc8Llsuz", "signatures": ["ICLR.cc/2026/Conference/Submission18822/Reviewer_U52G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18822/Reviewer_U52G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18822/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559115670, "cdate": 1761559115670, "tmdate": 1762930709804, "mdate": 1762930709804, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Semantic segmentation models are critical for applications like autonomous driving but remain highly susceptible to adversarial perturbations. The authors introduce a two-stage region-based attack that targets the central vehicle area while preserving background semantics. First, a pseudo-ground-truth mask is created by inpainting the vehicle region to simulate its absence. Then, an ℓ∞-bounded perturbation is optimized using a hybrid loss combining MSE, total variation, and class-wise IoU terms, followed by region-specific cross-entropy refinement. On Cityscapes, the attack achieves over 92% accuracy within the target zone and 93% background preservation on SegFormer, demonstrating that spatial constraints offer limited defense against structured, localized attacks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors address an important and security-related topic.\n- It's good that three different datasets were tested.\n- The additional metrics PSNR and SSIM used are good.\n- The models used are well-known and frequently applied; it is beneficial that both CNNs and transformers were utilized.\n- I find it beneficial that ablation studies are being conducted."}, "weaknesses": {"value": "- \"recent uncertainty-based detection approaches highlight the need for more targeted defenses (Xu et al., 2021; Halmosi et al., 2024).\" Xu introduces divide-and-conquer adversarial training and Halmosi re-evaluate a number of well-known robust segmentation models in an extensive empirical analysis - so no uncertainty. Uncertainty-based detection occurs here: \"Uncertainty-based Detection of Adversarial Attacks in Semantic Segmentation\" Maag et al.\n- In Figure 2, GT image is more of an RGB image than a mask. \n- In related work, the section on semantic segmentation is very brief/incomplete and not necessarily relevant here, as it deals with attacks and not with developing new models. \n- The related work section is incomplete. E.g. \n1. Section 2.3.: “Universal Adversarial Perturbations Against Semantic Image Segmentation” Metzen et al., as well as other targeted attacks such as FGSM, PGD, and DAG, can be trained so that, for example, only car pixels are converted into roads. \"Chen et al. (2022) proposed semantically stealthy patches\" They do not use patches. \n2. In Section 2.4, \"Recent transfer-based approaches Agnihotri et al. (2024) enhance black-box efficacy\" Agnihotri present white-box adversarial attacks. \n3. In section 2.5, the defense/detection methods should also be for semantic segmentation, e.g. \"Improved Noise and Attack Robustness for Semantic Segmentation by Using Multi-Task Training with Self-Supervised Depth Estimation\" Klingner et al, \n\"Characterizing Adversarial Examples Based on Spatial Consistency Information for Semantic Segmentation\" Xiao et al, \n\"Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation\" Yatsura et al.\n- The paper is not written very clearly. For example, there is no source reference for Telea's algorithm in section 3.2, the heading in section 4 is blue, and references to the appendix are incomplete.\n- Accuracy as the sole metric is less meaningful than mIoU, as large classes simply dominate.\n- There were no baseline/state-of-the-art comparisons with other attacks."}, "questions": {"value": "- In section 2.3, is the point here that not all pixels receive noise, as is the case with patch attacks, for example, in “Evaluating the Robustness of Semantic Segmentation for Autonomous Driving\nagainst Real-World Adversarial Patch Attacks” by Nesti et al, or that not all pixels are attacked in the prediction? This is unclear here. \n- Why is the number of forward queries per image so high, and doesn't that cause the runtimes to increase dramatically?\n- \"We obtain an instance segmentation of vehicles\" Why use instance segmentation rather than semantic segmentation if no distinction is made between instances in the further course?\n- Adamstep is used and the gradient of the loss function is determined, so access to the model is required and therefore a whitebox setting? In addition, the method requires access to the logits, which corresponds more to a gray box than a black box.\n- What are the training/test splits for the datasets? Because the three models used were each trained on each dataset, right? \n- \"Results across models and scenarios show that, for DeepLabV3+ (ResNet101) and SegFormer, generating a random square area with a random class is simpler than inpainting and creating fake masks.\" Yes, that makes sense. Was this compared or done somewhere?\n- Why does background accuracy increase with the perturbation level? I would actually say that the stronger the attack, the more likely it is to affect background pixels."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oy4Ialq9rS", "forum": "BTmc8Llsuz", "replyto": "BTmc8Llsuz", "signatures": ["ICLR.cc/2026/Conference/Submission18822/Reviewer_BgSG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18822/Reviewer_BgSG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18822/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761670103189, "cdate": 1761670103189, "tmdate": 1762930652337, "mdate": 1762930652337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work explores the black-box attack on semantic segmentation task, by proposing a two-stage query-based method. In particular, detected vehicle masks are filled into one-third of the input image region to generate a pseudo ground-truth segmentation image. Meanwhile, mean squared error loss, spatial consistency loss, and class intersection-over-union loss are employed to misclassify the target category. Then, region-specific cross-entropy loss is used to mislead vehicle pixels toward surrounding categories while preserving background consistency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. To achieve stealthy attacks, the authors introduce a combination of mean squared error loss, spatial consistency loss, class intersection-over-union loss, and region-specific cross-entropy loss to jointly optimize the adversarial perturbations. The idea seems nice.\n\n2. This manuscript is generally well-structured and easy to read."}, "weaknesses": {"value": "1. Lack of comparison with other query-based black-box attack methods. The experiments in the paper only compare different variants of the proposed method itself, without including other existing query-based black-box attack approaches. If there are currently no query-based black-box attacks specifically designed for semantic segmentation, the authors could introduce and compare with traditional image-level query-based methods as baselines.\n2. Limited evaluation across different scenarios. The authors specifically target the autonomous driving scenario, performing stealthy attacks on vehicles located in the front one-third region of the image. However, they should also consider attacking other critical objects such as pedestrians or other potential safety hazards to demonstrate broader applicability and effectiveness.\n3. Unclear motivation. The authors primarily rely on pre-generated pseudo-label masks and apply a targeted adversarial attack to achieve stealthiness. However, their key contribution appears to lie in the generation of these pseudo-labels, while the rationale and justification for combining multiple loss functions in the subsequent stages are not clearly explained or grounded in solid methodology.\n4. Insufficient ablation studies and visualization. The authors lack visual comparisons demonstrating the impact of different loss functions through ablation studies. Additionally, important evaluation metrics such as computational time cost (e.g., frames per second, FPS) are missing. Furthermore, the authors should provide results showing how attack effectiveness evolves with the number of queries to better evaluate query efficiency.\n5. There are many semantic segmentation benchmarks, but this work only focuses on Cityscapes and a little on KITTI, which is insufficient to provide solid experimental results.\n6. The implementation details are missing without any code attached."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vhRhHv3NHM", "forum": "BTmc8Llsuz", "replyto": "BTmc8Llsuz", "signatures": ["ICLR.cc/2026/Conference/Submission18822/Reviewer_ktsQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18822/Reviewer_ktsQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18822/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913423495, "cdate": 1761913423495, "tmdate": 1762930581974, "mdate": 1762930581974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}