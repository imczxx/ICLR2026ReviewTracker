{"id": "vQXnPpi74f", "number": 15392, "cdate": 1758250895251, "mdate": 1759897309863, "content": {"title": "SSL4RL:  Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning", "abstract": "Vision-language models (VLMs) have shown remarkable abilities by integrating large language models with visual inputs. However, they often fail to utilize visual evidence adequately, either depending on linguistic priors in vision-centric tasks or resorting to textual shortcuts during reasoning. Although reinforcement learning (RL) can align models with desired behaviors, its application to VLMs has been hindered by the lack of scalable and reliable reward mechanisms. To overcome this challenge, we propose \\textbf{SSL4RL}, a novel framework that leverages self-supervised learning (SSL) tasks as a source of verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL objectives—such as predicting image rotation or reconstructing masked patches—into dense, automatic reward signals, eliminating the need for human preference data or unreliable AI evaluators. Experiments show that SSL4RL substantially improves performance on both vision-centric and vision-language reasoning benchmarks. Furthermore, through systematic ablations, we identify key factors—such as task difficulty, model scale, and semantic alignment with the target domain—that influence the effectiveness of SSL4RL tasks, offering new design principles for future work. We also demonstrate the framework’s generality by applying it to graph learning, where it yields significant gains. SSL4RL establishes a versatile and effective paradigm for aligning multimodal models using verifiable, self-supervised objectives.", "tldr": "", "keywords": ["Vision Language Models", "Self-supervised Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ec7d3f276601a2a294a0e25ed60237417fdbbbe8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "SSL4RL reframes classic self-supervised tasks (rotation, jigsaw, contrastive, position) as verifiable intrinsic rewards for RL post-training, using GRPO to improve visual grounding and reasoning without human judges. It achieves significant gains on MMBench, SEED-Bench, and ImageNet-1K, identifies key design factors (task difficulty, model scale, semantic alignment), and generalizes to graph learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well written with clear and smooth logic.\n- The method is novel, with rich experiments validated across different tasks.\n- The analysis is comprehensive, including relationships between performance and task difficulty, as well as model size."}, "weaknesses": {"value": "- What is the method’s performance upper bound, and how does it scale with data? At present, the achievable ceiling appears limited.\n- The experiments are not sufficient: the VLM benchmarks include only MMBench and Seed-Bench, and the validation uses only the Qwen-2.5-VL series. It would be better to verify effectiveness on other VLMs and benchmarks.\n- The tables in the main text detail category-wise performance on MMBench and Seed-Bench, but there is no analysis of how different self-supervised tasks impact performance on different benchmark sub-tasks."}, "questions": {"value": "- Characterize the relationship between performance and data scaling: how does the method’s effectiveness evolve as data volume increases?\n- Validate on additional benchmarks and train models beyond the Qwen-2.5-VL series to assess generality.\n- Analyze how different self-supervised tasks affect performance across distinct benchmark sub-tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wSesVSk7Yl", "forum": "vQXnPpi74f", "replyto": "vQXnPpi74f", "signatures": ["ICLR.cc/2026/Conference/Submission15392/Reviewer_5KPi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15392/Reviewer_5KPi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760771806865, "cdate": 1760771806865, "tmdate": 1762925672162, "mdate": 1762925672162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method that uses the verifiability of self-supervised learning tasks to enable RLVR on LVLMs without other labels. The method is interesting and clever."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is clever.\n2. he writing is clear, experiments are solid."}, "weaknesses": {"value": "1. The tested benchmarks are somewhat limited. It's unclear how this method affects the model's multimodal alignment and visual reasoning.\n2. Only tested on the Qwen-VL architecture, which limits the generalizability of the method."}, "questions": {"value": "Obviously, this method may enhance the model's robustness. Have the authors evaluated this type of metric?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6XZG8sm8PU", "forum": "vQXnPpi74f", "replyto": "vQXnPpi74f", "signatures": ["ICLR.cc/2026/Conference/Submission15392/Reviewer_Sy7L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15392/Reviewer_Sy7L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994309592, "cdate": 1761994309592, "tmdate": 1762925671694, "mdate": 1762925671694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reframes classic self-supervised tasks (rotation, jigsaw, contrastive, patch position) as verifiable rewards for RL training of VLMs and optimizes them with GRPO. Mainly on 3B level models, it reports consistent gains on MMBench and SEED-Bench. Ablations reveal that task difficulty must match model capacity, gains diminish at 7B scale, and naive multi-reward mixing does not help."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of using SSL tasks as verifiable RL rewards is simple and practical, avoiding noisy LLM-as-judge signals, however, it can only apply to certain few tasks (e.g., image captioning is not feasible).\n2. The experimental spans reasoning benchmarks, ImageNet, and even graph tasks, showing some cross-domain generality.\n3. The presented qualitative attention maps and error analyses help connect the method to observed reductions in language priors."}, "weaknesses": {"value": "1. The baseline coverage is limited: there are no direct comparisons with strong RLHF/DPO or verifier-driven multimodal RL training pipelines, making it hard to judge competitiveness. Also, the improvements on some cases are very marginal.\n2. The 7B results show small gains, supporting a capacity–difficulty mismatch; the method’s effectiveness at modern larger scales remains unclear without harder SSL tasks.\n\n3. Since the RL training is often unstable, it would be good to see the training dynamics such as loss curve and entropy.\n4. Can the authors provide some insights into not-easy-to-verify tasks, since the chosen tasks are very easy being converted to verifiable rewards."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p43ySbcfOg", "forum": "vQXnPpi74f", "replyto": "vQXnPpi74f", "signatures": ["ICLR.cc/2026/Conference/Submission15392/Reviewer_1F8A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15392/Reviewer_1F8A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762382784697, "cdate": 1762382784697, "tmdate": 1762925671328, "mdate": 1762925671328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}