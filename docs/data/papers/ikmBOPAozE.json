{"id": "ikmBOPAozE", "number": 1434, "cdate": 1756882112752, "mdate": 1759898209348, "content": {"title": "Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?", "abstract": "Large reasoning models (LRMs) with multi-step reasoning capabilities have shown remarkable problem-solving abilities, yet they exhibit concerning safety vulnerabilities that remain poorly understood. In this work, we investigate why safety alignment fails in reasoning models through a mechanistic interpretability lens. Using a linear probing approach to trace refusal intentions across token positions, we discover a striking phenomenon termed as \\textbf{refusal cliff}: many poorly-aligned reasoning models correctly identify harmful prompts and maintain strong refusal intentions during their thinking process, but experience a sharp drop in refusal scores at the final tokens before output generation. This suggests that these models are not inherently unsafe; rather, their refusal intentions are systematically suppressed. Through causal intervention analysis, we identify a sparse set of attention heads that negatively contribute to refusal behavior. Ablating just 3\\% of these heads can reduce attack success rates below 10\\%. Building on these mechanistic insights, we propose \\textbf{Cliff-as-a-Judge}, a novel data selection method that identifies training examples exhibiting the largest refusal cliff to efficiently repair reasoning models' safety alignment. This approach achieves comparable safety improvements using only 1.7\\% of the vanilla safety training data, demonstrating a less-is-more effect in safety alignment.", "tldr": "", "keywords": ["safety", "alignment", "mechanistic interpretability", "llm", "reasoning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1595f49370a9565f082d27138ec83aed31d3ae69.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Reasoning models seem to exhibit more safety vulnerabilities than non-reasoning models; that is, they tend to be less likely to refuse harmful requests. This paper investigates *why* reasoning models are less robust. The authors train a probe on model activations to predict refusal, and find that the probes scores are fairly strong throughout the reasoning trace, but abruptly drop (\"fall off a cliff\") at the end. The authors find that there is a small set of heads that suppress refusal. The authors additionally use the probe score as a way of efficiently selecting good datapoints to improve robustness, by selecting datapoints where the \"refusal cliff\" is most prominent."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Cheap probe-based data selection\n\t- The final section suggests an idea to cheaply identify training data that would most effectively safety tune a model. It works by identifying samples with the largest \"refusal cliff\", as these are intuitively the most \"problematic\" samples, that the model could learn most from. Going from the \"refusal cliff\" observation to this data selection procedure is a nice, practical idea."}, "weaknesses": {"value": "- Insufficient technical detail\n\t- I feel that there is insufficient technical detail for me to confidently believe in the results.\n\t- See the questions section for a list of my questions regarding technical details, some of which are critical to interpreting the main claims of the paper (e.g., the main claim of a \"refusal cliff\" is predicated on the refusal probe results, which are in turn predicated on the technical details of probe training and dataset construction).\n- Probe design concerns\n\t- The probe is trained only on final token positions at the final layer, but then applied across all positions and potentially all layers. This distribution shift is not addressed, raising questions about what the probe actually measures at intermediate positions.\n\t- Training on the final layer at the final position likely captures something close to the output logit direction (e.g., \"Sorry\" vs \"Sure\" unembedding), which may not reflect genuine \"refusal intention\" at earlier reasoning steps.\n- Shallow mechanistic analysis\n\t- Any direction that shows up in the residual stream must be written to by some model component (an attention head or MLP). It is thus not surprising to find a set of model components writing to the probe direction, or that ablating these model components weakens the probe direction. It is therefore unclear to me why the section on \"refusal suppression heads\" is included in the manuscript; I don't think it adds to any understanding of what's going on.\n- Lacks statistical rigor\n\t- There are no error bars or confidence intervals in any of the results.\n- Clarity and presentation issues\n\t- Figure 5's \"thinking pruning\" terminology is confusing (does 100% mean fully pruned or fully retained?).\n\t- Figure 8's grouping makes within-model comparisons difficult.\n\t- Unclear how to reconcile Figure 7 with Table 2."}, "questions": {"value": "- What are the details of assessing the safety of generations?\n\t- The paper explains the LlamaGuard-4 is used. What text is given to LlamaGuard-4 for judgement? Is it just the final output (excluding reasoning)? Or is it the assistant's whole response (including reasoning)? \n\t- Also, in the paragraph \"Refusal Prober\", it suggests that you assess safety by searching for individual substrings like \"Sorry, I cannot\" (although details are not provided). I think having one standardized way of categorizing \"refusal\" vs \"non-refusal\" would make more sense.\n- Refusal prober details\n\t- Why is the probe trained on the final token position?\n\t\t- The main experiment runs the probe over all token positions. Isn't this potentially very out of distribution for the probe? If the intention is to run the probe over all token positions, why not train it on activations from all token positions?\n\t- Why is the probe trained on the final layer?\n\t\t- The final layer likely contains most strongly information about the unembeddings. Since you're probing at the final token position, this likely corresponds to something like the \"Sorry\" unembedding direction minus the \"Sure\" unembedding direction (not sure about the exact tokens, since this will depend on what tokens the model prefers to start its refusal vs non-refusal responses with, but hopefully you get the point).\n\t\t- Why is this a reasonable thing to do?\n- Dataset construction\n\t- What are the details of dataset construction?\n\t\t- For example, for each model that you train a probe for, do you generate completions from that model on your dataset of prompts, and then use those rollouts to train the probes for that model? Or do you do rollouts from a single model, and use that single model to train probes across all models? What are the details of generation? Given a reasoning trace, does the model consistently refuse or not refuse? Or is it probabilisitic?\n\t\t- Do you include any harmful prompts that are not refused?\n\t- For the JailbreakBench OOD evaluation, is this dataset balanced? I'd think that JailbreakBench only contains harmful prompts that should be refused. Is there also a harmless OOD dataset that you use to balance the total OOD set?\n- Figure clarifications\n\t- In Figure 5, what does \"thinking pruning\" mean? Does 100% thinking pruning mean that 100% the reasoning has been pruned, and so there is no reasoning left? Or does it mean that 100% of the reasoning remains (e.g., nothing has been pruned)?\n- Attention head interventions\n\t- In Section 4.2, at which token position(s) is the intervention performed?\n\t\t- Consider changing the grouping of bars in Figure 8 so that each group is the same model, and the colors differentiate the intervention. We ideally want to compare the interventions within each model.\n\t- In equation 4, why include the bias term?\n\t\t- I believe the bias should not be included, as you are interested in whether heads contribute positively or negatively; adding a bias can change this. E.g., if the bias is positive, then a negative head contribution might show up as positive. \n\t\t- It's also unclear how this score is \"A score close to 1 indicates that the head promotes refusal, whereas a score close to 0 implies that it suppresses refusal.\" There's no reason for the result of equation 4 to be within 0-1. In fact, in Figure 6, they are clearly not.\n- Cliff-as-a-judge experiments\n\t- How did you choose the number of examples for each method in Table 2?\n\t- How do you explain the discrepancy between Figure 7 and the claim that \"our approach optimally balances data efficiency with safety performance\"?\n\t\t- Figure 7 seems to show that LLM-as-a-judge is as good (or even better) than cliff-as-a-judge."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iVrQwXSEJ5", "forum": "ikmBOPAozE", "replyto": "ikmBOPAozE", "signatures": ["ICLR.cc/2026/Conference/Submission1434/Reviewer_tcSd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1434/Reviewer_tcSd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760991178895, "cdate": 1760991178895, "tmdate": 1762915768370, "mdate": 1762915768370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates an interesting and important phenomenon that reasoning models' refusal intention abruptly falls the cliff before generating the final output. This case is related to previous work like llm faithfulness[1][2] and model internal representation[3][4]. Through causal intervention, the authors figure out a set of “Refusal Suppression Heads” responsible for the cliff. Building on this insight, they propose Cliff-as-a-Judge, a data selection method that prioritizes training examples with the largest refusal cliffs, achieving strong safety improvements with only 1.7% of standard safety data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The investigated question is truly important and realistic, which makes the motivation strong and meaningful.\n\n2. The previous rethinking part is meticulous and convincing with detailed ablation studies.\n\n3. The data selection method to tackle this problem is impressive, with fewer data to achieve a better defense performance.\n\n4. The authors include different types and sizes reasoning models in the rethinking part, which convinces the experimental results of this paper. \n\n5. The presentation is good and easy to follow."}, "weaknesses": {"value": "1. This paper doesn't investigate the over-refusal problem after data selection. It's unclear if the low ASR comes from over-refusal to input questions.\n\n2. Human evaluation is lacked. The experiment ASR is measured by LLaMA-Guard-4, however, the human evaluation is lacked. It's very important to include human evaluation to reduce the False Positive Rate and False Negative Rate. This makes the ASR reported in the article less convincing.\n\n3. I really like the probing part (Lines 194-206) that the author separates the problem from an in distribution(trained on vanilla) and out of distribution(test on adversarial jailbreak) aspect. However, the main method of this paper: CLIFF-AS-A-JUDGE utilizes both \"vanilla attacks and more challenging adversarial jailbreak cases\" as the training dataset (Lines 421-423), which mixes the in distribution and out of distribution setting. Therefore, the testing results in Table 2 seems to become an in distribution test. Out of distribution test is truly important in safety alignment because attacks are often unseen and unknown[1][2][3]. Therefore I strongly suggest the authors to include \nat least some unseen attacks like GCG, Simple Adaptive Attacks, Code Attacks to demonstrate the generalization ability of their method, or separate clearly the in distribution and out of distribution settings in training and testing time.\n\n4. The Cliff-as-a-Judge method requires access to internal hidden states, so it's a white box setting. However, this paper mainly include black-box attacks. It remains unclear if it's also effective on white box attacks.\n\n5. It's OK to test with MMLU-Pro and ARC-C to show the reasoning ability, but the general ability like if-eval, Alpaca-eval, the hard math ability like MATH is also important for a reasoning model.\n\n6. The investigated problem is similar to the unfaithfulness[4] and hallucination[5] problem of Large Reasoning Models. So please include some discussion on the similarity and difference between them.\n\n[1] Improving Alignment and Robustness with Circuit Breakers\n\n[2] Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training\n\n[3] Safety Reasoning with Guidelines\n\n[4] Reasoning Models Don't Always Say What They Think\n\n[5] Are Reasoning Models More Prone to Hallucination?"}, "questions": {"value": "1. How do you see the \"REFUSAL FALLS OFF A CLIFF\" phenomenon's connection to the unfaithfulness and hallucination of LRMs?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9JNKBmjNIb", "forum": "ikmBOPAozE", "replyto": "ikmBOPAozE", "signatures": ["ICLR.cc/2026/Conference/Submission1434/Reviewer_1dk4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1434/Reviewer_1dk4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705180556, "cdate": 1761705180556, "tmdate": 1762915768174, "mdate": 1762915768174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why safety alignment often fails in Large Reasoning Models (LRMs) by introducing a novel phenomenon called the Refusal Cliff—where models maintain strong refusal intentions throughout the reasoning process but suddenly lose them at the final output stage.\nThey propose Cliff-as-a-Judge, a data selection method that leverages internal representation signals to select high-impact alignment samples, achieving comparable safety improvement with only 1.7% of standard training data. The work aims to offer both mechanistic insight and practical efficiency in safety alignment for reasoning models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a clear, mechanistic explanation for why LRMs sometimes fail to refuse harmful prompts despite seemingly correct internal reasoning. The identification of Refusal Suppression Heads offers a concrete interpretability advance.\n\n2. The proposed Cliff-as-a-Judge achieves significant safety improvement with minimal fine-tuning data, showing cost-effectiveness and potential for scalable safety alignment."}, "weaknesses": {"value": "1. The paper does not clearly justify why the proposed approach is necessary, given that many recent reasoning-aligned models (e.g.,  Qwen3-30B, RealSafe) already achieve near-perfect refusal rates. The incremental need over existing safety-aligned models is not well articulated.\n\n2. Only two benchmarks (JailbreakBench and WildJailbreak) are used, which are relatively simple. Missing evaluations on over-refusal (false positive safety behavior) and generalization benchmarks such as WildChat or OR-Bench.\n\n3. The paper should include a direct comparison to STAR-1: Safer Alignment of Reasoning LLMs with 1K Data, which also claims efficient safety alignment with small datasets. Without this, the “less-is-more” claim remains weakly supported.\n\n4. The paper’s flow is somewhat confusing, the method section appears too late, and the transitions between probing analysis and data-selection pipeline could be reorganized for clarity."}, "questions": {"value": "1. How would the proposed approach work for closed-source reasoning models (e.g., GPT, Gemini), where internal representations cannot be accessed?\n\n2. Since the probe can accurately estimate refusal behavior, could it be integrated into reinforcement learning frameworks to guide alignment dynamically rather than only during SFT?\n\n3. Why was STAR-1 excluded as a baseline, given that it also focuses on aligning reasoning LLMs with minimal data (1K samples)?\n\n4. Has the method been tested on harmless prompts to ensure it does not increase over-refusal or degrade helpfulness?\n\n5. Considering some aligned models already achieve almost perfect safety, in what scenarios would Cliff-as-a-Judge still be necessary or beneficial?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pbX9GrEYKw", "forum": "ikmBOPAozE", "replyto": "ikmBOPAozE", "signatures": ["ICLR.cc/2026/Conference/Submission1434/Reviewer_K119"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1434/Reviewer_K119"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916618395, "cdate": 1761916618395, "tmdate": 1762915768067, "mdate": 1762915768067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a safety-specific vulnerability in reasoning models — the **refusal cliff**. The phenomenon describes how, during generation, a model may reverse its refusal behavior just before the final few tokens, resulting in failure to reject risky prompts that should have been refused. Based on this observation, the authors further propose a mechanism-driven, efficient refusal data selection approach: by incorporating samples with significant refusal cliff characteristics into training, they achieve better refusal effectiveness, while maintaining stable performance on MMLU and ARC."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Defines an intriguing phenomenon — **refusal cliff** — and proposes a novel explanatory mechanism for the stability (or instability) of model outputs.\n2. Uses causal intervention to identify a sparsely distributed set of attention heads in the Transformer (only about 3% of heads) that negatively regulate refusal behavior in deep layers.\n3. Proposes an efficient method for generating safety-related training data: leverages the refusal cliff to selectively choose data, achieving near-baseline safety performance using only 10% of the data — without performance degradation on MMLU and ARC."}, "weaknesses": {"value": "1. Although the authors initially pose the research question *“What mechanism makes the safety alignment vulnerable in reasoning models?”*, the premise of this question is debatable. Many prior studies have found that safety alignment in reasoning models is actually *more* robust than in non-reasoning models. The claimed vulnerability seems more related to training data issues than an inherent mechanism. To argue for a mechanism-level cause, more experiments are needed to exclude data-related effects.\n2. The assumption that *refusal ≈ safety* is insufficient; more clarification or a scope limitation is needed. Similarly, using a judger that detects refusal only by matching keywords like “I can’t” risks substantial false positives.\n3. Using linear probes to predict refusal behavior may overlook complex non-linear interactions. Although linear models have strong interpretability, the actual refusal mechanism may involve high-dimensional features; thus, a linear probe can only reflect correlation, not conclusively prove the existence of a “refusal intent.” High probe scores may merely indicate harmfulness detection rather than the actual refusal decision.\n\n-  **Typos**\n1.  Line 41 zhang2025realsafe\n2. Line 43， invaluable insights？"}, "questions": {"value": "1. Current experiments are insufficient. It remains unverified whether the refusal cliff inevitably occurs under different training methods, reasoning chain lengths, and safety paradigms. The set of training methods, baseline models, and benchmarks is small. The authors should also evaluate on over-refusal benchmarks (e.g., XSTest) to check if their method introduces excessive refusals.\n2. The authors mention the “superposition” phenomenon (a single activation vector may contain multiple sub-directions). Is there a more theoretical way to explain why the experimental results are not affected by this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GFYe5AlHVT", "forum": "ikmBOPAozE", "replyto": "ikmBOPAozE", "signatures": ["ICLR.cc/2026/Conference/Submission1434/Reviewer_jYPS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1434/Reviewer_jYPS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762092729256, "cdate": 1762092729256, "tmdate": 1762915767923, "mdate": 1762915767923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}