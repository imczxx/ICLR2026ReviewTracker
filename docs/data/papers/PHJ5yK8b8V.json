{"id": "PHJ5yK8b8V", "number": 11917, "cdate": 1758204633423, "mdate": 1759897546172, "content": {"title": "PerfCoder: Large Language Models for Interpretable Code Performance Optimization", "abstract": "Large language models (LLMs) have achieved remarkable progress in automatic code generation, yet their ability to produce high-performance code remains limited—a critical requirement in real-world software systems. We argue that current LLMs struggle not only due to data scarcity but, more importantly, because they lack supervision that guides interpretable and effective performance improvements.\nIn this work, we introduce **PerfCoder**, a family of LLMs specifically designed to generate performance-enhanced code from source code via interpretable, customized optimizations. PerfCoder is fine-tuned on a curated collection of real-world optimization trajectories with human-readable annotations, and preference-aligned by reinforcement fine-tuning using runtime measurements, enabling it to propose input-specific improvement strategies and apply them directly without relying on iterative refinement. \nOn the PIE code performance benchmark, PerfCoder surpasses all existing models in both runtime speedup and effective optimization rate, demonstrating that performance optimization cannot be achieved by scale alone but requires optimization stratetgy awareness. In addition, PerfCoder can generate interpretable feedback about the source code, which, when provided as input to a larger LLM in a planner-and-optimizer cooperative workflow, can further improve outcomes. Specifically, we elevate the performance of 32B models and GPT-5 to new levels on code optimization, substantially surpassing their original performance.", "tldr": "", "keywords": ["LLM", "Code Optimization", "Code Generation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a623f32906171159b2f0ef1a52f6dd6feed84124.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces **PerfCoder** to address the gap between functionally correct code generation and high-performance code optimization. The authors argue that existing LLMs fail at performance tasks due to a lack of supervision that connects optimizations to interpretable, human-readable strategies. They fine-tune PerfCoder under a two-stage fine-tuning process that first supervised fine-tune on a PIE restructure, and then using GRPO for futher refinement. Experiments on the PIE benchmark show that PerfCoder (7B) outperforms all single-step baselines in terms of runtime speedup, demonstrating the effectiveness and transferability of its learned strategies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Well-Defined Problem:** The paper tackles a critical and high-value limitation of current code-generation models. Moving from functional correctness to performance optimization is a necessary next step for LLMs to be truly useful in production software engineering.\n2. **Small Model as Planer:** The insight that a small, specialized model can generate high-quality, interpretable strategies to guide a large, general-purpose optimizer model is impressive."}, "weaknesses": {"value": "1. **Generalizability and Benchmark Limitations:** The models are trained on a reconstructed PIE dataset and evaluated on the original PIE benchmark, which may favor a particular subset of benchmark but not representative of real-world code. The transferability study to PolyBenchC is a good addition, while the resulting speedup is modest (1.053), suggesting the learned strategies may not generalize well to other domains without further domain-specific fine-tuning. \n2. **Reliance on Oracle Training Data:** The \"Global-best replacement\" strategy requires access to the best-known solution for a given problem to create the training data. This strong oracle assumption is not that practical for most real-world software engineering tasks. This limits the scalability of the data creation pipeline to new, arbitrary codebases.\n3. **RL Reward Mechanism:** The reward signal for GRPO is R = speedup^2 for successful optimizations. This non-linear, unbounded reward could be noisy and unstable, disproportionately rewarding simple fixes with massive speedups (e.g., 10x speedup -> 100 reward) over more complex optimizations with modest gains (e.g., 2x speedup -> 4 reward).  The underlying reward signal itself is highly skewed (even GRPO has group normalization), which could bias the model toward low-hanging fruit. Moreover, I have concern on the sparse nature of the reward function when most solutions fail to compile or slower than the baseline.\n4. **Minor**: While the paper defines its terms, the methodology in Section 2 can be dense. The introduction of multiple terms (e.g., \"PerfCoder Jr.\", \"plan+code mode\", \"plan-only mode\", \"[SUGG/]\", \"[OPT/]\") without in-place explaination makes the pipeline slightly difficult to follow on an initial read."}, "questions": {"value": "1. To improve reproducibility, could the authors please provide more specific details on the GRPO training? \n\n2. In Section 2.1, the notation x^(u,p) is used. It appears the user (u) and promble (p) are simply indexs to identify a unique data point (a specific user's submission to problem p). Is this understanding correct, or is the user identity somehow used as a feature trained in the model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "07wIWCt8Cu", "forum": "PHJ5yK8b8V", "replyto": "PHJ5yK8b8V", "signatures": ["ICLR.cc/2026/Conference/Submission11917/Reviewer_ouK2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11917/Reviewer_ouK2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761416725300, "cdate": 1761416725300, "tmdate": 1762922923730, "mdate": 1762922923730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PerfCoder, where LLMs are fine-tuned to produce human-readable optimization strategies and the corresponding optimized code in a single pass. Further alignment with GRPO shows additional improvements. On the PIE dataset, the best 7B shows a 2.5x single-step speedup and is a strong planner for larger models, and also performs better than a collection of open/closed models on code performance optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The optimization pipeline is interpretable, the model includes a suggestion block and an optimization block during generating optimized code, making edits auditable and reusable; Fig.3 makes this easy to understand\n- Solid one-shot results with PerfCoder-QC-7B, with 2.5x improvement. Two step procedure also shows good improvements on larger models like Qwen2.5 32B and GPT-5.\n- Reconstruct and rebalance the PIE dataset to reduce category bias and target clear optimization endpoints."}, "weaknesses": {"value": "- When comparing against the PIE paper, PerfCoder's \"PIE-xxx-HQ\" baselines are plain SFT models on the HQ dataset; the PIE paper shows that performance conditioning during training can make the best@1 and best@8 speedups significantly better. The performance-conditioned baselines are not included in PerfCoder's main table. \n- Lines 86-87 mention PIE-CodeLlama at 1.89x speedup while Table 2 lists 1.73x.\n- PerfCoder says it follows the PIE evaluation protocol, but does not explicitly confirm gem5 usage, which is central to PIE's deterministic evaluation. Clarification is needed for reproducibility."}, "questions": {"value": "- Did the “PIE‑CodeLlama‑HQ / PIE‑Qwen2.5‑Coder‑HQ” baselines include performance‑conditioning at training time or target performance tags at inference (for example 10/10) as in the best PIE results? \n- The original PIE paper used a temperature of 0.7 during decoding. Is there a reason to use greedy search in PerfCoder? How would sampling with T=0.7/1.0 affect the results of this paper?\n- Is the evaluation of speedups in gem5, or on host hardware? If gem5 is not used, please detail the hardware, OS isolation, compiler, and flags (PIE uses GCC9.3.0 -o3), and how you control variance, so that the results in the paper are reproducible."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pJy4DY1dSy", "forum": "PHJ5yK8b8V", "replyto": "PHJ5yK8b8V", "signatures": ["ICLR.cc/2026/Conference/Submission11917/Reviewer_qh5w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11917/Reviewer_qh5w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980459037, "cdate": 1761980459037, "tmdate": 1762922923392, "mdate": 1762922923392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PerfCoder, a system that trains an LLM to emit a structured optimization plan (comprising strategy categories and short explanations) and then generate the optimized code in a single pass using control tokens. This enables the model to function both as an end-to-end optimizer and as a planner that produces strategies for a larger, frozen \"optimizer\" LLM to follow. The planner is further fine-tuned with RL. Reported results on PIE show a 7B single-step model achieving a 2.50× average speedup when deployed as a planner. The training data is reconstructed to retain only the final (or global-best) submissions, organized into 15 optimization categories."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Planner-only RL for optimization: The GRPO setup rewards strategies based on measured speedups while keeping the optimizer model frozen. This is my favorite contribution from the paper.\n\n* Strong empirical results: The single-step 7B model achieves a reported 2.50× speedup, and the planner substantially boosts larger models, suggesting that the emitted plans are actionable (but see my notes on the harness)."}, "weaknesses": {"value": "* The relationship and differences between PerfCoder and PIE's CoT/rationale require clearer explanation (reason for overall rating = 4)\n\nPerfCoder claims to \"generate human-readable optimization strategies tailored to the program and apply them transparently,\" which closely resembles PIE's approach via reasoning/rationales (CoT-style). To better clarify the comparative value, consider including the following baseline comparisons: (a) a strong free-form CoT (no schema) planner using the same RL, and (b) a retrieval-augmented edit learner (retrieving top-K slow => fast exemplars) without schema, both with and without RL. These comparisons are necessary to determine whether PerfCoder's structured schema and planner-RL approach offers substantial improvements over PIE-style CoT or retrieval methods, rather than simply reframing similar strategies.\n\n\n* Runtime measurement details are insufficient for a speedup-focused paper (reason for overall rating and soundness score = 2)\n\nThe paper does not fully specify the measurement harness (hardware, compiler & flags, warm-ups, aggregation method, etc.). Clear reporting of these elements is critical for program optimization work. Please confirm that you will release the exact runtime harness (scripts, Docker configuration, CPU pinning details) and report variance/confidence intervals. Explicitly document compiler flags and any gcov usage."}, "questions": {"value": "Please see weaknesses. I also had two nits (not important, please don't worry about addressing these)\n\n\n1. Confining suggestions to 15 categories may help with supervision but constrains the hypothesis space, potentially omitting complex tactics.\n\n2. On a related note, using only the last submission (or global-best replacement when the last is far from optimal) simplifies labeling but discards intermediate trajectories that often contain useful micro-optimizations and diverse tactics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tBOcZg7zJ5", "forum": "PHJ5yK8b8V", "replyto": "PHJ5yK8b8V", "signatures": ["ICLR.cc/2026/Conference/Submission11917/Reviewer_hkgH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11917/Reviewer_hkgH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762116419887, "cdate": 1762116419887, "tmdate": 1762922922921, "mdate": 1762922922921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}