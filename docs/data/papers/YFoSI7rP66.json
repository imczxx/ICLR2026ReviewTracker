{"id": "YFoSI7rP66", "number": 15773, "cdate": 1758255108581, "mdate": 1759897283183, "content": {"title": "Personalized Prediction By Learning Halfspace Reference Classes Under Well-Behaved Distribution", "abstract": "In machine learning applications, predictive models are trained to serve future queries across the entire data distribution. Real-world data often demands excessively complex models to achieve competitive performance, however, sacrificing interpretability. Hence, the growing deployment of machine learning models in high-stakes applications, such as healthcare, motivates the search for methods for accurate and explainable predictions. This work proposes a Personalized Prediction scheme, where an easy-to-interpret predictor is learned per query. In particular, we wish to produce a \"sparse linear\" classifier with competitive performance specifically on some sub-population that includes the query point. The goal of this work is to study the PAC-learnability of this prediction model for sub-populations represented by \"halfspaces\" in a label-agnostic setting. We first give a distribution-specific PAC-learning algorithm for learning reference classes for personalized prediction. By leveraging both the reference-class learning algorithm and a list learner of sparse linear representations, we prove the first upper bound, $O(\\mathrm{opt}^{1/4} )$, for personalized prediction with sparse linear classifiers and homogeneous halfspace subsets. We also evaluate our algorithms on a variety of standard benchmark data sets.", "tldr": "", "keywords": ["PAC-learning", "personalized learning", "agnostic learning", "selective classification"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8aca305274356220ff2e13ed32bc6bc0e2d7d0d4.pdf", "supplementary_material": "/attachment/144ad513d3d4a43ae88b129b462a6f679057342c.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies a new personalized prediction problem where the objective is defined by some PAC-like inequality. Specifically, the hypothesis class is considered to be the class of sparse linear functions and the \"personalization\" constraint is modeled by halfspaces. The paper draws a connection with the reference class learning problem and builds algorithm for the personalized prediction problem based on some improvement of algorithms therein."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is very technical but well-written. In my opinion, the contribution of the paper lies in two-fold:\n\n- The formulation of the personalized prediction problem. The paper proposes a new formulation, and it has not been studied in the literature. \n- The formulation draws connection with the reference class learning problem. The paper twists Huang and Juba (2025) and improves the algorithm and adapts it to the context of personalized prediction problem."}, "weaknesses": {"value": "For the context of writing this review, there is a clear stream of papers working on the topic of this paper: Diakonikolas et al., 2020b;c; 2021; 2022; 2024; Juba and Li (2020); Huang and Juba (2025). I, as the reviewer, am not familiar with this stream of work; I have moderate exposure to learning theory and write papers using learning theory. So my judgment could be, on the one hand, biased by the limited scope of my own background, but on the other hand, it could be representative of the general ML people who do research at the intersection of theory and methodology.\n\nIn this light, my main concerns are:\n- The studied formulation is too niche and doesn't provide much practical insight. Personalized prediction is nevertheless an important problem, but the algorithms and analyses in this paper don't provide much insight for practical usage of these algorithms. \n- The theoretical contribution along the existing literature above is unclear from my reading of the paper. I understand that the problem studied is new and can't be resolved by directly applying the existing algorithms. But the question is whether the new treatment and analysis in this paper is generalizable to other theory problems, say, to be used for other problems to make it possible/improve the analysis therein. \n\nTo this end, I feel the paper is more suitable for COLT, where the technical contribution can be better accessed."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "awzrUaS8nf", "forum": "YFoSI7rP66", "replyto": "YFoSI7rP66", "signatures": ["ICLR.cc/2026/Conference/Submission15773/Reviewer_8waY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15773/Reviewer_8waY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15773/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950287614, "cdate": 1761950287614, "tmdate": 1762926005328, "mdate": 1762926005328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an algorithm for personalized prediction for more interpretable learning by focusing on a subset of feature points. Its algorithm combines a robust list learner for the class of sparse linear classifiers and a reference class learner. Specifically, the learning is based on projected gradient descent with an analysis that each step of the update will improve the correlation between the candidate classifier and the assumed ground truth that has small prediction error with respect to the underlying distribution. It is shown that the algorithm runs in polynomial time with respect to parameters $d, 1/\\epsilon, 1/\\delta$."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper uses (mostly) existing techniques and builds an algorithm to solve the personalized prediction problem. The algorithm works under the more general distributional assumptions, i.e. well-behaved marginal distribution that resembles uniform, Gaussian, and many log-concave distributions, by scarifying a small amount of accuracy. From the technical perspective, it improves upon the existing algorithmic guarantees over that of Huang and Juba (2025) by showing the existence of the candidate point $x$ in predicted set $S$.\n\nFrom a broader perspective, the paper builds a learner for a more interpretable model, using sparse linear classifiers. It maintains the accuracy at the same time by personalizing on a specific query $x$. The theory is sound and the algorithms runs in polynomial time with empirical study on benchmark data sets."}, "weaknesses": {"value": "Comparing to its prior work, e.g. Huang and Juba (2025), Diakonikolas et al. (2022), the contribution seems incremental. Besides a slight different setting, i.e. query $x' \\in S$ and well-behaved distribution, the algorithm and analysis resume those were already in prior works. Hence, it is more like a new application of the existing techniques in a personalized learning setting. I wonder how broad the applications of the proposed algorithm can be."}, "questions": {"value": "See weaknesses. What is the essential contribution for the proposed algorithms in personalized learning setting, besides interpretability? What is the key difference between personalized learners and simple invoking the traditional PAC learners for sparse linear classifiers? Some examples on specific $x$ where traditional PAC learner would fail yet the personalized learner can accurately and efficiently predict it, would help much more."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NYFAiBrlM7", "forum": "YFoSI7rP66", "replyto": "YFoSI7rP66", "signatures": ["ICLR.cc/2026/Conference/Submission15773/Reviewer_cGFC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15773/Reviewer_cGFC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15773/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185740232, "cdate": 1762185740232, "tmdate": 1762926004799, "mdate": 1762926004799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Reference"}, "comment": {"value": "[1]Huang, J., \\& Juba, B. (2025). Distribution-Specific Agnostic Conditional Classification With Halfspaces. The 13th International Conference on Learning Representations.\n    \n[2]Wiener, Y., \\& El-Yaniv, R. (2011). Agnostic selective classification. Advances in neural information processing systems, 24.\n        \n[3]Wiener, Y., \\& El-Yaniv, R. (2015). Agnostic pointwise-competitive selective classification. Journal of Artificial Intelligence Research, 52, 171-201.\n        \n[4] Diakonikolas, I., Kane, D., Kontonis, V., Liu, S., \\& Zarifis, N. (2023). Efficient testable learning of halfspaces with adversarial label noise. Advances in Neural Information Processing Systems, 36, 39470-39490.\n    \n[5]Gollakota, A., Klivans, A. R., \\& Kothari, P. K. (2023, June). A moment-matching approach to testable learning and a new characterization of rademacher complexity. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing (pp. 1657-1670).\n        \n[6]Diakonikolas, I., Kane, D., Liu, S., \\& Zarifis, N. (2024, June). Testable learning of general halfspaces with adversarial label noise. In The Thirty Seventh Annual Conference on Learning Theory (pp. 1308-1335). PMLR.\n\n[7]Diakonikolas, I., Kontonis, V., Tzamos, C., \\& Zarifis, N. (2022, June). Learning general halfspaces with adversarial label noise via online gradient descent. In International Conference on Machine Learning (pp. 5118-5141). PMLR."}}, "id": "z7IV6v3MgS", "forum": "YFoSI7rP66", "replyto": "YFoSI7rP66", "signatures": ["ICLR.cc/2026/Conference/Submission15773/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15773/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15773/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763446463890, "cdate": 1763446463890, "tmdate": 1763446463890, "mdate": 1763446463890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "this paper propose a personalized prediction framework for sparse binary classification, where instead of learning one universal model, the approach learns a dedicated classifier for each query by focusing on a relevant subpopulation that best represents the query. The classifier is also designed to be sparse and interpretable. The key idea is that focusing on such subpopulations, characterized by homogeneous halfspaces, makes learning and interpretation easier, since simpler representations can capture local structures within a subset of the data more effectively than a single global model. The authors also provide algorithms with theoretical guarantee and conduct numerical experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and clearly structured, making both the motivation and technical developments easy to follow. The exposition of definitions, algorithms, and proofs is systematic and readable.\n\n2. The paper provides algorithms generating useful results with theoretical guarantees, which is a non-trivial extension of existing conditional learning results to the personalized prediction setting."}, "weaknesses": {"value": "The numerical section is a major weakness of the paper. Even after carefully reading the appendix, it remains difficult to understand what the experiments are actually demonstrating. This lack of clarity significantly undermines the overall quality of the paper, especially given that the theoretical section is otherwise well written.\n\n1. From a conceptual standpoint, it is unclear how the numerical experiments connect to the theoretical results. Simply stating that “the results are lower” is too vague to be meaningful. The authors should explain why a lower value supports the theorem, and how the intuition from the theoretical analysis is reflected in the empirical behavior.\n\n2. The authors also fail to properly explain Table 2, which appears to be the central empirical result. The statement “* indicates statistically significant improvement with 95% confidence (over SPARSE for PERS, and over PERS for the other baselines)” is confusing because the metric and experimental setup are never formally defined. Moreover, in the HYPO dataset, XGB (0.142*) and PERS (0.379*) differ by a large margin (nearly a factor of two) yet both are marked as significant. Without a clear description of the evaluation protocol, metrics, and comparison criteria, the empirical results are difficult to interpret and fail to substantiate the theoretical claims.\n\nI recommend that the authors improve the numerical section in the appendix."}, "questions": {"value": "The authors should also provide intuition on why the chosen UCI datasets are appropriate for supporting the theoretical results (provide reference if already validated in other works). Since the theory relies on assumptions such as well-behaved distributions and sparse feature relevance, it would be helpful to include illustrations or diagnostic plots showing whether the datasets approximately satisfy these assumptions (e.g., marginal distributions, sparsity patterns, or subspace structure). \n\nAdditionally, what is the experimental setup? Readers need information about the parameter settings, number of trials, and criteria for statistical significance to properly evaluate the experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1AEdODfDMl", "forum": "YFoSI7rP66", "replyto": "YFoSI7rP66", "signatures": ["ICLR.cc/2026/Conference/Submission15773/Reviewer_Hoge"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15773/Reviewer_Hoge"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15773/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762234054140, "cdate": 1762234054140, "tmdate": 1762926004203, "mdate": 1762926004203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}