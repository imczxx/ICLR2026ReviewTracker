{"id": "fENEDBJAV3", "number": 12384, "cdate": 1758207468598, "mdate": 1759897513424, "content": {"title": "Graph Alignment for Benchmarking Graph Neural Networks and Learning Positional Encodings", "abstract": "We propose a novel benchmarking methodology for graph neural networks (GNNs)\nbased on the graph alignment problem, a combinatorial optimization task that\ngeneralizes graph isomorphism by aligning two unlabeled graphs to maximize\noverlapping edges. We frame this problem as a self-supervised learning task and\npresent several methods to generate graph alignment datasets using synthetic ran-\ndom graphs and real-world graph datasets from multiple domains. For a given\ngraph dataset, we generate a family of graph alignment datasets with increasing\ndifficulty, allowing us to rank the performance of various architectures. Our ex-\nperiments indicate that anisotropic graph neural networks outperform standard\nconvolutional architectures. To further demonstrate the utility of the graph align-\nment task, we show its effectiveness for unsupervised GNN pre-training, where the\nlearned node embeddings outperform other positional encodings on three molecular\nregression tasks and achieve state-of-the-art results on the PCQM4Mv2 dataset\nwith significantly fewer parameters. To support reproducibility and further research,\nwe provide an open-source Python package to generate graph alignment datasets\nand benchmark new GNN architectures.", "tldr": "We introduce a new task to benchmark GNNs and show how it can be leveraged to generate high quality positional encodings.", "keywords": ["Graph Neural Network", "Benchmark", "Positional Encodings", "Molecular Regression", "Graph Alignment"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ee97af7dc524564297c64b56a10afbef31f7e130.pdf", "supplementary_material": "/attachment/f0dc39d248fca98b44693ad238820a0767e49d75.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes two distinct, though related, contributions. The first contribution is a benchmarking methodology based on self-supervised graph alignment, where existing benchmarks can be used to pre-train a graph ML method by approximating the alignment between tasks. This is done by first corrupting the graph and then approximating the identity or a random permutation over the notes. The second contribution considers pre-trained node embeddings as positional encodings for other graph ML architectures and argues that such encodings allow to achieve better performances with fewer parameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of the paper is clear: use graph alignment as a proxy task that helps GNNs “understand” the structure of graphs. There is value in this, as it is still unclear today what patterns 1-WL GNNs can really capture in practical scenarios.\n\nThe manuscript is overall easy to read thanks to a linear organization of the content that makes it easy to follow the flow."}, "weaknesses": {"value": "The first thing that I noticed while reading the introduction, and later the paper, is that it is not so clear why the structure-only graph alignment is a good benchmark. More precisely, I could not understand why the key features (lines 54-55) described in the paper – which ones? -- allow us to gain a deeper understanding of common challenges in GNNs (lines 64-65). Questions that come to mind are: why should one care if two GNNs in the same WL-class achieve better performances on the graph alignment task? Does this give a certain indication that a GNN would perform better in practical contexts where features also play a role?\n\nI do not think that the paper contains an answer to these questions, but I would like to hear the authors’ opinion. One cannot use the second part of the paper (Section 6) to justify the first contribution, as they are logically separate: the fact that pre-trained embeddings help on some tasks does not imply that a benchmarking methodology makes sense, rather it means that the authors found a good pre-training strategy.\n\nThe authors also argue that the benchmarking methodology provides “controllable levels of difficulty (line 78), but this statement does not seem supported by evidence. There is no formal nor intuitive connection in the paper between the noise increase and the difficulty of the learning task. Could the authors clarify why the solution of Eq. 2 is certainly harder to find for increasing values of the noise? My skepticism is reflected in Figure 3: low and high noise levels are associated with similar performances across models (and only for 2 out of 3 tasks considered), whereas the noise amount that generates the biggest performance gaps between models s highly dependent on the dataset. This makes me believe that the benchmarking methodology is also far from being controllable, in addition to not being well motivated as mentioned before.\n\nOn another note, it is hard to understand why one should evaluate the performance of models using the LAS algorithm. I could not find how the reward matrix is defined in the main paper, nor why a model should approximate the solution of Eq 2 if it is trained on a different loss. Could the authors explain what is the purpose of using LAS if there is always a known permutation that can be used to compute some matching score?\n\nIn Section 4, the authors first permute the graph by adding and removing edges, and then consider a random permutation or the identity when the noise level is small. What happens when the noise level is larger is also unclear. Also, what does small and large mean in this context is not explained. I wonder if one could first permute, then corrupt, in order to know what was the “right” permutation. Anyhow, the description of the process lacks clarity and is scattered here and there in the text.\n\nEmpirically speaking, I respectfully disagree with the statement in lines 321-323 that results are less sensitive to hyper-parameter choices because of the large gap. It should be the other way around: after careful hyper-parameter tuning, one notices a statistically significant gap on test set performances. Judging from table E.5, no hyper-parameter tuning has been performed for the baselines. In addition, the authors talk about training and validation data, with no mention to test set data. I believe that the lack of hyper-parameter tuning, which seems to be the case here, invalidates the validity of all results proposed in the paper, since the authors report performances for a single configuration rather than a risk estimate of a specific model family (e.g., GAT, GCN, etc.).\n\nIn my opinion, the addition of Section 6 degraded the quality of the paper, since many helpful sections seems to have been relegated to the Appendix. I do not think that Section 6 adds much to a paper that is clearly centered around the benchmarking methodology. Overall, the feeling is that this paper is not ready for publication because the motivation and impact of the benchmarking methodology are unclear, together with methodological choices such as LAS. I would encourage the authors to work more on showcasing why the graph alignment self-supervised tasks allows us to gain insights and a better understanding on GNNs. One alternative I could also recommend is to change the narrative altogether and split the paper into two contributions."}, "questions": {"value": "- In which sense is the metric proposed a more “fine-grained” one in understanding graph structures? What insights does the metric give?\n- Can the authors provide evidence that the datasets of Figure 1 share similar underlying topologies, for instance in terms of degree distributions, betweenness-centrality etc. ?\n- Lines 234-242: why does the use of 2 different probabilities give rise to the same amount of deleted and added edges?\n- Line 332: what is a “general-purpose GNN” ?\n- Can you provide the total time required to arrive at results of Table 1, including the pre-training phase?\n\n### Minor points:\n- Inconsistencies in the use of the terms unsupervised/supervised/self-supervised throughout the paper. For instance, unsupervised pre-training is different from self-supervision or supervision. \n- Terms such as “anisotropic” are not used so commonly in the GNN literature, so they should be clearly defined."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vvn2qrM5ND", "forum": "fENEDBJAV3", "replyto": "fENEDBJAV3", "signatures": ["ICLR.cc/2026/Conference/Submission12384/Reviewer_3xkD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12384/Reviewer_3xkD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761131183330, "cdate": 1761131183330, "tmdate": 1762923286795, "mdate": 1762923286795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a graph alignment benchmarking procedure which is based on using a Siamese architecture devised in the Nowak 2017 paper. The authors then propose the creation of alignment datasets to test this architecture on."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and nicely presented with a very interesting focus on the graph alignment problem."}, "weaknesses": {"value": "I feel the heart of the authors paper is the reliance on Siamese architecture developed in Nowak et al. 2017. I think it does not become clear whether there is any real contribution adding to this method or whether the authors rely entirely on the 2017 architecture. If so the main contribution made in this paper is the application to their graph alignment datasets that are created using Section 4."}, "questions": {"value": "- How is the reward matrix $\\mathbf{R}$ defined? I think this is important for assessing what is actually minimized.\n- What is the dimensionality of R $N*N$ or should it be $N\\times N$? \n- What do the authors add to the construction of the 2017 Nowak paper? Does their approach simply use this architecture or is there any modification of the Siamese construction?\n- Are there any bounds on the subgraph approach for large graphs that the authors discuss in Section 4? I guess only probabilistic bounds are realistic as there might surely be cases when applying there method for large graphs only on subgraphs fails to reflect the large scale graph?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "78l26c6XEQ", "forum": "fENEDBJAV3", "replyto": "fENEDBJAV3", "signatures": ["ICLR.cc/2026/Conference/Submission12384/Reviewer_ToCb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12384/Reviewer_ToCb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761684660804, "cdate": 1761684660804, "tmdate": 1762923286550, "mdate": 1762923286550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is focused on the topic of graph alignment and how it can be used to benchmark GNNs as well as the learned embeddings can be used as positional encodings. The paper frames graph alignment as a self-supervised learning task and present methods to generate datasets with controllable difficulty levels from both synthetic random graphs and real-world graph datasets across citation networks and molecular domains. It claims the two main contributions as: (i) a benchmarking strategy that isolates structural understanding of GNNs by comparing multiple architectures (which show anisotropic GNNs such as GatedGCN, GAT outperform standard GCNs), and (ii) a method for unsupervised GNN pre-training where learned node embeddings serve as positional encodings (which they call GAPE) that achieve improved results on PCQM4Mv2 with fewer parameters than some baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The use of graph alignment for benchmarking task can be useful in understanding strengths of different GNNs, going beyond what other benchmarking works in GNNs usually do.\n2. Unlike prior synthetic/mathematical benchmarks (such PATTERN, CLUSTER, CSL, etc.) this approach is dataset agnostic and can adapt to diverse graph topologies without modifying the underlying structure.\n3. The alignment objective can be used for self-supervised learning and can lead to meaningful learned embeddings, as shown in the GAPE results.\n4. It address a challenge with existing PE literature for graphs where there exist multiple PE functions which are defined on the graph structure, and as a result proposes a learnable PE method.\n5. The connection between graph alignment and the expressive power of GNNs is well supported by prior work, given the extensive literature on WLGNNs and the fundamental role of graph isomorphism in both areas."}, "weaknesses": {"value": "1. While the paper demonstrates that GAPE as a result of their study works well empirically, the benchmarking utility is not robustly demonstrated. For instance, what is the relationship between alignment accuracy and downstream task performance? For the 3 datasets in Table 1, if the proposed benchmarking gives a trend/rank of different models, do these translate to real task performance?\n2. One takeaway from the formulation of graph alignment could be that higher order WLGNNs would be better than MPNNs due to those being better at detecting non-iso graphs. A reflective empirical experiment would better support this takeaway however, which is lacking.\n3. The noise level or other parameters in graph alignment datasets may affect the robustness of the learned GAPE.\n4. Minor point-part of the insights such as anisotropic GNNs performance revealed by the insights is known in the literature."}, "questions": {"value": "Are the GAPE embeddings robust to different parameters chose in graph alignment datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lNaiFE2HaU", "forum": "fENEDBJAV3", "replyto": "fENEDBJAV3", "signatures": ["ICLR.cc/2026/Conference/Submission12384/Reviewer_qmjg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12384/Reviewer_qmjg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894476992, "cdate": 1761894476992, "tmdate": 1762923286297, "mdate": 1762923286297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a graph alignment task applicable to arbitrary datasets, and demonstrate its applicability as a graph learning benchmark. They then use the proposed task in a contrastive pre-training strategy using siamese GNNs, and use the resulting embeddings (GAPE) as positional embeddings for downstream tasks. GAPE exhibits good performance on a variety of molecular datasets, and attains SOTA performance on PCQM4Mv2."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper combines several potentially impactful contributions in (a) extending the graph alignment augmentation from Nowak et al. (2017) to arbitrary datasets, (b) demonstrating its viability as a benchmarking task, and (c) leveraging it as an unsupervised pre-training task for learnable PEs with good empirical results.\n2. The paper construction is overall quite sound; despite the density of the paper it is quite easy to follow and the individual contributions tie into each other very well — kudos for that.\n3. The empirical results, particularly the PCQM4Mv2 ones in Table 2 are quite impressive (despite some reservations, see Questions)"}, "weaknesses": {"value": "1. I think something the authors somewhat underplay is that it is likely non-trivial to find the optimal noise level for a given dataset; one needs to be familiar with the datasets to estimate appropriate noise level or sweep over a range to determine apt values, similar to section 5. This carries over to generating GAPE as learnable positional embeddings are highly sensitive on the datasets pre-trained on (something also demonstrated by the GPSE paper [1]) as well as the noise level, as the authors point out. Therefore, unlike many conventional PSEs like LAP and RWSE, GAPE requires additional work to “tune” the pre-training dataset & noise level, in addition to the model itself, to get good downstream encodings.\n2. There are several issues regarding GAPE evaluation — specific suggestions are listed in the next section:\n   1. As a learnable PE, it’s imperative to compare GAPE encodings with other learnable PEs, such as GPSE [1, 2] and PEARL [3]. Without these comparisons, it is difficult to evaluate the true potential impact of GAPE.\n   2. The empirical gains provided by GAPE in Table 1 seem somewhat limited (contrary to Table 2), especially considering it requires costly contrastive pre-training. GAPE typically results in slight improvements over RWPE, and is outperformed by SignNet on ZINC. Additionally, both GPSE and PEARL present results that outperform GAPE (both use GINE, but GPSE also reports better GatedGCN results, i.e. the same convolution used by GAPE), despite GPSE using a different pre-training dataset (albeit a similar, molecular one in MolPCBA). This is not to say that these learnable PEs are necessarily better than GAPE since they evaluate on different architectures and hyperparameters, but rather reinforces the previous point in the need to _fairly_ compare them.\n   3. I think for a primarily empirically-driven paper, the paper is somewhat lacking in additional ablation studies on model and dataset variety; transferability and scaling properties, as well as evaluating overall computational costs compared to alternative methods.\n3. The paper occasionally states established results without references in their analysis: The conclusions presented in section 5 is already fairly established by Dwivedi et al. (2022) [4] and Tailor et al (2022) [5]. Similarly, the conclusions in section 6, particularly on the reliance of graph Transformers on PSEs, is also very well-established [6]. The paper should clearly state how their experimental analysis complements/confirms these findings rather than state them as their own standalone contributions.\n4. Typos (non-comprehensive, no effect on score):\n   - L214: restricting -> restrictive\n   - L248-249: For $\\eta$ small -> For small $\\eta$\n   - L440: leaderbord -> leaderboard\n   - L1319: pretrain -> pretrained\n\n[1] Cantürk, S., Liu, R., Lapointe-Gagné, O., Létourneau, V., Wolf, G., Beaini, D., Rampášek, L. (2024). Graph Positional and Structural Encoder. Proceedings of the 41st ICML 2024, PMLR 235:5533-5566.\n\n[2] Billy Joe Franks, Moshe Eliasof, Semih Cantürk, Guy Wolf, Carola-Bibiane Schönlieb, Sophie Fellenz, Marius Kloft. Towards Graph Foundation Models: A Study on the Generalization of Positional and Structural Encodings. In Transactions on Machine Learning Research, 2025.\n\n[3] Charilaos I Kanatsoulis, Evelyn Choi, Stephanie Jegelka, Jure Leskovec, and Alejandro Ribeiro. Learning efficient positional encodings with graph neural networks. International Conference on Learning Representations, 2025.\n\n[4] Vijay Prakash Dwivedi, Chaitanya K. Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. J. Mach. Learn. Res. 24, 1, Article 43, 48 pages, 2022.\n\n[5] Shyam A. Tailor, Felix Opolka, Pietro Lio, and Nicholas Donald Lane. Adaptive filters for lowlatency and memory-efficient graph neural networks. In International Conference on Learning Representations, 2022.\n\n[6] Luis Müller, Mikhail Galkin, Christopher Morris, Ladislav Rampášek. Attending to Graph Transformers. In Transactions on Machine Learning Research, 2024."}, "questions": {"value": "1. Using LapPE as node embeddings directly results in a very weak baseline that I don’t think serves a clear purpose at the moment. Are there any reasons to keep them as such?\n2. It is well known that random node features can be leveraged to break 1-WL expressivity [7, 8].  Is it possible to inject node identifiability into contrastive pre-training in the form of random node features to improve GAPE embeddings even further?\n3. Results in Tables 1 and 2 should be expanded to include learnable PEs like GPSE and PEARL, using the same pre-training datasets as GAPE when viable. I additionally suggest the authors to compare and contrast GAPE with them not just empirically but also analytically, describing the shared or different methodological or theoretical components between GAPE and these prior learnable PE methods. How does model pre-training differ? How costly is the contrastive pre-training method compared to the self-supervised GPSE pre-training? Is GAPE more/less generalizable? These are some sample questions that may serve as potential avenues of comparison.\n4. Unlike the reasonably successful results in Table 1, Table 2 results are _remarkable_ in that they seem to outperform the SOTA in the OGB leaderboard by almost halving the validation MAE. What are the main differentiators of the performance here? As I understand, both the embedding GNN pre-training and the downstream Transformer model were pre-trained on PCQM4Mv2. Is it the size of the dataset that determines the quality of the embeddings? If so, scaling studies to demonstrate the effect of pre-training dataset size would be helpful (perhaps not necessarily on the full dataset for computational convenience). It is also reported in Appendix I.3 that  model training for the full PCQM4Mv2 took about 2 days — does this include both pre-training and downstream, or just one? How does performance change when different molecular datasets are considered for pre-training? Clarifying such specifics would help back up the results in Table 2.\n5. As per W2.3, I think there is a need to expand empirical results beyond the simple setting of pre-training the embeddings and the downstream model on the same dataset — for real-life utility, transferability of the embeddings are of utmost importance. The authors demonstrate some transferability in Table H.8, but it’s hardly a comprehensive evaluation especially for OOD generalization. Similarly, I am curious on wall-clock estimates of pre-training and how performance changes with scaling the number of graphs. \n\n**Conclusion:** Overall I’m quite borderline on this paper, because the contributions claimed are quite valuable. However, there are several issues discussed in the Weaknesses and Questions that I want to see addressed before I can commit to an acceptance. I should note that regardless the core of the paper is very sound, and a genuine attempt to address _most_ of the aforementioned issues, time permitting (I am aware that the amount of questions and clarifications posed may result in additional work) should push me towards acceptance.\n\n[7] Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random features strengthen graph neural networks. In Proceedings of the 2021 SIAM International Conference on Data Mining (SDM), pp. 333–341. SIAM, 2021.\n\n[8] Ralph Abboud, Ismail Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. The surprising power of graph neural networks with random node initialization. In IJCAI, 2021."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "juoQOPljH0", "forum": "fENEDBJAV3", "replyto": "fENEDBJAV3", "signatures": ["ICLR.cc/2026/Conference/Submission12384/Reviewer_Aedn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12384/Reviewer_Aedn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762255427737, "cdate": 1762255427737, "tmdate": 1762923285987, "mdate": 1762923285987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}