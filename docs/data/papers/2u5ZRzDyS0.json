{"id": "2u5ZRzDyS0", "number": 18238, "cdate": 1758285498107, "mdate": 1759897117406, "content": {"title": "Your Models Have Thought Enough: Training Large Reasoning Models to Stop Overthinking", "abstract": "Large Reasoning Models (LRMs) have achieved impressive performance on challenging tasks, yet their deep reasoning often incurs substantial computational costs. To achieve efficient reasoning, existing reinforcement learning methods still struggle to construct short reasoning path during the rollout stage, limiting effective learning. Inspired by Evidence Accumulation Models, we find that LRMs have accumulated sufficient information early in reasoning, making further reasoning steps redundant. Based on this insight, we propose Just-Enough Thinking (JET), which trains models to proactively terminate unnecessary reasoning. JET performs trajectory truncation during rollout to expose the model to short, distributionally consistent reasoning paths. Besides, it uses a quality-controlled length reward to better encourage concise reasoning while maintaining correctness. Extensive experiments demonstrate that JET significantly improves reasoning efficiency without sacrificing accuracy. Especially, DeepSeek-Distill-Qwen-1.5B achieves a 4.6% accuracy gain while reducing output length by 46.3% on the Olympiad benchmark. Our code is available in the GitHub.", "tldr": "", "keywords": ["Large Language Models", "Efficient Reasoning", "Overthink"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/27a18d009e81f7dc8b6676a810af740403fcdecd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Just-Enough Thinking (JET) tries to address the well-known **overthinking** problem in Large Reasoning Models (LRMs) through introducing (1) a trajectory truncation mechanism that helps early terminate unnecessary reasoning and increases effective rollout sizes, and (2) a quality-controlled length reward used in their DAPO-based RL policy optimization. Through experiments on `DeepSeek-Distill-Qwen` models (training through math datasets), the paper demonstrates considerable reduction in reasoning length on both in-domain and out-of-domain datasets without compromising accuracies."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Overall the paper's writing is clear and easy-to-follow.\n- The piloting experiment provides a good intuition and justification on the huge compressibility within LRM's reasoning traces (given that the full reasoning trace leads to correct answer in the first place).\n- The main experiments seem comprehensive, including a range of in-domain and out-of-domain tasks and multiple alternative efficient reasoning baselines.\n- The paper also includes concrete case studies in the appendix."}, "weaknesses": {"value": "The weaknesses below are **ranked from high priority to low** \n\n_(priority in terms of how they would influence my final rating to this manuscript):_\n\n**Weakness 1: Limited evaluation beyond `Deepseek-Distilled-Qwen`**\n\nRecent RLVR research (not limited to efficient reasoning) often relies on the `Qwen` + `Math` setup. However, several studies -- such as *“RL with One Example”* [[1]](https://arxiv.org/abs/2504.20571) and *“RL with Random Rewards”* [[2]](https://www.interconnects.ai/p/reinforcement-learning-with-random) -- have raised concerns that performance gains might stem from factors like mid-training on math problems or potential data leakage, rather than the RL signal itself. While I am not asserting that such confounding effects necessarily apply here, it would strengthen the paper’s robustness to demonstrate that the reported improvements generalize to other model families beyond Qwen (e.g., the Llama series).\n\n**Weakness 2: Incomplete treatment of overthinking phenomena**\n\nWhile the paper’s pilot experiment provides an interesting motivation for truncation -- showing that correct reasoning traces are often compressible -- it only captures one side of the broader “overthinking” issue in LLM reasoning. The analysis (particularly the `ARR` metric) is conditioned on problems that are *already solved correctly* with full reasoning traces. This design primarily reflects case (i): verbose reasoning where the correct answer appears early but is followed by unnecessary continuation.  \n\nHowever, overthinking also includes case (ii): overly long or convoluted reasoning that ultimately leads to *incorrect* answers due to confusion or drift. The current truncation approach and pilot study seem to overlook this dimension. Similarly, in the reward design, the use of a constant **0** length reward for all incorrect rollouts fails to distinguish between long and short reasoning traces that are both incorrect.\n\n**Weakness 3: Missing ablations and unclear design justifications**\n\nAlthough the paper includes numerous additional experiments in the appendix, several (e.g., the curriculum learning experiment) appear unrelated to the main narrative and do not directly clarify the proposed method’s key design choices. More targeted experiments are needed, especially in two areas:\n\n1. **Ablation: Effect of PES without the length reward.**  \n   It remains unclear how much of the observed improvement comes from the proposed length reward versus PES. Since PES-based truncation effectively constructs “low-likelihood” reasoning trajectories, GRPO/DAPO training might already encourage shorter reasoning paths by increasing the likelihood of these constructed rollouts. An ablation removing the length reward would help isolate its true contribution.\n\n2. **Design Choice: Comparison with established length reward baselines.**  \n   While the paper proposes alternative reward designs and demonstrates why the chosen one performs better, prior works such as [*Training Efficient*](https://arxiv.org/abs/2502.04463) and [*ShorterBetter*](https://arxiv.org/abs/2504.21370) have already explored efficient reasoning using length-based rewards. Comparing directly against these established formulations -- rather than self-introduced variants -- would provide a more meaningful and standardized evaluation of the proposed approach."}, "questions": {"value": "In addition to questions mentioned in the `Weaknesses` section:\n\n1. What's the rollout size for baseline methods like DAPO/AdaThink?\n\n2. Why finetune SFT/DPO with LoRA in baseline implementation (instead of full finetuning to be consistent with the RL training)?\n\n3. It seems that some important citations (esp. for the baseline methods) are missing, e.g. the Laser paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SLW7ttWoKx", "forum": "2u5ZRzDyS0", "replyto": "2u5ZRzDyS0", "signatures": ["ICLR.cc/2026/Conference/Submission18238/Reviewer_tagM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18238/Reviewer_tagM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761446034504, "cdate": 1761446034504, "tmdate": 1762927972762, "mdate": 1762927972762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Language Reasoning Models (LRMs) generally achieve higher accuracy as their reasoning depth increases; however, this improvement often comes at the cost of generating excessive tokens, which significantly slows down inference. Even when an LRM has already gathered sufficient information to reach the correct answer, it frequently continues producing unnecessary reasoning steps — a phenomenon referred to as overthinking.  \n  \nThis paper proposes a reinforcement learning framework called Just-Enough Thinking (JET), which enables LRMs to learn efficiently while suppressing overthinking. The core of JET lies in two main components: (1) a two-stage rollout construction, and (2) a reward design based on the principles of Correctness First, Conciseness Preference, and Per-Question Normalization.  \n  \nIn the two-stage rollout construction, JET uniformly truncates the model’s full reasoning trajectories at several intervals using a strategy called Progressive Early-Stopping (PES). Because PES utilizes trajectories generated directly by the model itself, it maintains the model’s natural consistency during training. Both full trajectories and truncated trajectories are then employed in reinforcement learning, allowing the model to learn when to stop reasoning without losing accuracy.  \n  \nThe reward function prioritizes correctness above all. Among responses that are correct, shorter reasoning paths are assigned higher rewards, thereby encouraging concise and efficient reasoning.  \n  \nJET is applied to DeepSeek-Distill-Qwen-1.5B and 7B models and evaluated across five mathematical reasoning benchmarks — GSM8K, MATH500, AIME24, AMC, and Olympiad — as well as three out-of-domain datasets: CSQA, GPQA, and MMLU.  \n  \nAcross both in-domain and out-of-domain tasks, JET consistently improves accuracy while substantially reducing output token length, leading to faster inference compared with the base model.  \n  \nThe analysis further examines the effectiveness of the PES strategy and compares token length variations under different length reward designs, demonstrating the efficiency and robustness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### 1. Overall Assessment\nThe proposed method, JET, is both simple and effective. Because it leverages trajectories directly generated by the model, JET preserves the model’s natural consistency during training. The approach is also computationally efficient, achieving faster training speeds without sacrificing accuracy.  \n  \n### 2. Originality\nThe work presents a well-motivated application of Evidence Accumulation Theory to computational reasoning, highlighting meaningful parallels between human cognitive efficiency and model reasoning.\nThe design of the Progressive Early-Stopping (PES) mechanism and length-aware reinforcement learning is particularly well-executed. The reward function is carefully structured into three components — format reward, accuracy reward, and length reward — with the principles of Correctness First, Conciseness Preference, and Per-Question Normalization clearly reflected in the mathematical formulation.  \nUnlike previous approaches that rely on fixed token budgets or handcrafted truncation, JET autonomously learns when to stop reasoning in a self-consistent and context-sensitive manner.  \n  \n### 3. Clarity\nThe paper is clearly written and well-organized, providing a strong motivation, detailed mathematical formulation, and a comprehensive algorithmic description (as presented in the Appendix). Figures such as Figure 1 (token length distribution) and Figure 2 (two-stage rollout diagram) effectively convey the core ideas and experimental insights."}, "weaknesses": {"value": "### 1. Need for Further Discussion\nSeveral aspects of the paper would benefit from deeper discussion. In the mathematical reasoning tasks, the Laser methods generally perform better on average and often outperform JET, achieving either higher accuracy or shorter token length. Moreover, Laser shows strong performance on out-of-domain datasets such as CSQA, GPQA, and MMLU. However, the paper provides almost no comparative discussion of these results, and notably, it does not cite the paper “Learn to Reason Efficiently with Adaptive Length-based Reward Shaping,” which originally proposed Laser-D/DE.  \nIn the PES (Progressive Early-Stopping) strategy, trajectories are truncated arbitrarily at 25%, 50%, and 75% of their original length. The paper does not discuss the potential side effects of this truncation. While the authors state that JET preserves the model’s natural generation distribution, one could question whether cutting reasoning trajectories at arbitrary points might itself introduce unnatural discontinuities.  \nFinally, the paper lacks a discussion of its limitations — for instance, in scalability or potential failure cases.   \n  \n### 2. Overstated Claims\nSome of the claims in the paper appear somewhat overstated.  \nAt line 400, the statement “the harder, the stronger” seems based on dataset-level observations and may be an overgeneralization. Since the study includes only eight datasets, this conclusion about performance trends appears insufficiently supported. An analysis at the sample-level difficulty would provide a stronger empirical basis for such a claim.  \nAdditionally, while the paper states that JET’s generation process remains consistent with the model’s natural distribution, there is no experiment directly validating this claim. The intuition that JET should be “natural” is understandable. However, the paper does not include comparative experiments showing how other methods—for example, those using explicit length control or artificially shortened data—may lead to unnatural generation, nor how such unnaturalness affects performance, while JET maintains naturalness. Including such comparisons would strengthen the discussion.  \nIn the 7B model, JET actually leads to accuracy drops on GSM8K and MATH500, which requires further analysis. Although the reward function is designed to favor correctness over length, these results suggest that this objective may not always hold in practice.\n  \n### 3. Experimental Scope  \nThe study primarily evaluates JET on DeepSeek-Distill-Qwen models. To strengthen claims of generality, additional experiments on other architectures (e.g., Phi-4) would be desirable.   \n  \n### 4. Minor Issues  \nTypographical errors: e.g., line 303, “we teste”; and in Equation (3), redundant parentheses “))”."}, "questions": {"value": "Why did you not cite the Laser methods paper? The Laser methods generally show better average performance than the proposed JET approach on the target task (mathematical reasoning), and also perform strongly on out-of-domain benchmarks. In what aspects can JET be considered to have clear advantages over the Laser methods?  \n  \nYou argue that JET preserves the model’s natural reasoning distribution. Could you provide empirical validation for this claim? The Progressive Early-Stopping (PES) uses fixed truncation ratios (25%, 50%, 75%). Did you observe any cases where these arbitrary cutoffs caused the model to stop reasoning unnaturally — even if the truncated reasoning still led to correct answers?  \n  \nExperiments are limited to the DeepSeek-Distill-Qwen models. Have you tested or considered applying JET to other models (e.g., Phi-4)?  \n  \nHow does the training time of JET compare to that of the Laser methods?  \n  \nFinally, could you elaborate on potential scenarios where JET underperforms or fails?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OzaCnf24n7", "forum": "2u5ZRzDyS0", "replyto": "2u5ZRzDyS0", "signatures": ["ICLR.cc/2026/Conference/Submission18238/Reviewer_6BQF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18238/Reviewer_6BQF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869438789, "cdate": 1761869438789, "tmdate": 1762927972291, "mdate": 1762927972291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method called Just-Enough Thinking (JET) that trains LRMs to proactively terminate unnecessary reasoning. Specifically, the paper first performs pilot experiments to reveal that LRMs have accumulated sufficient information early in their reasoning. Based on this phenomenon, JET adopts a Progress Early-Stopping (PES) strategy to sample reasoning trajectories with varying length and introduces a linearly normalized length reward to encourage more efficient reasoning. Experimental results show that JFT significantly improves reasoning efficiency without sacrificing accuracy on 5 math benchmarks and some other reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper is mostly well written and presents the topic clearly.\n\n2.The pilot studies are interesting and directly supports the design of the JET method.\n\n3.The proposed method (JET) is described in sufficient detail and appears technically sound."}, "weaknesses": {"value": "Major Concerns\n\n1.Potential distributional inconsistency. The paper introduces a truncation strategy that also involves inserting an additional prompt. There is insufficient evidence to confirm that this modification does not introduce a harmful distributional inconsistency problem.\n\n2.Sub-optimal performance on math tasks. The results on mathematical reasoning tasks do not clearly demonstrate the superiority of the proposed method. As shown in Table 1, the JFT-DeepSeek-Distill-Qwen-7B model underperforms the Laser-DE model in both accuracy (-0.5%) and efficiency (using more tokens).\n\n3.Insufficient experiments. The empirical evaluation is constrained to a specific set of models (DeepSeek-Distill-Qwen-1.5B and DeepSeek-Distill-Qwen-7B). The experiments should be extended to include more recent model families (e.g., the Qwen3 series) and potentially more model sizes."}, "questions": {"value": "My main questions for the authors directly relate to the major concerns identified above. Addressing these points during the rebuttal period would be crucial for re-evaluating my rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EldjgSnS8i", "forum": "2u5ZRzDyS0", "replyto": "2u5ZRzDyS0", "signatures": ["ICLR.cc/2026/Conference/Submission18238/Reviewer_Zu3p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18238/Reviewer_Zu3p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903714358, "cdate": 1761903714358, "tmdate": 1762927971822, "mdate": 1762927971822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes JET, a reinforcement learning method based on DAPO to reduce unnecessary reasoning steps in Large Reasoning Models. The approach uses Progressive Early-Stopping to create truncated reasoning trajectories during training and employs a length-aware reward to encourage concise outputs. Experiments on DeepSeek-Distill-Qwen models (1.5B and 7B) show token reductions while maintaining accuracy on math benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper demonstrates that LRMs accumulate sufficient information early in reasoning chains, providing good empirical justification for the approach.\n\n2. JET generates shortened trajectories from the model's sampled outputs, reducing distribution mismatch issues that arise from using external short answers.\n\n3. The paper includes  evaluations across multiple benchmarks and model sizes."}, "weaknesses": {"value": "1. In Section 1, line 87, I think that assigning length penalties to longer correct trajectories is problematic because the model cannot guarantee that longer sequences always contain redundant information. Although the authors use a \"Correctness first\" rule where shorter responses receive better reward signals, penalizing longer correct answers does not make much sense.\n\n2. The values of non-negative coefficients w_f, w_acc, and w_ℓ are shown in the appendix as w_acc = 0.9, w_f = 0.1, and w_ℓ = 1. More diverse strategies should be explored since these coefficients significantly influence performance.\n\n3. Since JET is implemented based on DAPO, the accuracy of DAPO should theoretically represent the upper bound for JET. However, I am confused why GSM8K improves by 3.8 points when length decreases from 826 to 605, AIME24 improves by 6 points when length decreases from 8000 to 6000, and Olympiad improves by 5 points when length decreases from 5000 to 4000.\n\n4. When the model scales from 1.5B to 7B, most baseline models decrease or slightly increase sequence length on AIME24, AMC, and Olympiad. However, JET increases sequence length from 6641 to 7981 on AIME24, 3872 to 4301 on AMC, and 4121 to 5083 on Olympiad. What explains this counterintuitive behavior?\n\n5. In Table 2 regarding language reasoning tasks, JET's sequence length is 1013, which is comparable to other baseline models, indicating that JET does not achieve expected length reduction on these tasks.\n\n6. In Table 2, why do baseline models decrease length when scaling from 1.5B to 7B, while JET increases from 800 to 1000? More analysis should be provided here.\n\n7. The paper presents performance on many benchmarks but lacks analysis of model training dynamics such as KL, policy entropy, gradient norm, etc.\n\n8. In Section 3.2, there is ambiguity: does \"ℓ_min and ℓ_max are the shortest and longest correct responses\" refer to the responses themselves or their lengths?\n\n9. In Formula 6, what would happen if the \"+ δ\" term were removed?"}, "questions": {"value": "See the Weaknesses described above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IANPT6kCWo", "forum": "2u5ZRzDyS0", "replyto": "2u5ZRzDyS0", "signatures": ["ICLR.cc/2026/Conference/Submission18238/Reviewer_A4Dw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18238/Reviewer_A4Dw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966680432, "cdate": 1761966680432, "tmdate": 1762927971430, "mdate": 1762927971430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}