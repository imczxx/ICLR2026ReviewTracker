{"id": "FihSkzyxdv", "number": 15257, "cdate": 1758249372839, "mdate": 1763655786688, "content": {"title": "VibeVoice: Expressive Podcast Generation with Next-Token Diffusion", "abstract": "Generating long-form, multi-speaker conversational audio like podcasts poses significant challenges for traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking. We present VibeVoice , a novel model designed to synthesize expressive, long-form speech with multiple speakers in a zero-shot manner. A core component of our approach is the continuous speech tokenizers operating at an ultra-low frame rate of 7.5. This tokenizer effectively preserves audio fidelity while significantly boosting computational efficiency for processing long sequences. To facilitate training on authentic conversational dynamics, we have developed an annotation pipeline that generates pseudo transcriptions and turn-taking labels for extensive podcast data. Leveraging this data and our efficient tokenizer, VibeVoice  employs the next-token diffusion framework. This enables VibeVoice  to: (1) synthesize long-form speech (up to 30 minutes) with up to 4 speakers, surpassing the typical 1-2 speaker limits of many prior models; and (2) achieve a high degree of naturalness in turn-taking, pacing, and the rendition of subtle non-lexical cues (such as breaths and lip smacks), which are crucial for listener immersion and capturing the authentic vibe of expressive conversations.", "tldr": "VibeVoice can synthesize long-form speech for up to 90 minutes (in a 64K context window length) with a maximum of 4 speakers, capturing the authentic conversational vibe and surpassing open-source and proprietary dialogue models.", "keywords": ["Text-to-Speech; Podcast Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c6ae2c6f10924c0a98a5f6a33c4acc3a860a8ca6.pdf", "supplementary_material": "/attachment/5d9d3d028919fab09acde8ab911ccb5181db2158.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces VibeVoice, a novel zero-shot Text-to-Speech (TTS) model designed for synthesizing long-form, multi-speaker conversational audio (e.g., podcasts, up to 30 minutes with up to 4 speakers). This addresses the limits of traditional TTS in handling scalability, speaker consistency, and natural turn-taking.\n\nA core technical element is an ultra-low frame rate (7.5 Hz) continuous speech tokenizer that significantly boosts computational efficiency for long sequences while preserving audio fidelity. Using an extensive new dataset with pseudo transcriptions and turn-taking labels, VibeVoice employs a next-token diffusion framework. This enables the model to achieve a high degree of naturalness in turn-taking, pacing, and subtle non-lexical cue rendition (like breaths), crucial for authentic, immersive conversational output."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important problem in speech generation. Synthesizing long-form, multi-speaker conversational audio is highly relevant for real-world applications such as podcast generation, enhanced virtual assistants, and dialogue systems. The scenario is undoubtedly innovative and highly applicable"}, "weaknesses": {"value": "The core weaknesses of this work lie in a lack of sufficient technical novelty and inconsistent experimental evaluation.\n\n1. Methodology: The next-token diffusion (or next-distribution) framework, a central component of VibeVoice, has been previously introduced in the literature [1]. The paper does not clearly articulate the incremental technical novelty VibeVoice contributes beyond adapting this existing framework.\n\n2. Scenario: The goal of synthesizing speech for multi-speaker (more than two people) scenarios, particularly for long conversational contexts, is also not entirely new. Prior work [3,4], has specifically designed and discussed methods for long conversational speech generation for podcasts and similar applications for more than 2 people [2].\n\n3. While the paper addresses long-context dialogue generation, it fails to include a fair and direct comparison against key existing models that tackle similar challenges in multi-speaker and long-form synthesis [3,4]. This makes it difficult to ascertain the true performance gain of VibeVoice.\n\n4. The 7.5Hz tokenizer appears to be the core technical contribution, yet no sufficient details are provided to fully evaluate its novelty or effectiveness\n\n5. There is very little detail about the data preparation pipeline.\n\nMoreover, there are many inconsistent and incomplete experimental evaluation:\n\n1. Baseline Inconsistency: The choice of baselines across the evaluation tables is inconsistent. The paper should use a uniform set of relevant baselines across all comparative tables (e.g., Table 1 and Table 2) to provide a clear and comprehensive performance picture.\n\n2. Missing Subjective Metrics: Table 1, which appears to focus on multi-speaker/turn-taking quality, lacks subjective (human) evaluation metrics for some systems. \n\n3. Missing Single-Speaker Results: The evaluation in Table 2 is incomplete as it lacks results for the single-speaker scenario. \n\nFinally, the paper fails to include a complete and up-to-date list of references for all cited or discussed works. This is an essential requirement for a conference paper.\n\n[1] Zhu, Xinfa, Wenjie Tian, and Lei Xie. \"Autoregressive speech synthesis with next-distribution prediction.\" arXiv preprint arXiv:2412.16846 (2024).\n\n[2] Xie, Kun, et al. \"Fireredtts-2: Towards long conversational speech generation for podcast and chatbot.\" arXiv preprint arXiv:2509.02020 (2025).\n\n[3]Zeghidour, Neil, et al. \"Streaming sequence-to-sequence learning with delayed streams modeling.\" arXiv preprint arXiv:2509.08753 (2025).\n\n[4] Ju, Zeqian, et al. \"MoonCast: High-quality zero-shot podcast generation.\" arXiv preprint arXiv:2503.14345 (2025).\n\n[5] Zhang, Leying, et al. \"CoVoMix: Advancing zero-shot speech generation for human-like multi-talker conversations.\" Advances in Neural Information Processing Systems 37 (2024): 100291-100317.\n\n[6] Mitsui, Kentaro, Yukiya Hono, and Kei Sawada. \"Towards human-like spoken dialogue generation between ai agents from written dialogue.\" arXiv preprint arXiv:2310.01088 (2023)."}, "questions": {"value": "1. VibeVoice claims to synthesize long-form audio with up to four speakers. Given the high risk of speaker timbre drift and identity confusion in long-context, zero-shot TTS, what explicit architectural mechanisms are implemented to ensure robust and consistent speaker similarity for all four voices?\n\n2. How to prepare and collect data for more than two speakers? For transcription, it is unclear whether to rely only on pseudo-transcription, or should real data also be used? Furthermore, why doesn't pseudo-transcription negatively impact model performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "E7mhtulzD2", "forum": "FihSkzyxdv", "replyto": "FihSkzyxdv", "signatures": ["ICLR.cc/2026/Conference/Submission15257/Reviewer_jDkA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15257/Reviewer_jDkA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761409171453, "cdate": 1761409171453, "tmdate": 1762925558182, "mdate": 1762925558182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "VibeVoice is a next-token diffusion multispeaker long-context TTS model that benefit from the following innovations: 1) a 7.5 Hz framerate tokenizer; 2) an annotation pipeline that generates pseudo transcription and turn-taking labels."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. this work studies an important problem - long-form conversation generation, and is one of the few papers in this field. The idea of using continuous features leads to significantly reduced sequence length which enables long context.\n2. the data annotation pipeline is an important contribution"}, "weaknesses": {"value": "1. a big contribution of the the paper as noted in the abstract is data pipeline, but it's described in appendix, and there is not evaluation on the design choices of the data pipeline\n2. the writing could be improved, for example, it should be noted that in the prompt part, the speech features are just short segments, while the text are the entire conversation (including both prompt and what's to be generated?)\n3. missing RTF"}, "questions": {"value": "1. why do we use a $\\sigma\\text{-VAE}$ as opposed to just an Autoencoder for acoustic feature?\n2. In appendix G, subjective eval interface is annotated with Chinese, are human listeners in the subjective evaluation native speakers of English?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "URRtcMhSV4", "forum": "FihSkzyxdv", "replyto": "FihSkzyxdv", "signatures": ["ICLR.cc/2026/Conference/Submission15257/Reviewer_eyTk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15257/Reviewer_eyTk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761428604221, "cdate": 1761428604221, "tmdate": 1762925557211, "mdate": 1762925557211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents **VibeVoice**, a system designed for **long-form, multi-speaker conversational text-to-speech (TTS)** synthesis. The work addresses three major challenges in the field, **scalability**, **speaker consistency**, and **natural turn-taking with long-context stability**, particularly in the context of podcasts and other extended dialogues.\n\nVibeVoice adopts a **hybrid acoustic–semantic representation**, where speech and text are encoded separately and later fused. The semantic stream is modeled using a fine-tuned **Qwen2.5 language model**, while the acoustic stream is generated via a **next-token diffusion model** that predicts continuous latent acoustic features at **7.5 Hz**.\n\nIn addition, the authors introduce a **customized data annotation pipeline** that includes segmentation, speaker diarization, and automatic filtering of low-quality samples.\n\nEmpirical results demonstrate improved **turn pacing**, **context-aware prosody**, and **speech naturalness** over long durations (up to 45 minutes), outperforming several leading commercial and open-source speech generation systems in both **subjective and objective evaluations**."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. **Comprehensive System for Long-Form Conversational TTS**  \n- The paper presents a well-integrated end-to-end system capable of generating long, multi-speaker conversations with smooth and natural conversational flow, a significant advancement for practical TTS applications.\n\n2. **Innovative Representation and Generation Mechanism**  \n- The use of a **7.5 Hz hybrid acoustic–semantic representation** effectively reduces context length for speech modeling.\n- The **autoregressive next-token diffusion** architecture is a sound and well-motivated design choice for achieving high-quality, coherent speech synthesis.\n\n3. **Robust Data Preparation Pipeline**  \n- The proposed data pipeline, combining segmentation, diarization, and transcript filtering, demonstrates careful system design and contributes to improving training data quality.\n\n4. **Thorough and Convincing Evaluation**\n- The model is extensively evaluated through both subjective and objective metrics.  \n- The **subjective evaluation includes 6 hours of audio** and **over 24 evaluators**, which makes the results convincing for an academic paper.  \n- The **blind subjective evaluation setup (Appendix G)** appears well-designed and appropriate for assessing perceptual quality."}, "weaknesses": {"value": "Overall, I did not find any obvious weaknesses. There are minor typographical issues. For example, in **Table 3**, the last row appears to incorrectly include an *Nq* value even though the proposed speech tokenizer does not use a quantizer."}, "questions": {"value": "1. Could the authors clarify the **key differences between VibeVoice and MoonCast**?  \n2. In the demo, the model appears capable of **singing**. Is this behavior intentional or emergent? Additionally, the singing quality seems suboptimal, are there potential ways to improve this aspect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "w2obpg5IkO", "forum": "FihSkzyxdv", "replyto": "FihSkzyxdv", "signatures": ["ICLR.cc/2026/Conference/Submission15257/Reviewer_Tqnw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15257/Reviewer_Tqnw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554053993, "cdate": 1761554053993, "tmdate": 1762925556749, "mdate": 1762925556749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for generating long-form, multi-speaker audio podcasts. It introduces a continuous hybrid (semantic and acoustic) speech tokenizer that achieves high-quality audio reconstruction at a frame rate of 7.5 Hz. Building upon this, VibeVoice employs next-token diffusion to enable high-quality audio generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors design a continuous hybrid (semantic and acoustic) speech tokenizer that achieves high-quality audio reconstruction at 7.5 Hz.  \nThey leverage next-token diffusion to enable high-quality audio generation.  \nAn efficient pipeline for processing raw podcast data is established.  \nThe method achieves impressive results compared to top-tier models."}, "weaknesses": {"value": "The proposed method in this paper draws heavily on the design of LatentLLM. Moreover, the designs of both the semantic tokenizer and the acoustic tokenizer have already been extensively explored in numerous audio-related works."}, "questions": {"value": "Why does the acoustic tokenizer perform well with a single speaker, but exhibit a significant drop in WER when a second speaker is introduced? Could this be due to insufficient training on multi-speaker data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QS7aIb9lYt", "forum": "FihSkzyxdv", "replyto": "FihSkzyxdv", "signatures": ["ICLR.cc/2026/Conference/Submission15257/Reviewer_WPLm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15257/Reviewer_WPLm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790757261, "cdate": 1761790757261, "tmdate": 1762925556325, "mdate": 1762925556325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new model VibeVoice  for long-form zero-shot text-to-speech. \nVibeVoice can generate up to 90 minutes of speech from a text script and a voice-condition, supporting up to 4 speakers and outperforms SOTA models on podcast generation (as shown by quantitative and qualitative metrics).\n\nThis method relies on key design choices:\n- a separation of acoustic and semantic tokens to get the best of both worlds between text intelligence and acoustic reconstruction\n- ultra low frequency (7.5 Hz) tokens (this speeds up LLM inference time)\n- next-token-diffusion objective (continuous targets) as opposed to the standard cross-entropy objective (discrete targets).\n\nThe authors show extensive ablatation experiments to validate their design choices and compare their generations to state of the art TTS models. They provide detailed training details, code and sample outputs (long-form generated conversations) for reproducibility."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- SOTA model quality on long-form podcast generation\n- Great level of detail on all fronts : tokenizer training, LLM training, data preprocessing, inference settings\n- original contribution of concatenating acoustic and semantic tokens for LLM input - with a soft target (diffusion). They do ablations on those design choices to validate them\n- extensive ablation studies on design choices (tokenizer choice, base model size, diffusion inference settings)\n- extensive evaluation on the model, with a new benchmark VIBEVOICE-Eval  + subjective human evaluation"}, "weaknesses": {"value": "- The WER metric on the CFG figure is not very stable, there are no clear trends (as opposed to SIM-O which shows clear trends). This questions the robustness of WER metric in the comparison figures to other models.\n- This paper compares to other models in terms of performance but not speed, the  inference time appendix does not show other models as comparison"}, "questions": {"value": "clarifications:\n- what is the diffusion training schedule and loss weighting that was used? Same question regarding inference schedule\n- what number of diffusion steps was used for all metrics reported in comparison table?\n- on the tokenizer ablation , it looks like using hybrid only shows benefits at 2+ speakers, while acoustic is better at 1 speaker. Could you elaborate on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "deagjNjDGo", "forum": "FihSkzyxdv", "replyto": "FihSkzyxdv", "signatures": ["ICLR.cc/2026/Conference/Submission15257/Reviewer_1JL1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15257/Reviewer_1JL1"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911860655, "cdate": 1761911860655, "tmdate": 1762925555059, "mdate": 1762925555059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents VibeVoice, a framework for zero-shot, expressive, long-form, multi-speaker podcast generation. It introduces two ultra-low frame rate latent (acoustic and semantic) operating at 7.5 Hz and integrates them into a large language model with a next-token diffusion mechanism for scalable, high-fidelity audio synthesis. The system is capable of generating podcasts up to 90 minutes with up to four speakers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles an important and challenging task—long-form podcast generation, which combines the difficulties of expressive multi-speaker speech synthesis, discourse coherence, and long-context modeling. Addressing such a complex problem is both timely and valuable for advancing speech generation research.\n2. The proposed approach is methodologically reasonable and well-motivated. The reduction of frame rate to 7.5 Hz effectively shortens the sequence length for long-context modeling. The design choices—including (1) the use of continuous latent representations rather than discrete tokens, (2) the hybrid modeling of acoustic and semantic features, and (3) the introduction of a diffusion head to improve latent prediction—all contribute to handling the challenges of long-form expressive audio generation.\n3. The evaluation includes a variety of subjective metrics."}, "weaknesses": {"value": "1. Unclear Training Data Source and Scale: The training data is vaguely described as an internal pseudo-labeled podcast collection without specifying its source, composition, or duration. While the paper mentions ~80 billion training tokens, this could roughly correspond to around 3 million hours of audio, which is an enormous scale. Clarifying the dataset origin and scale is critical for assessing the reproducibility, fairness, and ethical validity of the work.\n\n2. Lack of Analysis on Frame Rate (7.5 Hz) Choice. The paper repeatedly emphasizes the importance of the 7.5 Hz frame rate but does not justify why this particular rate is optimal. It would be highly informative to include experiments at alternative frame rates (e.g., 12.5 Hz or 15 Hz) to show how longer sequences affect LLM modeling difficulty, as well as lower rates (e.g., 3.75 Hz) to measure the trade-off between sequence compression and degradation in WER, expressiveness, or audio quality.\n\n3. Limited Discussion on Scaling and Comparability. Since most compared baselines (e.g., ElevenLabs, Gemini) are black boxes with unknown data and architectures, the fairness of comparison is somewhat constrained. It would be helpful if the authors could supplement scaling experiments, such as: (1) Varying the amount of training data to show scaling behavior. (2) Increasing model capacity (e.g., from 7 B to 30 B) to explore potential performance gains. Such results would make the conclusions more robust and generalizable."}, "questions": {"value": "1. I wonder to know that should we feed the text scripts at the beginning of sequence all at once (like VibeVoice), or in an interleaved manner (e.g., alternating segments of text and audio, as in FireRedTTS2)? Which one is better?\n\n2. The paper frequently refers to “tokenizers,” but the representations used are actually continuous latent variables, not discrete tokens. It might be clearer to use terminology such as latent encoders or continuous representation extractors to avoid confusion.\n\n3. In Figure 3(b), the SIM-o metric decreases as the number of diffusion steps increases. Why does this happen? Intuitively, more denoising steps might be expected to yield smoother or more consistent outputs—so the observed negative correlation deserves explanation."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "The training data is vaguely described as an internal pseudo-labeled podcast collection without specifying its source, composition, or duration. While the paper mentions ~80 billion training tokens, this could roughly correspond to around 3 million hours of audio, which is an enormous scale. Clarifying the dataset origin and scale is critical for assessing the reproducibility, fairness, and ethical validity of the work."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mMoh4SJfh1", "forum": "FihSkzyxdv", "replyto": "FihSkzyxdv", "signatures": ["ICLR.cc/2026/Conference/Submission15257/Reviewer_h5UP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15257/Reviewer_h5UP"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission15257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976931907, "cdate": 1761976931907, "tmdate": 1762925554066, "mdate": 1762925554066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "From the Authors"}, "comment": {"value": "We would like to thank all reviewers and the Area Chairs for their time and insightful feedback throughout the review process.\n\nFollowing the received comments, we have updated the manuscript with several substantial revisions (All revisions are highlighted in red), including:\n -  evaluation of the data pipeline (Appendix A, Table 5),\n -  additional analyses and experiments on the speech tokenizer design (Appendix B and D),\n -  the Real-Time-Factor (RTF) of VibeVoice (Appendix E, Table 9).\n \nWe have responded to the reviewers’ questions and comments, and we hope our clarifications can help address the raised concerns.\n \nWe welcome further discussion and appreciate the reviewers’ thoughtful engagement.\n\nThank you again for your time and consideration."}}, "id": "9Q26feoFs5", "forum": "FihSkzyxdv", "replyto": "FihSkzyxdv", "signatures": ["ICLR.cc/2026/Conference/Submission15257/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15257/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission15257/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763656365331, "cdate": 1763656365331, "tmdate": 1763657202925, "mdate": 1763657202925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}