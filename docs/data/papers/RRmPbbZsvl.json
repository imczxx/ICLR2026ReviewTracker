{"id": "RRmPbbZsvl", "number": 20848, "cdate": 1758310872267, "mdate": 1759896955781, "content": {"title": "Two (narrow) heads are better than (an arbitrarily wide) one", "abstract": "In this paper, we establish a dimension- and precision-independent impossibility result for a simplified transformer model. Due to their size, a comprehensive understanding of the internal operations of frontier large language models (LLMs) is beyond the reach of current methods, but research into small and interpretable models has proven successful. We study the representational limits of attention, the core of transformer models, through the lens of the Endpoint Selection Problem (ESP), a simple yet expressive learning task defined over arcs of a directed graph. ESP is closely related to the 2-hop induction head problem studied in prior work, which itself can be formalized as a function composition task.\n\nOur main theoretical results are twofold: (i) no 1-head, 1-layer, attention-only transformer can solve ESP on any graph containing a cycle, even with unbounded dimension and precision; (ii) in contrast, a 2-head, 1-layer, attention-only transformer can solve ESP on arbitrary directed graphs with constant embedding dimension and logarithmic precision. Prior lower bounds were conditional on bounds on dimension and precision. We complement our 1-head result by showing that, while a zero-error model exists for any directed acyclic graph, it is NP-complete to even approximate the best single-head model that minimizes error on the arcs of an arbitrary directed graph.\n\nFinally, we validate our theory with experiments and observe that gradient-based optimization can reliably find 1-head solutions for DAGs and 2-head solutions for arbitrary graphs with cycles, whereas 1-head models struggle to reach the optimal solution in graphs with cycles. We believe that our techniques are of independent interest and have the potential to establish a new fine-grained hierarchy of transformer architectures, each with greater problem-solving power than the last.", "tldr": "We prove a dimension-independent impossibility result for single-head transformers and study the representational limits of attention via a graph-based task.", "keywords": ["Impossibility Result", "Transformers", "Attention mechanism", "Graphs", "Theory", "Induction heads", "In-context learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/23db1f3ff88eda288a26da9b160a367ec478c85f.pdf", "supplementary_material": "/attachment/27bac1b94133b44ff54ff7759d6ccb67065607ca.zip"}, "replies": [{"content": {"summary": {"value": "The authors analyze a simplified transformer model (1-layer, 1- or 2-head, attention-only) on the endpoint selection problem (ESP). Given a graph, ESP is a distribution of problem instances over the edges in the graph. Given a uniformly sampled edge $(u, v)$, the task is to select either $u$ or $v$, based on whether the next token is $1$ (select $u$) or $2$ (select $v$). The authors show that a 1-head, 1-layer, attention-only transformer can solve ESP in $n$-node DAGs with embedding dimension $n$ (Theorem 1 is slightly more general, applying to directed graphs with large enough acyclic subgraphs, with accuracy degrading as the subgraph shrinks). In contrast, the authors show that 1-head, 1-layer attention-only transformers cannot (exactly) solve ESP in cyclic graphs, and finding the best-approximating transformer is NP-complete. Additionally, they show 2-head models can solve ESP for any directed graph. Experimental results support the theory, but indicate bound are loose."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Well-written paper overall and fairly clear, modulo some minor issues noted below\n2. Nice combination of theoretical results and experiments.\n3. Topic (analysis of transformers) is important and relevant. Significance is mixed; narrow problem and model, but the area is still under-explored, which I think overrules the narrowness\n4. Overall quality is high, what I checked of the proofs seems sound"}, "weaknesses": {"value": "1. The connection to induction heads felt shoe-horned in; the connection to index is much clearer, which highlights a critical piece of missing related work [1], whose results need to be discussed, as they at first seem to contradict the results here (see question 2)\n2. Given the results of [1] on Index for 1-layer transformers with FFN, the importance of this analysis of 1-layer attention-only transformers needs to be justified\n\n\n### Overall evalution\nI actually think this is a nice paper, but the relationship to [1] really needs to be addressed before publication, hence my initial low score. I am open to recommending acceptance if the authors can address these concerns in rebuttal/discussion and amend the paper accordingly."}, "questions": {"value": "### Suggestions/comments\n- In 1.1, the definition of ESP is confusing, as the first sentence of 1.1 says it's a problem in a graph and the second sentence has no graph in the input to the problem. Only after carefully thinking about the third sentence (where the graph affects the distribution of instances generated) does the problem begin to make sense. Clarifying sooner that ESP in a graph is not one input-out pair but a distribution over input-output pairs would help.\n- [1] should be cited and its Index result discussed wrt ESP (see question 2). \n- in the transformer model definition, all terms should be named in the text ($E$, $P$, $X$, $Q$, $K$, $V$), even if they are standard.\n- the style of plot in figure 2 (with accuracy shown as labels only and a hyperparameter on the y axis) is a bit odd, as it doesn't show the accuracy of models with other embedding dimensions. (e.g., how do we know there isn't a smaller one-head model with accuracy 100%, from that plot?) I'm not sure how to fix this best... maybe, for size $n$, also show the accuracy of the next-smaller model, so we see that shrinking it worsens performance?\n- Theorem 5 is only mentioned in the introduction; discussing it in the results section as well would be better. And theorem 4 is never mentioned! This is a nice result to complement theorem 2, which otherwise might feel overly restrictive with its exact solution requirement.\n - Great title. But I would suggest adding something about edge endpoint selection.\n - The related work does a reasonable job overall, but there are some other papers that could have been mentioned [3-9]. See [2] for a nice survey. Of course, no one paper needs a totally exhaustive related work (unless it's a survey), but these might be useful places for the authors to look--and at least pointing to the survey would be useful for readers. (And I was surprised to see no cited papers by several important people in this area, like Strobl, Hahn, Chiang, and Bhattamishra.) For a different perspective on several relevant problems like Index and graph reachability, see also [10].\n\n\n### Questions \n1. The connection between 2-hop induction heads feels quite tenuous, and it's not clear to me that we can say ESP reduces to 2-hop induction heads. This \"reduction\" requires transforming the input, so it's not clear that it's valid to say that \"a circuit capable of two-hop induction can directly solve ESP,\" (p. 13) as that circuit is not capable of performing the input transformation. If we allow arbitrary input transformations, then we could \"reduce\" any problem to another by placing the answer at the end of the new input. In Turing machine polynomial-time reductions, this style of issue is addressed by ensuring the TM itself can perform the input transformation in polynomial time. But here, it's not clear how to make the transformer do the input transformation to then apply the 2-hop induction heads circuit. How can we make this notion of reduction well-defined?\n2. Instead of drawing a parallel between an ESP instance and 2-hop induction heads, the much more natural analogy is to the Index problem: given an input and an index, retrieve the item at the index. This is exactly what ESP is. But [1], Theorem 1 shows that there is a 1-layer transformer with log width and precision that solves Index, and as far as I can tell their construction has only a single head, which seems to contradict the authors' Theorem 2. Their log-width also seems better than the O(n) embedding dimension of Theorem 1. What explains the differences in the results? Ah, I think it's that the results here are for attention-only transformers, whereas [1] uses a (small) feed-forward network. Does their construction become impossible with just a linear layer + softmax instead of the FFN? My intuition is that index still feels possible by attending to the position embedding.... (then again, the logic of theorem 2 makes sense, but it's worth identifying exactly what about their construction fails to translate)\n3. Defining ESP as including the query token seems unnecessary; why not just say the input is $(u, v, i)$?\n4. Given that the results of [1] on Index are for a more realistic transformer model than attention-only, why are the results here for attention-only transformers on a restricted Index problem still valuable? (I don't claim they are not, but this needs to be justified)\n\n\n### References\n[1] Bhattamishra, S., Hahn, M., Blunsom, P., & Kanade, V. (2024). Separations in the representational capabilities of transformers and recurrent architectures. Advances in Neural Information Processing Systems, 37, 36002-36045.\n\n[2] Strobl, L., Merrill, W., Weiss, G., Chiang, D., & Angluin, D. (2024). What formal languages can transformers express? a survey. Transactions of the Association for Computational Linguistics, 12, 543-561.\n\n[3] Hahn, M. (2020). Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8, 156-171.\n\n[4] Hahn, M., & Rofin, M. (2024, August). Why are Sensitive Functions Hard for Transformers?. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 14973-15008).\n\n[5] Chiang, D. (2024). Transformers in uniform TC $^ 0$. arXiv preprint arXiv:2409.13629.\n\n[6] Strobl, L. (2023). Average-hard attention transformers are constant-depth uniform threshold circuits. arXiv preprint arXiv:2308.03212.\n\n[7] Bhattamishra, S., Ahuja, K., & Goyal, N. (2020, November). On the Ability and Limitations of Transformers to Recognize Formal Languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 7096-7116).\n\n[8] Chiang, D., & Cholak, P. (2022, May). Overcoming a Theoretical Limitation of Self-Attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 7654-7664).\n\n[9] Strobl, L., Angluin, D., Chiang, D., Rawski, J., & Sabharwal, A. (2025). Transformers as transducers. Transactions of the Association for Computational Linguistics, 13, 200-219.\n\n[10] Schnabel, T., Tomlinson, K., Swaminathan, A., & Neville, J. (2025). Lost in transmission: When and why LLMs fail to reason globally. arXiv preprint arXiv:2505.08140."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4Ws5GK0oE1", "forum": "RRmPbbZsvl", "replyto": "RRmPbbZsvl", "signatures": ["ICLR.cc/2026/Conference/Submission20848/Reviewer_kCWe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20848/Reviewer_kCWe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761433196055, "cdate": 1761433196055, "tmdate": 1762936526633, "mdate": 1762936526633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this article, the authors present new results about the expressivity of transformers.\n\nThe main contributions are:\n\n- the introduction of a problem as testbed: the Endpoint Selection Problem (ESP) to study the expressive power of Transformers\n\n- using this problem, the authors show a separation between one-head and two-heads transformers,  in the sense that (i) some instances of this problem cannot be solved exactly, for any one-head transformer, (ii) for any instance of this problem, there is a two-head transformer that solves it with zero error.\n\n- an NP-complete result for the approximation of the best single-head model to minimize error on the arcs of graph.\n\n- illustration of their results via experiments"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- At a high-level, the results are appealing and interesting for the science of transformers, which is a fundamental part of all the recent progress in AI.\n\n- Technically speaking the results hold for any dimension of the one-head, and there are quantitative bounds for the two-head, which gives a clean separation.\n\n- The authors give a good overview of the existing results in the literature of the expressivity of transformers."}, "weaknesses": {"value": "- The authors do not elaborate and give convincing arguments on why ESP is interesting to study in the first place (except some mention of reductions in the beginning of page 4, but then it is unclear why focus not on the other reductions). Since the authors are introducing a first in-depth study via the ESP, this is an important part missing to the article in my opinion.\n\n- Related to the previous point, despite the clean seperation result, the nature of the result and significance feels limited. (to the authors: please feel free to clarify during rebuttal)."}, "questions": {"value": "- In the Appendix A., the example given for Induction Heads, do the authors mean: (a,a,a, .., a, b, a, ..., a), where b is in i-th position? Similar question for the Two-Hop induction.\n\n- Could the authors elaborate on their choice of the ESP for the separation, in particular what lead them to consider this problem?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FNKXhW9E3l", "forum": "RRmPbbZsvl", "replyto": "RRmPbbZsvl", "signatures": ["ICLR.cc/2026/Conference/Submission20848/Reviewer_LMjh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20848/Reviewer_LMjh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761694809603, "cdate": 1761694809603, "tmdate": 1762936477995, "mdate": 1762936477995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the representational limits of attention-only transformers through the Endpoint Selection Problem (ESP), where models must select endpoints of directed graph arcs based on an indicator token. The authors establish dimension- and precision-independent impossibility results: no 1-head, 1-layer attention-only transformer can solve ESP on graphs with cycles, while 2-head models can solve ESP on arbitrary graphs with $\\mathcal{O}(n)$ dimension and $\\mathcal{O}(1)$ precision, or $\\mathcal{O}(1)$ dimension and $\\mathcal{O}(\\log n)$ precision. For directed acyclic graphs (DAGs), 1-head models suffice with $\\mathcal{O}(n)$ dimension. The paper proves it is $\\mathsf{NP}$-complete to approximate the minimum error achievable by 1-head models and validates findings experimentally, demonstrating that gradient descent reliably finds 1-head solutions for DAGs and 2-head solutions for cyclic graphs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Strong theoretical contributions with unconditional lower bounds**. The paper's main result (Theorem 2) establishes impossibility for 1-head models on graphs with cycles without any restrictions on embedding dimension or precision, which is a significant advance over prior conditional lower bounds by Peng et al. (2024) and Sanford et al. (2024c) that required bounded dimension and precision. The geometric proof technique using Lemmas 1 and 2 is elegant and provides clear intuition through the half-space analysis shown in Figure 1. The matching upper bounds (Theorems 1 and 3) with constructive proofs and the computational hardness result (Theorem 5) create a complete characterization of the problem.\n\n2. **Clean problem formulation with practical relevance**. ESP provides a well-motivated testbed that connects to the important induction head phenomenon studied in transformer interpretability (Appendix A demonstrates ESP as a special case of 2-hop induction heads). The problem is simple enough to admit rigorous analysis yet expressive enough to capture fundamental graph traversal and selection primitives underlying many reasoning tasks. The experimental validation on transitive tournaments and complete digraphs (Figure 2) demonstrates practical implications of the theoretical results.\n\n3. **Comprehensive complexity-theoretic analysis**. Beyond expressivity results, the paper proves that finding optimal 1-head solutions is computationally intractable. Theorem 4 shows NP-completeness of finding minimum-error models, while Theorem 5 strengthens this to APX-hardness with an inapproximability factor of 1.3606 (lines 695-721). This is achieved through reduction from the Maximum Acyclic Subgraph problem, and Corollary 3 directly implies \"gradient descent cannot compute (even approximate) the global minimum...in polynomial time, unless P = NP.\" These results provide crucial theoretical justification for why the optimization gaps observed in experiments (Section 5) may be fundamental rather than artifacts of training procedures."}, "weaknesses": {"value": "1. **Limited scope due to attention-only restriction**. The paper focuses exclusively on attention-only transformers without feed-forward networks (FFN), as acknowledged in lines 065-067: \"Since FFN with a single layer is already a universal approximator (Hornik et al., 1989), in this paper we focus on attention-only transformers.\" While this isolation enables clean theoretical analysis, it significantly limits practical relevance since real transformers crucially rely on FFN layers.\n\n2. **Incomplete coverage of related work on transformer depth and circuit complexity**. The related work section (lines 136-150) discusses some depth-related results, mentioning that \"transformers with constant depth, context length n, and precision O(log n) can be simulated by uniform constant-depth threshold circuits\" (Merrill & Sabharwal, 2023) and that \"log-depth transformers can solve graph connectivity\" (Merrill & Sabharwal, 2024a), but omits several highly relevant recent works establishing fundamental complexity bounds for transformer architectures. Chen et al. (2024) [1] analyze circuit complexity bounds for RoPE-based transformer architectures, Cao et al. (2025) [2] show the circuit complexity bounds for vision transformers and their application, and Chiang (2025) [3] proves transformers operate in uniform $\\mathsf{TC}^0$. These works are particularly relevant because RoPE is the dominant positional encoding in modern LLMs while this paper uses basic additive positional embeddings, and the circuit complexity perspective provides complementary lower bounds that could strengthen the impossibility results. \n\n### References\n\n[1] Bo Chen, Xiaoyu Li, Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song, Jiahao Zhang. “Circuit Complexity Bounds for RoPE-based Transformer Architecture”. EMNLP 2025.\n\n[2] Yang Cao, Yubin Chen, Zhao Song, Jiahao Zhang. “Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria”. arXiv:2508.07102. \n\n[3] David Chiang. “Transformers in Uniform TC0”. TMLR 2025."}, "questions": {"value": "1. Does the 1-head impossibility result (Theorem 2) extend to transformers with feed-forward layers?\n2. Can your geometric proof framework establish similar hierarchies for other architectural dimensions beyond head count?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hvq87IZusQ", "forum": "RRmPbbZsvl", "replyto": "RRmPbbZsvl", "signatures": ["ICLR.cc/2026/Conference/Submission20848/Reviewer_UJCu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20848/Reviewer_UJCu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883615133, "cdate": 1761883615133, "tmdate": 1762936422664, "mdate": 1762936422664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors provide a theoretical analysis of single-layer attention-only transformers and their ability to solve a simple problem called the Endpoint Selection Problem (ESP). This problem is interesting and motivated by the fact that it is foundational in many reasoning tasks. They show that single-layer transformers with a single attention head are not able to solve this problem in its general form even with unbounded model dimension. On the other hand, the authors show that the same model with two attention heads can easily solve the problem with limited model dimension and precision. They provide experimental results that further corroborate their theoretical findings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written overall and the proofs are well-structured and easy to follow.\n - The authors show that even with unbounded model dimension, single-head attention-only transformers are not able to learn the ESP task for cyclic graphs (but they are able to easily do so for DAGs).\n - The authors also show that the same model with two attention heads is able to learn the ESP task on cyclic graphs with limited model dimension and precision.\n - Experiments are conducted to corroborate their theoretical observations."}, "weaknesses": {"value": "- The scope and implications of the contributions are not immediately obvious (or they seem to be somewhat limited). Perhaps some discussion about how the findings would generalize to non-attention-only transformers or transformers with more than one layer would help.\n - It is unclear why it is not possible to write a construction that is agnostic to the distribution of the graphs (see below)."}, "questions": {"value": "In the ESP task, all of the information needed to solve the problem is given in the model's input: the ordered pair representing the edge as well as the selector. Then why is the model's ability to solve this task dependent on the distribution of edges/graph structure? Is it not possible to write/learn a distribution-agnostic circuit to solve ESP in one layer? For example, if the query token were excluded from the input (i.e., the input only consists of the vertex pair and the indicator), it would be straightforward to construct a single attention-head transformer to solve the task in a distribution-agnostic fashion: Set the token embedding for vertex v_i to be the unit vector with 1 in dimension i; the token embedding for 1 and 2 are unit vectors with 1 in dimensions n and n+1, respectively; the position embedding for position i is a unit vector with 1 in dimension n+2+i, and so the embedding dimension is n+5. The attention matrix A is set as follows: A[n+0,n+2+0]=C, A[n+1,n+2+1]=C, for some positive constant C and zeros elsewhere. The last token (the selector) would simply copy from the token at position 0 or 1 depending on the value of the selector. Given such a construction exists, why is it worthwhile to consider the case where the last token must be a query token?\n\nWhile the work focuses on attention-only transformers, since arbitrarily wide FF layers are universal approximators, it would still be interesting to explore how the analysis would change if we allow for bounded width FF layers. Would the ESP problem become much easier to solve? Similarly, would an attention-only transformer with one attention head and two layers be able to solve the ESP problem on cyclic graphs?\n\nThe font size in Figure 1 is rather small which makes it somewhat difficult to read (especially the expression in grey).\n\nFigure 2: The font size for the percentages is a bit too small."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jlNvfaMyNU", "forum": "RRmPbbZsvl", "replyto": "RRmPbbZsvl", "signatures": ["ICLR.cc/2026/Conference/Submission20848/Reviewer_WJsn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20848/Reviewer_WJsn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983621241, "cdate": 1761983621241, "tmdate": 1762936341180, "mdate": 1762936341180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}