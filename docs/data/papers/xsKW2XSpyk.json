{"id": "xsKW2XSpyk", "number": 16015, "cdate": 1758258638451, "mdate": 1759897267382, "content": {"title": "Patch-Level Kernel Alignment for Dense Self-Supervised Learning", "abstract": "Dense self-supervised learning (SSL) methods showed its effectiveness in enhancing the fine-grained semantic understandings of vision models. However, existing approaches often rely on parametric assumptions or complex post-processing (e.g., clustering, sorting), limiting their flexibility and stability. To overcome these limitations, we introduce Patch-level Kernel Alignment (PaKA), a non-parametric, kernel-based approach that improves the dense representations of pretrained vision encoders with a post-(pre)training. Our method propose a robust and effective alignment objective that captures statistical dependencies which matches the intrinsic structure of high-dimensional dense feature distributions. In addition, we revisit the augmentation strategies inherited from image-level SSL and propose a refined augmentation strategy for dense SSL. Our framework improves dense representations by conducting a lightweight post-training stage on top of a pretrained model. With only 14 hours of additional training on a single GPU, our method achieves state-of-the-art performance across a range of dense vision benchmarks, demonstrating both efficiency and effectiveness.", "tldr": "We present a dense self-supervised learning framework built on robust augmentations and patch-level kernel alignment, achieving state-of-the-art results in visual in-context learning and unsupervised semantic segmentation.", "keywords": ["Dense self-supervised learning", "Self-supervised learning", "Unsupervised semantic segmentation", "In-context scene understanding"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/855c4b2e07a32267432d52aef4eac2249f2487e2.pdf", "supplementary_material": "/attachment/da13c8165e72e26ea83eaa11ae52f12e0f909ff3.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates SSL loss for dense-prediction tasks. The paper proposes a lightweight post-training phase with a new SSL loss to improve the dense-feature quality of a DINOv2-pretrained backbone. The paper's main contributions are:\n1) A new PaKA loss that adopts Centered Kernel Alignment (CKA) to compare patch distributions between student and teacher views, after alignment of the local patches through an ROI process.\n2) A new augmentation strategy that maximizes overlap between student and teacher views and reduces the strength of teacher data augmentation.\n\nThe authors validate their approach by post-training a DINOv2R baseline and testing it on VOC and ADE20K using both visual in-context learning and linear probing. Their approach shows improvement over DINOv2R or NeCo baselines. The authors also perform an ablation to show the impact of the different contributions."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Paper introduces a lightweight post-training to performance of DINOv2 backbone\n- Paper performs careful ablation to study the impact of the different components (CKA loss, data-augmentation)"}, "weaknesses": {"value": "My main issue with the paper is that the empirical results do not seem to fully support the claim:\n- It seems that the reported numbers are quite low for the baselines. For instance, Table 3 in [1] reports that DINOv2 obtains a score of 83.1 on PascalVOC and 49.5 on ADE20K. In contrast, DINO2R is getting around 74.2 and 35.0 on VOC/ADE20K in Table 2. Are you using a different base model, and would the proposed approach transfer to a stronger base model?\n- CKA is proposed as one of the main contributions in the paper but does not lead to a significant improvement in the linear probing protocol in Table 5a.\n- The authors do not control for post-training on a different data distribution (COCO). What would be the performance of the DINO baseline, post-trained on COCO using the regular DINO loss and gram anchoring?\n- Other supervised baselines could be included, such as PESpatial or AM-RADIOv2.5, and the evaluation could go beyond the linear probing protocol.\n\n[1]: DINOv3, Siméoni et al., 2025."}, "questions": {"value": "My main question is related to the first weakness, why do we see a gap in term of performance on dense linear probing compare to result reported to the litterature and would the gain obtain by the approach transfer to stronger backbone ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XltbYoI8Hh", "forum": "xsKW2XSpyk", "replyto": "xsKW2XSpyk", "signatures": ["ICLR.cc/2026/Conference/Submission16015/Reviewer_CCN1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16015/Reviewer_CCN1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761669629107, "cdate": 1761669629107, "tmdate": 1762926219850, "mdate": 1762926219850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PaKA (Patch-level Kernel Alignment), a method for improving dense visual representations via self-supervised learning. Unlike clustering- or sorting-based approaches (e.g., iBOT, NeCo), PaKA proposes a non-parametric kernel-based alignment objective grounded in Centered Kernel Alignment (CKA). The method aligns the relational structure of patch embeddings between a student and teacher network, trained under an EMA framework.\nKey design choices include:\n- An CKA loss aligning pairwise patch similarities between student and teacher.\n- A “clean teacher” (minimal augmentations) and high-overlap crops for consistent patch correspondence.\n- Lightweight post-pretraining fine-tuning applicable to pretrained ViT encoders.\nEmpirically, PaKA achieves state-of-the-art performance on dense vision benchmarks (PASCAL VOC, COCO-Stuff, ADE20K) and demonstrates improved efficiency (–37% training time, –24% memory vs. NeCo). The paper is positioned as a generic refinement step rather than a full pretraining framework.\n\nSummary of the review: The paper proposes PaKA, a simple and efficient refinement to dense self-supervised learning using a CKA-based objective that improves segmentation and depth performance while reducing compute and memory cost. The approach is practical and well-engineered, showing robustness and strong applicability to pretrained encoders. However, it lacks deeper conceptual novelty, mechanistic insight, and breadth of validation—key ablations, teacher analysis, and evaluations on classification, detection, or multimodal models are missing, and efficiency comparisons need clearer fairness. Overall, the work is technically sound and useful but remains primarily incremental rather than foundational, justifying a rating of 4 for solid engineering with limited conceptual advance.\n\nReproducibility Comment: Code provided is clean and structured with clear instructions. In the paper there is a clear provision of different evaluations and setups used. The main details for reproducibility are provided."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Simplicity: CKA provides a clean, non-parametric alternative to cluster-based dense SSL.\n2) Empirical robustness: Consistent gains on dense segmentation and depth benchmarks.\n3) Efficiency: Lower compute and memory cost than NeCo.\n4) Practical relevance: A lightweight post-pretraining recipe applicable to strong pretrained encoders."}, "weaknesses": {"value": "1. Batch-size dependence (missing ablation).\nThe authors acknowledge batch importance but show no study of how varying batch size affects convergence or downstream accuracy.\n\n--> include 8–64 batch ablation for segmentation/hummingbird evaluation metrics.\n\n2. Insufficient validation of “clean teacher.”\nAll evidence for the augmentation-free teacher is based on unsupervised segmentation, which is noisy.\n\n--> Test the teacher-augmentation ablation using linear segmentation or Hummingbird-style stable probes that are less dependent on clustering variance.\n\n3. Weak mechanistic motivation.\nThe paper demonstrates empirical gains but lacks an analysis explaining why the CKA objective enhances semantics (would also be interesting to see if it improves other metrics like object-detection as well and whether the motivation also provides insights for their task generalisability, therefore further explanation is expected ).\n\n4. Limited understanding of PaKA's capabilities in global understanding.\nPaKA’s effect on global understanding remains untested.\n\n--> evaluate k-NN or linear classification on ImageNet vs. DINOv2R to ensure that dense refinement does not degrade global representations.\n\n5. No exploration of VLM encoders (CLIP, SigLIP).\nGiven their prevalence, applying PaKA to CLIP or SigLIP vision encoders would clarify if the method benefits or harms text aligned vision models.  \n\n--> Report zero-shot classification, retrieval, segmentation performance, and hummingbird evaluation pre/post PaKA on them.\n\n6. Finetuning evaluation limited to linear heads.\nThere are no non-linear decoders used for tasks such as segmentation (as well as depth estimation)\n\n--> Evaluate with non-linear decoders (e.g.,Mask2Former) to ensure benefits persist in end-to-end settings.\n\n7. Limited generalisation to non-dense tasks.\nNo other evaluations to simple segmentation, and depth estimation.\n\n--> Evaluate with other benchmarks like object-detection (VITDET), Panoptic Segmentation (e.g., Mask2Former), since both PaKA and its predecessor seem very good in the hummingbird evaluation (fetching related patches) the question that is raised from my end is how good the features are also in key point matching (use the features from PaKA as feature descriptors to evaluate how good are they in key point matching eg on the HPatches Dataset [3]), and the Multiview feature consistency [1]\n\n8. Dataset choice unexplained.\nWhy is COCO preferred for post-training instead of ImageNet or mixed-domain corpora? \n\n--> A dataset ablation would clarify domain dependence.\n\n9. Lacks of novelty to the creation of a new paradigm, seems more as an engineering investigation.\nThe method feels like a well-tuned refinement of existing self-distillation pipelines rather than a fundamentally new paradigm.\n\n--> clarify more the position of PaKA ( is it an empirical improvement or a conceptual shift)\n\n10. Computational (Execution and Memory) Cost Improvements are inconclusive\nThe reported efficiency gains (−37% time, −24% memory) may partly arise from engineering or implementation differences rather than inherent algorithmic simplicity. If PaKA employs xformers, torch compiled models, lighter augmentations, smaller intermediate tensors, mixed-precision optimization, or simply uses newer and more optimised libraries and/or has optimised their code further while NeCo’s public implementation does not, the comparison becomes uneven. \n\n-->  (a) clarify the exact setup of execution comparisons, (b) since the code structure is very similar to NeCo provide a side by side comparison of computational costs (time and memory) on a per part of the code that you introduced eg (data processing, augmentations, forward pass, loss, etc),  (c) potentially average the computational costs over multiple runs to avoid any hardware noise,\n\n\n[1] Banani, M. E., Raj, A., Maninis, K.-K., Kar, A., Li, Y., Rubinstein, M., … Jampani, V. (2024). Probing the 3D Awareness of Visual Foundation Models. arXiv [Cs.CV]. http://arxiv.org/abs/2404.08636\n\n[2] Cheng, B., Misra, I., Schwing, A. G., Kirillov, A., & Girdhar, R. (2022). Masked-attention Mask Transformer for Universal Image Segmentation. arXiv [Cs.CV].  http://arxiv.org/abs/2112.01527\n\n[3] Balntas, V., Lenc, K., Vedaldi, A., & Mikolajczyk, K. (2017). HPatches: A benchmark and evaluation of handcrafted and learned local descriptors. arXiv [Cs.CV].  http://arxiv.org/abs/1704.05939"}, "questions": {"value": "Questions for Authors are derived from the weaknesses:\n\n1. Batch-size dependence\nCould you include an ablation studying how varying batch size (e.g., 8–64) affects convergence, stability, and downstream accuracy?\nThis is particularly relevant since relational losses (like CKA) are sensitive to sample diversity and normalization across the batch.\n\n2. Validation of the “clean teacher” design\nCan you validate the effect of the augmentation-free teacher using more stable metrics, such as linear segmentation or Hummingbird-style evaluations, instead of unsupervised clustering-based segmentation, which is inherently noisy?\nThis would strengthen the causal link between the teacher’s augmentation level and representation quality.\n\n3. Mechanistic motivation and conceptual clarity\nCould you expand on why the CKA objective improves semantics?\nDo you observe improved structure preservation or inter-class separation in the learned representation?\nWould this mechanistic intuition also explain potential improvements in object detection or other structured tasks?\n\n4. Global understanding and classification ability\nHow does PaKA influence global-level understanding?\nPlease evaluate on k-NN or linear classification (e.g., ImageNet-1k) compared to DINOv2R to ensure that dense refinement does not harm global semantic performance.\n\n5. VLM encoder evaluation (CLIP, SigLIP)\nHave you tested PaKA on vision encoders from multimodal models such as CLIP or SigLIP (with frozen text towers)?\nReporting zero-shot classification, retrieval, and segmentation performance (and optionally Hummingbird scores) before and after PaKA would clarify whether the method generalizes to text-aligned encoders without degrading cross-modal alignment.\n\n6. Finetuning with non-linear decoders\nCan you evaluate PaKA-pretrained encoders with decoder-based architectures (e.g., Mask2Former [2]) for segmentation and/or depth estimation?\nThis would show whether improvements persist in end-to-end fine-tuning beyond linear heads.\n\n7. Generalisation to broader visual tasks\nCould you test PaKA on additional dense and geometric benchmarks such as:\na. Object detection (e.g., ViTDet),\nb. Panoptic segmentation (e.g., Mask2Former),\nc. Keypoint matching using PaKA features as descriptors on HPatches [3], and\nd. Multiview feature consistency or 3D awareness tests as in Banani et al. (CVPR 2024) [1]?\nThis would better establish the method’s versatility and geometric consistency.\n\n8. Dataset choice for post-training\nWhy was COCO selected as the post-training dataset instead of ImageNet or a mixed-domain corpus?\nCould you provide a small dataset ablation (e.g., ImageNet vs COCO) to demonstrate that PaKA’s gains are not domain-dependent?\n\n9. Positioning and novelty of PaKA\nHow should readers interpret PaKA in the broader SSL landscape—do you consider it a conceptual advance or a well-engineered refinement of self-distillation pipelines?\nClarifying this positioning would help reviewers and readers understand its intended scope and contribution level.\n\n10. Fairness of computational efficiency claims\nThe reported runtime and memory reductions (−37% time, −24% memory) might depend on implementation-specific optimizations.\nCould you:\na) Clarify whether both PaKA and NeCo were benchmarked under identical optimization conditions (precision, compiler, libraries, data loading, etc.)?\nb) Provide a side-by-side breakdown of time and memory usage per module (data loading, augmentations, forward pass, loss computation, etc.)?\nc) Average timings over multiple runs to reduce hardware noise?\nThis would ensure the efficiency comparison is methodologically fair.\n\n[1] Banani, M. E., Raj, A., Maninis, K.-K., Kar, A., Li, Y., Rubinstein, M., … Jampani, V. (2024). Probing the 3D Awareness of Visual Foundation Models. arXiv [Cs.CV]. http://arxiv.org/abs/2404.08636\n\n[2] Cheng, B., Misra, I., Schwing, A. G., Kirillov, A., & Girdhar, R. (2022). Masked-attention Mask Transformer for Universal Image Segmentation. arXiv [Cs.CV].  http://arxiv.org/abs/2112.01527\n\n[3] Balntas, V., Lenc, K., Vedaldi, A., & Mikolajczyk, K. (2017). HPatches: A benchmark and evaluation of handcrafted and learned local descriptors. arXiv [Cs.CV].  http://arxiv.org/abs/1704.05939"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MXWzyyPpJo", "forum": "xsKW2XSpyk", "replyto": "xsKW2XSpyk", "signatures": ["ICLR.cc/2026/Conference/Submission16015/Reviewer_DF2M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16015/Reviewer_DF2M"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779823177, "cdate": 1761779823177, "tmdate": 1762926219090, "mdate": 1762926219090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel post-(pre)training method for improving the low-level quality of self-supervised learning representations, and thus increase the downstream performance on tasks such as segmentation or overclustering. Experimental results show that applying on a DINOv2 model leads to significant improvements on benchmarks such as Pascal VOC, COCO, or ADE20k."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The presented method is well-motivated, in particular the need for a non-parametric local-matching approach in dense self-supervised learning, and matching gram matrices on locally aggregated patch-level representations. The Centered Kernel Alignment (CKA) approach is also well motivated as opposed to using the actual gram matrix.\n\n- The paper is very clear and well-written, the proposed method is correctly formalized and the figures go straight to the point and help the understanding. \n\n- The evaluation setup is strong and makes the results convincing. Large-scale datasets are used (COCO, ADE20k), against strong baselines (DINOv2R), with meaningful evaluation protocol (linear probing)."}, "weaknesses": {"value": "- The presented method should be agnostic to any pretraining method. It would have been great to show this by post-training on more than a single model, right now there is no way of saying if this post-training is specific to DINO or if it works in the general case.\n\n- The specific backbone used for most of the experiment is not specified in the main text or in the figures captions. I think this is a very important detail for practitioners that should be highlighted.\n\n- Where does DINOv2R line performance come from in Table 1,2,3,4 ? Is it your reproduction or from another paper ? I can’t see these results on the original paper. This is why specifying the backbone is important, otherwise comparisons are meaningless.\nWhy do you compare your results to this specific version of DINO ? Why not DINOv2 or v3 ?\n\n- I don’t think Section 4 is relevant, at least it should as much importance because these findings on data augmentation are very specific to the particular method that is proposed, and particular to the fact that this is post-training and not pre-training. The writing might be a bit misleading “Motivated by the limitations of this inherited augmentation paradigm” makes it believe that this is a new go-to augmentation strategy.\n\n- There are a number of small presentation issues: \n\n- L851 typo “evalutaions”\n\n- L23 typo “Kernal”\n\n- “For a fair comparison, we also post-trained same backbone model by NeCo (Pariza et al., 2025),” This is confusing , do you post-train from neco, which is pretrained from dinov2r according to the Table 1 ?\n\n- Line 41 “another line of research (Lebailly et al., 2024; Pariza et al., 2025; Stegmüller et al., 2023; Ziegler & Asano, 2022) has focused on dense representation learning via self-distillation” DINOv2 and Ibot can also be considered as self-distillation methods\n\n- Some figures are blurry, make sure to use .pdf instead of .png or .jpeg\n\n- “while reducing computation by 37% and memory usage by 24% compared to prior methods” which one ? This needs to be precise and only can compare to post-training techniques. The answer is actually in section 5.4: “more memory-efficient compared to NeCo” it should be mentioned above."}, "questions": {"value": "The method is used as a post-(pre)training refinement stage. Have you thought about making it a pretraining method ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MDWidQbltW", "forum": "xsKW2XSpyk", "replyto": "xsKW2XSpyk", "signatures": ["ICLR.cc/2026/Conference/Submission16015/Reviewer_qWgt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16015/Reviewer_qWgt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846729695, "cdate": 1761846729695, "tmdate": 1762926218541, "mdate": 1762926218541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}