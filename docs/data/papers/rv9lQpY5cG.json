{"id": "rv9lQpY5cG", "number": 16453, "cdate": 1758264722356, "mdate": 1759897239803, "content": {"title": "Omni-iEEG: A Large-Scale, Comprehensive iEEG Dataset and Benchmark for Epilepsy Research", "abstract": "Epilepsy affects over 50 million people worldwide, and one-third of patients suffer drug-resistant seizures where surgery offers the best chance of seizure freedom. Accurate localization of the epileptogenic zone (EZ) relies on intracranial EEG (iEEG). Clinical workflows, however, remain constrained by labor-intensive manual review. At the same time, existing data-driven approaches are typically developed on single-center datasets that are inconsistent in format and metadata, lack standardized benchmarks, and rarely release pathological event annotations, creating barriers to reproducibility, cross-center validation, and clinical relevance. With extensive efforts to reconcile heterogeneous iEEG formats, metadata, and recordings across publicly available sources,\nwe present $\\textbf{Omni-iEEG}$, a large-scale, pre-surgical iEEG resource comprising $\\textbf{302 patients}$ and $\\textbf{178 hours}$ of high-resolution recordings. The dataset includes harmonized clinical metadata such as seizure onset zones, resections, and surgical outcomes, all validated by board-certified epileptologists. In addition, Omni-iEEG provides over 36K expert-validated annotations of pathological events, enabling robust biomarker studies. Omni-iEEG serves as a bridge between machine learning and epilepsy research. It defines clinically meaningful tasks with unified evaluation metrics grounded in clinical priors, enabling systematic evaluation of models in clinically relevant settings. Beyond benchmarking, we demonstrate the potential of end-to-end modeling on long iEEG segments and highlight the transferability of representations pretrained on non-neurophysiological domains. Together, these contributions establish Omni-iEEG as a foundation for reproducible, generalizable, and clinically translatable epilepsy research.", "tldr": "Omni-iEEG is a large, expert-validated iEEG dataset and benchmarks to bridge machine learning and epilepsy research.", "keywords": ["Computational neuroscience", "iEEG", "Epilepsy", "Computer Aided Diagnosis", "Neurophysiology"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a45332433ed0ba5742e5543ef734495b65aafbdd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Omni-iEEG, a large-scale dataset and benchmark for epilepsy research, comprising 302 patients and 178 hours of high-resolution intracranial EEG (iEEG) recordings from eight leading epilepsy centers. It addresses challenges in clinical EZ localization by providing harmonized data formats, metadata, and over 36K expert-validated annotations of pathological events (e.g., spkHFOs). The dataset supports reproducible research with standardized tasks—Pathological Event Classification and Pathological Brain Region Identification—plus exploratory tasks, demonstrating potential for end-to-end modeling and transfer learning from non-neurophysiological domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Originality**: First large-scale, multi-center iEEG dataset with harmonized annotations and clinical metadata.\n- **Quality**: Expert-validated data and diverse tasks ensure robustness and clinical relevance.\n- **Clarity**: Clear task definitions and dataset structure, supported by visuals.\n- **Significance**: Bridges ML and epilepsy research, enhancing reproducibility and translatability."}, "weaknesses": {"value": "- **Methodological Flaws**: Inter-rater reliability for 36K annotations is not quantified, risking bias. HFO detection algorithm selection lacks justification.\n- **Experimental Gaps**: No baseline model performance or cross-validation results are provided for benchmark tasks. Transfer learning potential is theoretical without empirical support.\n- **Oversight**: Data privacy protocols beyond de-identification are unclear. Scalability of annotation processes for future expansions is unaddressed.\n- **Validation**: Claims of clinical translatability lack pilot study results or online validation."}, "questions": {"value": "1. Can the authors provide inter-rater reliability metrics (e.g., Cohen’s kappa) for the 36K annotations to ensure consistency?\n2. What criteria were used to select and tune the HFO detection algorithms (e.g., Navarrete et al., 2016), and how were artifacts filtered?\n3. Can baseline performance metrics (e.g., AUC, F1) be provided for the benchmark tasks on Omni-iEEG?\n4. How was transfer learning from non-neurophysiological domains tested, and what specific performance gains were observed?\n5. What additional privacy measures were implemented beyond de-identification, and how will annotation scalability be managed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Lctgm8T03z", "forum": "rv9lQpY5cG", "replyto": "rv9lQpY5cG", "signatures": ["ICLR.cc/2026/Conference/Submission16453/Reviewer_ngqJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16453/Reviewer_ngqJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761372866153, "cdate": 1761372866153, "tmdate": 1762926564745, "mdate": 1762926564745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The core contribution of this paper is the construction of a large-scale, comprehensive iEEG dataset—Omni-iEEG, along with standardized benchmark tests for the dataset. The dataset includes patient data from multiple epilepsy centers and ensures data quality through expert annotations. The authors also conducted a series of machine learning model tests and evaluations on this dataset, particularly for clinical tasks such as epilepsy event classification and focal region identification.\n\n1. The paper seems to primarily focus on the construction of the dataset, while ICLR typically places more emphasis on algorithmic, model, or methodological innovations. How does this dataset significantly differ from existing EEG/SEEG datasets? If it is simply an extension of existing datasets, can the authors clarify its novelty more specifically?\n2. The paper mentions that the dataset will be made publicly available. Could the authors further clarify the dataset's release method and the specific platform for access?\n3. While the paper mentions the potential applications of the dataset, there is no concrete benchmark testing or application scenario presented. Can the authors provide one or two actual use cases or examples to show how this dataset can advance epilepsy detection and prediction models?\n4. The quality of the dataset is crucial for its application. While the paper mentions the annotation process and methods, it does not provide detailed information about the accuracy and consistency validation of these annotations. Could the authors elaborate on this aspect?\n5. Does the dataset include a diverse range of epilepsy patients to ensure its representativeness? Can the authors provide detailed statistics on the dataset, such as patient age, gender, and epilepsy type?\n6. Details regarding data preprocessing and collection methods could be further refined. Adding easy-to-understand flowcharts or tables would help readers better understand the dataset construction process."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.The Omni-iEEG dataset contains data from multiple epilepsy centers, offering a large sample size and diversity, which better represents different populations and pathological types.\n2.The dataset's annotations are performed by experts, ensuring high quality and accuracy, making it suitable for machine learning model training and clinical applications.\n3.The dataset provides standardized benchmark tests, enabling other researchers to evaluate the dataset easily, promoting model and method comparisons and validations in future studies.\n4.The dataset has wide applications in epilepsy detection, prediction, and seizure foci identification, with great potential to advance research in related fields.\n5.The dataset will be made publicly available, promoting wide usage in both academia and industry, encouraging data sharing and collaboration."}, "weaknesses": {"value": "1.The paper focuses mainly on dataset construction, while ICLR typically emphasizes innovation in algorithms, models, or methods. The dataset's contribution does not highlight any novelty in algorithms or methods, which may not meet ICLR's review standards.\n2.No concrete application scenarios presented: While the paper mentions the potential applications of the dataset, it does not demonstrate its actual effectiveness or value through specific benchmark tests or application scenarios, lacking real-world examples to support its claims.\n3.Insufficient detail on annotation accuracy and consistency: Although the paper mentions the annotation process, it does not provide a detailed description of how the accuracy and consistency of annotations are verified, leaving uncertainty about the reliability of the annotations.\n4.Lack of diversity and representativeness of the dataset: Although the dataset includes data from multiple centers, it does not adequately address whether it includes patients with various types of epilepsy. Detailed statistics about patients’ age, gender, epilepsy types, etc., are lacking.\n5.Insufficient detail on data preprocessing and collection methods: The paper does not provide enough detailed descriptions of the data preprocessing and collection methods, making it difficult for readers to fully understand the dataset construction process. There is a lack of clear flowcharts or tables to support this.\n6.Unclear data release plan: While the paper mentions that the dataset will be made publicly available, it does not specify the exact release methods and platforms, lacking transparency and a clear plan for making the dataset accessible."}, "questions": {"value": "1. The paper seems to primarily focus on the construction of the dataset, while ICLR typically places more emphasis on algorithmic, model, or methodological innovations. How does this dataset significantly differ from existing EEG/SEEG datasets? If it is simply an extension of existing datasets, can the authors clarify its novelty more specifically?\n2. The paper mentions that the dataset will be made publicly available. Could the authors further clarify the dataset's release method and the specific platform for access?\n3. While the paper mentions the potential applications of the dataset, there is no concrete benchmark testing or application scenario presented. Can the authors provide one or two actual use cases or examples to show how this dataset can advance epilepsy detection and prediction models?\n4. The quality of the dataset is crucial for its application. While the paper mentions the annotation process and methods, it does not provide detailed information about the accuracy and consistency validation of these annotations. Could the authors elaborate on this aspect?\n5. Does the dataset include a diverse range of epilepsy patients to ensure its representativeness? Can the authors provide detailed statistics on the dataset, such as patient age, gender, and epilepsy type?\n6. Details regarding data preprocessing and collection methods could be further refined. Adding easy-to-understand flowcharts or tables would help readers better understand the dataset construction process."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yjegATlENe", "forum": "rv9lQpY5cG", "replyto": "rv9lQpY5cG", "signatures": ["ICLR.cc/2026/Conference/Submission16453/Reviewer_Ew4m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16453/Reviewer_Ew4m"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896899414, "cdate": 1761896899414, "tmdate": 1762926563621, "mdate": 1762926563621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper assembles a multi-center presurgical iEEG benchmark by merging several public cohorts, harmonizing metadata/recordings, and adding expert-validated event labels (spike-associated HFOs). It defines clinically motivated tasks (event classification; pathological channel/patient-level analyses tied to SOZ/resection/outcome), proposes subject-level splits, and reports baselines spanning biomarker-centric pipelines and long-context end-to-end models. The authors state that data, code, and checkpoints will be released."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Consolidates fragmented iEEG datasets into a unified benchmark with consistent structure and task definitions, which could materially improve comparability in the area.\n\n2. Tasks and evaluation targets are tied to familiar clinical surrogates, increasing practical relevance.\n\n3. Adds a sizable layer of expert-validated event annotations (spkHFOs) with a described protocol and agreement checks.\n\n4. Includes both biomarker-driven and long-context end-to-end baselines, highlighting trade-offs and opening room for future work.\n\n5. If released with strong artifacts (schema, loaders, splits, checkpoints), the resource can become a de-facto standard."}, "weaknesses": {"value": "1. It’s hard to separate what is newly curated/validated post-merge (re-labeling, unified clinical ontology, normalized resection masks, QC decisions) from what is simply inherited. Please enumerate concrete new artifacts.\n\n2. Pooled or random subject splits are insufficient for a multi-center resource. The paper needs leave-one-center-out/per-center reporting for the primary tasks, not only a subset, to demonstrate robustness to site effects.\n\n3. “Harmonized” is described procedurally (e.g., resampling, montage, channel cleaning), but there’s little quantitative evidence that results are stable to these choices (referencing/resampling/artifact policy). Short ablations would increase trust.\n\n4. SOZ/resection/outcome fields appear inherited rather than re-adjudicated. Without normalization across centers, surrogates may encode site-specific conventions; sensitivity analyses to alternative definitions would help.\n\n5. The set omits a canonical EEG CNN (e.g., EEGNet/DeepConvNet-class) or a clear rationale for excluding it; a simple linear baseline on strong features would also anchor expectations.\n\n6. Code (preprocessing, splits, evaluation) and exact split files/checkpoints are not available during review; data access is deferred. For a benchmark paper, this materially limits verifiability"}, "questions": {"value": "1. Provide a bullet list of post-merge artifacts created by the authors (new labels, unified ontologies, resection mask normalization, QC) versus fields inherited unchanged.\n\n2. Report leave-one-center-out and per-center results for all primary tasks, with thresholds fixed on training centers and calibration (e.g., reliability) reported.\n\n3. Add brief ablations for referencing, resampling rate, and artifact policy to show conclusions are not artifacts of these choices.\n\n4. Summarize the inter-rater protocol for event labels and how detector-seeded candidates avoid biasing the class distribution; include agreement statistics and adjudication steps.\n\n5. Share (or commit to camera-ready) the repository with preprocessing/evaluation code, exact split files (including center IDs), model checkpoints, and prediction files used to compute tables; include a datasheet/dataset card and a concrete data availability statement (host, license/access path, date).\n\n6. Either add a canonical EEG CNN (or justify its omission) and a simple linear baseline on robust features, or clearly explain why they are not applicable here."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "O3VA6uaC2n", "forum": "rv9lQpY5cG", "replyto": "rv9lQpY5cG", "signatures": ["ICLR.cc/2026/Conference/Submission16453/Reviewer_5BzA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16453/Reviewer_5BzA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925797005, "cdate": 1761925797005, "tmdate": 1762926562912, "mdate": 1762926562912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}