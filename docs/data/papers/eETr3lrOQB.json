{"id": "eETr3lrOQB", "number": 2487, "cdate": 1757120859659, "mdate": 1759898145202, "content": {"title": "VQ-Transplant: Efficient VQ-Module Integration for Pre-trained Visual Tokenizers", "abstract": "Vector Quantization (VQ) underpins modern discrete visual tokenization. However, training quantization modules for state-of-the-art VQ-based models requires significant computational resources which, in practice, all but prevents the development of novel, cutting-edge VQ techniques under resource constraints. To address this limitation, we propose VQ-Transplant, a simple framework that enables plug-and-play integration of new VQ modules into frozen, pre-trained tokenizers by replacing their native VQ modules. Crucially, the proposed transplantation process preserves all encoder-decoder parameters, obviating the need for costly end-to-end retraining when modifying the quantization method. To mitigate decoder-quantization mismatch, we introduce a lightweight decoder adaptation strategy (trained for only 5 epochs on ImageNet-1k) to align feature priors with the new quantization space. In our empirical evaluation, we find that VQ-Transplant allows obtaining near state-of-the-art reconstruction fidelity for industry-level models like VAR while reducing the training cost by 95%. VQ-Transplant democratizes quantization research by enabling resource-efficient integration of novel VQ techniques while matching industry-level reconstruction performance.", "tldr": "VQ-Transplant enables efficient plug-and-play integration of new vector quantization techniques into frozen models with minimal computational overhead, achieving high fidelity while reducing training costs by 95%.", "keywords": ["VQ-Transplant", "Plug-and-play integration", "Computational cost reduction", "Pre-trained tokenizers"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e9128b49700013baaa3114ad9156175b153a0ae5.pdf", "supplementary_material": "/attachment/0df5f6bc1c1667d671a9811da4e3f650b35e91d1.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new method named VQ-Transplant, a computationally efficient framework for discrete visual tokenization designed to enable cheap and rapid exploration of novel Vector Quantization (VQ) techniques. To improve the compatibility of VQ-Transplant, the authors also propose a new variant called MMD-VQ. The paper conducts extensive experiments on image reconstruction to evaluate the proposed methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is well written.\n\n - The proposed method is plug-and-play and can be applied to any pretrained discrete image tokenizer.\n\n - This method effectively reduces the training time of the codebook."}, "weaknesses": {"value": "- Table 1 is intended to demonstrate that VQ-Transplant requires very few resources for training. However, I have some concerns. First, it only compares training time without comparing performance, so we cannot tell how well VQ-Transplant actually performs. Second, this comparison is also unfair, because VQ-Transplant benefits from a pre-trained tokenizer — comparing post-training cost with pre-training cost together is not a fair evaluation.\n\n - Table 2 shows that when the codebook size is 4096, the performance of MMD-VAR is comparable to VAR, but does not surpass it.\n\n - I believe that after a VQ tokenizer is trained, the encoder–decoder parameters and the codebook are highly correlated (otherwise the model would collapse). In this case, if a strong tokenizer is provided, retraining the intermediate codebook will naturally make it very close to the original one. I somewhat suspect that VQ-Transplant might simply be replicating the original codebook. The authors should conduct verification on the codebook to check whether it is merely a simple copy of the original.\n\n - The paper lacks evaluation on generative capability. Since the tokenizer is ultimately intended for generation tasks, whether it can outperform the baseline in generation is also a crucial part of evaluating the tokenizer’s performance.\n\n - It is recommended that the authors also conduct experiments on continuous VAEs to further validate the effectiveness of the proposed model."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HZor1NbrW3", "forum": "eETr3lrOQB", "replyto": "eETr3lrOQB", "signatures": ["ICLR.cc/2026/Conference/Submission2487/Reviewer_tdwt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2487/Reviewer_tdwt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2487/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810668452, "cdate": 1761810668452, "tmdate": 1762916253058, "mdate": 1762916253058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VQ-Transplant, a framework that allows plug-and-play replacement of vector quantization (VQ) modules in pre-trained visual tokenizers such as VAR, without retraining the full model.  \nThe method freezes the encoder and decoder, replaces the native VQ module, and applies a lightweight decoder adaptation for only five epochs on ImageNet-1k to mitigate decoder–quantization mismatch.  \nThe paper also introduces MMD-VQ, a new quantization method using Maximum Mean Discrepancy to align feature and codebook distributions without assuming Gaussianity, making it well-suited for real-world data.\n\nExperiments show that VQ-Transplant achieves comparable or even superior reconstruction quality to fully trained baselines (e.g., VAR, VQGAN) while reducing computational cost by up to 95%.  \nIt generalizes well across datasets (ImageNet-1K, FFHQ, CelebA-HQ, LSUN-Churches) and supports both fixed-scale and multi-scale quantization settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "### 1. Practical contribution  \nThe proposed idea of transplanting VQ modules into frozen tokenizers is both novel and highly practical. It enables flexible experimentation with quantization methods without the need for costly end-to-end retraining, significantly improving research efficiency.  \n\n### 2. Theoretical soundness  \nThe introduction of MMD-VQ offers a principled approach to aligning feature and codebook distributions without relying on Gaussian assumptions. This formulation enhances robustness and provides a clear theoretical improvement over prior methods such as Wasserstein VQ.  \n\n### 3. Empirical performance  \nExtensive experiments demonstrate strong empirical results and substantial reductions in training cost, with consistent improvements across multiple datasets and model architectures, validating both the effectiveness and efficiency of the proposed framework."}, "weaknesses": {"value": "### 1. Limited scope of tokenizer architectures  \nThe experiments are limited to VAR-based tokenizers. Evaluating the proposed method on diffusion-based or hybrid tokenizers (e.g., LDM, SANA) would better demonstrate its generality and robustness across different visual tokenization paradigms.  \n\n### 2. Limited evaluation in image generation and multimodality  \nIn modern settings, visual tokenizers are widely applied not only to image generation but also to multimodal understanding and reasoning.  \nIt would strengthen the paper if the authors could evaluate their approach on multimodal comprehension benchmarks such as Unified-IO, 4M (Massively Multimodal Masked Modeling), or TokLIP, if feasible within the rebuttal period."}, "questions": {"value": "1. Can MMD VQ be extended to video or multimodal quantization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yQhL889kYz", "forum": "eETr3lrOQB", "replyto": "eETr3lrOQB", "signatures": ["ICLR.cc/2026/Conference/Submission2487/Reviewer_auQc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2487/Reviewer_auQc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2487/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823192059, "cdate": 1761823192059, "tmdate": 1762916252354, "mdate": 1762916252354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an experimental approach to study the efficacy of different vector quantization (VQ) strategies in image tokenizers. The \"transplant\" aspect of their approach comes from the fact that they bootstrap from a pretrained image tokenizer. A sketch of their approach is as follows: (1) take a pretrained image tokenizer and replace the VQ module and learn a new codebook, (3) fine-tune the decoder of the pretrained image tokenizer with respect to the new VQ module. The authors demonstrate that they are able to improve a pretrained image tokenizer (VAR) using this approach with a new VQ algorithm MMD-VQ -- which is based on distributional similarity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The general idea of being able to plug and play different components to explore a space of algorithmic inductive biases is welcome. MMD-VQ is an interesting proposal, but there is limited analysis of its properties provided for it in the paper."}, "weaknesses": {"value": "In many ways this approach resembles a fairly standard practice of taking a pretrained vision model as a feature extractor and training a new classification layer. The representational biases of the features will still be biases to the pretraining task -- or in this case, the VQ method used by the original tokenizer. This is further exemplified by the fact that the decoder needs to be fine-tuned in order for this approach to work -- at this point, why not also finetune the encoder? How I see it, bootstrapping from a pretrained model is where all of the computational savings are coming from, not from freezing any part of the network.\n\nThe MMD approach is interesting and potentially novel. However, for me to be convinced that it is a good approach, the paper requires a more detailed analysis (empirical or mathematical) of its properties. The approach only seems to only work if the encoder features are fixed distribution -- joint optimization of the encoder I anticipate will result in mode collapse. This is usually where the Gaussian assumptions (KL loss) come in to stabilize training."}, "questions": {"value": "Is the optimization objective for MMD-VQ: Y* = argmin_Y D^2(X, Y), where X is a batch of images? If so, please specify this more explicitly in the paper. I also can't find where you specify the batch size used in the experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mCxBrElqHX", "forum": "eETr3lrOQB", "replyto": "eETr3lrOQB", "signatures": ["ICLR.cc/2026/Conference/Submission2487/Reviewer_8aY8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2487/Reviewer_8aY8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2487/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946905772, "cdate": 1761946905772, "tmdate": 1762916252143, "mdate": 1762916252143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the high computational cost of training quantization modules for state-of-the-art VQ-based visual tokenizers by proposing VQ-Transplant, a framework enabling plug-and-play replacement of VQ modules in pre-trained tokenizers without costly end-to-end retraining. it also introduces MMD-VQ, a novel VQ algorithm using Maximum Mean Discrepancy to align feature and codebook distributions, which eliminate Gaussian data assumption, for better compatibility with VQ-Transplant. VQ-Transplant achieves near state-of-the-art reconstruction fidelity on models like VAR, reduces training cost, and shows strong cross-dataset generalization on FFHQ, CelebA-HQ, and LSUN-Churches."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. VQ-Transplant gives efficiently an admirable performance with various VQ method. In Table 1 demonstrate a comparison over training cost among different work, which shows a large amount of time saving. \n2. MMD-VQ eliminate gaussian data assumption, which shows a certain of potential.\n3. MMD-VQ shows a competable performance as Wasserstein VQ and shows an ability of generalization to other domain."}, "weaknesses": {"value": "1. VQ-Transplant is only experimentally valid on VAR. The framework is exclusively validated on the VAR pre-trained tokenizer, with no tests on other mainstream visual tokenizers (e.g., VQGAN, LDM) to verify its compatibility across architectures.\n2. While the VQ transplant framework with frozen encoder weights is designed to enable the plug-and-play integration of new VQ modules into frozen, pre-trained tokenizers, it still lacks a full-scale training experiment. Without such an experiment, a direct performance-versus-time comparison between full-scale training and VQ transplant cannot be established. Consequently, it remains unclear whether the proposed framework delivers a superior performance-time ratio."}, "questions": {"value": "1. In most experiments, Wasserstein VQ and MMD VQ exhibit quite comparable performance. Given this, what are the practical advantages of MMD VQ over the former? Or in another way, can you prove that MMD-VQ does provide a better performance on non-gaussian data than Wasserstein VQ?\n2.  Since VQ-Transplant and MMD VQ are two key contributions of this paper, consideration should be given to dividing these two components into separate subsections in Chapter 4. Although you claimed MMD-VQ is a method introduced within VQ-Transplant, MMD-VQ can be implemented without VQ-Transplant. Describing these two contributions in a more separate way can help understand the subsequent experimental results. \n3. In line 340 there seems to be a mistake. \"Table 3 tracks the progression of r-FID metrics throughout decoder adaptation on ImageNet-1K across training epochs.\", \"table 3\" should be \"table 4\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cLiszAI9yN", "forum": "eETr3lrOQB", "replyto": "eETr3lrOQB", "signatures": ["ICLR.cc/2026/Conference/Submission2487/Reviewer_DdHb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2487/Reviewer_DdHb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2487/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762317064512, "cdate": 1762317064512, "tmdate": 1762916251951, "mdate": 1762916251951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}