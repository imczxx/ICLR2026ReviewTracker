{"id": "RILri9w9IW", "number": 24093, "cdate": 1758352650713, "mdate": 1759896781984, "content": {"title": "FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs", "abstract": "Speech tokenization enables discrete representation and facilitates speech language modeling. However, existing neural codecs capture low-level acoustic features, overlooking the semantic and contextual cues inherent to human speech. While recent efforts introduced semantic representations from self-supervised speech models or incorporated contextual representations from pre-trained language models, challenges remain in aligning and unifying the semantic and contextual representations. We introduce FuseCodec, which unifies acoustic, semantic, and contextual representations through strong cross-modal alignment and globally informed supervision. We propose three complementary techniques: (i) Latent Representation Fusion, integrating semantic and contextual features directly into the encoder latent space for robust and unified representation learning; (ii) Global Semantic-Contextual Supervision, supervising discrete tokens with globally pooled and broadcasted representations to enhance temporal consistency and cross-modal alignment; and (iii) Temporally Aligned Contextual Supervision, strengthening alignment by dynamically matching contextual and speech tokens within a local window for fine-grained token-level supervision. We further introduce FuseCodec-TTS, demonstrating our methodology’s applicability to zero-shot speech synthesis. Empirically, FuseCodec achieves state-of-the-art performance in LibriSpeech, surpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy, perceptual quality, intelligibility, and speaker similarity. Results highlight the effectiveness of contextually and semantically guided tokenization for speech tokenization and downstream tasks.", "tldr": "FuseCodec proposes a unified codec framework that integrates semantic, acoustic, and contextual representations through aligned cross-modal fusion and globally informed supervision.", "keywords": ["Speech Tokenization", "Neural Codec", "Cross-Modal Alignment", "Zero-Shot TTS", "Representation Learning", "Multimodal Fusion"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5930dc1f805700f2bf74869cd7a724136c84ca2b.pdf", "supplementary_material": "/attachment/73ff51979362655b801041b71f10a281c1c2f335.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduce a neural speech codec framework as FuseCodes which is designed to unify acoustic, semantic and contextual representation. It proposes three strategies to enrich discrete speech representation from low-level acoustic feature to high-level semantic and contextual information, 1) FuseCodec-Fusion, integrate the global semantic and contextual embedding to encoder's latent space through cross modal attention 2) FuseCodecs-Distill, supervise the output token from the RVQ layer and use global pooled vector to enforce the temporal consistency. 3) FuseCodec-ContextAlign, dynamic windowing align to match the sequence the contextual token to the audio token as fine-grained cross-modal alignment. \n\nIn the modeling stage, it leverages wav2vec 2.0 as ASR model, BERT as language model, and HuBERT as self-supervised speech model and all of these pre-trained model are frozen. The training objective adopts a multi-objective including time-domain reconstruction, frequency-domain reconstruction loss, adv loss, feature matching and RVQ commitment loss, and introduces semantic-contextual supervision loss during distill stage and aligned contextual supervision loss during ContextAlign stage.\n\nIn the experiment, it demonstrates this FuseCodec achieve best result in a downstream text-to-speech (TTS) task on speech reconstruction quality via perceptual quality metrics, lowest WER, high similarity, etc."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes 3 characteristic that a good speech tokenization would have low-level acoustic feature, high-level semantic and contextual cues. Regarding these targets, it proposes a framework by using pretrained model (wav2vec2 as contextual representation, huBERT as semantic representation) combined with encoded RVQ speech latent with 3 strategies to fusion, supervise and temporal align with different level representations.  This speech codec design is novel and with clearly identification of current issues. It systematically explores three distinct, complementary strategies (pre-quantization fusion, post-quantization global supervision, and post-quantization aligned supervision) .and provides a valuable comparison on them."}, "weaknesses": {"value": "1. The experiment database is 100 hours librispeech, which is not diverse or big enough to prove the generalization and representation capability for speech from different aspects such as channel, noise, speaker, accent, style, emotion and etc. While both wav2vec and HuBert are trained based on librispeech too, which would be a advantage for this experiment.\n\n2. The speech reconstruction evaluation only carry the objective measurements to evaluate the speech naturalness with ViSQOL, PESQ, UTMOS and similarity. There isn't any subjective measurement to prove the speech naturalness is highly related to real human perception.\n\n3. In this downstream task for TTS model it use both AR and NAR for 12 layer transformers. This architecture for TTS model is a little bit old. Nowadays, a \"good\" discrete speech codec should be with not only good representation for all important speech characteristic as this paper mentioned, but also with the high efficiency such as low framerate (such as 12.5hz, or even less as 7.5hz in VibeVoice to model very context speech) and easy prediction capability for LLM like models. These features and discussion are not motioned in this paper"}, "questions": {"value": "1. Besides separate techniques for the fusion, distill, contextalign, is there any experiment to combine any of them to achieve even better result?\n\n2. Is there any introduction of the speech reconstruction evaluation test set? Are they from the sampled librispeech or any other data? how many utterance or speaker includes, and how is the text difficulties such as long text which could show the high-contextual expressiveness advantages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w2bYJYuXP1", "forum": "RILri9w9IW", "replyto": "RILri9w9IW", "signatures": ["ICLR.cc/2026/Conference/Submission24093/Reviewer_Bdeu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24093/Reviewer_Bdeu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743964384, "cdate": 1761743964384, "tmdate": 1762942933970, "mdate": 1762942933970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FuseCodec, a speech codec that injects semantic (SSL model) and linguistic context (LM hidden states) directly into the codec's latent space. It uses three mechanisms: latent fusion via cross-attention and additive mixing, global distillation: broadcasting global semantic/context embeddings to all timesteps, sliding-window matching: aligning LM token features to the corresponding RVQ codebook vectors. The goal is to make discrete speech tokens carry acoustics + semantics + linguistic context. Experiments report improvements on LibriSpeech reconstruction, Codec-SUPERB tasks, and zero-shot TTS."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a significant and well-recognized limitation in neural speech codecs: their deficiency in capturing high-level semantic and contextual information. Improving this aspect is crucial for downstream tasks like speech language modeling.\n2. The evaluation is thorough, covering multiple facets of a codec's performance. It includes standard reconstruction metrics, a challenging representation benchmark (Codec-SUPERB), and a practical downstream application (TTS). The inclusion of multilingual experiments in the appendix is also a plus."}, "weaknesses": {"value": "1. The paper's primary weakness is its methodological fragmentation. It presents three separate, mutually exclusive techniques (-Fusion, -Distill, -ContextAlign) rather than a single, principled framework. There is little to no discussion on why these three different approaches were chosen or how they might be combined. The results further highlight this issue: different variants excel on different tasks (e.g., -Fusion is best for reconstruction, -Distill for TTS intelligibility, -ContextAlign for TTS naturalness). This suggests an exploratory study of \"what works\" rather than a novel, unified contribution.\n\n2. The core idea of distilling knowledge from pre-trained models (like HuBERT or BERT) into a codec is not new, as the paper itself acknowledges (citing works like SpeechTokenizer and DM-Codec). The novelty lies in the specific mechanisms, some of which are poorly justified: (1) The reliance on global vectors (mean-pooled semantics and a single [CLS] token for context) in both the -Fusion and -Distill variants is a major simplification. This approach discards vast amounts of temporal information inherent in speech and text. The paper fails to justify why this crude, global representation is preferable to using the full, time-aligned sequences from the teacher models. (2) The claims in Table 1 feel overstated. The distinctions between \"Sim.\", \"Direct.\", and \"Align.\" are not clearly defined, making the table seem more like marketing than a rigorous technical comparison. For example, DM-Codec also performs alignment via similarity matching, so the checkmark system seems arbitrary.\n\n3. While the proposed methods achieve state-of-the-art results, the improvements over the strongest baselines are often marginal. For instance, in Table 2, FuseCodec-Fusion improves the WER over DM-Codec from 4.09 to 3.99 and ViSQOL from 3.20 to 3.47. These gains, while real, may not be significant enough to justify the substantial increase in system complexity. The training process requires running three large pre-trained models (ASR, LM, SSL) to generate guidance signals, which is computationally expensive.\n\n4. On the Codec-SUPERB benchmark (Table 3), the performance is mixed. FuseCodec does not achieve the best results on ASR or ASV, two critical downstream tasks. It excels in ER and AEC. This further reinforces the impression that the learned representations are not universally better, but rather tuned for specific aspects, undermining the claim of creating a more robust, unified representation."}, "questions": {"value": "1. The three proposed methods are presented as separate variants. Did you attempt to combine them? For example, could one use latent fusion (-Fusion) and also apply temporally-aligned supervision (-ContextAlign)? If so, what were the results? If not, why is a unified approach not explored?\n\n2. Could you provide a stronger justification for using globally-pooled/CLS vectors for semantic and contextual guidance in -Fusion and -Distill? How does this compare, for instance, to using a standard cross-attention mechanism between the encoder's latent states and the full hidden state sequences from the teacher models?\n\n3. The \"FuseCodec (Baseline)\" in Table 2 performs worse than several existing codecs. Could you elaborate on this baseline's architecture? Is it a reproduction of a standard model like EnCodec? A stronger, more competitive baseline would better isolate the true contribution of your proposed fusion and supervision techniques.\n\n4. The alignment algorithm in FuseCodec-ContextAlign appears heuristic. Have you considered more established sequence alignment methods from the speech community, such as using a forward-sum algorithm inspired by CTC or attention-based alignment, to derive a more principled matching between text and audio frames?\n\n5. What is the computational overhead (in terms of training time and/or memory) of incorporating the three teacher models compared to training the baseline FuseCodec? An analysis of the performance-vs-complexity trade-off would be valuable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zFB4cF6Wqc", "forum": "RILri9w9IW", "replyto": "RILri9w9IW", "signatures": ["ICLR.cc/2026/Conference/Submission24093/Reviewer_cHbF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24093/Reviewer_cHbF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880737636, "cdate": 1761880737636, "tmdate": 1762942933617, "mdate": 1762942933617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FuseCodec, a neural audio codec that aims to integrate acoustic, semantic, and contextual information through three complementary strategies: latent representation fusion, global semantic-contextual supervision, and temporally aligned contextual supervision. The idea is to enrich discrete codec representations using multimodal cues from pretrained self-supervised speech and language models. The work is positioned as an extension over recent multimodal codecs such as DM-Codec and SpeechTokenizer, aiming to provide tighter cross-modal alignment and improved downstream performance. \n\nWhile the motivation of unifying contextual and semantic representations is sound, the technical contribution appears incremental and heavily engineered. The simplest variant performs best—latent fusion of global vectors. The added complexity of the global and temporally aligned supervision variants does not clearly translate into consistent or interpretable gains. In particular, I would have expected ContextAlign to improve content preservation but yields higher WER and only better UTMOS, which might indicate that perceptual quality improves independently of alignment accuracy. The experimental evidence does not fully support the stated motivation for complex alignment schemes. Moreover, the paper lacks a minimal baseline comparison using direct latent fusion of global semantic-context vectors without cross-attention or other modules, which would clarify the necessity of the proposed mechanisms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Attempt to unify acoustic, semantic, and contextual modeling within a codec framework is interesting.\n\n2. Evaluation across reconstruction, representation quality, and downstream TTS tasks.\n\n3. Comparison with strong baselines (EnCodec, SpeechTokenizer, DM-Codec, etc.).\n\n4. Demonstrates consistent improvements for the fusion variant on several benchmarks and bitrate-efficient design (4 kbps)."}, "weaknesses": {"value": "1. Most improvements come from latent fusion, while the additional supervision/align variants add complexity without proportional gains.\n\n2. The claim of “temporal dynamics” from global pooled vectors is conceptually unclear; global mean-pooled embeddings carry no temporal information.\n\n3. Cross-attention between already global vectors seems unnecessary and not well justified.\n\n4. ContextAlign underperforms in WER/WIL despite being motivated as improving alignment.\n\n5. Missing comparison against a simpler baseline that fuses global semantic and contextual embeddings directly without extra attention or alignment or supervision layers.\n\n6. Limited discussion of statistical significance of the results, no confidence intervals for any of the metrics."}, "questions": {"value": "1. How does cross-attention between globally pooled vectors meaningfully contribute beyond simple concatenation or addition?\n\n2. Why is the “temporal dynamics” claim justified when both semantic and contextual signals are globally aggregated?\n\n3. Can you report a baseline with direct latent fusion of global semantic/contextual vectors without cross-attention or supervision losses?\n\n4. For the ContextAlign variant, what explains the significantly higher WER yet better UTMOS? There are no failure case audio samples in the supplementary materials."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HinCRtCVhT", "forum": "RILri9w9IW", "replyto": "RILri9w9IW", "signatures": ["ICLR.cc/2026/Conference/Submission24093/Reviewer_Bd29"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24093/Reviewer_Bd29"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884408922, "cdate": 1761884408922, "tmdate": 1762942933286, "mdate": 1762942933286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FuseCodec, a speech tokenization framework that enriches neural codec tokens with semantic (SSL speech) and contextual (LM) information using three complementary techniques: (i) Latent Representation Fusion (injects semantic/contextual guidance into encoder latents before RVQ), (ii) Global Semantic-Contextual Supervision (distills pooled semantic/context vectors into first-stage RVQ tokens), and (iii) Temporally Aligned Contextual Supervision (a local window matching algorithm to align RVQ tokens with LM tokens for token-level losses). Using frozen wav2vec2 (ASR), BERT (LM), and HuBERT (SSL), the method reports SOTA on LibriSpeech reconstruction (intelligibility, quality, speaker similarity) and strong results on Codec-SUPERB at 4 kbps, and demonstrates applicability to zero-shot TTS."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper organizes prior semantic/contextual guidance ideas into a single codec framework with three variants that are easy to ablate and combine.\n\n- The windowed token-matching for contextual supervision is simple, inexpensive, and addresses speech/text rate mismatch better than global pooling.\n\n- The authors report ASR WER/CER, STOI, perceptual (NISQA/UTMOS), speaker similarity, and downstream TTS. These results show evidence that the learned tokens carry useful information beyond acoustics."}, "weaknesses": {"value": "- My main concern is that the paper is purely engineering and no theoretical justification. The window update heuristic (dynamic shift) is intuitive but somewhat ad-hoc; no analysis of stability/complexity vs sequence length or error propagation when early matches are wrong.\n\n- The models are trained primarily on LibriSpeech/LibriTTS (English, read speech). It is unclear whether gains persist on conversational, noisy, or non-English corpora; contextual alignment may degrade with ASR errors and long-form prosody.\n\n- Improvements may hinge on the quality of the ASR/LM/SSL guides (w2v2/BERT/HuBERT). The paper does not quantify sensitivity to these choices or to ASR transcription errors used to obtain LM tokens.\n\n- While components are described, it is not fully clear (a) how much each of Fusion vs Global-Distill vs ContextAlign contributes under identical training budgets, (b) whether Global-Distill + ContextAlign are complementary or redundant, and (c) how performance scales with RVQ depth/codebook size and 50 Hz frame rate."}, "questions": {"value": "- How does ContextAlign perform when transcripts are noisy (e.g., phone ASR with 15–20% WER)? Any results with weaker/stronger ASR or end-to-end speech-LM context (without text)?\n\n- What happens if we replace BERT with a recent LLM or a Conformer LM, or HuBERT with wavLM/Wav2Vec2-Large?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rhftxEWfwC", "forum": "RILri9w9IW", "replyto": "RILri9w9IW", "signatures": ["ICLR.cc/2026/Conference/Submission24093/Reviewer_mcfd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24093/Reviewer_mcfd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762038964915, "cdate": 1762038964915, "tmdate": 1762942932997, "mdate": 1762942932997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}