{"id": "SVI1ZnmFmx", "number": 2623, "cdate": 1757168286069, "mdate": 1759898136900, "content": {"title": "Lookahead Unmasking Elicits Reliable Decoding in Diffusion Language Models", "abstract": "Masked Diffusion Models (MDMs) as language models generate by iteratively\nunmasking tokens, yet their performance crucially depends on the inference-\ntime order of unmasking. Prevailing heuristics, such as confidence-based sam-\npling, are myopic: they optimize locally, fail to leverage extra test-time compute,\nand let early decoding mistakes cascade. We propose Lookahead Unmasking\n(LookUM), which addresses these concerns by reformulating sampling as path\nselection over all possible unmasking orders without the need for an external\nreward model. Our framework couples (i) a path generator that proposes paths\nby sampling from pools of unmasking sets with (ii) a verifier that computes the\nuncertainty of the proposed paths and performs importance sampling to subse-\nquently select the final paths. Empirically, erroneous unmasking measurably in-\nflates sequence-level uncertainty, and our method exploits this to avoid error-prone\ntrajectories. We validate our framework across six benchmarks, such as mathe-\nmatics, planning, and coding, and demonstrate consistent performance improve-\nments. LookUM requires only two to three paths to achieve peak performance, demon-\nstrating remarkably efficient path selection. The consistent improvements on both\nLLaDA and post-trained LLaDA 1.5 are particularly striking: base LLaDA with\nLookUM rivals the performance of RL-tuned LLaDA 1.5, while LookUM further\nenhances LLaDA 1.5 itself—showing that uncertainty-based verification provides\northogonal benefits to reinforcement learning and underscoring the versatility of\nour framework. Code will be publicly released.", "tldr": "", "keywords": ["generative models", "diffusion language models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/87710e41a32b1fe8cee3b7c648255f869f3a894b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes **Lookahead Unmasking (LookUM)**, a method to improve inference in masked diffusion models by optimizing the order of token unmasking through a two-step process: a *path generator* that proposes candidate unmasking sets, and a *path verifier* that scores these candidates using uncertainty estimated from one-step-ahead (“lookahead”) predictions. The core idea that I get is that local unmasking errors increase sequence-level uncertainty, and that LookUM can potentially avoid these error-prone trajectories by verifying potential paths before committing to an unmasking order. The paper shows empirical gains across six benchmarks (mathematical reasoning, planning, and code generation) using LLaDA and LLaDA 1.5 models, claiming improved accuracy with minimal computational overhead. The results suggest that uncertainty-based lookahead offers complementary benefits to reinforcement learning–based fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors reformulate unmasking as a path selection problem, offering a conceptually clean framing of inference-time decision-making in diffusion language models.\n\n- They demonstrate strong empirical improvements across multiple reasoning tasks (up to +8 points on MBPP, +4 on GSM8K), even over RL-tuned LLaDA 1.5.\n\n- It's computationally efficient — optimal performance achieved with only 2–3 candidate paths, comparable in cost to classifier-free guidance.\n\n- The paper is clearly written and well-organized, with ablation studies separating the impact of each component (path generator, verifier, sampling scheme).\n\n- Demonstrates complementary improvements to existing reinforcement learning–based optimization, suggesting broader applicability."}, "weaknesses": {"value": "1. The claimed theoretical motivation for the verifier is superficial. While the paper suggests that uncertainty correlates with path correctness, there is no formal justification (like via KL optimal control or stochastic path planning theory) linking the proposed verifier to optimal decoding behavior.\n\n2. The definition and construction of the “lookahead state” $\\tilde{x}_{t-1}$ isn't well put forth. It is unclear whether this state is sampled deterministically (argmax) or stochastically (categorical sampling), or whether such a proxy meaningfully represents the true next-step state distribution.\n\n3. The verifier’s uncertainty signal is evaluated only using entropy or confidence; there is no evidence is given that this correlates causally with improved unmasking fidelity or that it generalizes beyond the tested benchmarks.\n\n4. GSM8K performance values in Table 2 (≈70s) do not match Figure 3 (≈30), suggesting either metric mismatch or reporting inconsistency?\n\n5. Key hyperparameters (e.g., the threshold in Certainty Filtering, Table 4) are missing, and sensitivity analyses for these thresholds are not reported.\n6. The biggest weakness is by far the lack of comparison to **Path Planning (P²)** sampling (Peng et al., 2025), which addresses an almost identical problem—selecting optimal unmasking sequences via planning. As a result, this paper lacks significant novelty.\n\n7. The improvement claims depend entirely on *in silico* reasoning accuracy. There are no wet-lab or real-world experimental validations to show whether uncertainty-guided decoding actually leads to more robust or interpretable outcomes beyond benchmark accuracy."}, "questions": {"value": "1. How is the “lookahead state” $\\tilde{x}_{t-1}$ sampled — via argmax, sampling, or another scheme? How sensitive is the verifier’s performance to this choice?  \n\n2. Can the authors provide quantitative evidence (with correlation plots or ablation curves) showing that entropy-based uncertainty correlates with true reasoning correctness?  \n\n3. What is the threshold used in Certainty Filtering (Table 4), and how does performance vary with different threshold values?  \n\n4. How do the authors reconcile the mismatch between GSM8K scores reported in Table 2 and Figure 3?  \n\n5. Why haven't the authors compared LookUM to Path Planning (P2) or other planning-based decoding frameworks? If not, can they justify the omission?  \n\n6. Could a variant of LookUM integrate model-internal signals (attention entropy, gradient magnitudes, etc.) into the verifier?  \n\n7. Beyond benchmark metrics, the authors shoudl perform a wet-lab or grounded validation to test whether uncertainty-based decoding yields outputs that are more interpretable, verifiable, or experimentally meaningful?\n\nWith sound responses to these questions, I'd be willing to raise the score to a 4 or even a 6."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MCgwJdhKqe", "forum": "SVI1ZnmFmx", "replyto": "SVI1ZnmFmx", "signatures": ["ICLR.cc/2026/Conference/Submission2623/Reviewer_nMFR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2623/Reviewer_nMFR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761514400135, "cdate": 1761514400135, "tmdate": 1762916312458, "mdate": 1762916312458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Lookahead Unmasking (LookUM), an inference-time decoding framework for masked diffusion language models (MDLMs). The method reframes unmasking as a path selection problem: at each step, multiple unmasking “paths” are proposed, and a verifier based on sequence-level uncertainty selects the most consistent one. The approach is model-agnostic, requires no fine-tuning, and achieves consistent performance gains on reasoning, coding, and planning benchmarks with LLaDA and LLaDA-1.5."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written, clear, and motivated by a relevant problem — improving diffusion model decoding efficiency.\n\nThe formulation of unmasking as path selection is intuitive and offers a unifying framework that could, in principle, incorporate uncertainty, reward, or heuristic guidance.\n\nThe experimental setup is extensive, covering multiple reasoning and coding benchmarks and both base and RL-tuned diffusion LMs."}, "weaknesses": {"value": "#### **1. Unfair comparison due to unequal compute budgets**\n\nThe main experimental results (e.g., Table 2 ) compare LookUM — which explicitly samples **multiple paths (2–4 per step)** — against baseline methods that only use **a single sampling trajectory**. This means that LookUM’s inference-time compute is **2–4× higher**, as each path requires a separate forward pass through the model .\nThe authors claim that “the verifier overhead is negligible,” but the dominant cost in MDLM inference is model evaluation itself, not verifier scoring. Thus, a method using (k) paths incurs roughly (k\\times) inference cost. Comparing multi-path results against single-path baselines is **not a fair comparison** of sampling quality per unit compute.\n\nFor example, if LookUM achieves higher accuracy on HumanEval or GSM8K using 2 paths, it is effectively performing twice the work. The proper control would be either (a) match compute (e.g., let baselines sample twice), or (b) report performance *per unit of FLOPs* or wall-time. Without this normalization, the empirical improvement is difficult to interpret.\n\n#### **2. Comparison with Baselines**\n\nThe conceptual framework of treating unmasking as *path planning or path selection* has already appeared in earlier diffusion-decoding work. Prior studies (e.g., those exploring **path-planning for masked diffusion models** and **\nTrain for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions**.  The authors should compare with them."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u1ArA7E1w5", "forum": "SVI1ZnmFmx", "replyto": "SVI1ZnmFmx", "signatures": ["ICLR.cc/2026/Conference/Submission2623/Reviewer_Labt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2623/Reviewer_Labt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851494983, "cdate": 1761851494983, "tmdate": 1762916312268, "mdate": 1762916312268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Lookahead Unmasking (LookUM) for diffusion language models, reframing decoding as path selection over unmasking orders. A path generator samples candidate unmasking sets from a high-certainty pool, and a verifier scores one-step lookahead states using sequence-level uncertainty (avg. negative entropy or confidence), selecting paths via importance sampling (SMC/NIS). LookUM yields consistent gains on several reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is conceptually neat and easy to plug into existing pretrained diffusion language models. It's also a good combination of search algorithms and diffusion language models.\n- The proposed method consistently improves performance on several reasoning benchmarks (math, code, sudoku) compared with recent baselines.\n- This paper conducts detailed ablation studies on components of the proposed method and explores the integration with external reward models."}, "weaknesses": {"value": "- While the proposed method shows improvements on benchmarks, the score difference compared to the best baselines is not very large, especially given that the proposed method's computational cost is 2-3$\\times$ as much.\n- This paper doesn't show or compare measurements on the actual inference cost. That would make the performance-cost trade-off clearer.\n- The number of lookahead steps is an important hyperparameter for the proposed method. Throughout the paper it's fixed to be $1$. Why not consider more lookahead steps (e.g., more candidate branches, fewer lookahead steps VS fewer candidate branches, more lookahead steps, under the same computational budget)? What could be the difficulties?"}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "htp6lhn130", "forum": "SVI1ZnmFmx", "replyto": "SVI1ZnmFmx", "signatures": ["ICLR.cc/2026/Conference/Submission2623/Reviewer_6sEz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2623/Reviewer_6sEz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997245190, "cdate": 1761997245190, "tmdate": 1762916312093, "mdate": 1762916312093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Masked Diffusion Models (MDMs) train with an any-order objective, allowing multiple possible sampling paths. This paper addresses the problem of finding an optimal unmasking path during inference. Existing heuristic strategies are typically locally greedy and fail to capture sequence-level dependencies. To address this, the authors propose Lookahead Unmasking (LookUM) — a method that uses the average uncertainty of the next step to guide path selection. LookUM reframes decoding as a path selection problem, consisting of two components: a path generator that proposes candidate unmasking paths, and a verifier that scores these paths using sequence-level uncertainty. The paper reports strong empirical results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a simple yet effective inference-time approach for identifying optimal unmasking paths without modifying the training process.\n- The method is evaluated across multiple benchmarks and achieves strong empirical performance compared to existing baselines."}, "weaknesses": {"value": "- While LookUM increases inference-time computation, the paper does not clearly quantify this overhead compared to baseline methods. A more detailed analysis of inference time should be included in the paper.\n- Another way to improve diffusion model performance under a higher inference-time budget is to increase the number of denoising steps. It would be interesting to compare LookUM against this baseline under a fixed compute or time budget, to better understand its effectiveness.\n- The approach performs multiple forward passes for each inference step. Although conceptually simple, this can substantially increase inference time. However, it might not be necessary to perform LookUM during each of the inference steps. It would be useful to explore whether using any method could decide when to apply lookahead unmasking — potentially avoiding unnecessary lookahead steps and reducing the inference cost without significant performance loss."}, "questions": {"value": "- Would LookUM perform better if the verifier did not average negative entropy over all tokens? Perhaps focusing on a subset of the most uncertain tokens could reduce noise in the uncertainty estimation.\n- Is there any intuitive reason of the performance drop as we increase the number of paths (e.g., in MATH500)? How frequently does it occur? One possible explanation is that the estimated uncertainty does not perfectly correlate with the true path quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9WnJMmCyQc", "forum": "SVI1ZnmFmx", "replyto": "SVI1ZnmFmx", "signatures": ["ICLR.cc/2026/Conference/Submission2623/Reviewer_FvKa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2623/Reviewer_FvKa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762222961621, "cdate": 1762222961621, "tmdate": 1762916311157, "mdate": 1762916311157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}