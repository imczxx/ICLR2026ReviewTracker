{"id": "64f8NQ9Ptj", "number": 19338, "cdate": 1758295483835, "mdate": 1759897044629, "content": {"title": "Stochastic Layer-wise Learning: Scalable and Efficient Alternative to Backpropagation", "abstract": "Backpropagation underpins modern deep learning, yet its reliance on global gradient synchronization limits scalability and incurs high memory costs. In contrast, fully local learning rules are more efficient but often struggle to maintain the cross-layer coordination needed for coherent global learning. Building on this tension, we introduce Stochastic Layer-wise Learning (SLL), a layer-wise training algorithm that decomposes the global objective into coordinated layer-local updates while preserving global representational coherence.  The method is ELBO-inspired under a Markov assumption on the network, where the network-level objective decomposes into layer-wise terms and each layer optimizes a local objective via a deterministic encoder. The intractable KL in ELBO is replaced by a Bhattacharyya surrogate computed on auxiliary categorical posteriors obtained via fixed geometry-preserving random projections, with optional multiplicative dropout providing stochastic regularization. SLL optimizes locally, aligns globally, thereby eliminating cross-layer backpropagation. Experiments on MLPs, CNNs, and Vision Transformers from MNIST to ImageNet show that the approach surpasses recent local methods and matches global BP performance while memory usage invariant with depth. The results demonstrate a practical and principled path to modular and scalable local learning that couples purely local computation with globally coherent representations.", "tldr": "", "keywords": ["Local Learning", "Representation Learning", "Contrastative Learning", "ELBO"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bcaa6f99fa981a33e0975c94058b2c4e0cac557d.pdf", "supplementary_material": "/attachment/1837facfeb6a28437767db6b17d239963b21fc2c.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposes Stochastic Layer-wise Learning (SLL), a backprop-free training scheme that optimizes per-layer objectives derived from an ELBO-style decomposition under a Markov assumption. To couple layers without backward gradients, the method compares layer-wise auxiliary posteriors via a Bhattacharyya-based (BC) surrogate; these posteriors are produced from fixed, geometry-preserving random projections of activations and are optionally regularized with multiplicative dropout. The authors present a “blockwise” variant for deep CNNs/ViTs. Across MLPs, CNNs, and ViTs, SLL outperforms several local-learning baselines and approaches—sometimes slightly exceeds—BP accuracy, while keeping training memory nearly depth-invariant"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This approach coordinates layers by aligning their local posterior, which reduces reliance on symmetric feedback and dual passes.\n\n2.  Framing each layer’s training via an ELBO-style objective ties the method to well-studied variational principles, giving a clear theoretical lens for why optimizing local terms can support global performance."}, "weaknesses": {"value": "1. Figure 2 shows that no matter how many layers/blocks the model have, SLL's memory usage remain constant. This is a little bit counterintuitive as more layers would need more memory to train, even with layer-wise learning. Could authors explain this?\n\n2. All experiments are conducted on small datasets, not large scale ones. The reviewer is curious about the performance on large scale datasets such as ImageNet(not **Imagenette**, this is a small and simple dataset).\n\n3. The baselines in Table 1 are too old, there are some new BP free methods recently. The reviewer is wondering how SLL performs compare to these models.[1-3]\n\n4. Using BC between discrete posteriors after aggressive random projection may lose information. Can author provide the conditions that ensures faithful coordination across layers?\n\n[1] Kappel, David, Khaleelulla Khan Nazeer, Cabrel Teguemne Fokam, Christian Mayr, and Anand Subramoney. \"A variational framework for local learning with probabilistic latent representations.\" In 5th Workshop on practical ML for limited/low resource settings.\n\n[2] Zhang, Aozhong, Zi Yang, Naigang Wang, Yingyong Qi, Jack Xin, Xin Li, and Penghang Yin. \"Comq: A backpropagation-free algorithm for post-training quantization.\" IEEE Access (2025).\n\n[3] Cheng, Anzhe, Heng Ping, Zhenkun Wang, Xiongye Xiao, Chenzhong Yin, Shahin Nazarian, Mingxi Cheng, and Paul Bogdan. \"Unlocking deep learning: A bp-free approach for parallel block-wise training of neural networks.\" In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4235-4239. IEEE, 2024."}, "questions": {"value": "Please see weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n8qPTe39kX", "forum": "64f8NQ9Ptj", "replyto": "64f8NQ9Ptj", "signatures": ["ICLR.cc/2026/Conference/Submission19338/Reviewer_9nT6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19338/Reviewer_9nT6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760822094776, "cdate": 1760822094776, "tmdate": 1762931279146, "mdate": 1762931279146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Stochastic Layer-wise Learning (SLL), a layer-wise training algorithm inspired by the ELBO framework, which achieves significant memory savings compared to standard backpropagation. The paper is well-written with rigorous theoretical development. However, the effectiveness and scalability of the proposed algorithm require further experimental validation with more competitive methods and comprehensive benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper explores local learning from a probabilistic variational perspective, deriving a layer-wise learning objective based on the ELBO formulation. This theoretical contribution provides a fresh perspective on local learning.\n2. The paper is well-organized with clear exposition and rigorous logical flow, making the technical content accessible to readers.\n3. Beyond quantitative results, the paper provides additional visualizations including weight distributions and t-SNE-based representation analysis, which help understand the learned representations."}, "weaknesses": {"value": "1. Assumption 2 restricts the conditional dependence to only adjacent layer representations, leading to KL divergence-based local supervision that follows a first-order Markov assumption. This posterior estimation may neglect important long-range cross-layer information exchange, potentially limiting the model's expressive power.\n\n\n2. The use of random projection for dimension reduction may result in constrained representation learning and raises concerns about scalability to large-scale training with complex data distributions. The fixed random projection scheme may not adapt well to varying data characteristics across different layers and datasets.\n\n3. The experimental section lacks comparisons with strong baseline methods, particularly the absence of comprehensive evaluation on ImageNet. This limitation makes it difficult to assess the method's effectiveness and scalability on large-scale benchmarks."}, "questions": {"value": "1. Could Assumption 2 be extended by introducing a hyperparameter to control the range of conditional dependencies, thereby achieving a better trade-off between memory overhead and performance? For instance, making representation $h_i$ depend on both $h_{i-1}$ and $h_{i-2}$ could potentially capture richer hierarchical information.\n\n2. In the layer-wise objective function illustrated in Eq. (5), there appears to be no balancing coefficient between the expected likelihood term and the KL divergence term. As highlighted in prior work, shallow layers primarily serve feature extraction rather than direct classification. Would it be beneficial to assign lower weights to the expected likelihood term in shallow layers while emphasizing it more in deeper layers?\n\n3. From my perspective, the aggressive dimensionality reduction via random projection appears to be a critical bottleneck affecting model performance. Random projection may not be an optimal design choice; instead, the projection strategy should adapt to the data distribution or the statistical properties of layer-wise representations. Moreover, the paper lacks ablation studies on the dropout probability used in random projection.\n\n4. The current experimental results in Table 2 do not include comparisons with competitive baseline methods. Could you provide comparative results against InfoProp [1], AugLocal [2], and SGR [3] on ImageNet with various architectures including but not limited to ResNet and ViT?\n\n[1] Wang, Y., Ni, Z., Song, S., Yang, L., and Huang, G. Revisiting locally supervised learning: an alternative to end-to end training. In ICLR, 2021.\n[2] Ma, C., Wu, J., Si, C., & Tan, K. C. Scaling Supervised Local Learning with Augmented Auxiliary Networks. In ICLR, 2024.\n[3] Yang, Y., Li, X., Alfarra, M., Hammoud, H.A.A.K., Bibi, A., Torr, P. &amp; Ghanem, B.Towards Interpretable Deep Local Learning with Successive Gradient Reconciliation. ICML 2024.\n\n\n5. What is the motivation and theoretical justification for using the Bhattacharyya as a surrogate for the KL divergence term in the ELBO? Please elaborate on the advantages of this choice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HpYR9XIgCt", "forum": "64f8NQ9Ptj", "replyto": "64f8NQ9Ptj", "signatures": ["ICLR.cc/2026/Conference/Submission19338/Reviewer_GStK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19338/Reviewer_GStK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881893168, "cdate": 1761881893168, "tmdate": 1762931278724, "mdate": 1762931278724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Stochastic Layerwise Learning (SLL), which is a training framework where each network layer is optimized independently using a local stochastic loss instead of full backpropagation. Each layer projects its activations $h_i$ into a lower-dimensional space $v_i = R_i h_i$ and compares this projection to the target through a Bhattacharyya divergence. The paper formulates a variational bound linking local and global objectives and discusses information preservation under random projections. Experiments on MLPs, CNNs, and ViTs report reduced memory use and performance comparable to backprop, with projection dimension $d_0$ controlling the accuracy–cost trade-off."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The main strengths of this paper are:\n\n- $S_1$: This paper addresses one of the important practical bottlenecks in backpropagation (BP): the memory cost, as BP needs to store activation memory and computational graph which are especially heavy for ViTs and long sequences.\n- $S_2$: This paper introduces an alternative to BP that is conceptually simple. The overall training technique decouples units and supervise with a low-dimensional summary via a simple local divergence.\n- $S_3$: The presented empirical results report clear memory reductions on ViTs without major accuracy collapses.\n- $S_4$: The ablation study on $d_0$ are interesting, showing accuracy vs. projection size tradeoffs."}, "weaknesses": {"value": "While the empirical results regarding memory seem promising, major weaknesses prevent me from recommending anything but reject for now. Some of those may be easily corrected by modifying the paper ($W_2$ for example).\n\n- $W_1$: After reading the paper in detail (and the appendix), I am questioning its theoretical correctness:\n\n> The “average layerwise ELBO $\\leq$ network ELBO” inequality relies on two strong, unstated assumptions in the main text (monotone predictive gain across depth under the same variational measure; a global “KL budget” inequality). These are not consequences of standard factorization and, without them, the inequality can fail. \n\n> Furthermore, the mutual-information preservation claim under random projection theorem seems invalid as written: (i) mutual information for deterministic continuous mappings is ill-posed/infinite without an explicit noise model; (ii) the proof introduces Gaussian conditionals ad hoc; (iii) JL does not imply MI lower bounds for general distributions to the best of my knowledge.\n\nI will be more than eager to discuss this matter with the authors during the discussion phase.\n\n---\n\n- $W_2$: The FLOPs analysis (Table 1) is not correct:\n\n> It omits the cost of the projection $v_i = R_i h_i$ and the back-projection $R_i^T$ which adds $2d_od_i$ per layer per step.\n\n> It uses a single “max width $N$” big-O that hides layer-wise structure and $d_0$’s contribution, creating the misleading impression that SLL is $O(N)$ for dense layers.\n\n> For the ablation-coherent setting (3×1000 MLP with $d_0 \\approx 700$), SLL’s per-step compute exceeds BP’s; which contradicts the compute narrative.\n\n---\n\n- $W_3$: A smaller weakness which overall lowers the quality of the paper is that the ablation experiments are not coherent across architectures. \n\n> The projection size guidance is inconsistent: MLP ablations suggest $d_0\\approx 500–700$ for accuracy, but ViT experiments set \n$d_0=K$. The method’s accuracy–compute tradeoff thus depends on a hyperparameter whose selection principles are not unified."}, "questions": {"value": "Apart from answering the underlying questions present in the weakness section, I have the following questions that I would like to have a discussion about:\n\n- $Q_1$: Is it possible to state the theorem 1 in the main paper with all assumptions required in the appendix (monotone predictive gain; KL budget inequality)? Under what architectural or training conditions are these assumptions expected to hold? Can you provide counterexamples where they fail, or a restricted regime where they are provably satisfied?\n\n- $Q_2$: Can you either (i) add an explicit noise model and a rigorous MI bound (with proof or authoritative citation), or (ii) withdraw the MI claim and replace it with a JL-based geometric statement sufficient to justify your surrogate divergence? As of now, it is unsound.\n\n- $Q_3$: Can you present a principled rule for choosing $d_0$ that balances compute and accuracy and generalizes across architectures and datasets? Can you include accuracy vs. FLOPs curves for several $d_0$?\n\n- $Q_4$: Can you provide an end-to-end wall-clock training time and energy use (or GPU utilization) at equal accuracy to substantiate practicality beyond memory savings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4cJuQOABop", "forum": "64f8NQ9Ptj", "replyto": "64f8NQ9Ptj", "signatures": ["ICLR.cc/2026/Conference/Submission19338/Reviewer_KAMr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19338/Reviewer_KAMr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907256964, "cdate": 1761907256964, "tmdate": 1762931278287, "mdate": 1762931278287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Stochastic Layer-wise Learning (SLL), a probabilistic local-learning framework intended as a scalable and memory-efficient alternative to backpropagation. The authors reformulate deep network training as a variational inference problem, treating each layer’s activation as a latent variable and decomposing the global ELBO into layer-wise objectives under a Markov assumption. Each layer optimizes a local objective that combines a predictive likelihood term and a Bhattacharyya divergence surrogate for inter-layer alignment, computed on auxiliary categorical posteriors derived from random projections. This design enables local updates without global gradient propagation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a conceptually interesting and theoretically motivated attempt to unify local learning and probabilistic inference.\n\n2. The algorithmic formulation is elegant and modular, avoiding explicit backpropagation while maintaining representational coherence through stochastic projections.\n\n3. Experiments demonstrate that SLL can achieve performance close to backpropagation across several architectures, confirming the feasibility of local probabilistic learning."}, "weaknesses": {"value": "1. The Markov factorization across layers and the replacement of the KL term with a Bhattacharyya surrogate are heuristic. The paper does not prove that optimizing these surrogates reliably improves the global ELBO or overall convergence.\n\n2. The experiments focus on standard vision benchmarks such as MNIST, CIFAR, and ImageNette, which are relatively small and may not sufficiently test scalability or robustness. Larger-scale or non-vision domains would strengthen the claims.\n\n3. The method’s optimization behavior, variance properties, and potential degeneracies are not studied. It remains unclear under what conditions SLL will converge or fail.\n\n4. While accuracy comparisons are reported, the paper provides limited analysis of why SLL works, how layer-wise representations evolve, or how global coherence is preserved without backpropagation."}, "questions": {"value": "1. Can the authors clarify the exact conditions under which the arithmetic mean of local ELBOs forms a valid lower bound to the global objective?\n\n2. How sensitive is SLL to the choice of projection dimension and to the randomness of the fixed projection matrices?\n\n3. Are there any empirical diagnostics to demonstrate that local learning indeed maintains global representational alignment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vqjFyLIF42", "forum": "64f8NQ9Ptj", "replyto": "64f8NQ9Ptj", "signatures": ["ICLR.cc/2026/Conference/Submission19338/Reviewer_n4sL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19338/Reviewer_n4sL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974030773, "cdate": 1761974030773, "tmdate": 1762931277738, "mdate": 1762931277738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}