{"id": "vNIfmUPPfw", "number": 24767, "cdate": 1758360135759, "mdate": 1759896749804, "content": {"title": "Channel-Similarity Aware Spike Encoding for Multivariate Time-Series Forecasting", "abstract": "Spiking Neural Networks (SNNs) have attracted increasing attention for multivariate time-series forecasting due to their intrinsic energy efficiency and suitability for modeling temporal signals. However, existing SNN-based studies have largely focused on temporal dynamics, restricting their scope to spike encoding and temporal modeling. In contrast, recent advances in artificial neural networks demonstrate that modeling inter-channel similarity can substantially enhance forecasting performance. Despite this, such perspectives remain underexplored in the context of SNNs. In this work, we introduce a method that explicitly models inter-channel similarity through channel clustering and integrates it into temporal spike encoding. Specifically, we employ attention-based clustering to quantify channel similarity as cluster memberships, and leverage the Straight-Through Estimator (STE) to enable both end-to-end optimization and seamless integration into spike-based encoding. We evaluate the proposed approach on six benchmark datasets spanning diverse domains and temporal characteristics, using recurrent-, transformer-, and convolution-based SNN backbones. Experimental results show consistent improvements over baseline SNNs, achieving relative reductions in RRSE ranging from 3.0\\% to 6.5\\%. These findings highlight the potential of inter-channel similarity modeling as a complementary dimension to temporal dynamics in advancing the forecasting capabilities of SNNs.", "tldr": "We propose a spike-form channel clustering method that injects inter-channel similarity into temporal spike encoding, yielding up to 20.1% forecasting improvement across diverse SNN backbones and datasets.", "keywords": ["Spiking Neural Network", "Spike Encoding", "Inter-Channel Similarity"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03c4f808f44903c6b41db22355da8bd4065237d0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a preprocessing module with channel-similarity aware spike encoding for multivariate time-series forecasting. It provides additional spikes as subsequent model's input, and is shown to be useful in time-series forecasting."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method sounds reasonable for time-series forecasting."}, "weaknesses": {"value": "1. There are some writing problems. For example, in line 242, 'Then', 'Here' are in capital form without full stops. In line 254, 'fo' should be 'of' and 'By' should be 'by'. Table 1, 2, 3 and Figure 2, 3 are not referred in the main text. In line 268, the works regarding RNN, Spike-RNN, iTransformer, Spikformer, TCN, and Spike-TCN are not cited.\n2. See questions."}, "questions": {"value": "1. In the method part, what is the relationship of dimension variables, including $d$, $C$, $K$, $T$? What is the dimension of $X_i$ and $M$? I am not quite sure about them.\n2. In Figure 2, what is the meaning of Rank? If it is the rank of RRSE or some other indicators, it's strange to put it as an axis in the figure, since the two axes has similar meanings.\n3. In Table 1, why there are only 3 baseline/ours with 6 architectures? I cannot get it.\n4. In Table 2, what is the setting of 'SNN Time' and 'Channel'? It seems that they are not clearly stated.\n5. In Table 4, why the energy is increased so much with the proposed method? the FLOPs is not increased so much. By the way, does 'fr' mean firing rate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pGKNOAkwPZ", "forum": "vNIfmUPPfw", "replyto": "vNIfmUPPfw", "signatures": ["ICLR.cc/2026/Conference/Submission24767/Reviewer_YQYf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24767/Reviewer_YQYf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761498683162, "cdate": 1761498683162, "tmdate": 1762943191041, "mdate": 1762943191041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SummaryThis paper identifies a gap in SNN models for multivariate time-series forecasting: they typically focus on temporal dynamics and ignore inter-channel similarity. To address this, the authors propose a CSE module. This module first uses an attention-based clustering mechanism (adopted from ANN literature) to generate \"soft\" cluster membership probabilities ($M^{soft}$) for each channel. These soft probabilities are then converted into binary, spike-compatible \"hard\" cluster assignments ($M^{hard}$) using Bernoulli sampling and a Straight-Through Estimator (STE). Finally, this binary cluster information (a $K$-dimensional vector, where $K$ is the number of clusters) is concatenated along the SNN's time axis ($T_s$). The authors evaluate this method on six benchmark datasets using three SNN backbones (Spike-RNN, Spikformer, Spike-TCN) and report consistent performance improvements over the baseline SNNs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper correctly identifies an interesting and underexplored research gap: most SNN forecasting models process channels independently, ignoring inter-channel relationships that are known to be important in ANNs.\n\n\n- The proposed CSE module is designed to be modular and is demonstrated to work with multiple SNN backbones (recurrent, transformer, and convolutional).\n\n\n- The paper's presentation is a strength. The methodology is explained clearly, and Figure 1 provides a strong visual aid to understand the data flow."}, "weaknesses": {"value": "-  The main appeal of SNNs is low energy consumption. This method, by extending the SNN time axis, increases computational load and leads to a 2.7x to 3.3x increase in theoretical energy consumption (Table 4). The authors' claim in the conclusion that the \"accuracy-efficiency trade-off remains favorable\" is unsubstantiated and appears incorrect. A ~300% energy cost for the reported accuracy gains is a terrible trade-off.\n- The paper claims \"consistent improvements,\" but the main results table (Table 1) and text (Section 5.2) show this is not the case. The method provides only \"marginal\" gains on ETTh1, \"near-identical results\" on METR-LA, and \"slight degradations\" on Electricity. This makes the method unreliable.\n- The source of the energy increase is the integration method itself: concatenating $K$ static cluster vectors onto the $T_s$ time axis. This artificially inflates the sequence length that the SNN must process at every time step, which is an inefficient and naive way to inject static information. More efficient methods (e.g., using cluster IDs to modulate neuronal parameters, using a separate, smaller network to process cluster info) were apparently not explored."}, "questions": {"value": "Please refer to my weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6VyCr6Zs5c", "forum": "vNIfmUPPfw", "replyto": "vNIfmUPPfw", "signatures": ["ICLR.cc/2026/Conference/Submission24767/Reviewer_3oyS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24767/Reviewer_3oyS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848565810, "cdate": 1761848565810, "tmdate": 1762943190835, "mdate": 1762943190835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a spike encoding method that incorporates channel similarity awareness for multivariate time-series forecasting using SNNs. It addresses the limitation of existing SNN approaches, which primarily focus on temporal dynamics while neglecting inter-channel similarities. The proposed Channel-Similarity Encoding module employs attention-based clustering to compute soft channel memberships, converts them to binary spike forms via a Straight-Through Estimator, and integrates them into temporal spike encoding by concatenation along the SNN time axis. Testing on multi-domain datasets shows a decrease in RRSE, indicating the method's effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) The study adapts channel clustering concepts from ANNs to SNNs by proposing a framework that converts inter-channel similarity into a spike-compatible representation, utilizing the STE to binarize clustering results and align them with the SNN's sparse computation paradigm.\n\n2) The method integrates with various SNN backbone architectures and encoding techniques, showing consistent performance improvements across multiple datasets from different domains, as supported by experimental results."}, "weaknesses": {"value": "1) The results in Table 3 indicate that the optimal performance is achieved with n_cluster set to 3, yet the analysis lacks dataset-specific insights, such as visualizations of clustering outcomes, to substantiate the meaningfulness of clustering across different channels.\n\n2) A notable concern is that when k = 1 (i.e., without clustering), the results still show a significant performance improvement compared to the baseline without clustering. This raises questions about the necessity and specific contribution of the clustering component."}, "questions": {"value": "1) In the clustering process, does setting K=1 on the input equate to a no-clustering scenario, or does it effectively correspond to concatenating identical clustering information across all time steps (Ts) following the original sequence? \n\n2) If so, why do the ablation results for K=1 deviate from the baseline, and could the authors provide a detailed analysis to elucidate the underlying factors contributing to this discrepancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "41EOVxSsz8", "forum": "vNIfmUPPfw", "replyto": "vNIfmUPPfw", "signatures": ["ICLR.cc/2026/Conference/Submission24767/Reviewer_sNRp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24767/Reviewer_sNRp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888751225, "cdate": 1761888751225, "tmdate": 1762943190472, "mdate": 1762943190472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method to improve multivariate time-series forecasting in Spiking Neural Networks (SNNs) by incorporating inter-channel similarity. The core idea is that existing SNNs focus mostly on temporal dynamics and process channels independently, missing out on valuable cross-channel information. The proposed Channel-Similarity Encoding (CSE) module first uses an attention mechanism to cluster channels based on their similarity, producing soft cluster assignments. These soft assignments are then converted into binary, spike-compatible representations using a Straight-Through Estimator (STE). Finally, this channel information is concatenated with the standard temporal spike encoding and fed into various SNN backbones. Experiments across six datasets show that this method consistently reduces forecasting error compared to baseline SNNs, albeit with an increase in computational and energy costs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a clear and sensible gap in the literature. The motivation, that SNNs for time-series have largely ignored inter-channel relationships, a proven benefit in the ANN worldâ€”is well-argued and provides a strong foundation for the work.\n2. The proposed method is technically sound and modular. Using an attention-based clustering mechanism is a reasonable approach, and the use of the Straight-Through Estimator (STE) to integrate this into a spike-based framework is appropriate. The design allows it to be plugged into different SNN backbones (RNN, Transformer, TCN), which is a nice feature.\n3. The experimental validation is reasonably comprehensive. The use of six standard benchmarks and three different types of SNN architectures demonstrates the general applicability of the proposed CSE module. The inclusion of an analysis on theoretical energy consumption is also commendable, as it provides a more complete picture of the trade-offs involved."}, "weaknesses": {"value": "1. The trade-off between accuracy and efficiency needs more discussion. The results show a relative RRSE reduction of around 5.2% on average (Line 299), but Table 4 indicates a significant energy consumption increase of 2.7x to 3.3x. This is a substantial cost for a modest accuracy gain and seems to undermine the primary motivation for using SNNs, which is their efficiency. The paper presents these numbers but doesn't really delve into the implications of this trade-off.\n2. While the method improves upon SNN baselines, the performance still lags considerably behind standard ANN models. For instance, in Table 1, the proposed method with Spike-TCN on the Electricity dataset (horizon 96) achieves an RRSE of 0.350, whereas the iTransformer baseline achieves 0.226. This is a significant performance gap. A more direct acknowledgment and discussion of this remaining gap would help contextualize the contribution.\n3. The justification for the specific design choice of concatenating cluster information along the time axis could be stronger (Section 4.4). The paper mentions it is done to \"preserve binary spike values\" (Line 258), but it's not immediately obvious why this is superior to other potential fusion mechanisms. An ablation study or a more detailed explanation of alternatives considered would strengthen this part of the methodology."}, "questions": {"value": "1. Regarding the accuracy-energy trade-off highlighted in Table 4: could the authors elaborate on the practical applications or scenarios where a ~3x increase in energy cost is justified for the performance gains observed? Given that SNNs are often targeted for resource-constrained environments, this seems like a critical point to address.\n2. In Section 4.4, the binary cluster memberships are replicated and concatenated along the SNN time axis. Have other methods for integrating this information been considered? For example, could the cluster embeddings be used to modulate the firing thresholds of neurons or act as a form of attention mechanism over the temporal spikes? A little more insight into the design process here would be helpful.\n3. The clustering process described in Section 4.2 seems to generate a single set of cluster assignments for the entire input time-series window. This implies an assumption that inter-channel relationships are static within that window. Was any thought given to capturing more dynamic relationships, where channels might cluster differently at different points in time?\n4. The parameter analysis in Section 5.4 and Table 3 shows that performance is quite sensitive to the choice of n_cluster. Specifically, performance degrades when moving from 3 to 4 clusters. Is there an intuition for why this might be the case? Does this suggest that the datasets used have a small number of inherent, dominant channel groups?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zAA5APOhIo", "forum": "vNIfmUPPfw", "replyto": "vNIfmUPPfw", "signatures": ["ICLR.cc/2026/Conference/Submission24767/Reviewer_ioK9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24767/Reviewer_ioK9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762125914997, "cdate": 1762125914997, "tmdate": 1762943190148, "mdate": 1762943190148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}