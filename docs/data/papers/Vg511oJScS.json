{"id": "Vg511oJScS", "number": 18140, "cdate": 1758284273353, "mdate": 1759897129061, "content": {"title": "On the Expressiveness of State Space Models via Temporal Logics", "abstract": "We investigate the expressive power of state space models (SSM), which have recently emerged as a potential alternative to transformer architectures in large language models. Building on recent work, we analyse SSM expressiveness through fragments and extensions of linear temporal logic over finite traces. Our results show that the expressive capabilities of SSM vary substantially depending on the underlying gating mechanism. We further distinguish between SSM operating over fixed-width arithmetic (quantised models), whose expressive power remains within regular languages, and SSM with unbounded precision, which can capture counting properties and non-regular languages. In addition, we provide a systematic comparison between these different SSM variants and known results on transformers, thereby clarifying how the two architectures relate in terms of expressive power.", "tldr": "", "keywords": ["state space models", "expressiveness", "temporal logics"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8678349030efe025dcd2f00d5f2999af6b7d9c98.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a theoretical analysis of the expressive power of State Space Models (SSMs), establishing a formal hierarchy of their capabilities based on gating mechanisms (diagonal vs. time-invariant) and arithmetic precision (fixed vs. logarithmic). The core contribution is a set of lower bounds linking different SSM variants to specific fragments of temporal logic (PLTLf and its extensions), thereby characterizing which classes of formal languages each variant can recognize.\n\nThe work is purely theoretical, focusing on abstract computational expressiveness rather than practical performance or model design. It provides no empirical validation, engineering insights, or direct applications to real-world tasks. The motivation stems from a foundational interest in comparing SSMs to transformers within a formal logic framework, continuing a line of theoretical research in deep learning. While it clarifies the theoretical landscape, the paper's findings have no immediate or clear practical application for practitioners designing or training SSMs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper's primary strength is its “systematic” and “rigorous” approach. It aims to establish “precise”, “formal” connections between SSM variants and both well-understood and less well-known fragments of temporal logic (PLTLf), not adequately justified.\n\nThe work establishes a hierarchy of expressiveness. It differentiates the capabilities of diagonal-gated, time-invariant, and mixed SSMs, and further shows the critical impact of arithmetic precision (fixed vs. logarithmic). \n\nIt provides constructive proof sketches (proofs are in the appendix) showing how to translate a temporal logic formula into a corresponding SSM architecture. This offers some insight into the mechanisms (e.g., gates, recurrence) that enable specific computational behaviors.\n\nThe paper provides a formal comparison between the two competing architectural paradigms. It shows how the recurrent, state-based nature of SSMs leads to a different expressive profile compared to the attention-based transformers."}, "weaknesses": {"value": "This is a purely theoretical work, and as such it expected to be rigorous. However, many terms are not explained (e.g. $\\Sigma$ at line 84, $\\mathcal{P}$ at line 132, $H \\varphi$, $w$ at line 184). Not clear why the different logics where introduced, they appear out of the blue and it is not clear their practical importance. Why the considered extensions of pLTLf have been introduced?\n\nIts most significant weakness from a practical standpoint is the absence of any empirical evidence. The results are \"in vitro\" – they show what is possible in a formal model, but not what is practically efficient, learnable, or scalable with gradient descent on real data.\n\nThe analysis is decoupled from the practical motivations that drive SSM development (e.g., training speed, long-context handling, inference efficiency). It remains unclear how these theoretical expressiveness limits correlate with the models' performance on real-world tasks like language modeling or reasoning. A practitioner would not know if reaching for a more expressive (theoretically) SSM variant would yield any tangible benefits.\n\nThe paper focuses exclusively on language recognition, a very specific and limited task. It does not address the expressive power of SSMs for sequence-to-sequence tasks, generation, or their ability to compute functions over sequences, which are more relevant to their practical use.\n\nThe constructions rely on idealized components, such as hand-crafted weight matrices and specific non-linear functions. It does not address whether these precise configurations can be discovered through standard training procedures or if they are stable under stochastic gradient descent.\n\nThe heavy reliance on formal logic, circuit complexity, and automata theory makes the work largely inaccessible to a broad machine learning audience, limiting its immediate impact outside a specialized theoretical community."}, "questions": {"value": "The paper introduces several extensions of PLTLf (e.g., PLTLf[#↓], PLTLf[MOD]) without a clear, upfront motivation. Could the authors explicitly justify why these specific logical fragments are the most relevant or natural for analyzing SSMs? For instance, what is the intrinsic property of SSMs that makes the pure-past, backward-looking counting operator #↓ a better fit than a future-looking one?\n\nSeveral key symbols are used before being defined or are used inconsistently (e.g., Σ at line 84, P at line 132, w at line 184, H φ). For a paper aiming for precision, this hinders readability. Could the authors add a \"Preliminaries\" section or a notation table to ensure every symbol is formally defined upon first use?\n\nThe paper states that its results \"subsume\" the lower bounds of Sarrof et al. (2024). Could the authors provide a more detailed discussion, perhaps with a concrete example, of exactly what new expressiveness their construction captures that the prior work did not?\n\nThe constructive proofs show that an SSM exists which can recognize a given language. However, it relies on hand-crafted, precise weights. What is the authors' intuition or any theoretical insight on whether such configurations can be discovered via gradient-based learning, or are they likely to be in regions of the parameter space that are hard to reach?\n\nThe constructions, especially for fixed-precision arithmetic, seem to depend on exact numerical values to avoid state collisions. How robust are these constructions to the inevitable noise and approximation errors that occur during stochastic training? Would a small perturbation to the gate weights cause the SSM to fail on the languages it is supposed to recognize?\n\nThe entire analysis is framed around formal language recognition. How do the authors envision these theoretical results informing the design of SSMs for core practical tasks like next-token prediction (a function computation problem) or sequence-to-sequence translation? For example, does the inability of a fixed-precision diagonal SSM to recognize (aa)* imply a fundamental limitation on its ability to model certain syntactic or semantic structures in natural language?\n\nThe paper maps SSMs to a known hierarchy of circuit complexity and logic. However, for a practitioner, the key question is: \"Which SSM variant should I use for my problem?\" Could the authors speculate on what their expressiveness hierarchy implies for real-world performance? For instance, does the ability to express modular predicates (like in time-invariant SSMs) suggest an advantage for tasks requiring periodicity or hierarchical structure?\n\nWhile a purely theoretical paper need not include large-scale experiments, could the authors propose a minimal, synthetic experiment that could falsify or corroborate their theoretical predictions? For example, training different SSM variants on a synthetic dataset for (aa)* or $a^n b^n$ and observing if the results align with the theoretical capabilities (e.g., diagonal SSMs failing to learn (aa)*).\n\nThe heavy reliance on temporal logic and circuit complexity can limit the paper's impact. Have the authors considered adding a more extensive, intuitive overview in the introduction that explains the \"story\" and the practical implications of the different logic fragments (e.g., \"PLTLf corresponds to pattern X, while adding counting allows for pattern Y\") before diving into the formal details? A summary figure comparing the practical capabilities (e.g., \"can detect repeated patterns,\" \"can count events\") of each SSM variant would be immensely helpful.\n\nThe sentence “We use this restriction, because in contrast to transformers, where the attention mechanism is usually able to attend to all positions, SSM can only see information from previous positions.” at lines 157-159 is unclear. Please clarify its meaning and why this sentence is important; similarly, for the other sentence at lines 166-167, whose role in the paper is not known."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "InWnGYHXYm", "forum": "Vg511oJScS", "replyto": "Vg511oJScS", "signatures": ["ICLR.cc/2026/Conference/Submission18140/Reviewer_FYk9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18140/Reviewer_FYk9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761230609021, "cdate": 1761230609021, "tmdate": 1762927898201, "mdate": 1762927898201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the expressive power of state space models (SSMs), an alternative to transformers, exploiting formal methods. Various fragments and extensions of Linear Temporal Logic (LTL) over finite traces (LTL$_f$) are used to proxy the SSM expressiveness. It investigates two aspects: the SSM gating mechanism (diagonal, depending on the input, vs time-invariant, remaining constant) and the arithmetic precision (fixed-width vs log-precision). Each setting combination recognizes different languages (e.g., SSM with time-invariant gates recognizes the past fragment of LTL$_f$ with unary past operators and the MOD operator with fixed-precision, while it recognizes the extended version of the same language with a past-counting operator with log-precision). All the results are formally proved, and I appreciate this a lot. The paper also positions and discusses its results with respect to relevant transformer architectures, adding value to the overall contribution. The paper is excellent and presents its results very clearly. I will recommend acceptance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "(S1) The paper is exquisite with a clean presentation. The mathematics is there, and the authors know what they are doing. \n\n(S2) The paper's expressiveness study is impactful as it sets new grounds for what SSMs can express or not.\n\n(S3) The work bridges the gap between symbolic and neural AI. For me, this kind of investigation allows us to see temporal, formal logic through a new lens. \n\n(S4) Conjecture 1 makes sense. I don't have a formal argument, but I think the provided argument is sound.\n\n(S5) Even if the study addresses only SSMs, the contextualization of the results with respect to other transformer architectures adds value. It closes the circle elegantly."}, "weaknesses": {"value": "(W1) I really enjoyed the mathematical framework and arguments (with all the proofs), but I think the paper could benefit from an additional figure showcasing how such results are related (e.g., which Lemma(s) lead to which Theorem(s)). I expect this to improve the navigation of the paper's results.\n\n(W2) The layout/presentation of Figures 1, 2, and 3 should be standardized. These figures could benefit from a more descriptive caption. \n\n(W3) Some minor inconsistencies that I've identified:\n\n- line 84: FNN is not defined (it is, however, defined at line 104)\n- lines 148--156: pLTL$_f$ with both past- and future-counting operators seems counterintuitive. Since \"p\" stands for past, I would have presented it differently; for example, \"LTL$_f$ with both past- and future-counting operators extends LTL$_f$ $\\ldots$\" and then (at line 157) I would have said that \"By pLTL$_f$ with past-counting operator we denote the logic in which every counting subformula has $b_j = 0$ for all $j$, a fragment of LTL$_f$ with both past- and future-counting operators.\" Moreover, I would rather use $\\sigma$ instead of $w$ to denote the words (as in the semantics at lines 134--140).\n- line 252: missing closing period\n- line 353: $\\ell$ should be $l$ (see line 350)\n- line 631: since $N_1 : \\mathbb R^{m_1} \\to \\mathbb R^{n_1}$ and  $N_2 : \\mathbb R^{m_2} \\to \\mathbb R^{n_2}$, then $N_1 \\circ N_2 : \\mathbb R^{m_2} \\to \\mathbb R^{n_1}$ if $n_2 = m_1$; thus, $\\mathbb R^{m_1} \\to \\mathbb R^{n-2}$ is wrong. In any case, I would rather use $N_2 \\circ N_1 : \\mathbb R^{m_1} \\to \\mathbb R^{n_2}$ with $n_1 = m_2$; it is just simpler to read.\n- lines 635 & 638: $\\text{diag}(A \\mathbf x_t)$ vs $(A \\mathbf x_t)$\n- line 707: it should be $\\varphi_1 S \\varphi_2$ instead of $\\varphi S \\varphi_2$ \n- Proofs of Lemma 12 & Theorem 10: for the cases, please use either bold or emphasize, but not both."}, "questions": {"value": "(Q1) Can you please add an additional figure to improve the accessibility to readers to navigate the results? \n\n(Q2) I would use different aesthetics for Figures 1, 2, and 3. For example, the \"dashed\" versus \"non-dashed\" inclusions could have symbols on the edges (e.g., $\\subset$). Also, Figure 1 uses $\\ge$, while Figures 2 and 3 use $\\subseteq$; indeed, Figure 1 says \"SSM with diagonal gates $\\ge$ pLTL$_f$ with past-counting operator ($\\log$-precision) (Thm. 4)\", while Figure 2 says \"pLTL$_f$ with past-counting operator $\\subseteq$ diagonal SSM (Thm. 4)\". The captions could be improved as well. These inconsistencies detract from the overall readability. Can you please improve all these aspects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R8J0tqDPdg", "forum": "Vg511oJScS", "replyto": "Vg511oJScS", "signatures": ["ICLR.cc/2026/Conference/Submission18140/Reviewer_M8MD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18140/Reviewer_M8MD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823254418, "cdate": 1761823254418, "tmdate": 1762927897861, "mdate": 1762927897861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies variants of state space models (SSMs) from a theoretical perspective and seeks to establish lower bounds on their expressivity. It builds upon prior work that provided upper bounds via circuit complexity. It uses fragments and extensions of (pure-past) linear temporal logic on finite traces (pLTLf) to capture the lower bounds.\n\nThe paper begins by presenting a formal model of SSMs and the lower bounds on the expressivity are established through the explicit construction of SSM layers that encode different logical operators. The two types of SSMs considered are SSMs with diagonal and time-invariant gates. Their results establish that SSMs with diagonal gates can capture pLTLf and the time-invariant version can capture the unary operator fragment un-pLTLf, along with modular counting. Additionally, log-precision arithmetic that scales precision logarithmically with input length can allow the models to perform counting."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The work builds upon an established literature that studies the limits of expressivity of different ML architectures from a theoretical lens. The paper studies the full breadth of different SSM variants and a clear expressivity hierarchy is established. Counterexamples have also been provided where possible to show that the lower bounds are \"tight\" at least from the temporal logic perspective."}, "weaknesses": {"value": "The primary weakness is that these results do not suggest any ways to improve ML techniques and are far removed from practice.\n\nThere is a lot of room for improving the presentation. The abstract conveys the main message well but the introduction is severely lacking in details and a reader unfamiliar with formal logic/complexity theory would find it hard to comprehend. Many aspects are not provided with an explanation. For example, the introduction never discusses what it means to recognize a language or what LTL fundamentally is because the general ML community would not be familiar with temporal logics or formal languages. The presentation would be greatly improved with an extended introduction that puts more effort in explaining the results intuitively. This is challenging considering the amount of formalisms required but it is necessary.\n\nComments about notation:\n- There are a few times when a layer is referred to by an $\\ell$ instead of $l$.\n- FNN abbreviation is used in line 84 before the expanded form is mentioned.\n- The operators tt, ff, and H would be obvious to a person familiar with formal logic but not to everyone. They need to be provided with an intuitive explanation.\n- ex. is used instead of $\\exists$ or just exists which is non-standard.\n- First-order logic is used but never defined.\n- Circuit complexity classes are also used several times but never mentioned what they mean."}, "questions": {"value": "Please comment on the weaknesses mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YQvufTmrVK", "forum": "Vg511oJScS", "replyto": "Vg511oJScS", "signatures": ["ICLR.cc/2026/Conference/Submission18140/Reviewer_hFRJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18140/Reviewer_hFRJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929709747, "cdate": 1761929709747, "tmdate": 1762927897324, "mdate": 1762927897324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the expressive power of State Space Models (SSMs) in recognizing formal languages through Propositional Linear Temporal Logic (PLTL).\nDiagonal SSM (with fixed-precision) can recognize PLTL_f, which is equivalent to star-free regular languages (FO[<]), but they cannot\nrecognize non-star-free languages like (aa)*.\nDiagonal SSM with log-precision can use a backward counting operator, so they can handle UN-PLTL_f.\nTime-Invariant SSM (fixed-precision) can recognize UN-PLTL_f[MOD] by using cyclic permutation matrices to maintain modulo counters and so they can recognize languages like (aa)* which are not star-free.\nTime-Invariant with log-precision also handle backward counting.\n\nMixed SSM\n   - Fixed-precision: Recognize all regular languages in AC0 (which is PLTL_f[MOD]).\n   - Log-precision: Recognize PLTL_f.\n\nComparison to Transformers: \n- Diagonal fixed-precision SSMs are comparable to UHAT without positional encodings (FO[<]). \n- Time-invariant fixed-precision SSMs are similar to UHAT with positional encodings (FO[<,MOD]).\n- Mixed log-precision SSM align with AHAT in expressiveness."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The proofs builds on top of the  temporal logic theory and circuit complexity (AC⁰/TC⁰) using established formal frameworks from transformer expressiveness research.  \n- Bounds are tight: Diagonal SSMs’ impossibility to recognize `(aa)*` is proven via star-free language separation.  \n- Gate constructions are explicit and computable.  \n- Language recognition capabilities are formally verified, not empirically approximated.  \n- Limitations (e.g., TC⁹ upper bound not yet reduced to AC⁰ for SSMs) are presented."}, "weaknesses": {"value": "- Empirical validation of depth hierarchies suggested but not explored."}, "questions": {"value": "Could you provide preliminary experiments demonstrating mixed SSM’s ability/failure on synthetic languages under finite precision? If not, is such validation planned for the final version?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HncarDEzy0", "forum": "Vg511oJScS", "replyto": "Vg511oJScS", "signatures": ["ICLR.cc/2026/Conference/Submission18140/Reviewer_FusE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18140/Reviewer_FusE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762164541088, "cdate": 1762164541088, "tmdate": 1762927897006, "mdate": 1762927897006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}