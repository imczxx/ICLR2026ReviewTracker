{"id": "mKFbXHpOjb", "number": 20375, "cdate": 1758305268145, "mdate": 1759896981159, "content": {"title": "Tabular Learning with Background Information: LLMs, Knowledge Graphs, or Both?", "abstract": "Tables have their own structure, calling for dedicated tabular learning methods with the right inductive bias. These methods outperform language models. Yet, many tables contain text that refers to real-world entities, and most tabular learning methods ignore the external knowledge that such strings could unlock. Which knowledge-rich representations should tabular learning leverage? While large language models (LLMs) encode implicit factual knowledge, knowledge graphs (KGs) share the relational structure of tables and come with the promise of better-controlled knowledge. Studying tables in the wild, we assemble 105 tabular learning datasets comprising text. We find that knowledge-rich representations, from LLMs or KGs, boost prediction, and combined with simple linear models they markedly outperform strong tabular baselines. Larger LLMs provide greater gains, and refining language models on a KG boosts models slightly. On datasets where all entities are linked to a KG, LLMs and KG models of similar size perform similarly, suggesting that the benefit of LLMs over KGs is to solve the entity linking problem. Our results highlight that external knowledge is a powerful but underused ingredient for advancing tabular learning, with the most promising direction lying in the combination of LLMs and KGs.", "tldr": "Extensive experiments on real-life datasets show that knowledge-rich representations boost tabular learning, the most promising alley being to refine LLMs on Knowledge Graphs", "keywords": ["Tabular Learning", "Large Language Models", "Knowledge Graphs"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ef22e609646764a237155289659d7fd56018d331.pdf", "supplementary_material": "/attachment/2347278be3334101074f380f9a976626d5faa80c.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a large-scale empirical study to determine the best source of background knowledge for tabular learning tasks, specifically focusing on textual data within tables. The authors compare representations derived from pure Large Language Models (LLMs), structured Knowledge Graphs (KGs), and hybrid models that refine LLMs on KGs. Using a newly assembled benchmark of 105 tabular datasets containing text, the study finds that knowledge-rich representations significantly boost predictive performance, even allowing simple linear models to outperform strong tabular baselines like XGBoost. A key finding is that larger LLMs provide greater gains. Furthermore, on a controlled subset of 15 tables with pre-solved entity linking, pure KG embeddings perform on par with LLMs of similar size. This leads the authors to conclude that the primary advantage of LLMs is not superior knowledge storage but their innate ability to solve the \"symbol grounding\" or entity linking problem. The paper advocates for hybrid LLM+KG approaches as the most promising future direction."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses the important and practical problem of leveraging external knowledge from textual features in tabular data . The assembly of a new, large-scale benchmark of 105 datasets is a significant contribution to this area of research.\n2. The study provides a large-scale, systematic comparison across a wide spectrum of models. This includes multiple LLM families (e.g., Llama-3, Qwen3, ROBERTa, T5) and sizes , various hybrid LLM+KG models (e.g., ERNIE, KGT5, Knowledge Card) , classic pure KG embedding methods, and several distinct downstream tabular learners (Ridge, XGBoost, TabPFNv2).\n3. The paper clearly articulates the conceptual gap between traditional tabular learning and knowledge-rich modeling. This framing highlights why background knowledge matters for textual tables and situates the problem within the broader challenge of symbol grounding and structured reasoning, all presented with clear and precise exposition."}, "weaknesses": {"value": "1. My most significant concern is the paper's narrow definition of its own problem domain. In Section 3.1, the authors state, \"we remove all numerical columns to focus our study on text-based knowledge\". This single methodological choice fundamentally changes the problem from heterogeneous tabular learning (the domain of XGBoost, TabPFNv2, and real-world tables) to a short-text prediction problem. This makes the central claim (Finding 1) that their method \"outperforms strong tabular baselines\" deeply problematic. The SOTA baselines (XGB, TabPFN) are being benchmarked in an artificial setting they were not designed for (text-only). A fair comparison would require a heterogeneous setup.\n2. The SOTA comparison in the paper is incomplete. The authors claim to “markedly outperform strong tabular baselines,” yet their evaluation is limited to models such as XGBoost and TabPFNv2. More critically, the study omits comparisons with genuine state-of-the-art methods for deep tabular learning—such as **FT-Transformer**, **SAINT, CARTE**, or competitive **MLP-based baselines**—which are explicitly designed to handle the kind of heterogeneous tabular data that this paper excludes. Additionally, **LLM-based approaches** like **TabLLM** are not considered, further weakening the strength of the claimed performance advantages.\n3. The paper's conclusions are drawn from an experimental setup heavily skewed towards small-data regimes. The methodology explicitly states, \"To simulate small-data scenarios... we sample training sets of varying sizes, $n_{train}\\in\\{64, 256, 1024\\}$\"14. All major results and ranking diagrams (e.g., Fig. 7, 18) are reported at $n=1024$. While the rationale (external knowledge is most critical here) is valid, this is a \"cherry-picked battlefield\" that maximizes the utility of external knowledge and disadvantages GBDTs, which are known to excel as $n_{train}$ grows. The paper provides no evidence that these conclusions generalize to larger, more realistic training sets (e.g., $n_{train} > 10,000$).\n4. The novelty of the second finding (\"Refining LLMs on KGs is a promising combination\") is limited. As the paper's own related work section details, this is a well-established research direction. The paper's contribution here is to validate this on their benchmark, but the finding that this refinement only \"boosts models slightly\"  makes this contribution feel incremental rather than a significant breakthrough."}, "questions": {"value": "1. To make the comparison to SOTA tabular learners fair and the conclusions relevant to *tabular learning*, the authors should include experiments on the original, heterogeneous data. The most direct approach would be to **concatenate** the numerical features (which were removed) with the new text-based row embeddings. How do the results change in this (more realistic) setting?\n2. Can the authors provide results on at least a medium-sized training set (e.g., $n_{train}=10240$) to demonstrate the generalizability of their findings?\n3. The authors are encouraged to extend their SOTA comparison by including modern deep tabular and LLM-based baselines—such as FT-Transformer, SAINT, DANet, TabNet, or TabLLM. Including these would greatly strengthen the validity of the paper’s claims and situate the proposed method more clearly within the current landscape of tabular learning research."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yLFF2SNfrf", "forum": "mKFbXHpOjb", "replyto": "mKFbXHpOjb", "signatures": ["ICLR.cc/2026/Conference/Submission20375/Reviewer_NUSu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20375/Reviewer_NUSu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761052271333, "cdate": 1761052271333, "tmdate": 1762933828615, "mdate": 1762933828615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a large-scale empirical study on leveraging external background knowledge—specifically from large language models (LLMs) and knowledge graphs (KGs)—to improve tabular learning on datasets containing textual entity mentions. The authors assemble a benchmark of 105 real-world and semi-synthetic tabular datasets, systematically evaluate a wide range of representation methods (including pure LLMs, pure KG embeddings, and LLMs refined on KGs), and analyze performance under controlled settings—most notably using a subset of 15 tables with pre-linked Wikidata entities. Key findings include: (1) knowledge-rich representations significantly outperform traditional text encodings (e.g., TF-IDF); (2) larger LLMs yield consistent gains; (3) refining LLMs on KGs provides modest but reliable improvements in performance and parameter efficiency; (4) when entity linking is solved, KG embeddings perform on par with same-sized LLMs, suggesting that LLMs’ main advantage lies in implicit symbol grounding rather than superior knowledge quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Comprehensive and well-structured benchmark: The collection of 105 diverse datasets from three distinct sources (TextTabBench, CARTE, WikiDBs) provides strong external validity. The construction of a 15-table “entity-linked” subset is a methodological highlight that enables clean isolation of the entity linking factor.  \n\n- Clear, actionable insights: The paper convincingly demonstrates that representation quality—not just model architecture—is the bottleneck in text-rich tabular learning. The conclusion that LLMs primarily help by solving symbol grounding is both nuanced and valuable for the community.  \n\n- Reproducibility: Experimental protocols (train sizes, random seeds, PCA dimensionality, estimator choices) are thoroughly documented, and runtime costs are reported—enhancing practical utility.  \n\n- Balanced model coverage: The evaluation spans a wide spectrum of models, from classic KG embedders (RotatE, ComplEx) to modern LLMs (Llama-3, Qwen3) and hybrid approaches (Knowledge Card, ERNIE)."}, "weaknesses": {"value": "- Narrow scope of KG evaluation: Pure KG models are only evaluated on the 15 linked tables, which represent a best-case scenario. The paper does not assess how KG-based methods degrade under realistic, noisy, or partial entity linking—thus overestimating their practical applicability.  \n\n- Lack of privacy or robustness considerations: Given the focus on external knowledge, the paper overlooks critical issues such as leakage of sensitive entities via embeddings, vulnerability to adversarial entity perturbations, or compatibility with privacy-preserving learning (e.g., federated or differentially private settings).  \n\n- Downstream estimator mismatch: The use of PCA to compress high-dimensional LLM/KG embeddings before feeding them to XGBoost or TabPFNv2 may discard useful structure. The paper does not explore alternative integration strategies (e.g., late fusion, attention-based conditioning)."}, "questions": {"value": "- Extend KG experiments to realistic linking scenarios: Include experiments with automatic entity linking (e.g., using BLINK or OpenTapioca) and report performance as a function of linking accuracy. This would bridge the gap between idealized and real-world deployment.  \n\n- Discuss privacy and security implications: Even a brief discussion of risks (e.g., membership inference from KG-enhanced embeddings) would align the work with contemporary concerns in data-centric AI.  \n\n- Explore alternative integration mechanisms: Beyond simple embedding + linear model, consider lightweight adapters or cross-attention modules that preserve the geometry of knowledge-rich representations when used with tabular foundation models.  \n\n- Clarify the role of column context: While row serialization includes column names, the ablation on contextual disambiguation (e.g., “Cambridge, UK” vs. “Cambridge, MA”) is only mentioned qualitatively. A quantitative analysis would strengthen the claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FzE3vT6f4t", "forum": "mKFbXHpOjb", "replyto": "mKFbXHpOjb", "signatures": ["ICLR.cc/2026/Conference/Submission20375/Reviewer_QSCG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20375/Reviewer_QSCG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749103878, "cdate": 1761749103878, "tmdate": 1762933827275, "mdate": 1762933827275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how external knowledge can enhance tabular learning, particularly when tables include textual fields referencing real-world entities. The authors systematically compare two major sources of background knowledge:\n(1) LLMs that implicitly encode extensive factual and semantic information; and\n(2) Knowledge Graphs (KGs) that provide explicit, curated relational structures but depend on entity linking.\nTo enable a controlled comparison, the paper introduces a benchmark of 105 tabular datasets (drawing from TextTabBench, CARTE, and WikiDBs), encompassing both classification and regression tasks. A wide range of representation strategies are evaluated, e.g. non-pretrained encoders, LLM-based embeddings, KG embeddings, and hybrid LLM+KG refinements—paired with several tabular predictors, including ridge regression, XGBoost, and TabPFNv2. The study reveals several key findings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The work addresses an underexplored yet practically important research question—how to inject background knowledge into tabular learning—bridging symbolic reasoning (KGs) and neural representation learning (LLMs).\n- The experiments cover a broad spectrum of models, from lightweight text encoders to multi-billion parameter LLMs, as well as multiple downstream learners. This comprehensive setup strengthens the credibility of the conclusions.\n- The empirical observation that LLMs and KGs converge in performance after entity linking offers an interesting theoretical insight into how implicit and explicit factual knowledge may complement each other."}, "weaknesses": {"value": "- The study mainly explores feature-level embeddings followed by downstream predictors. It omits more advanced fusion methods (e.g., cross-attention, joint training, or representation alignment) that could yield richer interactions between tabular and knowledge-based features.\n- The finding that ridge regression outperforms more complex learners may stem from dimensionality reduction artifacts (e.g., PCA bottlenecks), potentially underestimating the capabilities of non-linear models like XGBoost and TabPFNv2.\n- The paper reports aggregate metrics but does not provide qualitative case studies to illustrate where LLMs or KGs perform particularly well or poorly (e.g., domain-specific tables, rare entities, or ambiguous text).\n- The definition of “refinement” is inconsistent across baselines (ERNIE, Knowledge Card, KGT5). Without further controlled ablations isolating architecture, scale, and pretraining data, it remains unclear what drives the observed gains.\n- Some Wikipedia-derived datasets may resemble document classification rather than genuinely heterogeneous tabular learning, weakening claims about handling tabular structure.\n- The paper could better position itself relative to: 1) Retrieval-Augmented Generation (RAG) approaches that dynamically incorporate knowledge. 2) Multimodal table encoders such as TaBERT, TURL, and TAPAS, which explicitly integrate table structure and text."}, "questions": {"value": "- Could retrieval-based approaches (e.g., RAG-style KG or LLM lookups) outperform static embeddings while retaining interpretability?\n- How does the method behave under noisy or partial entity linking? Could uncertainty in linking be explicitly modeled?\n- Have the author(s) considered multi-column or relational dependencies, such as type hierarchies or foreign-key relationships, beyond row-level concatenation?\n- Would fine-tuning smaller models with knowledge-based pretraining narrow the performance gap with large LLMs?\n- Could benchmarking against retrieval-based tabular models (e.g., RAG-TAB, KnowTab) provide deeper insight into dynamic vs. static knowledge integration?\n- Why does ridge regression outperform TabPFNv2 after embedding projection—does PCA distort representation geometry or reduce model flexibility?\n- Are the performance improvements primarily due to semantic enrichment (better factual knowledge) or dimensional expansion (higher embedding capacity)?\n\n\nI would consider raising my score if the authors can adequately address these questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bpvxWgxOsO", "forum": "mKFbXHpOjb", "replyto": "mKFbXHpOjb", "signatures": ["ICLR.cc/2026/Conference/Submission20375/Reviewer_wUU1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20375/Reviewer_wUU1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921514548, "cdate": 1761921514548, "tmdate": 1762933826858, "mdate": 1762933826858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how external background knowledge can be integrated into tabular learning, focusing on textual columns that reference real-world entities (e.g., drugs, companies, locations).\nThe authors benchmark 105 text-containing tabular datasets and compare representations derived from **large language models (LLMs)**, **knowledge graphs (KGs)**, and **hybrid LLM + KG models**.\nThey find that knowledge-rich representations substantially improve downstream prediction—often more than using sophisticated tabular learners—and that larger LLMs provide greater gains. Refining LLMs on KGs improves parameter efficiency, and in the idealized case where all entities are perfectly linked, KG embeddings perform on par with LLMs.\nThe study concludes that combining LLMs and KGs is a promising direction for future tabular foundation models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper tackles a novel and meaningful problem—bridging tabular learning and external knowledge.\n* Comprehensive empirical study across 105 datasets, with diverse textual attributes.\n* Systematic comparison of LLM, KG, and hybrid models; clear identification of the entity-linking bottleneck.\n* Results suggest interesting scaling trends and show that “representation quality > model complexity” in importance."}, "weaknesses": {"value": "* **Over-restrictive assumptions:** removing all numerical features and focusing solely on text columns creates an artificial setting; results may not generalize to realistic multi-modal tables.\n* **Limited real-world relevance:** experiments are confined to small-data regimes (64 / 256 / 1024 samples), which are uncommon in industrial tabular tasks.\n* **Lack of raw quantitative results:** only normalized gains are reported; absolute AUC/R² improvements may be modest.\n* **No discussion of KG construction or cost:** while KG embeddings are used, the paper does not analyze the effort required for entity linking or graph maintenance, undermining claims of practical benefit.\n* **Reproducibility issues:** key configuration details (exact sampling splits, variance across seeds, hyper-parameters for embedding extraction) are only briefly mentioned."}, "questions": {"value": "1. Since the same test sets are used across training sizes {64, 256, 1024}, how does performance evolve with more training samples? Do the relative advantages of LLM / KG embeddings diminish as data grows?\n2. Could the authors release or at least summarize the **raw experimental tables** (AUC/R² per dataset) to enhance reproducibility and allow independent meta-analysis?\n3. A valuable extension would be to re-introduce **numerical features** and study how numerical and textual features interact—are their contributions orthogonal or redundant? This would make the findings more applicable to real-world tabular pipelines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4phmuzevZA", "forum": "mKFbXHpOjb", "replyto": "mKFbXHpOjb", "signatures": ["ICLR.cc/2026/Conference/Submission20375/Reviewer_PSi8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20375/Reviewer_PSi8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002883678, "cdate": 1762002883678, "tmdate": 1762933826356, "mdate": 1762933826356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}