{"id": "tP01GCr0nS", "number": 3775, "cdate": 1757517497398, "mdate": 1759898070348, "content": {"title": "Why Reinforcement Fine-Tuning Enables MLLMs Preserve Prior Knowledge Better: A Data Perspective", "abstract": "Post-training algorithms such as Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large language models to downstream tasks. While effective at task adaptation, their impact on prior knowledge remains unclear. In this paper, we introduce jigsaw puzzles as a novel task absent from existing pretraining corpora and systematically study the behavior of SFT and RFT on open-source multimodal model, Qwen2.5-VL series. Our experiments reveal a sharp trade-off: SFT enables rapid task acquisition but leads to catastrophic forgetting, whereas RFT learns more slowly but maintains prior knowledge. We study this phenomenon through learning dynamics by examining both the magnitude and direction of how training data influence prior knowledge. Our analysis shows that RFT mainly reinforces correct samples naturally aligned with the base model’s probability landscape, leading to weaker interference with prior knowledge. Moreover, training on RFT-simulated rollouts, which exert a small magnitude of influence and are well aligned in direction to prior knowledge, allows SFT to preserve prior knowledge better while rapidly learning new tasks. These findings suggest that distribution of training data, rather than algorithmic differences, plays a central role in forgetting, and highlight RFT's potential for stable continual learning in multimodal large language models.", "tldr": "", "keywords": ["Reinforcement Fine-Tuning", "Catastrophic Forgetting", "Data Distribution", "Learning Dynamics"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/31bdcde77fc627e70b609c69a1b23023ace67cdf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the differences between Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) regarding catastrophic forgetting. The authors find that RFT learns new knowledge more slowly but significantly reduces catastrophic forgetting, while SFT learns faster but forgets old knowledge more quickly. To study this, the paper introduces a jigsaw puzzle task, which is novel to existing models. The authors analyze how RFT and SFT perform on this task and conclude as above. They also find that using reasoning trajectories generated by the fine-tuned model itself (e.g., Qwen2.5-VL) for SFT—rather than those generated by other models like GPT-4o—greatly mitigates catastrophic forgetting, while also retaining higher training efficiency than RFT. This leads to a balanced approach between learning speed and knowledge retention."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a comprehensive comparison between RFT and SFT in both learning efficiency and resistance to catastrophic forgetting, yielding valuable insights.\n- It proposes a method that uses model-generated reasoning trajectories for SFT, achieving both faster learning and stronger retention of prior knowledge."}, "weaknesses": {"value": "- All conclusions are drawn from experiments on the jigsaw puzzle task using only the Qwen2.5-VL family. This setting is rather narrow and makes it difficult to claim general applicability. Therefore, the current experimental scope may not fully guarantee that the conclusions hold across other models or tasks.\n- Although Rea-GRPO-Rollout converges faster than standard RFT during training, it requires an additional stage of RFT training and reasoning trajectory generation beforehand. Hence, the overall time cost (RFT training + data preparation + SFT) is likely higher, which may reduce its practical appeal.\n- The poor performance of SFT-Non-Rea might partly stem from the overly uniform format of its answers, rather than SFT itself. If the training data included more diverse question and answer styles, the extent of catastrophic forgetting might be reduced."}, "questions": {"value": "- Have the authors experimented with other models or tasks beyond jigsaw puzzles? Even partial results on different setups could strengthen the generality and credibility of the conclusions.\n- Have the authors examined whether using SFT data with more stylistic diversity (e.g., mixing reasoning-based, multiple-choice, direct-answer, or natural-language descriptions of the answer) could also alleviate catastrophic forgetting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SsBCJ9D0yf", "forum": "tP01GCr0nS", "replyto": "tP01GCr0nS", "signatures": ["ICLR.cc/2026/Conference/Submission3775/Reviewer_DcMQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3775/Reviewer_DcMQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761481315741, "cdate": 1761481315741, "tmdate": 1762916982287, "mdate": 1762916982287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically investigate the impact of supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT) on prior knowledge retention in multimodal large language models (MLLMS) by introducing a jigsaw puzzle task as a novel learning scenario.    This paper present an interesting and important finding: RFT outperforms SFT in avoiding catastrophic forgetting, and further provide a theoretical explanation in terms of data distribution and learning dynamics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1、This paper explicitly address a problem that is crucial at the intersection of Continual Learning (CL) and Reinforcement Learning (RL) : how to avoid catastrophic forgetting when adapting to a new task.\n2、Based on the theoretical analysis of Learning Dynamics and eNTK, this paper explain the impact of different data distributions on forgetting from two dimensions of magnitude and direction, which provides a new perspective for understanding the advantages of RFT."}, "weaknesses": {"value": "1. Although the paper studies \"catastrophic forgetting\", the discussion of existing continuous learning (CL) methods is relatively superficial, and the classical CL methods (e.g., EWC, LwF, ER, etc.) are not fully compared.\n2. While the authors note that resource constraints prevented experimentation with more multimodal models and large language models, experimenting on more methods is key to improving the contribution of this work.\n3. The learning dynamic analysis formulation in Section 5.4 is dense and not friendly enough for readers with a non-theoretical background.\n4. The calculation method of A_(i,t) is not defined in formula (1)."}, "questions": {"value": "1. RFT requires ~ 27k training steps while SFT only takes a few hundred, which may lead to an unfair comparison (different amount of training)?\n2. The paper assumes that eNTK remains stable during training, but it is questionable whether this holds in MLLM fine-tuning?\n3. Is the generation prompt of the \"Rea-4o-Rollout\" data in Section 4 consistent with the inference process generated by RFT?  If not, does it affect the fairness of the comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Hp2CxJXL5i", "forum": "tP01GCr0nS", "replyto": "tP01GCr0nS", "signatures": ["ICLR.cc/2026/Conference/Submission3775/Reviewer_sRUL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3775/Reviewer_sRUL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761486866753, "cdate": 1761486866753, "tmdate": 1762916982019, "mdate": 1762916982019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) affect prior knowledge retention in Multimodal Large Language Models (MLLMs), using jigsaw puzzles as a novel task. It finds that SFT causes severe catastrophic forgetting despite rapid task learning, whereas RFT learns more slowly but preserves prior knowledge effectively. The key insight is that the data distribution, not the algorithm itself, drives forgetting; RFT naturally generates low-perplexity, model-aligned training samples that interfere less with existing knowledge."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The forgetting is less expored in multimodal setting.\n2. Provide theorectical analysis."}, "weaknesses": {"value": "1. Limited Novelty of Core Insight\nGiven that [1] has already demonstrated that reinforcement learning can effectively mitigate catastrophic forgetting, the contribution of this work appears limited. In fact, [1] also conducted experiments showing that it is the on-policy training paradigm (e.g., RFT) that helps overcome catastrophic forgetting. The key distinction between the data used in SFT and RFT lies in whether they are on-policy or offline. Beyond the discussion in L153–L157, the authors should provide a more detailed comparison between their study and [1]. To me, this paper merely provides a possible explanation for why using on-policy data can help overcome catastrophic forgetting, rather than being the one that discovers this phenomenon.\n- In L55, the paper claims that RFT can “discover novel and useful problem-solving strategies.” However, I find it difficult to draw this conclusion from the preceding discussion about RFT, which merely states that “RFT requires several tens of thousands of training steps to successfully solve jigsaw puzzles.”\n- The paper lacks a detailed description of the construction process of the jigsaw puzzle task, such as the method used to shuffle the image tiles, and it also does not report the detailed training cost of the experiments. This omission makes it difficult for other researchers to reproduce the results.\n\n[1] Rl’s razor: Why online reinforcement learning forgets less\n\n2. Unjustified Focus on Multimodality\nWhile the study is conducted using multimodal models (MLLMs), the analysis and conclusions are not inherently tied to multimodal reasoning. The mechanisms explored—such as data distribution and perplexity—are equally applicable to unimodal language models, raising questions about the necessity of the multimodal framing.\n\n3. Opaque Theoretical Framework\nThe theoretical analysis, based on learning dynamics and neural tangent kernels, is presented with limited intuitive explanation or clear connection to the empirical findings. This obscures how the theory substantiates the main claims and reduces its accessibility to a broader audience."}, "questions": {"value": "- In eq 1, why the author set $\\pi_{\\theta_{\\mathrm{old}}}(\\cdot)=\\pi_\\theta(\\cdot)$, instead of using the standard GRPO recipe? Can you provide the experiment result using the standard GRPO recipe?\n- In the rationale shown in Figure 7, I noticed that the model’s reasoning process differs from how humans typically solve a jigsaw puzzle. As a human, I do begin by identifying the overall scene depicted in the image—similar to the reasoning shown in Figure 7—by observing the misaligned image tiles. However, my step-by-step reasoning primarily involves progressively determining the position of each tile. Once a tile’s position is confirmed, humans usually infer the positions of adjacent tiles based on the edge pixels of the confirmed piece. In contrast, the rationale presented in Figure 7 does not capture this gradual confirmation process. Please analyze the underlying reason for this discrepancy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U7OPVXOQYi", "forum": "tP01GCr0nS", "replyto": "tP01GCr0nS", "signatures": ["ICLR.cc/2026/Conference/Submission3775/Reviewer_Ssap"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3775/Reviewer_Ssap"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761661321838, "cdate": 1761661321838, "tmdate": 1762916981813, "mdate": 1762916981813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}