{"id": "0kHbD6ad07", "number": 16613, "cdate": 1758266731499, "mdate": 1759897229584, "content": {"title": "Language Models are Injective and Hence Invertible", "abstract": "Transformer components such as non-linear activations and normalization are inherently non-injective, suggesting that different inputs could map to the same output and prevent exact recovery of the input from a model’s representations. In this paper, we challenge this view. First, we prove mathematically that transformer language models mapping discrete input sequences to their corresponding sequence of continuous representations are injective and therefore lossless, a property established at initialization and preserved during training. Second, we confirm this result empirically through billions of collision tests on six state-of-the-art language models, and observe no collisions. Third, we operationalize injectivity: we introduce SipIt, the first algorithm that provably and efficiently reconstructs the exact input text from hidden activations, establishing linear-time guarantees and demonstrating exact invertibility in practice. Overall, our work establishes injectivity as a fundamental and exploitable property of language models, with direct implications for transparency, interpretability, and safe deployment.", "tldr": "We prove that transformers are (a.s.) injective and propose an algorithm that provably inverts their hidden representations back to the original input prompt.", "keywords": ["transformers", "language models", "invertibility", "injectivity", "inversion", "privacy"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5fdd6814f17bf1027b4e89194d5d749badb016f6.pdf", "supplementary_material": "/attachment/809cf90fa54e139cbd86c77a915e303504c8bf60.zip"}, "replies": [{"content": {"summary": {"value": "This is a primarily theory paper that discusses decoder only transformers being injective. Specifically that each unique prompt maps to a unique last token embedding (from final transformer layer). Paper provides theoretical setup and justification for this argument under acceptable assumptions at initialization (Theorem 2.2) and post SGD-like gradient based training (Theorem 2.3). It is to be noted, that the theorems state how injectivity is very high probability (1 or near 1) event under stated assumption and not proven. For eg., paper discusses how collisions (two different input mapping to same last token representation) can still occur if an adversarial crafts data specifically so (end of Section 2). \n\nPaper also introduces a prompt recovery method from last token representation. The method \"Sequential Inverse Prompt via ITerative updates\" or SIP-IT. This method says that at each index position given a current + previous token hidden representation, check each token in vocab to see which will produce current index hidden representation (formal algorithm describe in Algorithm 1). The paper further discusses computational cost of SIP IT.\n\nReview Note: Since the primary results are theoretical, I reduce my confidence to 3 in the reading and review."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This is a unique and very science paper on LLMs, something we rarely see. Thanks for working on this. (I have what takeaways from this concern but more on that below).\n- It cites most related work I know and use proper baselines in experiments."}, "weaknesses": {"value": "The paper seems poorly organized, I felt lot of important results were pushed into appendix and main paper felt very repetitive at times. I have listed my main qualms in the questions section."}, "questions": {"value": "1. What is \"measure-zero parameter choices\"? \n1. If you consider the prompt length progressively growing (i.e. --> \\inf) with no upper bound, but the model width (i.e. embedding or hidden representation size) is fixed, then the model's representation has to be overloaded to some extent. This would challenge the injectivity directly since it won't be lossless anymore. So I would argue, in the limit, since the vector dim of hidden dim is capped, transformers are not injective and infact more and more lossy. What do the authors think?\n1. Since the language follows power law of Zipf distribution [1], the words occurs as per an exponentially decaying frequency. One example, let us have LLM predict the next token on \"Complete this: The quick brown fox jumps over the lazy [dog]\" where \"dog\" is not part of prompt. If this exact phrase is used use with two distinct prompts (i.e. prefixes of this phrase), the next token would always be \"dog\" for most useful LLMs. Can you please run this as experiment with varied internet data used as prefix? If the next token prediction is same, I expect the last token to be super close if not identical and see if it meets the threshold of 1e-6 used in paper. There can be other examples as well, like cases where \"should\" is followed by \"have\" (assuming \"should have\" token is not part of vocab) with vastly different prompts. This is more inline with Zipfian distribution argument.\n1. Why threshold of 1e-6?\n1. Can you explain what exactly is the BRUTEFORCE method in Table 2?\n1. \"distinct prompts produce distinct hidden states under standard initialization and training\" is good summary of the paper.\n1. I think that under high precision, lookup of hidden activation against data is already expected. This steps from how deterministic models are. Can you clarify, under your threat model, the extractor of data have ONLY the last token embedding/representation with none of the prompt (and no knowledge of it's length either) or its access to each of previous token's embedding/representation as well? Because latter is kind of trivial (to me at least) and former is very difficult and the experiments might be convincing in that case. So yeah, the exact threat model is unclear to me, and kind of not stated clearly enough in the paper.\n\n[1] https://en.wikipedia.org/wiki/Zipf%27s_law"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RBlejdM4WN", "forum": "0kHbD6ad07", "replyto": "0kHbD6ad07", "signatures": ["ICLR.cc/2026/Conference/Submission16613/Reviewer_fDrk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16613/Reviewer_fDrk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536780615, "cdate": 1761536780615, "tmdate": 1762926683817, "mdate": 1762926683817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is a primarily theory paper that discusses decoder only transformers being injective. Specifically that each unique prompt maps to a unique last token embedding (from final transformer layer). Paper provides theoretical setup and justification for this argument under acceptable assumptions at initialization (Theorem 2.2) and post SGD-like gradient based training (Theorem 2.3). It is to be noted, that the theorems state how injectivity is very high probability (1 or near 1) event under stated assumption and not proven. For eg., paper discusses how collisions (two different input mapping to same last token representation) can still occur if an adversarial crafts data specifically so (end of Section 2). \n\nPaper also introduces a prompt recovery method from last token representation. The method \"Sequential Inverse Prompt via ITerative updates\" or SIP-IT. This method says that at each index position given a current + previous token hidden representation, check each token in vocab to see which will produce current index hidden representation (formal algorithm describe in Algorithm 1). The paper further discusses computational cost of SIP IT.\n\nReview Note: Since the primary results are theoretical, I reduce my confidence to 3 in the reading and review."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This is a unique and very science paper on LLMs, something we rarely see. Thanks for working on this. (I have what takeaways from this concern but more on that below).\n- It cites most related work I know and use proper baselines in experiments."}, "weaknesses": {"value": "The paper seems poorly organized, I felt lot of important results were pushed into appendix and main paper felt very repetitive at times. I have listed my main qualms in the questions section."}, "questions": {"value": "1. What is \"measure-zero parameter choices\"? \n1. If you consider the prompt length progressively growing (i.e. --> \\inf) with no upper bound, but the model width (i.e. embedding or hidden representation size) is fixed, then the model's representation has to be overloaded to some extent. This would challenge the injectivity directly since it won't be lossless anymore. So I would argue, in the limit, since the vector dim of hidden dim is capped, transformers are not injective and infact more and more lossy. What do the authors think?\n1. Since the language follows power law of Zipf distribution [1], the words occurs as per an exponentially decaying frequency. One example, let us have LLM predict the next token on \"Complete this: The quick brown fox jumps over the lazy [dog]\" where \"dog\" is not part of prompt. If this exact phrase is used use with two distinct prompts (i.e. prefixes of this phrase), the next token would always be \"dog\" for most useful LLMs. Can you please run this as experiment with varied internet data used as prefix? If the next token prediction is same, I expect the last token to be super close if not identical and see if it meets the threshold of 1e-6 used in paper. There can be other examples as well, like cases where \"should\" is followed by \"have\" (assuming \"should have\" token is not part of vocab) with vastly different prompts. This is more inline with Zipfian distribution argument.\n1. Why threshold of 1e-6?\n1. Can you explain what exactly is the BRUTEFORCE method in Table 2?\n1. \"distinct prompts produce distinct hidden states under standard initialization and training\" is the argument of the paper I buy. But this doesn't imply injectivity in any meaningful way. Just that models -> last hidden state is a deterministic mapping -- which it is (ignoring GPU non-determinism). Can authors explain why/how injectivity is meaningful?\n1. I think that under high precision, lookup of hidden activation against data is already expected. This steps from how deterministic models are. Can you clarify, under your threat model, the extractor of data have ONLY the last token embedding/representation with none of the prompt (and no knowledge of it's length either) or its access to each of previous token's embedding/representation as well? Because latter is kind of trivial (to me at least) and former is very difficult and the experiments might be convincing in that case. So yeah, the exact threat model is unclear to me, and kind of not stated clearly enough in the paper.\n\n[1] https://en.wikipedia.org/wiki/Zipf%27s_law"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RBlejdM4WN", "forum": "0kHbD6ad07", "replyto": "0kHbD6ad07", "signatures": ["ICLR.cc/2026/Conference/Submission16613/Reviewer_fDrk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16613/Reviewer_fDrk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536780615, "cdate": 1761536780615, "tmdate": 1763053253308, "mdate": 1763053253308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors claim that a common thought is that language models are have a many to one relationship with regard to input and latent representation (respectively). The authors argue that language models are injective and that this suggests that there is an invertible relationship such that given a latent state, the input provided to arrive at this state is recoverable. \n\nThe authors base the analysis on the belief that initialized transformers don't typically create collisions between prompts and that after a fixed number of training updates, the parameters have not been changed sufficiently to induce collisions. \n\nThe authors propose a method to obtain the input to a language model. However, the method requires knowledge of the total last layer hidden state of the model for every substring of the prompt whose first token is the first token in the prompt."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "This paper presents an interesting analysis I have not seen of the transformer architecture and convinced me that the transformer is often real analytic (though that depends on the choice of activation functions)."}, "weaknesses": {"value": "Terms like measure-zero and injective need to be defined. \n\nA collision must be defined. It's not sufficient to say that a collision didn't occur because a value differed in any way. Some bound must be placed on the distance between the output representations that constitutes a collision. \n\nMany activation functions are not real analytic functions. \n\nMy understanding is that the zero set necessarily is measure-zero (given that the model is real analytic and we know the model is not the zero function). This doesn't seem to necessarily imply that no two inputs to the function will provide a similar output. I believe the authors intend for the reader to understand that the difference of two activations given dissimilar prompts is itself a real analytic function which then suggests that the difference between any two prompts whose value is zero is a measure-zero set. The problem with this logic is that it is possible that the difference between the last stage activations for two prompts is either the zero function or epsilon close to the zero function for some small epsilon without violating anything in the proof. \n\nMy primary contention is that showing that the zero set is measure-zero is not sufficient to make the strong claim that there are no collisions. A collision should reasonably be considered anything within some small distance of zero rather than those items which have identical values given the reality of executing floating point operations a real machine.   \n\nThe SipIT method is trivial - given one knows the latent representation of every substring (which is less probable than simply knowing the input text to begin with), the algorithm can produce the substring by evaluating every possible token in this location. Further, the analysis of the algorithm seems to be either obscured or incorrect. For a prompt of length N and vocabulary of k, the algorithm will in the worst case require k^N time. This is exponential where the paper claims a linear time guarantee. Perhaps the paper meant linear with respect to vocabulary? But this is meaningless since the vocabulary is stationary and the input length is the changing factor."}, "questions": {"value": "Decoder-only models are claimed to be a lossless representations of their input. This seems to be in contradiction to existing results that transformers are universal function approximators - a universal function approximation ought to be able to learn a many to one mapping if appropriate. However, function approximation is based on an epsilon bound. This seems to suggest that if instead of examining the size of the zero set, one were to examine the set which is epsilon close to the zero set, this would have no theoretic guarantee of being measure-zero. How does this not eliminate the practicality of the paper's analysis?\n\nWhat degree of fidelity is necessary for this result to hold? If lower precision approximations are used, online batching for practical serving, or significant context information is present, these will create random noise which ought to obscure any such precise mapping as is necessary for the analysis to hold."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N3esymTKtb", "forum": "0kHbD6ad07", "replyto": "0kHbD6ad07", "signatures": ["ICLR.cc/2026/Conference/Submission16613/Reviewer_yqXp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16613/Reviewer_yqXp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844453139, "cdate": 1761844453139, "tmdate": 1762926683420, "mdate": 1762926683420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a strong theoretical study showing that decoder-only Transformer architectures are (almost surely) injective: different prompts map to distinct last-token hidden states. Building on this, the authors introduce SIPIT, an algorithm that recovers the exact input prompt from the final hidden state. The method is theory-driven and, across extensive experiments on a wide variety of models, the authors find no collisions, providing compelling empirical support for injectivity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main strength of the paper are as follows:\n\n1. This paper makes a significant theoretical contribution by proving that standard decoder-only Transformer language models are almost surely injective – different input prompts will (with probability one) produce distinct hidden representations. The authors use real analysis to show that collisions (two different prompts yielding the same last-token state).\n2. Building on the injectivity result, the novel algorithm (SipIt) to recover the exact input text from a model’s hidden activations with provable efficiency is introduced. As prior approaches could only approximate prompts via heavy training of inversion models, whereas SipIt achieves exact recovery by directly using the model's own representations.\n3. The paper backs up its theory with comprehensive experiments on two models (a kind of old GPT-2 and more novel Gemma). The authors performed an exhaustive search for representation collisions using 100k prompts drawn from diverse sources, amounting to billions of pairwise comparisons. They report no collisions in any model or layer tested, distinct prompts always yielded distinct last-token embeddings, with clear separation margins.\n4. The SipIt algorithm is shown to be not just theoretically sound but practically effective. On GPT-2, SipIt was able to reconstruct 20-token prompts perfectly (100% token-wise accuracy) in reasonable time, without any additional training or approximation.\n5. By establishing invertibility as a fundamental property, the work has broad implications. It provides a sound basis for interpretability: knowing that the full input is encoded in the last-layer state means any failure to probe knowledge is due to method limits, not information loss."}, "weaknesses": {"value": "I would highlight the following weaknesses of this paper:\n\n1. Large vocabulary scaling. How does SipIt handle very large vocabularies (e.g., 100k+ tokens)? Does runtime grow linearly in practice, or do the gradient-based heuristics keep it manageable?\n2. Uncertain theoretical result of not-analytic estimation. Most modern models use SwiGLU or SiLU activations. Since your proofs assume analytic activations, can you confirm that these fit the theory?\n3. In continuation to the previous point, it is not clear, what happens with the quantized models. It seems that it can be the main source of the collision.\n4. The experiments are provided for models of relatively small size, thus, we cannot asses what empirically happens with the larger number of parameters."}, "questions": {"value": "My questions to the authors are as following:\n\n1. Have you estimated, how SipIt works on datasets that were seen by the models during training, and on OOD samples, that were not observed by the model. Or even some random sequences of tokens? Does inversion speed or accuracy change compared to natural text?\n2. Instruction-tuned models with identical answers: For instruction models (or even pre-trained ones) where many prompts lead to the same answer (e.g., \"yes\" or \"no\"), do the hidden states remain well separated? Have you measured how close they get?\n3. Did you ever find prompts whose hidden states were almost identical? If so, what kind of prompts were they?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NEQrw2fPom", "forum": "0kHbD6ad07", "replyto": "0kHbD6ad07", "signatures": ["ICLR.cc/2026/Conference/Submission16613/Reviewer_uusJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16613/Reviewer_uusJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991887519, "cdate": 1761991887519, "tmdate": 1762926683042, "mdate": 1762926683042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors make a significant theoretical and practical contribution by rigorously establishing that standard decoder-only transformer language models are almost surely injective - meaning different input prompts map to distinct last-token hidden representations under common initialization and training regimes. The authors leverage real-analyticity to prove that collisions (non-injective behavior) occur only on a measure-zero set of parameters, and they further demonstrate that gradient descent preserves this injectivity. Building on this foundation, the paper introduces SipIt, a novel algorithm that efficiently reconstructs the exact input prompt from hidden activations with provable linear-time guarantees. This work bridges theory and practice, offering new insights into model transparency, interpretability, and safety."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The following strong points can be highlighted:\n1. The theoretical analysis is rigorous, leveraging real-analyticity and measure theory to derive almost-sure guarantees.\n2. The injectivity results are novel and counter widespread assumptions about information loss in Transformers.\n3. SipIt is both theoretically grounded and empirically validated, demonstrating exact recovery with linear-time complexity.\n4. Extensive experiments across multiple models (e.g., GPT-2, Gemma, Llama) confirm the absence of collisions in practice.\n5. The paper clearly discusses implications for privacy, interpretability, and model auditing.\n\nThe paper’s core contribution is proving injectivity and enabling exact inversion, which addresses foundational questions in deep learning and has immediate relevance to interpretability and safety. The proofs are meticulous, and the experiments are comprehensive, spanning multiple model families and scales. The introduction of SipIt provides a tangible tool for future research, while the theoretical guarantees are robust and well-supported. These qualities align with ICLR’s emphasis on impactful, rigorously evaluated work."}, "weaknesses": {"value": "Whereas the paper is technically solid, there are several weak points I would like to mention:\n1. The scope is limited to decoder-only Transformers with analytic components, excluding architectures with non-analytic activations (e.g., ReLU) or encoder-decoder models.\n2. The practical utility of SipIt, while theoretically appealing, is primarily evaluated in a noiseless setting; its robustness to quantization or approximate hidden states is less explored.\n3. The discussion of related work, while adequate, could better contextualize how these results complement or challenge prior beliefs about Transformer expressivity.\n4. Some proofs in the appendix are highly technical and may be inaccessible to readers without deep mathematical backgrounds.\n5. It will be very interesting to analyze injectivity correlation with other internal transformer characteristics like anisotropy and intrinsic feature dimension using such frameworks as LLM-Microscope (https://arxiv.org/abs/2502.15007, https://github.com/AIRI-Institute/LLM-Microscope)"}, "questions": {"value": "1. How does SipIt perform under noisy or quantized hidden states, and can the theory be extended to account for such perturbations?\n2. Could the injectivity results generalize to encoder-decoder Transformers or models with non-analytic components (e.g., ReLU)?\n3. The paper claims gradient descent preserves injectivity - does this hold for adaptive optimizers like Adam, or only for GD?\n4. Are there practical scenarios where the linear-time complexity of SipIt becomes prohibitive, e.g., for very large vocabularies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5qmwowMJ1M", "forum": "0kHbD6ad07", "replyto": "0kHbD6ad07", "signatures": ["ICLR.cc/2026/Conference/Submission16613/Reviewer_D7YR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16613/Reviewer_D7YR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762195808370, "cdate": 1762195808370, "tmdate": 1762926682638, "mdate": 1762926682638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Answer"}, "comment": {"value": "We sincerely thank all the reviewers for their time dedicated to reviewing this paper. We recognize the technical nature of the work made this a challenging task, and we truly appreciate their effort and thoughtful feedback. We are pleased that the paper has been recognized for its rigorous theoretical contributions (`uusJ`, `D7YR`, `fDrk`), presenting novel and well-supported injectivity results with strong empirical validation (`uusJ`, `D7YR`), demonstrating a practical and efficient inversion method through SIPIT (`uusJ`, `D7YR`) supported by comprehensive experiments and appropriate baselines (`uusJ`, `D7YR`, `fDrk`), and standing out as a distinctive and deeply scientific contribution to LLM research (`fDrk`, `yqXp`), “something we rarely see” (`fDrk`).\n\nWe invite each reviewer to consult our individual responses, where we address all points raised in detail. In this general reply, we provide a list of all the **additional** **experiments performed in light of your feedback**. These experiments fall into two categories:\n\n**Collision Experiments:**\n\n1. 4-bit and 8-bit quantized models (Reviewers: `uusJ`,  `yqXp`)\n2. Large models, up to 70 billion parameters (Reviewers: `uusJ`)\n3. Instruction-tuned Models (Reviewers: `uusJ`)\n4. Exact same next-token (Reviewers: `uusJ`, `fDrk`) \n5. Correlation between minimum margin ($\\varepsilon$-collisions) and anisotropy / intrinsic dimension (Reviewer: `D7YR`)\n\n**SIPIT Experiments:**\n\n1. Vocabulary scaling ablation (Reviewers: `D7YR`, `uusJ`)\n2. 4-bit quantized models (Reviewer: `D7YR`, `yqXp`)\n3. In-distribution vs Out-of-distribution inversion performance ablation  (Reviewer: `uusJ`)\n\nGiven the extensive additional experiments we ran and the detailed theoretical clarifications we crafted to address the reviewers’ questions, we will now incorporate all of these results and discussions into an updated version of the paper in the coming days."}}, "id": "whMx154neP", "forum": "0kHbD6ad07", "replyto": "0kHbD6ad07", "signatures": ["ICLR.cc/2026/Conference/Submission16613/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16613/Authors"], "number": 18, "invitations": ["ICLR.cc/2026/Conference/Submission16613/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763723286649, "cdate": 1763723286649, "tmdate": 1763725334338, "mdate": 1763725334338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}