{"id": "sVeI9ZHyNN", "number": 12431, "cdate": 1758207752750, "mdate": 1759897510290, "content": {"title": "WebCIR: Bringing New Web Culture Concepts to Compositional Image Retrieval", "abstract": "This paper proposes an attention-editing-based network knowledge infusion method aimed at enhancing the comprehension and utilization of complex web-scale knowledge in Compositional Image Retrieval (CIR) models. Addressing the limitations of conventional multimodal models in processing massive web knowledge, this study develops an innovative attention-guided knowledge infusion framework through the construction of a structured knowledge-enhanced dataset. The proposed method achieves progressive transmission of web knowledge from coarse to fine granularity via a carefully designed prompt localization system and a hierarchically controlled masking mechanism. Specifically, structured prompt templates encode web knowledge into learnable semantic units, while dynamic attention editing governs the knowledge injection process, enabling the model to adaptively filter and integrate heterogeneous multi-source web knowledge. Experimental results demonstrate that this approach not only significantly improves the model's efficiency in capturing implicit web knowledge but also effectively mitigates knowledge conflicts and redundancy issues. Our work establishes a new technical paradigm for knowledge distillation and transfer in multimodal retrieval systems.", "tldr": "This paper introduces an attention-editing framework for Compositional Image Retrieval that efficiently integrates web-scale knowledge through structured prompting and dynamic masking.", "keywords": ["Multimodal Models", "Compositional Image Retrieval", "Attention Editing", "Web-Scale Retrieval", "Hierarchical Masking", "Prompt Localization"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5becf8fdc92f991941a0a4a93a7fc0bf4ed96e0c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces WebCIR, a new dataset for Compositional Image Retrieval (CIR) focused on internet culture semantics, generated through distillation from a large language model (GPT-40). Concurrently, the authors propose an attention-editing framework designed to infuse this web-scale knowledge into CIR models. The method's effectiveness is evaluated on established benchmarks like CIRR and FashionIQ, as well as a new synthetic test set, POKEMON."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Resource Contribution: The work contributes a new large-scale dataset, WebCIR, which could potentially stimulate further research into understanding complex, non-literal semantics in the CIR domain."}, "weaknesses": {"value": "* Circular and Unconvincing Evaluation: The paper's central claim of improved generalization is not well-supported. The model is trained on WebCIR, a dataset synthetically generated by GPT-40, and its strongest out-of-distribution performance is reported on the POKEMON benchmark , which was also synthetically generated by another large model (Qwen2-VL-72B-Instruct). This evaluation is circular; it primarily shows that a model trained on LLM-generated data excels on other LLM-generated data, rather than proving robust generalization to real-world, human-generated web content.\n* Weak Performance on Human-Curated Benchmarks: The proposed method fails to demonstrate clear superiority on established, real-world datasets. On FASHIONIQ, the authors admit the performance is merely \"comparable,\" with their model even trailing a baseline (MCL) in the Recall@5 metric. This lack of a convincing win on a standard benchmark significantly undermines the claimed effectiveness and superiority of the framework.\n* Methodological Clarity: The proposed attention-editing mechanism is presented with a series of complex formulations (semantic gating, context enhancement, cross-unit suppression) that lack clear intuition and justification. It is difficult for the reader to understand why this specific, complex design is necessary or superior to simpler alternatives. The ablation study confirms its components are useful but does not sufficiently justify the design choices themselves."}, "questions": {"value": "1. How does demonstrating strong performance on the synthetic POKEMON dataset validate the model's ability to generalize to genuine, human-generated internet culture, as opposed to simply showing an aptitude for the patterns inherent in LLM-generated data?\n2. Given that the core motivation is to handle complex web semantics, why does the proposed method not clearly outperform existing models on the FASHIONIQ benchmark, which involves nuanced and subjective real-world semantics?\n3. The authors should provide a more intuitive explanation for the design of the triple-level representation transformation in the attention-editing module. What is the core hypothesis behind this specific formulation over other potential knowledge infusion techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZsIM0Iinur", "forum": "sVeI9ZHyNN", "replyto": "sVeI9ZHyNN", "signatures": ["ICLR.cc/2026/Conference/Submission12431/Reviewer_P4pB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12431/Reviewer_P4pB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760538347317, "cdate": 1760538347317, "tmdate": 1762923319487, "mdate": 1762923319487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a challenge in Compositional Image Retrieval (CIR): the inability of existing models to understand and process complex, non-literal, and culturally contextualized queries common on the internet (e.g., \"make it look like a 90s anime screenshot\"). Experiments on standard benchmarks and a POKEMON demonstrate competitive performance and generalization ability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Holistic Contribution: The work is commendable for its comprehensive approach, contributing both a WebCIR and a novel model, rather than focusing on just one aspect.\n\n- Clear methodological explanation: The technical description of the attention-editing mechanism and the triple-level representation transformation is detailed and well-supported with formulas, making the approach replicable in principle."}, "weaknesses": {"value": "- Lack of novelty: The Proposed method only consists of existing components, such as ViT, causal transformer, attention masks in an other way, which is quite incremental.\n\n- Performance on FashionIQ: The results on FashionIQ are mixed. While leading in Recall@1, the model trails MCL significantly in Recall@5. The authors correctly identify this as a limitation, but the explanation (\"inherent ambiguity,\" \"subjective fashion semantics\") is somewhat hand-wavy. A deeper analysis of why their web-culture-focused approach doesn't translate as well to this domain is needed.\n\n- Computational Cost omission: The paper claims \"superior computational efficiency\" but provides no concrete data (e.g., FLOPs, training/inference time, parameter counts beyond the backbone) to support this claim. Compared to a 20B parameter model (GME), this is plausible, but a comparison of training costs and inference latency with the other baselines (Pic2Word, MCL) is missing.\n\n- Clarity on Knowledge infusion: The term \"knowledge infusion\" is used throughout, but the mechanism is primarily the curated WebCIR dataset and the attention-editing module. It's not entirely clear if any external knowledge base beyond the generated (image, text, modified text) triplets is being used. The process seems more like \"knowledge-aware training\" rather than dynamic infusion from an external source.\n\n- Ablation on Dataset Size: The ablation study shows that using the full Flickr-32k dataset is better than smaller subsets, but it does not include an ablation where the model is trained without the WebCIR data (i.e., only on standard data). This makes it difficult to disentangle the contribution of the novel dataset from the contribution of the novel architecture."}, "questions": {"value": "As mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dQckfbmVqI", "forum": "sVeI9ZHyNN", "replyto": "sVeI9ZHyNN", "signatures": ["ICLR.cc/2026/Conference/Submission12431/Reviewer_H3b8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12431/Reviewer_H3b8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760768858691, "cdate": 1760768858691, "tmdate": 1762923318989, "mdate": 1762923318989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors target the problem of the comprehension and utilization of complex webscale knowledge in  compositional image retrieval(CIR) models. They develop an attention-guided knowledge infusion framework to better improve the models' efficiency in capturing complex web knowledge, which improves models' performaces on CIR tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem is important and interesting, I appreciate the authors efforts in this problem.\n2. The framework is well designed and integrates both the dataset generation and model-level improvements.\n3. The experimental results reveal that the designed framework is efficient and effective in solving CIR tasks."}, "weaknesses": {"value": "1. The writing is dense and not clear enough; key ideas could be more intuitively explained, especially in the attention-editing section\n2. The evaluation scopes are somehow limited and misaligned with the targets. \n3. The correspondence between the framework design and the experimental objectives needs further explanation."}, "questions": {"value": "1. The data generation method is not clearly explained, either in the text or in Figures 1 and 2.\nThe CIR task involves extracting and modifying components within an image, but the data generation process seems to only refine textual descriptions. How do these correspond to each other? Do you imply that merely focusing on textual data is sufficient for the model to learn \"culturally contextualized expressions\"?\nIf Section 2.2.2 involves processing the images, it is recommended to clarify this part further so that other researchers can quickly understand it.\n2. The authors claim that the proposed framework is designed to address dynamically evolving, culturally contextualized expressions in the CIR task. While the data-level design may account for the culturally contextualized expressions, it is not clear how the framework handles the dynamic adaptation aspect. Is there any mechanism or module specifically addressing the dynamic evolution of expressions over time or across contexts? This point requires further clarification and explanation.\n3. I wonder the reason why Section 2.3 is named \"INSTRUCTION-FOLLOWING PIPELINE\", as this part includes the explanation of model-level improvements. \n4. Since the stated goal of this paper is to address dynamically evolving, culturally contextualized expressions in the CIR task, the evaluation dataset should arguably place greater emphasis on benchmarks that capture internet-specific phenomena such as memes or other culturally dynamic content. Are such benchmarks currently available? If so, it is recommended that the evaluation focus more on these datasets, as they better reflect the real-world characteristics and challenges of culturally and temporally evolving visual-textual expressions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yAO1m4PkAL", "forum": "sVeI9ZHyNN", "replyto": "sVeI9ZHyNN", "signatures": ["ICLR.cc/2026/Conference/Submission12431/Reviewer_BWPg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12431/Reviewer_BWPg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834094255, "cdate": 1761834094255, "tmdate": 1762923317960, "mdate": 1762923317960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WebCIR, which brings contemporary web-culture concepts (memes, slang, stylized aesthetics) into CIR. The authors construct a culture-aware training set (i.e., POKEMON) via an GPT-4o and propose an attention-editing knowledge-infusion framework with hierarchical masks and contrastive learning to inject and gate external web knowledge while preserving visualâ€“text grounding. The pipeline produces instruction-aware embeddings and is evaluated on CIRR, FashionIQ, and the proposed POKEMON benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1.\tThe idea of this paper is easy to understand.\n2.\tIt is reasonable to leveraging web concepts for CIR training."}, "weaknesses": {"value": "1.\tThe related works section is overlooked.\n2.\tLack of the novelty. The concept of leveraging web knowledge to build synthetic triplets for CIR training has been explored in MagicLens [1]. The use of MLLM for synthetic triplet construction has also been explored in [2,3], which the authors fail to acknowledge. \n3.\tLimited technology contribution. The \"innovative semantic alignment framework\" with learnable queries, which is a key contribution of this paper, has already been explored in prior CIR works [4,5].\n4.\tConcern of the data quality of the POKEMON. The POKEMON dataset is entirely built through GPT-4o auto-generation without provide sufficient details, analysis (e.g., size, context, domain) and human evaluation.\n5.\tConcerns of the reproducibility. The paper lacks sufficient explanation on CoT prompt design, and not given code or example data. It makes me significant concern the reproducibility of this paper.\n6.\tPoor writing. The paper seems unfinished and lacks refinement, possibly due to being written on short notice.\n\nReferences\n\n[1] Zhang K, Luan Y, Hu H, et al. Magiclens: Self-supervised image retrieval with open-ended instructions[J]. arXiv preprint arXiv:2403.19651, 2024.\n\n[2] Zhou J, Liu Z, Liu Z, et al. Megapairs: Massive data synthesis for universal multimodal retrieval[J]. arXiv preprint arXiv:2412.14475, 2024.\n\n[3] Jang Y K, Kim D, Meng Z, et al. Visual Delta Generator with Large Multi-modal Models for Semi-supervised Composed Image Retrieval[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 16805-16814.\n\n[4] Bai Y, Xu X, Liu Y, et al. Sentence-level prompts benefit composed image retrieval[J]. arXiv preprint arXiv:2310.05473, 2023.\n\n[5] Ke H, Shi J, Zhang Y, et al. Task-Aware Prompt Gradient Projection for Parameter-Efficient Tuning Federated Class-Incremental Learning[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025: 2631-2641."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y7UnDJ1oHY", "forum": "sVeI9ZHyNN", "replyto": "sVeI9ZHyNN", "signatures": ["ICLR.cc/2026/Conference/Submission12431/Reviewer_nwot"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12431/Reviewer_nwot"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858632631, "cdate": 1761858632631, "tmdate": 1762923317555, "mdate": 1762923317555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}