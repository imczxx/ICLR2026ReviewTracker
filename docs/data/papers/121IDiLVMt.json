{"id": "121IDiLVMt", "number": 10165, "cdate": 1758162738147, "mdate": 1763698845314, "content": {"title": "Dual-Solver: A Generalized ODE Solver for Diffusion Models with Dual Prediction", "abstract": "Diffusion models deliver state-of-the-art image quality. However, sampling is costly at inference time because it requires many model evaluations (number of function evaluations, NFEs).\nTo reduce NFEs, classical ODE multistep methods have been adopted. Yet differences in the choice of prediction type (noise/data/velocity) and integration domain (half log-SNR/noise-to-signal ratio) lead to different outcomes.\nWe introduce Dual-Solver, which generalizes multistep samplers by introducing learnable parameters that continuously (i) interpolate among prediction types, (ii) select the integration domain, and (iii) adjust the residual terms.\nIt maintains the traditional predictor-corrector structure and guarantees second-order local accuracy.\nThese parameters are learned with a classification-based objective using a frozen pretrained classifier (e.g., ViT or CLIP).\nOn ImageNet class-conditional generation (DiT, GM-DiT) and text-to-image (SANA, PixArt-$\\alpha$), Dual-Solver improves FID and CLIP scores in the low-NFE regime ($3\\le$ NFE $\\le 9$) across backbones.", "tldr": "This work generalizes multistep samplers via parameterizing the prediction type, integration domain, and second-order residual term, and introduces classification-based parameter learning.", "keywords": ["Generative Models", "Diffusion Models", "Fast Sampling", "ODE Solver"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/17899d2580de75a9c177c025203c00c678b35112.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Diffusion models deliver state-of-the-art image quality. However, sampling is costly at inference time because it requires many model evaluations (number of function evaluations, NFEs). To reduce NFEs, classical ODE multistep methods have been adopted. Yet differences in the choice of prediction type (noise/data/velocity) and integration domain (half log-SNR/noise-to-signal ratio) lead to different outcomes. This paper introduces Dual-Solver, which generalizes multistep samplers by introducing learnable parameters that continuously (i) interpolate among prediction types, (ii) select the integration domain, and (iii) adjust the residual terms. It maintains the traditional predictor-corrector structure and guarantees second-order local accuracy. These parameters are learned with a classification-based objective using a frozen pretrained classifier (e.g., ViT or CLIP). On ImageNet class-conditional generation (DiT, GM-DiT) and text-to-image (SANA, PixArtα), Dual-Solver consistently improves FID and CLIP scores in the low-NFE regime (NFE ≤ 6) across backbones."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Generalizes multistep samplers by introducing learnable parameters that continuously interpolate among different prediction types (noise/data/velocity), select the integration domain, and adjust residual terms, addressing the issue of inconsistent outcomes caused by differences in prediction type and integration domain choices.\n2. Structural compatibility and guaranteed accuracy: Maintains the traditional predictor-corrector structure (ensuring compatibility with existing multistep method frameworks) while guaranteeing second-order local accuracy, balancing structural familiarity and precision.\n3. Learns the key parameters via a classification-based objective, leveraging frozen pretrained classifiers (e.g., ViT, CLIP) — this avoids the need for heavy independent training and capitalizes on existing well-performing models."}, "weaknesses": {"value": "see Questions"}, "questions": {"value": "1. For a single prediction model, does Dual-Solver need to first transform the prediction into eps and $x_0$?\n\n2. Is dual prediction necessary? Given that a predefined noise scheduler has deterministic prediction transforms( eps to v, v to eps, v to x0), I have doubts regarding the arguments for dual prediction. Although other works [1, 2] use a single prediction, no discriminator is employed for learning.\n\n[1] Shuai Wang, Zexian Li, Tianhui Song, Xubin Li, Tiezheng Ge, Bo Zheng, Limin Wang, et al. Differentiable solver search for fast diffusion sampling. arXiv preprint arXiv:2505.21114, 2025.\n\n[2] Neta Shaul, Uriel Singer, Ricky TQ Chen, Matthew Le, Ali Thabet, Albert Pumarola, and Yaron Lipman. Bespoke non-stationary solvers for fast sampling of diffusion and flow models. arXiv preprint arXiv:2403.01329, 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XHAx62X6mj", "forum": "121IDiLVMt", "replyto": "121IDiLVMt", "signatures": ["ICLR.cc/2026/Conference/Submission10165/Reviewer_8gbp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10165/Reviewer_8gbp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761026750315, "cdate": 1761026750315, "tmdate": 1762921534154, "mdate": 1762921534154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Dual-Solver, a learnable, generalized ODE sampler that unifies noise/data/velocity predictions via dual prediction, and combines a log–linear domain transform with a second-order predictor–corrector and learnable residuals for stable few-step sampling.\n\nIt learns per-step parameters and the timestep schedule end-to-end using discriminative objectives (e.g., classifier/CLIP), yielding large quality gains at very low NFE across diverse backbones and tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is novel and solid theoretical. This work analyzes currently popular semi-linear samplers and different prediction targets and proposes a more general PC-based sampler. Contributions include:\n    - unifies different prediction according to a dual-prediction parameterization. \n    - uses a learned change of variables in semi-linear family for integration\n    - second order residual adjustment for p1c2\n\n2.  Experimental sufficient. This work provides strong, multi-backbone results on FID. And there's enough ablations over predictor/corrector order and parameter freedoms\n\n3. Limitations are acknowledged. \n\nOverall, this work proposed a well-motivated and clearly articulated solvers, especially for few-step sampling."}, "weaknesses": {"value": "1. Personally, the article writing is somewhat difficult to follow up on. The overall structure is somewhat confusing to me.\n\n2. The description of the parameter training part is insufficient. Especially the training process. \n\n3. The discussion of details for certain algorithms lacks intuitive understanding."}, "questions": {"value": "1. About the training process, my current understanding is that after sampling M-1 steps, we directly optimize the corresponding 10*(M-1) groups of parameters with a set of labels or captions? Once these parameters are trained, the parameters can be reused for this backbone? Can the author provide a more specific training process?\n\n2. Why classification objectives beat regression is not deeply unpacked. The paper shows a V-shaped accuracy–FID trend and argues “reaching the right decision region” is enough, but it stops short of a deeper mechanism: when/why classification consistently wins across backbones/NFE, and under what conditions it might overfit decision boundaries.\n\n3. Consistency vs. transfer across backbones/settings.\nIt observes that learned parameter curves look similar across NFEs for a given backbone, yet recommends training per backbone/setting. Readers may wonder about the practical transfer value of those similarities (warm-starts, NFE interpolation, shared parameterizations), which isn’t explored methodologically.\n\n4. Fig2 is not mentioned in the article. And I feel that the Sec 4 and 5 are somewhat abrupt logically, it seems subsection of Sec2? \n\n\nOverall, my main concern is 1. Please clarify the approach for obtaining parameters for all steps. If my concern addressed, I would be willing to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6ByyPKwJw6", "forum": "121IDiLVMt", "replyto": "121IDiLVMt", "signatures": ["ICLR.cc/2026/Conference/Submission10165/Reviewer_qooq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10165/Reviewer_qooq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761503303902, "cdate": 1761503303902, "tmdate": 1762921533647, "mdate": 1762921533647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank the reviewers for their thoughtful feedback and constructive comments.\nWe have incorporated the feedback into the revised manuscript, with major changes summarized below for your convenience.\n\n*For ease of review, we also note that the major newly added sentences are highlighted in blue in the revised manuscript.*\n\n1. **Additional Experiments**  \n- **Fig. 5, Main quantitative results:** We updated Fig. 5 by adding new experiments for SANA and PixArt-$\\alpha$ at NFE = 7, 8, and 9.  \n- **Sec. 6.2 (B), Parameter settings for $(\\gamma,\\tau,\\kappa)$:** We added experiments to evaluate the feasibility of using global parameters and updated the section accordingly.  \n- **Sec. 6.2 (C), Parameter learning methods:** We conducted additional experiments to more clearly explain the transition from regression-based to classification-based learning.  \n- **Sec. 6.3, Parameter interpolation across NFEs:** We added experiments to test the robustness of parameters across NFEs.\n\n2. **Revisions and Clarifications**  \n- **Fig. 2, Euler updates for noise, velocity, and data predictions:** We improved Fig. 2 and clarified the accompanying explanation.  \n- **Alg. 2, Hard-label classification for parameter learning:** We added Algorithm 2 to clarify the overall process of classification-based learning.  \n- **Sec. 5, Solver Parameter Learning:** We discussed regression- and classification-based learning in more depth.\n- **Fig. 4, Solver parameter learning methods:** As Sec. 5 was expanded, we updated Fig. 4 to reflect and compare the detailed learning methods.\n\nWe hope that the extensive revisions and additional experiments adequately address the reviewers’ concerns.\nWe sincerely appreciate the reviewers’ efforts and hope that the revised manuscript meets the standards for acceptance."}}, "id": "Fvovk3BOm3", "forum": "121IDiLVMt", "replyto": "121IDiLVMt", "signatures": ["ICLR.cc/2026/Conference/Submission10165/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10165/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10165/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763698965692, "cdate": 1763698965692, "tmdate": 1763698965692, "mdate": 1763698965692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Dual-Solver, which accelerates the sampling of diffusion models using learnable parameters $\\gamma$, $\\tau$, and $\\kappa$. This framework incorporates and extends many existing multi-step and learning-based solvers, and abandons high-NFE (Number of Function Evaluations) teacher trajectories; instead, it updates the learnable parameters via classification loss or CLIP loss. The paper reports experimental results on ImageNet class-conditional and text-to-image diffusion models, effectively demonstrating the sampling performance of Dual-Solver under low NFE."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It constructs a generalized ODE solver based on a \"predictor-corrector\" structure. Through explicit interpolation of $\\gamma$ and domain transformation via $\\tau$, it unifies noise/data/velocity prediction methods within a single framework.\n2. It proposes a solver parameter update method based on classification loss or CLIP loss, abandoning the parameter update learning strategy that relies on teacher trajectories. This reduces computational overhead and improves performance under low NFE.\n3. The paper features rigorous mathematical derivations, covering the mathematical foundations of dual prediction and log-linear domain mapping. It also provides a complete proof of the second-order local truncation error."}, "weaknesses": {"value": "1. The method is derived based on second-order accuracy, but it does not discuss potential issues when extending to higher-order schemes, which limits the further performance improvement of Dual-Solver.\n2. It lacks comparative results with current advanced learning-based solvers, such as EPD-Solver[1] and AMED-Solver[2].\n3. The parameter update of Dual-Solver relies on the cross-entropy loss of a classifier or CLIP loss, which means the parameter update of Dual-Solver will fail for unguided conditions.\n\n[1] Zhu B, Wang R, Zhao T, et al. Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025: 19557-19566.\n\n[2] Zhou Z, Chen D, Wang C, et al. Fast ode-based sampling for diffusion models in around 5 steps[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 7777-7786."}, "questions": {"value": "1. How does Dual-Solver perform compared to current advanced learning-based solvers (e.g., EPD-Solver[1], AMED-Solver[2])?\n2. The paper proposes that using a \"weak classifier\" with lower accuracy can alleviate the problem of reduced sample diversity, but the results in Table 7 do not seem to clearly demonstrate the effectiveness of this strategy. Could you provide further explanation?\n3. The paper learns parameters for each step but does not use ablation studies to quantify the effect of globally shared parameterization. Given that shared parameters can effectively reduce the number of learned parameters, could the authors conduct more detailed comparative experiments?\n\n[1] Zhu B, Wang R, Zhao T, et al. Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025: 19557-19566.\n\n[2] Zhou Z, Chen D, Wang C, et al. Fast ode-based sampling for diffusion models in around 5 steps[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 7777-7786."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X4ZSIOzyLm", "forum": "121IDiLVMt", "replyto": "121IDiLVMt", "signatures": ["ICLR.cc/2026/Conference/Submission10165/Reviewer_g7wy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10165/Reviewer_g7wy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722901937, "cdate": 1761722901937, "tmdate": 1762921533222, "mdate": 1762921533222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Dual-Solver, a learned predictor–corrector sampler for diffusion/flow-matching models that targets the low-NFE regime. It introduces three per-step learnable knobs: γ (interpolates prediction type across noise/data/velocity), τ (a log–linear domain transform that interpolates between λ- and ρ-domains), and κ (a residual term that preserves second-order local accuracy). The method keeps second-order local truncation error, learns the timestep schedule, and trains all solver parameters via a classification-based objective that uses a frozen classifier."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Unified, principled parameterization. γ cleanly bridges noise/data/velocity predictions; τ smoothly interpolates λ↔ρ domains; κ adjusts second-order residuals\n\n2. Strong low-NFE results across backbones. Consistent FID/CLIP gains at ≤6 steps on DiT, SANA, PixArt-α, and GM-DiT.\n\n3. Classification-based training removes the need for high-NFE teacher samples, reducing preparation cost and directly optimizing a perceptual proxy."}, "weaknesses": {"value": "1. Training introduces ten learnable parameters per step (in addition to a learned schedule), which could increase brittleness or overfitting to specific backbones and guidance settings. Also, does the method require separate training runs for different NFE targets?\n\n2. Because CE/CLIP is computed with a frozen classifier, the optimization might bias samples toward ‘classifier-friendly’ patterns. How do the authors mitigate proxy-objective leakage and ensure alignment with the intended generative quality?"}, "questions": {"value": "Does the method require separate training runs for different NFE targets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "23SfScO2Ev", "forum": "121IDiLVMt", "replyto": "121IDiLVMt", "signatures": ["ICLR.cc/2026/Conference/Submission10165/Reviewer_TpbM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10165/Reviewer_TpbM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996516148, "cdate": 1761996516148, "tmdate": 1762921532729, "mdate": 1762921532729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}