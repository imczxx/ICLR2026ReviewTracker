{"id": "TUd3c7Vr1z", "number": 9361, "cdate": 1758120041380, "mdate": 1759897729413, "content": {"title": "Towards Self-Robust LLMs: Intrinsic Prompt Noise Resistance via CoIPO", "abstract": "Large language models (LLMs) have demonstrated remarkable and steadily improving performance across a wide range of tasks.\nHowever, LLM performance may be highly sensitive to prompt variations especially in scenarios with limited openness or strict output formatting requirements, indicating insufficient robustness.\nIn real-world applications, user prompts provided to LLMs often contain imperfections, which may undermine the quality of the model's responses.\nTo address this issue, previous work has primarily focused on preprocessing prompts, employing external tools or even LLMs to refine prompt formulations in advance.\nHowever, these approaches overlook the intrinsic robustness of LLMs, and their reliance on external components introduces additional computational overhead and uncertainty.\nIn this work, we propose a Contrastive Learning-based Inverse Direct Preference Optimization (CoIPO) method that minimizes the discrepancy between the label-aligned logits produced by the model under a clean prompt and its noisy counterpart, and conduct a detailed analysis using mutual information theory.\nWe augment the FLAN dataset by constructing paired prompts, each consisting of a clean prompt and its corresponding noisy version for training.\nAdditionally, to evaluate the effectiveness, we develop NoisyPromptBench, a benchmark enhanced and derived from the existing PromptBench. \nExperimental results conducted on NoisyPromptBench demonstrate that our proposed method achieves a significant improvement in average accuracy over the current state-of-the-art approaches.\nThe source code of CoIPO, pair-wise FLAN datasets, and NoisyPromptBench have already been released on https://anonymous.4open.science/r/CoIPO-61D6.", "tldr": "We propose CoIPO, a method that improves LLM robustness by reducing the performance gap between clean and noisy prompts.", "keywords": ["large language model", "robustness", "post training"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fe5a13a91544dd4ec91540e6a88925350ad7620d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the problem of prompt robustness for large language models and introduces CoIPO (Contrastive Learning-based Inverse Direct Preference Optimization), a post-training method that enhances intrinsic prompt noise resistance without dependence on external preprocessors. CoIPO is trained on paired clean and noisy prompts from an augmented FLAN dataset, with a loss function that uses contrastive learning and inverse DPO to minimize the discrepancy between outputs for clean and semantically similar noisy prompts, while maximizing it for semantically different pairs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a principled intrinsic robustness enhancement method (CoIPO) for LLMs based on contrastive learning and inverse DPO. The formulation is clear and easy to understand.\n2. The authors offer an information-theoretic perspective (mutual information) to justify the approach, which is interesting."}, "weaknesses": {"value": "1. The research problem of prompt optimization is important but the research scope of this work is somehow limited, since the noisy prompts in this paper primarily refer to typos (character level, word level etc.), while real-world prompt imperfections can be more varied, such as semantic ambiguity, non-standard grammar, than those benchmarked here.\n2. More experiment baselines should be incorporated. For example, DPO should be considered as a baseline, since CoIPO uses both contrastive training and inverse-DPO and contrastive training baseline COIN is used, DPO should also be included as a baseline.\n3. Most experiments are conducted on tasks with deterministic answers —prompts with more open-ended or generative outputs should be considered."}, "questions": {"value": "1. How does CoIPO handle more nuanced and complex prompt noise types? Can it generalize to unseen perturbation types?\n2. Could minimizing output discrepancies between noisy and clean prompts reduce the model's flexibility? Does CoIPO risk making the model less sensitive to subtle but meaningful variations?\n3. It is not mentioned in the paper how the prompt for another task, i.e. P_2 is selected. Is it selected randomly or according to some heuristics? How does the selection method of P_2 affect the performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZmswAzhhCf", "forum": "TUd3c7Vr1z", "replyto": "TUd3c7Vr1z", "signatures": ["ICLR.cc/2026/Conference/Submission9361/Reviewer_eHqb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9361/Reviewer_eHqb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9361/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761035136764, "cdate": 1761035136764, "tmdate": 1762920981639, "mdate": 1762920981639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed CoIPO, an novel algorithm that aims to train robust LLMs under imperfect user inputs without pre-processing. CoIPO combines contrastive learning and preference optimization by reducing the gap between clean prompt and noisy prompt while enlarging the gap between the noisy prompt and a unrelated clean prompt. Theoretical analysis on relative entropy gain is presented and experiments are conducted on Llama and Qwen-2.5 architectures. CoIPO significantly improves accuracy for both clean and noisy scenarios on PromptBench, surpassing the baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* CoIPO tackles a significant issue, handle imperfect user input in daily use of the LLMs.\n* The CoIPO algorithm leverages contrastive learning and direct alignment, backed with theoretical insights from the perspective and relative entropy gap maximization.\n* Comprehensive experiments are conducted for different architectures."}, "weaknesses": {"value": "* Though CoIPO alone is a novel and effective algorithm for increasing LLM robustness. My concern is that if this procedure will hurt the models performance on tasks like math reasoning and coding. Since it would appear costly to me if we sacrifice these reasoning capabilities to replace pre-processing tools for imperfect input.\n* No algorithm specific hyper-parameter is presented in the paper, could the author elaborate a bit more on the details hyper-parameter if they are included in the algorithm?\n* Preference optimization is relatively light weight but still requires time and compute, could the authors elaborate on the time cost for running CoIPO compared to pre-proccessing?"}, "questions": {"value": "Please see weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "kBfxPfSXAR", "forum": "TUd3c7Vr1z", "replyto": "TUd3c7Vr1z", "signatures": ["ICLR.cc/2026/Conference/Submission9361/Reviewer_yVsx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9361/Reviewer_yVsx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9361/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977596217, "cdate": 1761977596217, "tmdate": 1762920981295, "mdate": 1762920981295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CoIPO, a post-training method to enhance LLMs' intrinsic robustness against prompt perturbations. Unlike existing approaches that rely on external preprocessing tools, CoIPO trains models to directly handle noisy prompts by minimizing discrepancies in label-aligned logits between clean and noisy prompts. The authors construct a paired FLAN dataset and develop NoisyPromptBench for evaluation. Experiments on Llama-7B and Qwen2.5-7B across five datasets demonstrate improvements over baselines, with theoretical justification provided through mutual information analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Well-motivated problem: The paper clearly articulates limitations of external preprocessing approaches and makes a strong case for intrinsic robustness.\nSolid theoretical foundation: The mutual information analysis (Equations 9-16) provides principled justification for the method.\nComprehensive evaluation: NoisyPromptBench with multiple perturbation types and the decoding radius analysis (Section 4.1, Figure 5) provide thorough robustness assessment.\nAblation studies: Table 3 effectively demonstrates the necessity of both inverse DPO and contrastive learning components."}, "weaknesses": {"value": "Limited scope of evaluation:\n\n● Only 7B parameter models tested; unclear if findings generalize to larger models (13B, 70B+)\n\n● Only 5 datasets from GLUE-style tasks; robustness on generation tasks, reasoning, or code generation is unexplored\n\n● Training data limited to 25 FLAN subsets; impact of training data scale not studied\n\nInsufficient baseline comparisons:\n\n● Only compares to COIN for intrinsic robustness methods\n\n● Missing comparisons to recent prompt optimization methods (e.g., PromptAgent, RoP mentioned in related work)\n\n● No comparison to instruction-tuning methods that may implicitly improve robustness\n\nTheoretical gaps:\n\n● The connection between Equation 15 and Equation 8 relies on several approximations that may not hold in practice\n\nMissing analyses:\n\n● No error analysis showing which types of errors CoIPO successfully handles vs. fails on\n\n● Computational cost comparison not provided (training time, memory, inference latency)"}, "questions": {"value": "Scalability: Have you tested CoIPO on larger models (13B+)? Do the improvements scale, or do larger models already have better intrinsic robustness?\n\nTraining efficiency: What is the computational overhead of CoIPO compared to standard fine-tuning? How many training epochs are needed for convergence?\n\nComparison to instruction tuning: How does CoIPO compare to simply training on more diverse instruction data? Could increased data diversity achieve similar robustness?\n\nGeneration tasks: All experiments focus on classification. How does CoIPO perform on open-ended generation where output quality is harder to measure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fdeuO094O0", "forum": "TUd3c7Vr1z", "replyto": "TUd3c7Vr1z", "signatures": ["ICLR.cc/2026/Conference/Submission9361/Reviewer_7U7C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9361/Reviewer_7U7C"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9361/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045031245, "cdate": 1762045031245, "tmdate": 1762920980974, "mdate": 1762920980974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}