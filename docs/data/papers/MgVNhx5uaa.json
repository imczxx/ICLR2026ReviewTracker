{"id": "MgVNhx5uaa", "number": 11801, "cdate": 1758203919629, "mdate": 1763210822406, "content": {"title": "ATOM-Bench: From Atoms to Conclusions in Objective Evaluation of Large Multimodal Models Reasoning", "abstract": "Chain-of-Thought (CoT) reasoning has significantly enhanced the ability of Large Multimodal Models (LMMs) to tackle complex image–text tasks, establishing itself as a cornerstone of multimodal learning. Despite significant progress, the impact of CoT on LMMs still lacks objective evaluation and in-depth research. Current CoT evaluation paradigms rely on powerful LLMs as judges of free-form text, but this introduces bias and hallucination from the evaluator itself. Moreover, it may penalize models for stylistic variations rather than genuine reasoning failures, thereby undermining the fairness and reliability of the assessment. To address this gap, we introduce ATOM-Bench, a CoT evaluation framework built on objective atomic questions. ATOM-Bench decomposes complex reasoning tasks into a series of atomic nodes, covering 570 high-resolution real-world images and 2,920 questions across 4 cognitive dimensions, and 12 domains, including architecture, text, transportation, culture, climate, and geology. Our benchmark introduces three novel quantitative metrics to objectively analyze reasoning faithfulness, consistency, and robustness. Extensive experiments with 22 LMMs validate the effectiveness of our framework. The results reveal that even the strongest models often exhibit a mismatch between surface-level correctness of final answers and their underlying evidence comprehension, while also exposing cognitive rigidity when faced with objective facts.We believe that ATOM-Bench, as a more objective and diagnostic tool, will advance LMMs toward more reliable and faithful reasoning.", "tldr": "We introduce ATOM-Bench, a diagnostic benchmark for evaluating Chain-of-Thought reasoning in Large Multimodal Models via objective atomic questions, spanning 2,920 QAs over 570 real-world images, to address challenges of reasoning reliability.", "keywords": ["multimodal Large Language Models", "benchmark", "chain of thought"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/c947afaa84cde87110caac1da6882a889667f04a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper is about the evaluation of large multimodal model reasoning. The authors claim that they introduce a CoT evaluation framework built on objective atomic questions, covering 570 high-resolution real-world images and 2,920 questions across 4 cognitive dimensions, and 12 domains, including architecture, text, transportation, culture, climate, and geology. They have tested a number of large multimodal models with the proposed evaluation framework."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors have evaluated a number of multimodal large language models with the proposed evaluation framework.\n2. The proposed benchmark covers a wide range of domains."}, "weaknesses": {"value": "1. The authors claim that \"Current CoT evaluation paradigms rely on powerful LLMs as judges of free-form text, but this introduces bias and hallucination from the evaluator itself.\" However, the proposed evaluation framework also relies on LLMs and shares the weakness of prior works.\n2. The proposed evaluation framework relies on \"rigorous human review and refinement\", as the authors claim. However, it is not clear how the authors ensure rigor in this process. Are there any human errors in this process? How to ensure the human experts have checked the dataset with care?\n3. The evaluation framework relies heavily on human efforts in checking many details in the evaluation, which is not practical and hard to scale with paid annotators. Additionally, the proposed benchmark is relatively small. Although the authors claim it covers a wide range of fields and cognitive dimensions, I wonder whether these fields or dimensions are properly represented with limited data.\n4. Given the fact that the dataset is small and the method is not practical, I doubt whether this paper has made enough contribution."}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IWuQrflMwV", "forum": "MgVNhx5uaa", "replyto": "MgVNhx5uaa", "signatures": ["ICLR.cc/2026/Conference/Submission11801/Reviewer_WwKZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11801/Reviewer_WwKZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761394506256, "cdate": 1761394506256, "tmdate": 1762922823269, "mdate": 1762922823269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "YoQTdpiZMG", "forum": "MgVNhx5uaa", "replyto": "MgVNhx5uaa", "signatures": ["ICLR.cc/2026/Conference/Submission11801/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11801/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763210821681, "cdate": 1763210821681, "tmdate": 1763210821681, "mdate": 1763210821681, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper notes that while Chain-of-Thought (CoT) reasoning improves Large Multimodal Models (LMMs) in complex image-text tasks, current CoT evaluation—relying on LLMs as judges—suffers from bias, hallucination, and mispenalizing stylistic variations over real reasoning failures.\nTo address this, it proposes ATOM-Bench: a framework that decomposes complex tasks into atomic questions (covering 570 high-res images, 2,920 questions across 4 cognitive dimensions and 12 domains) and introduces three quantitative metrics (RCS, HI, RRS) to turn subjective evaluation into evidence-based diagnostics, solving the \"black-box evaluating a black-box\" issue.\nExperiments on 22 LMMs show even state-of-the-art models mismatch final answer correctness with evidence comprehension and have cognitive rigidity. The paper contributes an objective, reproducible CoT evaluation framework, the first high-res process-oriented CoT benchmark, and insights into LMMs’ gaps in reasoning faithfulness and flexibility to advance reliable LMM research."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Instead of relying on Large Language Models (LLMs) as judges in traditional paradigms, ATOM-Bench adopts \"atomic questions\" to eliminate the \"black-box evaluating black-box\" dilemma. \n2. The benchmark is built on 570 high-resolution real-world images, validated through human-machine collaboration (including expert cross-reviews of clue authenticity and distractor rationality) to guarantee data quality. \n3. It decomposes complex reasoning tasks into clue-level (CLQ) and conclusion-level (CoLQ) atomic nodes, covering 4 cognitive dimensions and 12 real-world domains. T"}, "weaknesses": {"value": "1. The benchmark only centers on single-image geolocation, failing to cover complex scenarios like video temporal reasoning or cross-modal generation. It cannot fully measure LMMs’ performance across diverse CoT tasks.\n2. All questions are multiple-choice, with no assessment of free-text reasoning chain generation. It cannot evaluate models’ ability to express logical steps in open text for real-world applications.\n3. Complex reasoning is decomposed into pre-defined \"standard chains,\" ignoring the diverse reasoning paths models may actually take. It fails to reflect models’ real logical decision-making processes."}, "questions": {"value": "1. Its atomic decomposition relies on preset logic. Is this decomposition consistent with humans’ actual reasoning paths? \n2. ATOM-Bench lacks samples from low-resource regions (e.g., small countries). Does it plan to supplement such data to improve evaluation comprehensiveness? \n3. It doesn’t specify the weight of image vs. text clues. When clues conflict, can current metrics fairly measure models’ decision rationality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q3TyhMYSY4", "forum": "MgVNhx5uaa", "replyto": "MgVNhx5uaa", "signatures": ["ICLR.cc/2026/Conference/Submission11801/Reviewer_db8N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11801/Reviewer_db8N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643187697, "cdate": 1761643187697, "tmdate": 1762922822771, "mdate": 1762922822771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The author proposes a novel atomic-question-based CoT evaluation framework, comparing to the previous benchmark which reasons over the CoT process using traditional \"LLM as a Judge\", the evaluation framework focuses on objective and fairness, including three new evaluation metrics, RCS(reasoning-conclusion support), HI(Hallucinated Inference), RRS(Reasoning Revision Score).\n- Besides the evaluation framework, the authors introduce ATOM-Bench, the benchmark includes 2,920 multi-choice questions across 4 cognitive dimensions and 12 subtasks.\n- Authors also evaluates 22 leading models and provide insights including even the state-of-the-art models like Gemini-2.5-Pro and GPT-5 can show post-hoc fallacies, models often fail to revise errors when confronted with indisputable ground-truth evidence."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The originality of the paper is good, the paper focuses on the fair and objective evaluation without llm-as-the-judge process.\n- The dimension of the ATOM-Bench is good, it includes 14 different atomic skills, including spatial reasoning.\n- The evaluation models are sufficient, including 22 leading models with both open-sourced and close-sourced ones.\n- The data curation process is very clear to the readers:\nEach step clearly specifies which data sources were used, what criteria were applied, and how the results were verified.\nHuman verification and inter-annotator agreement evaluation were introduced to ensure annotation quality.\nThe logic behind the question categorization is clear and task-oriented.\n- The structure of the paper is easy to read."}, "weaknesses": {"value": "- Overall, I appreciate the readers for the presentation of this paper, however, **examples** are significantly insufficiant for both methodology part and evaluation part. And I think this is one of the biggest weakness of this paper. I search very carefully for more detailed examples in the appendix and only find failure analysis and a few failure examples.\n\n     More specifically, authors should provide more examples regarding:\n1. full multimodal reasoning process of a model regarding the answer, how to evaluate based on that example\n2. examples of how atomic tasks compose into complex reasoning\n3. lacks visual illustrations of multimodal input and error analysis\n\n- The CoT evaluation framework and benchmark samples are only applied on geolocation, which according to my knowledge, this pipeline and benchmark could also be used to a broader domain for evaluating reasoning process, e.g. Mathematical Reasoning, Science Reasoning, which also needs step-by-step objective reasoning process in order to successfully answer a question.\n\n- About the evaluation metric, the RCS (Reasoning Consistency Score) and HI (Hallucination Index) is overlapping with each other, e.g. high reasoning consistency scores indicates low hullucination index. The evaluation dimension is not diverse enough. Have you considered other evaluation metrics, for example, evaluate perception error and reasoning error separately. \n\n- Although the benchmark fucos on objective evaluation, the structure of reasoning, the completeness and the soundness of the answer, however, are the keys to evaluate the correctness of the answer, but the paper entirely ignores them."}, "questions": {"value": "I provide the following questions for authors:\n- The motivation of the paper is to reduce the biased evaluation of \"llm-as-the-judge\", however, in the evaluation metrics, there is some human threshold. e.g. In RCS, the τ=0.75 is set by human. This step also introduces human bias. Why is τ=0.75? Do you have explanations on it?\n- The author claims that the proposed evaluation metric is more objective and fair than traditional evaluation method. Do you have quantitative results to prove that? For example, sample a subset of Atom-Bench and using standard \"llm-as-the-judge\" process to evaluate and compare it against the proposed evaluation metrics.\n- Can the proposed evaluation framework generalize to other domains except for the geographical reasoning task, e.g. for a broader domain, e.g. in Mathematical Reasoning? If so, could you provide examples regarding how to apply this framework into a broder domain, e.g. Mathematical Reasoning using a standard Benchmark like Mathvista?\n- **Especially**, you should provide more examples regarding: (as listed in weakness)\n1. full multimodal reasoning process of a model regarding the answer, how to evaluate based on that example\n2. examples of how atomic tasks compose into complex reasoning\n3. visual illustrations of multimodal input and error analysis\n\nI will consider raise my score if you can address my concerns listed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X8f3AtT1WA", "forum": "MgVNhx5uaa", "replyto": "MgVNhx5uaa", "signatures": ["ICLR.cc/2026/Conference/Submission11801/Reviewer_sR7U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11801/Reviewer_sR7U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862846988, "cdate": 1761862846988, "tmdate": 1762922822102, "mdate": 1762922822102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ATOM-Bench, a benchmark for evaluating reasoning processes in large multimodal models. It reformulates complex reasoning into atomic multiple-choice questions with ground-truth answers to enable objective measurement. The dataset contains 570 real-world images and 2,920 questions covering four cognitive dimensions and twelve subdomains. The authors introduce three metrics to assess reasoning consistency, hallucination rate, and robustness when models are given corrected evidence. Experiments on 22 multimodal models are conducted to analyze their reasoning behavior and the relationship between answer accuracy and reasoning consistency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Atomic multiple-choice questions provides objective, interpretable, and reproducible evaluation results.\n2. The analysis highlights a clear gap between correctness and reasoning quality, offering concrete empirical observations."}, "weaknesses": {"value": "1. The dataset is relatively small and limited in diversity, containing only 570 real-world images, which restricts coverage of varied visual scenes and reduces the stability of cross-model comparisons.\n2. The task scope is overly narrow, as the framework is primarily validated on single-image geo-localization, limiting its generalizability to other multimodal reasoning tasks.\n3. Error analysis remains anecdotal and lacks systematic statistics on error types or cross-model differences, making it difficult to derive actionable insights for model improvement."}, "questions": {"value": "1. Could it be extended to cover more complex or diverse multimodal reasoning tasks beyond single-image geolocation?\n2. Could the authors provide a more detailed error analysis with statistics and a deeper look at failure patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qyea8A8FPG", "forum": "MgVNhx5uaa", "replyto": "MgVNhx5uaa", "signatures": ["ICLR.cc/2026/Conference/Submission11801/Reviewer_sAqG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11801/Reviewer_sAqG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985117171, "cdate": 1761985117171, "tmdate": 1762922821622, "mdate": 1762922821622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}