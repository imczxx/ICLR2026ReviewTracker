{"id": "AzmtrYcj2C", "number": 19665, "cdate": 1758298092589, "mdate": 1763698968526, "content": {"title": "GraphPrompt: Black-box Jailbreaks via Adversarial Visual Knowledge Graphs", "abstract": "Multimodal Large Language Models (MLLMs) introduce structured visual interaction paradigms into conversational systems, where Visual Knowledge Graphs (VKGs) are emerging as a primary input modality that models can directly parse and manipulate. VKGs significantly enhance models' ordered reasoning and planning capabilities by explicitly encoding semantic topological relationships and task workflows. However, this advancement also introduces new security attack surfaces: when sensitive or malicious intent is decomposed and implicitly encoded within graph topology and visual style cues, and further paired with surface-neutral textual descriptions, MLLMs may bypass traditional text-based safety filters and follow covert parse-then-execute pathways, exhibiting jailbreak behaviors such as instruction hiding and ambiguity amplification. The safety implications of such structured visual inputs for MLLMs nevertheless remain largely unexplored. To systematically assess this risk, we introduce GraphPrompt, a black-box jailbreak evaluation framework that exploits this attack surface through a three-layer obfuscation pipeline: (1) role-play rewriting masks harmful queries as benign tasks; (2) knowledge graph encoding decomposes procedures into entity–relation structures; and (3) visual rendering transforms graphs into adversarial VKG images. This framework automatically generates high-quality adversarial datasets while providing standardized evaluation. Systematic experiments on six state-of-the-art MLLMs reveal alarming safety risks: GraphPrompt achieves a 94\\% average attack success rate with only 1.25 attempts per query on average. Ablation studies identify graph complexity and image resolution as first-order attack factors, while visual styling has minimal impact. Layer-wise analysis demonstrates that VKG inputs effectively suppress activation in safety-critical layers, providing mechanistic evidence for their jailbreak efficacy. Overall, our work establishes structured visual inputs as an under-explored attack surface and offers a reproducible framework for developing structure-aware defenses.", "tldr": "", "keywords": ["Multimodal Large Language Models", "Black-box Jailbreak Attacks", "Visual Knowledge Graphs", "Cross-modal Safety Alignm"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d847d7bf977794ca671ce3367cb8776f84072f48.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces GraphPrompt, a typographic jailbreaking attack on Multimodal Large Language Models (MLLMs) that utilizes a Visual Knowledge Graph (VKG). To break the alignment of the MLLM, GraphPrompt embeds a jailbreak prompt into a VKG. It begins by constructing a Knowledge Graph (KG) using a Large Language Model (LLM), which is then transformed into a VKG using Mermaid. The VKG is subsequently input into the target MLLM through the visual channel alongside a benign textual prompt. The effectiveness of the jailbreaking is evaluated using an LLM-based judge model. Experiments demonstrate that GraphPrompt successfully jailbreaks various MLLMs with a high Attacking Success Rate (ASR), underscoring the vulnerabilities of MLLMs in processing VKGs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well organized and easy to follow.\n2. Embedding malicious intention into VKG appears to be novel.\n3. Experiments show GraphPrompt is promising for jailbreaking SOTA MLLM."}, "weaknesses": {"value": "1. MLLMs are known to be vulnerable to typographic jailbreaking attacks, where malicious textual questions are converted into images. This approach takes advantage of the model's image understanding capabilities to circumvent textual filters, effectively \"breaking the safety alignment.\" Consequently, the novelty of transforming harmful textual knowledge graphs into typographic images (VKGs) is somewhat limited. While GraphPrompt shows better performance than FigStep, the underlying reasons for this difference have not been thoroughly examined.\n\n2. There is limited discussion on defense methods against GraphPrompt. Although the Related Work section includes a paragraph on defenses against jailbreaking, there are no experiments investigating how basic OCR-based filters or input-moderation defenses (e.g., converting the images into a textual description as a supplement to the benign textual input) can reduce the effectiveness of GraphPrompt."}, "questions": {"value": "Please refer to the Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jf8mcpjMOo", "forum": "AzmtrYcj2C", "replyto": "AzmtrYcj2C", "signatures": ["ICLR.cc/2026/Conference/Submission19665/Reviewer_1Aot"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19665/Reviewer_1Aot"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727828909, "cdate": 1761727828909, "tmdate": 1762931516127, "mdate": 1762931516127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GraphPrompt, a novel black-box jailbreaking attack framework for Multimodal Large Language Models (MLLMs). The core idea is to encode harmful intent not in plain text, but within the topological structure and visual cues of Visual Knowledge Graphs (VKGs). The attack pairs these adversarial VKG images with a benign-looking textual prompt (e.g., \"analyze the tasks in this graph\"), which tricks MLLMs into bypassing text-based safety filters and executing the embedded harmful instruction through a \"covert parsing-execution pathway\"."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **High Efficacy and Realistic Threat Model**: The most significant strength is the attack's high effectiveness. Achieving a 94.0% average ASR—and rates as high as 100% on Gemini and 98% on GPT-5 and Qwen2.5-VL—is impressive. This is accomplished under a strict black-box assumption (no access to weights or gradients), making it a practical and realistic threat.\n\n2. **Data Generation Pipeline**: The framework's ability to \"automatically construct high-quality adversarial sample datasets\" is a useful contribution. This provides a scalable method for red-teaming MLLMs against this new structured-visual threat dimension.\n\n3. **Ablation Studies**: The ablation studies provide a clear picture of why the attack works. The findings that topology and resolution are the dominant factors, while visual elements like color and background are \"second-order\", are key takeaways that can inform future defense strategies."}, "weaknesses": {"value": "1. **Limited and Potentially Insufficient Evaluation Dataset**: The primary weakness is the reliance on the SafeBench-Tiny dataset, which contains only 50 harmful queries. While the authors justify this for \"reproducibility and experimental control\", claiming a 94-100% ASR based on such a small sample size is a major overstatement. The high success rates could be an artifact of these specific 50 queries, and the results may not generalize to a more diverse and larger-scale benchmark.\n\n2. **Questionable Novelty Compared to Prior Work (FigStep)**: The paper claims to be the first to leverage VKGs , but its novelty relative to existing \"text-in-image\" attacks like FigStep  is not sufficiently established. FigStep also works by decomposing instructions into steps and rendering them in an image. While the mechanism is interesting, the motivation for jailbreaking MLLMs is a very crowded research area. The paper does not sufficiently motivate why this specific vector is substantially different or more dangerous than the \"abundant llm jailbreak attacks\" that already exist such as FigStep.\n\n3. **Lack of Experimental Defense Evaluation**: The paper discusses potential defenses in the conclusion, such as \"structure-aware safety filtering\" and \"uncertainty-aware refusal\". However, it presents no experiments to evaluate the efficacy of these or any other defenses. An attack paper is made much stronger by demonstrating how the uncovered vulnerability might be patched."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BlDJpQ2iCl", "forum": "AzmtrYcj2C", "replyto": "AzmtrYcj2C", "signatures": ["ICLR.cc/2026/Conference/Submission19665/Reviewer_JRvZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19665/Reviewer_JRvZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789847919, "cdate": 1761789847919, "tmdate": 1762931515365, "mdate": 1762931515365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GraphPrompt, a novel black-box jailbreaking framework that exploits the structural and semantic properties of Visual Knowledge Graphs (VKGs) to bypass safety alignments in Multimodal Large Language Models (MLLMs)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelty: This is the first work to systematically explore the security risks posed by VKGs in MLLMs, leveraging structural and semantic paradoxes for adversarial attacks.\n2. Clarity and Coherence: The paper is well-structured and written with reasonable experimental design.\n3. Practical Relevance: The attack framework is practical and poses a realistic threat to deployed MLLMs."}, "weaknesses": {"value": "Methodological Detail Lacked: The paper lacks sufficient detail in key parts of the method. For example:\n1.How exactly is semantic decomposition and topology-borne encoding performed?\n2.How are visual encoding parameters (e.g., color, layout) adjusted during optimization?\n3.How are graph size parameters (|V|, |E|) controlled or modified?\n\nDataset Scale: The use of SafeBench-Tiny (only 50 queries) limits the statistical reliability and generalizability of the results.\n\nVKG Generation Process: It is unclear which model or tool is used to generate VKGs from Mermaid code, and how the quality or diversity of generated graphs is ensured.\n\nJudge Model Validation: Although manual spot-checking is mentioned, there is no quantitative evaluation of the judge model’s accuracy or consistency.\n\nInconsistent Model Usage: Not all six models from Table 1 are included in the following experiments, which limits the completeness of the analysis."}, "questions": {"value": "1. How was the semantic decomposition step implemented? Was it rule-based or model-based?\n2. What was the rationale behind the ongoing contest scenario in the user prompt? How might this influence model behavior?\n3. Why were only some of the six models used in the ablation studies?\n4. Was the judge model’s performance evaluated? If so, what were the results?\n5. See Weaknesses for more questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tLBJb2m6BZ", "forum": "AzmtrYcj2C", "replyto": "AzmtrYcj2C", "signatures": ["ICLR.cc/2026/Conference/Submission19665/Reviewer_XpE2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19665/Reviewer_XpE2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988991175, "cdate": 1761988991175, "tmdate": 1762931514699, "mdate": 1762931514699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GraphPrompt, a novel black-box jailbreaking framework that exploits Visual Knowledge Graphs (VKG) to bypass safety alignment in MLLMs. By embedding harmful intents into graph topologies and pairing them with benign textual prompts, GraphPrompt induces a \"parse-then-execute\" pathway that evades text-based filters. The work underscores critical vulnerabilities in MLLMs’ cross-modal reasoning and proposes future defenses centered on structure-aware safety mechanisms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Compelling experimental results: The paper achieves exceptionally high ASR under strict black-box settings, significantly outperforming strong baselines like MM-SafetyBench and FigStep.  \n\n- Rigorous ablation analysis: The systematic ablation studies (e.g., varying node count, resolution, color schemes) clearly illustrate how graph topology and visual encoding affect VKG-driven attack success\n\n- Insights for future defense insights: The success of VKG attack inspires future efforts toward VKG-like jailbreaks."}, "weaknesses": {"value": "- Mechanistic explanation of attack efficacy: While the paper empirically demonstrates high ASR, it lacks a deeper theoretical or mechanistic explanation of why VKG so effectively bypasses safety alignment. For instance, how does graph parsing alter the model’s internal reasoning trajectory? \n\n- Cost analysis of black-box optimization: The feedback loop involves iterative querying, but the computational cost of generating adversarial VKGs is not quantified. \n\n- Reference Issue: The manuscript mentions \"Appendix 4\" in Section 4.1, but no appendices are included in the submission."}, "questions": {"value": "Please see the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rYFzK3yHpj", "forum": "AzmtrYcj2C", "replyto": "AzmtrYcj2C", "signatures": ["ICLR.cc/2026/Conference/Submission19665/Reviewer_3Na9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19665/Reviewer_3Na9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007425103, "cdate": 1762007425103, "tmdate": 1762931513955, "mdate": 1762931513955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}