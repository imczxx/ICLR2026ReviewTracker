{"id": "eGwRxSuApK", "number": 13061, "cdate": 1758213195703, "mdate": 1759897468090, "content": {"title": "FedQAPer: Query Attention Pooling for Dimension Alignment in Federated Non-IID Time-series Forecasting with Personalized Heads", "abstract": "Federated learning (FL) has shown great promise for time-series forecasting, yet a key challenge in real-world applications is feature heterogeneity. Unlike prior work that assumes uniform feature spaces, we construct a more realistic feature-level non-independent and identically distributed (non-IID) scenario by allocating subsets of features to each client. The number of features varies from 1 up to a defined maximum. We introduce FedQAPer, a novel FL framework that combines Query Attention Pooling (QAP) with FedPer algorithm that uses personalized heads for each client to capture local patterns. QAP projects heterogeneous client feature dimensions into a unified representational space, enabling collaborative backbone training across diverse feature configurations. FedPer transforms these aligned representations back to each client’s original feature dimension through personalized heads, achieving both global knowledge integration and local specialization. FedQAPer works for various backbone architectures, including both artificial neural network (ANN) models and spiking neural network (SNN) models. Experiments on multivariate time-series benchmarks demonstrate that FedQAPer effectively handles feature heterogeneity and consistently improves forecasting performance across different backbone models.", "tldr": "FedQAPer is a federated learning framework that addresses feature heterogeneity in time-series forecasting by using Query Attention Pooling to align different client feature dimensions and personalized heads to preserve local specialization.", "keywords": ["Federated learning", "Time series forecasting", "Dimension alignment", "Personalized head"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9bee9407a44a54aaafb1ca93b676be0cf4e985c6.pdf", "supplementary_material": "/attachment/34909773de8f0b13b80ae81a84df543c298d02d5.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes a federated time series learning framework FedQAPer to solve the problem of feature-level non-independent and identically distributed (non-IID). QAP projects heterogeneous features into a unified latent space, enabling collaborative training of the shared backbone with different input dimensions. At the same time, FedPer uses a personalized head to preserve the client's personalized capabilities. Extensive experimental results on several datasets show that FedQAPer outperforms existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Overall, this paper is well-written and easy to follow.\n\n2. This work introduces feature-level non-IID for federated learning of time series is novel. \n\n3. Extensive experimental results are promising."}, "weaknesses": {"value": "1. The introduction section does not explain the feature clearly. Can it be understood as a variate in the context of time series? The main contribution lies in the challenge of different dimensional features, but a lack of the combination of QAP and FedPer.\n\n2. QAP maps any $F_i$ to a fixed $d_{qap}$, and then the personalized head maps it back to $F_i.$ The paper does not discuss whether this mapping can preserve the cross-information between the original features.\n\n3. The baselines are compared with time series models rather than the FL for the time-series framework.\n\n4. The experiments in this work consider the participation of all clients, but real-world scenarios usually involve partial participation, which should also be verified.\n\n5. Only parts of the results in Table 2 have confidence intervals. It would be better to provide confidence intervals for both Tables 1 and 2.\n\n6. There are many commonly considered datasets for time series, but the experiments only test two, which are not solid enough.\n\n7. The work mentions that SNN has advantages in energy consumption/communication, but the article does not consider the computational and communication overhead.\n\n8. The presentation of figures could be improved. The font in Figure 1 is too small, and Figure 1 could be further improved. While Figure 2 and 3 are relatively big."}, "questions": {"value": "Shown in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vMniELhCT0", "forum": "eGwRxSuApK", "replyto": "eGwRxSuApK", "signatures": ["ICLR.cc/2026/Conference/Submission13061/Reviewer_gxN3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13061/Reviewer_gxN3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761360710152, "cdate": 1761360710152, "tmdate": 1762923791330, "mdate": 1762923791330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FedQAPer, a personalized federated learning framework for heterogeneous time series forecasting. FedQAPer is built on a scenario where each client holds time series data with different variables. Based on the FedPer framework, the authors introduce a Query Attention Pooling (QAP) module within FedQAPer to align heterogeneous client features into a unified latent space, and incorporate a personalized head on each client without communication to achieve personalization. In addition, the authors conduct experiments using five time series forecasting backbones on two multi-point time series datasets."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "**S1.** The authors provide complete source code for reproduce.\n\n**S2.** The challenge of feature heterogeneity in real-world time series forecasting is make sense."}, "weaknesses": {"value": "**W1.** Line 63, the authors claim their work 'unlike prior work' and 'constrcut a more realistic FL setting where each client has a different number and type of feature'. However, the environment that each client has a different number of type and type of feature has been well-explored in prior work like [1, 2].\n\n[1] Federated foundation models on heterogeneous time series. AAAI 2025.\n\n[2] Tackling data heterogeneity in federated time series forecasting. arXiv 2024.\n\n**W2.** Regarding the author's claim on Line 102, distributing from a single dataset into clients carrying varying numbers of features is less challenging than ***W1*** because they are to some extent homogeneous. Therefore, the term “limited research” here does not imply that the topic was unexplored due to its difficulty, but rather that further exploration is unnecessary.\n\n**W3.** Section 2.2 is ambiguous. There is a wrong citation in Line 127 (i.e., FedLECYu et al. (2025)).\n\n**W4.** The notation in Algorithm 1 and Algorithm 2 is confusing and difficult to understand.\n\n**W5.** FedQAPer is a fully incremental effort, largely built upon existing methods such as FedPer's personalization mechanism and existing deep forecasting models. The author's description of FedQAPer in Section 3 inevitably leans heavily toward engineering details, resembling more of a course assignment.\n\n**W6.** The experimental section is too weak and insufficient: \n\n* Only two datasets were considered, and the configuration was not optimal. Since each variable in Traffic and Electricity represents data from a single site, the authors' partitioning strategy (i.e., assigning different numbers of sites to a client to simulate heterogeneity) should have prompted them to first understand the dataset characteristics.\n* It is unclear what the purpose of Table 1 is, as it only presents a comparison of various existing deep learning models within the FedQAPer framework, while the section highlighting FedQAPer's effectiveness (superiority) is absent (i.e., comparison with other FL/PFL algorithms).\n* Framework analysis and hyperparameter analysis are missing."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AFWNtz2A8I", "forum": "eGwRxSuApK", "replyto": "eGwRxSuApK", "signatures": ["ICLR.cc/2026/Conference/Submission13061/Reviewer_QtAY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13061/Reviewer_QtAY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892754717, "cdate": 1761892754717, "tmdate": 1762923790692, "mdate": 1762923790692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents FedQAPer, a federated learning framework aimed at addressing feature heterogeneity among clients in time-series forecasting. It introduces a Query Attention Pooling module that maps each client’s local features into a shared latent space, enabling collaborative training of a common model while maintaining client-specific personalization heads, similar to FedPer. The framework is evaluated on the Electricity and Traffic datasets using multiple backbone architectures, including iTransformer, TimeMixer, DLinear, SpikeRNN, and Spikeformer. The results indicate that FedQAPer enhances both global and personalized model performance across all tested configurations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Effectively tackles a key challenge in federated learning: managing clients with heterogeneous feature spaces, an aspect often overlooked in previous studies.\n\nThe proposed approach is simple and adaptable, making it easily integrable with various backbone architectures."}, "weaknesses": {"value": "•The method mainly combines existing ideas like feature projection and personalized heads. It feels close to FedPer/FedProx with an added pooling layer. The authors do not clearly explain what new insight QAP offers beyond a simple shared linear projection.\n\n•The paper doesn’t compare against straightforward baselines such as zero-padding, global MLP projection, or other feature-alignment FL methods. This makes it unclear whether QAP itself brings any real benefit.\n\n•Unclear aggregation of metrics: its written in the paper that “Each client predicts a subset of features,” but still reports a single global MAE and MSE across all clients. It’s not explained how these are computed fairly when clients have different targets.\n\n•In Table 5, when Fmax increases from 20 to 30, the iTransformer MAE suddenly drops from 0.3317 to 0.2057 — a huge jump that seems unrealistic. This raises a concern about a possible setup or data leakage issue.\n\n•SNN claims not supported by experiments. The paper states that “Spike-based architectures in FedQAPer yield energy-efficient communication and faster inference” (Section 4.4), but no energy or latency results are provided. Without such experiments, these claims are not validated.\n\n•The method involves transmitting shared parameters from QAP, but the paper doesn’t analyze communication overhead or possible privacy leakage.\n\n•The proposed model feels like an extension of FedPer with attention-based pooling. It lacks deeper analysis or strong theoretical justification."}, "questions": {"value": "How is QAP fundamentally different from using a shared linear layer or MLP to project each client’s features into a fixed dimension?\n\nDid you test with partial client participation instead of all clients joining every round?\n\nHow exactly do you compute the global MAE/MSE when clients have different output features?\n\nWhy does performance change so drastically in Table 5 when Fmax increases from 20 to 30? Could this be due to different data splits or leakage?\n\nDid you run experiments with multiple random seeds or client partitions to ensure stability?\n\nWhat is the computational cost of QAP on clients compared to FedPer without QAP?\n\nDoes sharing QAP parameters risk leaking feature-level information from clients? Any privacy considerations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cYtukEUn6y", "forum": "eGwRxSuApK", "replyto": "eGwRxSuApK", "signatures": ["ICLR.cc/2026/Conference/Submission13061/Reviewer_PcMa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13061/Reviewer_PcMa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909029028, "cdate": 1761909029028, "tmdate": 1762923790360, "mdate": 1762923790360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FedQAPer, a novel federated learning framework for time-series forecasting designed to handle feature heterogeneity, where clients have different input features. The method uses Query Attention Pooling (QAP) to project heterogeneous client features into a unified space, enabling collaborative training of a shared backbone model. It combines this with the FedPer algorithm, which uses personalised heads on each client to map the unified representations back to their original feature dimensions, capturing local patterns. Experiments show FedQAPer is compatible with both ANN and SNN backbones and effectively improves forecasting performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The feature-level non-IID problem is a core challenge in real-world FL deployments (especially involving heterogeneous sensor networks). Research on this specific problem is still relatively limited, making the paper's focus highly valuable.\n2. The solution proposed by FedQAPer is intuitive and sound. Using a learnable module (QAP) to achieve dimensional alignment, combined with FedPer for personalisation, is a logical path to solving this problem.\n3. he design of QAP considers robustness. In addition to cross-attention, it introduces a \"side-channel fusion\" of statistical features (mean, max). This is a good design choice to prevent the attention mechanism from becoming unstable or degenerating when the number of features is very small."}, "weaknesses": {"value": "1. In the ablation study (Table 2), the authors compare FedPer (i.e., FedQAPer) against \"Local-only\" (clients trained independently). The authors claim FedPer \"consistently outperforms\" local-only training. However, the data in Table 2 clearly show that for the Electricity dataset, the \"Local\" versions of both TimeMixer and Spikeformer performed better (lower error) than the \"Fed\" versions on both MSE and MAE metrics. \nTimeMixer (Electricity): Fed (MSE 0.220, MAE 0.325) vs Local (MSE 0.165, MAE 0.276).\nSpikeformer (Electricity): Fed (MSE 0.232, MAE 0.339) vs Local (MSE 0.228, MAE 0.325).  \nThis indicates that, in these cases, the federated collaboration (knowledge sharing) actually led to negative transfer, harming model performance. This finding severely undermines the paper's core argument that FedQAPer provides a clear benefit over isolated training. The authors must provide a deep analysis and explanation for this phenomenon.\n2. The main results (Table 1) only show the performance of different backbones under the FedQAPer framework. It does not compare FedQAPer as a holistic method against other federated learning algorithms."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rNOWGtvQVO", "forum": "eGwRxSuApK", "replyto": "eGwRxSuApK", "signatures": ["ICLR.cc/2026/Conference/Submission13061/Reviewer_5jVQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13061/Reviewer_5jVQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928902516, "cdate": 1761928902516, "tmdate": 1762923789959, "mdate": 1762923789959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FedQAPer, a federated learning framework for non-IID time-series forecasting where clients have heterogeneous feature dimensions. It extends FedPer by adding a Query Attention Pooling (QAP) module that projects each client's features into a shared latent space for dimension alignment before federated aggregation, while keeping personalized output heads. The method is evaluated under different ANN (iTransformer, TimeMixer, DLinear) and SNN (SpikeRNN, Spikeformer) backbones on the Electricity and Traffic datasets. However, the experiments appear to present only FedQAPer's own results, with no baseline comparisons or ablations, making it challenging to assess relative performance or the actual benefit of QAP."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The work addresses a relatively understudied challenge in federated learning: heterogeneity in feature dimensions across clients, which is distinct from the usual label or distributional non-IID settings. The proposed Query Attention Pooling (QAP) mechanism is a conceptually simple and general idea that could, in principle, be applied to other settings as well. The paper is well written and organized, with clear diagrams explaining the role of QAP and personalized heads in the pipeline."}, "weaknesses": {"value": "The paper reports only FedQAPer's own results across backbones, with no quantitative comparisons against established federated baselines, making it challenging to judge its relative performance.\n\nThere is no evaluation isolating the effect of the Query Attention Pooling (QAP) module, so its actual contribution is unclear.\n\nThe method is a somewhat incremental architectural extension of FedPer, reusing its shared-backbone and personalized-head structure with a minor attention-based preprocessing layer.\n\nThe paper provides no formal/theoretical reasoning showing that QAP achieves meaningful feature alignment across clients."}, "questions": {"value": "How does FedQAPer quantitatively differ from FedPer? Can you show a side-by-side comparison or ablation to isolate the impact of the Query Attention Pooling?\n\nWhy are no baseline results included? Without benchmarking, how should readers interpret the claimed improvements?\n\nHow is feature heterogeneity simulated? Are the random feature splits representative of real-world multi-sensor or multimodal setups?\n\nCan you provide any empirical evidence of alignment (e.g., embedding similarity or visualization) to confirm that QAP actually harmonizes client feature spaces?\n\nWhat is the communication and computational overhead introduced by QAP compared to FedPer?\n\nDoes the framework generalize to partial feature overlap or clients with entirely disjoint features?\n\nCould QAP be integrated into other personalized FL schemes or is it somehow specific to FedPer-style architectures?\n\nHave you considered any real-world non-synthetic datasets where feature heterogeneity naturally occurs (e.g., multi-site sensor networks or IoT data)?\n\nWhy does the validation loss across QAP dimensions using the iTransformer backbone jumps so suddenly for d_QAP = 256?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gAmkmz7F6g", "forum": "eGwRxSuApK", "replyto": "eGwRxSuApK", "signatures": ["ICLR.cc/2026/Conference/Submission13061/Reviewer_wiXn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13061/Reviewer_wiXn"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission13061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762025582818, "cdate": 1762025582818, "tmdate": 1762923789564, "mdate": 1762923789564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}