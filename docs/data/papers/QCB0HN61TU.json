{"id": "QCB0HN61TU", "number": 16954, "cdate": 1758270552333, "mdate": 1759897208045, "content": {"title": "Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs", "abstract": "Video Large Language Models (VideoLLMs) extend the capabilities of vision-language models to spatiotemporal inputs, enabling tasks such as video question answering (VideoQA). Despite recent advances in VideoLLMs, their internal mechanisms on where and how they extract and propagate video and textual information remain less explored. In this study, we investigate the internal information flow of VideoLLMs using mechanistic interpretability techniques. Our analysis reveals consistent patterns across diverse VideoQA tasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frame interactions in early-to-middle layers, (2) followed by progressive video-language integration in middle layers. This is facilitated by alignment between video representations and linguistic embeddings containing temporal concepts. (3) Upon completion of this integration, the model is ready to generate correct answers in middle-to-late layers. (4) Based on our analysis, we show that VideoLLMs can retain their VideoQA performance by selecting these effective information pathways while suppressing substantial amount of attention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide a blueprint on how VideoLLMs perform temporal reasoning and offer practical insights for improving model interpretability and downstream generalization.", "tldr": "This paper presents a systematic analysis of where and how information flows in VideoLLMs for temporal reasoning in VideoQA, revealing key patterns and effective pathways.", "keywords": ["Video Large Language Models", "Information Flow Analysis", "Video Question Answering"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ef84c9fd68e9bf761f1c7fffda9c0c5c39a6929e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper conducts a mechanistic interpretability study to uncover how Video Large Language Models (VideoLLMs) internally process temporal information for video question answering (VideoQA). Using techniques such as Attention Knockout and Logit Lens, the authors trace how video and text signals propagate across model layers. They identify a consistent three-stage information flow pattern: (1) temporal reasoning begins with active cross-frame interactions among video tokens in early-to-middle layers; (2) video-language integration emerges in middle layers as video representations align with linguistic embeddings tied to temporal concepts; and (3) answer generation occurs in middle-to-late layers once integration is complete. Additionally, the authors show that retaining only the identified “effective information pathways” preserves VideoQA performance while suppressing up to 58% of attention edges in LLaVA-NeXT-7B-Video-FT, demonstrating that these pathways are sufficient for temporal reasoning. The study further validates that these patterns generalize across model sizes, architectures, and both multiple-choice and open-ended question settings."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This paper tackles an important yet under-explored problem: how spatiotemporal information is internally processed and propagated across layers in VideoLLMs.\n- The experimental design is rigorous and well thought out, making effective use of mechanistic interpretability techniques such as Attention Knockout, Logit Lens, and attention map visualizations.\n- The study successfully identifies the “effective information pathways” that underpin temporal reasoning in VideoLLMs, providing valuable insights into their inner workings.\n- The paper is clearly written, logically structured, and easy to follow."}, "weaknesses": {"value": "- The paper does not clearly articulate how its findings can be leveraged to guide the future development or improvement of VideoLLMs, limiting the practical implications of the insights.\n- Most analyses are conducted on a single benchmark (TVBench), which may constrain the generalizability of the conclusions. Incorporating additional video temporal reasoning benchmarks (e.g., [1,2,3,4,5]) would strengthen the robustness of the results.\n- The attention map visualizations in Fig. 5 are only presented for a single task. Including examples from all five task types listed in Table 1 would provide a more comprehensive understanding of the observed behaviors.\n- The paper overlooks several relevant studies on video temporal reasoning benchmarks [1,2,3,4,5] and related methodological advances [6,7].\n\n[1] Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis.\n\n[2] TempCompass: Do Video LLMs Really Understand Videos?\n\n[3] MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models.\n\n[4] Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos.\n\n[5] TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models.\n\n[6] Temporal Preference Optimization of Large Multimodal Models.\n\n[7] Temporal Reasoning Transfer from Text to Video."}, "questions": {"value": "- If we perform the same analysis on non-temporal video understanding tasks (e.g., \"What is the main object presented in the video?\"), what would the information flow be like?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lUWrlY0BVo", "forum": "QCB0HN61TU", "replyto": "QCB0HN61TU", "signatures": ["ICLR.cc/2026/Conference/Submission16954/Reviewer_5oZh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16954/Reviewer_5oZh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16954/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761481735769, "cdate": 1761481735769, "tmdate": 1762926975874, "mdate": 1762926975874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the internal mechanisms of Video Large Language Models (VideoLLMs) on VideoQA tasks using mechanistic interpretability techniques. By applying tools such as Attention Knockout and Logit Lens, the authors identify a consistent three-stage information flow: (1) cross-frame interactions among video tokens in early-to-middle layers, (2) video-language integration onto temporal keywords in the middle layers, and (3) answer generation in the middle-to-late layers. The authors validate these \"effective pathways\" by demonstrating that a pruned model, which retains only these key connections (e.g., 42% of edges), largely maintains its baseline performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a critical and under-explored area. As VideoLLMs grow in capability, understanding how they perform temporal reasoning (their internal mechanism) is as important as if they can (their performance). Applying established interpretability techniques from text/images to the spatiotemporal domain is a valuable and logical contribution.\n2. The paper's main contribution is a clear \"blueprint\" of the temporal reasoning process in VideoLLMs (Fig. 1). The identified 3-stage pathway—cross-frame interaction, integration, and generation—is intuitive and provides a strong conceptual framework for the research community."}, "weaknesses": {"value": "1. Selection bias in analysis: The analysis is exclusively performed on samples where the model outputs the correct answer. This introduces a selection bias, as the study only explains the mechanism of successful reasoning. A complete mechanistic understanding should also include a failure-mode analysis. It is unclear if model errors are due to a breakdown in these same pathways (e.g., failed video-language integration) or if they follow entirely different, pathological pathways.\n2. Limited Temporal Scope of Inputs: The entire analysis is conducted on models processing only 8 frames of video. This is a short temporal window, and the findings may not generalize to more complex, long-form video reasoning.\n3. Reliance on Multiple-Choice Format: The paper's core findings—such as the convergence of information onto the \"true option\" tokens —may be an artifact of the multiple-choice task format itself. This format steers the model toward a \"verification\" task (i.e., selecting the best option) rather than a \"generative\" one. This makes it unclear if the identified pathway is fundamental to temporal reasoning or just a learned shortcut for solving MC-QA. While the Appendix (SC) attempts to address this with an open-ended analysis, this analysis is limited, as it only includes three of the five tasks, potentially omitting more complex reasoning scenarios."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZXBCsoehTC", "forum": "QCB0HN61TU", "replyto": "QCB0HN61TU", "signatures": ["ICLR.cc/2026/Conference/Submission16954/Reviewer_trVM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16954/Reviewer_trVM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16954/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977406401, "cdate": 1761977406401, "tmdate": 1762926975457, "mdate": 1762926975457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on uncovering the internal information flow mechanisms of VideoLLMs for VideoQA tasks. Using interpretability techniques (e.g., Attention Knockout), it analyzes how VideoLLMs extract and propagate spatiotemporal and textual information across layers and modalities. It identifies consistent temporal reasoning patterns across diverse VideoQA tasks and validates the sufficiency of \"effective information pathways\" for maintaining model performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Overall, this paper is well written.\n\n1. This paper pioneers using mechanistic interpretability (Attention Knockout, Logit Lens) to unpack internal information flow, which is an interesting and novel topic.\n\n2. Multiple models (LLaVA-NeXT-7B/13B, Mini-InternVL-4B, VideoLLaMA3-7B) and benchmarks (TVBench, TOMATO, LongVideoBench) are tested, with validations for scalability and generalization, ensuring reliability."}, "weaknesses": {"value": "The paper observes that spatial concepts emerge in very early layers and temporal concepts in middle layers (via Logit Lens analysis on video tokens) but fails to probe the underlying causes. For instance, it lacks ablations on critical factors directly tied to this phenomenon: visual encoder design (e.g., CLIP-ViT-L-336px in LLaVA-NeXT models vs. SigLIP in VideoLLaMA3-7B, as noted in Table C) and the temporal information density of video fine-tuning data—both of which could explain why concept emergence timelines differ. Deeper investigation into these root factors would significantly strengthen the persuasiveness of its findings on concept emergence patterns."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fjDkKivJyG", "forum": "QCB0HN61TU", "replyto": "QCB0HN61TU", "signatures": ["ICLR.cc/2026/Conference/Submission16954/Reviewer_wzb7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16954/Reviewer_wzb7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16954/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986662916, "cdate": 1761986662916, "tmdate": 1762926975146, "mdate": 1762926975146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a mechanistic interpretability study of Video Large Language Models (VideoLLMs) to understand how they perform temporal reasoning for video question answering (VideoQA). Using techniques like Attention Knockout and Logit Lens, the authors trace the flow of information through the model's layers. They identify a consistent, multi-stage process where the model does temporal encoding, followed by video-language integration and finally answer formulation. \nTo validate their findings, the authors demonstrate that by preserving only these identified \"effective information pathways\" and pruning a substantial portion of the attention edges (e.g., 58% in LLaVA-NeXT-7B), the model's performance on VideoQA benchmarks remains largely intact, whereas random pruning of the same number of edges causes a significant performance drop. The study's findings are shown to be consistent across different model architectures and scales."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "**Important and Timely Problem**: While VideoLLMs are advancing rapidly, their internal reasoning mechanisms are poorly understood. There is a large body of work done on LLMs, and somewhat less so for Vision Language Models (VLMs) (although recently has gathered much attention). This paper addresses a critical gap by providing one of the first mechanistic analyses of temporal reasoning in these models, moving beyond the more common studies on static image-based VLMs.\n\n**Rigorous and Causal Methodology**: The authors employ established and powerful interpretability techniques (Attention Knockout, Logit Lens). The use of Attention Knockout is particularly strong, as it allows for causal interventions, disabling specific information pathways and measuring the direct impact on output probabilities. This provides much stronger evidence than purely observational methods like attention visualization.\n\n**Clear Narrative and Strong Empirical Support**: The paper presents a compelling and easy-to-follow \"blueprint\" of information flow. Each stage of the proposed process is backed by targeted experiments. For instance, the comparison between image- and video-tuned models clearly demonstrates the emergence of cross-frame reasoning (Figure 2), and the analysis of information flow to question vs. option tokens provides a nuanced view of integration points (Figures 6 & 7).\n\n**Thoroughness and Generalization**: The authors validate their findings across multiple modern VideoLLMs (LLaVA-NeXT 7B & 13B, Mini-InternVL-4B, VideoLLaMA3-7B) and datasets (TVBench, TOMATO). This demonstrates that the identified information flow patterns are a general property of current VideoLLMs rather than an idiosyncrasy of a single model."}, "weaknesses": {"value": "**Reliance on Attention Knockout**: The primary evidence for this model of information flow is based on attention knockout, which is conceptually sound. However, this makes the takeaways prone to method-specific idiosyncrasies, so having an experiment to validate the conclusions which isn’t knocking out pathways for effective flow would strengthen the paper significantly. \n\nFor example, complex reasoning might involve more iterative processes, where later layers revisit or re-contextualize information from earlier layers (e.g., text representations directing further attention to specific video frames). The current knockout methodology, which blocks entire pathways across layers, may not capture such dynamic or recursive patterns. Blocking i->j for some layers but not all risks leaking alternative pathways of obtaining such information, which captures a flawed picture of what is happening. One example of this is Table 2, where a total cross-frame attention knockout from L1-16 could be extended to looking at the L1-N, N=1,...,32, to see when the integration is important until. \n\n**Limited Scope of Tasks Format for Reasoning**: The analysis is primarily focused on multiple-choice VideoQA on the TVBench benchmark, which features relatively short videos and structured questions. It is unclear if this clean, sequential \"blueprint\" holds for more complex, long-form video tasks such as summarization, script generation, or open-ended dialogue, where temporal relationships are more complex and reasoning is less anchored to specific keywords. How are current pathways measured for multiple output token generations? Could such a method hold up for longer generations?\n\n**Coarse-Grained Analysis of Representations**: The paper successfully identifies where and when information flows (i.e., which layers and between which token groups) but offers less insight into the substance of the information itself. For example, while Logit Lens shows the emergence of \"temporal concepts\" as vocabulary items, it doesn't reveal what features in the hidden states (e.g., motion, object state change, event boundaries) constitute these concepts. The analysis remains at the level of token interactions rather than diving into the underlying neural representations. The logit lens analysis could also be used to understand the spatial distribution of how the model is distributing attention, much like previous VLM works [1,2]. Something to this effect as an addition to this paper would be significant. \n\n[1] Jiang et. al. Interpreting and Editing Vision-language Representations to Mitigate Hallucinations, ICLR 2025. \n\n[2] Chen et. al., Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas, ICML 2025."}, "questions": {"value": "* The analysis convincingly shows that the \"true option\" token is a key integration point in multiple-choice QA. In a purely open-ended generation task without explicit options, where does this critical video-language integration converge? The paper suggests it shifts to the last token, but is the process as effective and localized without the explicit semantic anchors provided by the options?\n* The pruning experiment is performed post-hoc on a trained model. Do you think these findings could be leveraged during training? For instance, could a regularization technique that encourages this sparse, efficient information flow lead to more robust, generalizable, or computationally efficient VideoLLMs? For example, if visual integration is important to occur early (as also observed in [3]) and is useful, can the embeddings be fed back in earlier? \n* How do you hypothesize these pathways would adapt to videos where the crucial temporal information is not uniformly distributed, such as in \"needle-in-a-haystack\" tasks common in long-video understanding? Would the cross-frame interactions in early layers become more dynamic or would the model rely on different mechanisms entirely?\n* The study focuses on temporal reasoning. Did you perform any analysis on tasks that are purely spatial (e.g., \"what color is the object on the left throughout the video?\") to see if the information pathways differ significantly, perhaps by bypassing the cross-frame interaction stage (see above comment on spatial distribution)?\n* In Figure 1(b), why does Question -/-> Last have an increase in probability for the later layers?\n\n[3] Nikankin et. al., Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs. arXiv 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JZF4gtGhd6", "forum": "QCB0HN61TU", "replyto": "QCB0HN61TU", "signatures": ["ICLR.cc/2026/Conference/Submission16954/Reviewer_NrDF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16954/Reviewer_NrDF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16954/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762746059373, "cdate": 1762746059373, "tmdate": 1762926974691, "mdate": 1762926974691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}