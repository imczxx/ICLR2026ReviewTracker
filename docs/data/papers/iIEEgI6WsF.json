{"id": "iIEEgI6WsF", "number": 17378, "cdate": 1758275237641, "mdate": 1759897178868, "content": {"title": "Revisiting Parameter Server in LLM Post-Training", "abstract": "Modern data parallel (DP) training favors collective communication over parameter servers (PS) for its simplicity and efficiency under balanced workloads. However, the balanced workload assumption no longer holds in large language model (LLM) post-training due to the large variance in sequence lengths. Under imbalanced workloads, collective communication creates synchronization barriers, leading to under-utilization of devices with smaller workloads. This change in training dynamics calls for a revisit of the PS paradigm for its robustness to such imbalance. We propose **On-Demand Communication (ODC)**, which adapts PS into Fully Sharded Data Parallel (FSDP) by replacing collective all-gather and reduce-scatter with direct point-to-point communication. Compared to FSDP, ODC reduces the synchronization barrier from once per layer to once per minibatch and decouples the workload on each device so that faster workers are not stalled. It also enables simpler and more effective load balancing at the minibatch level. Across diverse LLM post-training tasks, ODC consistently improves device utilization and training throughput, achieving up to a 36\\% speedup over standard FSDP. These results demonstrate that ODC is a superior fit for the prevalent imbalanced workloads in LLM post-training. We will open-source our implementation at camera-ready version.", "tldr": "We transform FSDP (ZeRO3) into a decentralized parameter server with on-demand communications, which relax the synchronization barrier from layer level to minibatch level, and achieves up to 36% acceleration on various LLM post-training settings.", "keywords": ["Distributed Training", "ZeRO Optimizer", "FSDP", "Parameter Server"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/18658b2106a041c1faf9c8bd0680d46ecd18b4e2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper analyzes synchronization inefficiencies in Fully Sharded Data Parallel (FSDP) training when applied to LLM post-training workloads characterized by heterogeneous sequence lengths and constrained microbatch sizes. The core claim is that per-layer collective communication (all-gather in forward/backward and reduce-scatter in backward) introduces fine-grained synchronization barriers, causing straggler-induced idle bubbles. This assumption of balanced workloads holds in vision and speech models but fails in long-context supervised fine-tuning and RL workloads, where compute scales quadratically with sequence length (attention) while memory scales linearly.\nThe proposed method decomposes collectives into point-to-point RDMA gather and scatter-accumulate primitives, enabling asynchronous parameter fetch and gradient push at the granularity of microbatch availability. The authors demonstrate that this transformation turns FSDP into a decentralized parameter-server architecture that retains ZeRO-style memory partitioning while removing per-layer synchronization points. Synchronization is preserved only once per minibatch to maintain identical global optimizer semantics.\nEvaluations on SFT (LongAlign and SWE-Smith datasets) and RL (AIME prompts with GRPO) across model scales 1.5B–32B show up to 36% throughput improvement and strong correlation with predicted imbalance (bubble rate)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Precise identification of bottleneck in sharded data parallelism. The per-layer max-runtime aggregation inherently propagates worst-case slowdowns, validated with measurements showing up to 50% idle bubbles.\n- The introduced method preserves DP correctness constraints: a) parameter, gradient, optimizer sharding identical to FSDP. b) deterministic synchronous update at minibatch boundaries, and c) No model semantic change.\n- Lightweight integration: <100 LOC modification using CUDA-IPC + NVSHMEM RDMA via Triton-Distributed."}, "weaknesses": {"value": "-  Benchmarking shows significant bandwidth degradation vs NCCL rings when D > G.\nInter-node performance constraints remain unresolved in this work.\n- The degree of compute–communication overlap is asserted but not rigorously quantified (e.g., timeline traces, GPU utilization per stage). This limited control-flow characterization does not give much confidence to the reader on the effectiveness in overlapping.\n- Comparison set omits strong baselines. There is no experimentation vs ZeRO++ hierarchical collectives, scheduling-based overlap strategies, or pipeline-staleness variants.\n- RL system constraints to some extend weaken conclusions. To be specific, forcing equal samples per device, reduces the effectiveness of proposed minibatch-level packing (LB-Mini).\n- Claims of benefits growing with scale lack large-cluster support are not supported by the results since the scaling experiments are not extended to >32 GPUs"}, "questions": {"value": "- What is the asymptotic slowdown of ODC communication when inter-node degree grows? Can you provide a model predicting crossover vs collectives?\n- Does the minibatch-level synchronization impose idle bubbles in the best-case balanced regime? If so, is dynamic minibatch staggering feasible?\n- In RL, how would delayed optimizer updates or local advantage normalization interact with async shard availability?\n- What is the concurrency behavior of the gradient accumulation daemon at high client cardinality? Any queue saturation or tail latency concerns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KHT2nj6a4s", "forum": "iIEEgI6WsF", "replyto": "iIEEgI6WsF", "signatures": ["ICLR.cc/2026/Conference/Submission17378/Reviewer_sQnr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17378/Reviewer_sQnr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559534905, "cdate": 1761559534905, "tmdate": 1762927287585, "mdate": 1762927287585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all reviewers for their feedback on our paper. Please find our responses to individual comments below."}}, "id": "rCwKPTN7CP", "forum": "iIEEgI6WsF", "replyto": "iIEEgI6WsF", "signatures": ["ICLR.cc/2026/Conference/Submission17378/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17378/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17378/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763364158697, "cdate": 1763364158697, "tmdate": 1763364158697, "mdate": 1763364158697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed on-demand-communication, which separate worker and parameter sever as old-fashion PS."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. good idea to call back to parameter server. Personally I like this call-back.\n2. clear presentation and experimental results"}, "weaknesses": {"value": "1. With PS, one major issue is we have inconsistency of parameters (i.e. parameter delay). For example, two GPUs , GPU0 pull the weights before some xyz updates, then GPU1 pull the same weights with xyz updates, then they are training on different weights but same iteration number, This parameter inconsistency will hurt model convergences. There is no discussion on model convergence in the whole paper.\n\n2. Indeed, FSDP/ZeRO they already incorporated async parameter pre-fetch, hiding weights updates behind OS (optimizer states) updates etc. This communication hiding mechanism works pretty well and I don't see the real motivation for deleting all the global collective into pair-wise p2p communication. The statement of this paper in L134 \"but this overlap does not remove the underlying synchronization points.\" is wrong, because, every GPU can pre-fetch a number of layers weights all together at once, they don't need to wait for each other at every layer's fwd or bwd\n\n3. In real world post-training, we do either batching or padding to make every GPU has same sequence length of input, thus the straggler problem this paper try to solve does not exists in real-world application scenarios."}, "questions": {"value": "Try to profile some real world workload, and real-world system setting (e.g., how to do data loading, data preprocessing (padding, batching), distributed FSDP+prefetch, LoRA, etc), for post training is a good way to find where is the real bottleneck."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WvQYmKLnvy", "forum": "iIEEgI6WsF", "replyto": "iIEEgI6WsF", "signatures": ["ICLR.cc/2026/Conference/Submission17378/Reviewer_7zfy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17378/Reviewer_7zfy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761614831959, "cdate": 1761614831959, "tmdate": 1762927287263, "mdate": 1762927287263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical limitation of modern data-parallel training (e.g., FSDP) in LLM post-training, where variable sequence lengths cause severe workload imbalance and render collective communication inefficient due to strict synchronization barriers. The authors propose ODC, a novel adaptation of the PS paradigm within FSDP that replaces collective all-gather/reduce-scatter with point-to-point communication. By decoupling device synchronization to the minibatch level and enabling asynchronous gradient exchange, ODC eliminates workload-induced stalls and facilitates fine-grained load balancing. Experiments across diverse LLM post-training tasks show consistent gains in device utilization and throughput, demonstrating that PS-style communication is not obsolete, but better suited to real-world LLM training dynamics. The approach is practical and novel."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Novelty: The proposed method adapts the parameter server concept into the FSDP framework by replacing collective all-gather/reduce-scatter operations with point-to-point communication, thereby reducing synchronization overhead.\n- Practicality: The authors implemented ODC and provided extensive experimental results demonstrating its reliability.\n- Readability: The paper is well-structured and easy to follow."}, "weaknesses": {"value": "No obvious drawbacks."}, "questions": {"value": "- Under the setting of activation recomputation, is ODC still applicable?\n- What are the potential limitations of ODC compared to FSDP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NnCSydLbrX", "forum": "iIEEgI6WsF", "replyto": "iIEEgI6WsF", "signatures": ["ICLR.cc/2026/Conference/Submission17378/Reviewer_ycWJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17378/Reviewer_ycWJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831447628, "cdate": 1761831447628, "tmdate": 1762927286905, "mdate": 1762927286905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes On-Demand Communication (ODC), which removes per-layer synchronization barrier from training LLM in FSDP. Because of the imbalanced workload in post-training, with a simplified load balancing strategy, ODC helps speedup over FSDP by replacing collective calls with P2P communication."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well-motivated and straightforward to implement.\n- An excellent complement to FSDP that only requires changing communication operators—theoretically and experimentally equivalent (minor precision differences from batch size variations have minimal impact)."}, "weaknesses": {"value": "The practical applications of ODC may be quite limited, requiring specific scenarios with load imbalance. In SFT, the paper only tested LongAlign and SWE-Smith; in RL, updating the actor is not the main bottleneck, and currently, partial-rollout or fully asynchronous training are more commonly used to improve overall system throughput."}, "questions": {"value": "Can ODC extend to support MoE models in FSDP? Though not efficient as expert parallel method, it might be better than the standard FSDP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vqi35qfnR7", "forum": "iIEEgI6WsF", "replyto": "iIEEgI6WsF", "signatures": ["ICLR.cc/2026/Conference/Submission17378/Reviewer_xMDa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17378/Reviewer_xMDa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880934301, "cdate": 1761880934301, "tmdate": 1762927286572, "mdate": 1762927286572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}