{"id": "aPeeUApKtW", "number": 6642, "cdate": 1757991144718, "mdate": 1759897903285, "content": {"title": "SWE-eval: Trajectory-Enhanced Evaluation for Agentic Issue Resolution", "abstract": "Agents and Language Models (LMs) demonstrate significant advancements in software engineering, particularly in issue resolution. Current benchmarks can qualitatively assess the correctness of generated patches. However, they lack mechanisms for quantitatively evaluating the trajectory, which is important to reveal the point of improvement. To obtain understanding of issue-resolving agents' working processes, we propose SWE-eval, a trajectory-augmented evaluation framework. SWE-eval additionally assesses a coding agent's reasoning trajectory across three dimensions: (1) Efficiency, measured by resource consumption; (2) Logical Consistency, where Intra-turns measures the logical consistency within a single turn and Inter-turns measures logical consistency across multiple conversation turns; (3) Tool Utilization, for which we design a metric Info-gain to assess how much new information the tool provides for solving problems. Our experiments on three agents and nine LMs demonstrate that SWE-eval effectively reveals underlying interpretations of agent performance and can guide development of more effective agents. First, our evaluations show that elevating trajectory-aware metrics is crucial for enhancing the % Resolved. Second, we trace divergent agent behaviors to shallow exploration, missing backtracking, and loop entrapment. We also show that fine-tuning on agents risks overfitting and scaling LMs improves trajectories. Third, LLM-based evaluations align closely with expert judgments and exhibit consistent stability, serving as reliable proxies.", "tldr": "", "keywords": ["Language models", "Natural language processing", "Software engineering"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/718d11ac06d7a23ec0e5d83c04712da246fa0434.pdf", "supplementary_material": "/attachment/14b3bffe06e0f9bef734c16f6131eea0d51173b9.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes SWE-eval, a trajectory-augmented evaluation for issue-resolving agents that scores (i) Efficiency (#tokens, #turns), (ii) Logical Consistency, and (iii) Tool Utilization. It also reports separability of resolved vs. unresolved cases, agent/LM case studies, and some inter-rater statistics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides quantitative analyses for each metric. Reliability analysis is also conducted: reports stability and human alignment (e.g., ICC and mean-diffs) for LLM-based scoring."}, "weaknesses": {"value": "Limited novelty; largely a packaging of known factors. Most components of the proposed “trajectory evaluation” (efficiency via #Tokens/#Turns, consistency via Inter-/Intra-turn, tool use via %Tool Success/Info-gain) are not new individually. The contribution reads as a consolidation of existing ideas applied to SWE-bench agent traces,.\n\nCorrelation to causation leap without interventions. The paper repeatedly infers “because success correlates strongly with trajectory-aware metrics, optimizing these metrics should improve %Resolved.” That is an unsubstantiated causal jump. There are no experiments, such as targeted modifications that improve a specific metric while holding others fixed to establish that moving the metric causes gains.\n\nUnclear practical utility beyond diagnosis. The paper emphasizes that the metrics “diagnose failure modes,” but it is not shown how practitioners should act on them to improve agents or benchmarks. Do these metrics guide dataset curation, agent retraining, or guardrail design better than simpler baselines? The application and downstream decisions are underspecified."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EELJe7T7cS", "forum": "aPeeUApKtW", "replyto": "aPeeUApKtW", "signatures": ["ICLR.cc/2026/Conference/Submission6642/Reviewer_qqKf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6642/Reviewer_qqKf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760753666446, "cdate": 1760753666446, "tmdate": 1762918958951, "mdate": 1762918958951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SWE-eval, a trajectory-augmented evaluation framework that assesses coding agents across three dimensions: (1) Efficiency: resource consumption, (2) Logical Consistency: stuck-in-loop, intra-turn and inter-turn consistency, and (3) Tool Utilization: tool success rate and information gain. This paper evaluates three agents (SWE-agent, OpenHands, Moatless) with nine language models on SWE-bench-Lite and SWE-bench-Verified, demonstrating that trajectory metrics correlate with success rates and reveal distinct failure modes. LLM-based evaluators show strong alignment with human judgments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**1. Originality**: The three-dimensional framework propose new metrics, such as Info-gain metric and systematic trajectory evaluation that presents meaningful contributions.\n\n**2. Experiment Setup**: In addition to human validation, this work conducts experiments on three coding agents across nine models, supporting findings with experimental results. \n\n**3. Writing Clarity**: This work presents structured writing with clear motivation."}, "weaknesses": {"value": "**1. Limitations In Methodology**\n\n- **Info-gain validation**: The paper does not validate that Info-gain actually measures information gain vs. general response quality.\n- **Prompt sensitivity**: No ablation studies on how prompt engineering affects LLM judge scores.\n- **Human baseline insufficiency**: Only 3 experts, unclear sample size, and no raw inter-annotator agreement reported before consensus.\n\n**2. Limitations In Experiments**\n\n- **Insufficient case study**: Django-12700 may not be representative.\n- **Missing baselines**: No comparison to simpler heuristics, such as edit distance for loop detection, TF-IDF for Info-gain.\n- **Agent confounds**: Different agents use different tools, so trajectory metrics may conflate agent reasoning quality with tool choices, making it unclear whether observed differences (e.g., more turns, lower tool success) reflect inferior agent capability or merely architectural differences in how agents decompose tasks.\n- **Missing analyses:** Some critical analyses are missing, such as the error analysis of when trajectory metrics fail to predict outcome, the discussion of computational overhead introduced by trajectory evaluation, and actionable guidance on how to use metrics to improve agents.\n\n**3. Limitations In Generalizability:**\n\n- **Limited Language/task specificity**: All experiments on Python, but unclear if findings can generalize and transfer to other programming languages and SWE tasks.\n- **Benchmark contamination risk**: Models may have seen SWE-bench instances during training.\n- **Cost analysis missing**: LLM-based evaluation can be very expensive, but no cost-benefit analysis and use alternatives are provided."}, "questions": {"value": "My questions are following several aspects mentioned in weakness:\n\n- Can you provide evidence that Info-gain measures information rather than correlating with simpler features (response length, confidence, tool success)?\n- How sensitive are LLM judge scores to prompt variations?\n- Can you provide human evaluation details to validate the support, such as how many trajectories were annotated by human experts, the raw inter-annotator agreement (before consensus), why three human experts can sufficiently support the conclusion?\n- For the identified failure modes (shallow exploration, missing backtracking, loop entrapment), what percentage of failures does each account for?\n- What is the cost (time/money) of trajectory evaluation vs. patch-only evaluation? Is it practical for large-scale use?\n- Have you tested on non-Python repositories? How do your evaluation framework and findings transfer to other programming languages and SWE tasks?\n- Does tool utilization unfairly favor agents with more tools? How do you control for this confound?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GLxdps5ILv", "forum": "aPeeUApKtW", "replyto": "aPeeUApKtW", "signatures": ["ICLR.cc/2026/Conference/Submission6642/Reviewer_Cqe5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6642/Reviewer_Cqe5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761445189076, "cdate": 1761445189076, "tmdate": 1762918958157, "mdate": 1762918958157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SWE-eval, a trajectory-augmented evaluation framework for agentic software issue resolution that goes beyond patch correctness by measuring (i) Efficiency, (ii) Logical Consistency, and (iii) Tool Utilization, with experiments on SWE-bench-Lite/Verified showing diagnostic value and reasonable alignment with expert ratings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear, useful shift from outcome-only scoring to process-aware evaluation; the three axes and their concrete metrics form a coherent rubric that exposes failure modes such as shallow exploration, missing backtracking, and loop entrapment. \n\n- Compelling cross-agent/LM analyses on SWE-bench-Lite and Verified: associations between lower turns/tokens and higher %Resolved, and ICC-based comparisons that show LLM-judge scores correlate with experts (strong on Info-gain, moderate on Intra-turns). \n\n- Case studies and tables highlight concrete failure patterns (e.g., loop traps; oversized patches from mis-extracted files) and suggest design fixes for agents and benchmarks (loop breakers, backtracking, stricter patch extraction)."}, "weaknesses": {"value": "- Reliance on LLM-as-judge needs tighter validation. Alignment with experts is uneven (e.g., weaker for Inter-turns), and prompts/models are central to metric values. Please provide fuller prompt/aggregation details, rater independence (ensuring the judge is not the actor model), seed sensitivity, and calibration (e.g., z-score or temperature scaling against human anchors). Also report per-metric confidence intervals/bootstrap for each model/agent. \n\n- Loop detection is brittle. The exact-string hash for Stuck-in-Loop may miss semantically equivalent repetitions (minor rephrasings) or action-level loops (same tool invocations with different wording). Consider a semantic or action-trace criterion (normalized tool call + args; Levenshtein/embedding similarity thresholds). \n\n- Benchmark and reporting scope. Main results focus on SWE-bench-Lite/Verified with a 30-call cap; broader generalization (e.g., SWE-bench-Live, multilingual variants, industrial datasets) is mostly deferred to appendix. Include per-repo/domain stratification, cost/latency breakouts, and framework-robustness checks (same trajectories scored across different agent shells) in the main paper."}, "questions": {"value": "Which exact models/prompts were used for Inter-/Intra-turns and Info-gain, and were judges ever the same family as the actors? Could you report cross-judge variability, and a cross-model triangulation (e.g., swap in a very different judge) to show the conclusions persist?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lTbQdRk4d8", "forum": "aPeeUApKtW", "replyto": "aPeeUApKtW", "signatures": ["ICLR.cc/2026/Conference/Submission6642/Reviewer_yV4p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6642/Reviewer_yV4p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980002454, "cdate": 1761980002454, "tmdate": 1762918957513, "mdate": 1762918957513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new evaluation framework for SWE task, termed SWE-eval. It is a trajectory-augmented evaluation framework. It considers three parts, 1) efficiency, 2) logical consistency, and 3) tool utilization. The experiments on 3 agents and 9 LLMs demonstrate that the proposed evaluation can effectively reveal underlying interpretations of agent performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The topic and direction are practical. \n2. The codes are provided, enabling reproducibility.\n3. The motivation is clear, and the case studies are comprehensive."}, "weaknesses": {"value": "1. The sub-capture and the content in Figure 1 are inconsistency. \n2. It is not a new evaluation of the SWE task or a new format of the SWE task. It seems a normal analysis of the trajectory of the agent during the researcher pay effort on the SWE bench. \n3. The token efficiency is easy to detect and is a normal metric. And the Stuck-in-Loop problem seems common and can be avoided during the rollout stage.\n4. Instead of these common metrics, what about conducting evaluations on different sub-tasks of the SWE task, like file reading, bug localization, patch writing, etc.?\n5. In Table 1 and Table 3, please explain the meaning of the number with a green background.\n6. In Table 1, for Inter-turns, the difference between resolved and unresolved trajectories is not significant."}, "questions": {"value": "See Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rbhZiiDw2j", "forum": "aPeeUApKtW", "replyto": "aPeeUApKtW", "signatures": ["ICLR.cc/2026/Conference/Submission6642/Reviewer_5Ggm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6642/Reviewer_5Ggm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762229879812, "cdate": 1762229879812, "tmdate": 1762918957109, "mdate": 1762918957109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}