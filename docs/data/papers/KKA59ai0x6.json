{"id": "KKA59ai0x6", "number": 17633, "cdate": 1758278616221, "mdate": 1759897163634, "content": {"title": "Beyond Classification Accuracy:  Neural-MedBench and the Need for Deeper Reasoning Benchmarks", "abstract": "Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning.\nWe introduce \\texttt{Neural-MedBench}, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics.\nThrough systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings.\nOur findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI.", "tldr": "We introduce Neural-MedBench, a reasoning-intensive benchmark that exposes how state-of-the-art vision-language models fail at clinical diagnosis despite excelling on standard medical AI benchmarks.", "keywords": ["vision-language models", "benchmark dataset", "medical AI evaluation", "reasoning-intensive tasks"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bd598f1a73c9ab433794ed6223856331c1300314.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work provided Neural-MedBench, which is an expert-curated, neurology-focused benchmark that shifts evaluation from breadth to depth by testing multi-modal clinical reasoning, showing current VLMs chiefly fail for reasoning rather than perception despite strong shallow scores. However, there remain some unresolved issues, with single-judge reliance, opaque challenge validation, and unclear length normalization tempering the claims."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. High reasoning density: a depth-oriented benchmark with greater clinical relevance than shallow QA-style responses.\n2. The benchmark and items are curated by medical experts, which is more credible than LLM-generated or auto-converted content.\n3. Annotations are not single-point labels but structured, narrative explanations that mimic clinical reasoning, yielding high clinical value.\n4. Items are challenging and come with explicit difficulty tiers."}, "weaknesses": {"value": "1. The paper should specify which baseline model(s) were used for the pre-release “challenge validation,” including the exact prompts and their performance.\n\n2. Grading relies primarily on GPT-4o as the automatic judge; despite high correlation with clinicians, model-specific bias may persist. Please explain why a multi-model, anonymous voting scheme was not adopted.\n\n3. “Normalized in length to control for spurious effects of input size on model performance”: length-related performance is itself part of model capability. Why normalize for this task, and how exactly was it implemented (e.g., truncation, abstractive summarization, padding to a fixed length, specific upper/lower thresholds, etc.)?"}, "questions": {"value": "See the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rRpxBlRVRk", "forum": "KKA59ai0x6", "replyto": "KKA59ai0x6", "signatures": ["ICLR.cc/2026/Conference/Submission17633/Reviewer_NAbK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17633/Reviewer_NAbK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895749551, "cdate": 1761895749551, "tmdate": 1762927492949, "mdate": 1762927492949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "This paper has certain limitations."}, "comment": {"value": "1. The paper suffers from several fundamental methodological and conceptual flaws that severely undermine its credibility. The most critical issue lies in its evaluation pipeline, where GPT-4o is simultaneously used as a benchmarked model and as the primary grader for assessing reasoning fidelity. This creates a self-evaluation bias that compromises the objectivity of the entire study. Although the authors claim to mitigate this bias through clinician-in-the-loop calibration, such correlation analysis cannot guarantee fairness or true independence. As a result, the reliability of the reported performance gaps and the validity of the main conclusions are questionable.\n\n2. Another fatal limitation concerns the small scale of the dataset. Neural-MedBench includes only 120 cases and 200 reasoning tasks, which is insufficient to support the strong claim that breadth and depth are largely uncorrelated. The statistical power of such a limited sample is extremely weak, and the human baseline, consisting of only ten participants, is far too small to provide meaningful comparisons. Consequently, the empirical findings may simply reflect sampling noise rather than robust trends.\n\n3. A further concern is the lack of formalization of the proposed Two-Axis Evaluation Framework. The concepts of “breadth” and “depth” remain qualitative and vaguely defined, with no mathematical or reproducible formulation that allows others to validate or extend the framework. In its current state, it functions more as a conceptual narrative than a scientific model. Moreover, the causal interpretation of the results is not properly supported. The assertion that models fail mainly due to reasoning rather than perception relies entirely on subjective annotation and lacks controlled ablation or counterfactual testing. Other factors such as prompt design, domain knowledge, or token limits could equally explain the observed errors.\n\n4. In addition, the benchmark’s data composition raises ethical and legal concerns. Several sources cited, including Radiopaedia and ADNI, impose restrictions on redistribution or commercial use. The claim that the dataset will be openly released with a public leaderboard may therefore violate existing licensing agreements, jeopardizing the benchmark’s legitimacy. The reproducibility of the experiments is also doubtful, as the authors test heterogeneous model versions under a single prompting template without verifying cross-model fairness."}}, "id": "ae304sN94K", "forum": "KKA59ai0x6", "replyto": "KKA59ai0x6", "signatures": ["~Qiang_Li46"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Qiang_Li46"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17633/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762951740256, "cdate": 1762951740256, "tmdate": 1762951740256, "mdate": 1762951740256, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Neural-MedBench, a compact yet reasoning-intensive benchmark designed to evaluate clinical reasoning capabilities of vision-language models (VLMs) in neurology. The authors propose a \"Two-Axis Evaluation Framework\" distinguishing between Breadth (statistical generalization on large datasets) and Depth (reasoning fidelity on complex cases). The benchmark comprises 120 expert-curated neurology cases with 200 tasks spanning differential diagnosis, lesion recognition, and rationale generation. Evaluation of state-of-the-art VLMs (GPT-4o, Claude-4, MedGemma) reveals significant performance drops compared to breadth-oriented benchmarks, with error analysis showing reasoning failures (51%) dominate over perceptual errors (27%)."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written. \n2. The paper presents the Two-Axis (Breadth/Depth) evaluation framework provides a compelling perspective on AI evaluation that addresses a genuine gap in current medical AI benchmarking.\n3. The rigorous multi-stage expert curation process involving senior neurologists and neuroradiologists, with consensus review, ensures benchmark quality and clinical validity."}, "weaknesses": {"value": "1. The bechmark is small-scale benchmark compared with exists medical benchmark, such as OmniMedVQA, GMAIMMbench.\n2. Some medical VLM method need to added to compared，such as, Huatuo-vision, Lingshu. While, GPT-4o maybe a outdate model, gpt5 is welcome."}, "questions": {"value": "1. Prompt Sensitivity: Did you test the sensitivity of the evaluation prompts to variations in wording?\n2. Medical Student Performance: What explains the substantially lower performance of medical students compared to existing large models in Table 2?\n3. BERTScore Discrimination: The BERTScore differences in Table 2 appear minimal across models. Does this metric provide sufficient discriminative power?\n4. Senior Physician Baseline: Given the relatively low performance of senior physicians on the benchmark, were all questions validated by multiple experts to ensure correctness and clinical appropriateness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "opAStRiMtr", "forum": "KKA59ai0x6", "replyto": "KKA59ai0x6", "signatures": ["ICLR.cc/2026/Conference/Submission17633/Reviewer_RaHn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17633/Reviewer_RaHn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929605627, "cdate": 1761929605627, "tmdate": 1762927492486, "mdate": 1762927492486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Neural-MedBench, a focused and reasoning-intensive benchmark designed to evaluate the clinical reasoning capabilities of vision-language models (VLMs) in the neurology domain. This benchmark integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition,\nand rationale generation. A hybrid scoring pipeline is proposed for better evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "A two-axis evaluation framework is proposed, which can be complementary assessments of both breadth (statistical generalization) and depth (reasoning fidelity). Neural-MedBench is the first neurology-focused benchmark explicitly designed to operationalize the Depth axis, comprising 120 multimodal, expert-curated diagnostic cases with 200 reasoning-intensive tasks. Experimental results show that current VLMs fail primarily at reasoning, despite strong performance on existing large-scale datasets. A systematic error analysis and a human performance baseline are also provided."}, "weaknesses": {"value": "1. The definition of three difficulty levels is unclear. \n2. A limitation section should be added in main body."}, "questions": {"value": "1. The details of grader validation and validation process are missing. \n2. Some important VLMs are missing, such as Gemini 2.5 pro and GPT-5.\n3. What the different of Base VLMs and General VLMs? \n4. It is intersting to provide more discussion for the following case: Why the pass@1 acc of medical student is significant lower than VLMs? Why the pass@1 acc of medical student in Multi-round dialogue is significantly better than Direct diagnosis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d6BqcY6Pnf", "forum": "KKA59ai0x6", "replyto": "KKA59ai0x6", "signatures": ["ICLR.cc/2026/Conference/Submission17633/Reviewer_Kijm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17633/Reviewer_Kijm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984265036, "cdate": 1761984265036, "tmdate": 1762927491517, "mdate": 1762927491517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}