{"id": "PAIlHOJBzN", "number": 12092, "cdate": 1758205641466, "mdate": 1763701541693, "content": {"title": "H2IL-MBOM: A Hierarchical World Model Integrating Intent and Latent Strategy for Opponent Modeling in Multi-UAV Game", "abstract": "In mixed cooperative-competitive scenarios, the uncertain decisions made by agents on both sides not only render learning non-stationary but also pose significant threats to each other's security. Existing methods typically predict policy beliefs based on opponents' actions, goals, and rewards, or predict trajectories and intentions solely from local historical observations. However, the above private information is unavailable and they neglect the intrinsic dynamics between mental states, actions, and trajectories for both sides. Inspired by human cognitive processes, which hierarchically infer intentions from opponents' historical trajectories, reason about latent strategies based on teammates' historical responses, and simulate the co-evolution of mental states and trajectories, we propose a Hierarchical Interactive Intent-Latent-Strategy-Aware World Model-based Opponent Model (H2IL-MBOM) and the Mutual Self-Observed Adversary Reasoning PPO (MSOAR-PPO). These methods enable the recursive and hierarchical prediction of opponents' intentions, latent strategies, and trajectories, while establishing a dynamic co-adaptation loop between the world model and policy. Specifically, the high-level world model integrates observations relative to opponents and learns learnable multi-intention queries to predict future intentions and trajectories. It then incorporates these inferred intentions into the low-level world model, which uses them to predict how the opponents' learnable latent strategies react and influence the trajectories of cooperative agents. Our method achieves faster convergence and higher rewards than state-of-the-art model-free/based reinforcement learning and opponent modeling approaches in multi-UAV games, demonstrating strong scalability up to 10 vs. 10 with improved win and survival rates. Cumulative error analysis and t-SNE visualizations verify effective reasoning about opponents' multiple intentions and latent strategies. It also outperforms existing methods on StarCraft Multi-Agent Challenge and Google Research Football benchmarks across the majority of scenarios. The videos can be accessible in the supplemental materials.", "tldr": "", "keywords": ["Multi-UAV game", "Multi-agent reinforcement learning", "Opponent modeling", "World model"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da95badb6ac59273c4803d9449c67cf53b9227d4.pdf", "supplementary_material": "/attachment/356381293d45f8b9b97cac66a50a44ee442f872f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces H2IL-MBOM, a framework for opponent modeling designed to address non-stationarity in multi-agent adversarial environments.  The method's core is a hierarchical world model that mimics human cognitive processes by decomposing the complex task of reasoning about an opponent into two levels.  At a high level, the model infers an opponent's macro \"intention\" by analyzing their historical trajectories.  Subsequently, at a low level, it uses this inferred intention as a condition to deduce the specific \"latent strategy\" the opponent is employing, taking into account the reactions and movements of allied agents.  This framework is implemented through a complex neural architecture based on Transformers and Hypernetworks (HyperHD2TSSM) and is used to guide a PPO-based reinforcement learning agent.  The authors report that their method demonstrates superior performance compared to various baselines in several testbeds, including multi-UAV combat, the StarCraft Multi-Agent Challenge, and Google Research Football."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a novel approach to opponent modeling inspired by human cognition, decomposing the complex problem into a two-level hierarchy of high-level \"intentions\" and low-level \"latent strategies\". This provides a structured and theoretically-grounded new perspective for the field."}, "weaknesses": {"value": "1. This paper, in its current state, is difficult to accept. The core issue is not just a matter of style, but a fundamental lack of clarity in its presentation that prevents a proper scientific review. The manuscript is plagued by a host of minor yet cumulative errors that suggest a lack of care in its preparation. For instance, citations are not properly formatted (lacking \\citep or \\citet), leading to overlaps with the text. There are basic punctuation errors (e.g., missing periods on lines 199 and 218), inconsistent formatting (the acronym HJLGT is sometimes italicized, sometimes not), and poor typesetting (some words are hyphenated across lines between 249-269). Furthermore, the figures are poorly executed; the text in Figure 3 is very small, while the architectural diagrams in Figures 5 and 6 are so cluttered they seem designed more to showcase the model's complexity than to explain it. The overall writing quality, with its convoluted sentence structures and excessive jargon, resembles unedited text generated by a large language model, a practice that should be acknowledged if used.\n\n2. This poor presentation directly obscures the methodology. The main body of the paper has been effectively \"hollowed out,\" with critical information relegated to the appendix. For example, MSOAR-PPO is listed as a key contribution, but its mechanics are entirely absent from the main methodology section. Similarly, the dimensionality of the core latent variables for \"intention\" and \"strategy\" — a crucial implementation detail — is only found in a table in the appendix. The reader should not have to be a detective, piecing together the core method from scattered fragments. This forces a reviewer to question the paper's central claims. The entire framework rests on a rigid two-level hierarchy where \"intention\" dictates \"strategy,\" a strong cognitive assumption presented without justification. The paper also fails to provide evidence that the learned latents, $z_I$ and $z_L$, are actually disentangled. The t-SNE plots are insufficient as they only visualize clustering, not semantic meaning.  A more rigorous analysis, such as performing interventional experiments (e.g., fixing the \"intention\" latent while varying the \"strategy\" latent and observing the impact on generated trajectories), is needed to validate that these variables meaningfully represent their claimed concepts.\n\n3. The experimental evaluation is similarly unconvincing.  The decision to place the results on standard benchmarks (SMAC and GRF) in the appendix is a major flaw that undermines the paper's claims of generalizability;  these should be in the main paper.  The primary UAV experiment relies on comparisons against opponent modeling baselines (e.g., ROMMEO, PR2) that perform catastrophically.  Their complete failure strongly suggests a lack of proper hyperparameter tuning for this complex, continuous-control environment.  For a fair comparison, the authors must either provide evidence of a thorough tuning process for these baselines or include stronger, more suitable ones.  The ablation study, while showing that components are useful, does not justify the model's immense complexity.  The fact that simpler variants (e.g., the only_intentions model from Fig. 4a and especially the transformer_shareadd model from Fig. 4f, which performs nearly identically to the full model) are still effective raises a critical question about the cost-benefit trade-off.  The authors should provide a discussion justifying why the marginal performance gain of their full architecture warrants its significant complexity over these simpler, yet competent, alternatives."}, "questions": {"value": "1. he prior distribution for an agent's latent state (e.g., $p(z_{I,i,t}|...)$ on page 5) explicitly conditions on the latent states and actions of its neighbors ($z_{I,n_i,t-1}, a_{n_i,t-1}$). How is this neighbor information accessed or communicated between agents during execution, especially within what is described as a decentralized execution paradigm?\n\n2. Appendix A.9 states that the value function for MSOAR-PPO \"does not incorporate respective opponents' observations,\" distinguishing it from MAPPO. However, the policy is conditioned on these observations ($O_{opp,i,t}$). In a centralized training paradigm, why would the critic be deprived of information that is available to the actor, as this typically undermines the core benefit of CTDE?\n\n3. In the scalability tests (Appendix A.11), a 4 vs. 6 engagement resulting in a 3:4 survival rate is described as a success where the smaller team \"destroys one more aircraft than the blue team\". Could you clarify this interpretation, as a 3:4 result (Red:Blue) means the 4-agent team lost one agent while the 6-agent team lost two, which is not an equal or better kill-death ratio per capita (0.25 vs 0.33 losses per agent)?\n\n4. The reward functions in Appendix A.3 are highly complex. Specifically, the formulas for \"reward regarding position of planes\" (Eq. 2) and \"reward regarding velocity of planes\" (Eq. 8) appear to use a very similar calculation for the variable `dd` based on antenna and aspect angles (ATA, AA). Could you explain the rationale for using this angular metric to modulate both a position-based and a velocity-based reward?\n\n5. In the H2TE module (Appendix A.5.1, Eq. 13), the weight $w_{H,i,j,t}$ for agent `i`'s view of opponent `j` is generated from $H_{j,t}$, which is defined as the observation history of opponent `j` relative to *all* N cooperative agents. How does an individual agent `i` get access to the opponent's historical observations relative to its teammates during decentralized execution?\n\n6. The problem is defined as a partially observable one where agents only have local observations. However, the key transition model `HJLGT` (Appendix A.6) and the prior distributions both explicitly use neighbor states and actions as inputs. Does this imply that agents are assumed to have perfect, noise-free observation of their immediate neighbors' states and actions, and if so, shouldn't this be stated as a key assumption in the problem formulation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zwzPNnCqdc", "forum": "PAIlHOJBzN", "replyto": "PAIlHOJBzN", "signatures": ["ICLR.cc/2026/Conference/Submission12092/Reviewer_3nuv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12092/Reviewer_3nuv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760523697461, "cdate": 1760523697461, "tmdate": 1762923061722, "mdate": 1762923061722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces H2IL-MBOM, a framework for opponent modeling designed to address non-stationarity in multi-agent adversarial environments.  The method's core is a hierarchical world model that mimics human cognitive processes by decomposing the complex task of reasoning about an opponent into two levels.  At a high level, the model infers an opponent's macro \"intention\" by analyzing their historical trajectories.  Subsequently, at a low level, it uses this inferred intention as a condition to deduce the specific \"latent strategy\" the opponent is employing, taking into account the reactions and movements of allied agents.  This framework is implemented through a complex neural architecture based on Transformers and Hypernetworks (HyperHD2TSSM) and is used to guide a PPO-based reinforcement learning agent.  The authors report that their method demonstrates superior performance compared to various baselines in several testbeds, including multi-UAV combat, the StarCraft Multi-Agent Challenge, and Google Research Football."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a novel approach to opponent modeling inspired by human cognition, decomposing the complex problem into a two-level hierarchy of high-level \"intentions\" and low-level \"latent strategies\". This provides a structured and theoretically-grounded new perspective for the field."}, "weaknesses": {"value": "1. This paper, in its current state, is difficult to accept. The core issue is not just a matter of style, but a fundamental lack of clarity in its presentation that prevents a proper scientific review. The manuscript is plagued by a host of minor yet cumulative errors that suggest a lack of care in its preparation. For instance, citations are not properly formatted (lacking \\citep or \\citet), leading to overlaps with the text. There are basic punctuation errors (e.g., missing periods on lines 199 and 218), inconsistent formatting (the acronym HJLGT is sometimes italicized, sometimes not), and poor typesetting (some words are hyphenated across lines between 249-269). Furthermore, the figures are poorly executed; the text in Figure 3 is very small, while the architectural diagrams in Figures 5 and 6 are so cluttered they seem designed more to showcase the model's complexity than to explain it. The overall writing quality, with its convoluted sentence structures and excessive jargon, resembles unedited text generated by a large language model, a practice that should be acknowledged if used.\n\n2. This poor presentation directly obscures the methodology. The main body of the paper has been effectively \"hollowed out,\" with critical information relegated to the appendix. For example, MSOAR-PPO is listed as a key contribution, but its mechanics are entirely absent from the main methodology section. Similarly, the dimensionality of the core latent variables for \"intention\" and \"strategy\" — a crucial implementation detail — is only found in a table in the appendix. The reader should not have to be a detective, piecing together the core method from scattered fragments. This forces a reviewer to question the paper's central claims. The entire framework rests on a rigid two-level hierarchy where \"intention\" dictates \"strategy,\" a strong cognitive assumption presented without justification. The paper also fails to provide evidence that the learned latents, $z_I$ and $z_L$, are actually disentangled. The t-SNE plots are insufficient as they only visualize clustering, not semantic meaning.  A more rigorous analysis, such as performing interventional experiments (e.g., fixing the \"intention\" latent while varying the \"strategy\" latent and observing the impact on generated trajectories), is needed to validate that these variables meaningfully represent their claimed concepts.\n\n3. The experimental evaluation is similarly unconvincing.  The decision to place the results on standard benchmarks (SMAC and GRF) in the appendix is a major flaw that undermines the paper's claims of generalizability;  these should be in the main paper.  The primary UAV experiment relies on comparisons against opponent modeling baselines (e.g., ROMMEO, PR2) that perform catastrophically.  Their complete failure strongly suggests a lack of proper hyperparameter tuning for this complex, continuous-control environment.  For a fair comparison, the authors must either provide evidence of a thorough tuning process for these baselines or include stronger, more suitable ones.  The ablation study, while showing that components are useful, does not justify the model's immense complexity.  The fact that simpler variants (e.g., the only_intentions model from Fig. 4a and especially the transformer_shareadd model from Fig. 4f, which performs nearly identically to the full model) are still effective raises a critical question about the cost-benefit trade-off.  The authors should provide a discussion justifying why the marginal performance gain of their full architecture warrants its significant complexity over these simpler, yet competent, alternatives."}, "questions": {"value": "1. he prior distribution for an agent's latent state (e.g., $p(z_{I,i,t}|...)$ on page 5) explicitly conditions on the latent states and actions of its neighbors ($z_{I,n_i,t-1}, a_{n_i,t-1}$). How is this neighbor information accessed or communicated between agents during execution, especially within what is described as a decentralized execution paradigm?\n\n2. Appendix A.9 states that the value function for MSOAR-PPO \"does not incorporate respective opponents' observations,\" distinguishing it from MAPPO. However, the policy is conditioned on these observations ($O_{opp,i,t}$). In a centralized training paradigm, why would the critic be deprived of information that is available to the actor, as this typically undermines the core benefit of CTDE?\n\n3. In the scalability tests (Appendix A.11), a 4 vs. 6 engagement resulting in a 3:4 survival rate is described as a success where the smaller team \"destroys one more aircraft than the blue team\". Could you clarify this interpretation, as a 3:4 result (Red:Blue) means the 4-agent team lost one agent while the 6-agent team lost two, which is not an equal or better kill-death ratio per capita (0.25 vs 0.33 losses per agent)?\n\n4. The reward functions in Appendix A.3 are highly complex. Specifically, the formulas for \"reward regarding position of planes\" (Eq. 2) and \"reward regarding velocity of planes\" (Eq. 8) appear to use a very similar calculation for the variable `dd` based on antenna and aspect angles (ATA, AA). Could you explain the rationale for using this angular metric to modulate both a position-based and a velocity-based reward?\n\n5. In the H2TE module (Appendix A.5.1, Eq. 13), the weight $w_{H,i,j,t}$ for agent `i`'s view of opponent `j` is generated from $H_{j,t}$, which is defined as the observation history of opponent `j` relative to *all* N cooperative agents. How does an individual agent `i` get access to the opponent's historical observations relative to its teammates during decentralized execution?\n\n6. The problem is defined as a partially observable one where agents only have local observations. However, the key transition model `HJLGT` (Appendix A.6) and the prior distributions both explicitly use neighbor states and actions as inputs. Does this imply that agents are assumed to have perfect, noise-free observation of their immediate neighbors' states and actions, and if so, shouldn't this be stated as a key assumption in the problem formulation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zwzPNnCqdc", "forum": "PAIlHOJBzN", "replyto": "PAIlHOJBzN", "signatures": ["ICLR.cc/2026/Conference/Submission12092/Reviewer_3nuv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12092/Reviewer_3nuv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760523697461, "cdate": 1760523697461, "tmdate": 1763091643021, "mdate": 1763091643021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the decision-making problems in mixed-motive scenarios where cooperation and defection coexist. The paper provides a method, taking into account the nested interaction between agents' (including opponents and allies) intents and strategies. Without relying on other agents’ private information, the method hierarchically infers opponents’ intents and intent-based latent strategies, and predicts their influence on the behaviors of allies. The experiments on Gym-JSBSim, SMAC, and GRF  validate the superior effectiveness of the proposed method over existing model-free and model-based approaches."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper focuses on both intent-based strategies and the interactions among agents’ intents and strategies. It proposes a transformer-based hierarchical opponent inference and decision-making method within the reinforcement learning framework, and extensive experiments across three environments verify its effectiveness. Overall, the paper presents a substantial amount of work with comprehensive experiments and detailed methodological development and contributes valuable insights to the study of intent modeling and opponent inference in mixed-motive multi-agent systems."}, "weaknesses": {"value": "1. It is not easy to follow this paper because of the inconsistent use of symbols and technical terms. See details in **Questions**\n2. In mixed-motive games, agents should consider both allies' and opponents' intents and strategies, while the proposed method insufficiently addresses allies' intents and strategies. It may lead to a failure of coordination within the team. \n3. The introduction does not include any citations. From the introduction, it is unclear how this project is related to mixed-motive games. Please further refine and reorganize the introduction section."}, "questions": {"value": "##### \n\n1. At the high-level, the observation model $p_{\\theta_i}$ predicts observations $O_{opp,i,t}$ based on intents $z_{I,i,t}$, while the observations are in turn used by $q_{\\phi_I}$ to infer intents. With such coupling, model errors may gradually accumulate. How do the authors address this issue? So does the low-level.\n2. In line 269, why does the policy take into account allies' intents and latent strategies? In mixed-motive games, individuals need to model not only their opponents but also consider the behaviors of their allies in order to achieve better coordination. \n3. Do $p_{\\theta_I}$ in line 257 and $p_{\\theta_L}$ in line 266 predict observations rather than trajectories?\n4. In section A.3, opponents' relation position, relative velocity, angles and distance relative to others are included in observation, which is inconsistent with the statement in line 224. It says opponents' actions are not observable.\n5. There seems to be no fundamental difference between stage 2 and stage 3 in subsection 3.1.\n6. The notation used in the paper is somewhat confusing. For example, in line 186, $ a_{i,t}\\sim \\pi(|o_{i,t}, z_{I,i,t}, z_{L,i,t}) $ shows agent $i$'s action only depend one cooperative agents' $o_{i,t}$, $z_{I,i,t}$ and $z_{L,i,t}$, with $N$ is the number of cooperative agents. It is inconsistent with \"cooperative agents update their policies based on trajectory and observations and inferred opponent intentions and strategies\" given in section 3. Please modify the problem formulation and unify the notation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0uZcmuUJs8", "forum": "PAIlHOJBzN", "replyto": "PAIlHOJBzN", "signatures": ["ICLR.cc/2026/Conference/Submission12092/Reviewer_JDt1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12092/Reviewer_JDt1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657636676, "cdate": 1761657636676, "tmdate": 1762923060954, "mdate": 1762923060954, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an opponent modeling method, i.e., H2IL-MBOM,  that integrates multi-intention and latent strategy inference into a world model. H2IL-MBOM combines high-level intention inference with low-level strategy prediction to deal with the non-stationary dynamics in multiagent environment. H2IL-MBOM is combined with PPO, which results in MSOAR-PPO. The effectiveness of the method is evaluated on  several multi-UAV games."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- the idea of employing world model to the field of opponent modeling is interesting."}, "weaknesses": {"value": "- The paper is poorly written. For instance, a lot of concepts, e.g.,  intentions, mental state, strategies, are lack of clear definition. Many figures in the experimental section are hard to interpret, and the captions are not informative (e.g., Figure 4.). Many abbreviations make the paper very hard to follow.\n\n- Lack of novelty of the proposed method. Similar ideas (e.g., reason about latent strategies based on teammates’ historical responses) have been intensively explored in previous opponent modeling methods, such as [1-3], which are missing out in the Related work section. \n\n- Most of the baselines are not targeting opponent modeling methods, e.g., MADDPG, MAPPO. It is necessary to compare with SOTA opponent modeling methods, such as [1-3].\n\n[1] Greedy when sure and conservative when uncertain about the opponents, ICML 2022\n\n[2] Conservative offline policy adaptation in multi-agent games, NeurIPS 2023\n\n[3] Opponent modeling with in-context search, NeurIPS 2024"}, "questions": {"value": "- What exactly do you mean by intentions, strategies, mental state? Could you have a clear definition of these concepts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VXwQh4Pjhe", "forum": "PAIlHOJBzN", "replyto": "PAIlHOJBzN", "signatures": ["ICLR.cc/2026/Conference/Submission12092/Reviewer_tF3n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12092/Reviewer_tF3n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966249922, "cdate": 1761966249922, "tmdate": 1762923060327, "mdate": 1762923060327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies cooperative-competitive MARL. The paper proposes to learn both intention and strategy representations of opponents and utilizes such information to update beliefs and policies/strategies of the agents involved in the game. The paper conducts experiments in several MARL benchmarks including Gym-JSBSim, SMAC, and GRF, in comparison with multiple baselines including both opponent-model free and opponent-model-based MARL algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a new approach to model opponents' decision making (with the goal is to separate intentions from strategies) and integrate it into strategy/policy learning of the agents in the game. \n\n2. Experiments show promising results as the proposed method is shown to perform better than other strong baselines in various benchmarks."}, "weaknesses": {"value": "1. Writing and Clarity\nThe paper is not well written. In particular, Section 3—the main technical section—requires substantial revision. The section consists of long, dense paragraphs that lack a clear and structured explanation of the proposed model. The heavy use of acronyms and lengthy equations further obscures the main ideas rather than clarifying them. More importantly, the intuitions and justifications behind the model’s components, as well as how they connect, are not clearly explained. A concise, intuitive description of the model and its motivation is necessary to make the paper accessible and convincing.\n\n2. Separation Between Intention and Strategy\nThe paper needs stronger justification for the proposed separation between intention and strategy. The key question is how the proposed components actually capture intention and distinguish it from strategy prediction. The manuscript does not clearly explain what specific mechanisms or characteristics of H2TE-MITD and LHTE-MLTD enable this distinction. The authors should provide clearer explanations to support the claim that these modules can meaningfully separate intentions from strategies.\n\n3. Cooperative–Competitive Setting\nAlthough the paper discusses a mixed cooperative–competitive setting, the proposed approach appears primarily designed to address competitive interactions. It remains unclear how the model effectively handles both cooperation and competition within the same framework.\n\n4. Baseline Performance and Reliability of Results\nThe reported performance of baseline methods, such as MAPPO on SMAC environments, is significantly lower than in established works (e.g., the recent HAPPO paper). This discrepancy raises concerns about the experimental setup and the reliability of the reported results. The authors should verify their implementations, hyperparameters, and training conditions to ensure a fair and credible comparison.\n\n5. Supplemental Material\nThe supplemental zip file could not be opened, preventing further examination of the additional materials. Please ensure that the supplementary files are correctly packaged and accessible."}, "questions": {"value": "Please address my concerns raised in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N8kTc3p28e", "forum": "PAIlHOJBzN", "replyto": "PAIlHOJBzN", "signatures": ["ICLR.cc/2026/Conference/Submission12092/Reviewer_fJ8q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12092/Reviewer_fJ8q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762020805074, "cdate": 1762020805074, "tmdate": 1762923059852, "mdate": 1762923059852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Some clarifications"}, "comment": {"value": "# Baseline Performance and Reliability of Results\n## SMAC\nWe would like to clarify that the performance evaluation of all algorithms reported in our study (with win rate as the core metric) is based on a unified benchmark **aligned with the advanced world model MAZero, using interaction steps at the scale of 1e4 to 1e6.** This approach is designed to facilitate the most direct and fair comparison of model efficiency, and the results genuinely and strongly support our core conclusions. \n \n## Experimental Setup Focused on Evaluating World Model Efficiency \nThe primary contribution of this work is a superior world model. Consequently, the fairest approach for comparison is to evaluate our model against the current state-of-the-art world model (e.g., MAZero) under identical simulation steps (1e4-1e6 steps), assessing their efficiency in learning and planning from equivalent interaction data. \n \n## Explicit Unified Benchmark and Core Metric \n**As explicitly stated in lines 1440–1441 of our paper and the last column of Table 6: \"The total interactive steps are aligned with the settings used in MAZero to ensure a fair and valid comparison.\"** Under this benchmark of limited interaction steps (1e4-1e5), we use win rate as the core performance metric to evaluate all algorithms. This means that our method (H2IL-MBOM), the baseline world model (MAZero), and reference model-free methods (e.g., MAPPO) all compete for the same win rate objective under identical, constrained interaction steps. \n \n## Rationale for Baseline Performance and Our Superiority \nThe reported performance of MAPPO is reasonable, accurate, and **consistent with results reported in MAZero(mappo results are depicted by green lines of Figure3 in MAzero)** literature under our experimental settings because:\n \n- **The interaction steps (1e4-1e6) used in our comparison are significantly fewer than the 1e7-4e7 steps typically required for model-free methods like MAPPO to reach peak performance.** \n \n- This constrained setting fairly tests all methods' learning efficiency. \n \n- Despite these constraints, our method demonstrates clear superiority over both MAZero and MAPPO. \n \nUnder this benchmark, the results incontrovertibly demonstrate the superiority of our Model \n \n- **First-Level Advantage (Superiority Among Peers)**: Our world model achieves a significantly higher win rate, performing markedly better than the baseline world model MAZero under the same interaction steps (1e4-1e6). This directly validates the advancement of our proposed hierarchical architecture. \n \n- **Second-Level Advantage (Efficiency Dominance)**: The win rate attained by our world model with limited data also substantially surpasses that of model-free methods (e.g., MAPPO) under the same step constraint. The relatively lower performance of MAPPO under this setting is expected and reasonable, which precisely underscores the remarkable sample efficiency of our world model, enabling it to learn and prevail in competition from scant data. \n \nThese dual advantages collectively form robust evidence of the excellence of our approach. \n \n## Experimental Reliability Assurance \n \nWe affirm that the implementations and hyperparameters of all baseline algorithms adhere to their original papers or authoritative code repositories. Multiple runs with different random seeds were conducted to ensure statistical reliability. \n \nIn summary, using the same limited interaction steps (1e4-1e6) as MAZero and measured by win rate, our proposed H2IL-MBOM framework demonstrates performance superiority over all types of baselines."}}, "id": "yxucfCAykk", "forum": "PAIlHOJBzN", "replyto": "PAIlHOJBzN", "signatures": ["ICLR.cc/2026/Conference/Submission12092/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12092/Authors"], "number": 41, "invitations": ["ICLR.cc/2026/Conference/Submission12092/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763564303322, "cdate": 1763564303322, "tmdate": 1763564303322, "mdate": 1763564303322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Some clarifications"}, "comment": {"value": "# Difference between ours and other methods \n\n\nOur approach differs fundamentally from existing baselines in the following aspects:\n\n**Information Constraints**: In our setting, cooperative agents and opponents are isolated, with no access to each other's **private information (including the other party's observations, actions, policy parameters, rewards, or intentions).**\n\n**Integrated Online Learning Framework**: Our method belongs to the **world model-based MARL paradigm**, where the world model for opponent modeling and the MARL policy are learned synchronously in an online manner. Specifically:\n\n- The MARL policy, conditioned on the intention $z_I$ and latent strategy $z_L$ inferred by the world model, interacts with the environment to collect data.\n\n- This interaction data is used to update the world model.\n\n- The updated world model then generates additional synthetic data through policy rollout.\n\n- Both real interaction data and model-generated data are jointly utilized to update the MARL policy, forming a closed-loop learning cycle.\n\nKey Differentiation from Prior Work: Unlike methods in references [1,2,3] that separately learn opponent models and RL policies:\n\n- These baselines typically rely on **pre-training strategy representations** using opponents' private information, followed by policy learning.\n\n- This represents an **offline learning paradigm**, whereas our approach achieves fully integrated **online learning** of opponent modeling and multi-agent decision-making\n- The purpose of offline learning is to use a **fixed dataset** to find a potentially optimal strategy. \n- In our method, **both teams are trained as independent learners**, avoiding built-in AI or self-play. Two teams engage in independent policy learning, value learning, and world model learning based on their local observations due to the limitation of imperfect game. During the execution phase, the policy of each agent relies solely on: (1) local observations\nrelative to cooperative adjacent agents ( O_c), (2) local observations relative to opponents ( O_opp),\nand (3) mental states inferred via the hierarchical model.\n\nExisting methods, such as [1], [2], and [3], share a common limitation: **their training processes heavily rely on opponents' private information (e.g., actions or policy parameters) as explicit supervision labels**, which is often impractical in real-world adversarial settings. Moreover, these approaches typically lack a transparent, hierarchical reasoning process. They fail to elucidate **how estimated intentions influence latent strategies**, how agents should react to these inferred mental states, or how their evolution guides the prediction of future trajectories. This opaque reasoning is compounded by the absence of an explicit transition model to capture the dynamics between mental states, leaving them **unable to continuously reason about the temporal evolution of intentions and strategies.**\n\nMoreover, these methods have primarily been evaluated in low-dimensional or simplified environments, such as two-player Kuhn Poker, 2D grid-world Predator-Prey, Multi-Agent Particle Environments (MPE), Level-Based Foraging, and Overcooked. Only the CSP method has been tested on the Google Research Football platform.\n\nIn contrast, our approach requires **no access to any private information** of the opponents and relies entirely on observational data for unsupervised learning, making it more **suitable for real-world applications**. Beyond this practical advantage, it establishes an explicit **hierarchy from intentions to strategies, with a dynamic model capturing their temporal evolution.**\n\nIn terms of experimental validation, we have compared different baselines including model-free MARL, opponent modelling methods, and world-model based MARL in large-scale, high-dimensional multi-agent cooperative–competitive experiments, covering scenarios with varying numbers of UAVs in adversarial games, diverse settings in SMAC, and complex simulations in Google Research Football. Notably, **in the Google Research Football benchmark, our method significantly outperforms CSP across all tested scenarios**: achieving a win rate of 92.54% in 3v1, 89.94% in RPS, and 93.09% in CA. These results robustly demonstrate our model's superior generalization capability and decision-making performance in complex, high-dimensional spaces. \n\n**On one hand, we have supplemented the description of these literature in the Related Work section. On the other hand, the existing experimental setup and results are sufficient to demonstrate the superiority of our method.**"}}, "id": "trZ7SmTFTz", "forum": "PAIlHOJBzN", "replyto": "PAIlHOJBzN", "signatures": ["ICLR.cc/2026/Conference/Submission12092/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12092/Authors"], "number": 42, "invitations": ["ICLR.cc/2026/Conference/Submission12092/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763564393679, "cdate": 1763564393679, "tmdate": 1763623341560, "mdate": 1763623341560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Some clarifications"}, "comment": {"value": "# Response to Model Complexity\n\nModel Complexity Analysis\n## 1. Hypernetwork-based Adaptive Architecture\nIn multi-agent systems, **conventional approaches either share networks (policy homogenization) or maintain separate networks (causing parameter growth with agent count)**. Our framework addresses both limitations through hypernetworks in the hierarchical representation model, transition model, and policy network (HEA). This design achieves agent **adaptation without parameter inflation**:\n\n- Hierarchical Model: The hypernetwork generates distinct parameters for each agent, enabling non-shared, individualized reasoning and **adaptation to varying opponents**\n\n- Transition Model: As specified in Lines 102-103 and 1070-1072 and Appendix A4, our efficient transition modeling **compresses historical information** into latent neural weights using a hyper-network, **eliminating the need for explicit historical storage** like TSSM while supporting **adaptive mental state estimation for all agents without parameter count increase**\n- **hypernetwork-based coordination**\n\n## 2. Implicit Model Ensemble through Multi-step Updates\nAs discussed in Appendix A7, multiple arbitrary-step updates within **a single model are equivalent to implicit averaging over an ensemble of models with different horizons (k=1,2,...,H, where H ∼ random(maximum horizon)), thereby further reducing model complexity while maintaining performance.**\n\n## 3. Ablation Study Validation\n\n- Figure 4(a): When only the intention reasoning module is retained, the model **cannot deduce which latent strategies employed by the opponents triggered teammates' specific reaction patterns, nor predict their impact on our team's behavior and future trajectories. This results in significant learning oscillations and slower convergence during training**\n\n- Figure 4(f): As mentioned earlier, using shared Transformers across multiple agents leads to reasoning homogenization. The results demonstrate that hypernetworks enable adaptive reasoning for different agents **while avoiding homogenization, without compromising reasoning performance. This approach achieves both faster learning speed and superior final performance**\n\n## 4. Challenges in Multi-UAV Game Environments\nThe multi-uav domain in gym-jsbsim presents unique challenges that existing methods cannot adequately address:\n\n- Each aircraft is equipped with four autonomously guided missiles with 20-second tracking time\n\n- Extreme velocity differentials: aircraft at 2 Mach (twice the speed of sound) vs missiles at 4 Mach \n\n- High-dimensional state space with complex spatial relationships\n\n- Critical importance of launch timing and resource management\n\n- Highly dynamic and rapidly evolving tactical situations\n\nThis demonstrates the critical need for reasoning about both opponents and their launched missiles. Given that missiles travel significantly faster than aircraft, agents must make early and continuous predictions to execute timely attitude adjustments or rapid maneuvers, avoiding disadvantageous positions and evading multiple incoming threats. Effective agents must learn to secure angular and altitude advantages while achieving maximum tactical gains at minimal cost. In this scenario, agents must first learn fundamental flight control before advancing to strategic game play. Existing methods struggle in such intense adversarial environments characterized by high-speed dynamics and rapid attitude changes, particularly in evading missiles (each hit incurs a -100 reward) or gaining positional superiority, which explains why current opponent-based approaches fail to learn effective policies.\n\n\n## 5. Advantages of Our Approach\nOur method addresses these limitations through:\n\n- Privacy-preserving modeling: Requires no access to opponent's private information (actions, policy parameters, or rewards)\n\n- Hierarchical world model: Enables multi-level mental state **reasoning about both $m$ enemy agents and the maximum observable $m$ weapon systems**\n\n- Temporal dynamics capture: Explicitly models the evolution of mental states over time\n\n- Complex environment adaptation: Successfully handles the challenges of high-speed aerial combat with sophisticated weapon systems\n\nThis design **balances model simplicity with competitive performance**, leveraging hypernetworks and hierarchical reasoning to  reason about multi-level mental states of $m$ enemy agents and the maximum observable $m$ weapon systems in complex multi-agent adversarial settings."}}, "id": "vPoesxg1bV", "forum": "PAIlHOJBzN", "replyto": "PAIlHOJBzN", "signatures": ["ICLR.cc/2026/Conference/Submission12092/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12092/Authors"], "number": 43, "invitations": ["ICLR.cc/2026/Conference/Submission12092/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763564579859, "cdate": 1763564579859, "tmdate": 1763611306163, "mdate": 1763611306163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "# justification, mechanisms，and validations\nOpponent modeling and intention reasoning lie at the heart of Theory of Mind (ToM), enabling agents to infer opponents' latent mental states, such as goals, beliefs, preferences, and strategies. The cognitive foundations for this capability run deep: research in developmental psychology demonstrates that even infants separate enduring goals from situational actions, recognizing that intentions remain stable while strategies adapt to contextual efficiency [1]. This intention-strategy dissociation is further supported by neuroscientific evidence showing distinct neural encodings for high-level goals in prefrontal regions versus action execution in inferior frontoparietal circuits [2]. Computational studies of human mentalizing reveal that this separation enables hierarchical causal reasoning: humans first infer others' task-relevant goals, then derive the specific action plans employed to achieve them [3]. \n\n**This provides direct theoretical justification for our model's hierarchical inference process, where intention recognition precedes strategy estimation.**\n\n[1]  György Gergely and Gergely Csibra. Teleological reasoning in infancy: The infant’s naive theory of\nrational action. Trends in Cognitive Sciences, 63(2):227–233, 1997\n\n[2] Antonia F de C. Hamilton and Scott T Grafton. Action outcomes are represented in human inferior\nfrontoparietal cortex. Cerebral Cortex, 18(5):1160–1168, 2008\n\n[3] Chris L Baker, Julian Jara-Ettinger, Rebecca Saxe, and Joshua B Tenenbaum. Rational quantitative\nattribution of beliefs, desires and percepts in human mentalizing. Nature Human Behaviour, 1(4):\n0064, 2017\n\nThe inputs of LDLRF are historical observations ${H_{c,t}} \\in {\\mathbb{R}^{N \\times 512 \\times D}}$ to cooperative agents, current observations ${O_c}$ to cooperative neighbors,  intent prediction ${z_I}$, and  latent strategies ${z_L}$, where the multi-dynamic latent strategies are initialized by the intentions feature ${z_I}$ generated by MITD: ${z_L} = MLP([Gate({z_{If}},MLP({z_I})),{z_{If}}]) \\in {\\mathbb{R}^{T \\times 2m \\times N \\times C}}$ as defined in Figure 6. The subsequent latent strategies embeddings are derived from the latent strategies inferred in the previous layer.\n\nIntention inference layer estimates by three core mechanisms: First, the observation-based encoding mechanism,  where H2TE exclusively uses our observations for opponents to construct feature representations, thoroughly avoiding interference from teammate response patterns and ensuring the purity of intention features. Second, the temporal consistency modeling mechanism, where by analyzing behavioral sequences over 512 time steps, extracting macro-behavioral trends that characterize persistent intentions. Third, the threat-centric interpretation mechanism, where MITD directly constructs intention queries around ``which ally faces the greatest threat.\" Thus, each agent enhances its intent prediction for a given opponent through the team's collective threat consensus. This integrated computational process is mathematically formalized through the Bayesian framework: $P(\\text{Intent} \\mid H_{opp}, O_{opp}) \\propto \nP(O_{opp} \\mid \\text{Intent}) \\cdot \nP(H_{opp} \\mid O_{opp}, \\text{Intent}) \\cdot \nP(\\text{Intent})$\n\nThe strategy inference layer estimates by building an inverse reasoning framework based on the behavioral mirror principle: LHTE forms a “behavioral mirror\" by encoding historical team states, comprehensively recording the characteristic response patterns of the team under various strategic pressures. On this foundation, MLTD implements Bayesian inverse reasoning to establish a complete causal chain from observed effects back to potential strategies. The core of this process lies in the concrete computation of the probability formula $P(\\\\text{Strategy} \\\\mid \\\\text{Response}, \\\\text{Intent}, O_c) \\\\propto\nP(O_c \\\\mid \\\\text{Intent}) \\\\cdot\nP(\\\\text{Response} \\\\mid \\\\text{Strategy}, O_c, \\\\text{Intent}) \\\\cdot\nP(\\\\text{Strategy} \\\\mid \\\\text{Intent})$: The latent strategy prior is embedded through query initialization, incorporating assumptions about strategy distributions given specific intentions. The intention self-cross attention module computes the observation conditioning term $P(O_c \\mid \\text{Intent})$, evaluating how current situational evidence aligns with inferred intentions. The likelihood term $P(\\text{Response} \\mid \\text{Strategy}, O_c, \\text{Intent})$ is calculated through the latent strategy cross-attention module, assessing how well latent strategies explain both historical patterns and current team reactions.\n\n\nThis dual-layer architecture preserves the advantages of direct observation in intention recognition while ensuring the causal rationality of strategy inference, ultimately achieving precise threat assessment and multi-agent cooperative decision-making through the team consensus mechanism."}}, "id": "Ah0CLpANDG", "forum": "PAIlHOJBzN", "replyto": "PAIlHOJBzN", "signatures": ["ICLR.cc/2026/Conference/Submission12092/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12092/Authors"], "number": 44, "invitations": ["ICLR.cc/2026/Conference/Submission12092/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763564889198, "cdate": 1763564889198, "tmdate": 1763610751295, "mdate": 1763610751295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Definition of ${O_{opp}}$ and ${H_{opp}}$"}, "comment": {"value": "## ${O_{opp}}$ is observation relative to opponent rather than opponent's observation\nFurthermore, as defined in Lines 181 and 674 of observation space and illustrated in Figure 1 of our manuscript, **${O_{opp}}$ refers to the observation relative to opponents from the ego agent's perspective**. Together with ${O_{c}}$, the observation relative to cooperative adjacent agents, they constitute the agent's local observation. **${O_{opp}}$ does not represent observations from the opponents' viewpoint.** \n## ${H_{j,t}}$  are the  history observations of all agens relative to the  j-th opponent\nBesides, as described in lines878, ${H_{opp,t}} \\in {\\mathbb{R}^{N \\times 512 \\times D}}$ is composed of ${O_{opp}}$， $D = m \\times {d_m}$ is the observation dimensionalities relative to $m$ opponents within the observation scope of each agent. Therefore,  **${H_{j,t}}$  represents the observation history of all agents relative to the  j-th opponent.**\n\nThis approach shares conceptual similarities with the Centralized Training with Decentralized Execution (CTDE) paradigm but differs in several key aspects. During the decentralized execution phase, each agent relies solely on: (1) local observations relative to cooperative adjacent agents ( ${O_{c}}$ ), (2) local observations relative to opponents ( ${O_{opp}}$), and (3) mental states inferred via the representation model q. Furthermore, the value function takes as input only the observations (their ${O_{c}}$, ${O_{opp}}$) and inferred mental states of the agent **itself and its allied neighbors, without incorporating observations from opponents**, unlike methods such as MAPPO, which often use global observations including those of adversaries during training. Consequently, neither the policy nor the value function  utilizes opponents' observations.\n\n## Noise observation input from gym-jsbsim\nJSBSim employs a real-time physics engine that integrates atmospheric models and sensor modeling to deliver a simulation environment closely approximating real-world flight conditions. In the Gym-JSBSim environment, sensors are indeed subject to noise influences, which are primarily manifested in the following aspects:\n\n**Sensor Noise Characteristics**\n\n- Aerodynamic Sensors: Directly measured atmospheric data (airspeed, barometric altitude) contains Gaussian white noise\n\n- Inertial Measurement Units (IMU): Accelerometers and gyroscopes exhibit drift errors and random walk noise\n\n- GPS Receivers: Position and velocity measurements contain colored noise and multipath effects\n\n- Attitude Sensors: Euler angle and quaternion estimations are affected by vibrational environmental factors\n\n By incorporating Kalman filters or complementary filters, the system effectively combines the high-frequency response of IMU with the low-frequency stability of GPS, thereby significantly enhancing the accuracy of attitude estimation."}}, "id": "XzSWhazhfI", "forum": "PAIlHOJBzN", "replyto": "PAIlHOJBzN", "signatures": ["ICLR.cc/2026/Conference/Submission12092/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12092/Authors"], "number": 46, "invitations": ["ICLR.cc/2026/Conference/Submission12092/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763565107900, "cdate": 1763565107900, "tmdate": 1763565107900, "mdate": 1763565107900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Reasons for the failure of other opponent modeling methods"}, "comment": {"value": "# Reasons for the failure of other opponent modeling methods\nWe have conducted extensive parameter tuning and five random seed experiments for all algorithms. The reasons for the failure of these methods are as follows: \n\n## 1. Inadequate Temporal Reasoning\nExisting opponent modeling methods are unable to continuously reason about the temporal evolution of intentions and strategies. They typically operate on static or short-term representations, failing to capture the dynamic nature of adversarial interactions in complex environments.\n\n## 2. Limited Environmental Validation\nThese methods have been primarily designed and validated in simplified settings:\n\n- PR2 and ROMMEO were only tested in matrix games and differential games\n\n- TDOM was evaluated solely in differential games, MPE, and predator-prey scenarios\n\n- AORPO was trained and tested only on climb tasks and MPE environments\n\nBesides, the methods [1,2, 3] recommended by the second reviewer were also been evaluated in low-dimensional or simplified environments, such as two-player Kuhn Poker, 2D grid-world Predator-Prey, Multi-Agent Particle Environments (MPE), Level-Based Foraging, and Overcooked. Only the CSP method has been tested on the Google Research Football platform.\n\nIn terms of experimental validation, we have compared different baselines including model-free MARL, opponent modelling methods, and world-model based MARL in large-scale, high-dimensional multi-agent cooperative–competitive experiments, covering scenarios with varying numbers of UAVs in adversarial games, diverse settings in SMAC, and complex simulations in Google Research Football. Notably, in the Google Research Football benchmark, **our method significantly outperforms CSP across all tested scenarios**: achieving a win rate of 92.54% in 3v1, 89.94% in RPS, and 93.09% in CA. These results robustly demonstrate our model's superior generalization capability and decision-making performance in complex, high-dimensional spaces.\n\n## 3. Challenges in Multi-UAV Game Environments\nThe multi-uav domain in gym-jsbsim presents unique challenges that existing methods cannot adequately address:\n\n- Each aircraft is equipped with four **autonomously guided missiles with 20-second engagement capability**\n\n- Extreme velocity differentials: **aircraft at 2 Mach (twice the speed of sound) vs missiles at 4 Mach**\n\n- High-dimensional state space with complex spatial relationships\n\n- Critical importance of launch timing and resource management\n\n- Highly dynamic and rapidly evolving tactical situations\n\n## 4. Advantages of Our Approach\nOur method addresses these limitations through:\n\n- Privacy-preserving modeling: Requires no access to opponent's private information (actions, policy parameters, or rewards)\n\n- Hierarchical world model: Enables multi-level mental state **reasoning about both $m$ enemy agents and the maximum observable $m$ weapon systems**\n\n- Temporal dynamics capture: Explicitly models the evolution of mental states over time\n\n- Complex environment adaptation: Successfully handles the challenges of high-speed aerial combat with sophisticated weapon systems\n\nThis demonstrates the critical need for reasoning about both opponents and their launched missiles. Given that missiles travel significantly faster than aircraft, agents must make early and continuous predictions to execute timely attitude adjustments or rapid maneuvers, avoiding disadvantageous positions and evading multiple incoming threats. Effective agents must learn to secure angular and altitude advantages while achieving maximum tactical gains at minimal cost. In this scenario, agents must first learn fundamental flight control before advancing to strategic game play. Existing methods struggle in such intense adversarial environments characterized by high-speed dynamics and rapid attitude changes, particularly in evading missiles (each hit incurs a -100 reward) or gaining positional superiority, which explains why current  opponent-based approaches fail to learn effective policies.\n\nThis represents the first method for opponent modeling in intense adversarial environments, as well as the first approach to employ world models for opponent modeling. It not only advances the development of world models but also promotes the progress of opponent modeling techniques and contributes to the field of multi-agent adversarial decision-making."}}, "id": "b7spFQxomo", "forum": "PAIlHOJBzN", "replyto": "PAIlHOJBzN", "signatures": ["ICLR.cc/2026/Conference/Submission12092/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12092/Authors"], "number": 47, "invitations": ["ICLR.cc/2026/Conference/Submission12092/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763565294164, "cdate": 1763565294164, "tmdate": 1763565294164, "mdate": 1763565294164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Observable States"}, "comment": {"value": "# Observable States\nWe appreciate the opportunity to clarify the important distinction between observable kinematic states and unobservable control actions in our problem formulation.\n\nThat various motion states can be measured by sensors. Indeed, in our environment:\n\n## Observable States (Sensor Measurements):\n\n- Relative position, velocity,\n\n - Angles and distance relationships\n\n- Altitude and orientation information\n\nThese measurements constitute the legitimate observations in our reinforcement learning framework and are consistent with real-world aerial surveillance capabilities. However, the critical distinction lies in what remains unobservable:\n\n## Unobservable Elements (opponents' action spaces described in A3 and opponents' observations relative to cooperative agents):\n**For unmanned aerial vehicles**:\n\nDirect aircraft control signals:\n\n- Aileron deflection angles\n\n- Elevator deflection angles\n\n- Rudder deflection angles\n\n- Thrust magnitude settings\n\n- Missile launch activation signals\n\n**For manned aircraft scenarios:**\n\nPilot's physical control inputs:\n\n- Control stick movements (forward/backward for pitch via elevator, left/right for roll via ailerons)\n\n- Rudder pedal inputs for yaw control\n\n- Throttle lever positions for thrust adjustment\n\n- Weapon release button/trigger activation\n\nWe would like to clarify that the relative distance, velocity, and attitude measurements in our model are indeed grounded in realistic aircraft sensing capabilities:\n1. **Relative Distance Measurement:** \n- **Active electromagnetic ranging using onboard radar systems (primary radar, secondary radar, millimeter-wave radar, lidar)**\n\n- The radar transmits directed electromagnetic pulses toward targets and measures the round-trip time Δt\n\n- Distance is calculated using the fundamental principle: R = c·Δt/2, where c is the speed of light\n\n**This provides accurate relative distance measurements to other aircraft**\n\n2. **Relative Velocity Measurement:**\n\nInertial/multi-sensor fusion approaches combining:\n\n- **IMU-based measurements** from MEMS accelerometers and gyroscopes\n\n- Continuous integration of velocity changes\n\n- **Fusion with GPS, barometric sensors, and visual odometry data**\n\n**This sensor fusion yields precise 3D velocity vectors relative to ground or surrounding targets**\n\n3. **Relative Attitude/Orientation Measurement:**\n\n- **Phased array radar beam pointing** using active electronically scanned array (AESA) radar\n\n- Electronic scanning rapidly locks onto multiple targets\n\n- Beam pointing angles and target echo phase differences directly provide: Target azimuth relative to aircraft axis,  Target elevation angles, Complete relative orientation information\n\nThese sensing methodologies are well-established in aerospace systems and provide the technical foundation for the observable kinematic states in our model. The measurements of relative position, velocity, and attitude **represent realistic sensor capabilities rather than idealized assumptions.**\n\nWe will add this technical explanation to the manuscript to better justify our observation space definition and its correspondence to real-world aerial combat sensing capabilities.\n\nOur end-to-end RL controller generates these low-level control signals directly from processed observations. The key insight is that while we can observe the results of opponent actions (kinematic states), we **cannot access their decision-making process, control inputs, opponents' observations relative to cooperative agents, nor the strategic intent behind them.**"}}, "id": "l7WIqinAAa", "forum": "PAIlHOJBzN", "replyto": "PAIlHOJBzN", "signatures": ["ICLR.cc/2026/Conference/Submission12092/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12092/Authors"], "number": 48, "invitations": ["ICLR.cc/2026/Conference/Submission12092/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763565521201, "cdate": 1763565521201, "tmdate": 1763565521201, "mdate": 1763565521201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}