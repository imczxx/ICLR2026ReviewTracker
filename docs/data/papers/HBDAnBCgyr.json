{"id": "HBDAnBCgyr", "number": 2694, "cdate": 1757210170491, "mdate": 1763606186273, "content": {"title": "Read the Scene, Not the Script: Outcome-Aware Safety for LLMs", "abstract": "Safety-aligned Large Language Models (LLMs) still show two dominant failure modes: they are easily jailbroken, or they over-refuse harmless inputs that contain sensitive surface signals. We trace both to a common cause: current models reason weakly about links between actions and outcomes and over-rely on \\emph{surface-form signals}, lexical or stylistic cues that do not encode consequences. We define this failure mode as \\textbf{\\textit{Consequence-blindness}}. To study consequence-blindness, we build a benchmark named \\textbf{\\textit{CB-Bench}} (\\textbf{\\textit{\\underline{C}}}onsequence-\\textbf{\\textit{\\underline{B}}}lindness \\textbf{\\textit{\\underline{Bench}}}mark) covering four risk scenarios that vary whether semantic risk aligns with outcome risk, enabling evaluation under both matched and mismatched conditions which are often ignored by existing safety benchmarks. Mainstream models consistently fail to separate these risks and exhibit consequence-blindness, indicating that consequence-blindness is widespread and systematic. To mitigate consequence-blindness, we introduce \\textbf{\\textit{CS-Chain-4k}} (\\textbf{\\textit{\\underline C}}on\\textbf{\\textit{\\underline{S}}}equence \\textbf{\\textit{\\underline{Chain}}}), a consequence-reasoning dataset for safety alignment. Models fine-tuned on \\csch show clear gains against semantic-camouflage jailbreaks and reduce over-refusal on harmless inputs, while maintaining utility and generalization on other benchmarks. These results clarify the limits of current alignment, establish consequence-aware reasoning as a core alignment goal and provide a more practical and reproducible evaluation path.", "tldr": "", "keywords": ["Safety Alignment", "AI Safety", "LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a343cc6ec376f8b14037359d66ae0a30ebdc651.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper differentiates between outcome risk and semantic risk for safety-related prompts. Semantic risk includes prompts that may use safety-flagged words or discuss a dangerous scenario, and outcome risk is prompts that could result in real harmful outcomes. Outcome risk is what we actually care about from a safety standpoint, but this paper contributes a benchmark demonstrating that most LLMs are consequence-blind and focus more on semantic risk in evaluating prompt safety and choosing when to refuse a response. To mitigate this issue, they also release a training dataset for consequence reasoning and show that fine-tuning LLMs on this dataset reduces their consequence-blindness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Originality:\n- the distinction between outcome and semantic risk is an important and interesting idea in safety alignment and they explain this clearly\n\nQuality:\n- the datasets seem useful\n\nClarity:\n- the writing is generally clear\n\nSignificance:\n- this seems like a very useful benchmark"}, "weaknesses": {"value": "1. The figures are very unclear and seem to be missing data in some places. Most critically, most of the plots are blank in Figure 5. And in Figure 4, why is the red line missing in the fourth column/second row? In terms of confusing figures/tables, in Figure 1, on the left, it seems to me like the model is doing the right thing so I find this difficult to interpret, and the graphic on the right was very confusing for me to understand the takeaway. I think focusing this figure on clear examples of when semantic risk and outcome risk differ would be more helpful. For tables, you should state somewhere what the blue row indicates and clarify in the caption what S/C and C/C mean. I also find Figure 3 quite confusing and find the icons more distracting than helpful. \n\n2. There is not enough detail on the benchmark and training dataset construction in the main body of the paper. We really need to see some example instances from both in the paper and it would help to move some of the details from the appendix into the main body. We don't need the prompts, but we need some of the wording."}, "questions": {"value": "I am open to increasing my overall rating if the authors can clarify my questions about the figures in the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "52qGKzZBwn", "forum": "HBDAnBCgyr", "replyto": "HBDAnBCgyr", "signatures": ["ICLR.cc/2026/Conference/Submission2694/Reviewer_Tcne"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2694/Reviewer_Tcne"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779835453, "cdate": 1761779835453, "tmdate": 1762916333221, "mdate": 1762916333221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the LLM safety problem through two key failure modes: vulnerability to jailbreak attacks and over-conservativeness. The authors argue that the root cause of these issues lies in current models' weak reasoning about the connection between actions and consequences, and their over-reliance on surface-form cues. To support this claim, they introduce CB-Bench (Consequence-Blindness Benchmark), a dataset designed to evaluate models’ ability to identify underlying intent and corresponding safety behavior. Additionally, they propose CS-Chain-4k, a consequence-reasoning dataset aimed at advancing safety alignment. The authors further validate their findings through comprehensive experimental results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) Interesting Problem:\n The problem of LLM safety, and the underlying relationship between surface form and semantic meaning, is both important and interesting.\n\n(2) In-Depth Experiments:\n The authors not only present the final evaluation results of their method but also provide in-depth analyses, such as probing and token attribution studies, to better understand the underlying mechanisms."}, "weaknesses": {"value": "(1) Limited Evaluation:\n The main evaluation in Table 2 only includes open-source models with ≤32B parameters, which is insufficient to fully assess the difficulty of the proposed task. I suggest including evaluation results from larger models, such as Qwen2.5-72B, LLaMA3.3-70B, DeepSeek-R1/V3, as well as closed-source models like GPT-4o and the Gemini series.\n\n(2) Interpretation of Scaling Effects (Line 237):\n The authors state that: “These results reveal that scaling impacts safety trade-offs differently across architectures, and larger models do not universally improve consequence-aware safety.” However, this conclusion may be biased, as it does not account for differences in pre-training datasets and only considers models from two series. As such, the evidence presented is not sufficient to support a strong claim against scaling parameter size.\n\n(3) Surface Form vs. Underlying Semantics:\n For the discussion on surface form versus underlying semantics, I recommend a more in-depth comparison with related work—for example, references [1, 2].\n\nReference:\n\n[1]  Yue Zhou, et al. \"Paraphrase and solve: Exploring and exploiting the impact of surface form on mathematical reasoning in large language models.\" arXiv preprint arXiv:2404.11500 (2024).\n\n[2] Yihang Yao,  et al. \"Your language model may think too rigidly: Achieving reasoning consistency with symmetry-enhanced training.\" arXiv preprint arXiv:2502.17800 (2025)."}, "questions": {"value": "(1) Limited Comparison of Reasoning vs. Non-Reasoning Models:\n The authors claim that Reasoning Enhancement Worsens Issues (page 4), suggesting that reasoning models perform worse than non-reasoning ones. However, the evaluation appears limited to small-scale models. Could you include further experiments comparing larger models, such as DeepSeek-V3 versus DeepSeek-R1?\n\n(2) Clarification on DeepSeek-R1 Reference (Lines 245–246):\n You mention that Reasoning models (e.g., DeepSeek-R1, Qwen3-Thinking) devote a large share of tokens to CoT. However, DeepSeek-R1 does not seem to appear in the context. Are you referring to the R1-Distilled models?\n\n(3) CoT Impact on Evaluation Consistency (Line 209):\n The authors briefly mention the impact of CoT on evaluation consistency. Could you elaborate on this point? For which tasks is CoT evaluation particularly critical, and why can verifiers not rely solely on the final answer?\n\n(4) Refusal and Semantic Risk Analysis:\n In the section analyzing the impact of refusal and semantic risk on output length, how is the comparison performed? When you refer to \"shorter CoT responses,\" do you mean outputs that include reasoning supporting the semantic risk, or are you referring to brief refusal messages?\n\n(5) Clarification of Table 4 Annotations:\n In Table 4, why is the \"+C/C\" condition highlighted? Additionally, could you clarify the meanings of the abbreviations \"W/C\", \"S/C\", and \"C/C\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0wQ5rEZi0C", "forum": "HBDAnBCgyr", "replyto": "HBDAnBCgyr", "signatures": ["ICLR.cc/2026/Conference/Submission2694/Reviewer_exPQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2694/Reviewer_exPQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941364601, "cdate": 1761941364601, "tmdate": 1762916333075, "mdate": 1762916333075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- Safety-aligned LLMs exhibit two major failure modes: susceptibility to jailbreaks and excessive refusals of harmless prompts with sensitive wording.\n - Both failures stem from consequence-blindness—models’ weak reasoning about action–outcome relationships.\n - Introduces CB-Bench (Consequence-Blindness Benchmark) covering four risk scenarios.\n - Existing models consistently fail to distinguish between semantic and outcome risk, confirming that consequence-blindness is systematic and widespread.\n - Proposes CS-Chain-4k (ConSequence Chain), a consequence-reasoning dataset for improving safety alignment.\n - Fine-tuning on CS-Chain-4k enhances resistance to semantic-camouflage jailbreaks and reduces over-refusal of benign prompts.\n - Performance on other benchmarks remains stable, demonstrating preserved utility and generalization.\n - Establishes consequence-aware reasoning as a key objective for future safety alignment and offers a practical, reproducible evaluation path."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Conceptually this is a very interesting framework: it's hard to judge the sensitivity and harmfulness of decisions without reflecting on their consequences. This is essentially a causal relationship between actions and their outcomes."}, "weaknesses": {"value": "A (perhaps reductionist?) perspective on this work this is that, the authors have annotated \"rationales\" or \"reasoning chains\" for why certain actions should or should not be made. And from the existing literature, there is ample evidence that articulating reasonings (by the model) will help it become more reliable in its decision making. With this lens, the contribution of this work is to supervise their model with more detailed reasoning chains. Curious if/how the authors would push back against this.  \n\n One issue about \"consequences\" is that they can be subjective and quite context-specific. Can the authors elaborate on how they went about resolving subjectivity in their construction? \n\n\n\nFig 2 is actually a bit confusing. \n- Left subfig: What is \"score\"?? \n- Right subfig: Which line corresponds to which y-axis? (specifically the left y-axis says \"percentage\" but unclear percentage of what exactly. )\n\n Minor: Consider changing your CB-scores to % to be compatible with the rest of the numbers."}, "questions": {"value": "See the previous box."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HUJP4M9jwj", "forum": "HBDAnBCgyr", "replyto": "HBDAnBCgyr", "signatures": ["ICLR.cc/2026/Conference/Submission2694/Reviewer_99Gi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2694/Reviewer_99Gi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950729603, "cdate": 1761950729603, "tmdate": 1762916332902, "mdate": 1762916332902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper discusses the shallow safety alignment of existing LLMs and argues that such behavior is due to the limited ability of the models to separate the semantic (surface) risk from the outcome risk, and that what explains the typically reported jailbreakability of most models and over-refusal rates of many of them. The paper introduces a benchmark (CS-Bench) with the goal of enabling the distinction between the two risks and present evaluation results on various models and uses the results to confirm the arguments about the shallow alignment. To address that problem, the paper also presents a safety alignment dataset (CS-Chain) that explicitly teaches models to reason about the outcome of answering user requests."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper introduces an interesting and sound distinction between semantic and outcome risk which is a valuable way for understanding the limited safety behaviors of today's models.\n\n2. The presented alignment dataset is shown to indeed improve the safety/over-refusal of the evaluated models."}, "weaknesses": {"value": "1. It is not clear what the value of the introduced benchmark CS-Bench is. It does not seem that the benchmark introduces any additional insights beyond those already established by evaluating on existing jailbreak attacks and over-refusal benchmarks. For example, table 3 in the paper uses CS-Bench and table 4 uses existing jailbreak attacks and XSTest. Both are leading to the same conclusion, and no new insights are provided by table 3. The benchmark can be made more useful if for example it provides more fine-grained categorization or understanding of the errors.\n\n2. The novelty with CS-Chain (the fine-tunning dataset) is that it encourages the model to reason about the consequences. The paper lacks a baseline in which models are just prompted to do so without any additional fine-tuning.\n\n3. The paper lacks qualitative and error analyses. It is essential to demonstrate the reasoning about the consequences behavior of the model. It is also important to provide an explanation of the still significant jailbreakability and over-refusal results in table 4. Was that mainly because of the model failure to reason about the consequences? or due to some other reason? Are there any limitations with CS-Chain that need to be addressed?"}, "questions": {"value": "Would you please confirm that SafeChain contains responses with reasoning, but they do not explicitly consider the consequences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D1wNIXLQfu", "forum": "HBDAnBCgyr", "replyto": "HBDAnBCgyr", "signatures": ["ICLR.cc/2026/Conference/Submission2694/Reviewer_Hmh6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2694/Reviewer_Hmh6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962971070, "cdate": 1761962971070, "tmdate": 1762916332582, "mdate": 1762916332582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Author Rebuttal Summary (Part 1/2 -  Exeperiments)"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful and constructive feedback. Your comments helped us identify places where additional experimental evidence, clearer exposition, and more concrete examples were needed. In the rebuttal phase, we carefully addressed each concern through new experiments, expanded analyses, and targeted revisions to the manuscript. Below, we summarize the key additions and improvements made in direct response to the reviewers’ suggestions.\n\n---\n\n## 1. **Newly Added Experiments**\n\n### (a) Expanded CB-Bench Evaluation to Large/Frontier-Scale Models (**motivated by exPQ's Weaknesses**)\nWe added CB-Bench results for large-scale and frontier models, including Qwen2.5-72B, LLaMA3.3-70B, DeepSeek-R1-70B, DeepSeek-R1-Full, GPT-4o, and Gemini-2.5-Flash. These results show that the cross-prompt consistency failures captured by CB-Bench persist even at frontier scale. Models with stronger general capabilities still struggle to maintain consistent decisions across Q1–Q4, reinforcing the difficulty and necessity of this evaluation.\n| Model               | Jailbreaked ↓ | Over-refusal ↓ | CB-Score ↓ |\n|---------------------|-------------|--------------|----------|\n| Qwen2.5-72B         | 56.20       | 24.66        | 0.31     |\n| LLaMA3.3-70B        | 56.80       | 11.33        | 0.24     |\n| DeepSeek-R1-70B     | 73.33       | 11.40        | 0.32     |\n| DeepSeek-R1-Full    | 62.18       | 29.72        | 0.37     |\n| GPT-4o              | 53.38       | 20.66        | 0.26     |\n| Gemini2.5-Flash     | 39.04       | 49.29        | 0.37     |\n\n---\n\n### (b) Prompt-Only vs. CS-Chain Fine-Tuning (**motivated by Reviewer Hmh6's Weaknesses**)\nWe consolidated all prompt-only numbers with CS-Chain results into a unified comparison table and newly expanded the prompt-only baseline to additional benchmarks (MMLU, HellaSwag, Sorry-bench). The unified analysis shows that prompting alone yields limited and unstable improvements, whereas CS-Chain provides consistent gains in consequence-aware alignment while preserving capabilities.\n| Model         | Method      | Sorry-bench ↓      | XSTest ↑        | MMLU ↑         | HellaSwag ↑     | StrongReject-PAP ↓ | StrongReject-GCG ↓ | StrongReject-Prefix ↓ |\n|---------------|-------------|---------------------|------------------|-----------------|------------------|----------------------|----------------------|-------------------------|\n| **Qwen2.5-7B** | Prompt     | 25.20 ± 1.5          | 50.0 ± 2.5       | 71.6 ± 0.4      | 80.0 ± 0.4       | 27.0 ± 2.0            | 61.2 ± 6.8            | 55.8 ± 5.1              |\n|               | Fine-tuned | 18.6 ± 1.2           | 70.6 ± 1.3       | 71.5 ± 0.4      | 79.6 ± 0.4       | 35.3 ± 3.5            | 37.3 ± 3.4            | 28.4 ± 1.0              |\n| **Mistral-7B** | Prompt     | 50.75 ± 3.7          | 64.2 ± 1.9       | 57.7 ± 0.4      | 82.0 ± 0.4       | 48.1 ± 3.4            | 89.7 ± 3.8            | 94.5 ± 2.2              |\n|               | Fine-tuned | 15.9 ± 1.0           | 67.9 ± 2.0       | 58.4 ± 0.3      | 79.2 ± 0.4       | 23.0 ± 5.5            | 33.3 ± 6.2            | 6.0 ± 1.2               |\n\n---\n\n### (c) Measuring Datasets' Consequence Reasoning (**motivated by Reviewer Hmh6's Questions and Reviewer 99Gi's Weaknesses**)\nTo distinguish CS-Chain from general safety datasets such as SafeChain, we introduced a new evaluation using LLM judges (Qwen2.5-7B, LLaMA3.1-8B, Gemma3-12B) that score each response on a 1–5 scale based on the extent to which the reasoning reflects actual downstream consequences. CS-Chain achieves substantially higher scores (≈0.8–1.3 point gains), demonstrating that it induces qualitatively different reasoning rather than merely longer explanations.\n| Method               | Qwen2.5-7B | Llama3.1-8B | Gemma3-12B |\n|----------------------|------------|--------------|-------------|\n| SafeChain (original) | 3.23       | 3.20         | 3.40        |\n| SafeChain (baseline) | 3.58       | 3.44         | 3.72        |\n| CS-Chain             | 4.54       | 4.38         | 4.45        |"}}, "id": "iKsw9xjYmM", "forum": "HBDAnBCgyr", "replyto": "HBDAnBCgyr", "signatures": ["ICLR.cc/2026/Conference/Submission2694/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2694/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission2694/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763606374081, "cdate": 1763606374081, "tmdate": 1763606462479, "mdate": 1763606462479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}