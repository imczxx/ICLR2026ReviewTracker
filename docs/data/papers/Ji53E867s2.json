{"id": "Ji53E867s2", "number": 10692, "cdate": 1758179703676, "mdate": 1759897635049, "content": {"title": "Decoupling Policy Improvement and Conservatism in Offline Reinforcement Learning", "abstract": "Offline reinforcement learning (RL) methods suffer from \\textit{extrapolation error}. However, current solutions face a dilemma - static constraints are over-conservative, while naive dynamic references perilously couple policy improvement with conservatism, which can lead the reference itself to an out-of-distribution (OOD) target. Our core insight is that these two objectives must be decoupled. In this paper, we introduce the \\textbf{C}overage-\\textbf{A}ware \\textbf{R}eference Policy (CAR), which instantiates this principle via a propose-and-verify mechanism: the learned policy proposes actions, and a verifier confirms data support before augmenting a reference policy. This tractably creates a progressively improving reference with theoretical coverage guarantees. Finally, CAR establishes state-of-the-art (SOTA) results on offline RL benchmarks and strong results in online fine-tuning.", "tldr": "", "keywords": ["Offline Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7005216ec6fcc31b2847a51f918da65502c1fd5a.pdf", "supplementary_material": "/attachment/6c7bb99383ca03db3d7788e2a8454cf438ebb909.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces the Coverage-Aware Reference Policy (CAR), a novel method for offline reinforcement learning that aims to resolve the conflict between policy improvement and conservatism. The core contribution is the principle of decoupling these two objectives via a \"propose-and-verify\" mechanism, where the learned policy proposes actions that are then verified for data support before augmenting a reference policy. The method is supported by theoretical guarantees and demonstrates strong, state-of-the-art empirical results across a wide range of D4RL benchmarks. The problem framing is insightful, and the empirical validation is thorough."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Insightful Problem Framing: The paper compellingly frames a core offline RL challenge as a harmful coupling of policy improvement and conservatism, offering an elegant \"propose-and-verify\" mechanism to decouple these objectives.\n\n2. State-of-the-Art Empirical Results: CAR achieves outstanding performance across diverse D4RL benchmarks, significantly outperforming strong baselines, particularly in challenging sparse-reward Antmaze tasks.\n\n3. Paper is well-written and easy to follow."}, "weaknesses": {"value": "1. Lacks some related work in the in-sample learning paradigm in offline RL, e.g., IQL, SQL. Please enhance this direction of research in offline RL. \n2. The authors empirically justify this choice in Section 5.2, showing that using $\\pi_t$ leads to higher final performance. While the results are strong, this creates a disconnect between the theory and the final algorithm. A deeper discussion on why this theoretically riskier approach works better in practice would significantly strengthen the paper's contribution.\n3. Potential Bias in Hyperparameter Tuning Strategy: The two-stage hyperparameter tuning strategy described for CAR may create an unfair advantage. First optimizing the regularization coefficient $\\lambda$ and then separately tuning the support threshold $\\epsilon_{ood}$ could result in a more exhaustive search for CAR compared to its baselines. The authors should clarify if an equivalent tuning budget was used for all methods to ensure the performance gains are attributable to the method itself and not the tuning process."}, "questions": {"value": "1. A question regarding Section 4.2 and Equation (5):\nThe mixing coefficient $\\tau_t(s)$ is defined in Equation (5) as an expectation, $\\\\mathbb{E}_{a \\sim \\pi_t}[\\mathbb{I}(\\ldots)]$, which implies it is a continuous probability value, $P\\in [0,1]$. However, the subsequent procedural description (\"...proposes an action... If the action is accepted...\") describes a single sampling event which results in a binary outcome (accept or reject). To clarify: should we interpret $\\tau_t(s)$ as the continuous probability of this acceptance event, and the described procedure as a single Bernoulli trial governed by that probability? Or is there an alternative implementation of the expectation in practice?\n\n2. Following question 1, in Algorithm 1 (Lines 11-13), I think $\\tau_t(s)$ is a binary outcome with a single sampling event. Could the authors please clarify how the algorithm ensures stability and avoids sample bias with this single-sample approach? Alternatively, if multiple samples are used to form a more stable estimate, could you comment on the associated computational overhead?\n\n3. Regarding the proof of **Theorem 1** in Appendix A.1:\n3.1 The proof contains algebraic steps that are difficult to follow and may contain typos, particularly in the inequality expansions. Additionally, the derivation appears to implicitly use the identity $\\tau_t(s) = Z_t(s)$ without explicitly stating it, creating a logical leap. Also, this clarification is highly related to Q1 and Q2. \n\n3.2 Could the authors please revise the proof to clarify these steps for improved readability and rigor? \n$\\begin{aligned} & =\\mathbb{I}\\left(\\pi_\\beta(a \\mid s) \\geq \\epsilon\\right) \\frac{\\pi_1(a \\mid s)}{\\pi_\\beta(a \\mid s)}+1-\\tau_1(s) \\\\ & \\leq \\frac{\\pi_1(a \\mid s)}{\\epsilon}-\\pi_1(a \\mid s)-\\sum_{a^{\\prime}} \\mathbb{I}\\left(\\pi_\\beta\\left(a^{\\prime} \\mid s\\right) \\geq \\epsilon\\right) \\pi_1\\left(a^{\\prime} \\mid s\\right)+1\\end{aligned}$\n\n4. Regarding the construction of the auxiliary dataset $\\mathcal{D}'$: The paper employs a progressive, \nappend-only approach, which is noted as a \"minor update\" per iteration. Have the authors explored more adaptive strategies for managing $\\mathcal{D}'$? For instance, have you considered periodically rebuilding the dataset to better reflect the current policy, or using a prioritized sampling scheme to manage its contents more effectively? An ablation study on these alternatives would be valuable to justify the current design choice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XO5qtluSQ8", "forum": "Ji53E867s2", "replyto": "Ji53E867s2", "signatures": ["ICLR.cc/2026/Conference/Submission10692/Reviewer_x78z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10692/Reviewer_x78z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10692/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761445534679, "cdate": 1761445534679, "tmdate": 1762921935575, "mdate": 1762921935575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an offline RL framework CAR. The key idea is to design a reference policy that incorporates information from the learned policy (which reflects the current improving signal) and the behavior policy (which provides support information). Benchmarks show potential improvement of proposed framework CAR when compared with other alternatives."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and the idea makes sense and intuitive. \nThe benchmarks show potential improvement of proposed framework CAR when compared with\nother alternatives. Some theoretical analysis are included."}, "weaknesses": {"value": "The topic is interesting, however the contribution is limited due to the following reasons:\n1) it is not clear whether problem formulation introduces any theoretical challenges for analysis.\n2) The improvements are not very significant for the benchmarks presented if variances are considered\ngive the small number of samples (only 5)."}, "questions": {"value": "Whether the problem formulation introduces any theoretical challenges for analysis (such as establish Theorem 1 &2)?\nWhether the two key hyperparameters change over the training process and task dependent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K6TOyb89dp", "forum": "Ji53E867s2", "replyto": "Ji53E867s2", "signatures": ["ICLR.cc/2026/Conference/Submission10692/Reviewer_o3V3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10692/Reviewer_o3V3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10692/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912517594, "cdate": 1761912517594, "tmdate": 1762921935240, "mdate": 1762921935240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Coverage-Aware Reference Policy (CAR) for offline reinforcement learning (RL).\nThe key idea is to decouple policy improvement and conservatism via a “propose-and-verify” mechanism:\nthe learned policy proposes actions, and a verifier checks data support to form a progressively improving reference policy.\nTheoretically, the authors provide a coverage guarantee and a performance lower bound; empirically, they report SOTA results on D4RL benchmarks and good online fine-tuning performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear motivation: The paper identifies a real issue in offline RL—entangling improvement with conservatism—and provides a plausible solution.\n\nTheoretical analysis: Two theorems (coverage guarantee and performance bound) are clearly stated and mathematically sound at a conceptual level.\n\nEmpirical results: The experiments are extensive, covering MuJoCo, AntMaze, and Adroit, with consistent performance improvements over strong baselines (e.g., CQL, SPOT, CPI).\n\nReadable structure: The paper is well organized and easy to follow, with clear algorithmic pseudocode and hyperparameter analysis."}, "weaknesses": {"value": "Incremental novelty:\nThe “propose-and-verify” mechanism is essentially a variant of support-constrained sampling as used in prior works such as OSC (Gao et al., 2025) and SPOT (Wu et al., 2022).\nThe core modification—masking out-of-distribution actions and reweighting with a dynamic threshold—feels like a minor algorithmic variation rather than a conceptually new principle.\n\nLack of theoretical depth beyond existing work:\nTheorems 1 and 2 closely mirror prior proofs on coverage guarantees (e.g., BEAR, CPI, SPOT). The results do not introduce fundamentally new analytical tools or insights.\n\nEmpirical claims are overstated:\nAlthough the table reports high scores, performance margins are small or within error bars on most MuJoCo tasks.\nThere is no statistical significance analysis, and no ablation isolating the specific effect of “decoupling” vs. the density model choice.\n\nAmbiguity in verification mechanism:\nThe implementation of the “verify” step is heuristic—based on a VAE reconstruction threshold rather than a formal likelihood test.\nThis weakens the theoretical connection between the coverage guarantee and the practical algorithm.\n\nOver-claiming of generality:\nThe method is presented as a “decoupling framework” for general offline RL, but its applicability beyond VAE-based density regularization (e.g., to actor-critic with other generative models) is unclear.\n\nWriting and clarity issues:\nSeveral grammatical errors and unclear sentences (e.g., “conservatism is enforced by a verification step that guarantees these actions are within the data’s support”) reduce professionalism.\nSome equations (e.g., Eq. 7, Eq. 9) lack intuition or clear derivation paths in the main text."}, "questions": {"value": "How does CAR differ principally from OSC (Gao et al., 2025) or CPED (Zhang et al., 2023)?\nPlease provide a conceptual comparison table.\n\nCan the authors show cases where the decoupling significantly improves learning stability, not just final score?\n\nIs the threshold ϵ fixed or adaptive? How sensitive is the algorithm to this choice beyond Figure 2(b)?\n\nPlease clarify whether the verifier can reject all actions in a batch, and what happens in that case.\n\nIt would help to include computational complexity comparisons in terms of FLOPs or wall time, not just ms/step."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q6vd1OpqLn", "forum": "Ji53E867s2", "replyto": "Ji53E867s2", "signatures": ["ICLR.cc/2026/Conference/Submission10692/Reviewer_zkx1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10692/Reviewer_zkx1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10692/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924939510, "cdate": 1761924939510, "tmdate": 1762921934693, "mdate": 1762921934693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes CAR, a coverage-aware reference policy in which the learned policy proposes actions and a verifier confirms whether the data are in-distribution before augmenting the reference policy. The core idea builds on designing a sampling mechanism to avoid directly estimating the normalization term. Additionally, a simple theoretical analysis supports the effectiveness of CAR. In the experiment, CAR consistently outperforms other baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly structured, allowing readers to follow the overall flow and reasoning easily.\n\n2. CAR identifies and addresses the limitations of previous approaches that relied on reference policies. Moreover, employing various behavior policy estimation methods demonstrates the robustness and validity of its claims.\n\n3. CAR outperforms 7 baseline methods on the MuJoCo, AntMaze, and Adroit tasks."}, "weaknesses": {"value": "1. For Table 1, only RORL is trained for 3M gradient steps, which might obscure the superior performance of CAR on the MuJoCo tasks. It would be helpful to reproduce the normalized score of RORL with 1M gradient steps for a fair comparison.\n\n2. Compared to the offline setting, where CAR is evaluated against 7 baseline methods, the offline-to-online experiments include comparisons with only 2 algorithms (SPOT and Cal-QL). A more detailed explanation and broader comparison would strengthen the analysis. For example, RLPD [1] and PEX [2].\n\n[1] Ball, Philip J., et al. \"Efficient online reinforcement learning with offline data.\" International Conference on Machine Learning. PMLR, 2023.\n\n[2] Zhang, Haichao, We Xu, and Haonan Yu. \"Policy expansion for bridging offline-to-online reinforcement learning.\" arXiv preprint arXiv:2302.00935 (2023)."}, "questions": {"value": "1. When fixing the VAE model during online fine-tuning, are you using only the online dataset, or a mixture of both offline and online data? If only the online data are used, the fixed VAE model may experience a distributional shift, leading to potential forgetting before it adapts again. On the other hand, if mixed data are used, the offline data may dominate the updates, making it difficult for the model to sufficiently reflect the influence of the online data.\n\n2. For Figure 2, the values of $\\epsilon_{ood}$ and $\\lambda$ appear to be highly sensitive. In Table 2, the balance coefficient ranges from 0.05 to 38, which seems excessively wide. Moreover, the key hyperparameter values of CAR for each dataset are not provided."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lsLW8xw1H6", "forum": "Ji53E867s2", "replyto": "Ji53E867s2", "signatures": ["ICLR.cc/2026/Conference/Submission10692/Reviewer_r5oJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10692/Reviewer_r5oJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10692/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974848687, "cdate": 1761974848687, "tmdate": 1762921934212, "mdate": 1762921934212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}