{"id": "FRp8cu1aKF", "number": 25529, "cdate": 1758368942156, "mdate": 1759896716819, "content": {"title": "On the (In)Significance of Feature Selection in High-Dimensional Datasets", "abstract": "Feature selection (FS) is assumed to improve predictive performance and highlight meaningful features. We systematically evaluate this across $30$ diverse datasets, including RNA-Seq, mass spectrometry, and imaging. Surprisingly, tiny random subsets of features (0.02-1\\%) consistently match or outperform full feature sets in $27$ of $30$ datasets and selected features from published studies (wherever available). In short, any arbitrary set of features is as good as any other (with surprisingly low variance in results) - so how can a particular set of selected features be ''important'' if they perform no better than an arbitrary set?  These results indicate the failure of the null hypothesis implicit in claims across many FS papers, challenging the assumption that computationally selected features reliably capture meaningful signals. They also underscore the need for rigorous validation before interpreting selected features as actionable, particularly in computational genomics.", "tldr": "Tiny random subsets of features match or outperform feature-selected sets across 27 out of 30 high-dimensional datasets, challenging conventional feature selection and highlighting the need for rigorous validation.", "keywords": ["feature selection", "null hypothesis testing", "negative result", "high-dimensional data", "computational biology"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b1568d4caaea8b0a11da0d7dfc28829f108f3352.pdf", "supplementary_material": "/attachment/74313f94cb5cba52bc324eefe9dd8552f43dcfa4.pdf"}, "replies": [{"content": {"summary": {"value": "This paper studies feature selection for classification tasks. It uses established methods and metrics to perform feature selection on a long list of datasets, focusing on biological data (and microarray data within that).\n\nThe paper provides an empirical study on the effectiveness of randomly choosing a small, random subset of features from a given dataset for downstream applications (here, explicitly, classification). It claims that randomly selected features perform at least as well as 'cleverly' selected features. It also claims that a small, random subset of features can achieve the performance obtained by the full feature set and proposes that the minimum random set size that achieves this performance is a useful metric in describing the data."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Reporting results on a long list of datasets. This is common in feature selection literature.\n- Demonstrating with workflows implementing established methods\n- Proposing a metric, minimum sufficient random sample size, that interpretably evaluates the collective strength of the features of a dataset"}, "weaknesses": {"value": "- The results do not seem to support a key claim of the paper (randomly selected features performing at least as well as cleverly selected features)\n  - In Table 2, all values in column D are lower than the corresponding values in column A.\n  - In Table 2, 2 out of 6 values in column E are higher than those in column A, but this is problematic:\n    - The difference between columns D and E is that E uses an ensemble of classifiers. Column A could presumably also benefit from such ensembling.\n    - The procedure for column E is not clear. The text says \"ensemble of LR, RF, and XGB trained on different random subsets of features of same size as the published study\" while the table says \"same number of randomly selected features\". If each ensemble uses a different random subset of the same size, then column E effectively uses up to 3 times more number of features. Alternatively if the total number of features available to column E is the same as that to column A, then it is not clear how the sizes of feature sub-subsets were allocated between LR, RF, XGB.\n\n- With the exception of the Madelon and Gisette subsets, the sample count is less than the feature count for all studied datasets. (In Madelon, the paper's claim does not hold and all features are needed to achieve full performance. In Gisette, sample count and feature count are close to each other; 7000 vs 5000.) However, sample counts being much larger than feature counts is very common in modern datasets. For instance, it is nowadays routine to profile millions of cells with scRNA-Seq. It is possible that the trend the authors observe concerns primarily the #feature < #sample regime.\n\n- The datasets are heavily skewed towards biological datasets. It is possible that the trend identified in this paper does not apply to all domains. I think the claim needs to be narrowed accordingly."}, "questions": {"value": "- Could you try the proposed experiment on a larger scRNA-Seq dataset? Many such datasets are publicly available together with their cell type annotations, disease status, etc.\n\n- (line 4141-416) This is confusing. Did the authors mean \"The only three cases where **the full feature set** does better than chance...\"? Table 1 does not show results for feature selection that is not random."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OeSAjricoL", "forum": "FRp8cu1aKF", "replyto": "FRp8cu1aKF", "signatures": ["ICLR.cc/2026/Conference/Submission25529/Reviewer_AW9D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25529/Reviewer_AW9D"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761610269638, "cdate": 1761610269638, "tmdate": 1762943463109, "mdate": 1762943463109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a large-scale empirical analysis challenging the assumption that feature selection improves predictive performance in high-dimensional datasets. Across 30 datasets, mostly gene expression microarray data focused on cancer the authors show that small random subsets of features (0.02–1%) consistently match or outperform models trained on full feature sets or published feature selections."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The authors address an important question regarding the utility of feature selection in high-dimensional datasets, a topic with significant implications for machine learning and computational biology."}, "weaknesses": {"value": "The paper has several limitations that reduce the strength of its conclusions:\n1) The dataset selection process is not described. The authors provide no inclusion or exclusion criteria, making it unclear how the 30 datasets were chosen.\n2) The dataset pool is heavily biased toward cancer-related gene expression studies from the Gene Expression Omnibus (GEO), yet the conclusions are generalized to the entire field of feature selection.\n3) Cancer and inflammation datasets are known to display large-scale, disease-specific expression changes, which increases the likelihood that randomly selected genes still carry predictive signal.\n4) Training on all available features may introduce substantial noise, artificially lowering performance and making small random subsets appear stronger than they are.\n5) The paper overlooks much of the recent literature on feature selection and related theoretical analyses."}, "questions": {"value": "I would ask the authors to address the weaknesses above:\n1) Please specify the inclusion and exclusion criteria for dataset selection. For the microarray datasets in particular, were they drawn from a defined subset of GEO, or selected manually from thousands of available series?\n2–3) The analysis should be extended to more diverse domains. Even within transcriptomics, including single-cell RNA-seq datasets would provide a stronger test case. In single-cell data, subtype classification often depends on a few key marker genes, making it unlikely that random subsets would perform comparably.\n4) Please include actual feature selection algorithms in the experimental comparisons rather than relying solely on published feature sets.\n5) I recommend consulting prior work on the multiplicity of genomic signatures, which offers theoretical grounding for the observed phenomena:\nhttps://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000790\nhttps://jmlr.org/papers/v14/statnikov13a.html\nhttps://link.springer.com/article/10.1007/s10618-020-00731-7"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZeNNS793y2", "forum": "FRp8cu1aKF", "replyto": "FRp8cu1aKF", "signatures": ["ICLR.cc/2026/Conference/Submission25529/Reviewer_49XN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25529/Reviewer_49XN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882591418, "cdate": 1761882591418, "tmdate": 1762943462820, "mdate": 1762943462820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is a refreshingly honest paper that questions the validity of results published in 30 biological studies and this paper shows that randomly choosing a small collection of features and training a simple ensemble model with Linear Regression, Random Forest and XGBoost, gives comparable results to the published results!"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "ML as a community needs more such papers that challenge the conventional norm.  The results are quite powerful and a bit surprising. Having analyzed many of these datasets, I couldnt but wonder why others havent published such papers earlier. \n\nA biologist can have one of two reactions to the claims in this paper: \"wow, this is surprising/shocking\" or \"the analysis is flawed because of ...\"! For either of these extreme reactions, this paper may be worth accepting to provoke deeper discussions. I actually think ICLR is the wrong venue for this paper for the visibility but the authors should have sent this to Nature/Science sub-journals."}, "weaknesses": {"value": "The paper may have flaws in the analysis but I think the authors have been honest about their analysis and opened up their code and tried to validate them independently."}, "questions": {"value": "A biologist can have one of two reactions to the claims in this paper: \"wow, this is surprising/shocking\" or \"the analysis is flawed because of ...\"! For either of these extreme reactions, this paper may be worth accepting to provoke deeper discussions. I actually think ICLR is the wrong venue for this paper for the visibility but the authors should have sent this to Nature/Science sub-journals.\n\nIf you want to cherry pick specific issues in the analysis, here are some trivial basic questions:\na. When they randomly select a sub-set of features, did the features they picked have strong correlations with the features deemed relevant by the published paper. If even a few features have strong correlations, then the random selection defeats the purpose.\n\nb. Did the authors check how was the original datasets generated? Was the single cell RNA or bulk RNA data imputed using any zero imputation algorithm before the dataset was published? This completely changes the equation. Many imputation algorithms used in the literature give different imputations on the same data with limited agreement in the generated matrix. This could completely change the results. \n\nc. Did the authors check if the generated data had any notion of experimental validation of gene types or cell types in the sequencing methodology?\n\nd. Did the authors check the number of zeroes in the matrix? Many raw RNA matrices have more than 50-95% of the features to be zero before imputation. So if you randomly choose 0.5-2% of numbers, most of the numbers may be zero. If this is not the case, then there may be serious issues from the imputation pipeline.\n\ne. Why did the fraction of randomly sampled features change from paper to paper? Are the authors optimizing their parameters to get the best outcome result for each dataset? That would constitute cheating in the analysis. They should clarify the same.\n\nf. Many of the published results typically highlight very few edges as \"outcome edges\" that have significance from their analysis. Now, are the authors are comparing against the right metrics reported from these papers. The 95+% accuracy with a limited set of features published in these papers seems to imply that many of these papers are reporting several features. Most Nature/Science/Cell/Immunity papers focus on \"one inference edge\" in a paper. Something seems off in the way the results are reported.\n\nDespite all these questions, I do love this paper and I think ICLR should accept such papers even if the results are flawed! Papers that challenge conventional wisdom are much harder to publish and should be encouraged."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UiSgD5zBRH", "forum": "FRp8cu1aKF", "replyto": "FRp8cu1aKF", "signatures": ["ICLR.cc/2026/Conference/Submission25529/Reviewer_1ZQK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25529/Reviewer_1ZQK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960498462, "cdate": 1761960498462, "tmdate": 1762943462511, "mdate": 1762943462511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}