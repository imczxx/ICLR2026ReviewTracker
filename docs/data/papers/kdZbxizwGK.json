{"id": "kdZbxizwGK", "number": 8675, "cdate": 1758094509141, "mdate": 1759897770210, "content": {"title": "ChainGPT: Dual-Reasoning Model with Recurrent Depth and Multi-Rank State Updates", "abstract": "Large language models, constrained by the fixed-depth Transformer architecture, struggle to solve complex reasoning tasks in an end-to-end manner. Existing approaches, such as Chain of Thought, improve reasoning depth to some extent but rely heavily on natural language generation, with computational costs increasing rapidly as the length of the generated sequence grows. To address these limitations, we propose ChainGPT, a dual-reasoning model that shifts reasoning into latent computational space. Within each layer, ChainGPT employs multi-substep state updates combined with state-guided sparse attention, enabling deep local computation and efficient long-range modeling without quadratic costs. Across layers, recurrent depth approach iteratively refine latent states, supported by adaptive training and stopping strategies that balance reasoning depth against computational budget. Theoretically, we show that ChainGPT can, in principle, simulate general computation, and empirically it delivers consistent improvements over comparable models, including on reasoning tasks that remain challenging for existing systems. By unifying efficiency and reasoning ability, ChainGPT provides a principled foundation for next-generation language models.", "tldr": "", "keywords": ["Latent Reasoning; Recurrent Depth; RWKV-Product; State-Guided Sparse Attention"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab426720ca7092ff3ff122b9af490565d0918092.pdf", "supplementary_material": "/attachment/6f94d80ae5bb9f95cd4e86904abf6632c9471a02.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes ChainGPT, a dual-reasoning architecture that pushes reasoning from token generation into latent space via (i) RWKV-Product multi-substep state updates inside each layer and (ii) State-Guided Sparse Attention (SGSA) with sliding windows plus periodic anchor. It uses a recurrent-depth core with entropy-based early stopping across layers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Clear architectural idea with theory hooks**: The dual mechanism (multi-substep “diagonal + rank-M” updates + sparse global anchors) is well motivated and analyzed (rank expansion, MQAR solvability, Turing-completeness under idealizations).\n\n* **Efficiency claims backed by complexity and ablations**: SGSA reduces attention cost to $O(T(W+T/G))$ and tracks full-global attention perplexity on PG-19 when using periodic anchors.\n\n* **Empirical improvements at matched scale**: On LM-Eval tasks, ChainGPT-0.5B/1.5B outperform Qwen2.5 models of the same size, respectively. The sub-step and recurrence ablations are thorough."}, "weaknesses": {"value": "* **Novelty vs prior recurrent/hybrid work is somewhat incremental**: RWKV-Product extends RWKV-7 with LoRA-style multi-substeps. Many components (looped/recurrent depth, window+anchors) resemble existing hybrid archs. Thus, positioning versus models like DeltaProduct, Jamba/Samba, Mamba-2, and HRM could be sharper.\n\n* **Claims on “hard tasks” need stronger rigor**: The ARC-AGI-1 (38.6%), Sudoku-Extreme (54.4%), and Maze-Hard (77.4%) numbers are promising but depend on small (30M) models and bespoke setups; fairness vs large LMs and exact eval pipelines deserve more detail.\n\n* **Theoretical claims hinge on idealized assumptions**: Turing-completeness requires unbounded memory/steps and arbitrary precision; practical implications for finite precision and training stability remain unclear.\n\n* **Paper structure can be polished**: From my reading perspective, the organization of Sections 3 and 4 could be improved to better align with the proposed pipeline and enhance overall clarity. For example, these two sections could be merged into a single, cohesive section. In that structure, the current Section 4.1 could serve as an overarching overview, explaining where the chain-block fits within the entire pipeline and its overall function. Subsequent subsections could then provide a more detailed introduction and analysis of the chain-block itself.\n\n* **Illustrations can also be improved**: The current Figures 1, 2, and 3 each contain limited information, and presenting them separately leads to fragmented understanding. It would be more effective to integrate all three into a single, comprehensive pipeline diagram. Additionally, the current figures appear blurry."}, "questions": {"value": "* **SGSA complexity & memory**: Can you quantify the runtime and activation memory of SGSA vs dense attention across $T∈[4k,32k]$ and report end-to-end wall-clock with/without anchors, beyond the single-GPU microbenchmarks? Also include KV-cache implications.\n* **Efficiency Analysis**: You report a comparison of computation time. Could you provide a more comprehensive efficiency analysis that also covers training GPU-hours and inference latency?\n* **Early-stopping robustness**: The entropy-diff rule uses k and threshold τ. How sensitive are quality and compute to these hyperparameters across tasks?\n* **Ablation on RWKV-Product substeps and rank**: Can you include an ablation removing either the multi-substep (K) and the rank-M updates in RWKV-Product?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pRYSAmnGig", "forum": "kdZbxizwGK", "replyto": "kdZbxizwGK", "signatures": ["ICLR.cc/2026/Conference/Submission8675/Reviewer_9THH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8675/Reviewer_9THH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760738591916, "cdate": 1760738591916, "tmdate": 1762920489456, "mdate": 1762920489456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper\n- Proposes the ChainGPT architecture and introduces RWKV-Product. The architecture proposed can achieve good performance on reasoning tasks compared to Qwen.\n- Mathematically proved that the proposed architecture has superior expressivity and is Truing complete under ideal conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I think the paper is sound provided the combination of math proof and supportive experiments."}, "weaknesses": {"value": "W1: Though you mentioned Geiping's work in intro, I don't find any comparison between existing recurrent models and yours, which is hard for me to judge your contribution to recurrent models."}, "questions": {"value": "Q1: In Chapter 3, I do not find the definition of \"multiple sub-steps reasoning\".\n\nQ2: Can you calculate the number of params explicitly so that I can compare to the normal models?\n\nQ3: There is a reference error in line 252."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l72ODe2ID0", "forum": "kdZbxizwGK", "replyto": "kdZbxizwGK", "signatures": ["ICLR.cc/2026/Conference/Submission8675/Reviewer_4WdW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8675/Reviewer_4WdW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761681247479, "cdate": 1761681247479, "tmdate": 1762920488995, "mdate": 1762920488995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ChainGPT, a dual-reasoning architecuture. In one block of ChainGPT, RWKV-Product achieves intra-layer communication, and SGSA achieves efficient long-range attention. The block design is aimed for \"internal multi-step reasoning\" (RWKV-Product) and \"sparse aggregation\" (SGSA). Across blocks, ChainGPT uses a recurrent paradigm to interatively refine internal states, with dynamic early stopping using entropy. Theoretical discussion and empirical experiments are conducted."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear Positioning**: The introduction provides a clear case for why fixed-depth Transformers and current architectural hybrids suffer from xpressive and computational limitations for deep reasoning.\n\n2. **Architecture Innovation with Theoretical Support**: This paper proposes a grounded achitecture innovation, including (1) intra-layer multi-substep reasoning using RWKV-Product, theoretically proven to expand representational power; (2) inter-layer recurrent refinement, with a theoretically motivated early stopping mechanism based on entropy.\n\n3. **Extensive Details**: Many ablations are given showing the effectiveness of each component, and details provided in the Appendix further improve the relia"}, "weaknesses": {"value": "1. **Concern on Component Integration**: ChainGPT appears to be an ensemble of several independent components, each building upon or modifying existing prior work. This approach could be argued to undermine the central academic contribution by suggesting the performance gains are primarily due to a complex engineering aggregation rather than a singular, fundamental architectural breakthrough.\n\n2. **Missing Discussion about Soft Thoughts**: The paper lacks a critical discussion comparing ChainGPT's approach to existing literature [1-3] that utilizes dense gist tokens (termed \"soft thoughts\"). These works also achieve reduced computational cost and perform reasoning via implicit states, making a comparison essential to fully delineate and underline the unique significance of ChainGPT's methodology.\n\n3. **SGSA Issue**: I'm a little concerned about the SGSA experiments. In Table 4, it seems that even a fully localized sliding window attention can achieve comparable PPL with long contexts, which raise doubts about the evaluation reliability. In practice, the selection of window size $W$ and anchor interval $G$ for all models are all set to 512 and 64, lacking empirical justification.\n\n4. **Experimental Issues**: Few related methods are compared with ChainGPT. Moreover, Since ChainGPT is framed as a \"dual-reasoning\" model, it is crucial that it be compared against SOTA CoT reasoning models. For example, Qwen3-1.7B can achieve an accuracy of $>0.8$ on the ARC-Challenge task, which significantly outperforms ChainGPT's $0.3$ accuracy, severely compromises the contribution of the proposed reasoning capabilities.\n\n> [1] Training Large Language Models to Reason in a Continuous Latent Space.\n> [2] CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation.\n> [3] LightThinker: Thinking Step-by-Step Compression.\n\n### Questions"}, "questions": {"value": "1. **Hyperparameter Sensitivity (Weakness 3 Related)**: The SGSA module relies on window size $W$ and anchor interval $G$. Can you provide guidance on selecting these hyperparameters in practice?\n\n2. **Empirical Comparison with Closest Works (Weakness 4 Related)**: Could the authors provide experimental results or more detailed discussion comparing ChainGPT’s dual-reasoning approach to releated methods? What are the measured gains or trade-offs in similar settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AR3LHhvK4X", "forum": "kdZbxizwGK", "replyto": "kdZbxizwGK", "signatures": ["ICLR.cc/2026/Conference/Submission8675/Reviewer_mHtM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8675/Reviewer_mHtM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909695968, "cdate": 1761909695968, "tmdate": 1762920488517, "mdate": 1762920488517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}