{"id": "ymJuBifPUy", "number": 22092, "cdate": 1758325849218, "mdate": 1759896886829, "content": {"title": "AutoMetrics: Approximate Human Judgments with Automatically Generated Evaluators", "abstract": "Evaluating user-facing AI applications remains a central challenge, especially in open-ended domains such as travel planning, clinical note generation, or dialogue. The gold standard is user feedback (e.g., thumbs up/down) or behavioral signals (e.g., retention), but these are often scarce in prototypes and research projects, or too-slow to use for system optimization. We present **AutoMetrics**, a framework for synthesizing evaluation metrics under low-data constraints. AutoMetrics combines retrieval from **MetricBank**, a collection of 48 metrics we curate, with automatically generated LLM-as-a-Judge criteria informed by lightweight human feedback. These metrics are composed via regression to maximize correlation with human signal.  AutoMetrics takes you from expensive measures to interpretable automatic metrics. Across 5 diverse tasks, AutoMetrics improves Kendall correlation with human ratings by up to 33.4% over LLM-as-a-Judge while requiring fewer than 100 feedback points. We show that AutoMetrics can be used as a proxy reward to equal effect as a verifiable reward. We release the full AutoMetrics toolkit and MetricBank to accelerate adaptive evaluation of LLM applications.", "tldr": "We use LLMs to automatically generate and validate task-specific evaluation criteria (metrics) that correlate well with human judgements, and release a library/framework for automatic metric induction.", "keywords": ["evaluation", "LLM-as-a-judge", "metrics", "human feedback", "open-ended tasks", "user-centered evaluation", "data-efficient evaluation", "automatic metric generation", "benchmarking"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9c8100ea144853548919629eb9251461ba4ced1b.pdf", "supplementary_material": "/attachment/9b38af8cb6ce46558d97e57d90839fd20ab36ade.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes AutoMetrics, a framework for LLM-as-a-judge that automatically generates task-specific evaluation criteria and dynamically assign weights to the criteria to form a scalar evaluation score. The framework consists of 4 steps: Generate, retrieve, regress and report. The paper conducts experiments on 5 benchmarks to show improvement in Kendall correlation with human judgement, compared with standard LLM-as-a-judge approaches."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper resolves a critical problem in prior works for LLM evaluation that rubrics need to be manually designed. Instead, the proposed framework automatically generates and selects criteria for specific tasks. The paper conducts sufficient experiments to support its claim."}, "weaknesses": {"value": "Though the framework is claimed as \"automatic\", it still contains human-curated components. For instance, the \"Retrieve\" step exploits MetricBank, a static metrics collection drawn from prior literature. Therefore, it remains in question whether the generated metrics are suitable for all tasks.\n\nParagraph is incomplete from line 304-315, which seriously hinders content understanding."}, "questions": {"value": "No additional questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KrcyHxhl54", "forum": "ymJuBifPUy", "replyto": "ymJuBifPUy", "signatures": ["ICLR.cc/2026/Conference/Submission22092/Reviewer_8L96"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22092/Reviewer_8L96"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723661361, "cdate": 1761723661361, "tmdate": 1762942063064, "mdate": 1762942063064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a system that automatically creates and learns evaluation metrics for LLM-generated outputs when you only have a small amount of human feedback. It does this by using LLMs to generate candidate evaluators like rubrics and judges, and then statistically combining the best ones so that their scores align closely with real human judgments.\n\nThe fundamental problem this approach addresses is the situation where you don’t have a reward signal to optimize models: data scarcity, limited human judgments, and no ability to run full-scale preference data collection. These automatically generated metrics can then act as a proxy reward model of sorts. In other words, it’s essentially a framework for **operationalizing limited human judgments.**\n\nIn terms of validity, the authors calibrate their learned metrics to human data using Kendall’s tau correlation, so the system is empirically optimized to fit whatever limited judgments are available. It also performs well on internal coherence tests, scoring high on both sensitivity and stability, meaning it penalizes degraded outputs and stays consistent under irrelevant variation, at least within the evaluated tasks. However, this evaluation setup is circular in logic: the system’s sensitivity and stability are tested using examples generated by other LLMs, so it is being validated by the same kind of model it’s built from. These tests therefore measure LLM-to-LLM agreement rather than genuine alignment with human reasoning.\n\nBut the claimed validity of this approach is itself limited in a more fundamental way. The authors demonstrate that their generated metrics align with human judgments, but those judgments come from a small, potentially unrepresentative sample of annotators or users. In other words, the system’s validity is only as sound as the data it’s validated against. If the underlying human feedback lacks breadth, consistency, or reliability, then demonstrating correlation with it doesn’t establish true validity: it just shows agreement with a narrow and possibly biased slice of human opinion. \n\nDespite this limitation in validity, it is my opinion that the system still demonstrates a useful way to operationalize limited human judgments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The reframing of evaluation under limited data as a learning problem is really the \"aha\" moment of this paper. Operationalizing limited human data into a compositional and interpretable set of metrics is useful. The paper validates its approach across 5 domains, and provides convincing ablations of each component's contribution. Relatively well-written and accessible."}, "weaknesses": {"value": "The main weakness is in its grounding of validity. The authors demonstrate strong correlations with small human-labeled datasets, but those datasets themselves are limited, noisy, and unrepresentative, which undermines the strength of their claims. Demonstrating correlation with a weak or narrow gold standard does not establish that AutoMetrics captures genuine human judgment, only that it fits that small sample relatively well; but whether this is truly *useful* remains an open question that is not demonstrated. Overfitting the metrics on this small data sample is the aim of the game here. Fundamentally, the missing validation is if human practitioners actually find these automated metrics meaningful or actionable in practice. The interpretability claim also remains untested- the nature of the approach itself is not sufficient evidence of it being \"interpretable\"."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZVCZhSK4Bt", "forum": "ymJuBifPUy", "replyto": "ymJuBifPUy", "signatures": ["ICLR.cc/2026/Conference/Submission22092/Reviewer_5zyv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22092/Reviewer_5zyv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049229817, "cdate": 1762049229817, "tmdate": 1762942062777, "mdate": 1762942062777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AutoMetrics, a novel framework for automatically constructing evaluation metrics that approximate human judgments in subjective, open-ended tasks with scarce human feedback data. AutoMetrics consists of the four steps: generation of task-specific criteria (both from LLM-generated and existing rubrics), retrieval of top-k metrics, regression with PLS equations between the top-k metrics and gold human labels, and reports of metrics for interpretability. AutoMetrics achieves a higher correlation with human preferences than existing baselines, with only 80 human examples."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper was quite easy to read and understand, with good clarity of method explanations. \n- Timely topic for LLM-guided evaluations for subjective tasks with scarce human feedback data available. It was highly interesting to show that AutoMetrics can be a good alternative as a reward model for agent optimization with reduced cost and complexity of LLM alignment."}, "weaknesses": {"value": "- The regression step fits PLS over a relatively large set of highly correlated candidate metrics after top-k retrieval. While AutoMetrics achieved a higher Kendall correlation with human annotations over baselines (Table 2), the noisy or negative tau values on the smallest tasks (notably CoGym) may indicate a known problem of spurious correlation in high-p, low-n settings induced by too many weak predictors. I acknowledge the claim that < 100 labels needed is true, but it needs more clarification to prevent a \"conditionally true\" statement. It seems that Autometrics could work when (1) generated, task-specific metrics are the majority among the top-k ones or (2) the task objective is close to existing metrics in MetricBank, but degrades when the top-k contains many loosely related metrics.\n\n- AutoMetrics consists of multiple LLM-generated evaluators, thus highly likely to contain inherent biases. The paper does not run the standard robustness test, such as learning the metric on model A's outputs and evaluating on model B's outputs. Some analysis of a simple cross-model generalization table (e.g., generating/regressing using GPT-4o-generated metrics, testing on Qwen outputs) can make the claim of a \"human-aligned\" (after regression) approach of automatic evaluator selection processes."}, "questions": {"value": "- As a suggestion for a stability metric, I would recommend a metric of RBO (rank-biased overlap) to measure a correlation between the rankings of AutoMetrics and human labels. RBO is top-weighted and allows unequal-length lists, a fairer metric than Kendall's tau. If you want to examine “do different runs of AutoMetrics pick roughly the same top metrics?”, RBO gives a more interpretable measure of set-level stability of the head of the ranking. Kendall's tau is a measure of pairwise ordering over the entire list, which can look bad even when the top 5 are identical.\n\n- Generally, the paper lacks a dedicated section on limitations. \n\n- (Not Required but Strongly Recommended as additional analysis to boost your claim) Did you run any informal user study where human users actually used these \"reports\" from AutoMetrics (step 4) or any adoption rate? Even a brief description would help justify this part of the contribution, measuring the actual implication/impact of AutoMetrics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cnEiAAyAjq", "forum": "ymJuBifPUy", "replyto": "ymJuBifPUy", "signatures": ["ICLR.cc/2026/Conference/Submission22092/Reviewer_raWk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22092/Reviewer_raWk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762143383081, "cdate": 1762143383081, "tmdate": 1762942062552, "mdate": 1762942062552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AutoMetrics, a framework for building task-specific evaluation metrics from fewer than 100 human labels. It generates new LLM-based metrics, retrieves existing ones from MetricBank, and combines them via PLS regression into a composite metric that best predicts human feedback. Results across five text-generation tasks show moderate improvements in Kendall’s $\\tau$ over baselines (LLM-as-a-Judge, existing metrics), and the paper introduces “Sensitivity/Stability\" tests and a $\\tau$-Bench proxy-reward case study."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The pipeline is clearly described and practically implementable.\n\n2. The construct validity probes (Sensitivity/Stability) are a useful addition to standard correlation metrics.\n\n3. The low-data claim (performance saturating at 80–100 samples) is empirically supported and relevant.\n\n4. The $\\tau$-Bench case study is interesting, showing the learned metric can act as a reward signal."}, "weaknesses": {"value": "1. On out-of-distribution tasks, “Generated-only” metrics perform as well as or better than the full hybrid version, suggesting the Retrieve step (MetricBank) adds little value.\n\n2. The PLS regression and metric generation both depend on the same small set of labels, and I am not sure if this is the principled approach (compared to split the data half and half).\n\n3. The AutoMetrics pipeline seems more complex and computationally expensive than the baselines it's compared against."}, "questions": {"value": "1. Given the small-N (N=80) and large-P (k=30) nature of the initial regression, how stable are the selected metrics? If you were to bootstrap the 80 human-labeled samples, would the top 5 selected metrics and their weights vary dramatically?\n\n2. What does the failure case look like (e.g. GPT4o in CoGym)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4GLO3dtjTE", "forum": "ymJuBifPUy", "replyto": "ymJuBifPUy", "signatures": ["ICLR.cc/2026/Conference/Submission22092/Reviewer_bvUH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22092/Reviewer_bvUH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762231921185, "cdate": 1762231921185, "tmdate": 1762942062312, "mdate": 1762942062312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}