{"id": "P7NUVF6wo4", "number": 22996, "cdate": 1758337914526, "mdate": 1759896836897, "content": {"title": "VeriBench: End-to-End Formal Verification Benchmark for AI Code Generation in Lean 4", "abstract": "Formal code verification offers a path to provably correct software, but evaluating language models' capabilities in this domain requires comprehensive benchmarks. \nProvably correct code would eliminate entire classes of vulnerabilities, mitigate critical system failures, and potentially transform software engineering practices through inherently trustworthy implementation methodologies. \nWe present \\textsc{VeriBench}, a benchmark for assessing \\textbf{end-to-end} formal code verification in Lean 4, requiring models to generate complete program implementations with tests, specifications/theorems, and machine-checked proofs from Python references. \nOur benchmark comprises 140 tasks across five difficulty levels: 56 HumanEval problems, 41 foundational programming exercises, 10 classical algorithms, 28 security-critical programs adapted from real-world vulnerabilities and 5 programs from the Python standard library. \nTo enable comprehensive capability assessment, we establish four hierarchical evaluation subtasks with explicit metrics: \n(1) Lean 4 compilation success, \n(2) proportion of unit test passing, \n(3) correctness-theorem synthesis quality, and \n(4) proof success rates with pass@1. \nHowever, evaluation reveals significant limitations in current models: Claude 3.7 Sonnet achieves only 35.0\\% compilation success but 40.6\\% of unit test passing – while LLaMA-70B fails to compile any programs despite 50 feedback-guided attempts on a previous version of \\textsc{VeriBench}. \nModels demonstrate similar performance on theorem evaluation, reaching 0.615\\% theorem accuracy as measured by a LLM judge. \nHowever, proof generation remains particularly challenging—our DSP (Draft Sketch Proof) proving agent achieves only 28.9\\% pass@1. \nIn contrast, our trace-based self-debug agent architecture achieves 49.3\\% compilation success, demonstrating the potential of iterative, feedback-driven approaches. \nTo enable scalable evaluation, we introduce a novel methodology for certifying the trustworthiness of LLM judges. \nWe validate our theorem/specification LLM judge by applying a novel trustworthiness methodology that verifies adherence to fundamental logical properties, such as consistency and monotonicity, thereby facilitating reliable, automated generation of theorems and specifications.\n\\textsc{VeriBench} establishes a rigorous foundation for developing AI systems capable of synthesizing provably correct, bug-free code, thereby advancing the trajectory toward more secure and dependable software infrastructure.", "tldr": "VeriBench introduces a benchmark for evaluating language models on end-to-end Lean 4 code verification, showing current models struggle while self-optimizing agents demonstrate meaningful progress toward provably correct software generation.", "keywords": ["formal verification", "Lean 4", "language models", "benchmark", "code synthesis", "correctness proofs", "software reliability", "generative AI", "secure programming", "trustworthy AI"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f24bd52a5b9139e4311109bdeee80b27c311d838.pdf", "supplementary_material": "/attachment/621bb2cbf21407713f08d9f7a147df5615b3dacd.pdf"}, "replies": [{"content": {"summary": {"value": "The paper introduces VeriBench, a benchmark for evaluating large language models’ (LLMs) ability to generate provably correct programs in Lean 4.\nVeriBench requires models to translate Python code into Lean 4 programs that include implementations, unit tests, specifications/theorems, and machine-checked proofs, offering an end-to-end assessment of formal code verification capabilities.\nThe benchmark spans 140 tasks across five difficulty levels — HumanEval, EasySet, CSSet, SecuritySet, and RealCodeSet — combining classical algorithmic and real-world security-critical programs.\nResults show that current models (e.g., Claude 3.7 Sonnet, DeepSeek-Prover V2) struggle severely with formal proofs and real-code verification, highlighting proof synthesis as a key bottleneck. VeriBench thus establishes a rigorous foundation for advancing trustworthy AI-assisted formal verification."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- End-to-End Benchmark\n\nVeriBench uniquely integrates all stages of formal code generation, implementation, testing, specification, and proof within a single Lean 4 environment.\n\n- Diversity\n\nInclusion of security-critical and production-grade programs (from MIT 6.858 and Python stdlib) moves beyond synthetic exercises.\n\n- Evaluation Framework\n\nIntroduce Trace-based feedback-driven agents"}, "weaknesses": {"value": "- Limited Scope and Generality.\n\n\n- Limited Scale and Coverage\n\nDespite diverse subsets, all benchmarks are Python-to-Lean 4 translations in a limited number. \nIts total of 140 tasks remains small compared to other large-scale code or theorem-proving datasets.\nThis limited scale constrains statistical robustness and may not fully reflect model generalization across domains, paradigms, or theorem types. In particular, the RealCodeSet and SecuritySet are valuable but too small (only 5 and 28 programs respectively) to support strong empirical conclusions. Besides, the benchmark assumes that LLMs can semantically align Python code, Lean syntax, and proof structures, which in practice remains difficult.\n\n- Reliance on the LLM Judge for the theorem evaluation\n\nThe dataset’s generation pipeline partially relies on LLM-assisted curation (o3, Claude) before human review. While human validation mitigates bias, it introduces a risk of training contamination — future or current LLMs may have seen similar Python or HumanEval code during pretraining.\nMoreover, because the Lean translations are bootstrapped from AI models, they may implicitly encode stylistic or syntactic priors that align with specific model outputs, reducing benchmark neutrality."}, "questions": {"value": "- Given that theorem sets cannot be exhaustive, how do the authors ensure that omitted properties do not unfairly penalize or reward certain model behaviors? Could a probabilistic coverage measure or human validation subset be added?\n\n- The paper validates the LLM judge’s monotonicity and consistency, but how robust is it across architectures (e.g., GPT-4 vs. Claude)? Would ensemble or symbolic judges improve objectivity?\n\n- Since the paper mentions plans to extend VeriBench to multi-language settings, how do the authors envision handling semantic gaps (e.g., between imperative C++ and functional Lean 4) while maintaining proof-level correspondence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vi0CqVf0Ea", "forum": "P7NUVF6wo4", "replyto": "P7NUVF6wo4", "signatures": ["ICLR.cc/2026/Conference/Submission22996/Reviewer_Q2Er"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22996/Reviewer_Q2Er"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760878031802, "cdate": 1760878031802, "tmdate": 1762942469769, "mdate": 1762942469769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VeriBench, a benchmark for evaluating LLMs' capabilities in end-to-end formal code verification using Lean 4. The benchmark comprises 140 tasks across five difficulty levels (HumanEval, EasySet, CSSet, SecuritySet, RealCodeSet), requiring models to translate Python code into Lean 4 with complete implementations, tests, specifications, and machine-checked proofs. The authors propose four evaluation subtasks and introduce an agentic evaluation framework using Trace, along with a validated LLM judge for theorem quality assessment."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Comprehensive benchmark design: The inclusion of security-critical programs (MIT 6.858 labs) and real production code (Python standard library) is a significant advancement over existing benchmarks that focus primarily on textbook algorithms.\n- Agentic framework: The Trace-based evaluation with self-debug and self-improve variants demonstrates the value of feedback-driven approaches"}, "weaknesses": {"value": "- Poor Presentation: The writing is difficult to follow, and the formatting in some paragraphs is broken (e.g., Section 5.1 LLM Judge and Lines 852-855).\n- Incomplete End-to-End Evaluation: The evaluation lacks a measurement of proof verification success, such as whether generated theorems are actually proven, rather than just passing tests or receiving LLM-generated scores.\n- Limited Experimental Analysis: There is no comparison of model-generated theorems against gold theorems to identify semantic gaps. Additionally, the discussion of why self-improvement performs worse than self-debugging is limited.\n- Questionable Benchmark Task Quality: For the theorems in Table 1, it's unclear if they can truly be proven and have ground-truth proofs, as many \"sorry\" and \"admit\" statements were found in the gold examples."}, "questions": {"value": "- Regarding Theorem Proving:\n  - What percentage of the \"gold standard\" theorems in Table 1 have complete, verifiable proofs, as opposed to those marked \"sorry\"?\n  - Can these theorems truly be proven with ground-truth proofs?\n- Regarding Model Performance:\n  - Please provide a failure case analysis, including specific examples where all models fail and the reasons why.\n  - For end-to-end generation, what percentage of model-generated proofs successfully verify without needing \"sorry\"?\n  - Why does the \"self-improve\" method underperform \"self-debug\"?\n- Regarding Evaluation:\n  - What is the quality of the test cases used to evaluate the specifications and code?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "This paper introduces VeriBench, a benchmark for evaluating LLMs' capabilities in end-to-end formal code verification using Lean 4. The benchmark comprises 140 tasks across five difficulty levels (HumanEval, EasySet, CSSet, SecuritySet, RealCodeSet), requiring models to translate Python code into Lean 4 with complete implementations, tests, specifications, and machine-checked proofs. The authors propose four evaluation subtasks and introduce an agentic evaluation framework using Trace, along with a validated LLM judge for theorem quality assessment."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ak377BASsX", "forum": "P7NUVF6wo4", "replyto": "P7NUVF6wo4", "signatures": ["ICLR.cc/2026/Conference/Submission22996/Reviewer_2bBm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22996/Reviewer_2bBm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854941247, "cdate": 1761854941247, "tmdate": 1762942469451, "mdate": 1762942469451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VERIBENCH, a comprehensive benchmark aimed at evaluating end-to-end formal verification in AI code generation using Lean 4. VERIBENCH’s tasks require large language models to translate reference Python code into fully verified Lean 4 programs, including implementations, unit tests, specifications, and machine-checked proofs. The benchmark comprises 140 tasks spanning five difficulty levels, with tasks sourced from HumanEval, foundational CS exercises, standard library code, classical algorithms, and adapted real-world security-critical programs. The paper introduces agentic evaluation architectures and proposes a hierarchical metric suite, including compilation, test, theorem, and proof success rates. Evaluation across LLMs/theorem provers and agentic strategies uncovers substantial limitations of current LLMs, highlights feedback-driven improvements, and details a methodology for certifying the trustworthiness of LLM-based evaluation judges."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive, End-to-End Scope: Each benchmark item includes Python code, Lean translation, unit tests, formal theorem statements, and machine-checked proofs, allowing for holistic, multi-stage evaluation\n2. Breadth of Benchmark: VERIBENCH spans a uniquely rich spectrum of tasks, incorporating both textbook-style problems and authentic security-critical code drawn from real-world vulnerabilities\n3. Contribution to Methodology: Describes a principled approach for verifying LLM judge trustworthiness, with sanity checks for monotonicity, correctness, and completeness, also the metrics for evaluation are nice"}, "weaknesses": {"value": "1. Uneven distribution of various problem types: as the author specified at line 166, this benchmark is essentially assembled from 4 subsets, yet the size of each subset differs. The overall scope of this benchmark is also limited, which pose the question as to whether this small size would be representative enough of the complexity of this task.\n\n2. small set of tested models: The set of models evaluated in this paper is quite limited, I feel that it warrants to put more SOTA models and if possible larger theorem provers e.g. DeepSeekProver671B, GoedelProver etc.\n\n3. Related work: is currently a big chunk and should visualize this comparison in a table to make it easier (e.g. 3 column: Name+Content+Diff with VeriBench). The current related work paragraphs seem (at least partially) LM-generated with its excessive use of em-dash. It would be better if this related work section brings more work that compares Lean+Computer Security/Theoretical Computer Science related work into comparison (e.g. in AI4MATH workshops)\n\nWhile I can see there's related work in appendix, still one should present the related work section better, because currently it's very hard to read and grasp the key idea there as it's too chunky and lacks a coherent central line of story."}, "questions": {"value": "Many nuances where the manuscript could be improved:\nLine 270 onwards: many repetitive appendix K in each para, could be merge into one umbrella signpost to Appendix K and instead maybe make a figure to show each subset, how it's curated and the capability it's supposed to evaluate (e.g. I can imagine a Venn diagram of how each subset tests different sets of capability would be nice to see)\n\nSpacing issue: e.g. Figure 1 takes a lot of space that could have been reallocated for 2 figures (this seems like the authors recently converted from 2-column format to single-column) which I understand is tricky, nevertheless this warrants further improvement for better presentation\n\nOverall I think the ideas in this work are generally nice, but the presentation could be massively improved as the current form strikes me as highly coarse and maybe (speculatively) the product of a rushing timeline, so it would be great if the authors could further polish the presentation of this work, the scope and size of both number of questions and number of models covered in evaluation also makes me believe that more time put into this work may present a much better contribution to the field, hence I encourage the authors to revamp and resubmit."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FG8RTMXBLS", "forum": "P7NUVF6wo4", "replyto": "P7NUVF6wo4", "signatures": ["ICLR.cc/2026/Conference/Submission22996/Reviewer_K6Sq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22996/Reviewer_K6Sq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762166686113, "cdate": 1762166686113, "tmdate": 1762958827816, "mdate": 1762958827816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VeriBench, a benchmark designed to evaluate large language models (LLMs) on end-to-end formal verification tasks. VeriBench aims to measure a model’s ability to generate provably correct Lean 4 code from Python references, including implementations, tests, specifications, theorems, and proofs. The benchmark contains 140 tasks divided into five subsets: HumanEval, EasySet, CSSet, SecuritySet, and RealCodeSet. The authors implement a feedback-driven evaluation pipeline using TRACE and DSPy agents and propose an LLM-based judge to assess theorem quality. Experiments show that current LLMs struggle with theorem proving and compilation, while iterative, self-debugging agents achieve moderate improvements."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Tackles a timely and critical topic: formal verification of AI-generated code.\n\n\n2. Diverse benchmark composition spanning five subsets, including real-world and security-critical programs.\n\n\n3. Inclusion of agentic, feedback-driven evaluation (TRACE, DSPy) reflecting current trends in LLM reasoning.\n\n\n4. Comprehensive baseline coverage across multiple model families.\n\n\n5. The trust-validation sanity checks for the LLM judge, though limited, show some attempt at methodological care."}, "weaknesses": {"value": "1. Unclear Benchmark Definition\n    - The task definition is inconsistent. The abstract claims “complete program implementations with tests, theorems, and proofs,” but Section 3 states “translate Python code with docstring and unit tests to Lean 4 implementation.” It is unclear whether unit tests are an input or output.\n\n\n    - The precise benchmark tasks and their expected inputs/outputs are not formally specified.\n\n\n    - In HumanEval, only 56 of 164 tasks are included, but selection criteria are unexplained.\n\n\n    - The role of the imperative version in Lean 4 is ambiguous. Lean 4 is fundamentally functional; if imperative variants use monads, this should be clarified.\n\n\n    - In the SecuritySet, the translation of vulnerabilities from Python to Lean 4 is unclear. Are tasks meant to repair vulnerabilities or merely prove properties?\n\n\n    - The dataset’s content boundaries are vague. Does it include correctness/equivalence proofs or only expect them to be generated?\n\n\n    - No transparent process is given for validating theorems or ensuring dataset quality beyond “human + AI curation.”\n\n\n2. Methodological and Evaluation Issues\n    - Example “golden” outputs (Listing 2) include sorry placeholders, proof stubs that invalidate verification integrity. The paper should state whether such cheats are filtered.\n\n\n    - The proof success metric is insufficiently defined: does one theorem failure mark the whole program as failed, or are partial passes counted?\n\n\n    - Reliance on an LLM judge for correctness evaluation is not rigorous. Although the authors test monotonicity and consistency, such heuristics do not guarantee semantic faithfulness.\n\n\n    - Using the same model (Claude 3.7) as both agent and judge risks self-bias and metric circularity.\n\n\n3. Lack of Analysis and Insight\n    - Results tables mostly show that models perform poorly (e.g., 0 % theorem-proving success on RealCodeSet) but offer no qualitative or failure-mode analysis.\n\n\n    - The claim that self-debug agents improve compilation from 35 %→49 % lacks breakdown of error categories or examples of corrected failure modes.\n\n\n    - The paper thus provides little scientific insight beyond “the task is hard.”\n\n\n4. Presentation and Technical Clarity\n    - Several notational and typographical errors impede understanding:\n        - Section 4: inconsistent Post(x) vs. Post(x, Prog) definitions.\n        - Line 193: “Input →” missing predicate; Line 197: “; (x) ==>” likely meant Pre(x).\n\n\n    - Minor errors:\n        - Line 166, “VeriBench consists of four subsets”, should be five subsets;\n        - Line 274 and 280, missing trailing dot and incomplete sentence;\n        - Line 292, “generation pipeline were a second human” should be “generation pipeline where a second human”;\n        - Line 411, duplicate “end-to-end”;\n        - Incomplete sentence in “Future work” subsection in Section 9, consider rephrasing them;\n\n\n5. Incremental Novelty\n    - VeriBench largely repackages existing ideas from VERINA (Lean end-to-end tasks), CLEVER (spec + proof), and FVAPPS (machine-generated verification problems).\n\n\n    - The addition of small real-code and security subsets and agentic evaluation loops does not constitute a substantial methodological advance."}, "questions": {"value": "Please address my comments in the \"Weaknesses\" section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QYWO9XYVqQ", "forum": "P7NUVF6wo4", "replyto": "P7NUVF6wo4", "signatures": ["ICLR.cc/2026/Conference/Submission22996/Reviewer_mnVJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22996/Reviewer_mnVJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762330911574, "cdate": 1762330911574, "tmdate": 1762942468820, "mdate": 1762942468820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VeriBench, a benchmark to formally verify automated code generation in Python. The paper considers previous benchmarks in the literature and constructs its own benchmark to address some of the shortcomings in the previous benchmarks. It further uses an automated LLM Judge to evaluate the correctness and comprehensiveness of the verification system. Based on that, it defines a new Quality Score. Finally, the paper evaluates the abilities of several LLMs on VeriBench based on its quality score. It reports that existing LLMs struggle on VeriBench while providing automated feedback to the LLMs help them improve their output and achieve higher accuracies."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The topic is novel and interesting in my view and the benchmark has the potential to be useful for the community.\n\nThe choice of problems in the benchmark are an improvement over the previous benchmarks."}, "weaknesses": {"value": "- My main concern is that results are mostly based on an LLM judge that may not be reliable and the paper does not make a convincing effort that LLM judge is reliable. My second main concern is that the quality score is based on comprehensiveness which seems subjective and not a concrete measure that can be reliably evaluated and generalized. The paper states: \"comprehensive is difficult if not impossible to guarantee even during the gold reference generation of the benchmark.\"\n\n- I would like to see clear justifications about the quality score and possibly other scores that are less subjective. It would be best if the quality score is somehow compared with other measures such as human judgement.\n\n- I find the method used by the paper for the LLM judge premature and unconvincing. Specifically, the paper does not establish that the LLM judge is reliable enough for a scientific publication. We have results in the literature that have reported very high accuracies using LLM judges while subsequent work have demonstrated that those LLM judges were not reliable overall. For example, Herald translator reported accuracy of 94% on miniF2F evaluated by an LLM judge -- they also evaluated their judge on some small subset suggesting that it is reliable. Later, it was revealed that accuracy of 94% was inaccurate and the correct accuracy was 67%. I would like to see concrete and reliable evaluation of results by this paper. For example, if the paper reports that all the evaluations in the paper are human checked, my concern will be resolved.\n\n\n\n- I would have liked to see one case study from beginning to end in the appendix as it appears in the dataset with the gold references along with some LLM outputs and the resulting scores.\n\n\n- Writing can be improved. Sometimes, writing is not smooth. Moreover, in many instances, the methodology is not described clear enough in my view.\n\n\n- Clearly, many LLMs are not very capable of generating Lean proofs for correctness of generated codes. This, most likely, has to do with the training set of these LLMs. Lean proofs for the correctness of Python codes is not abundantly available and most of such data is new. The related work section in the paper reflects this. It is very likely that these LLMs are not trained on such data while they are trained on large amounts of Python code. It would be insightful if the paper attempts to train a LLM and see if it can gain the capability for proving the correctness of generated codes.\n\n- The efforts of the paper in providing automated feedback is insightful but does not go far enough, in my view. For example, see the work by Goedel Prover v2 which trains its LLM to use the automated feedback generated by the Lean compiler. There are also other work in the literature that use the feedback from Lean compiler programmatically.\n\n\n\n\n\n-----------------\n\n\nMinor comments:\n\n- It seems to me that the list of contributions are inflated. Going from one item to another at the end of page 2, in my view, contributions are being repeated in different ways under different bullet points.\n\n- Writing can be improved. Some sentences do not read well, e.g., \"We discover model cannot prove a single theorem in this set.\" It is not clear which model the sentence refers to.\n\n- It is also not so clear to me what the paper means when t says: \"whereas starting from a fresh natural-language description would add an unnecessary detour that today’s LLMs no longer require.\" Does the paper mean that one can directly start from Python code because translating natural language description to Python is trivial for the LLMs? If that's what the paper is trying to say, I don't think this is a correct statement in general. Such sentence should be defined specifically for certain problems and not remain vague for the reader."}, "questions": {"value": "Please see the weaknesses. Any clarifications may be helpful in evaluating the contribution. I'd be happy to revise my score based on the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9ZhdeIH3NU", "forum": "P7NUVF6wo4", "replyto": "P7NUVF6wo4", "signatures": ["ICLR.cc/2026/Conference/Submission22996/Reviewer_FCFR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22996/Reviewer_FCFR"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762412778941, "cdate": 1762412778941, "tmdate": 1762942468431, "mdate": 1762942468431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}