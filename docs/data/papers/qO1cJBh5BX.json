{"id": "qO1cJBh5BX", "number": 21847, "cdate": 1758322656936, "mdate": 1759896900113, "content": {"title": "Probabilistic DiffusionNet: A geometry informed probabilistic generative surrogate for PDEs", "abstract": "We propose a probabilistic generative extension of the DiffusionNet architecture, widely used for surface learning tasks, by introducing latent random variables derived from a stochastic reformulation of the underlying diffusion process. The resulting probabilistic model can be used as a resolution-invariant and uncertainty-aware surrogate for the trace solution map of PDEs whose boundary conditions are determined by surface geometry. Such a surrogate can expedite and inform typical engineering design and optimisation processes that require computationally burdensome computational fluid dynamics (CFD) analysis pipelines. We demonstrate that the proposed architecture produces superior uncertainty quantification (UQ) performance on standard CFD datasets without sacrificing predictive accuracy, while enjoying lower computational cost compared to other prevalent geometry-informed PDE surrogates endowed with UQ capabilities.", "tldr": "We reformulate DiffusionNet as a probabilistic model to learn PDE solutions with UQ on 3D geometries", "keywords": ["SPDE", "DiffusionNet", "PDE", "CFD", "UQ", "Variational Inference", "VAE", "Generative Models", "Stochastic Process", "Gaussian Process", "Neural Operators"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc4994f56e0deb2347d396d9b889c862ebf15d7a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a methodology for producing uncertainty quantifying surrogate models where geometry plays a key role in the field to be estimated. They base their methodology on pre-existing GNNs based on eigen-decompositions of the discretized Laplace-Beltrami operator. The aim is a surrogate model whose confidence should vary on unseen data. The method is test on CFD data on car geometries and shows to predictive accuracy with high quality UQ."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper focuses on a very important, underexplored problem, in surrogate modelling for PDEs.\n- The proposed method is very well explained with great clarity.\n- Interesting and relevant theoretical results are reported.\n- Care is taken in conceptually comparing and contrasting the proposed method to existing approaches, and the special cases where these overlap.\n- The discussions in the appendix around UQ alternatives is usefull."}, "weaknesses": {"value": "- As mentioned in the conclusion, eigen-decompositions of large cotangent-Laplacian matrices can get quite expensive and will grow like $O(n^3)$. In light of this, I would think it useful to report some compute cost metric, such as train time and prediction time for example (or some other suitable metric). The method might still be worth the cost if much more expensive, but this is important information which might help practitioners decide which tool is right for their problem. Can you add this evaluation to the manuscript? Some information is provided is Table 7, although this is limited and does not cover all experiments.\n\n- Mentioned in the paper is the idea that UQ in this context is useful to help practitioners trust outputs from a model. It would be good have a test that shows high uncertainty for a collection of geometries which is considered \"out of distribution\".\n\n- Studying another class of PDEs would be useful. For example, the Helmholtz problem, where the solution field depends very strongly and nonlocally on the geometry. Problems in elasticity and shell deformation would also be of interest to practitioners."}, "questions": {"value": "- line 75: \"Graph Neural Graph Neural Network\"\n\n- Can the relevance of the theoretical results be better explained and how they relate to practice.\n\n- Did the diagonal approximation in the variational posterior cause any issues? Do you expect any gains to be made from better variational approximations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Srd9YgDGFL", "forum": "qO1cJBh5BX", "replyto": "qO1cJBh5BX", "signatures": ["ICLR.cc/2026/Conference/Submission21847/Reviewer_1PJd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21847/Reviewer_1PJd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761150081648, "cdate": 1761150081648, "tmdate": 1762941954893, "mdate": 1762941954893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a method based on DiffusionNet for predicting distributions on surfaces. The authors modify DiffusionNet with a stochastic Diffusion Operator to allow for uncertainty quantification. The results show that PDN is competitive with current geometric baselines in both accuracy and UQ metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The perspective of discretization invariance from diffusion is interesting\n- The results on uncertainty quantification are good, when compared to models that are not ensembled. \n- The theoretical derivations are well explained and seem to be well motivated"}, "weaknesses": {"value": "- The primary weakness is that the model does not outperform current baselines, such as Transolver. Additionally, newer geometric learning baselines (ABT-UPT (https://arxiv.org/pdf/2502.09692), Erwin Transformer (https://arxiv.org/abs/2502.17019)) have been proposed which outperform prior Transolver/GINO/GNN-based methods on ShapeNetCar and more complex CFD datasets. \n    - This isn’t necessarily a deal-breaker, since PDN/DN offers a new perspective on surrogate models, but is a weakness.\n- The prediction seems to only be for surface values (surface pressure, wall shear stress), however, in practice, volumetric fields such as velocity/pressure are also important and are jointly predicted by other baselines\n\nOverall, I think the idea is presented well and is interesting, although it does not outperform current methods."}, "questions": {"value": "- There seems to be a typo on line 380, should ‘PDE’ be ‘PDN’\n- The reference : \" Anima Anandkumar, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Nikola Kovachki, Zongyi Li,\nBurigede Liu, and Andrew Stuart. Neural operator: Graph kernel network for partial differential\nequations. In ICLR 2020 workshop on integration of deep neural models and differential equations,\n2020\" seems to have the wrong author order (https://arxiv.org/abs/2003.03485)\n- I am curious about wall shear stress predictions and drag/lift prediction (from integrating x or y component of WSS and pressure forces along the surface).\n    - Drag is commonly reported, and in general, is what practitioners care about in CFD analyses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "loJSxdvKN5", "forum": "qO1cJBh5BX", "replyto": "qO1cJBh5BX", "signatures": ["ICLR.cc/2026/Conference/Submission21847/Reviewer_ioK4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21847/Reviewer_ioK4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761673290378, "cdate": 1761673290378, "tmdate": 1762941954488, "mdate": 1762941954488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new architecture, Probabilistic DiffusionNet (PDN), for learning uncertainty-aware surrogate models for Partial Differential Equations (PDEs) where the solutions depend on surface geometry. The work extends the existing DiffusionNet (DN) architecture which is a powerful model for learning surfaces, by reformulating the core deterministic diffusion mechanism of DN to be stochastic in nature. This stochastic diffusion operator is derived from the spectral solution of a stochastic heat equation (SPDE) and through this reformulation, the authors are compelled to inject spatially-correlated noise directly into the message-passing layers of their model. The architecture is constructed as a hierarchical probabilistic generative model similar to a Variational Auto-Encoder (VAE) where latent random variables $z$ are introduced at each layer. The model is fitted via amortized variational inference by maximizing the Evidence Lower Bound (ELBO)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The formulation of PDN as a latent variable model is sound. The authors clearly define the generative process $p_\\theta(u|v, M^h)$, the observational model $p(y_s|u)$, and the amortized variational encoder $q_\\phi(Z|Y,V,\\mathcal{M})$. The use of the ELBO (Eq. 14) for optimization is a standard and appropriate choice for this framework. Central to the method—the stochastic diffusion operator $\\mathcal{SP}_{t,\\eta}$—there is nothing arbitrary. The operator is carefully derived from the spectral solution of the stochastic heat equation (Eq. (5)). This derivation is in Sec. 3.1 and proved in App. B which gives a rigorous mathematical justification for the construction of the model."}, "weaknesses": {"value": "1. In Appendix C, the authors ablate two types of amortized encoders: \"Partial Diffusion (PD)\" and \"DiffusionNet+ (DN+)\". They state that the DN+ encoder is immune to the \"basis ambiguity problem\", whereas the PD encoder (used in the main paper) suffers from it. The authors choose the PD encoder because it has ~4x fewer parameters and achieves similar performance. This is a slightly unsatisfying trade-off. It is surprising that the theoretically less-robust encoder performs just as well. The paper would be stronger if it either used the more principled DN+ encoder or provided a deeper analysis of why the basis ambiguity issue, which they acknowledge, does not seem to degrade performance in this specific VAE framework.\n\n2. The stochastic operator $\\mathcal{SP}_{t,\\eta}$ introduces a new and apparently important set of parameters, $\\eta$, which shape the noise spectrum through the relation $q_k = \\lambda_k^{2\\eta}$. These parameters determine how strong and what kind of stochastic perturbations occur at each layer. However, the paper never clearly explains how the $\\eta$ parameters are actually chosen or managed. Are these hyperparameters fixed or learned parameters that are optimized during training? If they are learned, an analysis of their converged values would be valuable. If they are fixed, an ablation study on their value is needed in order to determine the model’s sensitivity to this key UQ governing parameter.\n\n3. The authors correctly identify the reliance on the eigendecomposition of the cotan-Laplacian as a limitation for scalability. This cost is paid offline, so it doesn't affect inference or training time (as reported in Table 7), but it is a significant pre-computation step that could become prohibitive for meshes with millions of vertices. While this is noted as future work, it is a practical barrier to the \"industrial scale\" applications the paper targets."}, "questions": {"value": "1. The experiments are thoroughly conducted on two standard CFD benchmarks (ShapeNet car and Ahmed bodies), both involving the Reynolds-Averaged Navier-Stokes equations. While the results are strong, this focuses the paper's validation on a single class of physical problems (fluid dynamics). Could the authors comment on the expected generality of Probabilistic DiffusionNet? How readily would the proposed stochastic diffusion operator and its theoretical underpinnings (Theorem 3.1) apply to other types of PDEs on surfaces, such as problems in structural mechanics (e.g., shell elasticity) or electromagnetics (e.g., surface currents), which also rely heavily on geometric properties?\n\n2. Could the authors please expand on the choice of the \"Partial Diffusion\" encoder? Given that the \"DN+\" encoder is more theoretically robust by avoiding basis ambiguities, what is the intuition for why it did not outperform the simpler PD encoder (Table 4)? Does the basis ambiguity in the PD encoder perhaps act as a form of regularization for the variational approximation, or is the issue simply not a practical concern for these datasets?\n\n3. How are the noise spectrum parameters $\\eta_l$ for each stochastic block $l$ (from Eq. 7 and 8) treated? Are they learnable parameters optimized jointly with $\\theta$ and $\\phi$? If so, do they converge to different values at different layers, and how does this affect the multi-scale uncertainty injection? If they are fixed hyperparameters, how were they selected, and how sensitive is the model's UQ performance (e.g., NLL or MCAL) to this choice?\n\n4. The stochastic heat equation in Eq. (5), $\\partial_t v = \\Delta_M v + \\mathcal{W}^Q$, appears to use a time-independent spatial noise field $\\mathcal{W}^Q$. The mild solution in Eq. (1639) also suggests this. This formulation is slightly different from more common SPDE formulations that involve, for example, space-time white noise. Could the authors give the physical or mathematical justification for having chosen this particular form of SPDE? Was a formulation involving time varying noise considered? How might this affect the final stochastic operator $\\mathcal{SP}_{t,\\eta}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wqcisCLhQt", "forum": "qO1cJBh5BX", "replyto": "qO1cJBh5BX", "signatures": ["ICLR.cc/2026/Conference/Submission21847/Reviewer_nqZA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21847/Reviewer_nqZA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885822945, "cdate": 1761885822945, "tmdate": 1762941953962, "mdate": 1762941953962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a probabilistic version of DiffusionNet learning the map from the irregular geometries and boundary conditions to the PDE solution. The key idea is to replace the deterministic diffusion operator in DiffusionNet by a stochastic diffusion operator, which is equivalent to placing a prior over the message passing step. Then the paper uses variational inference to estimate the predictive distribution of the solution field."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is written clearly. \n2. The method looks sound."}, "weaknesses": {"value": "1. The motivation is a bit skeptical. There can be many simpler choices for learning a probabilistic version of DiffusionNet to enable UQ. The paper proposes a rather complex framework --- injecting noises via SDE in message passing, and performing amortized VI --- which does not seem a strong necessity. For instance, why not use MC dropout, Deep Ensemble, Laplace method, or SWA-Gaussian (SWAG)? These methods all require minor or even no modification of the existing model/training, with a little bit extra work. From the methodology perspective, it is unclear where the advantage of the proposed method stands out, in addition to extra complexity. \n\n2. Empirical performance is not supportive. The experimental results do not demonstrate the proposed method can largely improve simpler alternatives. Looking at Table 2, the performance of DN-DO/LA/ME is often comparable to or even better than PDN. It strengthens the doubt about the meaning/necessity of PDN --- why not we retreat to these classical, simple yet also powerful probabilistic training method? Why should we develop a new method? \n\n3. The experiments are not sufficient. Only two datasets are employed for testing, which is below the bar in this community. In addition, there is no standard deviations and significance analysis, making it hard to conclude whether or not  there is a difference between PDN and competing methods."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RE23CSMFCS", "forum": "qO1cJBh5BX", "replyto": "qO1cJBh5BX", "signatures": ["ICLR.cc/2026/Conference/Submission21847/Reviewer_BNtL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21847/Reviewer_BNtL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762181058741, "cdate": 1762181058741, "tmdate": 1762941953658, "mdate": 1762941953658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}