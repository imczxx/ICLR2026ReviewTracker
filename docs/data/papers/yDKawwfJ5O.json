{"id": "yDKawwfJ5O", "number": 1269, "cdate": 1756868548428, "mdate": 1759898217898, "content": {"title": "DeepEyesV2: Toward Agentic Multimodal Model", "abstract": "Agentic multimodal models should not only comprehend text and images, but also actively invoke external tools, such as code execution environments and web search, and integrate these operations into reasoning. In this work, we introduce DeepEyesV2 and explore how to build an agentic multimodal model from the perspectives of data construction, training methods, and model evaluation. We observe that direct reinforcement learning alone fails to induce robust tool-use behavior. This phenomenon motivates a two-stage training pipeline: a cold-start stage to establish tool-use patterns, and reinforcement learning stage to further refine tool invocation. We curate a diverse, moderately challenging training dataset, specifically including examples where tool use is beneficial. We validate DeepEyesV2 across real-world understanding, mathematical reasoning, and search-intensive benchmarks, demonstrating that systematic tool integration enables reliable and extensible multimodal reasoning behaviour. Moreover, DeepEyesV2 exhibits task-adaptive tool invocation, tending to use image operations for perception tasks and numerical computations for reasoning tasks. Reinforcement learning further enable complex tool combinations and allowing model to selectively invoke tools based on problem context. We hope our study can provide guidance for community in developing agentic multimodal models.", "tldr": "", "keywords": ["DeepEyesV2", "Agentic Multimodal Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fbc7304b93a8d85415c497e3e1297b2f9e79d31e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents a sound and well-executed study on building an agentic MLLM using SFT and RL, unifying code execution and web search. Through extensive experiments, the authors show that their work DeepEyesV2 presents significant gains over prior work. The analysis and ablation studies provided valuable insights into why direct RL on a base model doesn't elicit robust tool-use."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Extensive experiments using RL alone to post-train Qwen2.5-VL and the thorough analysis are interesting and insightful.\n\n2. Comprehensive evaluation on 8 benchmarks and analysis. Interesting and insightful comparison between pre-RL and post-RL in terms of tool use patterns.\n\n3. Strong results across benchmarks."}, "weaknesses": {"value": "1. Lack of comparison with a concurrent work WebWatcher, whose core methodologies seem very similar to this work. WebWatcher's performance is cited in this work but its methodological similarities are not discussed.\n\n2. Unclear how much of the gain on search benchmarks comes from the use of SerpAPI. Do baselines and this work use the same set of search APIs?"}, "questions": {"value": "1. Will the curated dataset be released? If not, the results would be hard to reproduce since this \"carefully curated training corpus\" is fundamental.\n\n2. SerpAPI was used for image search, but it's unclear if any API was used for text search. Does it simply use Google search? If so, did you encounter rate limiting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "09liJZwVaY", "forum": "yDKawwfJ5O", "replyto": "yDKawwfJ5O", "signatures": ["ICLR.cc/2026/Conference/Submission1269/Reviewer_xzDS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1269/Reviewer_xzDS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761517619233, "cdate": 1761517619233, "tmdate": 1762915723733, "mdate": 1762915723733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DeepEyesV2, an agentic multimodal model designed to actively invoke external tools like code execution and web search and integrate their outputs into its reasoning loop. The authors first demonstrate that direct reinforcement learning (RL) alone fails to teach robust tool use, often leading to reward hacking. To solve this, they propose a two-stage training pipeline: (1) a \"cold-start\" supervised fine-tuning (SFT) stage using a carefully curated dataset of difficult, tool-beneficial examples to establish reliable tool-use patterns , and (2) an RL stage to refine and adapt these skills. The experimental results showing DeepEyesV2 outperforms existing models on real-world understanding, mathematical reasoning, and search benchmarks. The analysis also reveals that the model learns task-adaptive tool invocation (e.g., image operations for perception, computation for reasoning) and that RL enables more complex, context-aware tool combinations and efficiency."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is well written and easy to follow.\n- The paper goes far beyond reporting final scores by providing deep insights into the learning dynamics.\n  - A discovery of \"Adaptive Thinking\", where the SFT model over-relies on tools, and the RL stage teaches it the efficiency of when not to use tools .\n- The performance is well, the authors validate DeepEyesV2 across a wide and diverse range of benchmarks, covering real-world understanding, mathematical reasoning, and search-intensive tasks."}, "weaknesses": {"value": "- A substantive weakness of the paper is its limited methodological novelty, as its core SFT + RL two-stage training paradigm and reasoning CoT data curation are well-established approaches for reasoning-based models, making the work feel more like a high-quality technical report than a novel research contribution.\n- The paper correctly observes that the SFT model \"over-relies on tools\" but fails to investigate if this is merely a statistical artifact of the cold-start data's high tool-calling density. More critically, the claim that RL fosters \"adaptive efficiency\" by decreasing tool use is presented as a raw observation without explaining the underlying mechanism, especially since the simple reward function ($R = R_{acc} + R_{format}$) provides no explicit incentive for this emergent efficiency.\n- While the paper provides strong analyses and experiments, it is questionable whether supervising only the correctness of the final answer can effectively ensure the accuracy and relevance of the tool-use process itself. For instance, if the model invokes an irrelevant or unnecessary tool but still manages to produce the correct final answer, it receives a high reward, indistinguishable from that of a model that used tools logically. This makes the reasoning process uninterpretable and unreliable."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8SwNhgWdMo", "forum": "yDKawwfJ5O", "replyto": "yDKawwfJ5O", "signatures": ["ICLR.cc/2026/Conference/Submission1269/Reviewer_zneT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1269/Reviewer_zneT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966041835, "cdate": 1761966041835, "tmdate": 1762915723625, "mdate": 1762915723625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DeepEyesV2, an agentic multimodal model that extends beyond traditional perception and reasoning capabilities to actively invoke external tools such as code execution environments and web search APIs. The authors observe that direct reinforcement learning alone fails to induce robust tool-use behavior, motivating a two-stage training pipeline: (1) a cold-start stage using supervised fine-tuning to establish initial tool-use patterns, and (2) a reinforcement learning stage to refine tool invocation strategies. The paper curates a diverse, moderately challenging training dataset with examples where tool use is beneficial, and validates DeepEyesV2 across multiple benchmarks including visual understanding, mathematical reasoning, and search-intensive tasks. A key finding is that DeepEyesV2 exhibits task-adaptive tool invocation, selectively using image operations for perception tasks and numerical computations for reasoning tasks, with reinforcement learning enabling complex tool combinations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Good presentaion demonstrates task-adaptive tool invocation (image ops for perception, computation for reasoning)\nPractical two-stage training approach (cold-start SFT + RL refinement)\n2. Task-Adaptive Behavior: The finding that models learn to selectively invoke different tools based on task requirements (image operations for perception, computation for reasoning) is interesting and suggests genuine understanding rather than blind tool use."}, "weaknesses": {"value": "1. Insufficient analysis of what RL learns: The paper lacks detailed examination of how tool-use patterns evolve during training, what new behaviors emerge, and when the model makes mistakes in tool invocation. Learning curves and failure analysis would strengthen the claims.\n2. Inadequate efficiency analysis: Missing quantitative data on tool call frequency, success rates, and computational overhead. \n3. Unclear generalization to novel tools: Evaluation limited to a fixed tool set. Whether the model can adapt to new tools or requires retraining remains unexplored.\n4. Limited novelty in training paradigm: The two-stage SFT + RL approach is standard in LLM literature. The paper needs clearer articulation of what makes multimodal tool use fundamentally different or novel techniques beyond established methods."}, "questions": {"value": "1. Can you provide ablation studies showing performance gains from RL versus SFT only, and analyze the primary failure modes in tool invocation?\n2. What are the quantitative metrics on tool call frequency, success rates, and computational overhead (training time and inference latency)?\n3. Can the model generalize to new tools not seen during training, and if so, how much additional data is required?\n4. What specific reward design prevents reward hacking, and what makes multimodal tool use fundamentally different from text-only approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ECKP4U6JQc", "forum": "yDKawwfJ5O", "replyto": "yDKawwfJ5O", "signatures": ["ICLR.cc/2026/Conference/Submission1269/Reviewer_UjKn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1269/Reviewer_UjKn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998534391, "cdate": 1761998534391, "tmdate": 1762915723497, "mdate": 1762915723497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DeepEyesV2, an agentic multimodal model designed to actively use external tools—such as code execution and web search—within its reasoning process. Unlike conventional multimodal large language models that passively interpret visual and textual inputs, DeepEyesV2 integrates tool invocation into a closed reasoning loop."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The work clearly articulates the challenge of building agentic multimodal models and distinguishes itself from prior “thinking-with-image” paradigms by integrating multiple heterogeneous tools (code + search) in a unified reasoning loop.\n- The proposed two-stage pipeline (cold-start + RL) is well-motivated and empirically justified, addressing the instability of direct RL for tool learning.\n- DeepEyesV2 consistently outperforms both general-purpose and tool-augmented baselines, often matching or exceeding larger models in accuracy.\n- The behavioral study of tool distribution and adaptive invocation provides convincing evidence that the model learns non-trivial reasoning strategies rather than merely following scripted patterns."}, "weaknesses": {"value": "- The success of the approach relies heavily on the curated cold-start dataset. Details about data sources, annotation quality, and scalability are somewhat underexplored.\n- Most benchmarks are vision-centric. It remains unclear how well the approach generalizes to other modalities such as audio, video, or 3D tasks."}, "questions": {"value": "- How robust is DeepEyesV2’s tool invocation under noisy or adversarial tool outputs (e.g., failed code execution or irrelevant search results)?\n- How does DeepEyesV2 compare to closed-source agentic systems like GPT-4o with “thinking with images” capabilities under controlled conditions?\n- Is there any mechanism to control when not to invoke a tool to reduce unnecessary computational overhead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4Wqk948aVm", "forum": "yDKawwfJ5O", "replyto": "yDKawwfJ5O", "signatures": ["ICLR.cc/2026/Conference/Submission1269/Reviewer_VFi3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1269/Reviewer_VFi3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762140313384, "cdate": 1762140313384, "tmdate": 1762915723369, "mdate": 1762915723369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}