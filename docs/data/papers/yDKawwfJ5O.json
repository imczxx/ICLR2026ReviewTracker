{"id": "yDKawwfJ5O", "number": 1269, "cdate": 1756868548428, "mdate": 1763611714835, "content": {"title": "DeepEyesV2: Toward Agentic Multimodal Model", "abstract": "Agentic multimodal models should not only comprehend text and images, but also actively invoke external tools, such as code execution environments and web search, and integrate these operations into reasoning. In this work, we introduce DeepEyesV2 and explore how to build an agentic multimodal model from the perspectives of data construction, training methods, and model evaluation. We observe that direct reinforcement learning alone fails to induce robust tool-use behavior. This phenomenon motivates a two-stage training pipeline: a cold-start stage to establish tool-use patterns, and reinforcement learning stage to further refine tool invocation. We curate a diverse, moderately challenging training dataset, specifically including examples where tool use is beneficial. We validate DeepEyesV2 across real-world understanding, mathematical reasoning, and search-intensive benchmarks, demonstrating that systematic tool integration enables reliable and extensible multimodal reasoning behaviour. Moreover, DeepEyesV2 exhibits task-adaptive tool invocation, tending to use image operations for perception tasks and numerical computations for reasoning tasks. Reinforcement learning further enable complex tool combinations and allowing model to selectively invoke tools based on problem context. We hope our study can provide guidance for community in developing agentic multimodal models.", "tldr": "", "keywords": ["DeepEyesV2", "Agentic Multimodal Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6086bf3e4e3b3d2d69fc0185301712d702191b4b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents a sound and well-executed study on building an agentic MLLM using SFT and RL, unifying code execution and web search. Through extensive experiments, the authors show that their work DeepEyesV2 presents significant gains over prior work. The analysis and ablation studies provided valuable insights into why direct RL on a base model doesn't elicit robust tool-use."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Extensive experiments using RL alone to post-train Qwen2.5-VL and the thorough analysis are interesting and insightful.\n\n2. Comprehensive evaluation on 8 benchmarks and analysis. Interesting and insightful comparison between pre-RL and post-RL in terms of tool use patterns.\n\n3. Strong results across benchmarks."}, "weaknesses": {"value": "1. Lack of comparison with a concurrent work WebWatcher, whose core methodologies seem very similar to this work. WebWatcher's performance is cited in this work but its methodological similarities are not discussed.\n\n2. Unclear how much of the gain on search benchmarks comes from the use of SerpAPI. Do baselines and this work use the same set of search APIs?"}, "questions": {"value": "1. Will the curated dataset be released? If not, the results would be hard to reproduce since this \"carefully curated training corpus\" is fundamental.\n\n2. SerpAPI was used for image search, but it's unclear if any API was used for text search. Does it simply use Google search? If so, did you encounter rate limiting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "09liJZwVaY", "forum": "yDKawwfJ5O", "replyto": "yDKawwfJ5O", "signatures": ["ICLR.cc/2026/Conference/Submission1269/Reviewer_xzDS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1269/Reviewer_xzDS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761517619233, "cdate": 1761517619233, "tmdate": 1762915723733, "mdate": 1762915723733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DeepEyesV2, an agentic multimodal model designed to actively invoke external tools like code execution and web search and integrate their outputs into its reasoning loop. The authors first demonstrate that direct reinforcement learning (RL) alone fails to teach robust tool use, often leading to reward hacking. To solve this, they propose a two-stage training pipeline: (1) a \"cold-start\" supervised fine-tuning (SFT) stage using a carefully curated dataset of difficult, tool-beneficial examples to establish reliable tool-use patterns , and (2) an RL stage to refine and adapt these skills. The experimental results showing DeepEyesV2 outperforms existing models on real-world understanding, mathematical reasoning, and search benchmarks. The analysis also reveals that the model learns task-adaptive tool invocation (e.g., image operations for perception, computation for reasoning) and that RL enables more complex, context-aware tool combinations and efficiency."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is well written and easy to follow.\n- The paper goes far beyond reporting final scores by providing deep insights into the learning dynamics.\n  - A discovery of \"Adaptive Thinking\", where the SFT model over-relies on tools, and the RL stage teaches it the efficiency of when not to use tools .\n- The performance is well, the authors validate DeepEyesV2 across a wide and diverse range of benchmarks, covering real-world understanding, mathematical reasoning, and search-intensive tasks."}, "weaknesses": {"value": "- A substantive weakness of the paper is its limited methodological novelty, as its core SFT + RL two-stage training paradigm and reasoning CoT data curation are well-established approaches for reasoning-based models, making the work feel more like a high-quality technical report than a novel research contribution.\n- The paper correctly observes that the SFT model \"over-relies on tools\" but fails to investigate if this is merely a statistical artifact of the cold-start data's high tool-calling density. More critically, the claim that RL fosters \"adaptive efficiency\" by decreasing tool use is presented as a raw observation without explaining the underlying mechanism, especially since the simple reward function ($R = R_{acc} + R_{format}$) provides no explicit incentive for this emergent efficiency.\n- While the paper provides strong analyses and experiments, it is questionable whether supervising only the correctness of the final answer can effectively ensure the accuracy and relevance of the tool-use process itself. For instance, if the model invokes an irrelevant or unnecessary tool but still manages to produce the correct final answer, it receives a high reward, indistinguishable from that of a model that used tools logically. This makes the reasoning process uninterpretable and unreliable."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8SwNhgWdMo", "forum": "yDKawwfJ5O", "replyto": "yDKawwfJ5O", "signatures": ["ICLR.cc/2026/Conference/Submission1269/Reviewer_zneT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1269/Reviewer_zneT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966041835, "cdate": 1761966041835, "tmdate": 1762915723625, "mdate": 1762915723625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DeepEyesV2, an agentic multimodal model that extends beyond traditional perception and reasoning capabilities to actively invoke external tools such as code execution environments and web search APIs. The authors observe that direct reinforcement learning alone fails to induce robust tool-use behavior, motivating a two-stage training pipeline: (1) a cold-start stage using supervised fine-tuning to establish initial tool-use patterns, and (2) a reinforcement learning stage to refine tool invocation strategies. The paper curates a diverse, moderately challenging training dataset with examples where tool use is beneficial, and validates DeepEyesV2 across multiple benchmarks including visual understanding, mathematical reasoning, and search-intensive tasks. A key finding is that DeepEyesV2 exhibits task-adaptive tool invocation, selectively using image operations for perception tasks and numerical computations for reasoning tasks, with reinforcement learning enabling complex tool combinations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Good presentaion demonstrates task-adaptive tool invocation (image ops for perception, computation for reasoning)\nPractical two-stage training approach (cold-start SFT + RL refinement)\n2. Task-Adaptive Behavior: The finding that models learn to selectively invoke different tools based on task requirements (image operations for perception, computation for reasoning) is interesting and suggests genuine understanding rather than blind tool use."}, "weaknesses": {"value": "1. Insufficient analysis of what RL learns: The paper lacks detailed examination of how tool-use patterns evolve during training, what new behaviors emerge, and when the model makes mistakes in tool invocation. Learning curves and failure analysis would strengthen the claims.\n2. Inadequate efficiency analysis: Missing quantitative data on tool call frequency, success rates, and computational overhead. \n3. Unclear generalization to novel tools: Evaluation limited to a fixed tool set. Whether the model can adapt to new tools or requires retraining remains unexplored.\n4. Limited novelty in training paradigm: The two-stage SFT + RL approach is standard in LLM literature. The paper needs clearer articulation of what makes multimodal tool use fundamentally different or novel techniques beyond established methods."}, "questions": {"value": "1. Can you provide ablation studies showing performance gains from RL versus SFT only, and analyze the primary failure modes in tool invocation?\n2. What are the quantitative metrics on tool call frequency, success rates, and computational overhead (training time and inference latency)?\n3. Can the model generalize to new tools not seen during training, and if so, how much additional data is required?\n4. What specific reward design prevents reward hacking, and what makes multimodal tool use fundamentally different from text-only approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ECKP4U6JQc", "forum": "yDKawwfJ5O", "replyto": "yDKawwfJ5O", "signatures": ["ICLR.cc/2026/Conference/Submission1269/Reviewer_UjKn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1269/Reviewer_UjKn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998534391, "cdate": 1761998534391, "tmdate": 1762915723497, "mdate": 1762915723497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DeepEyesV2, an agentic multimodal model designed to actively use external tools—such as code execution and web search—within its reasoning process. Unlike conventional multimodal large language models that passively interpret visual and textual inputs, DeepEyesV2 integrates tool invocation into a closed reasoning loop."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The work clearly articulates the challenge of building agentic multimodal models and distinguishes itself from prior “thinking-with-image” paradigms by integrating multiple heterogeneous tools (code + search) in a unified reasoning loop.\n- The proposed two-stage pipeline (cold-start + RL) is well-motivated and empirically justified, addressing the instability of direct RL for tool learning.\n- DeepEyesV2 consistently outperforms both general-purpose and tool-augmented baselines, often matching or exceeding larger models in accuracy.\n- The behavioral study of tool distribution and adaptive invocation provides convincing evidence that the model learns non-trivial reasoning strategies rather than merely following scripted patterns."}, "weaknesses": {"value": "- The success of the approach relies heavily on the curated cold-start dataset. Details about data sources, annotation quality, and scalability are somewhat underexplored.\n- Most benchmarks are vision-centric. It remains unclear how well the approach generalizes to other modalities such as audio, video, or 3D tasks."}, "questions": {"value": "- How robust is DeepEyesV2’s tool invocation under noisy or adversarial tool outputs (e.g., failed code execution or irrelevant search results)?\n- How does DeepEyesV2 compare to closed-source agentic systems like GPT-4o with “thinking with images” capabilities under controlled conditions?\n- Is there any mechanism to control when not to invoke a tool to reduce unnecessary computational overhead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4Wqk948aVm", "forum": "yDKawwfJ5O", "replyto": "yDKawwfJ5O", "signatures": ["ICLR.cc/2026/Conference/Submission1269/Reviewer_VFi3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1269/Reviewer_VFi3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762140313384, "cdate": 1762140313384, "tmdate": 1762915723369, "mdate": 1762915723369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response - 1 / 3"}, "comment": {"value": "We thank the AC for organizing the review process and all reviewers for their time and thoughtful evaluations. We are pleased that the adequacy of our experiments and analyses was recognized. Below, we provide a general response to the comments to further clarify our contributions and offer deeper analyses. Besides, we also add the **analysis of RL** and **performance comparison with proprietary models** in ***Appendix*** in **blue**. \n \n> **Novelty**\n \nWe adopt a two-stage training approach combining SFT and RL, which is standard in LLM literature (such as DeepSeek and Qwen), but we do not claim this training method as our innovation. Our core novelty and key contribution is being the first to develop a multimodal agent model that organically integrates code execution and search. We summarize our novelty as follows:\n \n1. **Code Execution for Tool Use.** Previous works such as DeepEyes and PixelReasoner expose a single tool via function calls. In contrast, DeepEyesV2 lifts this limitation by executing code, enabling flexible invocation of diverse tools. Unlike text-only works like ReTool that only are restricted to numerical computation, DeepEyesV2 also supports a range of image operations.\n \n2. **Unified, interleavable Multi-tool Agent.** Whereas previous works can only perform image operations or search in isolation, DeepEyesV2, however, is ***the first*** to organically integrate code execution with search within a single reasoning loop, where search and code execution can interact rather than operate independently. This integration significantly raises the upper limit of reasoning. As shown in Tables 1, 2, and 3, DeepEyesV2 outperforms existing models by a considerable margin, highlighting its strong reasoning capabilities.\n \n3. **Tool-benefit–driven Data Curation.** We propose a data pipeline which explicitly (a) filters by task difficulty and (b) labels whether tool use materially improves success. This benefit-oriented curation is absent in prior work.\n \n4. **In-depth Analysis.** We analyze the dynamics of tool-use behavior in DeepEyesV2, revealing task-adaptive patterns. Besides, we also find reinforcement learning can enable more complex tool combinations and adaptive, context-aware tool invocation. \n \nWe investigate how to build an agentic multimodal model from the perspective of data, training, and evaluation. The unified agent model, data methodology centered on tool-benefit, and the agentic behaviors in multimodal settings, including adaptive tool selection, cross-tool composition, and efficiency, go beyond established single-tool or single-modality extensions of the standard training template. \n \n \n \n \n \n> **Generalization of Tool Use**\n \nSince we utilize ***code*** instead of function call for tool execution, our tool invocation offers strong generalization and does not require specific cold-start data.\n \n1. **Diverse Tool Category.** Tools are implemented through code execution, which inherently enables diverse tool functionality and extensibility.\n \n2. **Comprehensive Evaluation Breadth.** Our evaluation is not constrained to a fixed, pre-defined tool set. We assess DeepEyesV2 across 4 distinct task categories spanning 20 benchmarks, ensuring coverage of a broad spectrum of tools rather than a narrow, predetermined subset.\n \n3. **Tool Usage Distribution.** ***Figure 4*** illustrates the tool invocation distribution, where the \"other\" category encompasses various operations including rotation, image enhancement, and additional functionalities. This demonstrates DeepEyesV2's capability to leverage a wide range of tool types in practice.\n \n4. **Emergent Tool Capabilities.** ***Figure 1*** showcases an example of tool generalization: DeepEyesV2 autonomously generates code to query a stock-price API, a capability not present in the cold-start training data but acquired through reinforcement learning. This illustrates that DeepEyesV2 can develop new tool usage patterns without extra cold-start data.\n \n5. **No Need for Additional Data.** DeepEyesV2 has strong tool generalization ability and can support various types of tools through code execution. Moreover, reinforcement learning enables the emergence of tool types that do not exist in cold start scenarios. Thus, DeepEyesV2 can directly generalize to unseen tools during training without requiring any additional SFT data.\n \nIn summary, DeepEyesV2 exhibits strong tool generalization ability enabled by code execution. Our comprehensive experiments and case studies demonstrate zero-shot adoption of previously unseen tools."}}, "id": "9WTMC9rB51", "forum": "yDKawwfJ5O", "replyto": "yDKawwfJ5O", "signatures": ["ICLR.cc/2026/Conference/Submission1269/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1269/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission1269/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763609862833, "cdate": 1763609862833, "tmdate": 1763609905652, "mdate": 1763609905652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}