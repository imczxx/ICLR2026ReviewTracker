{"id": "o7avj3PWNC", "number": 20666, "cdate": 1758308761932, "mdate": 1763549381517, "content": {"title": "BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs", "abstract": "Large language models (LLMs) have recently shown strong performance on mathematical benchmarks. At the same time, they are prone to hallucination and sycophancy, often providing convincing but flawed proofs for incorrect mathematical statements provided by users. This significantly limits the applicability of LLMs in theorem proving, as verification of these flawed proofs must be done manually by expert mathematicians.\nHowever, existing benchmarks that measure sycophancy in mathematics are limited: they focus solely on final-answer problems, rely on very simple and often contaminated datasets, and construct benchmark samples using synthetic modifications that create ill-posed questions rather than well-posed questions that are demonstrably false. \nTo address these issues, we introduce BrokenMath, the first benchmark for evaluating sycophantic behavior in LLMs within the context of natural language theorem proving. BrokenMath is built from advanced 2025 competition problems, which are perturbed with an LLM to produce false statements and subsequently refined through expert review.\nUsing an LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems and find that sycophancy is widespread, with the best model, GPT-5, producing sycophantic answers 29% of the time. We further investigate several mitigation strategies, including test-time interventions and supervised fine-tuning on curated sycophantic examples. These approaches substantially reduce, but do not eliminate, sycophantic behavior.", "tldr": "", "keywords": ["llm", "reasoning", "math", "mathematics", "sycophancy", "hallucinations", "trustworthy"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3290de75e57f4eb5c728680944ad1f3b587b0e31.pdf", "supplementary_material": "/attachment/7c575fe4707ae57b342828307a5a5df76eac6be5.zip"}, "replies": [{"content": {"summary": {"value": "This work finds that the LLMs are prone to hallucination and sophistry for theorem proving. Therefore, this work builds a benchmark named BrokenMath to evaluate the sycophantic behavior in LLMs in the natural language theorem proving."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This problem is interesting. This work proposes to evaluate the hallucination and sycophancy, focusing on theorem proving.\n* The dataset construction is clear. The figure 1 present the dataset construction pipeline.\n* This work discuss several interesting observation, such as self-sycophancy.\n* The experiment parts contain various models, suggesting that the sycophancy is common in current SOTA models.\n* The prompt is given in Appendix to help reproduce the resutls."}, "weaknesses": {"value": "* The evaluation uses LLM. Therefore, it is not clear whether the evaluation is correct\n* Qwen3-4B sycophancy is even lower than Qwen3-235B and DS-V3.1. The question is why the smaller model has a lower sycophancy rate?\n* Figure 7 presents that the model performance is significantly related to the prompt, which suggests that the evaluation variance may be large with different methods."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iJrOIV8aX1", "forum": "o7avj3PWNC", "replyto": "o7avj3PWNC", "signatures": ["ICLR.cc/2026/Conference/Submission20666/Reviewer_bA42"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20666/Reviewer_bA42"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761488013176, "cdate": 1761488013176, "tmdate": 1762934055642, "mdate": 1762934055642, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Rebuttal Common Response"}, "comment": {"value": "$\\newcommand{\\evjq}{\\textcolor{red}{evjq}}$\n$\\newcommand{\\xywr}{\\textcolor{blue}{XyWR}}$\n$\\newcommand{\\fxie}{\\textcolor{green}{FxiE}}$\n$\\newcommand{\\ba}{\\textcolor{purple}{bA42}}$\n\n\nWe thank all reviewers for their feedback, and we are happy to hear they find our work novel ($\\xywr$, $\\ba$), intriguing and important ($\\evjq$, $\\xywr$, $\\ba$), our methodology well-constructed and well-argumented ($\\evjq$, $\\fxie$, $\\ba$), and our evaluation thorough and impactful ($\\evjq$, $\\xywr$, $\\fxie$, $\\ba$). We now address common reviewer concerns and questions.\n\n**Q.1 Can the authors provide additional details about the LLM-as-a-judge validation, such as a more thorough error analysis and addressing potential biases? ($\\evjq$, $\\xywr$, $\\ba$)** \n\nOf course! First, we want to clarify that the task for the LLM judge is very simple: it only needs to identify the type of behavior displayed by the model’s answer, which is independent of the answer’s logical correctness. The very high agreement rate between our judge and a human annotator (95%) demonstrates that the LLM judge is reliable for our evaluation. We now further validated this through two additional analyses: (1) an error analysis of the judge’s classifications, and (2) a cross-family evaluation to test for judge-specific biases.\n\n1. Error Analysis (App A.4)\nWe have included a confusion matrix among the four classes in Figure 9.  Three main conclusions can be drawn from this matrix:\n    - The off-diagonal entries are very small, indicating that the judge achieves high overall accuracy.\n    - For our primary task (distinguishing between sycophantic and non-sycophantic answers) the accuracy increases to 98%, as most misclassifications occur within the three non-sycophantic categories. The remaining 2% come from highly ambiguous model outputs, such as answers that point out an error in the theorem only in an edge case while still attempting to provide a proof for all other cases.\n    - The few remaining errors typically occur between the “Correct” and “Detected” categories, where the human judge determined that the theorem has been only partially recovered, while the LLM judge classified the answer as fully “Correct.”\n\n2. Judge-specific biases (App A.4)\nTo address concerns about potential self-enhancement or model-family-specific biases (e.g., an OpenAI judge favoring OpenAI solvers), we conducted a new experiment. We re-ran our evaluation using Grok-4-Fast as the judge, which achieves a 96% accuracy when differentiating sycophantic and non-sycophantic answers, slightly lower than our current judge that has a 98% accuracy.\nThe results, presented below, show a negligible difference between the ratings from the 2 judges.\n\n| Judge / Solver                  | **GPT-5 (high)** | **o4-mini (high)** | **Grok 4 Fast (Reasoning)** | **Grok 4** |\n| ------------------------------- | ---------------- | ------------------ | --------------------------- | ---------- |\n| **GPT-5-mini (medium)**         | $29.0 \\pm 4.0 \\%$           | $46.6 \\pm 4.4 \\%$             | $40.0 \\pm 4.4 \\%$                      | $34.8 \\pm 4.2 \\%$     |\n| **Grok 4 Fast (Reasoning)**     | $29.2 \\pm 4.0 \\% $          | $46.4 \\pm 4.4 \\% $            | $40.4 \\pm 4.4 \\%$                      | $35.0 \\pm 4.2 \\%$     |\n| **Bias (Grok4Fast − GPT5mini)** | **+0.2%**       | **−0.2%**         | **+0.4%**                  | **+0.1%** |\n\nThe minimal bias across all models strongly indicates that our evaluation is robust and not influenced by the judge's model family. We have updated our paper with a new section (App A.2) discussing these findings.\n\n**Q.2 Can the authors evaluate how much different prompts may affect the sycophantic behaviour? ($\\xywr$, $\\ba$)**\n\nWe implemented and tested two additional prompting strategies designed to mitigate sycophancy:\n - Contradiction Search: We instruct the model to first find a counterexample before attempting to prove the statement.\n - Sycophancy Awareness: We explicitly make the model aware of its sycophantic tendencies and instruct it to avoid confirming the user's premise if it is uncertain.\n\nThe results on the Qwen3-4B model are presented below.\n\n|                 | No mitigation | Standard mitigation | Contradiction | Aware |\n| ------------------------------- | ---------------- | ------------------ | --------------------------- | ---------- |\n| **Qwen3-4B**         | $55.6 \\pm 4.4\\%$          | $43.8 \\pm 4.4\\%$             | $44.6 \\pm 4.4\\%$                  | $49.4 \\pm 4.5 %$    |\n\nAs shown, neither of these approaches improve upon our original mitigation strategy. This reinforces our paper's conclusion that while prompt engineering can provide some alleviation, it is insufficient to fully resolve sycophantic behavior in mathematical reasoning.\n\nWe have included the new prompts in Appendix D and added the analysis in App A.7."}}, "id": "q60eKEyNxL", "forum": "o7avj3PWNC", "replyto": "o7avj3PWNC", "signatures": ["ICLR.cc/2026/Conference/Submission20666/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20666/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20666/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763550067334, "cdate": 1763550067334, "tmdate": 1763551295571, "mdate": 1763551295571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BrokenMath, presented as \"the first benchmark for evaluating sycophantic behavior in LLMs within the context of natural language theorem proving.\" The benchmark contains 504 problems from 2025 mathematical competitions, where each problem is perturbed to produce a false statement. The authors evaluate 10 state-of-the-art LLMs and find that even the best model (GPT-5) exhibits \"sycophantic\" behavior 29% of the time. The paper investigates factors influencing this behavior (difficulty, problem type) and evaluates mitigation strategies including prompt engineering and fine-tuning."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **High-quality data curation**: The benchmark uses recent 2025 competition problems with expert verification, reducing contamination risks compared to prior work using GSM8k or AIME.\n\n2. **Comprehensive evaluation**: Systematic assessment of 10 frontier models across multiple dimensions, providing valuable empirical comparisons.\n\n3. **Fine-grained behavioral classification**: The four-category taxonomy (Ideal/Corrected/Detected/Sycophant) provides more nuanced information than binary correct/incorrect classification.\n\n4. **Thorough experimental execution**: Careful validation of LLM-as-a-judge (95% agreement with human labels), detailed ablation studies, and multiple experimental settings."}, "weaknesses": {"value": "### 1. **Fundamental construct validity failure: Confounding competence with alignment**\n\nThe paper claims to measure \"sycophancy\" (an alignment issue) but provides no mechanism to distinguish it from mathematical incompetence. True sycophancy requires that a model **can** judge a statement's validity but chooses not to due to user-pleasing tendencies. The paper's methodology cannot differentiate:\n\n- **Phenomenon A**: Model accepts false statement because it cannot judge truth/falsity (competence deficit)\n- **Phenomenon B**: Model can judge but suppresses critical thinking to please user (alignment sycophancy)\n\nA rigorous operationalization would require:\n```\nFor proposition P and its negation ¬P:\n  Step 1: Filter to problems where model can correctly prove P\n  Step 2: Further filter to problems where model can correctly disprove ¬P\n  Step 3: Only on this filtered set, if the model still attempts to prove ¬P without questioning or refuting it, does this represent sycophancy.\n```\n\nThe paper performs no such filtering, rendering all claims about \"sycophancy as an alignment problem\" unsubstantiated.\n\n### 2. **Table 2 reveals that the measurement is fundamentally contaminated**\n\nTable 2 presents \"sycophantic behavior\" split by whether models can solve the original problem:\n- Solved problems: 21.5% (GPT-5 example)\n- Unsolved problems: 47.7% (GPT-5 example)\n- Gap: 26.2 percentage points\n\nThe existence of this table itself demonstrates the methodological failure. The \"sycophancy\" measurements on unsolved problems are **not measuring sycophancy at all**—they are measuring the model's inability to judge mathematical validity. This is pure competence deficit misclassified as alignment failure.\n\nThe solved/unsolved gap does not reveal \"difficulty as an influencing factor.\" It reveals that the reported sycophancy rates are **contaminated metrics** mixing:\n- True potential sycophancy (from solved subset, and even this requires further validation per Step 2 above)\n- Competence limitations (from unsolved subset, which should not be in the dataset at all)\n\nThere should be no \"All/Solved/Unsolved\" breakdown because only the subset satisfying both Steps 1 and 2 should be included in the benchmark. The presence of unsolved problems in the evaluation fundamentally invalidates the sycophancy measurements.\n\n### 3. **Cascading invalidation of all derivative analyses**\n\nWith the core measurement conflating competence and alignment, all subsequent analyses become uninterpretable within the paper's claimed framework:\n\n- **Main results (§4.1)**: The reported rates (29%-70%) are inflated by competence limitations, cannot quantify alignment issues\n- **Difficulty analysis (§4.2)**: The paper interprets the solved/unsolved gap as \"higher difficulty implies higher sycophancy,\" but this correlation precisely demonstrates that the measurement captures the mixture of alignment and competence rather than only alignment\n- **Problem type comparison (§4.2, Fig 4)**: Cannot distinguish whether proof-style vs. final-answer differences reflect difficulty (competence) or alignment dynamics  \n- **Self-sycophancy (§4.3)**: Cannot determine if increased rates reflect consistency bias (competence-related) or alignment failures\n- **Mitigation strategies (§5)**: The \"modest\" improvements are uninterpretable—are they trying to enhance critical reasoning ability or adjust alignment? The experimental design cannot answer this\n\n### 4. **Mischaracterization of related work**\n\nThe paper states that prior works \"typically modify existing final-answer problems... Results consistently show that frontier models are prone to sycophancy\" (§2, citing Xue et al., Kirichenko et al., Liu et al., Sun et al., Ouyang, Rahman et al., Ma et al.). But indeed none of these seven papers use \"sycophancy\" as their core framework.\n\n### 5. **Method is isomorphic to prior work despite claims of novelty**\n\nDespite criticizing prior work for using \"ill-posed questions\" versus \"well-posed but false\" statements (§1), the perturbation approaches are structurally identical:\n- Prior work: Perturb problems (remove constraints/add contradictions) → test detection\n- This work: Perturb problems (change conclusions) → test detection\n\nBoth evaluate the same capability: detecting problematic mathematical inputs. The \"ill-posed\" vs. \"well-posed but false\" distinction does not constitute methodological innovation—both test critical reasoning."}, "questions": {"value": "The main questions and concerns are detailed in the Weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l8KRZJvlc8", "forum": "o7avj3PWNC", "replyto": "o7avj3PWNC", "signatures": ["ICLR.cc/2026/Conference/Submission20666/Reviewer_FxiE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20666/Reviewer_FxiE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915312713, "cdate": 1761915312713, "tmdate": 1762934055086, "mdate": 1762934055086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BROKENMATH, a benchmark to measure sycophancy - LLMs \"going along\" with incorrect prompts - in natural-language theorem proving. Authors build 504 problems by perturbing recent (2025) competition problems into false but plausible statements with LLM assistance and expert verification, then evaluate frontier models using an LLM-as-a-judge protocol. They find sycophancy is widespread and worse on harder/proof-style tasks; several mitigations (prompting, agentic variants, fine-tuning) help but don’t eliminate it."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Interesting and original approach that moves beyond final-answer math to proof-style tasks with verified false statements, addressing contamination and ill-posedness critiques of prior datasets.\n\n2. Studies self-sycophancy and agentic sycophancy (best-of-n, iterative verification), expanding the phenomenon’s scope.\n\n3. Careful comparisons across problem types and difficulties; shows sycophancy rises on problems a model cannot solve.\n\n4. Empirically important finding: even top models are sycophantic a non-trivial fraction of the time (29% for GPT-5), especially on proofs, helping recalibrate expectations for theorem-proving deployments."}, "weaknesses": {"value": "1. The main classification relies on an LLM-as-a-judge. Although a 95% agreement is claimed, the paper would benefit from a larger human-labeled audit, inter-annotator agreement, and/or error analysis (e.g., where the judge mistakes “Detected” vs “Corrected”). Also, the judge choice could correlate with family-level behaviors and subtly advantage similar model families. \n\n2. Perturbations are LLM-generated then expert-tuned; there’s a risk that models learn to spot stylistic artifacts of the perturbation procedure. \n\n3. Focuses on advanced high-school/undergrad level problems; unclear generalization to research-level math or to formal-proof ecosystems. Authors note this, but it constrains impact.\n\n4. While the dataset is constructed to span algebra, geometry, combinatorics, and number theory, the results don’t report per-domain sycophancy/utility or domain-specific failure modes. Given well-known differences in LLM math behavior (e.g., geometry often needs diagrammatic or synthetic reasoning; number theory leans on modular arguments), a topic-level analysis could surface systematic vulnerabilities and make the benchmark more diagnostic."}, "questions": {"value": "1. Can you report more details on the bracket/judge prompts for best-of-n, stopping criteria for iterative verification, and cost/latency, or did I miss them somewhere?\n\n2. The mitigation evaluation depth can be potentially extended to cover broader prompt families."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NWwRDlSQVl", "forum": "o7avj3PWNC", "replyto": "o7avj3PWNC", "signatures": ["ICLR.cc/2026/Conference/Submission20666/Reviewer_XyWR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20666/Reviewer_XyWR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998039241, "cdate": 1761998039241, "tmdate": 1762934053622, "mdate": 1762934053622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the first benchmark for evaluating sycophancy in natural-language theorem proving. The benchmark uses challenging 2025 math olympiad problems to minimize contamination, generating subtle false statements via an LLM and verifying them with experts. The dataset has 504 examples (321 proof, 183 final-answer). Evaluation of frontier models shows pervasive sycophancy, especially in proof tasks and with increasing difficulty. Existing mitigation techniques provide only limited benefit."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Important and timely focus on high-stakes alignment failure in mathematical reasoning.\n2. Rigorous counterfactual construction: LLM perturbation + expert verification → plausible and difficult false theorems.\n3. Clear exposition; strong motivation and methodology description.\n4. Empirical results provide useful diagnostic insights: proof settings and harder tasks produce more sycophancy."}, "weaknesses": {"value": "1. LLM-as-judge introduces circularity and evaluation risk; the judge may share the same biases.\n2. Dataset size (504 samples) limits robustness and granularity in difficulty-stratified analysis.\n3. Assumption of minimal contamination relies on recency; no quantitative verification.\n4. Limited mechanistic analysis of how proofs fail (e.g., where sycophancy manifests in reasoning chains)."}, "questions": {"value": "1. Can you provide quantitative evidence (e.g., perplexity checks, memory probing) supporting minimal pre-training contamination?\n2. What is the human vs. judge agreement rate on incorrect proofs where sycophancy is subtle?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fr45xKkssp", "forum": "o7avj3PWNC", "replyto": "o7avj3PWNC", "signatures": ["ICLR.cc/2026/Conference/Submission20666/Reviewer_evjq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20666/Reviewer_evjq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762353229211, "cdate": 1762353229211, "tmdate": 1762934052993, "mdate": 1762934052993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}