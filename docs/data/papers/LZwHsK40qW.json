{"id": "LZwHsK40qW", "number": 11176, "cdate": 1758192098248, "mdate": 1759897602659, "content": {"title": "LatentBreak: Jailbreaking Large Language Models through Latent Space Feedback", "abstract": "Jailbreaks are adversarial attacks designed to bypass the built-in safety mechanisms of large language models. Automated jailbreaks typically optimize an adversarial suffix or adapt long prompt templates by forcing the model to generate the initial part of a restricted or harmful response. In this work, we show that existing jailbreak attacks that leverage such mechanisms to unlock the model response can be detected by a straightforward perplexity-based filtering on the input prompt. To overcome this issue, we propose LatentBreak, a white-box jailbreak attack that generates natural adversarial prompts with low perplexity capable of evading such defenses.  LatentBreak substitutes words in the input prompt with semantically-equivalent ones, preserving the initial intent of the prompt, instead of adding high-perplexity adversarial suffixes or long templates. These words are chosen by minimizing the distance in the latent space between the representation of the adversarial prompt and that of harmless requests. Our extensive evaluation shows that LatentBreak leads to shorter and low-perplexity prompts, thus outperforming competing jailbreak algorithms against perplexity-based filters on multiple safety-aligned models.", "tldr": "", "keywords": ["Large Language Models", "LLM Jailbreaking", "Safety Alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eede63ca09e294f6a267d14af8be9f2927e20cd0.pdf", "supplementary_material": "/attachment/a028ea365953d41be71ae66c4547d403971fa907.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces LatentBreak, a white-box jailbreak attack that generates natural, low-perplexity adversarial prompts through latent-space feedback. Instead of appending adversarial suffixes or long templates, the method iteratively substitutes words in the harmful prompt with semantically similar alternatives, guided by a latent-space distance objective that moves the prompt’s representation toward a “harmless” centroid. Experiments across various LLMs and multiple defenses demonstrate the effectiveness of LatentBreak, while maintaining shorter prompts and evading perplexity-based filters."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written, clearly structured, and easy to follow.\n\n- The motivation that evades perplexity-based detectors by generating low-perplexity jailbreak prompts is intuitively sound and practically relevant."}, "weaknesses": {"value": "- Limited novelty beyond representation shifting:\nThe central idea—shifting the prompt’s latent representation toward the “harmless” region—is conceptually similar to prior work on representation-space analysis of jailbreaks (e.g., Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis, Li et al., 2025; Refusal in LMs Is Mediated by a Single Direction, Arditi et al., 2024). Compared to these studies, LatentBreak mainly turns the representation insight into a heuristic substitution algorithm. While effective, this feels incremental rather than a fundamental advance.\n\n- Relation to prior work:\nCompared with recent representation-based jailbreak studies, this paper provides less interpretability or insight into the safety mechanism itself. It would benefit from a more analytical discussion connecting its latent-shift behavior to known refusal vectors or alignment subspaces.\n\n- Baseline selection is dated:\nThe paper mainly compares against earlier 2023–2024 jailbreaks (GCG, GBDA, SAA, AutoDAN, PAIR, TAP). Recent attacks that exploit representation editing, activation subtraction, or multi-turn adaptive optimization are not included, making it difficult to assess LatentBreak’s true competitiveness."}, "questions": {"value": "While I appreciate the thorough evaluation and clear writing, the lack of new conceptual contribution and relatively outdated baselines reduce the paper’s impact."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hMhPpNAyBO", "forum": "LZwHsK40qW", "replyto": "LZwHsK40qW", "signatures": ["ICLR.cc/2026/Conference/Submission11176/Reviewer_oY46"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11176/Reviewer_oY46"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761301641777, "cdate": 1761301641777, "tmdate": 1762922333256, "mdate": 1762922333256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LatentBreak (LatB), a white-box jailbreaking method designed to bypass the perplexity defense. This proposed method modifies the original harmful prompt using token-level substitution guided by Latent-Space Feedback."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is efficient and achieves higher ASR over all baseline attacks against perplexity detection."}, "weaknesses": {"value": "1. Strict White-Box Dependency: LatB is strictly a white-box attack, requiring full access to the target model's internal intermediate activations for distance calculation. This severely limits its practical applicability against most commercially deployed LLMs (e.g., closed APIs) and is a significant constraint for real-world threat modeling.\n2. Incomplete Baseline Comparison: The paper primarily focuses on comparing against high-perplexity automated attacks (GCG, SAA, AutoDAN) and a few common black-box approaches (PAIR, TAP). It lacks comparison with modern black-box semantic or persuasion-based jailbreaks, such as \"How johnny can persuade LLMs to jailbreak them: Rethinking persuasion to challenge AI safety by humanizing LLMs.\" These methods also generate low-perplexity, and natural prompts, potentially representing a more direct threat and more relevant baseline for an attack focused on generating natural-sounding prompts. The absence of this comparison diminishes the demonstrated novelty in the \"naturalness\" dimension of the attack."}, "questions": {"value": "1. The authors mention the latent space of a specific layer $l$ is used, but the justification for the choice of $l$ (e.g., the 28th layer of a 32-layer model) is not thoroughly discussed. Did the authors perform a sensitivity analysis on the layer index $l$?\n\n2. The centroid $\\mu$ is crucial. How sensitive is the attack's success rate to the size and content of the $\\mathcal{D}_{harmless}$ dataset used to compute $\\mu$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3X1SlUh4r8", "forum": "LZwHsK40qW", "replyto": "LZwHsK40qW", "signatures": ["ICLR.cc/2026/Conference/Submission11176/Reviewer_7jw3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11176/Reviewer_7jw3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956431674, "cdate": 1761956431674, "tmdate": 1762922332897, "mdate": 1762922332897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LatentBreak, a novel white-box jailbreaking attack designed to bypass the safety alignment of Large Language Models (LLMs). The authors first demonstrate that existing state-of-the-art attacks, which typically rely on optimizing adversarial suffixes or using long templates, are easily detected by robust perplexity-based filters due to the unnatural, high-perplexity tokens they introduce. To overcome this, LatentBreak operates by making strategic, semantically-equivalent word substitutions within the original harmful prompt. The core innovation lies in its feedback mechanism: instead of optimizing based on output logits, LatentBreak is guided by feedback from the model's latent space, iteratively selecting word replacements that shift the prompt's internal representation closer to a pre-computed centroid of \"harmless\" prompts. The experimental results show that this method produces shorter, more natural, low-perplexity jailbreak prompts that are highly effective at evading perplexity-based defenses and achieve a higher attack success rate than other methods under these defensive conditions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Rather than chasing output logits or refusal tokens, this paper steers internal representations toward “harmless-like” regions. And this paper also introduces MaxPPL_W and shows it largely neutralizes suffix-style attacks (often ~0.0% ASR), yet the proposed attack retains high ASR (e.g., 71.1% on Mistral-7B, 66.7% on Vicuna-13B)."}, "weaknesses": {"value": "1. The Perplexity Defense as an Impractical \"Strawman\". The work frames itself as a necessary evolution to bypass perplexity-based filtering, but it's questionable whether such filtering is a realistic defense in the first place. Model service providers generally aim to maximize utility, allowing users to input anything from complex source code to creative prose. In fact, human-written text often has higher perplexity than AI-generated text [1], so high perplexity alone is not evidence that a user’s input is illegal. Implementing a strict PPL filter would be commercially and practically unviable, as it would block a large number of legitimate users. This makes LatentBreak's primary contribution (i.e., bypassing this strict PPL filter) feel like a solution to a non-problem. \n\n2. Questionable Contribution Given the White-Box/Defense Trade-off. The attack is a pure white-box method, which inherently limits its practical threat and makes it highly susceptible to \"overfitting\" on the specific latent states of a single, local model. This sacrifice in practical applicability (i.e., giving up the black-box setting) is justified only by the paper's central argument: that it is needed to bypass robust PPL defenses. However, as argued in the previous point, this defense is itself impractical. Therefore, the paper's sacrificing practical, black-box universality in order to defeat a non-viable strawman defense calls the significance of the overall contribution into question.\n\n[1] Mitrović, Sandra, Davide Andreoletti, and Omran Ayoub. \"Chatgpt or human? detect and explain. explaining decisions of machine learning model for detecting short chatgpt-generated text.\" arXiv preprint arXiv:2301.13852 (2023)."}, "questions": {"value": "1. The authors mention transferability as future work, but this seems like a critical missing piece. Since the attack optimizes against the specific latent space of a single victim model, how transferable are the resulting \"natural\" prompts? Does a prompt generated for Llama-3-8B have any success against Llama-3-70B, or against a black-box model like GPT-4? \n\n2. The attack algorithm appears computationally exorbitant. It involves up to $I=30$ iterations, and in each iteration, it loops through all $N$ words, generating $K=20$ candidates. Each candidate requires a forward pass to get the latent representation and, critically, a call to an LLM judge ($\\mathcal{J}_{intent}$). This seems orders of magnitude more expensive than GCG, which is purely gradient-based. Could the authors provide an analysis of the wall-clock time and computational cost (e.g., in GPU hours and API calls) to generate a single jailbreak, and compare this to the cost of SOTA attacks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "otUyFxshUp", "forum": "LZwHsK40qW", "replyto": "LZwHsK40qW", "signatures": ["ICLR.cc/2026/Conference/Submission11176/Reviewer_ByYu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11176/Reviewer_ByYu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974464490, "cdate": 1761974464490, "tmdate": 1762922332551, "mdate": 1762922332551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose LatentBreak, a white-box jailbreak attack to decrease the perplexity of the crafted prompt. In detail, the authors propose to substitute words with the help of the substitution models, maintaining their original meaning and decreasing the harmfulness of the crafted input."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1 The soundness of the proposed method is good.\n\n2 The writing is easy to follow.\n\n3 The experiments are quite solid."}, "weaknesses": {"value": "1 The accessibility of model representation limits the application scenario of this attack. It can only be applied to attack open-source models. However, as far as I know, there are a lot of approaches that can bypass the internal alignment capability of the open-source models. This limitation will make this attack less attractive to the readers.\n\n2 I think the perplexity-based filtering method proposed in this paper is not a realistic defense. From Table 2, we can summarize that the ASR after defense has negative correlations with the response length. It reveals a prominent disadvantage of this method: It brings a high false positive rate when the LLMs require a long thinking process to answer complex requests like an International Mathematical Olympiad Problem. \n\n3 Insufficient discussions with the most related works. In [2], they also propose a jailbreak attack from a representation space and more discussions are needed to highlight the novelty of this paper.\n\n4 Actually, there are lots of existing works that can not only keep the length of the suffix short and achieve low PPL. For example, AutoDAN [1] combines the dual goals of jailbreak and readability and achieves a low-perplexity prompt. The in-context attack in [2] adds adversarial context to the history of the models without affecting the PPL of the adversarial prompt.\n\n\n[1] AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models\n\n[2] xJailbreak: Representation Space Guided Reinforcement Learning for Interpretable LLM Jailbreaking\n\n[3] Jailbreak and guard aligned language models with only few in-context demonstrations"}, "questions": {"value": "1 In this paper, since the authors propose to substitute the sensitive words with semantically equivalent ones, can the attacks resist the prompt-based defense, such as PAT and RPO?\n\n2 Noticing that the algorithm only incorporates substitution to renew the prompt. But as far as I know, the initialization of the prompt is significant in the optimization process. Can you explain more details?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xLyD23sw44", "forum": "LZwHsK40qW", "replyto": "LZwHsK40qW", "signatures": ["ICLR.cc/2026/Conference/Submission11176/Reviewer_KRMS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11176/Reviewer_KRMS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984838222, "cdate": 1761984838222, "tmdate": 1762922332143, "mdate": 1762922332143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}