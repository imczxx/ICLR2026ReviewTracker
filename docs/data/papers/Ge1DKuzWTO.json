{"id": "Ge1DKuzWTO", "number": 12708, "cdate": 1758209624029, "mdate": 1759897492788, "content": {"title": "ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models", "abstract": "Prevalent retrieval-based tool-use pipelines struggle with a dual semantic challenge: their retrievers often employ encoders that fail to capture complex semantics, while the Large Language Model (LLM) itself lacks intrinsic tool knowledge from its natural language pretraining. Generative methods offer a powerful alternative by unifying selection and execution, tasking the LLM to directly learn and generate tool identifiers. However, the common practice of mapping each tool to a unique new token introduces substantial limitations: it creates a scalability and generalization crisis, as the vocabulary size explodes and each tool is assigned a semantically isolated token. This approach also creates a semantic bottleneck that hinders the learning of collaborative tool relationships, as the model must infer them from sparse co-occurrences of monolithic tool IDs within a vast library. To address these limitations, we propose ToolWeaver, a novel generative tool learning framework that encodes tools into hierarchical sequences. This approach makes vocabulary expansion logarithmic to the number of tools. Crucially, it enables the model to learn collaborative patterns from the dense co-occurrence of shared codes, rather than the sparse co-occurrence of monolithic tool IDs. We generate these structured codes through a novel tokenization process designed to weave together a tool's intrinsic semantics with its extrinsic co-usage patterns. These structured codes are then integrated into the LLM through a generative alignment stage, where the model is fine-tuned to produce the hierarchical code sequences. Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods, establishing a more scalable, generalizable, and semantically-aware foundation for advanced tool-augmented agents.", "tldr": "We enable LLMs to use massive toolsets by learning compact, compositional tokens whose structure is guided by collaborative semantics, boosting scalability and multi-tool reasoning.", "keywords": ["Large Language Models", "Tool-Augmented LLMs", "Scalable Tool Use", "Tool Learning", "Collaborative Semantics"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e3dbebb196b709c1a6dd2e85d3407b16e201350c.pdf", "supplementary_material": "/attachment/ccada2ca5a89e1a41f2269e73fbd62d2e87dd14a.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes ToolWeaver, a novel generative tool retrieval framework that encodes tools into hierarchical sequences with collaborative signals between tools. The collaborative-aware tokenization process allows the framework to weave tools' intrinsic semantics with their extrinsic co-usage patterns, encouraging functionally related tools to share codes. Experiments on the ToolBench benchmark demonstrate that the framework significantly outperforms state-of-the-art methods on tool retrieval and end-to-end performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Collaborative-aware tokenization process enables it to learn robust collaborative patterns from the dense co-occurrence of shared codes \n- It outperforms other SOTA methods on tool retrieval and usage"}, "weaknesses": {"value": "- Extending the vocabulary might require extensive training. In the appendix, it seems that about 2000 tool tokens are added. I wonder if all the tool tokens are being evaluated. \n- The vector quantization method is mostly borrowed from another work [1], which is the core idea of this work. Though it is interesting to see the method applied to tool learning\n- While the authros claim the work contributes to both tool retrieval and usage, it seems that the work is more focused on the tool retrieval side. I don't see any novelty in the tool usage side, such as generating correct tool parameters according to the API documentation. \n\n\n[1] Lee et al, Autoregressive Image Generation Using Residual Quantization, CVPR 2022"}, "questions": {"value": "- Please use parentheses for in-line citations (\\citep). \n- It would be helpful to show a full trajectory end-to-end for better understanding"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q4ytWr2dd1", "forum": "Ge1DKuzWTO", "replyto": "Ge1DKuzWTO", "signatures": ["ICLR.cc/2026/Conference/Submission12708/Reviewer_fdct"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12708/Reviewer_fdct"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761697137153, "cdate": 1761697137153, "tmdate": 1762923537398, "mdate": 1762923537398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem of enabling LLMs to scale in their use of external tools (APIs). Prior generative tool‐invocation methods map each tool to a unique special token, which (a) scales poorly (very large vocabularies as number of tools grows) and (b) fails to capture collaborative semantics between tools (i.e., when combinations of tools often co‐occur). To address this, the authors propose ToolWeaver, which encodes each tool as a hierarchical sequence of discrete codes derived via vector quantisation (RQ‐VAE) plus a collaborative‐aware regulariser on tool–tool similarity. These code sequences map to new vocabulary tokens (but many fewer than tools) and the LLM is then fine‐tuned to generate these sequences when selecting tools. They conduct large‐scale experiments on a benchmark of 46,985 tools (from ToolBench) showing improved tool retrieval (NDCG) and end‐to‐end task completion (SoPR, SoWR) compared to retrieval‐based and other generative baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) Problem importance: As tool‐augmented LLMs become more capable and more tools/APIs become available, scalability of tool invocation and generalisation to new tools is a genuine bottleneck. The paper motivates this well.\n\n(2) Novel method: The hierarchical code approach is a fresh take. The use of collaborative signals between tools (via a similarity matrix and graph‐Laplacian regulariser) is a nice idea to capture co‐usage semantics.\n\n(3) Large experimental scale: Using a very large tool corpus (~47k tools) and multiple generalisation splits (unseen tools, unseen categories) strengthens the empirical claims.\n\n(4) Empirical improvements: The reported gains in retrieval metric (NDCG) and end‐to‐end task performance (SoPR, SoWR) are compelling (e.g., in Table 1 the proposed method outperforms baselines across splits)."}, "weaknesses": {"value": "(1) Generality beyond benchmark: The tool corpus and benchmark are large, but it is unclear how well the method would generalise to very different tool‐spaces (e.g., domains with little documentation, highly dynamic APIs, or tools with very rare co‐usage statistics). The approach relies on a pre‐computed tool–tool similarity matrix; in practice this may be hard to build for many novel tools. The authors should discuss limitations and robustness to sparse co‐usage data.\n\n(2) New vocabulary tokens & effect on base LLM: The method still adds new vocabulary tokens (the code‐tokens). Although they argue that the vocabulary expansion grows logarithmically rather than linearly, there is a cost: random initialization of embeddings, fine‐tuning, and potential interference with the LLM’s linguistic knowledge. While they mention that they evaluate on general-capability NLP benchmarks, this is lightly treated. More analysis would strengthen the claim that general language capabilities are preserved.\n\n(3) Interpretability of codes & collaborative semantics: It is claimed that tools sharing parent codes capture collaboration. But the paper does not show concrete examples of what the hierarchical codes look like in practice (e.g., clusters of tools that share code prefix) and how the model uses them in multi‐tool reasoning (I3 tasks). Some qualitative analyses of code structure and usage patterns would strengthen the contribution."}, "questions": {"value": "(1) Can you pls provide qualitative examples of the hierarchical codes (e.g., show codes for 5–10 tools, especially ones that share parent codes) and how the model uses them in a multi‐tool task?\n\n(2) Can you pls evaluate (or report) the impact on general text generation (e.g., on standard NLP tasks) to show the LLM’s base linguistic capacity remains unaffected?\n\n(3) Pls consider adding a failure analysis: tasks where the model fails, error types (e.g., wrong tool selection, wrong parameter generation, missing tool collaborations) would be useful for the community."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EclAY9XcnL", "forum": "Ge1DKuzWTO", "replyto": "Ge1DKuzWTO", "signatures": ["ICLR.cc/2026/Conference/Submission12708/Reviewer_grmY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12708/Reviewer_grmY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762080911987, "cdate": 1762080911987, "tmdate": 1762923537153, "mdate": 1762923537153, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ToolWeaver. The key idea is to replace the traditional “one-token-per-tool” mapping with a hierarchical, compositional code representation derived via a collaborative-aware residual quantization (RQ-VAE) process. This allows logarithmic vocabulary growth with respect to the number of tools and enables shared subcodes among semantically related tools. Experiments on ToolBench show strong improvements over both retrieval-based (BM25, ToolRetriever) and generative (ToolGen) baselines across all settings, particularly in multi-tool reasoning and generalization to unseen tools and categories."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of using hierarchical compositional code representation for tools are interesting and well-motivated."}, "weaknesses": {"value": "1. It seems the current pipeline requires retraining for each newly-added tools, which raises concerns on the generalizability side.\n2. ToolBench is a widely-used dataset in the community. The authors should include more baselines in the paper."}, "questions": {"value": "1. Regarding results in table 1, I am a bit confused on the reported results. The author mentions that \"I1 Tool., I1 Cat., and I2 Cat., where 'Tool.' and “Cat.” denote tools and categories, respectively, that are unseen during training\". Intuitively, if the tools are not seen during training, the model performance will be lower. But the reported results are comparable even higher (for some columns) on unseen tools. Could author help me understand what is going on here?\n2. While ToolWeaver scales logarithmically in vocabulary size, there is a potential possibility for the total number of code combinations leading to combinatorial explosion at training and inference time. How would the author address this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OEcVE8BCT7", "forum": "Ge1DKuzWTO", "replyto": "Ge1DKuzWTO", "signatures": ["ICLR.cc/2026/Conference/Submission12708/Reviewer_sXtZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12708/Reviewer_sXtZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762116219113, "cdate": 1762116219113, "tmdate": 1762923536679, "mdate": 1762923536679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ToolWeaver, which changes the approach of \"each tool having a new, unique token\" to \"encoding tools into a structured code\" (like zip code/hierarchical barcode). This allows large models to scalably add many tools while learning how to use them collaboratively. It mainly replaces the one-token-per-tool mapping with a hierarchical, compositional code per tool, learned via collaborative-aware residual quantization (RQ-VAE), which is very interesting and well-motivated. On a dataset containing nearly 47,000 APIs, it is more accurate and better at handling multi-tool tasks than existing methods, without compromising the model's generalizability."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Scalability. Demonstrates logarithmic vocabulary growth with better or comparable accuracy vs. state-of-the-art while degrading general NLP ability far less than one-token approaches—highly relevant as tool libraries scale, new conrtibution based on the old trie-like data structures.\n\n2. Refactor tool IDs as structured codes learned with collaborative semantics, rather than flat special tokens; this work addresses vocabulary blow-up and enables shared structure across related tools. \n\n3. The pipeline (tokenization -> alignment tuning ->  constrained decoding) is well explained with equations and an overview figure; training/inference procedures are specified."}, "weaknesses": {"value": "1. The per-batch optimal-transport uniformity reduces collisions, but the stability, or compute overhead, and behavior under streaming addition of new tools (without retraining codebooks) are unclear. How do codes evolve when tools are added/removed?\n\n2. The eval isn't super crisp when GPT-4o-mini is the judge. ToolBench is the primary testbed; API-Bank is mentioned in related work but not evaluated. Given SoWR uses GPT-4o-mini as the judge (which can bias toward itself), the adjustment is noted but still leaves some concerns—showing human evals or diverse judges would help. \n\n3. Web-scale tools or so. Since this work focuses on tool management for LLM agents, trie-constrained decoding ensures valid IDs but implicitly relies on maintaining a global registry of code sequences, akin to a catalog or lookup table. Discussing memory/latency overhead and listing the anticipated cost would be more interesting."}, "questions": {"value": "The paper states **A** reflects “collaborative potential or functional similarity,” but does not fully specify data sources or preprocessing (e.g., co-usage in training trajectories vs. category metadata vs. external logs).\n\nCan you report retrieval/E2E vs. (i) total tokens added, (ii) code length, and (iii) decode latency? How close are you to capacity in your 47k-tool setting?\n\nWhat is the computational overhead of the Sinkhorn-Knopp algorithm per batch at your stated batch sizes (you have 50 batches, right?) ? Any instability observed (e.g., oscillations) and how it was controlled (entropy regularization, temperature)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kFlYzrZU4W", "forum": "Ge1DKuzWTO", "replyto": "Ge1DKuzWTO", "signatures": ["ICLR.cc/2026/Conference/Submission12708/Reviewer_brZq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12708/Reviewer_brZq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762140585807, "cdate": 1762140585807, "tmdate": 1762923536332, "mdate": 1762923536332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge of the scalability, generalization, and semantic limitations of the “one-token-per-tool” tool learning paradigm by introducing a collaborative-aware tokenization process to generate tool representation. Specifically, the proposed ToolWeaver framework uses an RQ-VAE to learn a compositional sequence for tool documentation, enabling structured representation of tool semantics beyond simple one-token identifiers. Experiments on ToolBench demonstrate the strong performance of the ToolWeaver."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The hierarchical quantization of tool semantics is novel and well-motivated. It offers a theoretically scalable alternative for tool representation.\n\n2. Introducing inter-tool similarity via a Laplacian regularization term is an interesting way to infuse functional relationships into discrete token learning.\n\n3. The experimental results show that ToolWeaver outperforms existing baselines significantly."}, "weaknesses": {"value": "1. The paper states that generative methods suffer from a semantic bottleneck for complex reasoning and that the “one-token-per-tool” paradigm faces critical scalability and generalization challenges. However, prior work, such as ToolGen, has already implemented semantic and hierarchical indexing for tools, addressing these challenges. Thus, while these are valid drawbacks of the basic one-token paradigm, presenting them as new challenges motivating ToolWeaver seems overstated. \n\n2. The paper is missing details on constructing the tool-tool similarity matrix, though it is central to the proposed Collaborative-Aware regularization.\n\n3. It remains conceptually ambiguous how ToolWeaver achieves the retrieval and usage of tools that were unseen during training."}, "questions": {"value": "1. For W1, the authors should acknowledge these earlier solutions and clarify where ToolWeaver provides new conceptual or empirical value beyond ToolGen.\n\n2. For W2, the collaborative regularization relies critically on A_{uv}, but the paper does not explain how it is computed in detail. Whether it’s based on tool co-occurrence, textual semantic similarity, functional metadata, or learned jointly? Without this, the interpretability of the collaborative signal is limited.\n\n3. For W3, the paper should clarify the mechanism enabling ToolWeaver to handle unseen tools (i.e., “Tool.” and “Cat.” testing set). Are their representations derived compositionally from previously learned tool components? Furthermore, how is ToolGen, a token-based model, evaluated under this “unseen tool” setting where it seemingly cannot generalize by design?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QEKtFx7Jc4", "forum": "Ge1DKuzWTO", "replyto": "Ge1DKuzWTO", "signatures": ["ICLR.cc/2026/Conference/Submission12708/Reviewer_1JHk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12708/Reviewer_1JHk"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762250332352, "cdate": 1762250332352, "tmdate": 1762923535564, "mdate": 1762923535564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ToolWeaver, a framework for scalable tool use with LLMs in large tool libraries. Instead of assigning each tool a dedicated special token (“one tool, one token”), ToolWeaver represents each tool as a multi-level sequence of discrete codes learned by a collaboration-aware residual quantization module (RQ-VAE). Tool documentation is first encoded into continuous embeddings, which are then quantized into several code levels; a tool–tool similarity matrix derived from usage logs regularizes the quantization so that semantically related and frequently co-used tools share parts of their codes. These codes are added as new vocabulary tokens to an LLM, and ToolWeaver aligns the model to generate tool codes via a two-stage process: (1) query → tool-code generation, and (2) full trajectory alignment with trie-constrained decoding to ensure only valid code sequences are produced. Experiments on ToolBench/StableToolBench with roughly 47K tools show that ToolWeaver improves tool retrieval quality (NDCG) and task metrics (SoPR/SoWR) over BM25, ToolRetriever, ToolGen, and other baselines, while reducing vocabulary growth from O(N) to O(log N)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Well-motivated problem setting.\nThe paper clearly articulates the limitations of the “one-tool-one-token” paradigm in terms of vocabulary explosion and lack of expressivity for tool co-usage patterns, especially in multi-tool scenarios. This is a realistic pain point for large API/tool ecosystems.\n\n2.Coherent end-to-end design.\nThe framework is not just a better embedding scheme but a full pipeline: collaboration-aware RQ-VAE for code learning, a uniform mapping step to resolve code collisions, and constrained generation over a code trie to plug the representation into LLM-based agents. The components fit together in a reasonably principled way.\n\n3.Empirical gains are consistent and meaningful.\nOn multiple settings of ToolBench (I1/I2/I3, unseen tools, unseen categories), ToolWeaver shows consistent improvements in both retrieval metrics and downstream task performance compared to strong retrieval-based and generative baselines, with the gains being particularly noticeable in the more complex I3 multi-tool regime.\n\n4.Clear scalability benefits.\nEncoding tools as compositional codes with L×K tokens supports KL tools and keeps vocabulary growth logarithmic in the number of tools. At the reported scale (~47K tools), this yields a much smaller vocabulary than assigning one special token per tool, which is practically useful when one wants to preserve general language capabilities.\n\n5.Some ablations and qualitative analysis.\nThe paper includes ablations on the collaboration regularization weight, comparisons against alternative tokenization schemes (numeric / hierarchical / purely semantic / atomic), and some qualitative visualizations of shared codes, which help justify key design choices."}, "weaknesses": {"value": "1.Evaluation scope is narrow.\nAll core experiments are conducted on the ToolBench / StableToolBench family. While this is a large benchmark, the tool distributions and usage patterns are specific. It remains unclear how well ToolWeaver transfers to very different tool ecosystems (e.g., enterprise APIs, programmatic or mathematical tools), or to highly dynamic tool catalogs.\n\n2.Construction and impact of the collaboration signal are under-analyzed.\nThe tool–tool similarity matrix is central to the method, but its construction (exact signal, sparsity, preprocessing) and the sensitivity of performance to its quality are not analyzed in depth. It is hard to tell how much of the gain comes from genuine collaborative structure vs. idiosyncrasies or bias in the usage logs.\n\n3.Cost and deployability are not quantified.\nCompared to a simple “one-token-per-tool + retriever” baseline, ToolWeaver adds RQ-VAE training, Sinkhorn-based uniform mapping, and trie-constrained decoding at inference. The paper does not provide concrete numbers for memory, throughput, or latency, making it difficult for practitioners to assess the cost–benefit trade-off.\n\n4.Incremental relative to prior collaboration-aware quantization.\nMethodologically, the approach is closely related to existing work on collaboration-aware residual quantization in recommender systems / generative retrieval. The main novelty is applying such ideas to tool representation and plugging them into LLM-based tool use. This is a solid extension, but conceptually more incremental than transformative."}, "questions": {"value": "1.Construction of the collaboration matrix.\nHow exactly is the tool–tool similarity matrix defined in the main experiments? Is it purely based on co-occurrence within the same usage trajectory, within the same collection, or do you incorporate additional signals (e.g., category information)? How are extremely sparse tools (few or no co-usage records) handled, and have you compared different construction strategies?\n\n2.Impact of noisy or biased co-usage.\nIn realistic logs, there may be templated trajectories, heavy-tailed “head” tools, or other biases. Have you studied whether ToolWeaver over-binds such tools in code space? Did you try thresholding, smoothing, or otherwise denoising the collaboration matrix, and how did that affect performance and robustness?\n\n3.Efficiency and scalability in practice.\nCan you provide more concrete numbers on resource usage? For example, in the I3 multi-tool setting, how does end-to-end inference latency and GPU memory consumption compare to ToolGen or a strong retrieval pipeline? Is Sinkhorn-based uniform mapping a bottleneck during training?\n\n4.Impact on general language capabilities.\nYou argue that one-tool-one-token can hurt language performance, and that ToolWeaver mitigates this via a smaller vocabulary. Could you show a more systematic comparison between ToolWeaver, ToolGen, and a no-tool baseline on standard NLP benchmarks, even if only on a small set, and ideally summarize those results in the main text?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MPubK4A8Xh", "forum": "Ge1DKuzWTO", "replyto": "Ge1DKuzWTO", "signatures": ["ICLR.cc/2026/Conference/Submission12708/Reviewer_uGA8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12708/Reviewer_uGA8"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission12708/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762949939792, "cdate": 1762949939792, "tmdate": 1762949939792, "mdate": 1762949939792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}