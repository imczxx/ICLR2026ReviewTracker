{"id": "zyLI9LEmry", "number": 10578, "cdate": 1758176240486, "mdate": 1759897642510, "content": {"title": "Preference-based Policy Optimization from Sparse-reward Offline Dataset", "abstract": "Offline reinforcement learning (RL) holds the promise of training effective policies from static datasets without the need for costly online interactions. However, offline RL faces key limitations, most notably the challenge of generalizing to unseen or infrequently encountered state-action pairs. When a value function is learned from limited data in sparse-reward environments, it can become overly optimistic about parts of the space that are poorly represented, leading to unreliable value estimates and degraded policy quality. To address these challenges, we introduce a novel approach based on contrastive preference learning that bypasses direct value function estimation. Our method trains policies by contrasting successful demonstrations with failure behaviors present in the dataset, as well as synthetic behaviors generated outside the support of the dataset distribution. This contrastive formulation mitigates overestimation bias and improves robustness in offline learning. Empirical results on challenging sparse-reward offline RL benchmarks show that our method substantially outperforms existing state-of-the-art baselines in both learning efficiency and final performance.", "tldr": "We introduce a contrastive framework that mitigates value overestimation in offline RL by training a policy to prefer successful trajectories over both observed and synthetic failures, leading to state-of-the-art results.", "keywords": ["Reinforcement Learning", "Offline Reinforcement Learning", "Preference-based Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4ef43b31950eff949a4099d4cb6f9c962b012a4a.pdf", "supplementary_material": "/attachment/617dc28b94758d66d1f61ebbf5b512505d07a855.zip"}, "replies": [{"content": {"summary": {"value": "This work presents a preference-based RL algorithm, which trains policies by contrasting successful demonstrations with failure behaviors present in the dataset. Experiments on offline benchmarks with sparse rewards validate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper proposes a contrastive preference learning framework to bypass direct value function estimation.\n- This paper provides both empirical and theoretical analyses.\n- The proposed approach outperforms baselines in various benchmarks."}, "weaknesses": {"value": "- The motivation is not adequately supported by evidence. The authors claim that existing methods are sensitive to support mismatches and prone to high variance or instability, particularly when data are limited or rewards are sparse, but no empirical results or references are provided to support these statements.\n- The paper does not provide results with other competitive baselines on MetaWorld, such as PREFORL [1] and CPL [2].\n- There is no sensitivity analysis on the representative segment length $k$ and the contrastive bias $\\lambda$.\n\nReferences:\n\n[1] Tarasov et al. \"Revisiting the Minimalist Approach to Offline Reinforcement Learning\", NeurIPS, 2023.\n\n[2] Hejna et al. \"Contrastive Preference Learning: Learning from Human Feedback without RL\", ICLR, 2024."}, "questions": {"value": "How is MetaWorld configured to use sparse rewards? By default, MetaWorld provides dense reward settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k0n2MAUUPo", "forum": "zyLI9LEmry", "replyto": "zyLI9LEmry", "signatures": ["ICLR.cc/2026/Conference/Submission10578/Reviewer_uvFa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10578/Reviewer_uvFa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761400604828, "cdate": 1761400604828, "tmdate": 1762921847507, "mdate": 1762921847507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel method named PREFORL, which utilizes a contrastive learning framework to learn preferences from successful trajectories and synthetically degraded trajectories, aiming to address the value overestimation problem in sparse-reward offline reinforcement learning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The idea is novel, introducing the concept of preference learning into offline RL to mitigate overestimation.\n\n2) It designs a scheme for generating negative trajectories and validates the algorithm's effectiveness through extensive experiments."}, "weaknesses": {"value": "1) The underlying theory and mechanism explaining why introducing the preference learning framework alleviates overestimation are unclear.\n\n2) The method for generating negative trajectories and the selection of the contrastive bias parameter vary across different tasks, and their impact on performance remains unknown.\n\n3) The proof for Lemma 3.1 is not rigorous, as the approximation between $\\hat{A}$ and $A^*$ is unreasonable."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yKVOrb3P6m", "forum": "zyLI9LEmry", "replyto": "zyLI9LEmry", "signatures": ["ICLR.cc/2026/Conference/Submission10578/Reviewer_k6nG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10578/Reviewer_k6nG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649043624, "cdate": 1761649043624, "tmdate": 1762921847146, "mdate": 1762921847146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PREFORL (PREFerence-based Optimization for Offline RL), a novel contrastive preference learning framework designed to train robust policies from sparse-reward offline datasets. The method aims to bypass the core challenges of conventional offline RL, specifically the extrapolation error and overestimation bias that plague value-based methods in data-limited, sparse-reward settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The key idea is fundamentally original: avoiding direct, unstable value function estimation (the common bottleneck in sparse-reward offline RL) by transforming the task into a more robust contrastive learning problem.\n\n2. The introduction of two controlled degradation operators ($\\mathcal{D}^{\\perp a}$ and $\\mathcal{D}^{\\perp s}$) is a highly creative mechanism for generating meaningful synthetic negative examples."}, "weaknesses": {"value": "1. For state-based degradation ($\\mathcal{D}^{\\perp s}$), the computation overhead associated with Nearest Neighbor Search is acknowledged to be non-negligible and potentially time-consuming for large-scale datasets. A brute-force, exact search is computationally prohibitive.\n\n2. The performance of the action-based degradation method is highly sensitive to the choice of the noise variance ($\\sigma$), which limits its robustness. This requires manual tuning per environment (or environment domain) to find the reasonably small number (e.g., $1\\%$ to $2\\%$) that maximizes the success rate."}, "questions": {"value": "The CPL baseline is excluded from the Maze2D evaluation, with the justification that the dataset lacks unsuccessful trajectories. Given that PREFORL's core novelty is to augment these negative examples, can you make a direct comparison showing that CPL fails while PREFORL succeeds due to the synthetic degradation? It would have been a more powerful demonstration."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BcROOsLT6P", "forum": "zyLI9LEmry", "replyto": "zyLI9LEmry", "signatures": ["ICLR.cc/2026/Conference/Submission10578/Reviewer_aGGo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10578/Reviewer_aGGo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990983465, "cdate": 1761990983465, "tmdate": 1762921846615, "mdate": 1762921846615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PREFORL, a preference-based offline reinforcement learning method that addresses value overestimation in sparse-reward settings. The approach trains policies via contrastive learning between successful demonstrations and synthetic degraded trajectories generated through action perturbation or state-based substitution. Core contributions include: A degradation framework augmenting sparse offline datasets, (2) A preference optimization loss bypassing explicit value estimation, and (3) Theoretical analysis linking the loss to policy imitation. Evaluations on Adroit, Sparse-MuJoCo, Maze2D, and MetaWorld benchmarks show PREFORL outperforms offline RL/imitation baselines in success rates and normalized scores."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel degradation framework towards both action and state level.\n- Comprehensive experimental validation.\n- Complete theoretical analysis.\n- Good writing for easy understanding."}, "weaknesses": {"value": "- More navigation tasks (e.g. Antmaze-umaze/medium/large-diverse/replay), as well as offline RL baselines, should be performed. \n- The ablation study of the degraded dataset size is lacking."}, "questions": {"value": "- The proposed paradigm includes both a plug-in data-augmentation pipeline and a corresponding contrastive training pipeline. Can this paradigm be implemented on other BC-based offline-RL methods like Decision Transformer[1]?  I am glad to see how this paradigm performs on the DT backbone with different scales of data and models.\n- Why is the state-degradation dataset constructed with the nearest neighbor state instead of directly adding noise like the action-degradation? I think there should be a comparison study of these two different state-degradation manners.\n\n[1] Decision Transformer: Reinforcement Learning via Sequence Modeling"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MdU5Lr0G6J", "forum": "zyLI9LEmry", "replyto": "zyLI9LEmry", "signatures": ["ICLR.cc/2026/Conference/Submission10578/Reviewer_oreG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10578/Reviewer_oreG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762180289063, "cdate": 1762180289063, "tmdate": 1762921844766, "mdate": 1762921844766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}