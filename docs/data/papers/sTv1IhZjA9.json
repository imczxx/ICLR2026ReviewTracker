{"id": "sTv1IhZjA9", "number": 14236, "cdate": 1758230954159, "mdate": 1759897382138, "content": {"title": "When three experiments are better than two: Avoiding intractable correlated aleatoric uncertainty by leveraging a novel bias--variance tradeoff", "abstract": "Real-world experimental scenarios are characterized by the presence of heteroskedastic aleatoric uncertainty, and this uncertainty can be correlated in batched settings. The bias-variance tradeoff can be used to write the expected mean squared error between a model distribution and a ground-truth random variable as the sum of an epistemic uncertainty term, the bias squared, and an aleatoric uncertainty term. We leverage this relationship to propose novel active learning strategies that directly reduce the bias between experimental rounds, considering model systems both with and without noise. Finally, we investigate methods to leverage historical data in a quadratic manner through the use of a novel cobias-covariance relationship, which naturally proposes a mechanism for batching through an eigendecomposition strategy. When our difference-based method leveraging the cobias-covariance relationship is utilized in a batched setting (with a quadratic estimator), we outperform a number of canonical methods including BALD and Least Confidence.", "tldr": "New methods for active learning by estimating and exploiting the bias-variance tradeoff", "keywords": ["active learning"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e80c7d5b5971aa9247020ce06a6ef91b8d76753.pdf", "supplementary_material": "/attachment/f78234bbb6be6047062d128c86dcaa2fd8003c79.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel framework for active learning that leverages the bias–variance–noise decomposition to better distinguish between reducible and irreducible sources of uncertainty. It introduces acquisition strategies that selectively target epistemic uncertainty, model bias, or a combination of both, enabling more principled experiment selection. They evaluate their methods on a set of synthetic toy experiments designed with controlled noise assumptions, showing that their approach outperforms standard uncertainty-based baselines such as BALD and LC."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well written.\n\n2. The separation of uncertainties through the bias–variance–noise decomposition is intuitive and well-motivated, making the proposed framework conceptually clear and broadly applicable across different active learning settings."}, "weaknesses": {"value": "1. The paper does not consider or compare against recent state-of-the-art active learning methods such as BatchBALD [1], BADGE [2], and BAIT [3]. These approaches are well-established benchmarks for active learning.\n\n2. The paper relies entirely on toy experiments to demonstrate its effectiveness. While these experiments are useful for illustrating theoretical properties, the method is not purely theoretical and would therefore benefit significantly from evaluation on real-world datasets.\n\n3. The structure of the paper is somewhat confusing. Section 2 includes a numerical experiment comparing the proposed methods to BALD and showing improved performance, yet BALD is subsequently disregarded in the main numerical experiment section, Section 4.\n\n4. It is not explicitly stated what models are being trained in the experiments. Are they are neural networks, Gaussian processes, random forests, or linear models?\n\n5. There is no limitations or weakness sections.\n\n6. Minor Issues:\n\n- The caption for Figure 1 blends into the body text and should be visually separated for improved readability.  \n- A visualization of each dataset listed in Table 2 would make the paper more accessible and easier to interpret.  \n- On line 157, should the expression be $\\delta_{\\inf}(x) = 0$ instead of $\\delta_{\\inf} = 0$?\n- The legends in Figures 2 and 4 are confusing. For example, in Figure 4 the solid blue line is unlabeled, and the legend references a “random” (solid black) line that does not appear in the figure. Consider moving legends outside the plot and ensuring every line has a corresponding label.\n\n[1] Kirsch, A., van Amersfoort, J., & Gal, Y. (2019). BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning. NeurIPS 2019.\n\n[2] Ash, J. T., Zhang, C., Krishnamurthy, A., Langford, J., & Agarwal, A. (2020). Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds (BADGE). ICLR 2020.\n\n[3] Kim, J., Lee, Y., & Shin, J. (2021). BAIT: Bayesian Active Learning by Information Theoretic Measures. NeurIPS 2021."}, "questions": {"value": "1. Why is Random better than BALD? Figure 2\n\n2. Why does Figure 4 & 5 include no BALD or other baseline methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z27WXSWyGE", "forum": "sTv1IhZjA9", "replyto": "sTv1IhZjA9", "signatures": ["ICLR.cc/2026/Conference/Submission14236/Reviewer_SFiS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14236/Reviewer_SFiS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760988436477, "cdate": 1760988436477, "tmdate": 1762924691595, "mdate": 1762924691595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a novel quantity derived from the bias-variance trade-off of statistical models that can serve as an optimization target for Active Learning (AL) acquisition functions. \nThey explain the derivation of this quantity in great detail and construct a series of toy experiments with different structures in the noise term to showcase the properties of their proposed method.\nAs this novel quantity is unknown in practice, the authors propose several way of approximating it and benchmark the variants of their methods against existing AL methods, as well as \"cheated\" version of their method that has direct access to the target quantity."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Very well structured construction of the novel quantity that the authors are planning to optimize\n- Principled experiment to show the potential of they method (if a perfect approximation would be possible)\n- Multiple approaches to approximate the novel quantity and initial comparison against competitors\n- Clear path for improving upon the proposed method"}, "weaknesses": {"value": "- (Minor) It is well known in AL that uncertainty-based methods (like the proposed one) are struggling for small labeled sets, as the model F cannot be appropiately trained. A comparison of Fig. 2 and Fig. 6 (App. A) clearly hints in this direction. For that reason, we really would have liked to see a comparison against a diversity based method like TypiClust or ProbCover to acertain, if the \"cheat\" variant of the proposed algorithm can surpass diversity based acquisition functions.\n- (Minor) Your comparison between \"BiasReduction\" and \"PEMSE\" variants (Fig. 4) is lacking depth. Both methods appear to be equal in performance, which should only be true if we only use MSE as metric. The advantage the PEMSE method can generate can only be visible in the calibration of the network, which should be easy to check by comparing $\\sigma_F$ to $\\sigma_Y$ at all 2500 points of your data grid. This way you could highlight the advantage that PEMSE should have over BiasReduction for model training."}, "questions": {"value": "- In Fig. 2, do you mean \"Cheat\" and \"Approximated\" instead of \"False\" and \"True\"?\n- Is Fig. 4 missing a curve for Random or is \"NA\" supposed to be random?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jk92mLhztr", "forum": "sTv1IhZjA9", "replyto": "sTv1IhZjA9", "signatures": ["ICLR.cc/2026/Conference/Submission14236/Reviewer_pAea"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14236/Reviewer_pAea"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761299095051, "cdate": 1761299095051, "tmdate": 1762924690343, "mdate": 1762924690343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel approach to active learning (AL) that seeks to optimize data acquisition by explicitly \naddressing the bias-variance tradeoff in noisy, heteroskedastic and correlated real-world settings. The core idea is to \nleverage a decomposition of the pointwise expected mean squared error into epistemic uncertainty, bias, and irreducible \naleatoric uncertainty. The authors introduce a \"cobias-covariance tradeoff\" and propose a quadratic bias estimation method, \nto guide data point acquisition that effectively reduces model bias and epistemic uncertainty."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper offers a compelling motivation for using the bias-variance tradeoff as a guiding principle for AL acquisition \nfunctions. The explicit consideration of heteroskedastic and correlated noise is generally an interesting and practically\nrelevant setting."}, "weaknesses": {"value": "- **Major Presentation and Structural Issues:** The paper's most significant weakness lies in its poor presentation and \nlack of clear structure. The narrative is disjointed, frequently mixing theoretical derivations with experimental \nresults and ablation studies across various sections (e.g., experimental results in Section 2.4, followed by another \ndedicated Section 4). This significantly hinders readability and comprehension. A dedicated section for related work is \nalso notably absent, making it difficult to contextualize the contributions.\n- **Lackluster Problem Statement and Notation:** The problem statement and introduction of notation (Section 2.1) are vague\nand inconsistent. Here are a few examples:\n  - The variable $Y$ is initially introduced as a \"random variable in space $\\mathcal{Y}$\" but later treated as a \n  stochastic process $Y(x)$\n  - What does $\\mathbb{E}_\\mathcal{Y}[]$ mean? Is $\\mathbb{Y}$ a probability space?\n  - $Q_k \\cap L_k = \\emptyset$ (section 2.3.1) should be $Q_k \\cap L_{k-1} = \\emptyset$\n  - Equation 7, introducing a \"gradient\" with respect to an integer step 'k', is mathematically ill-defined.\n- **Insufficient Experimental Detail in Main Text:** Crucial information about the experimental setup, such as the specific \nmodel architecture used is omitted from the main body. I acknowledge that Appendix B contains details on the experiments.\nHowever, this makes it impossible for readers to fully grasp the methodology without referring to supplementary material, \nwhich should ideally contain additional details, not core methodology.\n- **Limited Experimental Scope:** The evaluation only considers three specifically designed toy examples. While these toy \nexamples aim to highlight specific aleatoric noise conditions (i.e. no noise, uncorrelated noise and correlated noise), \nthe absence of evaluation on more (realistic) benchmarks raises questions about the generalizability and practical \napplicability of the proposed methods.\n- **Unclear Algorithmic Variants and Baselines:** The descriptions of algorithmic variants and baselines are not consistently \nclear, making it challenging to understand how comparisons were made.\n\nBased on the significant issues in presentation, clarity, and structural organization, alongside some fundamental \ninconsistencies in mathematical notation and insufficient experimental detail in the main text, I strongly recommend \nrejection of this paper in its current form."}, "questions": {"value": "No specific questions, please comment on the points below \"Weaknesses\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7kcKFNz6Fo", "forum": "sTv1IhZjA9", "replyto": "sTv1IhZjA9", "signatures": ["ICLR.cc/2026/Conference/Submission14236/Reviewer_FYQx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14236/Reviewer_FYQx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940515397, "cdate": 1761940515397, "tmdate": 1762924688958, "mdate": 1762924688958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}