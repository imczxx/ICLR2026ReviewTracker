{"id": "B192BGng4m", "number": 9007, "cdate": 1758107030149, "mdate": 1759897748347, "content": {"title": "A Video Is Not Worth a Thousand Words", "abstract": "As we become increasingly dependent on vision language models (VLMs) to answer questions about the world around us, there is a significant amount of research devoted to increasing both the difficulty of video question answering (VQA) datasets, and the context lengths of the models that they evaluate.\nThe reliance on large language models as backbones has lead to concerns about potential text dominance, and the exploration of interactions between modalities is underdeveloped.\nHow do we measure whether we're heading in the right direction, with the complexity that multi-modal models introduce?\nWe propose a joint method of computing both feature attributions and modality scores based on Shapley values, where both the features and modalities are arbitrarily definable.\nUsing these metrics, we compare $6$ VLM models of varying context lengths on $4$ representative datasets, focusing on multiple choice VQA.\nIn particular, we consider video frames and whole textual elements as equal features in the hierarchy, and the multiple choice VQA task as an interaction between three modalities: video, question and answer. \nOur results demonstrate a dependence on text and show that the multiple choice VQA task devolves into a model's ability to ignore distractors.", "tldr": "Using Shapley values to determine modality contributions in multiple-choice VQA with long context video.", "keywords": ["feature attribution", "shapley values", "modality preference", "visual question answering", "video understanding"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/327fb076348139d13d987d81291f94de093ba28b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper examines whether video–language models (VLMs) genuinely utilize visual information when performing multiple-choice video question answering (VQA), or whether their predictions are predominantly text-driven. To this end, the authors propose a Shapley-value–based attribution framework that operates on arbitrarily grouped features, such as entire video frames and textual components, to quantify both per-feature and modality-level contributions. These contributions are normalized to ensure independence from model accuracy and modality length, yielding two interpretable metrics: Modality Contribution and Per-Feature Contribution. Using this framework, the authors analyze six open-source VLMs (FrozenBiLM, InternVideo, VideoLLaMA2, LLaVA-Video, LongVA, and VideoLLaMA3) across four representative benchmarks (EgoSchema, HD-EPIC, MVBench, and LVBench). The findings reveal that: (1) VLMs systematically underutilize video information relative to text, (2) many questions can be answered reasonably well without the question text, and (3) increasing the number of answer options tends to reduce text dominance and encourage greater reliance on visual and question modalities, suggesting that current multiple-choice VQA often emphasizes distractor elimination rather than genuine multimodal reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The introduction of a Shapley-value–based attribution framework to investigate modality bias in VLMs is novel. This approach provides a principled and interpretable means of quantifying the relative contributions of visual and textual inputs, which has not been systematically explored in prior multimodal reasoning research.\n\n2. The paper presents an extensive and well-structured empirical study, evaluating six VLMs with varying context lengths across four diverse VQA datasets that differ in perspective (egocentric vs. exocentric) and video duration (short vs. long). \n\n3. The empirical findings, namely, that (1) video information is consistently underutilized compared to textual cues, (2) many questions can be answered even without the question text, and (3) increasing the number of answer options mitigates textual dominance, offer valuable insights into the limitations of current multiple-choice VQA formulations and provide meaningful guidance for future multimodal reasoning research."}, "weaknesses": {"value": "1. Despite the thorough experimental analysis, the paper lacks a clear methodological or algorithmic contribution that meets the technical novelty threshold typically expected at ICLR. The proposed attribution framework, while well-motivated, primarily extends existing interpretability techniques rather than introducing a fundamentally new learning paradigm or model architecture to address their findings.\n\n2. Many of the reported empirical findings reiterate observations that have been discussed in prior literature and thus offer limited novelty. For instance, the underutilization of visual information in VQA systems has been previously discussed in [1, 2] and the observation that increasing the number of answer options leads to decreased performance or altered modality reliance is largely intuitive.\n\n3. As acknowledged by the authors, the study’s scope is restricted to multiple-choice VQA, which limits the generality of its conclusions. It remains unclear whether the same modality attribution patterns would hold for open-ended VQA, captioning, or other multimodal reasoning tasks, thereby constraining the broader applicability of the findings.\n\n[1] Large Language Models are Temporal and Causal Reasoners for Video Question Answering\n\n[2] Generative Bias for Robust Visual Question Answering"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "psV3hVCHQ1", "forum": "B192BGng4m", "replyto": "B192BGng4m", "signatures": ["ICLR.cc/2026/Conference/Submission9007/Reviewer_HatP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9007/Reviewer_HatP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761460276479, "cdate": 1761460276479, "tmdate": 1762920734840, "mdate": 1762920734840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The MM-SHAP paper demonstrated the use of Shapley values in order to measure the relative importance of image and text modalities for VQA, and showed that, between images and text, mode collapse might happen in either direction.  This paper extends the same methods to long videos."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The extension to video generates new results that differ from the image+text results in somewhat interesting ways.  Specifically: (1) unlike images, video is always less important than text, though only a small number of questions can be correctly answered with the video completely masked, (2) the importance of the video increases if the number of candidate answers in a multiple-choice question is increased by rotating in some answers randomly chosen from other questions."}, "weaknesses": {"value": "Although the analyses of video are interesting, it is difficult to recommend acceptance because all of the proposed algorithms have previously been applied, in more or less the same form, to image-text VQA."}, "questions": {"value": "The last paragraph of section 3.2 defines the reward to be \"the logits of the predicted text tokens.\"  I first read that to say that you use the logits of the answer generated by the VLM, but if that were true, it would be impossible to measure \\phi^{gt} unless the VLM generated the correct answer, right?  I think you must be measuring the logit of the GT answer, even if the VLM does not choose to generate the GT answer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g8youFCyLK", "forum": "B192BGng4m", "replyto": "B192BGng4m", "signatures": ["ICLR.cc/2026/Conference/Submission9007/Reviewer_8H6M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9007/Reviewer_8H6M"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863367286, "cdate": 1761863367286, "tmdate": 1762920734342, "mdate": 1762920734342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Brief Summary: This is an investigative / diagnostic paper to find how much does the video modality in general contribute for VLM understanding, and authors primarily focus on the VQA task. This is somewhat known intuitively that video plays a downsized role, but the authors provide a framework based on multi-modal shapely values to find per-modality and per-feature contributions for an answer. Experiments are conducted over 4 benchmarks (Egoschema, hd-epic, mvbench, lvbench) which shows other than video-llama3, video contribution is quite low."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Pros: \n1. The paper is very well motivated from an evaluation perspective. Finding which modality plays part in final output is very important. I can see many downstream tasks requiring explainability.\n\n2. In a way, the authors are under-selling the paper as a diagnosis for methods only. I think especially from L299, this can be a good diagnostic for benchmark creation as well. Usual way for benchmark creation to ensure requirement of video modality is to show video-blind models perform similar to random. The proposed method is essentially taking it a step further by doing per-modality and per-feature representations. \n\n3. The authors provide reasonably detailed experiments including in the supplementary. Authors experiment on multiple datasets with diverse domains (ego and exo videos) and models frozen-bilm, intern-video, video-llama2/3, llava-video, longva.\n\n4. The gemini-oracle experiments are quite interesting, in that the frames disagree with each other."}, "weaknesses": {"value": "Cons:\n\n1. My main concern is that the only task evaluated in multiple-choice VQA. This severely restricts it applications (which authors also note in limitations section 5). Authors should at least experiment with full-string match? I am slightly confused why a trivial extension of proposed method cannot be done with say removing parts of the output tokens? It would be great to have the authors expand on this. \n\n2. The main takeaway is that video modality is under-represented. It is a good to have empirical evidence but is not super surprising. Also, quite confused, why image-language tasks are not considered here? Some direct comparison with MM-SHAP would be interesting to see. \n\n3. In usual VLM, an important aspect is that of tool calling. How is tool-calling being handled via this method? \n\n4. (Minor) I think it also makes sense to investigate even more modalities outside of video, such as audio or IMU as done in image-bind kind of works. It would significantly strengthen the paper.\n\n5. In the shapely value computation, the authors choose to zero-out the video and text is replaced with whitespace, but this design choice is not really justified. An alternative would be to simply remove the frame in the video and remove the token in text? (unless i missed some ablation experiment). \n\n\n----\n\nOverall: Rating 6/10\n\nThe overall idea mostly makes sense that we can use shapely values to estimate video influence in downstream task. The paper has numerous mostly well-thought out experiments which ought to be valuable for the VLM community in general. Main concern is that the approach is only used for MCQ setting which is too narrow. The paper could be improved by adding more details/discussion on how long-context, tool calling, black-box setting, additional baselines, additional modality comparisons."}, "questions": {"value": "Q1. How does the behavior change in long-context settings, such as going to hour-long videos which nearly fills the context window. \n\nQ2. Can the method be extended to black-box models such as gpt? Right now, we are using logit output, but is there a way to avoid using pure logit information, say via monte-carlo (via pass@K) ? Not sure.\n\nQ3. The evaluation by itself is quite expensive (as noted in appendix C). Is the 7200 gpu hours for each experiment or across all experiments? Is there some parallelization that can be done? Otherwise, it is not really of practical use at the moment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZYqHxBL8qO", "forum": "B192BGng4m", "replyto": "B192BGng4m", "signatures": ["ICLR.cc/2026/Conference/Submission9007/Reviewer_Wd6d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9007/Reviewer_Wd6d"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953590253, "cdate": 1761953590253, "tmdate": 1762920733861, "mdate": 1762920733861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}