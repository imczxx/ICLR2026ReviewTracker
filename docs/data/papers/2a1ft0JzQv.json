{"id": "2a1ft0JzQv", "number": 12141, "cdate": 1758205929466, "mdate": 1759897529470, "content": {"title": "Multi-view Latent Diffusion Reconstruction for Vision-enhanced Time Series Forecasting", "abstract": "Recent studies have explored diffusion models for time series forecasting, yet most methods operate directly on 1D signals and tend to overlook intrinsic temporal structures (e.g., periodicity and trend).\nThis often leads to suboptimal long-range dependency modeling and poorly calibrated uncertainty. \nTo this end, we propose LDM4TS, a vision-enhanced time series forecasting framework that visualizes time series into structured 2D representations and leverages the image reconstruction capabilities of diffusion models.\nRaw sequences are first converted into complementary visual inputs, forming multiple views that collectively capture diverse temporal structures.\nBy leveraging the generative nature of the diffusion process, the framework not only yields accurate point forecasts but also provides the capability to characterize predictive uncertainty.\nExtensive experiments demonstrate that LDM4TS outperforms various specialized forecasting models for time series forecasting tasks.", "tldr": "We propose LDM4TS, a vision-enhanced time series forecasting framework that transforms sequences into visual represetations and leverages multi-modal conditional-guided latent diffusion models for time series forecasting.", "keywords": ["time series forecasting", "diffusion models", "multi-modal learning"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dcf1cd6fe89a74dbf92885e9c6c6c99759961a26.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes LDM4TS, a vision-enhanced time series forecasting framework that transforms 1D temporal data into multi-view 2D visual representations, which are then reconstructed using a latent diffusion model. The reconstructed latent features are fused with temporal features for final predictions."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces an interesting integration of multi-view transformations and latent diffusion-based reconstruction, bridging recent advances from vision and generative modeling domains.\n2. The authors provide an insightful observation that 2D representations of time series can naturally reveal cross-periodic and structural correlations, which is indeed a meaningful perspective."}, "weaknesses": {"value": "1. Experiment is unconvincing.\n   - Although the paper claims extensive comparison, several key baselines are missing in the results table, such as recent vision-based or multimodal time-series models, such as VisionTS, TimeVLM, and DMMV, which are directly relevant to this line of research.\n   - The benchmark data selection is limited, omitting more diverse datasets (e.g., Illness, Solar) that are commonly used to evaluate model generality.\n   - The paper lacks forecast-horizon-specific results (e.g., per-horizon metrics like 96/192/336/720), making it hard to assess the robustness across varying prediction lengths.\n2. Model design appears over-engineered and poorly justified.\n   - The architecture combines latent diffusion, multiple-view visual transformations, and manually defined conditioning signals (frequency/text), yet the paper fails to clearly explain the design motivations and expected roles of each component."}, "questions": {"value": "1. What is the true benefit of using latent diffusion reconstruction, how does it differ from using a masked autoencoding approach (e.g., VisionTS) for image-based reconstruction?\n2. The authors claim that LDM4TS is the first to convert raw time series into multi-view visual representations, which is inaccurate — TimeVLM and other recent works already adopt multi-view with vision designs.\n3. Diffusion-based reconstruction typically incurs high training and inference costs. Has the paper measured this overhead or compared runtime against simpler visual forecasting baselines?\n4. Given the maturity of pre-trained 2D diffusion models, why not utilize existing pre-trained backbones or newer alternatives such as flow-matching or consistency models for efficient image reconstruction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dZRErW3ovC", "forum": "2a1ft0JzQv", "replyto": "2a1ft0JzQv", "signatures": ["ICLR.cc/2026/Conference/Submission12141/Reviewer_27Nf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12141/Reviewer_27Nf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761435904203, "cdate": 1761435904203, "tmdate": 1762923102647, "mdate": 1762923102647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **LDM4TS**, a framework for time series forecasting that integrates a latent diffusion model with a cross-modal conditional guidance mechanism. The method first transforms time series data into multi-view visual representations (mainly using SEG, GAF, and RP transformations) to capture temporal dependencies across multiple scales. A latent diffusion model is then applied, conditioned on both frequency-domain and textual semantic information. Finally, a gated fusion mechanism integrates visual and temporal representations for forecasting. Experiments demonstrate that LDM4TS outperforms existing baselines across several standard benchmarks, especially in long-horizon, few-shot, and zero-shot forecasting scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Well-structured framework:** The model architecture is complete and logically coherent, consisting of three major components — multi-view encoding, latent diffusion-based generation, and multi-modal fusion.\n- **Reasonable multi-modal design:** The use of multiple conditional sources (frequency-domain and semantic information) to guide the diffusion process is well-motivated and technically sound.\n- **Comprehensive experiments:** The paper includes extensive evaluations across multiple datasets, ablation studies, parameter sensitivity analysis, and generalization tests, which together demonstrate the robustness and effectiveness of the proposed method."}, "weaknesses": {"value": "- **Limited novelty:** The contribution appears to lie primarily in the engineering integration of existing techniques (visual encoding, diffusion modeling, and multimodal conditioning) rather than introducing fundamentally new modeling concepts.\n- **Complex design but insufficient clarity:** Several core implementation details are under-explained. For example, the **Text Encoder** lacks a clear description of its semantic input and its role across datasets — does it directly take raw time series data as textual input, which would seem conceptually questionable? The **Temporal Projection** module is also insufficiently defined — what exact feature representation does it consume, and from which stage of the pipeline? Furthermore, it remains unclear whether the **Encoder/Decoder** bridging pixel and latent spaces are pre-trained or trained from scratch, and whether they are fine-tuned or kept frozen during training.\n- **Lack of complexity analysis:** The paper does not quantify the computational cost or inference efficiency. Given the multi-stage pipeline that converts time series into images and employs diffusion sampling, a discussion on training and inference complexity compared to baseline models would be valuable."}, "questions": {"value": "See Weaknesses. And an additional one:\n\nRegarding the **Temporal Projection** branch: since it connects directly to the output through gated fusion, it appears to partially bypass the diffusion model.\n\n- How does this affect the contribution of the diffusion component?\n- What are the typical gating weights observed during inference?\n- If the gate frequently favors one branch over the other, does that diminish the importance of the diffusion pathway?\n\nAn ablation study that disables the gate and retains only one of the two branches would help clarify the relative contribution of each module."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "geO56YP4bq", "forum": "2a1ft0JzQv", "replyto": "2a1ft0JzQv", "signatures": ["ICLR.cc/2026/Conference/Submission12141/Reviewer_ncRQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12141/Reviewer_ncRQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722215090, "cdate": 1761722215090, "tmdate": 1762923102283, "mdate": 1762923102283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an architecture for multimodal information extraction and its application in time series forecasting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Combining multimodal methods to process time series is a direction worth exploring"}, "weaknesses": {"value": "1. The combination of multimodal information processing methods is certainly an interesting attempt, but the author did not provide a reasonable insight. Converting time series into images is a lossy transformation. Why is such a transformation beneficial? At the same time, the statistical characteristics of the data can be obtained through simple transformations, and adding textual information for description makes it more redundant. \n\n2. Lack of a unique combination mechanism between time series and multimodal data, and the design of network structures mostly follows existing research.\n\n3. The introduction of diffusion models is very incremental and marginal, and there are already many existing diffusion model methods for probabilistic forecasting. I am not sure about the specific significance of introducing diffusion models\n\n4. The comparison objects are not up-to-date enough, and many new methods in time series and diffusion domain, such as [1] and [2], have not been considered.\n\n[1] Liu Y, Hu T, Zhang H, et al. iTransformer: Inverted Transformers Are Effective for Time Series Forecasting[C]//The Twelfth International Conference on Learning Representations.\n\n[2] Wang C, Yang L, Wang Z, et al. A Non-isotropic Time Series Diffusion Model with Moving Average Transitions[C]//Forty-second International Conference on Machine Learning."}, "questions": {"value": "1. What is the significance of comparing zero-shot learning in Table 4? My understanding is that the author considered the zero-shot performance of the model because they used a pre-trained structure. However, the core component of the time series module is not pre-trained,  and this is actually more of a comparison of near shot transfer rather than zero shot. In addition, I am concerned that the zero-shot performance of the model comes from pre-trained models, which has nothing to do with whether the author's design can capture temporal information well. In other words, the zero-shot performance of the model is not the author's contribution.\n\n2. If my understanding is correct, Table 12 mainly shows whether there are specific statistical features in the text that have an impact. If so, the statistical features of the time series should be very easy to obtain. Why is it necessary to introduce a text model with a huge number of parameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PzP6RaqJRq", "forum": "2a1ft0JzQv", "replyto": "2a1ft0JzQv", "signatures": ["ICLR.cc/2026/Conference/Submission12141/Reviewer_XQFv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12141/Reviewer_XQFv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810648729, "cdate": 1761810648729, "tmdate": 1762923101845, "mdate": 1762923101845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LDM4TS, a forecasting framework that converts multivariate time series into multiple 2D visual representations and reconstructs them in a latent space using a conditional diffusion model.   The reconstructed visual features are fused with projected temporal features via a gated fusion module, and frequency-domain and text-like conditioning signals are used to inject global periodic and semantic priors and to support uncertainty estimation.   The model is evaluated on seven public benchmarks under full-data, few-shot, and zero-shot transfer, against Transformer-based, linear/decomposition, vision-enhanced, and diffusion-style baselines.  The paper reports lower or comparable MSE/MAE in most regimes and supports these claims with ablations and sensitivity analyses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents an end-to-end pipeline unifying multiview time-series-to-vision encoding, latent diffusion with multimodal conditioning, and gated fusion for prediction, forming a single forecasting framework.  \n2. The model injects global periodic structure and semantic priors via frequency-domain and text-like conditioning and is intended to support both point forecasts and uncertainty estimation.  \n3. The evaluation spans seven public benchmarks and multiple regimes (long-horizon, few-shot, zero-shot transfer) and compares against Transformer-based, linear/decomposition, vision-enhanced, and diffusion-style baselines. \n4. Reported results show lower or comparable MSE/MAE in many settings, including high-dimensional electricity load and cross-dataset transfer, and are backed by ablations and model-size sensitivity studies."}, "weaknesses": {"value": "1. Although the paper claims calibrated predictive uncertainty through a generative diffusion mechanism, the main text only reports calibration metrics such as QICE for a single forecast horizon and does not provide a deeper analysis of uncertainty quality across different horizons, high-noise or distribution-shift regimes.\n2. The zero-shot transfer results indicate maintained performance across datasets, but the study does not isolate how much generalization arises from the architectural design versus conditioning signals such as frequency-domain descriptors and text-like prompts provided at inference time, so the source of the observed transferability is not fully disentangled.  \n3. The paper reports performance (in MSE and MAE metrics) gains over multiple baselines and horizons but does not provide confidence intervals or significance tests for MSE and MAE differences, making it difficult to assess whether the average reported improvements are consistently reliable rather than specific to certain datasets or horizons."}, "questions": {"value": "1. During inference, are the frequency-domain and textual/statistical conditioning signals strictly derived from past context only, or can they encode information that summarizes or implicitly reflects the target forecast interval, and if so how is information leakage prevented? \n2. Can you provide an ablation in the zero-shot transfer setting where the model is evaluated without frequency or text conditioning, in order to separate architecture-driven generalization from prior injection via conditioning prompts? \n3. The appendix reports QICE-based calibration results only for a single forecast horizon. Can you provide calibration and coverage metrics (e.g., QICE or interval coverage) across multiple forecast horizons to assess whether uncertainty quality is consistent as horizon increases? \n4. The qualitative visualizations show predicted trajectories but do not include side-by-side comparisons with representative baselines. Can you add qualitative comparisons against strong baselines (e.g., a Transformer forecaster or a diffusion-style forecaster) to reveal systematic error differences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "elxyn54DTT", "forum": "2a1ft0JzQv", "replyto": "2a1ft0JzQv", "signatures": ["ICLR.cc/2026/Conference/Submission12141/Reviewer_yQCM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12141/Reviewer_yQCM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977875674, "cdate": 1761977875674, "tmdate": 1762923101126, "mdate": 1762923101126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}