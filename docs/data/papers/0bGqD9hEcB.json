{"id": "0bGqD9hEcB", "number": 9450, "cdate": 1758122923169, "mdate": 1759897723848, "content": {"title": "Toward Robust Feature Space in Long-Tailed Time Series Classification: A Multi-Scale Perspective", "abstract": "In recent years, significant progress has been made in time-series classification (TSC) research on balanced datasets.  However, research on long-tailed time-series classification tasks remains limited. In this paper, we propose a novel framework, TimeLT, designed to learn a robust feature space from long-tailed data, thereby improving the overall accuracy of long-tailed TSC, with a particular focus on tail classes.  Specifically, we introduce a perturbation-aware data augmentation strategy that increases the diversity of tail-class samples and enhances the model's robustness to extremely imbalanced data distributions.  Following this, we leverage a multi-scale temporal encoder to extract rich representations from time-series data. This multi-scale modeling approach facilitates the learning of unique features for each class. Furthermore, we propose a boundary-repulsion regularization term that encourages embeddings to move closer to their respective class centroids while being repelled from inter-class decision boundaries. This approach helps to form compact and discriminative feature spaces. To promote comprehensive research in this area, we consolidate a dedicated benchmark comprising several long-tailed datasets and over 14 advanced baselines. Extensive experiments conducted on this benchmark demonstrate that TimeLT significantly outperforms other advanced models, achieving performance improvements of up to \\textbf{49.4\\%} in highly imbalanced scenarios.", "tldr": "", "keywords": ["Time series classification", "Long-tailed recognition", "Contrastive learning"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c58812a6b2efd02d8b7df6c72fd0881f93c0447a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes TimeLT to tackle long-tail time series classification by considering multi-scal temporal encoding, data augmentation, and variant representation learning strategies. TimeLT demonstrates effectiveness across 4 selected datasets with ablation study and parameter analysis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of boundary-repelling regularization $L_b$ sounds interesting and novel to me, which encourages samples to be away to other classes, with the corresponding ablation study and parameter study to support this claim.\n2. The paper details the parameters and the training configurations, especially that the authors include the codebase as the supplementary, making this work reproducible.\n3. The ablation study and parameter analysis are extensive and comprehensive, with testing variants of each module, hyper-parameters, and the loss designs."}, "weaknesses": {"value": "1. The motivation about higher inter-class similarity for time series data is not convincing (Fig. 2 with L65-67). While dogs and pandas are different that can be easily separated, if we consider more fine-grained classes (e.g., black bear vs. brown bear), they probably may not be highly separable. Note that this concern is raised since the authors use \"walking\" and \"running\" as the time series examples, which is unfair to compare to coarse categories in the image domain. As this is the main (and only) motivation, this work does not sound necessary, even though it may demonstrate performance improvements.\n2. While TimeLT demonstrates effectiveness over existing baselines, the experiments are only conducted with 4 datasets; however, CFAMG used 53 time-series datasets for evaluations, causing the experiments in this paper lacking insufficient validation. Additionally, the used 4 datasets are different from the CFAMG paper, which is difficult to evaluate its validity from the tables.\n3. The third contribution of releasing a new benchmark remains unclear and questionable since the authors do not describe any motivations about it as well as why existing benchmarks are insufficient (e.g., UCR and UEA in the CFAMG paper)\n4. The multi-scale temporal encoding has been explored by existing works, e.g., [1, 2]. However, the authors do not include any related works for comparisons and discussions.\n5. The setting for head and tail classes (Table 2) is a bit confusing. Since it is an imbalance task, dividing with 50-50 to represent head and tail could largely cover classes that have similar portions. This raises the validity of interpreting Table 2.\n6. [Minor]: Missing reference in L676.\n\n[1] LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters\n\n[2] PromptTSS: A Prompting-Based Approach for Interactive Multi-Granularity Time Series Segmentation"}, "questions": {"value": "Q1: Regarding the claim \"representations of tail classes tend to cluster near or even overlap with those of head classes, leading to blurred decision boundaries\", could the authors prove or provide references to support this claim?\nQ2: What is the reason that many methods suffer from very worse performance for Epilepsy-LT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pXgwpOc4e7", "forum": "0bGqD9hEcB", "replyto": "0bGqD9hEcB", "signatures": ["ICLR.cc/2026/Conference/Submission9450/Reviewer_PqPE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9450/Reviewer_PqPE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761293494506, "cdate": 1761293494506, "tmdate": 1762921046059, "mdate": 1762921046059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TimeLT, a method designed to address the challenges of imbalanced long-tailed time series classification (TSC).\nTimeLT is composed of three main steps. Firstly, data augmentation via oversampling and perturbation is employed to enhance data diversity. Next, a temporal encoder is implemented for time series representation, consisting of a 1D-CNN embedding on multi-scale down-sampled series and a GRU as the backbone encoder. Finally, two regularization terms are applied to refine the decision boundary, with one pushing samples away from the boundary and the other pulling embeddings closer to the corresponding centroid.\nExperimental results across multiple datasets, in comparison with 16 baseline methods, demonstrate TimeLT's leading performance on long-tailed TSC. These results are further verified through analysis including ablation, visualization, and sensitivity test."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(1) The problem of long-tailed time series classification is important and timely within the community. The proposed method provides a well-motivated solution. \\\n(2) The experimental results are compelling and demonstrate the efficacy of the method. \\\n(3) The structure of the article is clear, and the paper is easy to follow."}, "weaknesses": {"value": "(1) The ablation is not rigorous. Specifically, in the preprocessing stage, oversampling is a commonplace method when dealing with imbalanced classes. The current \"**w/o O&A**\" ablation does not clarify whether the improvements stem from oversampling or perturbation. A more thorough ablation should include \"**w/o O**\", \"**w/o A**\", and \"**w/o O&A**\" to ascertain the contribution of each component. \\\n(2) The novelty of the proposed framework appears to be more of an engineering integration rather than an algorithmic breakthrough, given its reliance on established techniques. \\\n(3) There are several problems w.r.t. the writing. For example, grammatical faults like \"This is can be\" and format faults like \"**Analysis of** $\\beta$\" ($\\beta$ should be bolded) \\\n(4) Please refer to the questions for further potential weaknesses."}, "questions": {"value": "(1) The Imbalance Ratio (IR) is defined as the ratio between the largest and smallest class sample counts. This only considers the two extreme classes. For instance, compare two scenarios: one where class sizes decay linearly from $N_1$ to $N_C$, and another where only $N_C$ drops sharply with the others being similar. Under your IR definition and its application in the method, would there be different effects in these cases? \\\n(2)  In Eq.(3), the use of c/C presumes a linear decay of sample counts across classes. Is this assumption valid? Or would it be more appropriate to use $N_c$ / $N_C$? \\\n(3) The author suggests GRU is a better choice in comparison with the Transformer because of the sequential modelling capability. Two questions have arisen w.r.t. this argument. Firstly, there are certain variations of the Transformer models that can model temporal dependencies better than the vanilla Transformer, but these variations are not included in the analysis or the comparable experiments. Second, the author suggests that information leakage caused by the Transformer will cause less accurate modelling. However, variations like causal attention enable the model only having access to the past time steps. At the same time, the whole series is available for TSC, which differentiates it from time series forecasting, so I do not consider that there is an information leakage problem. \\\n(4) Can Eq.(8) and Eq.(9) be unified in one contrastive loss function that simultaneously pushes away from the boundary (negative sample pair) and pulls closer to the centroid (positive sample pair)? \\\n(5) The Related Work in Appendix A has only one subsection A.1, and only the long-tail learning is included. Is this part left to be unfinished?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pVII8mzwd8", "forum": "0bGqD9hEcB", "replyto": "0bGqD9hEcB", "signatures": ["ICLR.cc/2026/Conference/Submission9450/Reviewer_5fq8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9450/Reviewer_5fq8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828546906, "cdate": 1761828546906, "tmdate": 1762921045544, "mdate": 1762921045544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a framework, named TimeLT, to learn a robust feature space from long-tailed data, thereby improving the overall accuracy for long-tailed TSC. Additionally, a benchmark is released, which includes data processing protocols, diverse datasets, and multiple baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe problem addressed in this paper is common and significant within the field of time-series classification.\n2.\tThe authors publish a standardized benchmark that holds significant practical value for promoting fair comparisons and future development within the long-tail TSC domain.\n3.\tThe experiments are comprehensive, demonstrating the proposed method's effectiveness through comparisons with multiple baselines."}, "weaknesses": {"value": "1.\tThe components of TimeLT, including perturbation-aware data augmentation, multi-scale temporal encoding, and oundary-repulsion regularization, are well-established techniques that have been extensively studied, which to some extent limits the novelty of this paper. Authors should provide a more detailed explanation of why this combination can address the challenges posed by high inter-class similarity.\n2.\tThis paper lacks discussion of the computational overhead and training/inference time of the TimeLT framework, as multi-scale encoding will inevitably increase computational costs.\n3.\tThe description of the decision boundary embedding set B in Eq.(7) is vague, a more detailed computational process and explanation should be provided."}, "questions": {"value": "The authors only illustrate inter-class similarity through Figure 2, lacking quantitative analysis and description (such as DTW distance) to demonstrate that it is indeed higher than typical image datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "D7Wq2O5ukA", "forum": "0bGqD9hEcB", "replyto": "0bGqD9hEcB", "signatures": ["ICLR.cc/2026/Conference/Submission9450/Reviewer_Xejq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9450/Reviewer_Xejq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926717521, "cdate": 1761926717521, "tmdate": 1762921045220, "mdate": 1762921045220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the problem of long-tailed time series classification, where models often fail under severe class imbalance. The authors propose TimeLT, which integrates (1) a perturbation-aware augmentation to enhance tail-class diversity, (2) a multi-scale temporal encoder for rich feature extraction, and (3) a boundary-repulsion regularization to improve class separability. A new benchmark with 16 baselines is introduced, and experiments show consistent gains, especially on tail classes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles an underexplored yet practically relevant problem, long-tailed time series classification, and provides a unified benchmark that may serve as a foundation for future research.\n\n2. Extensive experiments across 16 baselines and multiple datasets convincingly show the effectiveness and robustness of TimeLT."}, "weaknesses": {"value": "1. The augmentation and regularization strategies appear to reuse existing ideas with limited novelty or justification specific to time-series data.\n\n2. The motivation-to-method alignment is weak and some design choices are introduced abruptly without sufficient reasoning."}, "questions": {"value": "1. The abstract lists three components of TimeLT, but the unsolved problems they connected to are unclear. \n\n2.  How does Gaussian noise outperform more structured augmentations such as frequency- or context-based methods?\n\n3. What role does the multi-scale, channel-independent design play in handling class imbalance?\n\n4.  The boundary-repulsion loss resembles margin-based or supervised contrastive objectives. Could the authors clarify its unique contribution and explain how it improves discrimination in long-tailed TSC?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TUe72Evunz", "forum": "0bGqD9hEcB", "replyto": "0bGqD9hEcB", "signatures": ["ICLR.cc/2026/Conference/Submission9450/Reviewer_sotr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9450/Reviewer_sotr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990357332, "cdate": 1761990357332, "tmdate": 1762921044422, "mdate": 1762921044422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}