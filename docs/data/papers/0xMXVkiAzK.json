{"id": "0xMXVkiAzK", "number": 15693, "cdate": 1758254012487, "mdate": 1759897288456, "content": {"title": "Quantitative LLM Judges", "abstract": "LLM-as-a-judge is a framework where a large language model (LLM) evaluates the output of another LLM. While LLMs excel at producing qualitative textual evaluations, they often struggle to predict human preferences and numeric scores. We propose quantitative LLM judges, which align evaluation scores of existing LLM judges to humans in a given domain using regression models. The models are trained to improve the score of the original judge using its rationale and score. We present four quantitative judges for different types of absolute and relative feedback, which showcases the generality and versatility of our framework. Our framework is more computationally efficient than supervised fine-tuning and can be more statistically efficient when human feedback is limited, which is expected in practice. We validate these claims empirically on four datasets using two base judges. Our experiments show that quantitative judges can improve the predictive power of existing judges through post-hoc modeling.", "tldr": "We propose quantitative LLM judges, which align evaluation scores of existing LLM judges to human scores in a given domain using classic machine learning models.", "keywords": ["LLM-as-a-judge", "LLM evaluation", "human feedback", "alignment", "classic machine learning models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/48e8569ee77a905c8c62b99c2a7e67e0f4e941d9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes \"*quantitative LLM judges*\", where they train a generalized linear model (GLM) on top of the embeddings and score prediction from an LLM-as-a-judge model. They propose several variants for pointwise and pairwise rating, including the least square judge, the multinomial judge, and two variants of Bradley-Terry-Luce (BTL) judges. On four datasets, they show that the quantitative judge is useful and works well when the training data is limited."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- This paper proposes a lightweight method to use and improve existing LLM-as-a-judge models \n- The framework can be used in both pairwise and pointwise LLM-as-a-Judge tasks\n- The method requires fewer training resources, including time and data."}, "weaknesses": {"value": "- The results do not consistently show that the quantitative judge is better than the baselines, including the absolute/relative base (e.g., Table 1 summarize from feedback Llama and Prometheus) or the SFT baselines. While the SFT baseline requires much training time, this might be acceptable since the model only needs to be trained once, and the training overhead is not a problem.\n- Lack of comparison to important baselines, including TRACT (which is cited in this paper) and Critique-out-loud reward model (CLoud). The two methods use regression loss to fine-tune for LLM as a judge, where TRACT uses the LM head to calculate the score, and Cloud trains a new regression head to predict the score. TRACT also shows that it can be applied to pairwise and pointwise evaluation datasets, so it seems like a reasonable baseline to be compared with. In line 44, the reviewed paper cites TRACT and argues that LLMs are poorly suited for regressing scores, while TRACT itself is designed to solve the problem. It seems somewhat odd not to include this model for comparison. Moreover, the method described in this paper, the squared judge, looks very similar to Cloud when fixing the LLM and only training the regression head, except that the least square judge in this paper additionally takes the score prediction as the input.\n- Lack of comparisons to simpler calibration methods: The proposed method is a method for calibration, where they want to use the original score/judgment prediction and the embedding from the base judge model, and predict a new *calibrated score*, and this calibration model is a small GLM. It is unclear whether it is necessary to learn such a calibration model based on the embedding and score, or whether we can simply use well-known calibration methods, including temperature scaling, to calibrate the score and improve the alignment to human preference.\n- Insufficient ablation studies: A critical question is where the performance improvement is from. Is it from the embedding of the judge or from the score prediction? A valid and critical ablation I would like to see is how the performance varies if we only use either the base judge's embedding or the score as the input. How would the performance become? Note that here, using only the score as the input to the GLM becomes the simple baseline mentioned in the previous weakness\n- Some unsupported sentences: The last two sentences on page 2 are unsupported by any references."}, "questions": {"value": "- Q1. What does it mean to say \"*While LLMs excel at producing qualitative textual evaluations, they often struggle to predict human preferences and numeric scores*\". Do you want to say it is good for predicting the CoT (rationale) for the evaluation, but the score may not be calibrated and aligned with the ground truth human score? If this is the case, can this claim be justified? More precisely, is there any past evidence that focuses on the quality of the rationales?\n- Q2. What does it mean by \"*LLM judges typically output both rationales and numeric scores, thus combining the comprehensiveness of human evaluation with the scalability of automated metrics*\"? It seems that there is a causal relation between the two sentences, indicated by the word *\"thus\"*. However, it is not directly clear how the two sentences are related to each other.\n- Some of the papers, including \"LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks\" and \"Tract: Regression-aware fine-tuning meets chain-of-thought reasoning for llm-as-a-judge\", are from ACL 2025, but they are cited in the arxiv format. It is recommended to cite the papers in their proceedings form. I am only listing the first two entries I found, and I leave checking other papers to the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wvrls8bVgO", "forum": "0xMXVkiAzK", "replyto": "0xMXVkiAzK", "signatures": ["ICLR.cc/2026/Conference/Submission15693/Reviewer_a3PY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15693/Reviewer_a3PY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761650382653, "cdate": 1761650382653, "tmdate": 1762925944688, "mdate": 1762925944688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a framework that extends the base judge model by introducing an additional stage after the original rationale and score generation. Specifically, a small model is trained to predict the final score based on the rationale embedding and the initial score produced by the base judge. This extension improves alignment with human evaluations while remaining computationally and data efficient compared to SFT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed framework is highly generalizable, as it relies only on rationales and scores and can therefore be applied to any base LLM judge.\n2. The paper effectively leverages the original rationales and scores by training a small model to predict the final score, improving alignment with human evaluations in a computationally and data-efficient manner.\n3. The paper tests various types of small models, such as the least-squares (LS) judge, multinomial (MN) judge, Bradley–Terry–Luce (BTL) judge, and two-headed BTL (BTL2) judge, to demonstrate the effectiveness of their framework."}, "weaknesses": {"value": "1. It is helpful to include experiments that use a larger and more complex model for the quantitative judges, such as a two-layer MLP.\n2. The paper currently lacks ablation studies and analyses investigating the impact of the rationale embedding and the initial score. In particular, it is necessary to include experiments that use only rationale embeddings and those that use only the score."}, "questions": {"value": "1. How is the rationale embedding $\\phi(e)$ specifically derived from the base judge?\n2. For embedding quality, it is helpful to use the Qwen3 Embedding [1] or other advanced embedding models [2] to provide a more convincing demonstration.\n\nReferences\n\n[1] https://huggingface.co/collections/Qwen/qwen3-embedding\n\n[2] https://huggingface.co/spaces/mteb/leaderboard"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "goxsV7oJHz", "forum": "0xMXVkiAzK", "replyto": "0xMXVkiAzK", "signatures": ["ICLR.cc/2026/Conference/Submission15693/Reviewer_fDpg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15693/Reviewer_fDpg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966545869, "cdate": 1761966545869, "tmdate": 1762925944202, "mdate": 1762925944202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for calibrating LLM-as-a-judge on human preference data. In particular, the method relies on the use of a lightweight model (GLMs and variants) to align an LLM’s numeric scores with human preferences. In empirical experiments, the quantitative LLM judges outperform base and fine-tuned LLM judges in several instances and require less computation than fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe calibration framework presented is simple, post hoc, and easy to implement.\n\n-\tIt is (pleasantly) surprising that a lightweight model such as ordinary least squares is sufficient to calibrate LLM-judges to human evaluations. In a similar vein, it is nice that this method is post hoc (efficient to optimize a GLM, no fine-tuning required). The proposed calibration framework is simple, elegant, and easy to implement.\n\n-\tThe paper is very well motivated, addressing the important and timely concern that LLM judges often fail to align with human preferences and can exhibit structural biases (e.g., favoring responses from models with similar architectures or training data). The authors do a nice job situating their work within this broader landscape of prior studies."}, "weaknesses": {"value": "-The proposed quantitative judges are trained and evaluated on datasets labeled by the same human preferences used to assess their alignment. This setup is circular: models are optimized to reproduce human scores and then evaluated on their ability to approximate those same scores. To ensure that the method is not simply overfitting to the human labels, a stronger experiment might evaluate calibration on held-out datasets (can be in a similar domain).\n\n-Dependence on Human Evaluations: The approach relies on the availability of human evaluation data for calibration, which can be challenging to obtain at scale, especially in settings where LLM-as-a-judge is intended to reduce human effort.\n\n-The study evaluates only two base models. It would be helpful to discuss or test whether the proposed calibration framework generalizes to models of different sizes or architectures.\n\n-Please provide a justification for the design choice of concatenating the judge’s score with its rationale. Why is this a natural or theoretically grounded way of combining the two signals? The concatenation of rationales with judge scores is an intuitive step, but there is no analysis of how much the textual component versus the numerical score contributes to predictive power. This makes it unclear whether the system is actually “reasoning better” or simply leveraging linguistic correlations in rationales.\n\n-In Section 4.1, the concept of “population bias” is introduced without clear explanation. Please provide a clear moviation for each new term, when it is introduced in the paper.\n\nMinor Stylistic /Presentation Revisions:\n\n-The phrasing “Comparison to base judges and fine-tuning them” reads colloquially; consider “Comparison to base and fine-tuned judges.”\n\n-Please ensure that font usage is consistent near the end of Section 5.2."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YlGmCJBLVd", "forum": "0xMXVkiAzK", "replyto": "0xMXVkiAzK", "signatures": ["ICLR.cc/2026/Conference/Submission15693/Reviewer_6cZR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15693/Reviewer_6cZR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966672669, "cdate": 1761966672669, "tmdate": 1762925943846, "mdate": 1762925943846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an evaluation method based on the LLMs-as-Judges framework. The central idea is to introduce an additional trainable module that maps the reasoning and judgment outputs of the base LLM to a final evaluation score. Depending on the evaluation paradigm, this module can take different forms. The paper addresses both pointwise and pairwise evaluation settings and experiments across a broad range of datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and well structured, with comprehensive coverage of both model architectures and datasets.\n\n- The proposed approach is conceptually straightforward and easy to follow."}, "weaknesses": {"value": "- The novelty of the proposed method appears limited. Prior research has already explored the use of trainable adapters or probing modules to enhance evaluation accuracy.\n\n- The experimental comparison is relatively weak. The method is compared with only one SFT baseline. To convincingly demonstrate superior performance, the paper should include a broader set of baselines—both training-free calibration methods (e.g., G-Eval) and approaches that incorporate additional training modules, such as linear probing methods [1].\n\n[1] Improving Preference Extraction In LLMs By Identifying Latent Knowledge Through Classifying Probes"}, "questions": {"value": "Is it possible to extend the framework to unsupervised evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zIXEC3L6Fb", "forum": "0xMXVkiAzK", "replyto": "0xMXVkiAzK", "signatures": ["ICLR.cc/2026/Conference/Submission15693/Reviewer_t8ti"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15693/Reviewer_t8ti"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762173555527, "cdate": 1762173555527, "tmdate": 1762925943370, "mdate": 1762925943370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}