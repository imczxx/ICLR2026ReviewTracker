{"id": "yMo7Z670f6", "number": 5768, "cdate": 1757933846618, "mdate": 1763637613587, "content": {"title": "TurboReBeL: 250$\\times$ Accelerated Belief Learning for large Imperfect-Information Extensive-Form Games", "abstract": "Recursive Belief-based Learning (ReBeL) provides a general framework for large-scale Imperfect-Information Extensive-Form Games (IIEFGs) by integrating self-play reinforcement learning with search. However, ReBeL suffers from prohibitive computational costs during training: each Public Belief State (PBS) sample requires $T$ iterations of Counterfactual Regret Minimization (CFR), and the PBS state space necessitates billions of samples for convergence. For example, training ReBeL on games such as Heads-Up No-Limit Texas Hold'em (HUNL) from scratch demands $4.5$ billion samples and $2$ million GPU hours. To address this, we propose TurboReBeL, which achieves a $\\sim 250\\times$ acceleration in training through two key innovations: (i) Single-Sample Multi-Iteration Generation: This core innovation fixes subgame strategies to CFR-averaged policies, generating data for all $T$ iterations in one sampling pass and yielding a theoretical $O\\left(T\\right)$ speedup. (ii) Isomorphic Data Augmentation: This technique enhances sample diversity through game-theoretic invariants (suit and chip isomorphism) with minimal overhead and no performance loss. Evaluations show that TurboReBeL matches ReBeL's exploitability in Turn Endgame Hold'em using approximately $0.4$\\% of the training cost, and achieves comparable performance on HUNL with $450\\times$ fewer samples. TurboReBeL is the first depth-limited solving framework that combines ultra-fast training, strong scalability, low exploitability, theoretical convergence guarantees, human-data-free training, and fast real-time decision-making, representing a fundamental breakthrough in solving IIEFGs.", "tldr": "We introduce a novel general depth-limited solving framework for large IIEFGs, achieving unprecedented training efficiency without sacrificing the strengths", "keywords": ["Game Theory", "Imperfect Information Games", "Counterfactual Regret Minimization", "Deep Reinforcement Learning", "Depth-limited Solving"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6126c303ae152a19ff4c652a8ef1859795de57a3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces TurboReBeL, a framework designed to accelerate the training of agents for large imperfect-information games by tackling the computational bottlenecks of the state-of-the-art ReBeL algorithm. The authors propose two key innovations: 1) Single-Sample Multi-Iteration Generation (SSMIG), which decouples strategy evolution from value estimation to generate multiple training data points from a single subgame solve, and 2) Isomorphic Data Augmentation, which applies game-theoretic symmetries to increase data diversity. Experiments on large-scale poker variants demonstrate a 250x reduction in training cost while achieving comparable performance to the original ReBeL."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The work addresses a critical bottleneck in the fieldâ€”the prohibitive training cost of strong equilibrium-finding algorithms. A 250x speedup is a significant practical contribution that could enable research on larger, more complex games.\n\n- The central idea of SSMIG is clever and well-motivated. Decoupling value estimation from a dynamically changing strategy is a fundamental insight that significantly improves data generation efficiency.\n\n- The proposed method is validated with compelling results on challenging benchmarks (HUNL and TEH) against strong baselines. The claims are further supported by theoretical analysis showing that the approach preserves the convergence guarantees of ReBeL."}, "weaknesses": {"value": "- The two main contributions (SSMIG and data augmentation) are primarily evaluated together. A more detailed ablation study isolating the impact of SSMIG would strengthen the paper by quantifying the precise benefit of the core innovation.\n\n- The paper claims a \"450-fold reduction in sample requirements\" but a \"250x training acceleration.\" The derivation of the latter from the former is not transparently detailed, making the headline claim difficult to verify.\n\n- The isomorphic data augmentation techniques are specific to poker. The paper would benefit from a more thorough discussion of the challenges and feasibility of applying similar techniques to other IIEFGs that may lack such obvious symmetries."}, "questions": {"value": "1.  Can you provide a more detailed breakdown of how the 450x sample reduction translates into the 250x overall training acceleration, perhaps in terms of total GPU hours?\n2.  How much of the performance gain is attributable to SSMIG alone? An ablation showing TurboReBeL without isomorphic augmentation would clarify this.\n3.  There appears to be a discrepancy in the data augmentation factor K mentioned in Figure 2 (K=24) and Appendix B (K=2). Could you clarify which was used in the main experiments and why?\n4.  How sensitive is the value estimation in Phase 2 to the quality of the fixed reference strategy? For instance, does performance degrade significantly if the strategy is computed with a much smaller number of CFR iterations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BOyJv5CN4a", "forum": "yMo7Z670f6", "replyto": "yMo7Z670f6", "signatures": ["ICLR.cc/2026/Conference/Submission5768/Reviewer_dzpC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5768/Reviewer_dzpC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761033451984, "cdate": 1761033451984, "tmdate": 1762918249208, "mdate": 1762918249208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an improvement on RebeL. RebeL is a general framework for performing depth-limited search in two-player zero-sum imperfect-information games using public-belief state value functions.\n\nThis paper presents two improvements.\n\nThe first (Section 5.1) is a method that adds training data for the value function for every iteration of CFR performed during depth-limited search, instead of only one training datum after CFR is completed.\n\nThe second (Section 5.2) is data augmentation for poker: for each training datum, we can create other PBSs that should have mathematically equivalent values, e.g. by permuting all the suits of the cards.\n\nThe paper empirically shows that the first method improves exploitability in Turn Endgame Hold-em."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The ideas are (to my knowledge) novel and the research direction of making training efficiency improvements to ReBeL is good.\n\nThe empirical results in Figure 2 dramatically demonstrate the training efficiency improvement of TurboReBeL over ReBeL + data augmentation."}, "weaknesses": {"value": "1. The main method of the paper (Section 5.1) isn't explained super clearly anywhere.\n\nSecond paragraph of Section 5.1: \"We hypothesize that for a given subgame, once the average strategy [...] is computed as a reference, we can efficiently estimate values for all intermediate belief states [...] using this fixed reference strategy\".\n\nI think this could benefit from formalization or more explicit description. It took me a while to realize that the intermediate belief states for the root public state came from the CFR solve of the *previous* subgame, not the current subgame being solved.\n\n2. The theorems seem vague and empty to me.\n\nI tried to follow the theorems and proofs but got lost. Perhaps my biggest big-picture issue with them is that it is claimed that TurboReBeL preserves ReBeL's convergence guarantees, but the theorems and proofs don't seem to actually address TurboReBeL's difference from ReBeL. In particular, by the assumptions made in TurboReBeL, we are training the value function with a lot of \"incorrect\" data. We would like to know how this imperfectness of the value function affects the soundness of the algorithm as a whole. However, in my understanding, this imperfectness is completely swept under the rug via the statement \"Under the assumption that the value network [...] approximates the CFV [...] with error at most $\\epsilon_{approx}$\" -- ignoring the fact that the error $\\epsilon_{approx}$ is entirely what we are interested in. (In ReBeL, this assumption raises no eyebrows because the value function error is a result of just the imperfection of the machine learning model and the imperfection of CFR.\n\nFurther critiques of Theorem 1:\n- The statement says \"let $\\beta_{s,t}$ be the PBS reached according to $\\hat{\\sigma}^t$. To be clear, $\\beta_{s,t}$ is any leaf PBS, not \"the\" PBS reached, right?\n- The statement introduces $\\hat{\\sigma}^T$ but doesn't define what it is. The proof relies on this strategy being part of an approximate equilibrium. Where does it come from?\n\n3. More details on the experiment would be helpful.\n\n- What is the implementation? Did you use an open-source implementation of ReBeL or implement from scratch?\n- The 250x speedup is mentioned often but is not actually borne out in experiments, right? Why not run an experiment with ReBeL vs. TurboReBeL to empirically show what the speedup is? It seems incorrect to simply claim a 250x speedup without actually testing the empirical speedup of the data augmentation (i.e. I might reasonably assume that 24x data augmentation does NOT lead to a 24x speedup, because the neural net by itself may learn the invariance). \n- What is in Figure 2? TurboReBeL with k=2 vs. ReBeL w/data augmentation w/k=24? You have to have ablations that compare TurboReBeL with k=2 vs. ReBeL w/data augmentation w/k=2. Or TurboReBeL k=24 vs. ReBeL k=24. And a comparison to normal ReBeL.\n\n4. Minor:\n- Section 5.2 describes Chip Isomorphism, and says that these transformations preserve Nash equilibrium properties. It's a minor detail, but this probably deserves a footnote at least saying that this isn't really true (at least it's not clear to me that it is) because chips and bets are integer amounts, so scaling the number of chips could alter the optimal strategies (especially when players have very few chips).\n- Some \"marketing\" language in the paper is unnecessary: \"Unprecedented Efficiency Gains\" (bottom of page 2), \"This innovative phase\" (top of page 7)"}, "questions": {"value": "1. Sorry if I missed this, but did you state your ReBeL implementation anywhere? Was it based on an open-source library, or written from scratch?\n2. Did you consider using some neural network architecture that reflects the invariance of suit permutations and/or chip isomorphisms instead of data augmentation?\n3. Just to be clear, the ReBeL+Data Augmentation line on Figure 2 is ReBeL + Section 5.2, right? and TurboReBeL Is ReBeL + Section 5.1 + Section 5.2? And they use the same implementation, they only differ in that one has 5.1 and one doesn't?\n4. Why is the fidelity of TurboReBeL so much higher in Figure 2 than ReBeL+Data Augmentation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DVtByGrs10", "forum": "yMo7Z670f6", "replyto": "yMo7Z670f6", "signatures": ["ICLR.cc/2026/Conference/Submission5768/Reviewer_vfXR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5768/Reviewer_vfXR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947569343, "cdate": 1761947569343, "tmdate": 1762976692406, "mdate": 1762976692406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ReBel (Brown et al.) used billions of samples to train their poker bot, leaving the community to wonder how much compute is actually needed to solve poker. This paper introduces two orthogonal ideas for generating more data to reduce the compute requirement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Theorem 1 provides the ground truth for generating more data with the hope of learning more using fewer samples. It is novel and of interest to the community as it enables generating more samples.\n\nSimilarly the first augmentation proposed here is reasonable even if canonicalization has been used before."}, "weaknesses": {"value": "The claims of the paper are not always appropriate in my opinion. Training efficiency doesn't scale with augmentation, otherwise random crops would yield $\\infty \\times$ efficiency gain.\n\nOne of the main cited contributions of this work is the data augmentation, yet it's not clear how the first augmentation compares against the canonicalization used in prior art like Deep CFR or how relevant the second data augmentation is given the stochastic stacks used during training. It is also not clear how the augmentation is sane given that it is conceivable that the strategies of the players change if the bigblind stays constant but the pots and stacks are multiplied.\n\nWhile Theorem 1 provides the groundwork for the methods proposed in the paper, the authors make no effort to ablate the effect of their method. Something as simple as the MSE between the true value (with paramter theta) vs estimated value (via the optimal policy) could have shed some lights on how off the estimation is. Also notably the learning error for this method doesn't subside over time\n\nTheorem 2 doesn't show what the paper claims it is showing. Concretely, it shows that the policy they find for a specific subgame is a Nash equilibrium. This, however, does not mean that the policy obtained from playing using this algorithm is safe. For instance ReBeL introduces CFR-AVG but does not prove its soundness. The theorem, as presented here, is misleading. Similarly, the error parameter of the previous theorem includes a $1/\\sqrt{t}$ approximation error which is problematic."}, "questions": {"value": "What is the source for T=250? The publicly available ReBeL implementation uses 1K or 1Ki iterations. Similarly, in algorihtm 1, the averaging coefficient should (t-1)/t not t/(t+1) to account for the zero value.\n\nI would argue that Bakhtin et al. are not using a variant of ReBeL, why do you consider it a variant?\n\n@250 The number of hands in poker 1326 not 2652\n\nIs it really fair to cite Li et al. 2024 for the use of ReBeL in no limit, specially sine Li et al do no external evaluation beyond slumbot? On a similar note, Since Li et al use 8 PH402 for their implementation, isn't the claim that rebel \"necessitates billions of samples\" hyperbolic? This hyperbole is again used @085 \"0.4% of the training cost\"\n\nWhy are data augmentations applied before inserting the sample into the buffer, wouldn't it make more sense to apply the augmentation on the fly?\n\n@735 why is the betting abstraction so small? for instance Supremus uses a much larger abstraction."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5YsM9NGYBo", "forum": "yMo7Z670f6", "replyto": "yMo7Z670f6", "signatures": ["ICLR.cc/2026/Conference/Submission5768/Reviewer_hYMG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5768/Reviewer_hYMG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762063589159, "cdate": 1762063589159, "tmdate": 1762918247929, "mdate": 1762918247929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes two additions to the rebel algorithm that appear to increase its sample efficiency by two orders of magnitude. These additions are the use of sampled symmetries during data generation and a modified version of generating value targets that generates multiple targets per subgame solution."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper, to the extent that I can tell, has really strong empirical results that drastically decrease the costs of running Rebel."}, "weaknesses": {"value": "I have some serious issues with the proofs in this paper and will adjust my score if these issues are resolved.\n## Major weaknesses.\n- The proof of theorem 1 relies on an unproven claim, namely that the value error of the composite strategy $\\sigma^{t,T}$ is bounded by the sum of the value errors of the strategy before reaching s and the strategy after reaching s (line 769). You have to actually show the \"the composition of strategies\" step. Both of the cited papers (Burch et al., 2014; Brown & Sandholm, 2017) use different safe subgame solving techniques to ensure that the combined policies do not increase the opponent's best response counterfactual values. I'm happy to change my score if this step is worked out fully and the proof holds properly.\n- I think the proof in the appendix on the convergence of TurboRebel (theorem 2) requires the authors to show their work. It's a fairly large leap to defer the proof to an unnamed section of another paper. Please do it out, you're not short on space. It also relies on theorem 1, which as mentioned, is not fully proven. \n\n\n## Minor\n- I do not believe the claim (in the intro and line 245) that Rebel influenced Bakhtin's no-press diplomacy paper is correct. The methods are incredibly different."}, "questions": {"value": "My questions are essentially stated in the weaknesses section. However, a few additional questions:\n\n- Theorem 1 and 2, as stated, is not actually a convergence proof for the TurboRebel algorithm, which derives its targets from $\\bar{\\sigma}^T$ whereas the proof relies on the intermediate $\\sigma^{t,T}$, values that are not stored during the TurboRebel process. Am I misunderstanding the gap between the algorithm and the proof?\n- When you apply the isomorphic transformation to rebel and deepstack, you use K=24. In turborebel, you use K=2 if I've understood the appendix correctly. Why this discrepancy between the two? Was the value tuned per-algorithm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jRSC3PRiuW", "forum": "yMo7Z670f6", "replyto": "yMo7Z670f6", "signatures": ["ICLR.cc/2026/Conference/Submission5768/Reviewer_uRCN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5768/Reviewer_uRCN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762825891809, "cdate": 1762825891809, "tmdate": 1762918247580, "mdate": 1762918247580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}