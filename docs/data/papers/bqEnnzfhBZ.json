{"id": "bqEnnzfhBZ", "number": 14606, "cdate": 1758239809290, "mdate": 1759897359850, "content": {"title": "Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs", "abstract": "Machine unlearning (MU) for large language models (LLMs), commonly referred to as LLM unlearning, seeks to remove specific undesirable data or knowledge from a trained model, while maintaining its performance on standard tasks. While unlearning plays a vital role in protecting data privacy, enforcing copyright, and mitigating sociotechnical harms in LLMs, we identify a new vulnerability post-unlearning: unlearning trace detection. We discover that unlearning leaves behind persistent \"fingerprints\" in LLMs, detectable traces in both model behavior and internal representations. These traces can be identified from output responses, even when prompted with forget-irrelevant inputs. Specifically, even a simple supervised classifier can determine whether a model has undergone unlearning, using only its prediction logits or even its textual outputs. Further analysis shows that these traces are embedded in intermediate activations and propagate nonlinearly to the final layer, forming low-dimensional, learnable manifolds in activation space. Through extensive experiments, we demonstrate that unlearning traces can be detected with over 90% accuracy even under forget-irrelevant inputs, and that larger LLMs exhibit stronger detectability. These findings reveal that unlearning leaves measurable signatures, introducing a new risk of reverse-engineering forgotten information when a model is identified as unlearned, given an input query.", "tldr": "", "keywords": ["LLM", "Machine Unlearning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f85a8e42767b729de4d2ecb732d03c5ee440d0cb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces and investigates a novel vulnerability in large language models termed unlearning trace detection. The authors posit that machine unlearning processes leave behind detectable fingerprints in the model's behavior and internal representations. They propose a simple supervised classifier to reliably distinguish an unlearned model from its original counterpart, often with over 90% accuracy. Extensive experiments on four different LLMs and two state-of-the-art unlearning methods (RMU and NPO) demonstrate the generalization of their key finding and the effectiveness of the classifier-based detection method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is the first to formalize and systematically study the problem of unlearning trace detection. It identifies a critical and practical vulnerability in the context of machine unlearning. \n2. The paper excels not just in demonstrating that unlearning is detectable, but in explaining why. The spectral analysis in Section 5 is a standout contribution, pinpointing the \"fingerprints\" in intermediate-layer activations for RMU and showing their non-linear propagation to the final layer.\n3. The claims are substantiated by a strong empirical study. The authors evaluate their hypothesis across four modern, instruction-tuned LLMs of varying sizes, two distinct and state-of-the-art unlearning methods (RMU and NPO), and multiple prompt datasets. The consistent results across these diverse settings strongly support the generality of the findings."}, "weaknesses": {"value": "1. The study primarily focuses on two optimization-based unlearning methods, RMU and NPO. While these are representative, the field of LLM unlearning includes other paradigms, such as gradient ascent-based methods and model editing techniques (e.g., task vector subtraction or knowledge editing with methods like ROME/MEMIT). It is unclear if the findings generalize to these other approaches\n2. The text-only detection is shown to be significantly less reliable for RMU on smaller models than pre-logits-based detection. The implications of the access on the logits requirement could be discussed more thoroughly, especially in the context of the evolving landscape of model access."}, "questions": {"value": "- Could you hypothesize how your detection framework would perform against other classes of unlearning algorithms?\n- Your results show that text-based detection for RMU is less effective on smaller models like Zephyr-7B but improves with scale. Is this an inherent property of the unlearning traces in smaller models being weaker, or could it be a limitation of the specific text-based classifier used (LLM2Vec + MLP)? Have you explored whether more powerful text classifiers or different feature extraction methods could close this performance gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kBinD4mK9F", "forum": "bqEnnzfhBZ", "replyto": "bqEnnzfhBZ", "signatures": ["ICLR.cc/2026/Conference/Submission14606/Reviewer_YGLw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14606/Reviewer_YGLw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761283588423, "cdate": 1761283588423, "tmdate": 1762924988805, "mdate": 1762924988805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces and formalizes the problem of unlearning trace detection, investigating whether it is possible to determine if a Large Language Model (LLM) has undergone an unlearning procedure based on its outputs. The authors demonstrate that unlearning, intended to remove information, paradoxically leaves behind detectable \"fingerprints.\" They frame detection as a supervised classification task and show that a simple classifier can distinguish an unlearned model from its original counterpart with high accuracy. They comprehensively evaluate the found phenomenon and the effectiveness of the proposed detection method across four different LLMs and two state-of-the-art unlearning methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper pioneers the formal study of unlearning trace detection, a timely and critical problem as unlearning becomes integral to LLM safety and privacy. The threat model is well-motivated: an adversary who can identify an unlearned model can more efficiently allocate resources to attack it, undermining the very purpose of unlearning.\n2. The experimental validation spans four modern LLMs of varying scales (7B to 34B), two distinct and state-of-the-art unlearning algorithms (representation-based RMU and preference-based NPO), and a diverse set of prompts (both forget-relevant and forget-irrelevant).\n3. The use of SVD and UMAP to visualize distributional shifts in activation space provides clear, intuitive evidence for why activation-based classifiers are so effective, linking the empirical results directly to the model's internal mechanisms."}, "weaknesses": {"value": "1. The proposed supervised detection approach requires the adversary to possess a labeled training dataset of outputs from both the original and the unlearned models. This is a very strong prerequisite, as it is unclear how an adversary would realistically obtain such paired models to train their detector.\n2. The most effective detection method presented relies on gray-box access to the model's pre-logit activations. While plausible for open-weight models, this assumption does not hold for the many powerful LLMs available only through black-box APIs.\n3. Experimental results show that NPO leaves much more obvious traces than RMU, suggesting that the invisibility of unlearning is highly algorithm-dependent. But the authors only evaluate two kinds of unlearning paradigms."}, "questions": {"value": "- The training of the detector requires access to outputs from both the original and unlearned models. Could the authors clarify the real-world scenario you envision for an adversary to acquire this training data?\n- Experimental results show that NPO leaves much more obvious traces than RMU, suggesting that the invisibility of unlearning is highly algorithm-dependent. Based on your analysis, what properties of an unlearning algorithm might lead to more \"stealthy\" unlearning that evades detection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fKNhBzvgyM", "forum": "bqEnnzfhBZ", "replyto": "bqEnnzfhBZ", "signatures": ["ICLR.cc/2026/Conference/Submission14606/Reviewer_Vq6t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14606/Reviewer_Vq6t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551874268, "cdate": 1761551874268, "tmdate": 1762924987938, "mdate": 1762924987938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a new vulnerability in large language model unlearning: even after forgetting specific knowledge, unlearned models leave behind detectable “fingerprints.” The authors show that a simple supervised classifier can distinguish an unlearned model from its original version based solely on output behavior, including logits and textual responses to forget-irrelevant prompts. The work covers both RMU and NPO unlearning methods and evaluates models of different scales, demonstrating that unlearning trace detectability persists widely. The authors further localize these fingerprints to low-dimensional structures in internal activations, arguing that this poses security and privacy risks for safety-critical deployments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The topic is timely and highly relevant, given growing attention to machine unlearning in LLM safety and compliance. The idea of “unlearning trace detection” feels original and fits naturally with recent concerns about reverse-engineering vulnerabilities. The authors perform extensive experiments across multiple model scales, unlearning methods, and datasets, which lends credibility to their claim that detectable traces persist even when prompts are unrelated to forget targets. The activation-space analysis provides an interesting interpretability angle that helps explain observed classifier performance rather than just reporting empirical results. The potential implications for privacy and model governance are significant, making the paper’s message important for the community."}, "weaknesses": {"value": "While the paper positions itself as identifying a “security vulnerability,” the actual adversarial threat model lacks rigor. The evaluation assumes access to pre-logit activations in open-weight settings, which is unrealistic for many practical deployments, and the black-box-only scenario (text output) results are meaningfully weaker for RMU. The paper does not show a concrete way to exploit these traces to meaningfully recover forgotten information, even though this is suggested early as a concern. The analysis focuses on simple supervised classifiers without exploring more robust baselines, so it is unclear how fragile these findings are under model sampling variability or prompt diversity. Finally, the paper’s writing is at times repetitive and could more clearly differentiate what is novel versus what follows from known model-identity classification work."}, "questions": {"value": "The paper implies that trace detection could guide attackers toward targeting specific models for relearning or jailbreak attacks. Is there any quantitative experiment showing that adversaries can exploit trace detection to substantially reduce attack cost or increase success rate? What prevents the classifier from learning spurious cues rather than genuine unlearning fingerprints (for example, decline in fluency for NPO models)? The results for RMU rely heavily on access to internal activations. How do the authors justify calling the overall vulnerability “practical” when the black-box case remains borderline? It would be helpful if the authors could further analyze model sampling randomness and prompting conditions: do these traces remain detectable under different decoding strategies, temperature, or paraphrased questions? Finally, the threat model should be formalized more explicitly: what are the exact attacker capabilities, and which findings hold under the strictest constraints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uNbE5INPVv", "forum": "bqEnnzfhBZ", "replyto": "bqEnnzfhBZ", "signatures": ["ICLR.cc/2026/Conference/Submission14606/Reviewer_WKcC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14606/Reviewer_WKcC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657428306, "cdate": 1761657428306, "tmdate": 1762924987408, "mdate": 1762924987408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an empirical study on post-unlearning trace detection based on the model's output. Specifically, the paper shows that even a simple supervised classifier (a trained MLP) can determine whether a model has undergone unlearning, using only its pre-logit activations or its textual outputs. The paper claims that these traces are embedded in intermediate activations and propagate nonlinearly to the final layer, forming low-dimensional, learnable manifolds in activation space. The paper demonstrates that unlearning traces can be detected with over 90% accuracy under forget-irrelevant inputs, and that larger LLMs exhibit stronger detectability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "+ I appreciate that the paper clearly defines the threat model, making it easier for readers to follow the setting.\n+ The proposed approach is conceptually simple yet demonstrates strong empirical effectiveness.\n+ The research question is well-motivated and interesting."}, "weaknesses": {"value": "## Major Issues\n+ Several important baselines are missing, such as Gradient Ascent, SimNPO [1], adopted DPO [2][3], etc. Experimenting with only 2 representative methods (RMU and NPO) and with WMDP only reduces the generalizability of this study.\n+ The problem studied in this paper seems method-dependent. To my understanding, RMU steers the forget-representation to a random vector, i.e., randomizing the forget-representation, while NPO maximizes the loss of forget-samples. This seems obvious that the outputs of these unlearned models will be noise or randomized, making them easier to trace. A critical point arises with the adopted DPO-based unlearning, which instead maximizes the likelihood of producing refusal responses (e.g., “I don’t know”) for forget-samples. Under this setting, is the problem posed in this paper appropriate?\n+ The detection performance strongly depends on hyperparameters, such as the scaling coefficient in RMU or the retain weight. The detection results presented in the paper are based on a selected set of hyperparameters, which risks limiting the generalization of the findings.\n+ MMLU and WMDP evaluate distinct knowledge domains; therefore, I suggest stronger experimental designs would involve using the **MMLU subcategories closely aligned with WMDP domains, such as MMLU College Biology, MMLU Virology, and MMLU Computer Security, which represent truly forget-irrelevant samples**. These sub-categories better capture domain-specific retention and represent truly forget-irrelevant samples for a more meaningful detection evaluation.\n\n## Minor issues\n+ Typos: Line 443: \"Table ??\", line 709 \"Qwen2.5-7B\".\n+ Line 175: The paper states that \"and $\\mathbf{v}$ is drawn from a standard uniform distribution\"; it should be \"and each element in $\\mathbf{v}$ is drawn from a standard uniform distribution.\"\n+ Line 170-172: \"RMU enforces forgetting by mapping the intermediate representations of samples $\\mathbf{x} \\in \\mathcal{D}_f$ to random vectors\" should be \"RMU enforces forgetting by mapping the intermediate representations of samples $\\mathbf{x} \\in \\mathcal{D}_f$ to a *fixed, predetermined* random vector.\" (e.g., WMDP-bio and WMDP-cyber each use one fixed random vector, c.f. [4])\n+ It would be nice if the paper defined the dimension of the latent representation, such as $M_{\\theta}(\\mathbf{x})$.\n\n## References\n\n[1] Fan, Chongyu, et al. \"Simplicity prevails: Rethinking negative preference optimization for llm unlearning.\" ICML, 2025.\n\n[2] Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary Chase Lipton, and J Zico Kolter. TOFU: A task of fictitious unlearning for LLMs. In First Conference on Language Modeling, 2024.\n\n[3] Xiaojian Yuan, Tianyu Pang, Chao Du, Kejiang Chen, Weiming Zhang, and Min Lin. A closer look at machine unlearning for large language models. In The Thirteenth International Conference on Learning Representations, 2025b.\n\n[4] Li, Nathaniel, et al. \"The WMDP Benchmark: Measuring and Reducing Malicious Use with Unlearning.\" Forty-first International Conference on Machine Learning.\n\n[5] Shi, Weijia, et al. \"MUSE: Machine Unlearning Six-Way Evaluation for Language Models.\" The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "+ MUSE [5] is also a representative unlearning dataset that adopts an open-ended evaluation format (WMDP uses multiple-choice QA). Is there a specific reason why MUSE was not included in the experiments?\n+ Would detection performance improve if a more complex or higher-capacity classifier architecture were used?\n+ How does varying the ratio between forget-relevant and forget-irrelevant samples affect the detection results?\n+ What is the correlation between the control scaling factor $c$ and detection performance? In other words, how does detection accuracy change as $c$ increases? Could the low detection performance of Zephyr and LLaMA on MMLU, as reported in Figure 3, be explained by this effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A9BzDDOpVE", "forum": "bqEnnzfhBZ", "replyto": "bqEnnzfhBZ", "signatures": ["ICLR.cc/2026/Conference/Submission14606/Reviewer_H79h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14606/Reviewer_H79h"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970504666, "cdate": 1761970504666, "tmdate": 1762924986934, "mdate": 1762924986934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}