{"id": "Gbau7RIG2C", "number": 10194, "cdate": 1758163494503, "mdate": 1759897667987, "content": {"title": "FedMAP: Meta-Driven Adaptive Differential Privacy for Federated Learning", "abstract": "Federated learning (FL) enables multiple clients to train a shared model without sharing raw data, but gradients can still leak sensitive information through inversion and membership inference attacks. Differential privacy (DP) mitigates this risk by clipping gradients and adding calibrated noise, but most DP-FL methods rely on static noise and clipping schedules. Such rigid designs fail to account for client heterogeneity, changing convergence dynamics, and the growth of cumulative privacy loss. To address these challenges, we propose FedMAP, a closed-loop framework for adaptive differential privacy in FL. FedMAP integrates three components. First, a client-side MetaNet predicts clipping bounds and noise scales $(C_t,\\sigma_t)$ from gradient statistics using a lightweight pretrained BERT-tiny backbone, enabling effective adaptation across communication rounds. Second, a server-side Rényi DP accountant tracks heterogeneous privacy costs, computes the global expenditure $\\varepsilon_{\\mathrm{global}}$, and broadcasts it as a budget signal that constrains cumulative loss and guides client adaptation. Third, a global feedback regularization mechanism combines local penalties on per-round privacy cost with global penalties from $\\varepsilon_{\\mathrm{global}}$, ensuring alignment between client adaptation and the overall budget. Experiments show that FedMAP improves privacy compliance, and offers stronger robustness against attacks compared with baselines.", "tldr": "", "keywords": ["Federated Learning", "BERT-based MetaNet", "Personalized Differential Privacy", "DP Accountant"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f04de871d3fa2141ed3b4c8f5f7968bade6e4011.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposed a novel Federated Learning framework that protects against membership inference and reconstruction attacks under Differential Privacy. At a high level, the paper fine-tunes a BERT-based model to predict the hyperparameters (C and $\\sigma$) for the DP-SGD algorithm to preserve the privacy of the client's data. The paper also focuses on a scenario in which each client has their own privacy budget. Thus, the paper proposed an updating mechanism and objectives to incorporate into this setting. The paper conducts extensive experiments to highlight the advantages of their proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method provides flexibility and automation in tuning DP-SGD for different clients.\n- Extensive Theoretical and Experimental results are provided to support the advantage of the proposed methods."}, "weaknesses": {"value": "- The motivation of the work is not convincing. Specifically, it does not explain why different clients require different privacy budgets. For instance, if we consider FL for medical data, how can a client define a privacy budget to protect this sensitive data? Previous works have approached adaptive hyperparameters (C and $\\sigma$) from a performance perspective, which is more convincing. Thus, I suggest that the author clarify this point.\n- Secondly, the curation process for the label of C and $\\sigma$ is unclear and not optimal. How do you determine that the curated labels are the best for the subsequent iterations? Isn't this process empirical, and does the curator need to tune different C and $\\sigma$ at each iteration?\n- Thirdly, although the paper considers different privacy budgets for different clients. This is not highlighted in the proposed method or in how it integrates this information. Furthermore, the loss function in Eq. 12 will encourage each client to have the same privacy budget, which is in contradiction with the paper's goal.\n- Next, given a predicted C and $\\sigma$ from the meta model, the proposed method cannot achieve the predefined privacy budget of the clients, resulting in weak protection. How do you ensure that the consumed privacy budget is lower than the budget predefined by the clients?\n- Finally, the experimental results for Table 1, Figure 3, and 4 do not mention the privacy budget for each client, which reduces the validity of these experiments."}, "questions": {"value": "- Please address all the points in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S5UEEawcHe", "forum": "Gbau7RIG2C", "replyto": "Gbau7RIG2C", "signatures": ["ICLR.cc/2026/Conference/Submission10194/Reviewer_RdTF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10194/Reviewer_RdTF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10194/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761435686441, "cdate": 1761435686441, "tmdate": 1762921558702, "mdate": 1762921558702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "An overly complex method is proposed for private federated learning that purports to achieve a better utility/privacy tradeoff. The method is flawed because it does not account for the privacy loss of the MetaNet mechanism."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "No notable strengths."}, "weaknesses": {"value": "In exactly the same way as another paper I have just reviewed (evidently by the same authors, since much of the text is copied), this method is overly complex and the purported gains are not supported by the experiments, and in any case would not justify the implementation and pretraining costs.\n\nTo support a claim of improved privacy-utility trade-off, one must compare model utility (e.g., test accuracy) while holding $\\varepsilon$ constant across all methods. The current experiments (e.g., Figure 3) compare utility against communication rounds, which is insufficient to demonstrate a superior trade-off. Nowhere in Section 4.1 does it state that all methods were calibrated to achieve the same total privacy budget $\\epsilon$ for a fair comparison.\n\nHowever the most problematic issue is that the mechanism is flawed: in fact it does not provide a formal DP guarantee for any level of $\\epsilon$. It releases data-dependent parameters without accounting for their privacy cost. The server broadcasts $\\varepsilon_\\text{global}$ to all clients at each round. $\\varepsilon_\\text{global}$ is dependent on the data of clients from the previous round. Suppose an honest-but-curious client $A$ participating at rounds $t-1$ and $t$ observes a large increase in $\\varepsilon_\\text{global}$. Client $A$ could deduce that some other client $K$ at round $t-1$ likely had low-variance per-sample gradients, leading to a small noise scale $\\sigma_K^{(t)}$. Since $\\varepsilon_\\text{global}$ is determined deterministically, $A$ can distinguish with certainty between two datasets $\\mathcal{D}$ and $\\mathcal{D'}$ that are identical except that in $\\mathcal{D}$, $K$ has low variance per-sample gradients, while in $\\mathcal{D'}$, $K$ has high variance per-sample gradients. This violates the definition of DP."}, "questions": {"value": "Is anything I stated in the weaknesses section incorrect?"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)", "Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "This is very similar to another paper I have just reviewed: \"10134\tFedANC: Adaptive Sparse Noise Scheduling for Federated Differential Privacy\". It is another flawed method using a deep learning \"controller\" to adjust DP parameters during FL. Much of the text, particularly in the introduction, is copied verbatim. The flavor of the intended \"contribution\" is similar, and they suffer from the same fatal flaw (not accounting for privacy loss of passing private data through the controller and releasing the result). I'm not certain whether this counts as dual submission, but I feel that the authors are intentionally submitting a set of very similar papers with the hopes that one of them will get a favorable set of reviewers. I would advise the program committee to check any other papers submitted by these authors."}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YEZ1ZPvSbB", "forum": "Gbau7RIG2C", "replyto": "Gbau7RIG2C", "signatures": ["ICLR.cc/2026/Conference/Submission10194/Reviewer_P77G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10194/Reviewer_P77G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10194/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850621825, "cdate": 1761850621825, "tmdate": 1762921558198, "mdate": 1762921558198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a method called FedMAP, which enhances federated learning’s defense against gradient inversion and membership inference attacks. FedMAP equips each client with a fine-tuned MetaNet that predicts clipping bounds and noise scales based on gradient statistics. On the server side, a Rényi differential privacy accountant is employed to track each client’s privacy cost and compute the overall global expenditure, which is then broadcast to all clients to constrain cumulative loss and guide adaptive local updates. Empirical experiments on standard federated learning benchmarks demonstrate that FedMAP provides stronger protection against both gradient inversion and membership inference attacks compared to existing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to follow.\n\n2. The authors address both gradient inversion and membership inference attacks in federated learning, which is a challenging and important problem.\n\n3. The idea of using a neural network to predict clipping thresholds and noise scales for differential privacy mechanisms is promising.\n\n4. The authors provide convergence results to support the theoretical soundness of the proposed FedMAP method.\n\n5. Extensive experiments demonstrate the effectiveness of FedMAP against multiple attack methods. Moreover, the model accuracy achieved by FedMAP remains close to that of the non-private baseline."}, "weaknesses": {"value": "1. The paper lacks a detailed description of the defense and attack models, which is crucial for helping readers understand the setup and assumptions of the considered DP-FL system.\n\n2. The proofs of Theorems 1 and 2 are missing, preventing readers from verifying their details and correctness.\n\n3. The rationale for selecting the four specific features as inputs to the MetaNet is not well justified, and further explanation or empirical evidence would strengthen this design choice."}, "questions": {"value": "1. Regarding the fine-tuning of the MetaNet, does this process occur on the client side, performed independently by each client? Clarifying where and how this fine-tuning is conducted would help readers better understand the workflow.\n\n2. If the above is true, and a client is currently training on the CIFAR dataset, is the MetaNet fine-tuned specifically on that client’s CIFAR data, or on a mixture of datasets such as CIFAR, FMNIST, and SVHN? The explanation in lines 174–175 of the paper is unclear and should be elaborated.\n\n3. The rationale for constructing the labels of $C$ and $\\sigma$ to be proportional to empirical observations is not well justified. This label design appears ad hoc and lacks theoretical or empirical support.\n\n4. As shown in Inequality (3), the Gaussian mechanism requires the noise scale to exceed a certain lower bound to ensure differential privacy. How does the MetaNet guarantee that the predicted noise scale always satisfies this requirement? This concern is especially important given that the proposed system does not seem to employ secure aggregation. If the server is semi-honest, it may still attempt inference attacks despite added noise.\n\n5. How do the authors derive Inequality (10)? A step-by-step derivation or reference to supporting materials would improve clarity.\n\n6. What is the practical meaning or role of $q_{max}$ in the paper? Its definition and influence on the overall algorithm are not clearly explained."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T9yWw7ynjd", "forum": "Gbau7RIG2C", "replyto": "Gbau7RIG2C", "signatures": ["ICLR.cc/2026/Conference/Submission10194/Reviewer_Dj1x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10194/Reviewer_Dj1x"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10194/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936509413, "cdate": 1761936509413, "tmdate": 1762921557641, "mdate": 1762921557641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FEDMAP, a closed-loop adaptive differential privacy (DP) framework for federated learning (FL). The key idea is to dynamically predict clipping thresholds and noise scales using a lightweight MetaNet based on a frozen-layer BERT-tiny architecture. FEDMAP further introduces global privacy accounting via Rényi DP and global feedback regularization to align local DP spending with global privacy budgets. Experiments on CIFAR-10, SVHN, and Fashion-MNIST demonstrate improved privacy-utility trade-offs and stronger robustness to gradient inversion and membership inference attacks compared to DP-SGD, Soteria, and CENSOR. Theoretical convergence guarantees and DP analyses are provided, and extensive ablations show sensitivity to client participation and hyperparameters .\n\nOverall, the work is timely, well-motivated, and empirically solid. The adaptive privacy calibration idea is intuitive and practical. However, some algorithmic and training details are ambiguous, experiments on larger models/datasets are missing, and several theoretical statements lack formal proofs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Adaptive DP calibration**. The framework introduces flexible and client-specific DP noise and clipping schedules, addressing client heterogeneity in FL.\n\n2. **Meta-learning-based privacy control**. A lightweight BERT-tiny MetaNet effectively maps gradient statistics to DP parameters, demonstrating a novel use of meta-learning for privacy.\n\n3. **Global privacy loss regularization**. The feedback mechanism aligns local DP spending with global budgets and prevents over-consumption of privacy.\n\n4. **Theoretical grounding**. Convergence bounds and DP accounting provide theoretical credibility.\n\n5. **Strong empirical validation across attacks**. Experiments evaluate multiple attacks and show competitive robustness and utility against baselines."}, "weaknesses": {"value": "1. **Unclear MetaNet training and update procedure (critical).**\nIt is ambiguous whether MetaNet parameters are frozen during private training or continually updated. The algorithm suggests frozen transformer layers and trainable heads during pretraining, but does not clarify if they continue updating during FL, and how privacy and global penalties influence MetaNet outputs during training.\n\n2. **Scalability to large-scale FL is uncertain.**\nAll experiments use small vision models (ResNet-18, LeNet) and datasets. It remains unclear if the method scales to transformers or large-scale NLP tasks, where computing gradient statistics and MetaNet inference might incur overhead.\n\n3. **Missing proofs in Appendix.**\nThe main theorems are stated without formal proof details, which reduces theoretical rigor."}, "questions": {"value": "1. **Difference between $D_t$ and $VarGrad_t$**.\nIn Eq. (4), both statistics quantify gradient variability. What unique information does each contribute? Is there a redundancy?\n\n2. **Cost of computing gradient statistics.**\nFor large models, computing $\\\\|g\\\\|_2$ and covariance-based metrics could be expensive. Can the author provide a detailed comparison of the actual runtime overhead on different architectures?\n\n3. **MetaNet training and DP interaction.**\n- Are MetaNet parameters frozen during private training?\n- If frozen, how can global penalty terms meaningfully influence privacy control beyond inference?\n- If trainable, how are MetaNet updated? Do they follow the same FL training procedure as the main model?\n\nClarifying this point is crucial to judge correctness and privacy guarantees."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nzIuxsqkn0", "forum": "Gbau7RIG2C", "replyto": "Gbau7RIG2C", "signatures": ["ICLR.cc/2026/Conference/Submission10194/Reviewer_9PBe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10194/Reviewer_9PBe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10194/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971022281, "cdate": 1761971022281, "tmdate": 1762921556808, "mdate": 1762921556808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear Area Chair and Program Committee,\n\nWe strongly reject Reviewer P77G’s (Submission #10194) accusations of plagiarism, dual submission, and technical flaws. The same reviewer, identified as e2Yr in Submission #10134, appears to have reviewed both of our papers — FedANC: Adaptive Sparse Noise Scheduling for Federated Differential Privacy and FedMAP: Meta-Driven Adaptive Differential Privacy for Federated Learning — and provided nearly identical reviews. These reviews contain false allegations, factual mistakes, and reflect malicious reviewing behavior rather than an objective evaluation. We respectfully request that the Program Committee investigate this reviewer’s conduct and potential conflicts of interest.\n\n1. On the Relationship Between the Two Papers\n\nWe acknowledge that both papers are indeed authored by our team. However, they address entirely different research questions and are based on distinct problem formulations, algorithmic designs, and technical contributions. It is misleading and unprofessional to equate them based on superficial similarities in background or structure, which are common across works in this research area.\n\n| **Aspect**           | **FedANC**                                                   | **FedMAP**                                                   |\n| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| Controller type      | LSTM-based Adaptive Noise Controller (ANC)                   | Lightweight MetaNet (BERT-tiny encoder)                      |\n| Controlled variables | Noise scale $β_t$ and sparsity ratio $\\gamma_t$              | Clipping threshold $C_t$ and noise scale $\\sigma_t$          |\n| Mechanism            | Sparse Top-k noise injection on selected gradient coordinates | Full-dimensional adaptive DP-SGD                             |\n| DP accounting        | Local privacy regularization $\\hat{\\varepsilon}_t = \\frac{\\sqrt{2\\gamma_t d\\ln(1.25/\\delta)}}{\\beta_t}$ | Server-side Rényi DP accountant tracking $\\varepsilon_{\\text{global}}$ |\n| Feedback design      | Local-only adaptation                                        | Closed-loop global feedback                                  |\n| Aggregation          | Sparsity-aware aggregation                                   | Standard FedAvg                                              |\n\n\nThese differences are clear, verifiable, and explicitly described in both manuscripts. The reviewer’s claim that the two papers are \"nearly identical\" is factually false.\n\n2. On the Misunderstanding of Privacy Accounting\n\nThe reviewer claims both papers \"ignore the privacy loss of passing private data through the controller\".\nThis is incorrect and shows a fundamental misunderstanding.\n\nFedANC:\nThe ANC runs only on the client side. Inputs $(|g_t|2, \\ell_t, \\beta_{t-1}, \\gamma_{t-1})$ are local and never transmitted. Outputs $(\\beta_t, \\gamma_t)$ only determine local Gaussian noise; controller outputs are not uploaded. By the post-processing property of DP, privacy guarantees remain valid. Theorem 2 bounds cumulative privacy loss as $\\varepsilon_R = O(\\beta_{\\min}^{-1}\\sqrt{R\\gamma_{\\max}d})$ under bounded parameters.\n\nFedMAP:\nThe MetaNet outputs $(C_t, \\sigma_t)$ locally to adjust clipping and noise. The server’s Rényi DP accountant integrates these into total user-level $(\\varepsilon, \\delta)$-DP. Both analyses are mathematically sound and complete.\n\nTherefore, the claim of a \"fatal flaw\" is entirely unfounded.\n\n3. Pattern of Repeated and Biased Reviews\n\nBoth submissions received nearly identical reviews, repeating claims such as \"copied introduction text\", \"flawed controller design\", and \"recommend checking all papers by these authors\", without evidence or engagement with technical content.\nThis repetition demonstrates a pattern of bias and possibly malicious intent, rather than independent evaluation. Such behavior undermines the fairness and integrity of peer review.\n\n4. Request for Investigation\n\nGiven the seriousness of these issues, we respectfully ask the Program Committee to:\n\n- Investigate potential conflicts of interest for Reviewer e2Yr/P77G, including any connections to competing FL–DP research.\n- Audit the reviewer’s activities to verify whether identical or template reviews were submitted across our papers.\n- Remind reviewers that ethical accusations such as plagiarism or dual submission must be supported by evidence and factual basis.\n- Examine whether the reviewer or their collaborators have submissions overlapping with ours, which could create an incentive for biased reviewing to improve their own acceptance chances.\n\nWe believe that scientific evaluation must be based on technical merit and factual accuracy, not speculation or personal bias.\nWe trust the Program Committee will uphold fairness, transparency, and academic integrity in handling this matter.\n\nSincerely,\n\n10194 Authors"}}, "id": "X4HG2XtFSd", "forum": "Gbau7RIG2C", "replyto": "Gbau7RIG2C", "signatures": ["ICLR.cc/2026/Conference/Submission10194/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10194/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10194/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762954013143, "cdate": 1762954013143, "tmdate": 1762960094897, "mdate": 1762960094897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for their time and effort. The comments help us understand the weaknesses of our paper, and we will revise the work with care. We also understand that this paper is unlikely to be accepted, and we respect the review process.\n\nWe would like to express one serious concern. One review includes an ethics accusation that targets the personal integrity of the authors. This accusation is not supported by evidence in the submission and goes beyond a fair evaluation of the scientific content. As researchers, we cannot accept a statement that harms our dignity.\n\nWe support a fair and transparent review process. We welcome the Area Chair to examine all submissions from our team. We are confident that such an examination will confirm that our work follows normal academic practice. We again thank all reviewers for their comments and will continue to improve our research."}}, "id": "3HOkMEotEa", "forum": "Gbau7RIG2C", "replyto": "Gbau7RIG2C", "signatures": ["ICLR.cc/2026/Conference/Submission10194/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10194/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission10194/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762992356606, "cdate": 1762992356606, "tmdate": 1762992356606, "mdate": 1762992356606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}