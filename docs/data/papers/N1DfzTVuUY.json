{"id": "N1DfzTVuUY", "number": 1838, "cdate": 1756950996969, "mdate": 1763565783368, "content": {"title": "Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer", "abstract": "Text-guided color editing in images and videos is a fundamental yet unsolved problem, requiring fine-grained manipulation of color attributes, including albedo, light source color, and ambient lighting, while preserving physical consistency in geometry, material properties, and light-matter interactions. Existing training-free approaches provide broad applicability across editing tasks but struggle with precise color control and often introduce visual inconsistency in both edited and non-edited regions. In this work, we present ColorCtrl, a training-free color editing method that leverages the attention mechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By disentangling structure and color through targeted manipulation of attention maps and value tokens, our method enables accurate and consistent color editing, along with word-level control of attribute intensity. Our method modifies only the intended regions specified by the prompt, leaving unrelated areas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate that ColorCtrl outperforms existing training-free approaches and achieves state-of-the-art performances in both edit quality and consistency. Furthermore, our method surpasses strong commercial models such as FLUX.1 Kontext Max and GPT-4o Image Generation in terms of consistency. When extended to video models like CogVideoX, our approach exhibits greater advantages, particularly in maintaining temporal coherence and editing stability. Finally, our method generalizes to instruction-based editing diffusion models such as Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility. Code will be released.", "tldr": "We introduce ColorCtrl, a training-free method for text-guided color editing in images and videos. It enables precise, word-level control of color attributes while preserving geometry and material consistency.", "keywords": ["Diffusion", "DiT", "Image Editing", "Video Editing", "Color Editing"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8c3d99bc548b0ee26c82a1921ec80cadb61f2bcf.pdf", "supplementary_material": "/attachment/f134d9560ab501a85d952de8d35896f9898550a8.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents ColorCtrl, a training-free, text-guided color editing method for images and videos built on Multi-Modal Diffusion Transformers (MM-DiT). The core idea is to disentangle “what should stay” (structure, materials, lighting geometry) from “what should change” (albedo or light color) by (i) swapping the vision-to-vision quadrant of the attention map from a source branch to a target branch to preserve geometry and view, (ii) extracting an edit mask from vision-to-text attention and copying vision value tokens outside the mask to preserve non-edited colors, and (iii) enabling word-level attribute intensity control by pre-softmax scaling in the text-to-vision attention region. Extensive comparisons on SD3 and FLUX.1-dev, plus CogVideoX for video, indicate state-of-the-art training-free performance on PIE-Bench and a new ColorCtrl-Bench; the method reportedly surpasses strong commercial systems (FLUX.1 Kontext Max, GPT-4o Image Generation) on consistency while remaining model-agnostic and compatible with instruction-based editors (Step1X-Edit, FLUX.1 Kontext dev)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Originality: The paper adapts attention-control editing to MM-DiT with a clear decomposition of attention quadrants: vision-to-vision for structure preservation, vision-to-text for mask extraction, and text-to-vision for controllable attribute strength. This differs from U-Net cross-attention methods and prior MM-DiT controls (e.g., DiTCtrl) by operating directly on attention maps and value-token routing without training.   \n\n* Quality: The mechanism is well specified: two-branch unrolling with cached source features; mask from vision-to-text attention; copying only vision value tokens for non-edited areas; and pre-softmax token-specific scaling for attribute re-weighting. Ablations show that structure preservation and color preservation cumulatively improve consistency metrics with minimal loss in CLIP alignment.   \n\n* Clarity: The paper provides a task formulation grounded in a rendering-style decomposition (G, L, A, S, C), detailed pipeline figures, and explicit implementation/inference settings (steps, CFG, mask threshold, fixed seeds). These details aid reproducibility."}, "weaknesses": {"value": "* Masking and subject detection reliance: The evaluation and parts of the pipeline hinge on subject keywords and a fixed attention-threshold ($\\epsilon=0.1$) for mask extraction; robustness to threshold choice, ambiguous subject words, or multi-object scenes is not deeply analyzed.  \n\n* Claims versus limitations: The paper acknowledges failures when the base model mislocalizes targets or confuses attributes (e.g., trees or lipstick casing). More systematic characterization of such failure modes—especially under crowded scenes, glossy materials, or colored lighting—would be valuable."}, "questions": {"value": "* How sensitive is performance to the mask threshold ϵ and to the choice of the “blended word” subject token? Please provide a small sensitivity analysis (e.g., ±0.05 around 0.1) and results for ambiguous subjects or multi-instance scenes. \n\n* The benchmark uses noise-to-image generation to isolate editing. Can you complement this with a real-image inversion benchmark that reports reconstruction error and edit success jointly, to reflect common editing workflows?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RKvMm0AW3P", "forum": "N1DfzTVuUY", "replyto": "N1DfzTVuUY", "signatures": ["ICLR.cc/2026/Conference/Submission1838/Reviewer_45WZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1838/Reviewer_45WZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760933401883, "cdate": 1760933401883, "tmdate": 1762915904681, "mdate": 1762915904681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response to reviewers"}, "comment": {"value": "We thank all reviewers for their valuable and insightful feedback. We are encouraged that the reviewers found our work to be **novel and original** (Reviewer QUHF, 45WZ),  recognized that our **state-of-the-art** results show high consistency and localized edits, outperforming several baselines and even strong commercial models (Reviewer Ld1H, QUHF, 45WZ), and acknowledged the **impact** on pushing the boundaries of real-world applications (Reviewer QUHF) as well as the **detailed task formulation** grounded in a rendering-style decomposition (Reviewer 45WZ).\n\nOur work has benefited substantially from your feedback. Below we summarize the main modifications to the manuscript (highlighted in blue in the revised PDF):\n1. **Additional evaluation metrics.** In response to Reviewer Ld1H’s comments, we added two structure-focused metrics, Y-Gradient Cosine Similarity (Y-GCS) and Y-Gradient Magnitude Similarity (Y-GMS), to more finely measure detail and structural preservation under color edits. We also incorporated Semantic Consistency (SC) and Perceptual Quality (PQ) from GEdit-Bench to better evaluate the success of the edits and the naturalness of the results. We evaluated all baselines, including both training-free and commercial models, on image and video editing tasks on PIE-Bench with SD3, FLUX.1-dev, and CogVideoX-2B. These results further demonstrate that our method produces more effective, more natural, and more structurally consistent edits than that of competing approaches (Appendix B.5).\n2. **Additional benchmark (real-image editing).** Building on the suggestions of Reviewer Ld1H and 45WZ, in addition to the existing noise-to-image setting, we added a real-image editing benchmark on PIE-Bench (Appendix B.6). We compare against the latest MM-DiT-based method FlowEdit (ICCV 2025 best student paper) and the classic U-Net–based method PnP-Inversion. Besides the standard evaluation metrics, we also report the newly added metrics above. The results show that our method remains highly effective in the real-image editing setting as well.\n3. **Additional qualitative results.** Following the suggestions of Reviewer Ld1H and 45WZ, we added more examples of multi-region, multi-object editing in Appendix B.7. In addition, we showcase other editing capabilities that require strong structural preservation beyond color changes, including style transfer, material changes, texture changes, and transparency changes (Appendix B.9).\n4. **Ablation study.** Incorporating feedback from Reviewer Ld1H, QUHF, and 45WZ, we conducted an ablation study on the mask threshold $\\epsilon$ in Appendix B.8. The results show that our method is not sensitive to this hyperparameter: even disable mask extraction and color preservation ($\\epsilon = 0$), the structural preservation remains highly strong.\n\nWe hope these revisions and clarifications address the main concerns raised in the reviews, and we are happy to further discuss any remaining questions. If you find our response satisfactory, we kindly ask you to consider raising your scores in recognition of our core contributions: a training-free, MM-DiT–based attention-control method that substantially advances the challenging task of color editing (for both images and videos) while strongly preserving geometry and structural details across backbones and settings."}}, "id": "b1kojxuDTe", "forum": "N1DfzTVuUY", "replyto": "N1DfzTVuUY", "signatures": ["ICLR.cc/2026/Conference/Submission1838/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1838/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1838/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763564498922, "cdate": 1763564498922, "tmdate": 1763564498922, "mdate": 1763564498922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ColorCtrl, a training-free framework for text-guided color editing using Multi-Modal Diffusion Transformers (MM-DiT). The method leverages attention maps from MM-DiT to extract semantic masks and reweight attention for localized color edits. The authors claim improved controllability and consistency without requiring model fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality: The idea of using MM-DiT attention maps for color-specific editing is novel in the context of training-free pipelines.The integration of word-level control adds granularity not commonly seen in prior works.\n2. Quality: The method is well-implemented and evaluated across multiple domains (images, videos, instructions). Results show high semantic consistency and localized edits, outperforming several baselines.\n3. Clarity: The paper is generally well-written, with clear motivation and structured methodology. Visual examples and comparisons are effective in illustrating the method’s capabilities.\n4. Significance: The approach is impactful for real-world applications where retraining is costly or infeasible. It contributes to the growing field of training-free multimodal editing, pushing the boundaries of what can be achieved with pre-trained models."}, "weaknesses": {"value": "1. Limited Novelty: While the use of MM-DiT is novel for color editing, similar pipelines have applied attention-based editing in other contexts. TextCrafter also leverages attention maps from MM-DiT to extract semantic masks and reweight attention for image editing. Add-It also leverages attention maps from MM-DiT to extract semantic masks. \n2. Insufficient Analysis of Key Components: \n(1) The mask extraction process is underexplained: How are attention maps selected? What thresholding strategy is used? How robust is the mask across different prompts?\n(2) The attention reweighting mechanism lacks detailed discussion: How is the reweighting computed?\n(3) A more thorough ablation study would help isolate the contribution of each component (e.g., mask quality, reweighting strategy)."}, "questions": {"value": "1. Could you elaborate on how the semantic mask is extracted from MM-DiT attention maps? Which layers are used? What criteria or thresholds are used to binarize or localize the mask?\n2. How is the attention reweighting applied during inference? Is it uniform across all heads/layers or selectively applied?\n3. How sensitive is the editing quality to the mask threshold and prompt?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "io5mSWJtQ4", "forum": "N1DfzTVuUY", "replyto": "N1DfzTVuUY", "signatures": ["ICLR.cc/2026/Conference/Submission1838/Reviewer_QUHF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1838/Reviewer_QUHF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907417918, "cdate": 1761907417918, "tmdate": 1762915904471, "mdate": 1762915904471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free color editing method for images and videos based on pretrained T2I/T2V MMDiT models. It disentangles the color and structure within the edited region, and achieves accurate and natural color editing without touching unrelated regions or attributes. The proposed method is evaluated on extensive examples with various base models, surpassing strong commercial models. A new benchmarking protocol is proposed by adopting the PIE-bench prompts on generated images and adjusted metrics. The proposed method also applies to videos and instructional image editing models seamlessly."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is dedicatedly designed for color editing and exclude other factors, achieving clean color editing results with faithfully maintained content.\n\n- The proposed method and benchmark highlights the importance of producing reasonable color edit with similar lighting etc. environmental conditions, over the traditional standard semantic CLIP alignment, making the output results more realistic and natural."}, "weaknesses": {"value": "- [Major] The only editing quality metric is still CLIP similarity, which doesn't align with the claim that CLIP usually leads to over saturated preference as it lacks details. The major upgrade of metrics only focus on the structure preservation. Given the focus of the paper, some improved editing metrics are necessary to solidate the evaluation. For example, at least aesthetics/harmony can be tested to show the edited colors fit in the environment well. Maybe other color spaces like HSV could also help to decompose? Texture preservation can also be considered (e.g. semantic not impacted), while Canny is relatively rough especially when the threshold is not low enough. For CLIP similarity, it could also potentially help if it is calculated separately between color words and object words etc.\n\n- [Major] The proposed method is tested mainly on generated images instead of real images, and the proposed benchmark also emphasizes this. Although a dedicated section is provided for application on real images, the major quantitative results are compared on synthetic data. It is claimed that testing on generated images can calibrate the impact of inversion quality etc., but in practice it's still important to evaluate on real data, in terms of both performance and methodology. The performance of all methods will be impacted by inversion, while some methods might cooperate better or worse with inversion, and the difference matters for real usage.\n\n- UNet-based models sometimes feature their tighter spatial correspondence over transformers. One or two strong baselines could be involved."}, "questions": {"value": "- For the failure cases described in Sec. B.5, would there be additional approach to improve? For example, would it be feasible to locate the editing region with the full object in the source image caption, i.e. use \"red trees\" or \"red lipstick\" instead of \"trees\" or \"lipstick\" to locate the desired region?\n\n- Would the proposed method be applied for other attribute editing like color that are uniform and doesn't change structures, e.g. textures, reflection/transparency? If so the scope of this paper could be largely extended."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mC4PxyZos4", "forum": "N1DfzTVuUY", "replyto": "N1DfzTVuUY", "signatures": ["ICLR.cc/2026/Conference/Submission1838/Reviewer_Ld1H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1838/Reviewer_Ld1H"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971491970, "cdate": 1761971491970, "tmdate": 1762915904287, "mdate": 1762915904287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}