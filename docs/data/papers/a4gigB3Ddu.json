{"id": "a4gigB3Ddu", "number": 295, "cdate": 1756734210030, "mdate": 1763608185842, "content": {"title": "Meta-Researcher: Empowering Planning and Reflection Mechanisms in Large Reasoning Models for Advanced Deep Research Abilities", "abstract": "Deep research significantly reduces the time and cost of information gathering for researchers by collecting and integrating vast amounts of data. However, its uncontrollable planning and reflection phases during reasoning lead to errors or gaps in information collection, and make it challenging to ensure timely reflection for correcting and supplementing information—thereby performing suboptimally in complex tasks requiring extensive data gathering. To address this limitation, we propose Meta-Researcher, an End-to-End Reinforcement Learning-based Deep Research Method designed to equip Large reasoning models (LRMs) and non-reasoning models with metacognitive capabilities for autonomously executing the research process of \"Task Planning - Information Gathering - Process Reflection - Problem Solving'', thereby effectively tackling complex problems that require multiple rounds of information collection and reasoning. Firstly, our approach standardizes LRMs to explicitly output controllable planning and reflection processes rather than implicitly including them within reasoning, thus ensuring that LRMs demonstrate metacognitive abilities in practice. Secondly, we perform end-to-end optimization through the Group Relative Policy Optimization (GRPO) strategy to enhance the active decision-making capabilities of LRMs while strengthening the metacognitive process. Extensive experiments on two tasks — closed-ended question answering and open-ended topic research — demonstrate that Meta-Researcher significantly outperforms existing deep search methods, deep research methods, and proprietary systems. Our approach enhances the reliability and applicability of LRMs in complex task scenarios, offering a new paradigm for developing intelligent agents with autonomous research capabilities.", "tldr": "We propose Meta-Researcher, an end-to-end reinforcement learning method that turns uncontrollable task planning and reflection processes of LRMs into controllable ones, so as to effectively enhance decision-making and metacognitive abilities of LRMs.", "keywords": ["Deep Research", "Large Reasoning Models", "Metacognitive Capabilities"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/89a475c9b6bd00ace289a4dfddce524a9ccfc41c.pdf", "supplementary_material": "/attachment/62b93a1b2c20d69cc46d315af63d1ea53ef493da.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Meta-Researcher, an end-to-end reinforcement learning framework that empowers Large Reasoning Models (LRMs) with explicit metacognitive capabilities for deep research tasks. The key innovation is making the planning and reflection processes controllable by explicitly outputting them rather than keeping them implicit within reasoning. The framework consists of four components: task planning, tool calling, process reflection, and question answering, optimized using Group Relative Policy Optimization (GRPO) with carefully designed rewards. Experiments on closed-ended QA (GPQA, GAIA, Bamboogle, HLE) and open-ended research (Glaive) demonstrate improvements over existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tClear framework design: The four-component architecture is intuitive and the use of \"virtual tools\" (Task Planning Tool and Process Reflection Tool) to ensure explicit outputs is a good design choice.\n2.\tComprehensive experimental evaluation: The paper evaluates on multiple diverse benchmarks covering both closed-ended and open-ended scenarios, with detailed ablation studies demonstrating the value of each component.\n3.\tPractical applicability: Experiments on different model scales (7B, 14B, 32B) and non-reasoning models show the framework's broad applicability."}, "weaknesses": {"value": "1.\tLimited algorithmic novelty: The paper essentially combines existing techniques (task decomposition, web search, reflection, GRPO) without introducing fundamentally new methods. The main contribution is engineering-focused rather than conceptually novel. It's primarily about formatting outputs explicitly and reward engineering.\n2.\tWeak theoretical foundation: (1) The proposed \"metacognition\" definition (Section 3) lacks rigorous justification and theoretical grounding; (2) No formal analysis of why explicit output of planning/reflection is superior to implicit reasoning; (3) The connection between the framework design and actual metacognitive capabilities is assumed rather than demonstrated.\n3.\tMissing critical information: (1) No computational cost analysis for training time, inference latency, API costs for web searches not reported; (2) How does the multi-round search and reflection compare to simpler approaches in terms of cost-effectiveness? (3) Limited scalability analysis: What happens with longer contexts or more complex problems?\n4.\tAll experiments use the same web search API and similar domains (academic/factual questions).\n5.\tIt’s unclear how the method performs with different information sources, noisy/contradictory information, or specialized domains.\n6.\tSome design may have problems. For example, the thinking length penalty (Equation 10) may hurt performance on genuinely complex problems."}, "questions": {"value": "1.\tHyperparameter sensitivity: How sensitive is the method to the specific reward weights and thresholds? Was there extensive hyperparameter search, and how would practitioners set these for new tasks?\n2.\tAblation on RL: What happens if you use the same explicit format with supervised fine-tuning only, without RL? Is RL necessary or is it primarily the format that helps?\n3.\tInformation loss: How much information is lost during context truncation, and how does this affect final performance? Can you quantify this?\n4.\tReflection mechanism: Why is it necessary to explicitly call a \"reflection tool\" rather than just prompting the model to reflect? What does the tool-calling format add?\n5.\tFailure modes: Can you provide systematic analysis of when the method fails? What types of questions or scenarios are still challenging?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IZXHW9D2ul", "forum": "a4gigB3Ddu", "replyto": "a4gigB3Ddu", "signatures": ["ICLR.cc/2026/Conference/Submission295/Reviewer_N2r2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission295/Reviewer_N2r2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761408004876, "cdate": 1761408004876, "tmdate": 1762915487788, "mdate": 1762915487788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Meta-Researcher, a reinforcement learning (RL) framework designed to endow large reasoning models (LRMs) with explicit metacognitive abilities for autonomous research. Instead of relying on implicit reasoning within the chain-of-thought, the framework enforces a structured “Task Planning → Information Gathering → Process Reflection → Problem Solving” loop. Meta-Researcher implements two virtual tool: a task-planning tool and a process-reflection tool;  to externalize these cognitive phases, allowing the model’s internal reasoning process to become observable, rewardable, and trainable. Training is performed using Group Relative Policy Optimization (GRPO) with layered rewards (format, accuracy, and thinking-length)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Making planning and reflection processes explicit and controllable through virtual tools is a sensible design choice.\n2. The paper evaluates on diverse tasks spanning closed-ended QA and open-ended research, with thorough comparisons against both RAG and autonomous search methods.\n3. The progression from format rewards to combined rewards addresses the challenge of lacking intermediate supervision in a principled way."}, "weaknesses": {"value": "1. The open-ended experiments rely on a small (30-sample) Glaive subset and LLM-as-judge scoring, limiting robustness.\n2. We don’t see qualitative cases of when reflection fails  e.g., reflection triggers but still misses key evidence, or over-reflects and gets penalized."}, "questions": {"value": "1. How were the specific reward weights (δ=1e-4, η=2048, ξ=1.0) chosen? How sensitive is performance to these choices?\n2. Do plan/reflect stages measurably improve evidence attribution (e.g., source coverage, citation precision/recall, redundancy reduction)?\nWhen does reflection hurt (over-reflection, topic drift)?\n3. if the planning tool is not called, the overall format reward becomes 0. This is a hard gate. Did the authors observe training instability or low sample efficiency in early stages due to this?\n4. The method encourages multiple tool calls and multiple reflections. What is the average tool budget per example compared to WebThinker, RAgent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V8SCA05h4Y", "forum": "a4gigB3Ddu", "replyto": "a4gigB3Ddu", "signatures": ["ICLR.cc/2026/Conference/Submission295/Reviewer_FGSG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission295/Reviewer_FGSG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964599077, "cdate": 1761964599077, "tmdate": 1762915487505, "mdate": 1762915487505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Deep research is a very useful tool. However, it is often difficult to control, and reasoning errors lead to gaps in the collected information. Therefore, they propose Meta-Researcher, an end-to-end RL-based Deep Research method that supports task planning, information gathering, process reflection, and, finally, problem-solving. Their structured approach makes deep research more controllable and allows RL to enhance decision-making abilities. Their experiments show that their method outperforms existing deep research recipes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors address one of the currently most interesting applications of LLMs, deep research. While most deep research recipes are proprietary, they make their recipe publicly available. \n2. The authors aim to decompose the complex deep research pipeline into  4 key components (task planning, tools calling, process reflection, question answering) that operate in two modes (closed-ended question answering and open-ended topic research, which are sensible. \n3. Designed a comprehensive list of reward components, which generally seem well motivated to enable effective RL fine-tuning.\n4. The benchmarks across closed-ended question answering and open-ended research are well selected.\n5. The authors provided the prompts used in every part of the pipeline in the Appendix, which can be useful for future work."}, "weaknesses": {"value": "1. Some important definitions (e.g., BGE model) or citations (GRPO) are missing \n2. Generally, there is a lack of ablation studies. In particular, no sensitivity analysis of the individual reward components. Therefore, it may be that the reward function is overly complex. Can you verify that all components are necessary? Also, the two-stage training is not ablated. Is it necessary?\n3. It would be valuable to see an analysis of how many steps/turns the deep research performs. Especially, how does the RL fine-tuning change this behavior? For example, does RL considerably increase the number of tool calls or result in generally longer interactions than not using RL? \n4. The outcome rewards for closed-ended question answering are clear. However, there is no clarity on how the outcome of open-ended research is graded. Is LLM as a judge used on the listed dimensions? How does performance vary if you switch the judge? How reliable are the judged scores even? An analysis of this would help the paper\n5. The paper focuses on Qwen 2.5-72B-Instruct. It is unclear if the findings transfer to other models. It would be essential to verify their findings with more recent reasoning models, such as Qwen3 (even if at a smaller scale), to ensure that all components and reward mechanisms are necessary. Otherwise, it is possible that the found recipe is specific to Qwen 2.5-72B.\n6. It would be helpful to the reader to provide more insights into the end-to-end RL component, such as learning curves and other important metrics, which are currently missing. This also includes the computational efforts, technical challenges, etc, that come with end-to-end RL training in a difficult multi-turn problem. At the moment, it seems like RL is more used as a black-box mechanism. \n7. Additionally, there is no comparison of wall-clock times of the compared methods in Table 1 (e.g., between direct reasoning, enhanced reasoning, and autonomous search). This would be important to see for users, to understand the tradeoffs of different approaches better."}, "questions": {"value": "Some questions are asked above. Here are some more: \n\n1. The “BGE model” is listed, but not cited or explained. Can you clarify why, what the abbreviation stands for, and how it works?\n2. What is $R_{temp}^{tool}$? This is not specified. \n4. Can you provide ablations on the importance of the reward components and two-stage training? \n5. Can you provide a quantitative analysis of how RL changes the agent's behavior (e.g., tool call frequency, etc.)?   \n5. In Appendix B.1.2, the authors state that OpenResearchBench is constructed by themselves. Is this dataset going to be released?\n6. How does the proposed system compare to proprietary deep research recipes from frontier labs? It is not necessary to beat these recipes, but for the reader, it would be important to see how these methods compare. If it is not feasible to conduct a full evaluation on all downstream tasks, an analysis on a restricted task set would also help."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sij2riFmRa", "forum": "a4gigB3Ddu", "replyto": "a4gigB3Ddu", "signatures": ["ICLR.cc/2026/Conference/Submission295/Reviewer_1j81"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission295/Reviewer_1j81"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981020217, "cdate": 1761981020217, "tmdate": 1762915487327, "mdate": 1762915487327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes a system, a prompting strategy and tool-calling harness to build a deep research system. The authors emphasize that the system uses planning, tool-calling, using search to gather information and reflection as crucial components. Secondly, the authors use end-to-end RL with GRPO to optimize the overall system."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Building a useful system to solve challenging tasks such as “deep research” on top of LLM remains a challenge and deserves careful attention from the research community. Tuning these systems end-to-end with RL is a promising direction and public research can have high impact."}, "weaknesses": {"value": "My biggest concern is around the training/evaluation methodology for the results involving RL: It seems there is no distinction between training and test tasks and I am concerned that RL optimization might have been performed directly on the test problems? \n\nFurthermore, there are very few ablations describing the impact of individual design choices of the overall system. For example: What is the impact of the two training stages described in 4.4.4? I.e. which gains come from training the model to adhere to the correct formatting vs actually improving the models capabilities? \n\n\n\nMinor comment: \n\nThe paper frequently employs excessive adjectives and overly complex terminology. While individual instances might be justifiable, the overall impression is one of pretentiousness. For example, terms like \"virtual tools,\" \"autonomous reflection,\" \"autonomous search,\" and \"multi-tool collaborative calling mechanism\" effectively describe one LLM initiating sub-LLM calls with predefined prompts.\n\nI also don’t understand why running RL with reward only for adhering to the toll-calling format is “genuinely instill(ing) metacognitive capabilities for autonomous reflection” (line 310)."}, "questions": {"value": "Do you use distinct training/test tasks for RL? Do you train one model across all benchmark sets and how to you mix the differently sized datasets?\n\nWhat exactly is the difference between “closed ended” and “open ended” mode? I infer that open-ended tasks use a LLM-as-judge to score the result. Figure 1 suggests open-ended is using additional tools and more reflection iterations? Lines 190 to 205 are not clearly expressing the difference. “integrating external information with the reasoning capabilities” seems to be necessary for both.   \n\nWhen the meta-researcher triggers a call to the task-planning or process-reflection tools, do the sub-agents receive more than the explicitly passed arguments? (the JSON tool-call syntax “{name: .. arguments: }” suggest arguments and intermediate results are passed explicitly only). The example on page 22 however suggests that the reflection tool is “inline” and has full access to the meta-researchers token stream?\n\nHow are the planning and reflection tools sub-trajectories handled for the RL updates? Are they doing their own GRPO update steps? What exactly comprises a group then? \n\nB.2 Implementation details mentions $s_p = 6$ for the reward calculation in the “Process Reflection Phase” What is this reward and how is it used? Just returned as a number to the meta-agent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PJGcpGOPPW", "forum": "a4gigB3Ddu", "replyto": "a4gigB3Ddu", "signatures": ["ICLR.cc/2026/Conference/Submission295/Reviewer_uoUv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission295/Reviewer_uoUv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000218079, "cdate": 1762000218079, "tmdate": 1762915487209, "mdate": 1762915487209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}