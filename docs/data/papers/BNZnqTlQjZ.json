{"id": "BNZnqTlQjZ", "number": 9109, "cdate": 1758111539784, "mdate": 1762997698811, "content": {"title": "Input Dimension Expandable Network: Integrating New Input Dimensions in Online Learning", "abstract": "Sensory augmentation experiments have demonstrated that the perceptual dimensions of the mammalian nervous system are expandable at different levels. This capacity enables mammals to acquire signals beyond the range of their inherent sensory systems and subsequently learn to utilize such signals. A critical question arises: how to enable a learning system to expand its input dimensions in an online manner? To address this challenge, we propose a hierarchical modular neural network architecture that supports multi-level and multi-regional expansion of input dimensions, along with a dimension integration algorithm designed to guide new dimensions to proper neuron circuits during online learning. To validate our computational model, we design a series of dimension expansion experiments at different levels. The experimental results confirm that our method effectively handles the input dimension expandable learning problem.", "tldr": "", "keywords": ["Input dimension expandable learning", "bio-inspired learning", "online learning", "multimodal learning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/c68595a1343f693b7e1fb5284b23b0f55cec3561.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Input Dimension Expandable Network (IDEN), a novel hierarchical and modular neural network designed to handle the expansion of input dimensions during online learning without requiring complete retraining. Inspired by the perceptual plasticity of the mammalian brain, the proposed architecture can integrate new information at multiple levels: adding new features to an existing input type, adding new feature types to a modality, or adding entirely new modalities. The authors propose a dimension integration algorithm that guides how new inputs are incorporated into the existing network structure. The method's effectiveness is demonstrated through a series of experiments on recombiniations of a multi-modal dataset, showing superior performance over existing methods in tasks involving channel expansion, online classification, and mixed-modality expansion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This work seems to present some novel ideas towards continual learning, enabling expansion of input dimensionalities and modalities. In the experimentational results presented, it outperforms existing related methods."}, "weaknesses": {"value": "Although three datasets are presented using combinations of data from two sources, they overlap in their samples and features, and the \"tasks\" seem to only be classification. This fails to demonstrate the method's generalizability and robustness.\n\nThe results tables are presented without defining the metric being measured (some sort of accuracy?), nor clearly specifying the task being performed and whether these results were run on holdout test splits of the dataset. These errors make the empirical results difficult to properly interpret or trust. Evaluating the models on holdout data is crucial to show the generalization abilities of the implemented methods: my understanding of your methods is like a nearest-neighbor lookup, but such methods are prone to overfitting on training data without generalizing to unseen examples.\n\nMinor edits:\n* The citation entitled \"Learning with feature evolvable stream\" is repeated twice with different publication years.\n* Use `\\citep` whenever the citation is not directly part of the sentence structure, for example \"...through brain computer interface (Thomson et al., 2017) and genetic engineering (Zhang et al., 2017)...\"."}, "questions": {"value": "How does your proposed model actually \"learn\" the weights outside of the architectural growth? Or are these generated randomly? In this case, how does the model control the computational complexity of expansion?\n\nWhat are the implementational details of your methods? In the current state of the paper, understanding the computational scale, let alone trying to use these methods and/or replicate the results, is difficult without many more details, such as initial and final architectural dimensions as well as dataset sizes. Only a few paremeter settings are given in the beginning of Section 4.\n\nIn the related works section, you state that other works are \"unable to perform classification using data from the original dimensions after new dimensions have been introduced\", but where are the results showing that your method can do so? Your results tables seem to only show some kind of overall accuracy, not splitting out (for example) performance on each partition of classes in Table 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j7EU265rIj", "forum": "BNZnqTlQjZ", "replyto": "BNZnqTlQjZ", "signatures": ["ICLR.cc/2026/Conference/Submission9109/Reviewer_Dm66"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9109/Reviewer_Dm66"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9109/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761563929455, "cdate": 1761563929455, "tmdate": 1762920807784, "mdate": 1762920807784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "VytmkebAkf", "forum": "BNZnqTlQjZ", "replyto": "BNZnqTlQjZ", "signatures": ["ICLR.cc/2026/Conference/Submission9109/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9109/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762997698076, "cdate": 1762997698076, "tmdate": 1762997698076, "mdate": 1762997698076, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a hierarchical modular neural network capable of expanding input dimensions in an online manner.  It is a multi-channel structure, where each channel consists of several feature areas and a unimodal association layer. Then all channels are  integrated using a multimodal association layer. An online integration algorithm is designed to bind new input dimensions with existing representations dynamically.  Corresponding experiments are designed  to realize and verify the dimension expandable learning paradigm."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper addresses a fundamental yet underexplored problem: how to enable a learning system to expand its input dimensions in an\nonline manner? The connection to biological inspiration is appealing.\n2.The proposed mechanism has strong potential in real-world applications such as adaptive robots or lifelong learning systems where new modalities or sensors are continuously introduced.\n3.The dimension expansion experiments demonstrate that the proposed framework can integrate new dimensions in an online manner, which is an important step toward adaptive systems."}, "weaknesses": {"value": "1.The paper spends substantial space describing the model architecture and algorithmic procedures, yet provides limited theoretical analysis and lacks clear explanation for the underlying design motivations. \n2.The “brain-inspired” aspect is mentioned but not rigorously analyzed. It would be stronger if specific neuroscientific parallels were empirically or theoretically grounded.\n3.The experiments are limited and do not include ablation studies. As a result, the empirical section does not convincingly demonstrate the effectiveness of each proposed component or justify the necessity of the hierarchical modular design.\n4.Figure 2 annotation errors and unclear labeling."}, "questions": {"value": "1.How is representational stability guaranteed when integrating new dimensions?\n2.What is the computational overhead of frequent dimension expansions?\n3.Please correct and unify the annotations in Figure 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VYliEVVRv0", "forum": "BNZnqTlQjZ", "replyto": "BNZnqTlQjZ", "signatures": ["ICLR.cc/2026/Conference/Submission9109/Reviewer_WnAf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9109/Reviewer_WnAf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9109/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810515696, "cdate": 1761810515696, "tmdate": 1762920807383, "mdate": 1762920807383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed Input Dimension Expandable Network (IDEN( is a hierarchical, modular neural architecture expanding its input space online at three levels: (i) adding new dimensions to existing features, (ii) adding new feature types within a modality (unimodal association), and (iii) adding entirely new modalities (multimodal association). A dimension integration algorithm routes and binds new inputs to existing circuits via ascending/descending pathways, with frequency-tagged signaling and Fourier-based aggregation at the multimodal layer. Experiments on small multimodal datasets evaluate channel expansion, open-environment learning, and a mix expansion setting. Results on limited datasets outperform selected baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "IDEN is a brain-inspired modular hierarchy model mimicing the organization of the mammalian sensory system and supporting online “plug-and-play”. It supports feature-level, unimodal, and multimodal expansion, a novel innovation not seen in many prior works.\n\nIt tackles open feature space learning with multi-level and multi-region expansion in a single unified framework. It addresses limitations in existing models by providing a unified framework for three distinct types of input expansion: adding new features to an existing input, adding new feature types within a modality, and adding entirely new modalities.\n\nThe dimension integration algorithm  how existing circuits absorb new inputs without full retraining, supporting both upward (perception) and downward (feedback/recall) flows. This allows adaptation to new input types during operation, enabling continual growth."}, "weaknesses": {"value": "The framework lacks theoretical guarantees with no analysis or proof of convergence, stability during dimension growth, prevention of catastrophic forgetting.\n\nThe datasets tested on are small and synthetic, with simple sensory modalities, so It’s not clear whether the model would scale to real-world multimodal or high-dimensional data \n\nThe paper overlooks computational cost and scaling results that rae very crucial for an online model. The experiments demonstrate performance but do not analyse how internal representations evolve when new dimensions are added. \n\nWhile inspired by neuroscience, the connection is more metaphorical than mechanistic. It has heavy mathematical notations (cosine activations, Fourier encoding, Gaussian likelihood gates) making the method hard to reimplement. The maths is dense and symbol-heavy, with minimal intuitive explanation.\n\nThe paper has long paragraphs, repetitive sections, and also lacks pseudocode. They also lacks visualisation and  quantitative metrics for representational consistency."}, "questions": {"value": "How does the dimension integration algorithm handle noisy or contradictory information from a new input source? For example, if a newly added sensor provides data that conflicts with well-established patterns from existing sensors, is there a mechanism to resolve this conflict or weight the more reliable source?\n\nThe experiments are focused on classification and cross-modal recall tasks. How do you envision the IDEN architecture being applied to other machine learning paradigms, such as reinforcement learning, where an agent would need to learn to associate new sensory inputs with optimal actions and reward signals?\n\nCan the authors quantify computational overhead as the network expands?\n\nCan the authors provide a visualisation and  pseudocode describing the algorithm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HfMv8eBRPO", "forum": "BNZnqTlQjZ", "replyto": "BNZnqTlQjZ", "signatures": ["ICLR.cc/2026/Conference/Submission9109/Reviewer_e1Nj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9109/Reviewer_e1Nj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9109/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953726571, "cdate": 1761953726571, "tmdate": 1762920807039, "mdate": 1762920807039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This article describes a neural network architecture meant to support online learning of new dimension in its input. For this, a dedicated neural network architecture"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an interesting problem with an unusual neural network architecture, namely, the addition and online adaptation to  new input dimensions. The experiments support that the claims of the paper using a dataset that seems to be constructed from multiple high-dimensional modalities (although these are preprocessed further)."}, "weaknesses": {"value": "- The motivation for input expansion is quite weak: enhancing animals' perception seems to work via existing cells, although they react to new stimuli\n- the paper is not very clearly written. Especially the description of the proposed network is quite unclear. Is this a recurrent network (since there is a \"period time\")? How are ascending and descending pathways integrated?\n- many experimental details remain quite vague: nature of the data, the features that are fed to the network, number of data samples, training method, parameters justification, parameters of baselines, ...\n- it is not clear how this network architecture would perform, e.g., classification tasks"}, "questions": {"value": "- Could you please give a link where the benchmark datasets can be obtained? \n- what would an application in the real world look like that profits from this kind of technique?\n- please give more information about the used datasets: dimensionality, number of samples, etc... \n- what are features that are actually received by model neurons? how many, what is their nature?\n- please explain how such a network is trained, I found no details in the paper. Training algorithm (eg, SGD), optimizer(Adam?), learning rates, learning rate schedule, ...\n- what are parameters for the baseline methods? How did you tune them?\n- how did you arrive at the parameters given in Sec.4? Cross-validation? On what data?\n- how would this network perform, e.g., classification?\n- can this network also operate on raw data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JMakY32TcJ", "forum": "BNZnqTlQjZ", "replyto": "BNZnqTlQjZ", "signatures": ["ICLR.cc/2026/Conference/Submission9109/Reviewer_Jtsv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9109/Reviewer_Jtsv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9109/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762272839851, "cdate": 1762272839851, "tmdate": 1762920806687, "mdate": 1762920806687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}