{"id": "b5oUWQ0ObU", "number": 8174, "cdate": 1758072499566, "mdate": 1759897801921, "content": {"title": "CSVQ: Channel-wise Shared-Codebook Vector Quantization for Stable and Expressive Discrete Representations", "abstract": "Vector quantization (VQ) is a cornerstone of discrete representation learning, but existing methods often depend on very large codebooks that increase capacity while hurting stability and utilization. We present Channel-wise Shared-codebook Vector Quantization (CSVQ), a simple tokenizer that quantizes each latent channel independently using one shared scalar codebook with per-channel normalization. CSVQ achieves the same representational capacity as vector-quantized latents while using a much smaller codebook, reduces gradient noise through channel-wise aggregation, and cuts codebook memory to scale linearly with the number of codewords rather than with both codewords and channels. Across multiple datasets and settings, CSVQ improves reconstruction quality and training stability, remains competitive under strict memory budgets, and scales favorably with model capacity. Finally, CSVQ shows improvements in reconstruction quality and downstream task performance compared to the state-of-the-art VQ methods. Ablation and multi-seed studies support the design choices. Code is available at https://anonymous.4open.science/r/csvq-CF06.", "tldr": "", "keywords": ["Vector Quantization", "Discrete Representation Learning", "Channel-wise Quantization; Shared Codebook"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/de665a39d52baa7ecfde67e3d81fd27b07605303.pdf", "supplementary_material": "/attachment/654e55682430fe939c546313b7538da322e0107f.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposes Channel-wise Shared-codebook Vector Quantization (CSVQ), a tokenization method that performs independent scalar quantization on each latent channel using a single shared codebook, preceded by channel-wise normalization. By quantizing channels independently while sharing a single codebook across all channels, CSVQ achieves the same representational capacity as vector-quantized latents but with a much smaller codebook size.\nExperiments on CIFAR-10 and ImageNet-64 show that CSVQ improves reconstruction quality across three experimental settings: fixed codebook budget, equal expressiveness, and channel scaling."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The presentation is clear and straightforward, making the paper easy to follow."}, "weaknesses": {"value": "**Limited Novelty**: \n\n- Channel-wise quantization with normalization: The core idea is similar to MoVQ [1] (Section 3.1, Multichannel Representation). Classic VQ partitions the latent space only along the spatial grid, producing discrete codes in $\\mathbb{R}^{H×W}$. The proposed method additionally splits the channel dimension into sub-spaces, yielding codes in $\\mathbb{R}^{H×W×ch}$.\n\n  The key difference is that CSVQ quantizes each channel value independently (i.e., sub-space dimension = 1), whereas MoVQ uses higher-dimensional sub-spaces. While this approach naturally increases latent capacity and improves reconstruction fidelity, it also significantly increases the code sequence length from $H×W$ to $H×W×ch$. This longer sequence makes autoregressive generation (for image generation from discrete tokens) computationally expensive and potentially more difficult to model effectively. This may explain why the authors did not conduct image generation experiments, which are a standard benchmark for VQ-based methods.\n\n- The theoretical contribution presented in the main text appears straightforward. Using more codes to represent an image while maintaining the same codebook size increases representational capacity is intuitive and directly leads to the theoretical results presented. The analysis does not provide substantial new theoretical understanding beyond this obvious observation.\n\n**Limited Experimental Scope**:\n\n- Experiments are restricted to CIFAR-10 and ImageNet-64, which are relatively small-scale datasets.\n- Missing image generation benchmark: The paper does not include image generation experiments, which are a common and critical application of VQ-based methods."}, "questions": {"value": "See the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VGg5tgcL8w", "forum": "b5oUWQ0ObU", "replyto": "b5oUWQ0ObU", "signatures": ["ICLR.cc/2026/Conference/Submission8174/Reviewer_G3gi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8174/Reviewer_G3gi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895280926, "cdate": 1761895280926, "tmdate": 1762920136946, "mdate": 1762920136946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Channel-wise Shared-codebook Vector Quantization (CSVQ) which use one shared scalar codebook to quantize each channel independently. The authors claim CSVQ delivers the expressiveness of vector VQ with a much smaller codebook, reduces gradient variance via cross-channel signal aggregation, and scales well in channels. To demonstrate it, the paper provides experiments on CIFAR-10 and ImageNet-64 and reports superior PSNR/SSIM to show better reconstruction quality. Additionally, the research offers better linear-probe and transfer results than VQ-VAE/RQ-VAE/FSQ/LFQ under the evaluated protocols."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper explores an alternative direction to improve the vector quantization technique.\n2. The authors provide both theoretical and empirical analyses."}, "weaknesses": {"value": "1. Novelty\n    1. There are some concerns about the novelty of this paper, as [1] has already proposed a similar channel-wise approach.\n2. Experiment:\n    1. The experiments are not conducted under standard settings [2, 3, 4, 5, 6, 7, 8], which raises doubts about CSVQ’s effectiveness in real-world scenarios and about the fairness of the comparisons among baselines.\n        1. Dataset. For image tokenizer, a standard setup typically uses ImageNet-1k with the resolution of 256 * 256 or 512 * 512. \n        2. Codebook & Dimension settings. \n            1. The latent bottlenecks are overly tight (like codebook for 64 and dimension for 1 or 4). That is not reasonable or applicable in a real-world scenario. A more applicable configuration would scale the latent space(like codebook size 512 with 32/64 dimensions on CIFAR-10[9]).\n            2. For the experiment, which set the baseline’s channel dimension as 1, [8] would be a useful and meaningful baseline for comparison.\n        3. Metrics. It is better to report codebook usage, Perplexity, LPIPS, and rFID score, which are commonly used in other baselines for a fair comparison.\n        4. Optimization targets. For image tokenizers, a standard setting is often based on VQGAN[6], where the final objective includes quantization loss and MSE, as well as GAN loss and perceptual loss (e.g., LPIPS) to emphasize perceptual quality and optimize the latents differently.\n        5. Downstream task. Image tokenizers are usually trained for generative models, yet there are no generation experiments on real-world datasets.\n3. Lack of support for claims:\n    1. The paper mentioned CSVQ can “reduce the gradient noise through channel-wise aggregation”, but there is no theoretical analysis or empirical analysis about it.\n    2. The paper mentioned CSVQ is “a simple, stable channel-wise scalar quantizer with a shared codebook that alleviates collapse”, but there is no evidence about the mitigation of collapse.\n    3. The CSVQ is “a simple, stable channel-wise scalar quantizer”, but there is also a lack of theoretical comparison about other scalar quantizers like LFQ and FSQ. \n4. Concerns about “Expressiveness”:\n    1. There is also a concern about the theoretical analysis, especially about the expressiveness. For VQ and RVQ, the usual setting for ch is not 1. That means the real expressiveness for VQ and RVQ should be larger than the cardinality of the symbol space |S|. Thus, the theoretical analysis framework has a preference for CSVQ and unfariness for VQ or RVQ.\n    2. The current theoretical expressiveness analysis hinges on independent per-channel selection, but real latents might not be independent across channels. The configuration counting is thus an upper bound, not realized capacity.\n5. Writing:\n    1. There is no appendix in the paper, despite being referenced multiple times.\n    2. For Equations (210–211), (215–216), (227–232), etc., the equation numbers are missing.\n    3. At the ends of lines 260, 269, and 280, there are odd square markers.\n\n[1] Yang, Tong, Xiangyu Zhang, and Wenqiang Zhang. \"Image Generation with Channel-wise Quantization.”\n\n[2]Zhu, Yongxin, et al. \"Addressing representation collapse in vector quantized models with one linear layer.\" *Proceedings of the IEEE/CVF International Conference on Computer Vision*. 2025.\n\n[3]Mentzer, Fabian, et al. \"Finite scalar quantization: Vq-vae made simple.\" *arXiv preprint arXiv:2309.15505* (2023).\n[4]Sun, Peize, et al. \"Autoregressive model beats diffusion: Llama for scalable image generation.\" *arXiv preprint arXiv:2406.06525* (2024).\n[5]Yu, Jiahui, et al. \"Vector-quantized image modeling with improved vqgan.\" *arXiv preprint arXiv:2110.04627* (2021).\n[6]Esser, Patrick, Robin Rombach, and Bjorn Ommer. \"Taming transformers for high-resolution image synthesis.\" *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2021.\n\n[7]Kouzelis, Theodoros, et al. \"Eq-vae: Equivariance regularized latent space for improved generative image modeling.\" *arXiv preprint arXiv:2502.09509* (2025).\n\n[8]Bachmann, Roman, et al. \"FlexTok: Resampling Images into 1D Token Sequences of Flexible Length.\" *Forty-second International Conference on Machine Learning*. 2025.\n\n[9] Van Den Oord, Aaron, and Oriol Vinyals. \"Neural discrete representation learning.\" *Advances in neural information processing systems* 30 (2017)."}, "questions": {"value": "1.  It would be beneficial if authors could  include an ablation study that increases the latent size and shows the corresponding reconstruction improvements, to validate that CSVQ is a general method across different latent configurations.\n2. It would be great if authors could consider addressing the points raised in the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LcZmzLRvcq", "forum": "b5oUWQ0ObU", "replyto": "b5oUWQ0ObU", "signatures": ["ICLR.cc/2026/Conference/Submission8174/Reviewer_HQdg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8174/Reviewer_HQdg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925429733, "cdate": 1761925429733, "tmdate": 1762920136629, "mdate": 1762920136629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Channel-wise Shared-codebook Vector Quantization (CSVQ), a new tokenizer that addresses the stability and efficiency issues of traditional VQ methods. Instead of quantizing entire vectors with large, complex codebooks, CSVQ quantizes each latent channel independently using a single, shared scalar codebook. The approach achieves exponential representational capacity with memory cost $O(K)$. Empirical results on CIFAR-10 and ImageNet-64 show that CSVQ outperforms existing VQ methods in reconstruction quality, stability, and downstream task performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method, CSVQ demonstrates excellent performance across reconstruction quality (as shown in Tables 2, 3, and 4), but also excels in downstream tasks such as linear probing and transfer learning (Table 6). \n- By sharing a single scalar codebook across all channels, CSVQ improved codebook utilization. This makes the model more reliable, easier to train, and more efficient in its use of learned parameters."}, "weaknesses": {"value": "- The paper's central claim of achieving high representational capacity ($K^{ch \\times H \\times W}$) by factoring in the channel dimension ($ch$) is questionable. The total number of bits required to represent an image is what truly matters. This is typically calculated by `(number of tokens) × log₂(codebook_size)`. For instance, in Table 3, CSVQ uses $8 \\times 8 \\times 4$ tokens, each from a codebook of size 64. A fair comparison would require baseline methods (like FSQ, LFQ) to use a configuration that yields a similar total bitrate. For example, a single spatial grid of $8 \\times 8$ tokens with a codebook of size $64 ^ 4$, or other equivalent configurations. \n\n- The proposed tokenizer creates an impractically large discrete space for generative modeling. For example, in Table 4, with $K=512$ and $ch=3$, the vocabulary size is $512^3 \\approx 134$ million. Training an autoregressive model (like a Transformer) on such a massive vocabulary is computationally infeasible with current methods. This severely limits the practical impact of CSVQ as a general-purpose tokenizer for generative tasks, which is a primary application for VQ methods. \n\n- The experimental section, while extensive, lacks sufficient qualitative and analytical depth. For instance, Table 6 shows that CSVQ leads to better downstream performance, but the paper offers no investigation into *why*. What specific features has the encoder learned that make the representations more semantically meaningful? Visualizations of the learned features, analysis of the per-channel statistics after normalization, or feature attribution maps would have provided much-needed insight into the model's behavior and could have strengthened the claims about its superior understanding capabilities.\n\n- The paper is missing critical ablation studies that would justify its specific design choices. A key component of CSVQ is the per-channel normalization step applied before quantization. However, its importance is not experimentally validated. An ablation study comparing CSVQ with and without this normalization step is essential to demonstrate its contribution to performance and stability. The paper discusses the trade-offs between codebook size ($K$) and channel depth ($ch$), but this relationship is not visually or comprehensively analyzed. A graphical analysis showing how reconstruction quality changes as $K$ and $ch$ are varied would provide a much clearer picture of these trade-offs."}, "questions": {"value": "- The paper positions CSVQ as a \"tokenizer\" but does not evaluate its performance in a generative setting (e.g., training a Transformer on the quantized latents to generate new images). How does CSVQ perform on standard image generation benchmarks compared to other VQ methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "b2lvQyilsh", "forum": "b5oUWQ0ObU", "replyto": "b5oUWQ0ObU", "signatures": ["ICLR.cc/2026/Conference/Submission8174/Reviewer_EbtN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8174/Reviewer_EbtN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966277912, "cdate": 1761966277912, "tmdate": 1762920136261, "mdate": 1762920136261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the issues of training instability and large codebook memory in Vector Quantization (VQ) models. This paper introduces CSVQ, which quantizes each latent channel independently using a single, shared scalar codebook with per-channel normalization. Extensive experiments and theoretical analysis demonstrate that CSVQ achieves comparable representation capacity with a smaller codebook."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Reducing gradient variance by roughly 1/ch where ch is the number of latent channels, thus enhancing training stability.\n\n2. The experiments on reconstruction tasks are convincing and the results on linear probing are interesting, suggesting that CSVQ acquires a degree of semantic understanding.\n\n3.Maintaining comparable performance with smaller codebook size."}, "weaknesses": {"value": "1.Missing Generation Results: A core objective of VQ-VAE models is to learn meaningful discrete representations that are applicable to generation tasks. Therefore, it is meaningful and necessary to present generation results to thoroughly evaluate the quality and effectiveness of CSVQ. It is currently unclear whether CSVQ can be effectively adapted to generative models such as autoregressive models or diffusion large language models (dllms). In contrast, patchify-based VQ codebooks, leveraging the spatially local relationships between image patches, have been widely validated and demonstrated effectiveness in both autoregressive models and dllms. \n\n2.Missing Baselines: There exits other methods that aim to solve training instability, including eqvae[1] and rotation trick[2].\n\n3.Limited results and metrics:Despite emphasizing smaller codebook sizes, experiments on the widely-used ImageNet 256 dataset and the comparisions between large cobebook size are absent. Additionally, the paper claims better codebook usuage, however codebook usage rate (as in SimVQ) is not reported.\n\n[1] Kouzelis T, Kakogeorgiou I, Gidaris S, et al. Eq-vae: Equivariance regularized latent space for improved generative image modeling[J]. arXiv preprint arXiv:2502.09509, 2025.\n[2] Fifty, Christopher, et al. \"Restructuring Vector Quantization with the Rotation Trick.\" ICLR. 2025."}, "questions": {"value": "1.A significant concern is the scalability of CSVQ for high-resolution image where the dimension length in the codebook is equal to h*w. For example, an 8x downsampling encoder of a 512x152 image yields a 4096-dimensional representations for each channel in the CSVQ setting. However, in traditional VQ-VAE, the length of the dimension is often less than 128 or even 8 [3] . It is crucial to clarify if CSVQ can achieve superior performance with a comparatively smaller codebook size (K) while handling this long demension length, and how its efficiency is impacted.\n\n2.Please clarify why sharing the same codebook across all channels, can improve stability in line 57-58. \n\n[3] Sun P, Jiang Y, Chen S, et al. Autoregressive model beats diffusion: Llama for scalable image generation[J]. arXiv preprint arXiv:2406.06525, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XvHpD4liTI", "forum": "b5oUWQ0ObU", "replyto": "b5oUWQ0ObU", "signatures": ["ICLR.cc/2026/Conference/Submission8174/Reviewer_L2Bx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8174/Reviewer_L2Bx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977383190, "cdate": 1761977383190, "tmdate": 1762920135352, "mdate": 1762920135352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}