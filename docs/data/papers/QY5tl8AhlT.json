{"id": "QY5tl8AhlT", "number": 23504, "cdate": 1758344677152, "mdate": 1759896811494, "content": {"title": "Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning", "abstract": "Large reasoning models (LRMs) achieve higher performance on challenging reasoning tasks by generating more tokens at inference time, but this verbosity often wastes computation on easy problems. Existing solutions—supervised fine-tuning on shorter traces, user-controlled budgets, or RL with uniform penalties—either require data curation, manual configuration, or treat all problems alike regardless of difficulty. We introduce Adaptive Length Penalty (ALP), a reinforcement‑learning objective tailoring generation length to per-prompt solve rate. During training, ALP monitors each prompt’s online solve rate through multiple rollouts and adds a differentiable penalty whose magnitude scales inversely with that rate, so confident (easy) prompts incur a high cost for extra tokens while hard prompts remain unhindered. Post‑training DeepScaleR-1.5B with ALP cuts average token usage by 50\\% without significantly dropping performance.  Relative to fixed‑budget and uniform‑penalty baselines, ALP redistributes its reduced budget more intelligently—cutting compute on easy prompts and reallocating saved tokens to difficult ones—delivering higher accuracy on the hardest problems with higher cost.", "tldr": "Training a model to allocate test-time compute budget based on difficulty of prompts.", "keywords": ["Large Language Models", "Large Reasoning Models", "Test-time scaling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/80af792449c30a1deddf33cf70d3c4e1db06f44e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "It introduces Adaptive Length Penalty (ALP), a reinforcement learning (RL) method that enables large reasoning models (LRMs) to dynamically adjust the length of their reasoning traces based on problem difficulty—using fewer tokens on easy problems and more on hard ones.\n\nA novel RL reward objective that scales token penalties inversely with an online estimate of problem difficulty, measured by empirical solve rate across multiple rollouts.\n\nALP learns to spend only 21% of tokens on the easiest 50% of problems, freeing up resources for harder ones—achieving a 5.35× higher token allocation ratio (hard vs. easy)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Compare against strong, recent baselines, including L1-Exact/Max, ThinkPrune, and R1-Alpha, covering supervised, user-controlled, and RL-based length-control strategies.\n2. Conduct multiple layers of analysis: Pareto efficiency curves, adaptation ratios, robustness to unknown difficulty mixtures, and fine-grained behavioral analysis of reasoning traces.\n3. The work addresses a critical bottleneck in deploying reasoning models at scale: the unsustainable cost of verbose, unadaptive chain-of-thought generation. By enabling models to use “just enough” reasoning, ALP offers a practical path toward more sustainable, responsive, and cost-effective AI systems—especially valuable as models are deployed in real-world settings with unknown or mixed-difficulty inputs."}, "weaknesses": {"value": "1. The model-scale of exp is limited to DeepScaleR-1.5B, which may not be suitable for large-scale model-size such as 7b/32b.\n2. The beta in Equation [2] may vary for different model-size, I think the exp should contain more analysis for 7/32b."}, "questions": {"value": "1. This paper analyzes the REASONING BEHAVIOR CHANGES of length reduction, it may change the over/under-thinking, could you show the over/under-thinking metric since the reasoning patterns may not reveal the performance of over/under-thinking.\n2. The beta in Equation [2] may vary for different model-size and domains, I am wondering the beta for logical reasoning, STEM and coding rather than math."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cPsktRWxaC", "forum": "QY5tl8AhlT", "replyto": "QY5tl8AhlT", "signatures": ["ICLR.cc/2026/Conference/Submission23504/Reviewer_xGHQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23504/Reviewer_xGHQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23504/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761410295949, "cdate": 1761410295949, "tmdate": 1762942688410, "mdate": 1762942688410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of computational inefficiency in Large Reasoning Models (LRMs) caused by overthinking, an excessive reasoning on simple problems. The authors propose Adaptive Length Penalty (ALP), a reinforcement learning objective that dynamically allocates computational effort based on problem difficulty. ALP estimates difficulty using the in-batch solve rate as a real-time proxy and applies a length penalty inversely proportional to it, encouraging concise reasoning for easy problems and longer reasoning for hard ones. The method achieves strong efficiency gains, reducing token usage by 50% without notable accuracy loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. ALP’s use of online solve rate as a self-contained difficulty signal is both intuitive and effective. The inverse scaling penalty directly enforces the desired behavior—concise on easy cases, thorough on hard ones—without external classifiers or manual curation.\n\n2. ALP achieves a 50% reduction in average tokens with minimal accuracy degradation. The Pareto efficiency and adaptation ratio analyses (e.g., 5.35× more tokens on hard vs. easy problems) convincingly demonstrate adaptive computation rather than uniform compression.\n\n3. Experiments on mixed-difficulty datasets (e.g., MATH-500 + AIME) show ALP dynamically adjusts computation while maintaining high accuracy, outperforming fixed-length baselines. This supports ALP’s real-world applicability.\n\n4. The paper defines “overthinking” as a concrete inefficiency pattern and positions ALP as a principled fix. The narrative clearly contrasts ALP with prior, static-budget approaches, enhancing clarity and motivation."}, "weaknesses": {"value": "1. The in-batch solve rate, estimated from limited rollouts, may be unstable early in training. Misclassifying an easy problem as “hard” can lead to weak penalties and inefficient behavior.\n\n2. The global penalty coefficient governing the accuracy–conciseness trade-off is fixed without sensitivity testing. It remains unclear how robust ALP is to this choice across models or domains.\n\n3. Evaluation is confined to mathematical reasoning tasks with binary correctness. It is uncertain how ALP’s difficulty signal generalizes to open-ended or subjective tasks lacking a clear notion of “solve rate.”\n\n4. The reasoning-pattern study relies on keyword matching (e.g., “plan,” “verify,” “backtrack”), which is a coarse and gameable proxy. Models may paraphrase such patterns, limiting interpretability of the behavioral findings."}, "questions": {"value": "How stable is the online solve rate estimate during training with limited rollouts? Did you observe inconsistent penalty assignments for the same problem across iterations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BUOQQykMap", "forum": "QY5tl8AhlT", "replyto": "QY5tl8AhlT", "signatures": ["ICLR.cc/2026/Conference/Submission23504/Reviewer_TiY2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23504/Reviewer_TiY2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23504/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793927981, "cdate": 1761793927981, "tmdate": 1762942688144, "mdate": 1762942688144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper deals with efficient reasoning in language models, specifically solving problems with the minimum number of required reasoning tokens. The key idea is a reward function that estimates difficulty using the model's pass rate on a prompt, and penalizes the token usage accordingly."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Timely and relevant problem.\n- Simple and intuitive approach based on using pass rates in the reward function."}, "weaknesses": {"value": "- The baselines have several potential confounding factors, such as the training data and hyperparameters. Moreover, the paper chooses two specific model checkpoints from the baselines (ThinkPrune-2K, and α=0.2). These make it difficult to reason about what specifically leads to the performance differences.\n   - For example, R1-$\\alpha$ performs pretty similarly and also shows adaptivity to problem difficulties (e.g., see Figure 1, Figure 4).\n- The paper concludes that \"These patterns suggest that explicit adaptive training through difficulty-aware objectives, as implemented in ALP, is necessary for models to develop internal calibration that translates to efficient computation allocation.\" However, we cannot conclude that the difficulty-aware objectives are *necessary* without trying a wide variety of alternative objectives with other variables held constant.\n- The related work mentions that prior work \"share a critical limitation: they do not adapt length based on the intrinsic difficulty of each problem instance. These approaches apply uniform policies across all problems, inevitably over-reasoning on simple tasks or under-reasoning on complex ones.\" It is unclear why a simple length penalty added to the reward could not address this issue. Indeed, the experimental results show that several methods are able to adapt to the intrinsic difficulty of each problem instance. For example, in Figure 4 R1-alpha also shows a correlation between token usage and difficulty.\n- Experiments are only done on math reasoning with a 1.5B model. It is unclear whether the results will generalize to other domains and model sizes."}, "questions": {"value": "Please address the points above, including:\n- Could the authors perform ablations with alternative choices for the reward design? I would particularly be interested in whether a simple length penalty would work, and methods that resemble the baselines but trained in the same setting.\n- The paper claims that existing methods cannot adapt to problem difficulty, but the experimental results (e.g., Figure 4 showing R1-alpha's adaptation) appear to contradict this. Can the authors clarify what specific aspect of adaptation ALP provides that other length penalty methods do not?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oJyWOgGTDY", "forum": "QY5tl8AhlT", "replyto": "QY5tl8AhlT", "signatures": ["ICLR.cc/2026/Conference/Submission23504/Reviewer_aiyV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23504/Reviewer_aiyV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23504/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961087289, "cdate": 1761961087289, "tmdate": 1762942687812, "mdate": 1762942687812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Adaptive Length Penalty (ALP), a reinforcement-learning objective that scales a per-token penalty by an online estimate of problem difficulty (the solve rate across K rollouts). The goal is to spend “just enough” tokens on easy prompts and allow longer reasoning on hard ones. Experiments on math benchmarks (AIME’24/’25, MATH-500, OlympiadBench) suggest similar or higher Pass@1 with ~50% fewer tokens than several baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written.\n2. Clear problem motivation and relevance.\n3. The method is simple and effective."}, "weaknesses": {"value": "- The core idea (scaling a length penalty by probelm difficulty) lands squarely within a fast-growing body of difficulty-aware, adaptive-thinking work. Recent papers already teach models when (and how much) to think via RL or reward shaping, e.g., AdaptThink learns to switch thinking modes based on problem difficulty, LASER frames efficient reasoning as length-based reward shaping with a target-length step reward. To establish genuine novelty, the paper should articulate a crisp conceptual and algorithmic delta relative to these methods and include matched, head-to-head comparisons under identical training/eval setups. Without such positioning and comparative ablations, the contribution is lack of novelty.\n\n[1] Learn to Reason Efficiently with Adaptive Length-based Reward Shaping\n\n[2] AdaptThink: Reasoning Models Can Learn When to Think\n\n[3] Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL\n\n- All results are reported on a single model (DeepScaleR-1.5B) and math-centric benchmarks; this makes it hard to assess robustness and transfer. At minimum, the study should replicate on diverse model families (e.g., Qwen and LLaMA) and multiple scales (3B, 7B, etc.) to test whether the learned policy and the compute–accuracy trade-off persist across architectures and capacities. Beyond math, a credible “general reasoning” claim requires evaluation on broad knowledge and hard science suites such as MMLU and GPQA, which probe very different skills than contest math. Including such benchmarks, and reporting both accuracy and efficiency (tokens, latency) at fixed budgets, would materially strengthen external validity.\n\n- Because the length penalty is scaled by the current batch/group solve rate, the learning signal can collapse in scenarios where the model already achieves high accuracy on both easy and hard items: if solve rates saturate (e.g., ~100% for both), the penalty coefficient becomes effectively identical, pushing the policy toward a uniform reduction in length rather than learning to differentiate token allocation by difficulty. This undermines the central claim of adaptive thinking, where models might simply shorten outputs across the board without acquiring a calibrated notion of “hard vs. easy.” The manuscript would be stronger with ablations that (a) remove the solve-rate term (pure length penalty) to test whether adaptation persists, (b) substitute alternative difficulty proxies (e.g., entropy, loss) to demonstrate the necessity of the chosen signal, and (c) include thorough sensitivity sweeps for $\\beta$ (penalty weight) and K (rollout count) to probe stability and variance effects; without these, it remains unclear whether the method genuinely learns difficulty awareness versus performing global length pruning."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7MH0zGC4et", "forum": "QY5tl8AhlT", "replyto": "QY5tl8AhlT", "signatures": ["ICLR.cc/2026/Conference/Submission23504/Reviewer_Cb1X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23504/Reviewer_Cb1X"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23504/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994889433, "cdate": 1761994889433, "tmdate": 1762942687169, "mdate": 1762942687169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}