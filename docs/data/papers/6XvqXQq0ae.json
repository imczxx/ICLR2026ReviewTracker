{"id": "6XvqXQq0ae", "number": 86, "cdate": 1756728623977, "mdate": 1759898276285, "content": {"title": "NextLocMoE: Enhancing Next Location Prediction via Location-Semantics Mixture-of-Experts and Personalized Mixture-of-Experts", "abstract": "Next location prediction is a key task in human mobility modeling. Existing methods face two challenges: (1) they fail to capture the multi-faceted semantics of real-world locations; and (2) they struggle to model diverse behavioral patterns across user groups. To address these issues, we propose NextLocMoE, a large language model (LLM)-based framework for next location prediction, which integrates a dual-level Mixture-of-Experts (MoE) architecture. It comprises two complementary modules: a Location Semantics MoE at the embedding level to model multi-functional location semantics, and a Personalized MoE within LLM’s Transformer layers to adaptively capture user behavior patterns. To enhance routing stability and reliability, we introduce a historical-aware router that integrates long-term historical trajectories into expert selection. Experiments on multiple real-world datasets demonstrate that NextLocMoE significantly outperforms existing methods in terms of accuracy, transferability, and interpretability. Code is available at: https://anonymous.4open.science/r/NextLocMOE-BAC8.", "tldr": "We propose NextLocMoE, a Mixture-of-Experts LLM framework for next-location prediction, which jointly modelslocation semantics and behavioral preferences  via dual expert modules and history-aware routing.", "keywords": ["next location prediction", "Mixture-of-Experts", "Large Language Model", "Location Function MoE", "Persona MoE"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f5b63891a6c4d26f62a5d31b7d29da7969c92e8c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "NextLocMoE is a next-generation location prediction framework based on large language models (LLMs). Its core innovation lies in integrating a dual-level Mixture-of-Experts (MoE) architecture, which aims to address the shortcomings of existing location prediction methods in capturing multi-dimensional semantics of locations and diverse behavioral patterns of user groups. Meanwhile, it enhances the routing stability, reliability, and generalization ability of the model."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper applies the MoE architecture to location semantics and user behavior in a hierarchical manner. It not only solves the problem of capturing multi-functional semantics of locations but also realizes the refined distinction of user group behaviors. This dual-level MoE collaborative modeling approach is innovative.\n\n2.The paper initializes expert parameters through natural language descriptions encoded by LLMs, which makes the semantic positioning of expert modules clearer. The expert activation of the Personalized MoE is highly aligned with the real user groups, thus achieving a certain degree of interpretability.\n\n3.The experiments in the paper are sufficient and comprehensive, and the provided code seems correct, which facilitates subsequent research work."}, "weaknesses": {"value": "1.Both the 10 user groups in the Personalized MoE and the 5 function categories in the Location Semantics MoE are manually predefined, lacking the ability of adaptive adjustment in terms of division granularity and category selection. If there are uncovered user groups or location functions in practical scenarios, it may lead to expert activation bias and affect the prediction performance.\n\n2.Although the Personalized MoE does not rely on user IDs, it still needs to guide expert selection through historical trajectory encoding. For new users with no historical trajectories at all (cold-start scenario), the historical-aware router cannot provide effective context."}, "questions": {"value": "1.We note that although NextLocMoE achieves excellent performance, there is still significant room for performance improvement. Is this related to the \"Undefined Persona\"?\n\n2.Can the length of users' historical trajectories be counted in the provided datasets? And do all users have historical trajectories? How should the cold-start problem be considered?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nDfGQgOiWi", "forum": "6XvqXQq0ae", "replyto": "6XvqXQq0ae", "signatures": ["ICLR.cc/2026/Conference/Submission86/Reviewer_WfzH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission86/Reviewer_WfzH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission86/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761202302684, "cdate": 1761202302684, "tmdate": 1762915448059, "mdate": 1762915448059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NextLocMoE, a dual-level Mixture-of-Experts (MoE) enhanced LLM framework for next location prediction. The framework integrates two complementary MoE modules:\n(1) Location Semantics MoE: Models the multi-functional semantics of locations (e.g., commercial, educational, entertainment) using a fixed top-k expert routing strategy.\n\n(2) Personalized MoE: Captures diverse user behavioral patterns via a confidence threshold-based dynamic routing strategy, enabling group-level personalization without explicit user IDs.\n\nAdditionally, the authors introduce a historical-aware router that incorporates long-term historical trajectories into expert selection, enhancing contextual stability and reliability. Extensive experiments on three real-world mobility datasets demonstrate that NextLocMoE consistently outperforms state-of-the-art baselines under both fully-supervised and zero-shot cross-city settings, while achieving significant inference speedups (e.g., 600× faster than Llama-Mob)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Extensive experiments and detailed analyses (e.g., routing strategies, historical encoding, alternative backbones) demonstrate the robustness and generality of the approach.\n\n2. The writing is clear, and the methodology is described in sufficient detail.\n\n3. The model not only improves accuracy but also drastically reduces inference time, making it suitable for large-scale applications."}, "weaknesses": {"value": "1. The research problem is too well-studied.\n\n2. The Personalized MoE relies on predefined user group descriptions. While LLM-encoded priors help, this may not cover all behavioral patterns in diverse populations.\n\n3. Although the model outperforms baselines in zero-shot transfer, absolute performance (e.g., Hit@1 ~16% on Kumamoto) remains low, indicating room for improvement in cross-city generalization."}, "questions": {"value": "The Personalized MoE uses 10 predefined user groups. Have the authors considered using unsupervised clustering or dynamic group discovery to reduce reliance on predefined categories?\n\nHow does the Personalized MoE handle users with ambiguous or mixed behaviors (e.g., the \"Undefined Persona\" category)? Is there a risk of over- or under-activating experts for such users?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Z9qYy3krew", "forum": "6XvqXQq0ae", "replyto": "6XvqXQq0ae", "signatures": ["ICLR.cc/2026/Conference/Submission86/Reviewer_hP3j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission86/Reviewer_hP3j"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission86/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761400665050, "cdate": 1761400665050, "tmdate": 1762915447845, "mdate": 1762915447845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NextLocMoE, an LLM-based next-location prediction framework that novelly introduces two complementary MoE modules: a Location Semantics MoE to model multi-functional semantics of locations, and a Personalized MoE to capture user behavioral heterogeneity. The authors also design a history-aware router that uses a TCN of long-term history to stabilize expert selection. The model outputs continuous coordinates and converts coordinates to discrete IDs at inference via KD-Tree retrieval. Experiments on three city datasets show improved Hit@k and much faster inference than LLM-based baselines; extensive appendices provide ablations and robustness checks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clearly articulate two practical failure modes of existing systems (single semantic embedding for locations; single shared model for all users)\n\n2. Innovatively integrating MoE for prediction. Location Semantics MoE gives multiple function-aware embeddings per place; Personalized MoE provides group-level personalization without user IDs — both are interpretable and demonstrated via case studies.\n\n3. Many ablations show the individual contributions of the modules."}, "weaknesses": {"value": "1. The authors claim an interesting argument that omitting the load-balancing loss allows the model to better capture the natural, imbalanced distribution of urban activities.  However, the paper lacks the qualitative experiments needed to validate it.  \n\n2. The framework has a high dependence on a fixed set of five location functions and ten user groups. This raises concerns about its generalizability and scalability. It is unclear how well these pre-defined categories would transfer to new cities with different functional layouts (e.g., an industrial city vs. a tourist city). \n\n3. Complex model. The system integrates multiple components (LLM, TCN, two MoEs, LoRA fine-tuning, and KD-Tree). It brings an extremely high cost, which may lead to problems such as difficulties in adjusting hyperparameters and reduced reproducibility."}, "questions": {"value": "1. Could the authors provide a qualitative analysis of the expert activation frequencies? For example, showing the activation rates of the five location experts for a specific type of user trajectory.\n\n2. Are these fixed categories still applicable when the model is applied to a new environment with a different urban layout (e.g., an industrial vs. a tourist city) or different cultural mobility patterns?\n\n3. How much does the quality of LLM semantic priors affect performance? Have you experimented with comparing the results (Detailed, rich descriptions vs. using simple labels (e.g., just \"Commercial\" or \"Student\")) for semantic priors? Or the influence of a slight perturbation of semantic priors on the results?\n\n4. Considering the overall complexity of the model, have you ever explored simplifying the model? For instance, if only one of the MoE modules is used, or if a simpler historical encoder (rather than a TCN) is used to boot the router, can a competitive balance be achieved between performance and complexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "loCHf3A3U6", "forum": "6XvqXQq0ae", "replyto": "6XvqXQq0ae", "signatures": ["ICLR.cc/2026/Conference/Submission86/Reviewer_Eynx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission86/Reviewer_Eynx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission86/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725007710, "cdate": 1761725007710, "tmdate": 1762915447743, "mdate": 1762915447743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an innovative framework for next-location prediction, NextLocMoE, which integrates large language models (LLMs) with a dual-layer Mixture-of-Experts (MoE) architecture to jointly model the multi-functional semantics of locations and the behavioral diversity of users. The framework consists of two key modules: Location Semantics MoE, which models multi-functional location semantics, and Personalized MoE, which captures individual behavioral preferences. In addition, a historical-aware router is introduced to enhance the stability of expert selection by incorporating temporal dependencies."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a well-structured and clearly motivated dual-layer MoE architecture that effectively captures both the multi-functional semantics of locations and the diversity of user behaviors. The architectural design is novel and well-executed.\n\n2. The historical-aware routing mechanism is innovative and well-motivated. By integrating trajectory history into expert selection, it effectively models long-term behavioral dependencies and improves the stability of routing decisions."}, "weaknesses": {"value": "1. It remains unclear how experts in NextLocMoE learn meaningful semantic roles or capture distinct user groups, since the model is trained without explicit supervision (e.g., no annotations for location functions or user groups) or loss terms that guide such specialization. Although the paper describes the existence of these roles, it does not explain how the experts are selected or differentiated.\n\n2. The experimental setup for zero-shot prediction is not clearly described—specifically, the training and test dataset splits for baselines such as LLM-Mob and ZS-NL (in Table 2) are insufficiently detailed.\n\n3. The cross-city generalization mechanism is not fully explained. It remains unclear how coordinate-based predictions can preserve region-level semantic information across datasets, especially when the same coordinates may have different meanings in different cities. The paper should further clarify how data and knowledge transfer are achieved in such scenarios."}, "questions": {"value": "Please refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dw66NEWRbh", "forum": "6XvqXQq0ae", "replyto": "6XvqXQq0ae", "signatures": ["ICLR.cc/2026/Conference/Submission86/Reviewer_93Nk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission86/Reviewer_93Nk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission86/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728760179, "cdate": 1761728760179, "tmdate": 1762915447546, "mdate": 1762915447546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}