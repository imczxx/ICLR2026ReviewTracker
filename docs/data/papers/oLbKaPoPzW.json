{"id": "oLbKaPoPzW", "number": 1842, "cdate": 1756951313625, "mdate": 1759898182846, "content": {"title": "MIGA: Make Train-Free Infinite Frame Generation Great Again for Consistent Long Videos", "abstract": "Without relying on significant computational or data resources, train-free long video generation aims to extend the duration of foundation video generation models, which are typically limited to short videos.\nDirect noise prediction on the entire long latents incurs substantial computational overhead.\nIn contrast, frame-level autoregressive frameworks, e.g., FIFO-diffusion, offer the advantage of generating infinitely long videos with constant memory consumption.\nHowever, the substantial gap between training and inference phases hinders the effective utilization of foundation models. Furthermore, maintaining long-term consistency is central to long video generation, yet existing methods pay insufficient attention to this aspect.\nTo mitigate these concerns, we propose **MIGA**, a novel infinite-frame long video generation method. \n**(i)** Firstly, considering that the training-inference gap mainly stems from the excessive noise span of latents fed to the model during inference, we propose an effective two-stage alignment mechanism. By partitioning the generation process of existing frameworks into two dedicated stages with reduced noise spans, the capabilities of advanced foundation models are efficiently unlocked.\n**(ii)** Additionally, building upon the intrinsic properties of frame-level autoregressive frameworks, we introduce an innovative dual consistency enhancement mechanism. \nSpecifically, our self-reflection approach evaluates and corrects early high-noise frames, while our long-range frame guidance approach leverages later low-noise frames with broad coverage to steer the generation process. These strategies jointly promote consistency in the generated content.\n**(iii)** Finally, extensive experiments on VBench and NarrLV demonstrate the state-of-the-art performance of MIGA.", "tldr": "", "keywords": ["long video generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f399477d73a03e10a30b627943334d2812452f5f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents MIGA, a training-free infinite-frame video generation method that extends foundation diffusion models for consistent long videos. It introduces two key designs: (1) a Two-Stage Training–Inference Alignment mechanism that reduces the noise-span mismatch between training and inference via zigzag denoising and unified refinement, and (2) a Dual Consistency Enhancement mechanism combining self-reflection–based anomaly correction and long-range frame guidance. Experiments on VBench and NarrLV benchmarks show that MIGA achieves state-of-the-art performance, improving subject and background consistency over prior training-free methods like FIFO-Diffusion and FreeLong, while maintaining infinite-frame generation capability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The two-stage denoising process is intuitive yet effective, reducing the noise span mismatch with minimal modification to the inference pipeline.\n2. The proposed self-reflection mechanism uses cosine similarity in latent space, eliminating dependence on external models like DINO and improving computational efficiency.\n3. The figures and diagrams are highly informative and well-designed."}, "weaknesses": {"value": "1. The proposed framework lacks a clear overarching motivation. Its components (e.g., TTA, self-reflection, and long-range frame guidance) appear to be designed independently rather than forming a unified, coherent approach to long video generation.\n2. The paper does not demonstrate strong scientific contributions; the proposed method resembles more of an engineering pipeline than a conceptually novel framework.\n3. Given the complexity of the pipeline, the authors should include a comparison of computational or time complexity with other sota methods.\n\nminor weakness:\n1. Line 161 mentions \"its length L equals the total number,\" but L seems absent from the context; did you mean T?"}, "questions": {"value": "1. What is the key contribution of the paper, and which component highlights it?  \n2. How do the proposed methods relate and work together to address the research problem, specifically the train-inference gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N3TdEedRUs", "forum": "oLbKaPoPzW", "replyto": "oLbKaPoPzW", "signatures": ["ICLR.cc/2026/Conference/Submission1842/Reviewer_qgSH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1842/Reviewer_qgSH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897223632, "cdate": 1761897223632, "tmdate": 1762915905450, "mdate": 1762915905450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MIGA, a train-free framework that turns short-video diffusion models into infinite-frame generators with constant memory. It addresses two key issues: the training–inference gap and long-term consistency. First, a Two-Stage Training-Inference Alignment (TTA) reduces the span of noise levels the model must handle at inference: Stage-1 maintains a zigzag latent queue so noise changes more slowly across frames; Stage-2 performs unified-level denoising once all latents reach the same noise level, closely matching the training condition. This yields fewer artifacts and drift and is detailed with an explicit queue-update algorithm.\nSecond, Dual Consistency Enhancement (DCE) combines Self-Reflection (detects anomalies on high-noise tail latents and selectively re-samples to correct them) and Long-Range Frame Guidance (feeds sparse, cleaner anchor frames alongside local windows to promote distant-frame interaction). DCE adds little overhead for guidance and offers tunable test-time scaling for self-reflection; ablations show consistent gains on VBench metrics (subject/background consistency, motion smoothness, etc.). Demonstrations include ~1000-frame videos on Wan2.1-1.3B and ~600-frame on VideoCrafter2"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "MIGA’s big win is that it’s truly train-free yet still turns short-video diffusion models into infinite-frame generators with constant memory. It closes the training–inference gap with a neat two-step trick— a zigzag latent queue to keep neighboring frames at similar noise levels, followed by unified-level denoising—so no drift or flicker as videos get longer. Then it boosts long-range consistency with Dual Consistency Enhancement: lightweight self-reflection to catch and fix anomalies on the fly, plus long-range frame guidance that pulls in clean anchor frames to keep characters and backgrounds stable. It’s plug-and-play (works with VideoCrafter2, Wan2.1, etc.), scales to 600–1000+ frames, and shows solid gains on VBench/Narrative metrics—practical, efficient, and easy to adopt."}, "weaknesses": {"value": "1. Novelty vs. prior art: The “train-free long-video” direction already includes methods like FreeNoise / FreeLong / FreePCA (extend duration by re-using or transforming noise) and Diffusion-Forcing / AR-Diffusion / FIFO-Diffusion (queue/FIFO autoregression with constant memory). MIGA builds on this paradigm by adding alignment and consistency enhancements (TTA + DCE), so its contribution is more incremental than first-of-its-kind on “∞-frame with constant memory.”\n2. Model diversity: Experiments are run on two base models only—VideoCrafter2 and Wan2.1-1.3B—with some discussion that stylistic differences can sway different metrics. Evidence of broad cross-model generalization is therefore limited.\n3. Benchmark/data breadth: The evaluation focuses mainly on VBench (subject/background consistency, motion smoothness, flicker) and NarrLV (narrative metrics) against baselines like FIFO-Diffusion, FreeLong, FreePCA, and ScalingNoise. The benchmark surface is relatively narrow—there’s little coverage of diverse, real-world datasets or large-scale human studies—so external validity remains to be strengthened. The appendix also shows failure cases (e.g., structural mismatches on objects), indicating consistency is not fully solved."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "epRJ63BQKA", "forum": "oLbKaPoPzW", "replyto": "oLbKaPoPzW", "signatures": ["ICLR.cc/2026/Conference/Submission1842/Reviewer_PNPU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1842/Reviewer_PNPU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900968778, "cdate": 1761900968778, "tmdate": 1762915905229, "mdate": 1762915905229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of extending pre-trained short video generation models to create coherent, long videos in a train-free manner. It identifies two major flaws in existing frame-level autoregressive frameworks, such as FIFO-Diffusion: 1) A significant \"training-inference gap,\" as models trained on latents at a single noise level are forced during inference to process a queue of latents with a wide span of different noise levels, leading to artifacts. 2) A failure to model long-term dependencies, which results in poor temporal consistency as the video progresses.\n\nTo solve these issues, the authors propose MIGA, a method with two main contributions. First, it introduces a Two-Stage Training-Inference Alignment (TTA) mechanism. Stage 1 uses \"zigzag iterative denoising,\" which groups latents by noise level (using a width of $L_{zig}$) to reduce the noise span seen by the model. Stage 2 completes the process by performing a unified denoising step once all latents have reached the same intermediate noise level $\\tau_{e-1}$, perfectly aligning with the model's training conditions. Second, MIGA employs a Dual Consistency Enhancement (DCE) mechanism. This includes a \"self-reflection\" approach to efficiently evaluate consistency on new, high-noise latents and trigger a corrective search if anomalies are detected, and a \"long-range frame guidance\" method that injects older, low-noise latents ($m_{guid}$) into the current processing window to enforce temporal coherence. Experiments show MIGA achieves state-of-the-art results on VBench and NarrLV benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper tackles a well-known, difficult problem (long-term consistency in long video generation) from a novel and practical angle. While it builds on existing frame-level autoregressive (AR) frameworks like FIFO-Diffusion, its contributions are highly original. The \"Two-Stage Training-Inference Alignment\" (TTA) mechanism, particularly Stage 2 (Unified Denoising), is a very clever solution to the training-inference gap by forcing the final denoising steps to perfectly match the training distribution (noise span of 1). Furthermore, the \"Dual Consistency Enhancement\" (DCE) mechanism's \"Self-Reflection\" component is also very original. The insight to use noisy-latent self-similarity as a computationally cheap proxy for clean-latent consistency (as shown in Fig. 3) to trigger a test-time search is a significant and non-obvious contribution.\n\n+ TThe methodology is well-motivated, and the two main components (TTA and DCE) directly address the two identified weaknesses of prior work (training-inference gap and long-term consistency). The experimental validation is thorough and convincing. The authors use two different foundation models (VideoCrafter2 and Wan2.1) and two standard benchmarks (VBench, NarrLV). The ablation study in Section 4.3 is exemplary; it methodically deconstructs the proposed system, providing strong evidence for the independent and combined contributions of TTA (Stage 1 and Stage 2) and DCE (Self-Reflection and Long-Range Guidance). The qualitative results, especially the step-by-step ablation in Figure 4, clearly visualize the impact of each component.\n\n+ The paper is exceptionally well-written and easy to follow. The core limitations of the baseline (FIFO-Diffusion) are explained with intuition (training-inference gap) and illustrated well (Fig. 2a). The proposed MIGA solution is logically broken down into its TTA and DCE components, each of which is explained with clear diagrams (Fig. 2b, Fig. A1) and, in the appendix, detailed pseudocode. The connection between the observed problem and the proposed solution is direct and compelling.\n\n+ Training-free methods for extending foundation models are of immense practical value, as they democratize access to powerful capabilities (like long video generation) without requiring massive computational resources for retraining. This paper provides a robust and well-engineered solution that demonstrably improves the state-of-the-art in a challenging domain. By solving key issues in AR frameworks, MIGA makes infinite-frame generation significantly more consistent and stable, pushing the boundaries of what can be achieved without additional training."}, "weaknesses": {"value": "- \"Infinite Generation\" Claim vs. TTA Stage 2: There appears to be a contradiction between the paper's claim of \"infinite-frame generation\" with \"constant memory\" (as inherited from FIFO-Diffusion) and the mechanics of the proposed TTA Stage 2. As described in Section 3.2 and Algorithm 5 (lines 16-26), Stage 2 collects all N partially denoised latents (where N is the total length of the final video) into a queue Q_gen. It then performs unified denoising steps on this entire queue. This implies that memory usage scales with N, the total number of frames to be generated. This is a fundamental departure from the streaming, constant-memory paradigm of FIFO-Diffusion, which generates one clean frame at a time. This re-introduces the very memory-scaling problem that AR frameworks were designed to solve. While the method is still \"autoregressive\" in Stage 1, the overall process seems to be for fixed-length (though long) video generation, not a truly \"infinite\" stream. This is a major weakness that needs to be clarified, as it affects the paper's core premise.\n\n- Missing Latency/Throughput Analysis: The paper focuses entirely on quality improvements (VBench/NarrLV scores) but provides no analysis of the computational overhead. The baseline FIFO-Diffusion is already computationally intensive. The proposed MIGA adds two new sources of overhead: TTA (specifically Stage 2, which processes all N frames e-1 times) and DCE (which involves a test-time search, as acknowledged in Appendix B.3). While Appendix B.3 discusses the cost, it provides no concrete numbers. A key part of evaluating a new method is understanding its trade-offs. How much slower is MIGA than FIFO-Diffusion? Is it 1.1x, 2x, or 10x slower? Without a \"Latency vs. Quality\" comparison (e.g., in Table 1), it's hard to judge the practical utility of the method. The gains in consistency may not be worth a massive drop in throughput."}, "questions": {"value": "- Memory Scaling of TTA Stage 2: Could the authors please clarify the apparent contradiction regarding the \"infinite\" and \"constant memory\" claims? Algorithm 5 suggests memory scales with N (total frames). The authors may explain the memory-management mechanism of Stage 2 that maintains constant memory.\n\n- Practical Latency Overhead: What is the wall-clock latency (or throughput in frames/sec) of the full MIGA (TTA+DCE) method compared to the baseline FIFO-Diffusion when generating a video of a fixed length (e.g., 128 or 161 frames, as in Table 1)? A concrete comparison is needed to evaluate the practical cost-benefit trade-off of the proposed mechanisms.\n\n- Effectiveness of \"Zigzag\" Denoising (TTA Stage 1): The \"zigzag\" denoising in Stage 1 is proposed to \"proactively narrow the noise span\". However, any sliding window of size f_0 (e.g., f_0=16) would still seem to cover at least two different noise levels (e.g., ...2, 2, 2, 3, 3...), giving a span of 2. The baseline FIFO-Diffusion has a span of f_0. Is the improvement (which is confirmed in Table 6) really just from going from a span of f_0 to a span of 2? Or is there another factor at play?\n\n- Robustness of Self-Reflection Proxy: The use of noisy-latent similarity as a proxy for clean-latent consistency (Fig. 3) is a key insight for the \"Self-Reflection\" component. How sensitive is this? The paper mentions it works even at high noise levels (e.g., 40/50). Does this proxy become more or less reliable at different noise levels? How was the judgment index f_judg (the noise step at which to perform this check) chosen?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bqtwCNiLDt", "forum": "oLbKaPoPzW", "replyto": "oLbKaPoPzW", "signatures": ["ICLR.cc/2026/Conference/Submission1842/Reviewer_cRYb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1842/Reviewer_cRYb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997174175, "cdate": 1761997174175, "tmdate": 1762915905068, "mdate": 1762915905068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}