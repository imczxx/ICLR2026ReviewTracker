{"id": "C35E46tK6T", "number": 25541, "cdate": 1758369017364, "mdate": 1759896716195, "content": {"title": "Timber: Training-free Instruct Model Refining with Base via Effective Rank", "abstract": "Post-training, which elicits a pretrained Base model into the corresponding Instruct model, is widely considered to be superficial.\nIn this work, we first reinforce this hypothesis by providing novel quantitative evidence from the weight level that the effective rank (eRank) remains negligibly changed. However, this superficiality also suffers a critical trade-off, improving the exploitation capabilities at the cost of limiting its exploration. To tackle this issue, we propose Timber, a simple yet effective training-free method that enhances the exploration capability of the Instruct model while preserving its exploitation. The key insight is to partially revert Instruct towards the paired Base model by subtle yet targeted refinement of the weight deltas. Extensive experiments on Llama and Qwen series demonstrate that Timber consistently improves vanilla Instruct models, particularly on Pass@k performance.  Our findings offer new insights into the post-training stage at the weight level and practical strategies to refine the Instruct model without training.", "tldr": "We propose a novel Timber, which is a training-free method to enhance Instruct model with paired Base model via effective rank.", "keywords": ["LLM", "training-free", "effective rank"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07b5488f2298a2f1080b6d74c85f46fc16a569d2.pdf", "supplementary_material": "/attachment/4d9d0091320e755b94d61d515520f9765d8dec06.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a method to interpolate weights between base and instruction-tuned language models to maximize exploitation (pass@k for low k, instruction-tuning is good at this) and exploration (pass@k at high k, base models are good at this). This involves interpolating in spectral space for the linear layers of the model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of getting \"best of both worlds\" is intriguing, and the method is simple and does not require re-training. \n- They study a few different variants which one might naturally envision, comparing them well thoroughly (-L vs full Timber, etc)."}, "weaknesses": {"value": "- Their evaluations and results are quite confusing and unconvincing. In particular: \n- Am I understanding correctly that Table 2 metric, Mean@k, is just the average one-shot accuracy over many trials, ie. E[pass@1]? If so, is the claim that Timber improves exploitation (pass@1) compared to the Instruct model? This is confusing because I thought the main claim was about balancing explore/exploit rather than pushing on one end of the spectrum alone. \n- If the claim is indeed that you push the pareto frontier on explore-exploit, this needs to be plotted clearly and directly. For instance, you deliberately seem to omit important baselines in Figure 5 that makes me suspicious. 1) Notice how you drop the small values of k in pass@k on the x-axis (if you're pushing the pareto frontier, Timber should be above the Instruct AND base at ALL values of k), and 2) notice how you don't plot the base model (which is the relevant baseline at large k). Of course Timber will beat Instruct at large k (by your assumption/statement that Instruct models have poor exploration), that's not the relevant comparison, the base model is. \n- You need a plot that spans all values of k and includes both base and instruct (which presumably intersect/cross at some critical k*) and to show that Timber remains above both of them throughout to make a claim about improving on both. I don't see this, and the strange x axes (which vary between the plots, even more confusingly) make me think that the results simply aren't that strong if you included the relevant baselines.\n- I appreciate the other plots are useful, eg. comparing Timber-L vs full Timber, comparing ablations like applying Timber to specific modules, showing that effective rank doesn't change much during instruction-tuning. This is all fine and great and clear, so good job. It's just that Table2/Fig5 seem to be the ones actually showing how well the method does compared to baselines, which is the most important thing at the end. So the fact that important baselines are omitted and/or the plots are quite unclear was a red flag for me. But I do recognize the other ablations are clear and well done. If you can make plots like the ones I outlined above, or clearly compare this to strong baselines and show it consistently outperforms (across model families and datasets) I would raise my score."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JapPTm4WRN", "forum": "C35E46tK6T", "replyto": "C35E46tK6T", "signatures": ["ICLR.cc/2026/Conference/Submission25541/Reviewer_WuUv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25541/Reviewer_WuUv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25541/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761601308904, "cdate": 1761601308904, "tmdate": 1762943467394, "mdate": 1762943467394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a method to combine base and instruct models, thereby improving performance on both exploitation (pass@1) and exploration (pass@k). The authors do so by removing part of the new instruct model, and replacing this with the base model. Such a method can help balance the capabilities of both models. The authors demonstrate that the proposed method can outperform existing instruct models across a variety of 7 datasets, demonstrating superior performance across values of $k$ for pass@k. The authors conclude by discussing relationships with other methods, such as truncated SVD and model merge."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper is well outside my domain of expertise, so I am not very confident in this review. Nonetheless: \n\n1. **Extensive Experiments** - The authors validate the performance of Timber with extensive experiments in Section 4. They compare against a variety of datasets, where they demonstrate that performance increases by ~1 point across datasets. Such improvements all come despite the need for training, highlighting the importance of their method. \n2. **Simple, Training-Free Method** - The method proposed by the authors is both simple and training-free, making it easy to use in practice. All that is required is access to the weights of the base and instruct models. From there, simple matrix operations allow for the computation of a new model which combines the best properties of both."}, "weaknesses": {"value": "1. **Requires more justification for whether this works** - While the proposed Timber method is simple and easy to implement, it is unclear why such a method should necessarily work. It is clear that the method extracts the most important components from base and instruct models, but there is no reason a priori that such methods should lead to the best of both worlds, improving both exploitation and exploration. While a fully rigorous justification might be difficult, it would be nice if the authors could shed more light on the intuition behind this with some justification for why it works. \n2. **Unclear what the impact on exploitation is** - In the experiments section, the authors largely investigate the impact on pass@k for various k, starting from ~50. However, one of the potential downfalls to combining instruct and base models is weaker performance on pass@1. It would be nice to see whether extending the graph to $k=1$ still leads to superior performance for Timber, or whether Timber leads to worse performance in the $k=1$ region."}, "questions": {"value": "1. What is the impact of incorporating Timber on Pass@1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "kCxqifPPmV", "forum": "C35E46tK6T", "replyto": "C35E46tK6T", "signatures": ["ICLR.cc/2026/Conference/Submission25541/Reviewer_ebkt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25541/Reviewer_ebkt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25541/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740196681, "cdate": 1761740196681, "tmdate": 1762943467180, "mdate": 1762943467180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates weight-level similarities between pretrained “Base” models and their finetuned “Instruct” counterparts. Using eRank, the authors show that linear layers in Instruct models have nearly identical effective rank to their Base counterparts. This suggests that instruction fine-tuning is superficial at the weight level, exploiting existing pretraining patterns without greatly increasing model capacity. However, this also creates an exploration–exploitation trade-off: Instruct models perform well on Pass@1 (first-answer accuracy) but produce less diverse outputs (lower Pass@k for larger k). To address this, the authors propose Timber, a training-free method that refines weights by algebraically operating on the difference between Base and Instruct model weights. Empirically, Timber is evaluated on Llama-3 and Qwen-3 models (0.6B–30B parameters) across benchmarks, including instruction-following, math reasoning, science QA, commonsense, and coding. Both variants, Timber-L and Timber, consistently improve over the original Instruct models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a novel analysis of instruction tuning. The use of effective rank as a diagnostic tool is insightful and clearly shows that Base and Instruct models maintain similar eRank distributions across layers.\n2. Timber is a simple, training-free method. It only requires computing SVDs of weight deltas and attenuating small singular values, without gradient updates or retraining.\n3. The paper is well-presented, with thorough experiments and reasonable comparisons. The empirical results consistently support the proposed method.\n4. The paper is clearly written and easy to follow."}, "weaknesses": {"value": "1. Although statistically consistent, the improvements are relatively moderate gain.\n2. Timber requires a paired base model for each instruct model. In many real-world cases, the exact base counterpart may not be publicly available (e.g. closed-source models or ensembles). It is not discussed how to handle mismatched architectures or when only the instruct model is accessible. This limits generality. This may limit its effectiveness.\n3. The attenuation factor in Timber is selected per model (via search on a held-out task). It can be difficult to choose such values.\n4. While described as low-overhead, applying Timber requires computing full SVDs of each linear layer’s weight-delta matrix (which can be very large). The paper does not quantify this cost.\n5. The experiments focus on reasoning and coding benchmarks (Pass@k-centric tasks). It is unclear how Timber affects other kinds of generation (e.g. open-ended conversation, summarization, translation)."}, "questions": {"value": "1. [Minor] Please explicitly state the meaning of the terminologies used in this paper, such as Pass@k, weight delta, etc.\n2. How stable are the results to the choice of α in Timber? The method relies on a tuned α per model. Have the authors evaluated performance across a range of α on held-out tasks?\n3. Computing SVDs on large weight matrices could be costly in memory and time. Did the authors measure the runtime or resource requirements of applying Timber to, say, a 30B-parameter model? Can this method be applied on-the-fly, or only as an offline preprocessing step?\n4. Timber was evaluated on reasoning and coding benchmarks. How does it perform on typical NLP tasks like summarization, Q&A, or dialogue?\n5. The paper reports Mean@k and Pass@k, but does Timber affect the quality of answers? For instance, does Pass@1 (first answer accuracy) ever drop?\n6. Why is the ceil(eRank) chosen as the cutoff? Is there any theory behind?\n7. Have the authors considered whether similar eRank-based refinement could apply to other model merge scenarios (e.g. mixing multiple experts or multimodal models)? Is the approach inherently limited to one base–instruct pair?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "McxP859RkU", "forum": "C35E46tK6T", "replyto": "C35E46tK6T", "signatures": ["ICLR.cc/2026/Conference/Submission25541/Reviewer_E9Zp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25541/Reviewer_E9Zp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25541/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926391086, "cdate": 1761926391086, "tmdate": 1762943466953, "mdate": 1762943466953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to discard or attenuate the lower singular values of the weight deltas between base and instruction tuned models.\nThresholding is done with effective rank. This reverts some loss of generation diversity due to instruction tuning and leads to better pass@k scores."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is simple and cost-efficient (training-free)."}, "weaknesses": {"value": "* There is no Related Work section or sufficient discussion in the main paper.\n\n* The main claim is that the method promotes output diversity which might be lost in instruction tuning. Simple relevant baselines are missing, e.g., sampling at higher temperature. More advanced baselines like [1,2] should also be compared to ideally.\n\n* Optimal attenuation factor depends on dataset (Figure 4), so it is not useful in practice, where an LLM developer would like to release one checkpoint, irrespective of dataset.\n\n* There is limited novelty apart from the attenuation of lower singular values, which is not that significant as described above.\n\n* The paper is imprecise and not well-written. See Questions section for details.\n\n[1] Pass@ k training for adaptively balancing exploration and exploitation of large reasoning models.\n[2] Planning in natural language improves llm search for code generation."}, "questions": {"value": "Questions / comments to improve the paper:\n* Title: \"Base via Effective Rank\" -> \"Base Model via Effective Rank\"\n* L029: NLP = Natural Language Processing (not Process)\n* L031: adapting -> adopting\n* L161: similar effective rank does not imply that singular subspaces are preserved\n* L175: \"by applying linear transformations among them\" is an unsound inference\n* L178-183: What is the point of this experiment and Figure 2?\n* L178-183: Showing the distribution of eRank-to-Rank ratios for weight deltas would make more sense.\n* L212: \"eRank is adept at preserving the majority of singular values\" -- not clear what this means\n* Table 2 / Section 4.2 (Main Results): What is k in Mean@k or Pass@k here? This is very important to interpret results.\n* Table 2 / Section 4.2 (Main Results): What would results be with k=1?\n* Table 2 / Figure 5: numbers / curves for base model are crucial to contextualize performance of Timber between Base and Instruct models.\n* Section 5.1 / 5.2: \"Discuss with\" -> \"Comparison with\"\n* L466-468: \"conclusion that FFN modules primarily store factual knowledge...\" this inference is not clear from the experiments.\n* References: Please cite conference version of papers if available, instead of arxiv preprints."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K8tK7Bi5Um", "forum": "C35E46tK6T", "replyto": "C35E46tK6T", "signatures": ["ICLR.cc/2026/Conference/Submission25541/Reviewer_JDKe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25541/Reviewer_JDKe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25541/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959005822, "cdate": 1761959005822, "tmdate": 1762943466773, "mdate": 1762943466773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}