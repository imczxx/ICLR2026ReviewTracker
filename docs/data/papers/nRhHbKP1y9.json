{"id": "nRhHbKP1y9", "number": 1909, "cdate": 1756963994127, "mdate": 1759898179406, "content": {"title": "Mimicking the Physicist's Eye: A VLM-centric Approach for Physics Formula Discovery", "abstract": "Automated discovery of physical laws from observational data in the real world is a grand challenge in AI. Current methods, relying on symbolic regression or LLMs, are limited to uni-modal data and overlook the rich, visual phenomenological representations of motion that are indispensable to physicists. This \"sensory deprivation\" severely weakens their ability to interpret the inherent spatio-temporal patterns within dynamic phenomena. To address this gap, we propose VIPER-R1, a multimodal model that performs Visual Induction for Physics-based Equation Reasoning to discover fundamental symbolic formulas. It integrates visual perception, trajectory data, and symbolic reasoning to emulate the scientific discovery process. The model is trained via a curriculum of Motion Structure Induction (MSI), using supervised fine-tuning to interpret kinematic phase portraits and to construct hypotheses guided by a Causal Chain of Thought (C-CoT), followed by Reward-Guided Symbolic Calibration (RGSC) to refine the formula structure with reinforcement learning. During inference, the trained VIPER-R1 acts as an agent: it first posits a high-confidence symbolic ansatz, then proactively invokes an external symbolic regression tool to perform Symbolic Residual Realignment (SR^2). This final step, analogous to a physicist's perturbation analysis, reconciles the theoretical model with empirical data. To support this research, we introduce PhysSymbol, a new 10,000-instance multimodal corpus. Experiments show that VIPER-R1 consistently outperforms state-of-the-art VLM baselines in accuracy and interpretability, enabling more precise discovery of physical laws.", "tldr": "", "keywords": ["Physics Formula Discovery", "Multimodal Scientific Reasoning", "Vision-Language Models"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a986d42d6fdab5877f969fb300f46f141466a24.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces VIPER, which combines visual information (from plotted graphs) with tabular trajectory data to train a VLM which reason to generate a symbolic equation to fit on the data. The paper also introduces a novel reward-guided training approach to learn on the given data, and use a refiner to learn the residual of the function that isn't modelled by the VLM. On a curated dataset, the method shows better performances compared to other VLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem is well-motivated, and the angle of using visual bias to \n\nThe writing is generally done well and easy to follow. Figs 1 and 3 give very clear overview of the method.\n\nThe experiments conducted are quite thorough, and the case analysis (App D) is quite interesting. If the space permitted I feel some examples from App D would have been worth adding to the main text for a better understanding of what the model is doing."}, "weaknesses": {"value": "A big negative point is the lack of experimentation against traditional symbolic regression (SR) benchmarks. In practice, a researcher could use traditional SR methods to fit the data, and possibly generate the reasoning ad-hoc. So even if these methods lack the reasoning capability by themselves, they should still be compared against to see how much better/worse it performs (even if SR only use one modality of data, to show that the other modality is actually important also). \n\nRelated, it feels a bit unfair to compare against traditional LLMs directly. From the experiments I assume that the other models in Table 1 are not fine-tuned on the same dataset (correct me if I am wrong -- but otherwise the exact methods the other MLMs are being tested and ran is unclear from the text and should be elaborated anyway), which may just suggest that VIPER performs better than others because it has seen the training data the others have not. \n\nIt would also be more convincing if the method can show generalisability to other settings or to examples outside the training data domain. It would be a stronger evidence for being able to perform reasoning, rather than just spotting existing patterns within the data. Similarly, I would like to see VIPER being tested on realistic data, which can make the motivation of real equation discovery stronger.\n\nTable 1 is missing confidence intervals -- difficult to make judgement on significance of trends. Additionally, it would be interesting to see the breakdown between each types of physics problems that are being tested on to see the limitations of the trained model.\n\nSome bits are a bit unclear still from the text. An example is how the reasoning chain example is generated and how they're exactly being trained on.\n\n[Minor point] The authors should define what acronyms LLM and VLM are in the paper for more completeness (this should take up like extra 6 words in the paper)."}, "questions": {"value": "1. In the residual realignment procedure, is the model able to tell when the residual it tries to fit on is just noise or whether there is actually data that should be learned from? In the case of noisy data, how would the model be able to reconcile the noise, or would it also fit on the noise as well?\n\n2. Regarding the portion on the curation of a new dataset -- how is this different from dataset which is already curated by LLM-SRBench? Is there a reason why the data from there is insufficient for the use case?\n\n3. How is the time and resources requirement for finding the equation for VIPER compared to other methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1UAP6Ne4JW", "forum": "nRhHbKP1y9", "replyto": "nRhHbKP1y9", "signatures": ["ICLR.cc/2026/Conference/Submission1909/Reviewer_PoUZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1909/Reviewer_PoUZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761297558690, "cdate": 1761297558690, "tmdate": 1762915943478, "mdate": 1762915943478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the VIPER-R1 framework, which aims to achieve automated discovery of physical laws by integrating visual perception (such as phase portraits and trajectory plots) with symbolic reasoning. The methodology involves a two-stage pipeline: first, hypothesis generation via Motion Structure Induction (MSI), followed by reinforcement learning-based optimization of the model's output through Reward-Guided Symbolic Calibration (RGSC). During inference, VIPER-R1 invokes an external symbolic regression tool to perform Symbolic Residual Realignment (SR^2), enhancing the consistency between hypotheses and empirical data. Experiments are conducted on the newly constructed PhysSymbol dataset, and comparisons with other Vision-Language Models (VLMs) are made based on structural matching and symbolic accuracy metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses an underexplored research area - multimodal physics formula discovery, attempting to establish connections between visual perception and symbolic reasoning. This direction distinguishes itself from methods relying solely on symbolic regression or pure textual reasoning approaches.\n\n2. A well-structured phased training pipeline is designed through the combination of MSI and RGSC. MSI serves for initial hypothesis generation, while RGSC performs structural optimization of the output via reinforcement learning. The introduction of SR² provides an adjustment mechanism to align theoretical models with empirical data.\n\n3. The paper establishes a systematic experimental foundation by constructing the PhysSymbol dataset containing various physical scenarios and proposing evaluation metrics such as structural score and accuracy score, providing both dataset resources and evaluation benchmarks for subsequent research."}, "weaknesses": {"value": "1. The core reinforcement learning framework of VIPER-RFT (the RGSC stage) shows significant similarity to Visual-RFT [1]. Both methods share common procedures:\n   • Generating multiple candidate responses from a policy model\n   • Employing rule-based reward functions (VIPER-RFT's structural reward vs. Visual-RFT's IoU/classification rewards) for output evaluation\n   • Optimizing policies through relative advantage normalization and KL regularization\n\n   Although VIPER-RFT customizes its reward function for physics formula structure, the high-level paradigm of \"verifiable reward-driven RL for multimodal tasks\" has been previously established by Visual-RFT, which somewhat diminishes the perceived innovativeness of the proposed framework.\n\n[1] Liu Z, Sun Z, Zang Y, et al. Visual-rft: Visual reinforcement fine-tuning. CVPR, 2025.\n\n2. The authors compare their fine-tuned VIPER-R1 (specifically adapted to the PhysSymbol dataset) against pre-trained, non-fine-tuned VLMs (e.g., GPT-4o, Gemini). This benchmark setup appears unfair since these base models lack task-specific adaptation. Given that large model fine-tuning is widely studied, a rigorous comparison should include:\n   • Other state-of-the-art VLMs fine-tuned on the same PhysSymbol dataset\n   • Ablation studies demonstrating the necessity of each component (MSI, RGSC) beyond simple baselines\n\n3. The PhysSymbol dataset is synthetic, comprising idealized trajectories and phase portraits. However, real-world physical data often involve noise, occlusions, and complex boundary conditions. The paper does not validate whether VIPER-R1's performance can generalize to noisy or real-world scenarios, raising concerns about its practical applicability.\n\n4. The SR^2 stage relies on external symbolic regression tools (e.g., PySR) for parameter refinement. This dependency may impact the method's reproducibility and scalability, particularly when the tools struggle with high-dimensional or noisy residuals. The paper lacks ablation studies analyzing the impact of different symbolic regression tools on final performance."}, "questions": {"value": "How are the weights (e.g., w_f, w_s, w_a) in the RGSC reward function determined? Is there experimental evidence demonstrating the dominant role of the structural reward (R_{\\text{structural}})?\n\nDoes the synthesis process of the PhysSymbol dataset consider real-world physical constraints (e.g., energy conservation)? How do you plan to extend it to real data in the future?\n\nIn the SR^2 stage, does the processing time of the symbolic regression tool become a bottleneck? How scalable is VIPER-R1 for complex systems (e.g., chaotic systems)?\n\nThe paper mentions that VIPER-R1 \"proactively invokes\" external tools during inference. Does this require manual intervention? What is the degree of automation of the framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F3d3u3FNjY", "forum": "nRhHbKP1y9", "replyto": "nRhHbKP1y9", "signatures": ["ICLR.cc/2026/Conference/Submission1909/Reviewer_5ST1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1909/Reviewer_5ST1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929120665, "cdate": 1761929120665, "tmdate": 1762915943232, "mdate": 1762915943232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VIPER-R1, a multimodal framework to discover physics formulas from the trajectory. It combines the visual information (like plots) with the trajectory data to address the \"sensory deprivation\" of current LLM-based methods. The model is trained via two stages and can act as an agent to combine with the SR tools to refine the initial hypothesis. Experiments show that the proposed method achieves the best in the authors' proposed benchmark PhysSymbol."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The method combines the visual perception and trajectory data with symbolic reasoning to discover formulas, addressing the sensory deprivation. The novelty is sound. \n2. The paper also proposed a dataset with 10k instances, PhysSymbol, beyond the model. The proposed dataset may inspire the community in physics formula discovery. \n3. The model reaches SOTA on the proposed PhysSymbol benchmark, surpassing all other close-sourced systems like GPT and Claude."}, "weaknesses": {"value": "- The paper's core contribution is ambiguous due to the lack of demonstrated generalizability. Although the proposed method seems to surpass all the models in the world, even GPT and Claude, the proposed VIPER-R1 is trained and validated on the self-proposed PhysSymbol benchmark. There is no evaluation on other benchmarks or real-world experimental physical data. So, for the method itself, the contribution is limited. The VIPER-R1 seems only to be the strong oracle baseline of PhysSymbol.\n- And this narrow experimental scope creates an awkward situation: As the main pages have few words on PhysSymbol, I think the authors try to convey that VIPER-R1 serves as the primary contribution. In this way, I think the contribution is not enough.\n- A more reasonable perspective is that the PhysSymbol dataset is the main contribution, or at least, has some words in the main pages, and VIPER-R1 just serves as a oracle model. However, if this is the case, the paper's structure is problematic. The dataset is only briefly mentioned in the main body, with critical details relegated to the Appendix (and still not enough to be treated as the full body of the main paper). This prevents the reader from fully evaluating the novelty and complexity of the benchmark in the main text."}, "questions": {"value": "For the teaser (Fig.1 left), the model predicts $2.36x - 2.83x^3 + 0.46620cos(t)$. And the SR2 refines the $0.46620cos(t)$ into $0.46620sin(t)$. Although it could have happened if the refined tools predict $-0.46620cos(t)+0.46620sin(t)$, it still seems to be a typo."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iy6yoEXDYR", "forum": "nRhHbKP1y9", "replyto": "nRhHbKP1y9", "signatures": ["ICLR.cc/2026/Conference/Submission1909/Reviewer_DAg8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1909/Reviewer_DAg8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000028187, "cdate": 1762000028187, "tmdate": 1762915942999, "mdate": 1762915942999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VIPER-R1, a vision-language model that “mimics the physicist’s eye” to discover governing equations by grounding symbolic reasoning in visual evidence plus motion data, addressing the uni-modal “sensory deprivation” of prior symbolic regression/LLM approaches. The training pipeline has two stages: Motion Structure Induction (MSI), which couples causal chain-of-thought supervision with supervised fine-tuning to induce the symbolic structure, and Reward-Guided Symbolic Calibration (RGSC), which uses reinforcement learning with a structural (parameter-agnostic) reward to purify the form of the law. At inference the model agentically calls an external symbolic regression tool for “Symbolic Residual Realignment” to fit remaining discrepancies. The authors release PhysSymbol, a 10,000-instance multimodal corpus spanning classical and broader dynamics with ground-truth equations and expert-style rationales. On this benchmark, VIPER-R1 outperforms strong VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and easy to follow\n2. Addresses a limitation of existing symbolic regression and LLM-based methods that ignore visual patterns\n3. Substantial improvements over SOTA VLMs across multiple metrics (structural score, accuracy, post-SR2 MSE)\n4. Comprehensive dataset contribution: PhysSymbol with detailed C-CoT annotations provides value to the community\n5. Thorough experimental evaluation: Multiple metrics, ablations, and detailed case studies with trajectory visualizations"}, "weaknesses": {"value": "* Limited scope and generalization: Training only on synthetic classical mechanics (2-5 terms) with clean trajectories\n* Missing computational analysis: No wall-clock time, training cost, or SR2 search time reported\n* Insufficient reproducibility details: Underspecified hyperparameters (GRPO batch size, reward weights w_f/w_s/w_a, SR2 configuration)\n* No failure mode analysis: Missing characterization of when/how model fails catastrophically or what happens when VLM generates incorrect structure"}, "questions": {"value": "* What happens on real experimental datasets with noise and incomplete observations?\n* How does performance degrade with equation complexity (more terms, higher-order terms, coupled systems)?\n* How does performance vary with C-CoT annotation quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XeAO3Gzn25", "forum": "nRhHbKP1y9", "replyto": "nRhHbKP1y9", "signatures": ["ICLR.cc/2026/Conference/Submission1909/Reviewer_Nuea"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1909/Reviewer_Nuea"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240591861, "cdate": 1762240591861, "tmdate": 1762915942482, "mdate": 1762915942482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}