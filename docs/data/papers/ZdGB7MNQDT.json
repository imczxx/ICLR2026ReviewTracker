{"id": "ZdGB7MNQDT", "number": 20197, "cdate": 1758303580604, "mdate": 1759896991525, "content": {"title": "GraphPlanner: Graph-Based Agentic Routing for LLMs", "abstract": "LLM routing has achieved promising results in integrating the strengths of di-\nverse models while balancing efficiency and performance. However, to support\nmore realistic and challenging applications, routing must extend into agentic LLM\nsettings—where task planning, multi-round cooperation among heterogeneous\nagents, and memory utilization are indispensable. To address this gap, we pro-\npose GraphPlanner, a heterogeneous graph-based agentic router that generates\nrouting workflows for each query and supports both inductive and transductive\ninference. GraphPlanner formulates workflow generation as a Markov Deci-\nsion Process (MDP), where at each step it selects both the LLM backbone and\nthe agent role (Planner, Executor, Summarizer). By leveraging a heterogeneous\ngraph, denoted as GARNet, to capture interactions among queries, agents, and\nresponses, GraphPlanner integrates historical and contextual information into\nricher state representations. The entire pipeline is optimized with reinforcement\nlearning, jointly improving task-specific performance and computational efficiency.\nWe evaluate GraphPlanner across 14 diverse LLM tasks and demonstrate that:\n(1) GraphPlanner outperforms strong single- and multi-round routers, improv-\ning accuracy by up to 9.3% while reducing GPU cost from 186.26 GiB to 1.04 GiB;\n(2) GraphPlanner generalizes robustly to unseen tasks and LLMs, exhibiting\nstrong zero-shot capabilities; and (3) GraphPlanner effectively leverages his-\ntorical interactions, supporting both inductive and transductive inference for more\nadaptive routing.", "tldr": "GraphPlanner is a graph-based framework that enables agentic LLM routing by modeling cooperation and memory with reinforcement learning, achieving scalable, efficient, and generalizable routing.", "keywords": ["Agentic LLM", "Memory utilization", "Heterogeneous agents"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b32b20d98d6be0bafda55c7a8200478b0624dbfd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel LLM routing paradigm, an agentic router setting. Unlike traditional routers, the proposed GraphPlanner not only selects the backbone LLM but also assigns specific agentic roles, e.g., planner, executor, or summarizer, to solve the initial query. The router is parameterized by GARNet, a graph-based model that captures the relationships between queries, agentic roles, and responses. GARNet is optimized using PPO with a joint loss that balances task-specific performance and computational costs, enabling adaptive router learning. Extensive experiments across 14 diverse tasks, spanning in-domain and out-of-domain settings, as well as inductive and transductive scenarios, demonstrate GraphPlanner's SOTA performance, exceptional balance of cost and performance, and strong generalization capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed agentic router setting is well-motivated and clearly articulated. While traditional single-round routers are limited to solving isolated queries and multi-round routers can only model sequential workflows, GraphPlanner addresses more complex graph-structured workflows that are under-explored in prior work.\n\n2. GraphPlanner is designed with sound principles, featuring a lightweight implementation and effective contextual history preservation. This ensures that the agentic router is simple to implement and extend. Additionally, the joint optimization of the router, combining task-specific loss and cost constraints, allows the model to learn policies tailored to solved task while maintaining efficiency.\n\n3. The experimental results are extensive, covering a wide range of downstream tasks in both in-domain and out-of-domain scenarios. The authors also include detailed cost-performance trade-offs and further analysis in inductive and transductive settings. These results highlight GraphPlanner's SOTA performance compared to single-round and multi-round routers, its robustness in inductive and out-of-domain tasks, and its favorable balance between performance and computational costs.\n\n4. The paper is well-written, with vivid figures and tables that significantly enhance comprehension."}, "weaknesses": {"value": "1. The description of Phase 1 optimization is somewhat unclear. For instance, in the case of Depth = 1 and Width = 3, does this imply a fixed agentic workflow where the first step always involves an initial planner role, followed by two roles (e.g., summarizer or executor)? Further clarification of this process would be helpful.\n\n2. The paper lacks illustrative examples showing how the router assigns roles to specific models to successfully solve queries. Including such examples would better demonstrate how GraphPlanner operates in practice.\n\n3. The experimented datasets appear relatively simple, as single-round routers already achieve good performance in some scenarios. It would be valuable to test GraphPlanner on more challenging benchmarks, such as agent-related benchmarks, which involve complex workflows and are better suited for role decomposition."}, "questions": {"value": "Please refer to Weaknesses part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o2t6C4H7zd", "forum": "ZdGB7MNQDT", "replyto": "ZdGB7MNQDT", "signatures": ["ICLR.cc/2026/Conference/Submission20197/Reviewer_SiqR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20197/Reviewer_SiqR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760691817687, "cdate": 1760691817687, "tmdate": 1762933700258, "mdate": 1762933700258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors proposed the GraphPlanner, which is a graph-based LLM router for agentic LLM flow prediction. Given a constructed workflow and history graph, the GraphPlanner predicts the best agent role and the LLM for the next step. by training the GNN model with PPO, it can generalize to unseen tasks and models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed methods show significant improvement over baseline methods under different scenarios.\n- The authors provide a comprehensive ablation study to prove the effectiveness of the proposed method."}, "weaknesses": {"value": "- The writing can be improved. In particular, I am pretty confused about the graph construction part.  Why are all nodes connected to the role hub node? Is there any particular consideration behind this design? For multi-round routing, will there be multiple role hub nodes? How are different rounds connected in the graph? Will the role hub node encode the role information of both the history graph and the workflow graph? I believe a more detailed illustration or figure is needed to allow the reader to better understand it.\n- For the agent role, the Graphplanner defined three different roles.  I am wondering what the rationale behind it is, and I am curious about whether the proposed methods can generalize to unseen roles after training. \n- What's the time cost for GraphPlanner compared to other baselines under different settings? What is the training time for GraphPlanner and other baselines?\n- The major contribution and the source of performance improvement come from the history graph. I am wondering, is it necessary to construct historical information into a graph? What if I simply describe all historical information to a (small )LLM and use the same training pipeline to optimize it?"}, "questions": {"value": "See above?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oEIbLJseY0", "forum": "ZdGB7MNQDT", "replyto": "ZdGB7MNQDT", "signatures": ["ICLR.cc/2026/Conference/Submission20197/Reviewer_3pJs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20197/Reviewer_3pJs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618468627, "cdate": 1761618468627, "tmdate": 1762933699765, "mdate": 1762933699765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GraphPlanner, a heterogeneous graph-based agentic router that extends LLM routing beyond static or multi-round settings into dynamic multi-agent coordination. The paper formulates workflow generation as a Markov decision process, where at each step both the LLM backbone and the agent role (Planner, Executor, Summarizer) are selected. A novel graph neural network, GARNet, integrates both the current workflow graph and historical interactions, enabling inductive and transductive inference. The proposed method is trained using proximal policy optimization. Experiments across 14 tasks and 6 domains show improved accuracy and reduced GPU cost over prior routers, with strong zero-shot generalization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **S1.** Novel formulation of LLM routing as a graph-based agentic workflow generation problem using an MDP framework. The reinforcement learning-based optimization adds a dynamic decision-making aspect missing in static routers.\n\n- **S2.** Integration of historical context through GARNet for inductive and transductive inference provides a principled way to leverage past interactions.\n\n- **S3.** Comprehensive evaluation across several tasks and domains. Demonstrated efficiency with substantial reductions in GPU usage and token consumption. Zero-shot generalization to unseen LLMs and tasks shows strong adaptability and robustness."}, "weaknesses": {"value": "- **W1.** The system fixes roles to Planner, Executor, Summarizer, which may constrain scalability to more complex or hierarchical workflows.\n\n- **W2.** The experiments fix depth = 1-2 and width = 2-3, which seems too limited for realistic multi-step tasks. \n\n- **W3.** The paper omits recent agent workflow generation systems (e.g., ADAS, AFlow, AgentSquare), which are directly relevant to the *workflow generation* claim in Table 3.\n\n\n- **W4.** Insufficient discussion of real-world deployment or integration with tool-based or API-based LLM ecosystems, despite the *agentic* framing."}, "questions": {"value": "- **Q1.** How would GraphPlanner perform when additional agent types (e.g., retriever, verifier) are introduced? Is the policy flexible enough to handle new roles dynamically?\n\n- **Q2.** Why are other graph encoders (e.g., GAT, GraphTransformer) not compared or ablated against GARNet?\n\n- **Q3.** What motivated the use of Longformer embeddings? Was long-context modeling empirically necessary?\n\n- **Q4.** Can the authors clarify what constitutes “existing LLM workflows” in Phase 1 evaluation (Table 1)? Were these synthetic or from real systems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PcR3ch6MiT", "forum": "ZdGB7MNQDT", "replyto": "ZdGB7MNQDT", "signatures": ["ICLR.cc/2026/Conference/Submission20197/Reviewer_E6qX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20197/Reviewer_E6qX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996952082, "cdate": 1761996952082, "tmdate": 1762933699368, "mdate": 1762933699368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}