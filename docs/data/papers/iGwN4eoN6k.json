{"id": "iGwN4eoN6k", "number": 6139, "cdate": 1757954189962, "mdate": 1763120157420, "content": {"title": "Predictive Embedding as Latent Action: Towards VLA Pretraining in the Wild", "abstract": "Vision-Language-Action models (VLAs) show promise for scalable robot learning, yet their progress is limited by small, narrow robot datasets. Human manipulation videos could provide richer learning material of skills, but current methods face a dilemma: either use expensive, precise labeled data with a limited scope, or abundant in-the-wild videos without hand tracking labels. We propose PELA, a pretraining framework that learns human motions by creating Predictive Embeddings that align with Latent Actions. Instead of trying to reconstruct every dynamics detail, PELA focuses on motion patterns that can be predicted from context and reflect real physical interaction. This creates a latent action space that captures motion dynamics across heterogeneous data sources. We build UniHand-Mix, a large hybrid dataset combining 5M carefully labeled lab recordings pairs with 2.5M pairs from in-the-wild human videos (7.5M total samples, >2,000 hours). This provides both reliable training signals and diverse real-world scenarios for large-scale learning. Our experiments show PELA generates realistic hand motions in both controlled and in-the-wild scenarios, and significantly improves downstream robot manipulation performance. The results demonstrate that predictive embeddings offer a practical route to scaling VLA pretraining using abundant human data.", "tldr": "", "keywords": ["Predictive Embedding", "VLA pretraining"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/2d5ce4c43b6c4b48c9ce52842f7926d9dab615ae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes PELA (Predictive Embedding as Latent Action), a pretraining framework that:\n- extracts predictive embeddings from mid-layer VLM features\n- aligns them to latent actions predicted by an inverse-dynamics module using boundary frames\n- reuses these embeddings in a flow-matching policy for robot control or hand motion prediction.\n\nThe data recipe mixes lab-annotated human-hand clips (with pose/motion labels) and in-the-wild videos (without labels). Two losses are used during pretraining: Masked Chunk Prediction (MCP) applied to lab clips, and an alignment loss applied to both lab and wild clips to construct a purportedly unified \"action-centric\" embedding space. The paper reports improved hand-motion generation and downstream control on LIBERO/RoboCasa, plus ablations on: (a) which embedding layer performs best for post-training, and (b) LIBERO performance vs proportion of in-the-wild data used during pretraining."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Relevant problem statement.** Aligning mid-layer predictive features to short-horizon latent actions (via boundary frames) tackles a limitation of pixel-reconstruction for fine hand motion: it provides an explicitly action-predictive signal to the backbone without requiring full future-frame synthesis.\n- **Supervision from in-the-wild videos.** The alignment mechanism enables learning from unlabeled human manipulation footage and can be applied at scale. This is also supported by ablations on LIBERO performance.\n- **Grounded latent-action learning.** When action labels are available, the method couples alignment with sequence prediction (MCP loss) so latent actions are anchored in actual motion trajectories. This differs from LAPA-style approaches that rely primarily on alignment to IDM targets without an explicit sequence-prediction objective on labeled data."}, "weaknesses": {"value": "- **Ablation fairness.** I think \"w/o align\" mode likely drops all in-the-wild supervision. The pretraining has two losses (MCP on lab, alignment on lab+wild videos). The \"w/o align\" setting seems to eliminate the only loss used on wild clips, effectively turning it into a lab-only run. This conflates loss removal with data removal, making it impossible to attribute gains specifically to alignment rather than to having extra wild data.\n- **No real-robot evidence.** All results are reported in simulation. For hand manipulation, practical deployment typically needs >15 Hz closed-loop control with low jitter. The paper does not report end-to-end inference latency, supported control frequency, or any on-robot trials. Without these, it is unclear whether the method is deployable for real robots/hands or only sim-viable.\n- **No alignment-only run.** It remains unclear whether alignment alone is a sufficient supervisory signal. Without MCP, do embeddings still encode motion-useful structure directly from videos?\n- **Alignment location is fixed during pretraining.** The paper ablates which embedding layer is best for post-training readout, but the alignment head is always attached to the same layer during pretraining. As a result, we can’t tell whether the gains come from where alignment shapes the backbone vs simply which layer is read out later. For instance, what would happen if alignment is done after the last transformer layer?\n-  **Representation claims lack analysis.** As lab samples receive MCP+alignment while wild samples receive alignment only, the unified space claim needs some analysis that wild embeddings encode motion rather than merely being dragged toward lab. For instance, would it work for zero-shot motion decoding on wild videos?"}, "questions": {"value": "Please address the points raised in Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VhbpOOOy0v", "forum": "iGwN4eoN6k", "replyto": "iGwN4eoN6k", "signatures": ["ICLR.cc/2026/Conference/Submission6139/Reviewer_E7mX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6139/Reviewer_E7mX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886396289, "cdate": 1761886396289, "tmdate": 1762918496754, "mdate": 1762918496754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "7Zi5fDJyHC", "forum": "iGwN4eoN6k", "replyto": "iGwN4eoN6k", "signatures": ["ICLR.cc/2026/Conference/Submission6139/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6139/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763120155995, "cdate": 1763120155995, "tmdate": 1763120155995, "mdate": 1763120155995, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PELA, a new framework for pretraining VLA models using large-scale human manipulation videos. Instead of reconstructing full visual dynamics as in prior latent-action approaches, PELA learns predictive embeddings that align with latent actions derived from inverse dynamics. These embeddings capture motion patterns that are predictable and physically meaningful, enabling learning from both labeled lab data and unlabeled in-the-wild videos."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper redefines how Vision-Language-Action (VLA) models can leverage human data by introducing predictive embeddings as latent actions, moving beyond traditional inverse- or forward-dynamics reconstruction paradigm. \n2. The UniHand-Mix dataset and the hybrid pretraining scheme are useful to the community.\n3. The paper is well-organized, beginning with clear motivation, followed by progressively detailed explanations of PELA’s architecture."}, "weaknesses": {"value": "1. Absence of real-world validation.\nThe paper lacks real-world robot experiments to verify that the proposed approach transfers effectively beyond simulation environments.\n\n2. Missing key comparisons and ablations.\nSeveral essential ablations are absent, making it difficult to substantiate the core technical claims:\n(1) Without LSP: Train the latent action perceiver end-to-end without the latent state perceiver, while keeping the masked chunk prediction and latent action alignment objectives. This would directly demonstrate the necessity of introducing LSP and the asymmetric EMA update.\n(2) Without annotated human videos: Pretrain the model solely with the latent action alignment loss, excluding labeled human data. This would clarify the contribution of annotated videos and illustrate why mapping to ground-truth actions remains important for latent action learning. (This is also a important distinction from LAPA method that do not use hand motion annotations)\n\n3. Lack of formal justification for predictive embeddings.\nWhile the intuition that predictive embeddings capture “motion patterns predictable from context” is appealing, it is neither theoretically formalized nor empirically analyzed. The paper does not provide clear evidence explaining why predictive alignment should generalize better than reconstruction- or masking-based objectives.\n\n4. Unanalyzed latent action space.\nThe claim that PELA constructs a “unified latent action space” across labeled and unlabeled videos is not supported by analysis. The paper provides no examination of what this space encodes or how well it aligns across domains such as lab, in-the-wild, and robot data.\n\n5. Questionable citation and authorship clarity.\nThe manuscript references a non-existent work (UniVLA Team. UniVLA: Unified vision-language-action model for cross-embodiment robotic learning. arXiv preprint, 2024a.), raising concerns about citation accuracy and the extent to which large language models may have been involved in the manuscript’s preparation.\n\n[Minor comment]\nThe reported performance improvements appear modest relative to existing baselines. In particular, according to Figure 3, the model underperforms Being-H0 when trained on the same 1,000+ hours of annotated laboratory data."}, "questions": {"value": "1. The paper claims that combining labeled and unlabeled human videos is crucial. Could the authors provide a variant trained only on unlabeled data using the latent-action alignment loss? How much does annotated data contribute quantitatively to downstream performance (e.g., LIBERO success rate)?\n2. Can the authors include or share results for an ablation that removes the Latent State Perceiver (LSP) and trains the Latent Action Perceiver end-to-end with the same losses?\n3. The paper states that PELA unifies labeled and unlabeled data in a shared latent space. Could the authors visualize or quantify this alignment?\n4. The reference “UniVLA Team, 2024a” appears to correspond to a non-existent paper. Could the authors clarify what work this refers to, or correct the citation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x1ONC9nDVi", "forum": "iGwN4eoN6k", "replyto": "iGwN4eoN6k", "signatures": ["ICLR.cc/2026/Conference/Submission6139/Reviewer_Mjen"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6139/Reviewer_Mjen"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890803591, "cdate": 1761890803591, "tmdate": 1762918496419, "mdate": 1762918496419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PELA (Predictive Embedding as Latent Action), a framework for scalable pretraining of Vision-Language-Action (VLA) models using both lab-collected and in-the-wild human videos. Instead of relying on heavy reconstruction losses, the authors propose learning predictive embeddings aligned with latent actions derived from inverse dynamics. This design aims to extract transferable motion cues without requiring explicit action labels for all data. The paper also presents UniHand-Mix, a large hybrid dataset (7.5M samples, ~2,000 hours) combining labeled human manipulation data with filtered egocentric videos. The experiments show consistent gains on both hand motion generation and robotic manipulation benchmarks such as LIBERO and RoboCasa."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear motivation and solid intuition: The paper tackles a well-known challenge in robot learning (limited high-quality robot data) and proposes a reasonable, well-motivated solution. The idea of “predictive embeddings” feels conceptually elegant and bridges the gap between labeled and unlabeled data in a meaningful way.\n\n- Large-scale hybrid data design: UniHand-Mix is an impressive contribution on its own, and combining precise lab recordings with diverse real-world videos is exactly what this community needs to move toward scalable embodied pretraining.\n\n- Strong empirical results: The model performs very well on both LIBERO and RoboCasa benchmarks, showing that the learned representations transfer effectively. The ablations are thoughtful — for example, the study on wild video ratios and hidden layer selection provides concrete insight into what drives performance.\n\n- Detailed technical presentation: The paper provides a clear formulation of the model, training objectives, and implementation details. It’s also well-structured and visually supported by helpful figures."}, "weaknesses": {"value": "- Questionable novelty at the conceptual level: The proposed framework is essentially a refined combination of existing ideas (latent action modeling, inverse dynamics alignment, and masked prediction). The incremental contribution is in the way these are fused under the “predictive embedding” umbrella.\n\n- Somewhat shallow theoretical grounding: The paper is heavy on empirical validation but light on analysis. It would help to include a clearer intuition (or a small-scale analysis) of why predictive embeddings outperform standard latent actions beyond empirical evidence.\n\n- Data quality concerns: The in-the-wild subset relies on automated annotation and filtering pipelines (e.g., Gemini-2.5-flash, HaWoR). There’s little discussion of the noise level or how label errors impact learning.\n\n- Generalization scope: Experiments are limited to hand-centric tasks. It’s unclear whether PELA generalizes to whole-body motion or broader robotic embodiments.\n\n- Scalability and efficiency: While presented as scalable, the method’s reliance on high-capacity backbones (InternVL3-2B, DINOv3, V-JEPA2) may restrict accessibility for smaller labs. Some efficiency comparison to Being-H0 or LAPA would make the contribution more convincing."}, "questions": {"value": "- How sensitive is the method to the choice of alignment layer (e.g., 19th)? Could this be learned or dynamically selected?\n\n- Does the asymmetric EMA update between LAP and LSP significantly impact stability? What happens if it’s removed?\n\n- Can predictive embeddings extend beyond hand motion — say, to full-body human videos or other action domains?\n\n- How do annotation errors in the in-the-wild subset affect training quality?\n\n- What’s the compute footprint compared to other 2B-scale VLA models like Being-H0 or UniVLA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "UxtwCLEwUi", "forum": "iGwN4eoN6k", "replyto": "iGwN4eoN6k", "signatures": ["ICLR.cc/2026/Conference/Submission6139/Reviewer_1uvT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6139/Reviewer_1uvT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935830119, "cdate": 1761935830119, "tmdate": 1762918495851, "mdate": 1762918495851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PELA, a pretraining framework for scalable Vision-Language-Action (VLA) models using both lab-annotated and in-the-wild human videos. The central idea is to move beyond full reconstruction of dynamics and instead align predictive embeddings (hidden states of a pretrained VLM) with latent actions derived from short-horizon visual transitions. The authors introduce two paired modules: Latent Action Perceiver (LAP) and Latent State Perceiver (LSP), that extract transition-level dynamics from boundary frames and context features, coupled via asymmetric EMA updates for stability. A new 7.5M-sample dataset, UniHand-Mix, combines labeled and unlabeled human manipulation videos for hybrid pretraining. The pretrained model is transferred to robot control tasks via a flow-matching head, achieving state-of-the-art results on LIBERO and RoboCasa benchmarks among sub-3B-parameter models."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper offers a novel latent-action formulation that connects inverse-dynamics-like latent supervision with predictive hidden states inside a VLM. The dual-Perceiver (LSP/LAP) design and EMA update mechanism are original contributions addressing instability when aligning heterogeneous modules. The hybrid data strategy and pretrain to flow-matching transfer pipeline are well-executed. The authors present ablations showing clear trends (e.g., scaling with in-the-wild data, layer selection effects), and comparisons against strong recent VLAs such as GR00T N1.5 and UniVLA demonstrate interesting gains."}, "weaknesses": {"value": "Clarification on Training:\nThe paper does not explicitly spell out which parameters are optimized concurrently during pretraining, e.g. whether predictive embeddings (VLM backbone) and LAP/LSP modules are co-trained end-to-end or updated under alternating schedules. The EMA coupling is described conceptually but lacks an algorithmic summary or pseudocode.\n\nAblation interpretation:\nThe w/o-alignment variant performs surprisingly well (85.8 % on LIBERO), close to the full model, suggesting either that MCP already encodes substantial motion priors or that alignment gains are task-dependent. Clarifying what fraction of unlabeled data contributes to the gap would help."}, "questions": {"value": "1. Are LAP and LSP co-optimized in the same update step with shared gradients, or alternately updated with EMA synchronization? Are VLM backbone parameters (from InternVL3-2B) fully trainable or partially frozen?\n\n2. The “w/o alignment” variant still performs strongly. Can the authors comment on what specific benefits alignment brings (e.g., improved robustness to in-the-wild data, faster convergence) beyond a small quantitative gain?\n\n3. Why do you believe latent actions derived from human hands transfer effectively to robot arms and grippers? Some supporting analysis or visualization would strengthen this claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Csjs095dly", "forum": "iGwN4eoN6k", "replyto": "iGwN4eoN6k", "signatures": ["ICLR.cc/2026/Conference/Submission6139/Reviewer_1Hgc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6139/Reviewer_1Hgc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6139/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762148801174, "cdate": 1762148801174, "tmdate": 1762918494875, "mdate": 1762918494875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}