{"id": "ZjidJlTOxd", "number": 21407, "cdate": 1758317242170, "mdate": 1763386332838, "content": {"title": "Stabilizing Reinforcement Learning for Honesty Alignment in Language Models on Deductive Reasoning", "abstract": "Deductive reasoning is the process of deriving conclusions strictly from the given premises, without relying on external knowledge. We define honesty in this setting as a model's ability to respond only when the conclusion is logically entailed by the premises, and to abstain otherwise. However, current language models often fail to reason honestly, producing unwarranted answers when the input is insufficient. To study this challenge, we formulate honest deductive reasoning as multi-step tasks where models must either derive the correct conclusion or abstain. We curate two datasets from graph structures, one for linear algebra and one for logical inference, and introduce unanswerable cases by randomly perturbing an edge in half of the instances. We find that prompting and existing training methods, including GRPO with or without supervised fine-tuning initialization, struggle on these tasks. In particular, GRPO optimize only for final task outcomes, leaving models vulnerable to collapse when negative rewards dominate early training. To address this, we propose Anchor, a reinforcement learning method that injects ground truth trajectories into rollouts, preventing early training collapse. Our results demonstrate that this method stabilizes learning and significantly improves the overall reasoning performance, underscoring the importance of training dynamics for enabling honest deductive reasoning in language models.", "tldr": "We propose a stabilization method for reinforcement learning with verifiable rewards that injects ground truth trajectories, improving honesty alignment on deductive reasoning in language models.", "keywords": ["Reinforcement Learning", "Verifiable Reward", "Honesty Alignment", "Curriculum Learning", "Language Models"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bbbe79dfac2be4b3ea58ffd8eb2ec0147f40f912.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the problem of abstaining giving an answer in deductive reasoning when the premises arent sufficient to answer a question. The authors show that untrained LLMs (albeit small LMs) are unable to do the task well as task complexity increases. The authors introduce ANCHOR; that adds a complementary SFT objective to GRPO to train models that can abstain."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation is clearly presented and the problem is relevant: Models should know when a question is unanswerable and rather than producing a confident incorrect answer\n- I like the set of tasks used in the paper to test the methods. Being able to systematically vary difficulty is a neat setup. Correspondingly, I like figs 1,2\n- The paper does a good job of evaluating existing models on their proposed tasks."}, "weaknesses": {"value": "- While the synthetic tasks provide a good knob to tune difficulty / complexity, I dont know how realistic the task setup is. The queries seem overly complicated, unnatural, and distant from how llms are used.  The unanswerable cases are created through edge removal etc. that may not reflect how unanswerable questions arise. Adding a more realistic / conventional task might help the paper with ecological validity.\n- My main concern are the counterintuitive GRPO+SFT results. Why does RL actively make the SFT’ed model worse? I also have concerns around the implementation of GRPO:\n    - Small group size of 5\n    - Very Off-policy: (global batch 1024, mini-batch 64, micro-batch 2)\n- The paper views honesty pretty narrowly as abstention.\n- ANCHOR requires access to ground truth trajectories during the RL phase which might be an unrealistic requirement.\n\n**Minor:**\n- place related work in the main draft\n- add examples of the task in the main, to improve clarity"}, "questions": {"value": "- How do stronger models perform on these tasks? Either open source frontier models or GPT-5 / Claude\n- How is the data for sft generated?\n- Could SFT+GRPO with a stronger KL penalty (e.g., 0.01 instead of 0.001) recover ANCHOR-like behavior? Since ANCHOR forces the model to be close to the SFT data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "agHhCsukTi", "forum": "ZjidJlTOxd", "replyto": "ZjidJlTOxd", "signatures": ["ICLR.cc/2026/Conference/Submission21407/Reviewer_YE4W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21407/Reviewer_YE4W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945454382, "cdate": 1761945454382, "tmdate": 1762941752028, "mdate": 1762941752028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Thank you to all reviewers for taking the time to evaluate our submission. We noticed several recurring misunderstandings and questions regarding our motivation, the scope of our work, and the nature of our contributions. To address these issues and to improve clarity, we have updated the Abstract, Introduction, and Related Work sections accordingly. We provide a consolidated explanation below.\n\nOur work is motivated by two central goals. First, we aim to design datasets for multi-step deductive reasoning in a way that reflects honest behavior: when a conclusion is logically derivable by the premises, the model should respond accurately, and when it is not valid, the model should abstain rather than fabricate information. Second, we aim to develop a method that can combine the advantages of GRPO and SFT, stabilizing policy gradient training while preserving the benefits of relative credit assignment. It is important that our contributions be understood through these two lenses, and we have revised our manuscript to make these motivations more explicit.\n\n1. A common question raised in the reviews is why we do not evaluate our method on real-world honesty-related datasets. Our research is specifically focused on honesty in deductive reasoning, which is proposed to isolate deductive structure from factual recall and other confounding factors. Realistic datasets tend to mix multiple skills: reasoning, domain knowledge, linguistic expectations, even stylistic cues, making it unclear what type of error a model makes when it fails. They could also introduce contamination risks because pretraining corpora may include similar examples. For this line of inquiry, synthetic datasets offer the crucial advantage of controllability: we can define tasks where the answerability is fully determined by the underlying graph structure, precisely manipulate difficulty and reasoning depth, and ensure clean separation between reasoning and knowledge. Our goal is not to improve performance on compositional realistic dataset benchmarks, but to understand and study one specific form of honesty: whether a model can recognize when premises are insufficient for a conclusion in multi-step deductive reasoning.\n\n2. Another question concerns our definition of honesty. Honesty includes two dimensions: (1) models should be aware of the limits of their own knowledge, and (2) they should recognize when a question is answerable from the provided information rather than fabricating conclusions. Our paper focuses on the second aspect, using deductive reasoning as a testbed where answerability is objectively defined and interpretations are unambiguous.\n\n3. Regarding originality, our contributions extend beyond a single methodological insight. First, we design controlled, multi-step deductive reasoning datasets specifically tailored for studying honest reasoning behavior. We analyze why existing approaches such as SFT, GRPO, and various patching strategies on them fail in this setting. We provide theoretical analysis showing how ANCHOR, injecting ground-truth reasoning trajectories, stabilizes policy gradient updates by mitigating gradient vanishing, and we demonstrate empirically that this approach leads to substantial improvements. To our knowledge, no prior work addresses all of these components together. The simplicity of our method is intentional, not a drawback: if a simple mechanism can resolve instability and enable models to learn long-range deductive patterns, introducing additional modules or constraints would add unnecessary complexity and overfit the datasets tested. We hope that readers will consider the full scope of our contributions rather than isolating one dimension and over-interpreting it.\n\n4. Why don’t we evaluate larger models or additional model families? (1) Our experiments are limited by available compute resources, and extending evaluations to substantially larger models would not offer a commensurate scientific benefit considering the cost. We would be happy to run such experiments if resources are available. (2) We chose Qwen models because they are currently the strongest open-source reasoning models, with performance profiles most appropriate for studying multi-step deductive reasoning. Other open-source families such as Llama or Mistral have not demonstrated comparable reasoning ability or maturity in this domain. If competitive reasoning-oriented models emerge from other families and have been thoroughly validated in other studies, we are eager to include them in future work."}}, "id": "LvnR0ZZfjx", "forum": "ZjidJlTOxd", "replyto": "ZjidJlTOxd", "signatures": ["ICLR.cc/2026/Conference/Submission21407/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21407/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21407/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763381797991, "cdate": 1763381797991, "tmdate": 1763381797991, "mdate": 1763381797991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of honesty alignment in LLMs—that is, ensuring models accurately indicate what they do and do not know. To overcome the scarcity of relevant data, the authors developed two new datasets. They evaluated three Qwen models on these benchmarks and observed that performance deteriorates as task complexity increases. Finally, the authors introduced *ANCHOR*, an approach that incorporates ground-truth trajectories into reinforcement learning algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. The study on the reasoning tasks of variying deductive difficulty is well-designed and provides good insights.\n3. The proposed datasets for honesty alignment are a good contribution to the community."}, "weaknesses": {"value": "1. The statement that \"when GRPO has all incorrect rollouts for one question, the so-called 'gradient vanish' problem will prevent any learning progress\" is somewhat debatable:\n    - The zero-advantage issue holds for a single question. However, in actual optimization, a mini-batch usually covers several questions, so having all zero advantages in a mini-batch is rare, and optimization can still proceed.\n    - The Adam/AdamW optimizer retains momentum, which allows optimization to continue even if a zero-gradient occurs rarely in a mini-batch.\n    - If the curriculum learning hypothesis—discussed in the paper—holds, GRPO will not completely prevent learning progress unless all questions are unanswerable. However, this may come at the cost of reduced training efficiency. The result of GRPO+Easy-to-Hard can provide some insight here.\n    - This issue pertains only to GRPO, not to all RL post-training techniques (e.g., PPO, ReMax). Discussion of this distinction is necessary.\n\n2. When forcing the ground-truth trajectory, the advantage of RL over SFT—namely, going beyond the dataset distribution—is likely diminished, as the LLM is being compelled to follow specific reasoning paths.\n   - There is a lack of comparison with SFT + Ground-Truth trajectories. SFT might be more efficient while sharing the same limitation.\n   - For larger LLMs (i.e., Qwen-3B), GRPO + Easy-to-Hard already achieves better performance, which may hint at the issue raised here.\n\n\n3.  The paper only conducts experiments on the small LLMs (0.6B, 1.7B, 3B). Considering the issue mentioned in W2, the results may not generalize to larger models (7B, 32B, 70B).\n \n4. In the evaluation, the authors claim that the tasks are deductive; i.e., the prompts are self-contained and provide all necessary information to answer the questions. However, this ignores the internal knowledge of the LLMs. What if the LLMs rely on their own knowledge to solve the task? How can we determine the behavior of the LLMs?\n\n5. The paper presents a variety of design choices, but each is not discussed in sufficient depth. In particular, the curriculum learning technique (Easy-to-Hard) appears to address most of the issues raised in the motivation—provided the base model is not too weak relative to the tasks. A more thorough discussion of the necessity and contributions of each individual design would strengthen the paper.\n\n> I personally think this work makes a valuable contribution and has notable strengths, although it also retains some unignorable weaknesses."}, "questions": {"value": "1. Need to clarify the LLM usage, as it is used as the target in research.\n2. Wrong results on Table 1? GRPO gets 1 on the unanswerable set and 0 on the answerable set."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rDvEjHQUqR", "forum": "ZjidJlTOxd", "replyto": "ZjidJlTOxd", "signatures": ["ICLR.cc/2026/Conference/Submission21407/Reviewer_n28i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21407/Reviewer_n28i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946201412, "cdate": 1761946201412, "tmdate": 1762941751775, "mdate": 1762941751775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ANCHOR (Augmented with Necessary Correct and HOnest Reasoning), a method that injects ground-truth reasoning trajectories into GRPO rollouts to stabilize reinforcement learning training for mathematical reasoning tasks. The authors introduce GRAPHLA, a dataset of multi-step linear algebra reasoning problems with answerable and unanswerable instances, and demonstrate that ANCHOR outperforms standard SFT and GRPO baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-motivated problem: The paper clearly articulates the instability issues in GRPO when negative rewards dominate, and the importance of honesty alignment in mathematical reasoning is well-established.\n2. Controlled experimental setup: The GRAPHLA dataset provides a clean testbed for studying deductive reasoning with formally verifiable answerability, avoiding confounds from factual knowledge.\n3. Clear presentation: The paper is generally well-written with clear motivation, methodology description, and experimental results."}, "weaknesses": {"value": "1. Critical Issue: Lack of Originality and Missing Related Work. The most significant problem with this submission is that the core contribution—injecting ground-truth/reference trajectories into GRPO rollouts to stabilize training—has been previously proposed and thoroughly investigated. Most notably: LUFFY (Learning to Reason Under Off-Policy Guidance) [Yan et al., 2025, arXiv:2504.14945] proposes essentially the same approach. Also, there is no comparison with related works in experiments.\n2. Limited Technical Novelty. This paper propose a straightforward approach: deterministically injecting one ground-truth trajectory per group is a natural and obvious extension of GRPO—much simpler than LUFFY's policy shaping mechanism.\n3. Limited scope: Evaluation is restricted to a single task domain (linear algebra) and relatively small models (≤3B parameters)"}, "questions": {"value": "See Limitation part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ktNCXqVtrj", "forum": "ZjidJlTOxd", "replyto": "ZjidJlTOxd", "signatures": ["ICLR.cc/2026/Conference/Submission21407/Reviewer_buC7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21407/Reviewer_buC7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986278554, "cdate": 1761986278554, "tmdate": 1762941751317, "mdate": 1762941751317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work investigates the LLM's ability to not only solve answerable problems but also to reliably identify and abstain from answering unanswerable ones through the lens of deductive reasoning. The authors introduce two novel datasets, GRAPHLA (linear algebra-based) and GRAPHLI (logical inference-based), featuring balanced instances of answerable/unanswerable questions. They propose ANCHOR (Augmented with Necessary Correct and HOnest Reasoning), which injects ground-truth trajectories into GRPO rollouts to stabilize reinforcement learning when all sampled responses are incorrect. The authors formally show that ANCHOR's gradient effectively adds a clipped, SFT-like term to the GRPO update, unifying supervised and reinforcement learning. Empirically, they demonstrate that ANCHOR successfully stabilizes training and outperforms baselines (SFT, GRPO, SFT+GRPO, and curriculum learning) on their new datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The diagnosis of the \"gradient collapse\" failure mode of GRPO in the context of honesty alignment is a novel and valuable contribution. And the ANCHOR method is simple, well-motivated.\n2. The heatmaps in Figure 1 demonstrate that baseline models fail at both reasoning and abstention as complexity increases."}, "weaknesses": {"value": "1. Both datasets are synthetic and highly structured. Real-world honesty alignment involves messier scenarios where unanswerability is not binary or deterministically verifiable. The clean graph structure may not capture the complexities of knowledge boundaries in practice.\n2. No comparisons to related works on stabilizing gradients, like Melo et al, 2025 [1] \n3. In many real-world scenarios, a single, verifiable ground-truth reasoning path is often unavailable, expensive to annotate. This is feasible for the synthetic datasets (GRAPHLA, GRAPHLI) created by the authors.\n4. Only Qwen models are tested. Experiments on other model families (Llama, Mistral, etc.) would strengthen claims.\n5. No convergence guarantees are provided. Under what conditions does ANCHOR converge?\n\n[1] Stabilizing Policy Gradients for Sample-Efficient Reinforcement Learning in LLM Reasoning https://arxiv.org/abs/2510.00819"}, "questions": {"value": "1. The tasks are extremely difficult even for humans (e.g., solving systems with 10+ variables, tracking 15-step logical chains). Models might fail not due to dishonesty, but simply because of insufficient capacity. Can this be possible?\n2. Process reward models that provide step-by-step supervision could be a strong baseline.\n3. Recent work on difficulty-aware RL (GRPO-LEAD) shows that careful curriculum design can significantly improve GRPO training. The learning results from the curriculum are presented as a limitation. Will techniques like this help? \n\n[1] GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models https://arxiv.org/abs/2504.09696"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2i0QhapXbo", "forum": "ZjidJlTOxd", "replyto": "ZjidJlTOxd", "signatures": ["ICLR.cc/2026/Conference/Submission21407/Reviewer_K8Nm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21407/Reviewer_K8Nm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055661619, "cdate": 1762055661619, "tmdate": 1762941751059, "mdate": 1762941751059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}