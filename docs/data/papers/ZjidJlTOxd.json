{"id": "ZjidJlTOxd", "number": 21407, "cdate": 1758317242170, "mdate": 1759896923576, "content": {"title": "Stabilizing Reinforcement Learning for Honesty Alignment in Language Models on Deductive Reasoning", "abstract": "Reinforcement learning with verifiable rewards (RLVR) has recently emerged as a promising framework for aligning language models with complex reasoning objectives. However, most existing methods optimize only for final task outcomes, leaving models vulnerable to collapse when negative rewards dominate early training. This challenge is especially pronounced in honesty alignment, where models must not only solve answerable queries but also identify when conclusions cannot be drawn from the given premises. Deductive reasoning provides an ideal testbed because it isolates reasoning capability from reliance on external factual knowledge. To investigate honesty alignment, we curate two multi-step deductive reasoning datasets from graph structures, one for linear algebra and one for logical inference, and introduce unanswerable cases by randomly perturbing an edge in half of the instances. We find that GRPO, with or without supervised fine tuning initialization, struggles on these tasks. Through extensive experiments across three models, we evaluate stabilization strategies and show that curriculum learning provides some benefit but requires carefully designed in distribution datasets with controllable difficulty. To address these limitations, we propose Anchor, a reinforcement learning method that injects ground truth trajectories into rollouts, preventing early training collapse. Our results demonstrate that this method stabilizes learning and significantly improves the overall reasoning performance, underscoring the importance of training dynamics for enabling reliable deductive reasoning in aligned language models.", "tldr": "We propose a stabilization method for reinforcement learning with verifiable rewards that injects ground truth trajectories, improving honesty alignment on deductive reasoning in language models.", "keywords": ["Reinforcement Learning", "Verifiable Reward", "Honesty Alignment", "Curriculum Learning", "Language Models"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8d602f00db15576713fce5ee8c70ee1b531a990c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the problem of abstaining giving an answer in deductive reasoning when the premises arent sufficient to answer a question. The authors show that untrained LLMs (albeit small LMs) are unable to do the task well as task complexity increases. The authors introduce ANCHOR; that adds a complementary SFT objective to GRPO to train models that can abstain."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation is clearly presented and the problem is relevant: Models should know when a question is unanswerable and rather than producing a confident incorrect answer\n- I like the set of tasks used in the paper to test the methods. Being able to systematically vary difficulty is a neat setup. Correspondingly, I like figs 1,2\n- The paper does a good job of evaluating existing models on their proposed tasks."}, "weaknesses": {"value": "- While the synthetic tasks provide a good knob to tune difficulty / complexity, I dont know how realistic the task setup is. The queries seem overly complicated, unnatural, and distant from how llms are used.  The unanswerable cases are created through edge removal etc. that may not reflect how unanswerable questions arise. Adding a more realistic / conventional task might help the paper with ecological validity.\n- My main concern are the counterintuitive GRPO+SFT results. Why does RL actively make the SFT’ed model worse? I also have concerns around the implementation of GRPO:\n    - Small group size of 5\n    - Very Off-policy: (global batch 1024, mini-batch 64, micro-batch 2)\n- The paper views honesty pretty narrowly as abstention.\n- ANCHOR requires access to ground truth trajectories during the RL phase which might be an unrealistic requirement.\n\n**Minor:**\n- place related work in the main draft\n- add examples of the task in the main, to improve clarity"}, "questions": {"value": "- How do stronger models perform on these tasks? Either open source frontier models or GPT-5 / Claude\n- How is the data for sft generated?\n- Could SFT+GRPO with a stronger KL penalty (e.g., 0.01 instead of 0.001) recover ANCHOR-like behavior? Since ANCHOR forces the model to be close to the SFT data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "agHhCsukTi", "forum": "ZjidJlTOxd", "replyto": "ZjidJlTOxd", "signatures": ["ICLR.cc/2026/Conference/Submission21407/Reviewer_YE4W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21407/Reviewer_YE4W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945454382, "cdate": 1761945454382, "tmdate": 1762941752028, "mdate": 1762941752028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of honesty alignment in LLMs—that is, ensuring models accurately indicate what they do and do not know. To overcome the scarcity of relevant data, the authors developed two new datasets. They evaluated three Qwen models on these benchmarks and observed that performance deteriorates as task complexity increases. Finally, the authors introduced *ANCHOR*, an approach that incorporates ground-truth trajectories into reinforcement learning algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. The study on the reasoning tasks of variying deductive difficulty is well-designed and provides good insights.\n3. The proposed datasets for honesty alignment are a good contribution to the community."}, "weaknesses": {"value": "1. The statement that \"when GRPO has all incorrect rollouts for one question, the so-called 'gradient vanish' problem will prevent any learning progress\" is somewhat debatable:\n    - The zero-advantage issue holds for a single question. However, in actual optimization, a mini-batch usually covers several questions, so having all zero advantages in a mini-batch is rare, and optimization can still proceed.\n    - The Adam/AdamW optimizer retains momentum, which allows optimization to continue even if a zero-gradient occurs rarely in a mini-batch.\n    - If the curriculum learning hypothesis—discussed in the paper—holds, GRPO will not completely prevent learning progress unless all questions are unanswerable. However, this may come at the cost of reduced training efficiency. The result of GRPO+Easy-to-Hard can provide some insight here.\n    - This issue pertains only to GRPO, not to all RL post-training techniques (e.g., PPO, ReMax). Discussion of this distinction is necessary.\n\n2. When forcing the ground-truth trajectory, the advantage of RL over SFT—namely, going beyond the dataset distribution—is likely diminished, as the LLM is being compelled to follow specific reasoning paths.\n   - There is a lack of comparison with SFT + Ground-Truth trajectories. SFT might be more efficient while sharing the same limitation.\n   - For larger LLMs (i.e., Qwen-3B), GRPO + Easy-to-Hard already achieves better performance, which may hint at the issue raised here.\n\n\n3.  The paper only conducts experiments on the small LLMs (0.6B, 1.7B, 3B). Considering the issue mentioned in W2, the results may not generalize to larger models (7B, 32B, 70B).\n \n4. In the evaluation, the authors claim that the tasks are deductive; i.e., the prompts are self-contained and provide all necessary information to answer the questions. However, this ignores the internal knowledge of the LLMs. What if the LLMs rely on their own knowledge to solve the task? How can we determine the behavior of the LLMs?\n\n5. The paper presents a variety of design choices, but each is not discussed in sufficient depth. In particular, the curriculum learning technique (Easy-to-Hard) appears to address most of the issues raised in the motivation—provided the base model is not too weak relative to the tasks. A more thorough discussion of the necessity and contributions of each individual design would strengthen the paper.\n\n> I personally think this work makes a valuable contribution and has notable strengths, although it also retains some unignorable weaknesses."}, "questions": {"value": "1. Need to clarify the LLM usage, as it is used as the target in research.\n2. Wrong results on Table 1? GRPO gets 1 on the unanswerable set and 0 on the answerable set."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rDvEjHQUqR", "forum": "ZjidJlTOxd", "replyto": "ZjidJlTOxd", "signatures": ["ICLR.cc/2026/Conference/Submission21407/Reviewer_n28i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21407/Reviewer_n28i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946201412, "cdate": 1761946201412, "tmdate": 1762941751775, "mdate": 1762941751775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ANCHOR (Augmented with Necessary Correct and HOnest Reasoning), a method that injects ground-truth reasoning trajectories into GRPO rollouts to stabilize reinforcement learning training for mathematical reasoning tasks. The authors introduce GRAPHLA, a dataset of multi-step linear algebra reasoning problems with answerable and unanswerable instances, and demonstrate that ANCHOR outperforms standard SFT and GRPO baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-motivated problem: The paper clearly articulates the instability issues in GRPO when negative rewards dominate, and the importance of honesty alignment in mathematical reasoning is well-established.\n2. Controlled experimental setup: The GRAPHLA dataset provides a clean testbed for studying deductive reasoning with formally verifiable answerability, avoiding confounds from factual knowledge.\n3. Clear presentation: The paper is generally well-written with clear motivation, methodology description, and experimental results."}, "weaknesses": {"value": "1. Critical Issue: Lack of Originality and Missing Related Work. The most significant problem with this submission is that the core contribution—injecting ground-truth/reference trajectories into GRPO rollouts to stabilize training—has been previously proposed and thoroughly investigated. Most notably: LUFFY (Learning to Reason Under Off-Policy Guidance) [Yan et al., 2025, arXiv:2504.14945] proposes essentially the same approach. Also, there is no comparison with related works in experiments.\n2. Limited Technical Novelty. This paper propose a straightforward approach: deterministically injecting one ground-truth trajectory per group is a natural and obvious extension of GRPO—much simpler than LUFFY's policy shaping mechanism.\n3. Limited scope: Evaluation is restricted to a single task domain (linear algebra) and relatively small models (≤3B parameters)"}, "questions": {"value": "See Limitation part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ktNCXqVtrj", "forum": "ZjidJlTOxd", "replyto": "ZjidJlTOxd", "signatures": ["ICLR.cc/2026/Conference/Submission21407/Reviewer_buC7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21407/Reviewer_buC7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986278554, "cdate": 1761986278554, "tmdate": 1762941751317, "mdate": 1762941751317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work investigates the LLM's ability to not only solve answerable problems but also to reliably identify and abstain from answering unanswerable ones through the lens of deductive reasoning. The authors introduce two novel datasets, GRAPHLA (linear algebra-based) and GRAPHLI (logical inference-based), featuring balanced instances of answerable/unanswerable questions. They propose ANCHOR (Augmented with Necessary Correct and HOnest Reasoning), which injects ground-truth trajectories into GRPO rollouts to stabilize reinforcement learning when all sampled responses are incorrect. The authors formally show that ANCHOR's gradient effectively adds a clipped, SFT-like term to the GRPO update, unifying supervised and reinforcement learning. Empirically, they demonstrate that ANCHOR successfully stabilizes training and outperforms baselines (SFT, GRPO, SFT+GRPO, and curriculum learning) on their new datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The diagnosis of the \"gradient collapse\" failure mode of GRPO in the context of honesty alignment is a novel and valuable contribution. And the ANCHOR method is simple, well-motivated.\n2. The heatmaps in Figure 1 demonstrate that baseline models fail at both reasoning and abstention as complexity increases."}, "weaknesses": {"value": "1. Both datasets are synthetic and highly structured. Real-world honesty alignment involves messier scenarios where unanswerability is not binary or deterministically verifiable. The clean graph structure may not capture the complexities of knowledge boundaries in practice.\n2. No comparisons to related works on stabilizing gradients, like Melo et al, 2025 [1] \n3. In many real-world scenarios, a single, verifiable ground-truth reasoning path is often unavailable, expensive to annotate. This is feasible for the synthetic datasets (GRAPHLA, GRAPHLI) created by the authors.\n4. Only Qwen models are tested. Experiments on other model families (Llama, Mistral, etc.) would strengthen claims.\n5. No convergence guarantees are provided. Under what conditions does ANCHOR converge?\n\n[1] Stabilizing Policy Gradients for Sample-Efficient Reinforcement Learning in LLM Reasoning https://arxiv.org/abs/2510.00819"}, "questions": {"value": "1. The tasks are extremely difficult even for humans (e.g., solving systems with 10+ variables, tracking 15-step logical chains). Models might fail not due to dishonesty, but simply because of insufficient capacity. Can this be possible?\n2. Process reward models that provide step-by-step supervision could be a strong baseline.\n3. Recent work on difficulty-aware RL (GRPO-LEAD) shows that careful curriculum design can significantly improve GRPO training. The learning results from the curriculum are presented as a limitation. Will techniques like this help? \n\n[1] GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models https://arxiv.org/abs/2504.09696"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2i0QhapXbo", "forum": "ZjidJlTOxd", "replyto": "ZjidJlTOxd", "signatures": ["ICLR.cc/2026/Conference/Submission21407/Reviewer_K8Nm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21407/Reviewer_K8Nm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055661619, "cdate": 1762055661619, "tmdate": 1762941751059, "mdate": 1762941751059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}