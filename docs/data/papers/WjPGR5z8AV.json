{"id": "WjPGR5z8AV", "number": 10741, "cdate": 1758180820104, "mdate": 1763018919144, "content": {"title": "AggLCF: Aggregation Enhanced Localized Conformal Factuality for Large Language Models", "abstract": "With the growing generative capabilities of large language models (LLMs) in question answering, their practical deployment is hindered by unreliable outputs. Conformal methods have been introduced to control sub-claim factuality with theoretical guarantees.\nIn particular, Conformal Factuality (CF) offers theoretical marginal guarantees controlling the overall error rate below $\\alpha$ via a global filtering threshold. Conditional Conformal (CC), aiming to improve information retention via conditional conformal, learns localized thresholds that optimize the number of retained sub-claims using a user-specific function class. However, the unstable local coverage limits its performance due to the sensitivity to the choice of function class, while significantly driving up the training cost from re-computation of conformal prediction on each gradient update.To address these issues, we propose a lightweight framework that offers Localized Conformal Factuality enhanced by multi-model Aggregation (AggLCF) with rigorous marginal coverage guarantees. By semantically clustering diverse responses from multiple LLMs and extracting structured features, AggLCF learns a localized threshold that empirically achieves $1 - \\alpha$ coverage per question while maximizing information retention. Without requiring fine-tuning or using any user-specific function class and re-computation, AggLCF outperforms the previous state-of-the-art in conditional conformal methods, and achieves both marginal and localized coverage on challenging inputs on the MedLFQA benchmark with the highest number of retained valid sub-claims.", "tldr": "", "keywords": ["Large Language Models", "Conformal Factuality", "Local Coverage", "Model Aggregation"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/4eba7ee4b42134e926c26eb88d5cd80c1e3f8da7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "* Proposes Aggregation-enhanced Localized Conformal Factuality for LLM long-form QA\n* Queries multiple LLMs in parallel and clusters all sub-claims by semantic similiary\n* Trains classifer on cluster level features to predict localized confidence threshold for conformal prediction that is adjusted per query to account for input difficulty\n* Experiments are conducted on several QA datasets to show better performance than several standard baselines to control sub claim factuality."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* The idea of multi-modal aggregation for conformal prediction is well-motivated and interesting; combines multiple LLMs to approximate truth oracle \n* The experiments seem to show improvement over baselines across datasets and target levels\n* Cluster features be offer interpretable representation for further downstream analysis"}, "weaknesses": {"value": "* The overall system is complex and not well justified. It is a \"kitchen sink\" approach with an exceptionally large number of components and hyperparameters. The pipeline includes: 10 different LLMs , GPT-4 for sub-claim decomposition , S-BioBERT for embedding , HDBSCAN for initial clustering , GPT-4 again for ClusterLLM correction , 17-D graph-Laplacian feature extraction , a CatBoost classifier for confidence scoring , and Engression for conditional density estimation. This high complexity makes the method difficult to reproduce, analyze, and practically deploy, and it is unclear which of these many components are essential.\n* High inference cost as need forward passes from multiple LLM, HDBSCAN for clusting, ClusterLLM corrections, GPT-4 API calls, extracting graph Laplacian features, running boosting/engression models. \n* The baseline comparison is unfair as they are only run on a single model (GPT-3.5 turbo). It is no suprise that a method with 10x more initial information can retain more valid sub-claims\n* The localized factuality claim is weak. The variance of miscoverage is high while the baselines have much lower variance. This does not support claim of \"robust local control\"\n* Labeling pipeline relies on GPT-4, which introduces bias into ground truth to determine if sub-claim is supported or refuted, which is problematic for guaranteeing validity of conformal guarantees. \n* Only evaluated on medical QA, unclear whether this framework would transfer to other domains, tasks, and settings. e.g. embedding model is S-BioBERT, pre-trained on biomedical data may not transfer to other datasets\n* Lack of strong baselines\n* Unclear causal link connection between aggregate feature choices and improvements in calibration performance."}, "questions": {"value": "* What is wall-clock time compared to baselines?\n* Could authors provide more ablation to justify each component of the proposed system? Which is the critical parts? What are the critical settings and hyperparameters? \n* How many LLMs are necessary? Which LLM models should be chosen? What happens if they are very similar or very different from each other? \n* I'm not convinced the exchaneability holds throughout the multiple steps (semantic clustering, re-labeling, feature graphs). How do you ensure the coverage guarantee remains intact? \n* Any stress tests where calibration/test come from different sub-datasets?\n* What happens when LLMs disagree? \n* What about human evals to judge which retained sets are more useful? Why not evaluate with humans to confirm actual utility for more useful or informative responses?\n* Will you release code, prompts, and cluster annotations to ensure reproducibility?\n* How to measure cluster purity  of correct and incorrect sub claims to guarantee factuality?\n* Did you measure label noise from LLM-annotation and how that would degrade performance is acc is lower? Can simulate experiments here. \n* What is the rejection rate of AggLCF? How often sub claim is filtered out from all models? \n* How is \"localized\" defined formally? Is it query-conditional?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "UlOwpTMKWf", "forum": "WjPGR5z8AV", "replyto": "WjPGR5z8AV", "signatures": ["ICLR.cc/2026/Conference/Submission10741/Reviewer_aVD4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10741/Reviewer_aVD4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761341484546, "cdate": 1761341484546, "tmdate": 1762921966628, "mdate": 1762921966628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Q51Sg9axBm", "forum": "WjPGR5z8AV", "replyto": "WjPGR5z8AV", "signatures": ["ICLR.cc/2026/Conference/Submission10741/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10741/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763018918183, "cdate": 1763018918183, "tmdate": 1763018918183, "mdate": 1763018918183, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AggLCF (Aggregation enhanced Localized Conformal Factuality), a framework for providing rigorous factuality guarantees for LLM outputs while maximizing information retention. The key innovation is aggregating responses from multiple LLMs and using localized conformal prediction to filter sub-claims with distribution-free marginal coverage guarantees."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Unlike prior work (CF, CC) that relies on single LLM outputs, AggLCF aggregates diverse responses from M=10 LLMs, potentially increasing the pool of valid sub-claims available for filtering.\n2. Theorem 3.1 proves marginal factuality control (P[error] ≤ α), maintaining the distribution-free coverage property of conformal prediction.\n3. More efficient than iterative methods (CC+CB), avoiding repeated conformal prediction recomputation."}, "weaknesses": {"value": "1. The paper claims (page 7) that their approach is \"(a) more hallucination-resistant (no self-assessment), (b) more robust to prompt/temperature variation (grounded in pairwise similarity and cluster geometry), and (c) more interpretable.\" None of these claims are validated experimentally. There are no ablations testing: Hallucination rates compared to self-assessment methods; robustness across different prompts or temperature settings, and user studies or interpretability evaluations\n2. High variance and inconsistent performance in Figure 1: AggLCF exhibits much larger standard deviation in retained sub-claims compared to all baselines, particularly evident in Figure 1(b). This suggests the method is unstable across trials. In addition, AggLCF appears to meaningfully outperform baselines only on HealthSearchQA (the largest dataset, n=3047). On smaller datasets (K-QA Golden, LiveQA), the error bars heavily overlap. In Figure 1(a), AggLCF actually shows higher miscoverage than CC on K-QA (Golden) and MedicationQA, contradicting claims of superior local control.\n3. The paper uses exactly 5 cluster-level features for the confidence score, but provides **no ablation on feature count**. How do we know 5 is sufficient or necessary? The 17-dimensional input feature vector Xi** (Appendix C) also appears arbitrary (3 graph types × 5 statistics + 2 indicators). No sensitivity analysis is provided.\n4. Figure 3 shows individual features perform poorly, indicating the score relies on complex, opaque feature interactions learned by CatBoost.\n4. While claiming to be \"lightweight,\" querying M=10 LLMs in parallel has non-trivial API costs ($$$) compared to single-model baselines and potential latency issues in production settings\n5. The scope of evaluation is limited. Only medical QA domain (MedLFQA) was tested, and there is no analysis of which LLMs contribute most to diversity (are all 10 necessary?). No ablation on number of models M (what happens with M=3, 5, 20?)\n6. Limited theoretical analysis of local coverage. Theorem 3.1 only provides marginal guarantees. While Figure 1(a) shows empirical local coverage, there's no theoretical characterization of when/why Engression-based conditional distribution estimation provides approximate local validity."}, "questions": {"value": "1. Why is the variance so high for AggLCF? This is a major practical concern. What causes the instability across runs?\n\n2. Is the improvement statistically significant? Can you provide confidence intervals or hypothesis tests showing AggLCF significantly outperforms baselines on K-QA, LiveQA, and MedicationQA (not just HealthSearchQA)?\n\n3. Why does AggLCF have higher miscoverage than CC on some datasets? This contradicts the claimed advantages.\n\n4. How many features are sufficient? Ablation needed on: (a) number of cluster features, (b) which cluster features are most important, (c) sensitivity to Xi feature extraction choices.\n\n5. What is performance with M=2, 3, 5 vs M=10? Is there a point of diminishing returns?\n\n6. Can you provide experiments showing hallucination-resistance, prompt/temperature robustness, and interpretability?\n\n7. What are the actual wall-clock time and API costs vs single-model baselines?\n\n8. Why does Engression outperform quantile regression? (Figure 4) Is this specific to your feature distribution or generally true?\n\n9. Can you test on other domains (e.g., legal, financial, general knowledge)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gz0C35hkey", "forum": "WjPGR5z8AV", "replyto": "WjPGR5z8AV", "signatures": ["ICLR.cc/2026/Conference/Submission10741/Reviewer_rQaz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10741/Reviewer_rQaz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761508888304, "cdate": 1761508888304, "tmdate": 1762921966190, "mdate": 1762921966190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AggLCF, a framework that enhances factuality calibration for long-form QA by combining multi-LLM aggregation with localized conformal thresholds. It collects answers from several LLMs, clusters semantically similar sub-claims, computes cluster-level confidence features, and learns a calibrated confidence estimator with CatBoost. A conditional quantile model is used to obtain localized thresholds with formal marginal coverage guarantees. Experiments on MedLFQA show improved factuality coverage and higher valid-claim retention compared to existing Conformal Factuality and Conditional Conformal methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of combining multi-LLM aggregation with localized conformal calibration is novel and practically meaningful for factual QA.\n\n2. The framework is well-structured, with clear modular design, theoretical guarantees, and detailed ablations (feature importance, α-sensitivity, etc.).\n\n3. The paper is clearly written, with transparent dataset splits, implementation details, and public reproducibility information."}, "weaknesses": {"value": "1.  AggLCF aggregates 10 models, while baselines use only one LLM. Some improvements may stem from richer candidate pools rather than calibration alone. A cost-matched comparison is needed.\n\n2.  Factuality labels rely on GPT-4o judgments without human validation. It would strengthen claims to include inter-rater or robustness analysis.\n\n3. Experiments are only on MedLFQA; generalization to open-domain or non-medical QA remains unclear.\n\n4. The cost and latency of multi-LLM aggregation are not reported, which limits practical assessment."}, "questions": {"value": "1. How does AggLCF perform under equal computational budgets or with a single-LLM setting?\n\n2. Can you show conditional or per-topic coverage curves to support the claim of “localized” reliability?\n\n3. How robust are results to different clustering algorithms or factuality evaluators (e.g., retrieval-based or human judges)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "avfzb1LO6H", "forum": "WjPGR5z8AV", "replyto": "WjPGR5z8AV", "signatures": ["ICLR.cc/2026/Conference/Submission10741/Reviewer_Fhoz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10741/Reviewer_Fhoz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844898395, "cdate": 1761844898395, "tmdate": 1762921965796, "mdate": 1762921965796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AggLCF (Aggregation-enhanced Localized Conformal Factuality) - a framework aimed at improving the reliability of large language models (LLMs) in question answering (QA) by leveraging multi-model aggregation and localized conformal prediction. Building on conformal prediction, which provides theoretical finite-sample coverage guarantees, the paper reviews two recent approaches: (1) Conformal Factuality (CF) provides marginal coverage guarantees using a global threshold but is overly conservative and ignores local variations, and (2) Conditional Conformal (CC) learns localized thresholds using a user-defined function class to retain more valid sub-claims but suffers from instability, high computational cost, and sensitivity to the chosen function class."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Introduces a multi-model aggregation mechanism to enhance response diversity.  Aggregates multiple LLM responses to the same question to enhance sub-claim diversity and robustness. Clusters semantically similar sub-claims and extracts structured features for localized threshold estimation.\n\nEvaluated on the MedLFQA benchmark, which emphasizes factual correctness in medical QA, an appropriate and challenging testbed. Experiments on the MedLFQA benchmark demonstrate that AggLCF achieves the highest number of valid retained sub-claims and superior factual coverage compared to CF and CC."}, "weaknesses": {"value": "Experiments are restricted to the MedLFQA benchmark, a specialized dataset. Broader evaluation on general-domain QA or multi-domain datasets is needed to establish the claim. \n\nThere is a lot of research on conformal methods for UQ and OOD (for example, \nVishwakarma, Harit, Alan Mishler, Thomas Cook, Niccolo Dalmasso, Natraj Raman, and Sumitra Ganesh. \"Prune'n Predict: Optimizing LLM Decision-making with Conformal Prediction.\" In Forty-second International Conference on Machine Learning; Su, Jiayuan, Jing Luo, Hongwei Wang, and Lu Cheng. \n\"Api is enough: Conformal prediction for large language models without logit-access.\" arXiv preprint arXiv:2403.01216 (2024); \n\"Polysemantic Dropout: Conformal OOD Detection for Specialized LLMs\" Gupta et. al. EMNLP 2025 https://arxiv.org/abs/2509.04655\nCampos, Margarida, António Farinhas, Chrysoula Zerva, Mário AT Figueiredo, and André FT Martins. \"Conformal prediction for natural language processing: A survey.\" Transactions of the Association for Computational Linguistics 12 (2024): 1497-1516.\nThe discussion of novelty with respect to the existing SOTA needs to be better discussed and empirically validated."}, "questions": {"value": "How does AggLCF ensure localized conformal coverage theoretically, given that it operates without explicit conditional quantile modeling or function class learning?\n\nHow sensitive is AggLCF to the choice and number of LLMs used in aggregation?\n\nCould you clarify how structured features are derived from aggregated sub-claims? \n\nHave you evaluated AggLCF in domains beyond medical QA, such as finance or law, to assess its adaptability to varying claim structures and factual baselines?\n\nHow does AggLCF compare with retrieval-augmented generation (RAG) or self-consistency-based approaches in factual coverage and efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rDFRTmH6NM", "forum": "WjPGR5z8AV", "replyto": "WjPGR5z8AV", "signatures": ["ICLR.cc/2026/Conference/Submission10741/Reviewer_SghF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10741/Reviewer_SghF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959027999, "cdate": 1761959027999, "tmdate": 1762921965066, "mdate": 1762921965066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}