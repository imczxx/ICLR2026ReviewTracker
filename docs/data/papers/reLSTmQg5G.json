{"id": "reLSTmQg5G", "number": 6479, "cdate": 1757986508576, "mdate": 1759897912340, "content": {"title": "LoRAGuard: An Effective Black-box Watermarking Approach for LoRAs", "abstract": "LoRA (Low-Rank Adaptation) has achieved remarkable success in the parameter-efficient fine-tuning of large models. The trained LoRA matrix can be integrated with the base model through addition or negation operation to improve performance on downstream tasks. However, the unauthorized use of LoRAs to generate harmful content highlights the need for effective mechanisms to trace their usage. A natural solution is to embed watermarks into LoRAs to detect unauthorized misuse. However, existing methods struggle when multiple LoRAs are combined or negation operation is applied, as these can significantly degrade watermark performance. In this paper, we introduce LoRAGuard, a novel black-box watermarking technique for detecting unauthorized misuse of LoRAs. To support both addition and negation operations, we propose the Yin-Yang watermark technique, where the Yin watermark is verified during negation operation and the Yang watermark during addition operation. Additionally, we propose a shadow-model-based watermark training approach that significantly improves effectiveness in scenarios involving multiple integrated LoRAs. Extensive experiments on both language and diffusion models show that LoRAGuard achieves nearly 100\\% watermark verification success and demonstrates strong effectiveness.", "tldr": "", "keywords": ["Watermarking", "LoRA", "Model ownership verification"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/87bf961ae4da540828215e7df16bcaa7cdfb95e7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes LoRAguard, a Yin-Yang-based watermarking technique - a black-box method that embeds a watermark without requiring alterations to the model's weights. \nThe main advantage of this watermarking technique is that is generic and can be easily applied without forcing the model's owner to perform an intesive re-training.\nThe work also tests the effectiveness of their technique by implementing it on language models and diffusion models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Watermarking is definitely an important topic, making the subject of this work relevant. \nThe proposed technique is lightweight and can be used without heavily modifying the weights of a pre-trained model. \nThis is particularly important for large models that cannot be fully retrained or altered, as it avoids wasting resources.\n\nStrenghts:\n\n- The proposed approach is black-box, making it easy to deploy.\n- Experimental evaluation demonstrates the applicability of this approach to different model types, including language models and diffusion models."}, "weaknesses": {"value": "- Security is purely heuristic, and no theoretical analysis of security is provided. Due to the fragility of watermarking, this makes the paper weak on this point.\n\n- The paper does not properly explain whether the watermarking scheme requires a secret key. Some sort of secret is required to have a robust security guarantee.\n\n- Several works have demonstrated (including from a theoretical perspective) that watermarking can be easily and generically removed. This work does not cite or compare itself against these attacks.\n\nOther comments:\n\nIn my opinion, the paper should improve its exposition on which watermarking scenarios it is trying to address. \nTo my understanding, the goal is to watermark the model itself rather than the model outputs. \nHowever, for verifying the watermark, the procedure requires submitting prompts/inputs to the target (likely in a black-box fashion) and analyzing its responses for the Yin-Yang watermark.\n\nBesides the fact that it is unclear whether verification requires a secret key, the paper misses important citations of works that theoretically explain why watermarking schemes are fragile and can be generically broken when embedded into outputs.\n\nFor example, [Zhang et al. ] demonstrates that performing a generic random walk over the output space of a watermarked model is sufficient to generate high-quality, unwatermarked outputs. \nRecently, [Francati et al.] demonstrated a similar generic attack at the bit level. Moreover, [Y] shows that crop-and-resize attacks can also effectively remove watermarks from images, aligning with the theoretical analysis.\n\nGiven these works, the paper should properly discuss how the proposed scheme may be resilient to such attacks. \nIn particular, a naive method leveraging [Zhang et al. ] or [Francati et al.] to generate unwatermarked outputs could be:\n\n- Generate a watermarked answer by running the LoRAguard-based model.\n- Before releasing the answer publicly, apply the procedure described in either [Zhang et al.] or [Francati et al.] to produce a similarly high-quality but unwatermarked answer.\n- Release the latter output.\n\nBased on the results of [Zhang et al. ,Francati et al.], the released outputs would likely not contain the watermark, making detection of the Yin-Yang watermark infeasible. \nThe paper should improve its evaluation by testing their technique against more malicious strategies (e.g., output-level attacks), including those described in [Zhang et al. ] and [Francati et al.], which also provide theoretical explanations of their inner workings."}, "questions": {"value": "- Can you clarify how the watermark is detected? Does it require making a query to the target model and observing its output? Is the query a specific prompt?\n\n- If detecting the watermark indeed requires observing the output, it is definitely important to test the proposed technique against the generic attacks proposed by [Francati et al.] and [Zhang et al.]. For more details, see the weaknesses section.\n\n[Francati et al.] https://eprint.iacr.org/2025/1620\n[Zhang et al.] https://proceedings.mlr.press/v235/zhang24o.html"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "KQRdS0Rac8", "forum": "reLSTmQg5G", "replyto": "reLSTmQg5G", "signatures": ["ICLR.cc/2026/Conference/Submission6479/Reviewer_qRvT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6479/Reviewer_qRvT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761152609320, "cdate": 1761152609320, "tmdate": 1762918858966, "mdate": 1762918858966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LoRAGuard, a black-box watermarking approach tailored to LoRA adapters. It targets two LoRA-specific challenges that make prior backdoor-style watermarking unreliable: composition with multiple LoRAs, which dilutes backdoor effects, and the use of negation operations, which can erase backdoors or flip behaviors."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The Yin–Yang construction is very simple and easy follow.\n2. The paper tries both LLMs and diffusion models, evaluates addition and negation, and varies key factors."}, "weaknesses": {"value": "1. It presumes the stolen LoRA is integrated into the same base model family and that owners can query the suspect system. Cross-base or cross-version behavior is not tested.\n2. It is still a backdoor watermark, which is detectable under careful forensic analysis. This watermark can be removed under heavy retraining or carefully designed purifications, and dependent on trigger rarity.\n3. There’s no formal analysis of query complexity, error rates, or optimal thresholds for black-box verification, which matters for real-world deployment.\n4. The employed LLM models are not sufficient. Add stronger baselines and broader experiments.\n5. Provide a clear black-box verification protocol with false positive/negative rates, number of queries, and thresholds."}, "questions": {"value": "Can you evaluate more removal/mitigation strategies relevant to LoRA (e.g., rank changes, reparameterization, quantization, cross-base integration, etc)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q5db57uGZp", "forum": "reLSTmQg5G", "replyto": "reLSTmQg5G", "signatures": ["ICLR.cc/2026/Conference/Submission6479/Reviewer_Dmqx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6479/Reviewer_Dmqx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570494281, "cdate": 1761570494281, "tmdate": 1762918858564, "mdate": 1762918858564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors propose a new **model** watermarking method used to trace the use of LoRA specifically. The main idea is to convert methods for backdoor into a method to identify a LoRA in a black-box setting through queries.\nThe stated goal of the authors is to solve this traceability problem under two threat scenarios:\n\n**Multi-LoRA Integration:** How to preserve the trigger effectiveness when the target LoRA is combined with other LoRAs via addition.\n\n**Negation Operation:** How to preserve the trigger effectiveness when a negation operation is applied to the LoRA, a technique used for tasks like unlearning or domain transfer.\n\nTo solve these problems, the authors propose to inject a backdoor made out of two component:\na so called \"yang\" component trained to be robust when added to other \"shadow LoRAs\" and a so-called \"yin\" component trained to be robust when the negation operation is applied.\n\nThe authors evaluate their method for a single old LLM (flan-t5) and a single diffusion model (a non specified Stable-Diffusion) under different attack scenarios such as fine-tuning, pruning and backdoor detection."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Given that the paper is fundamentally flawed in its method and experimental design (see weaknesses), I am unable to assess the strengths of the paper. \n\nFor what it's worth, the author do seem to have a produced a robust and effective backdoor method ! But this is not what is advertised or tested in the paper."}, "weaknesses": {"value": "**Lackluster experiments, low replicability**: Overall, the experimental study is insufficient. The method is evaluated on only 36 images generated by **watermarked** LoRAs and **0** (sic!) images generated by non-watermarked models. No comparison with another baseline is provided for comparison. Only one model per modality is used, and the stable diffusion model used is not even specified. Loras are trained in-house for Stable Diffusion on unspecified 10 images only. On the other hand, they are downloaded from Huggingface for Flan-T5, without giving good explanation for this choice. The author also use a subjective measure to decide on the success of their method (see after) which is highly unscientific.\n\n**Lack of clarity in the problem specification**: The author never formally define what they try to accomplish. Only the a vague \"We aim to trace the unauthorized misuse of LoRAs using watermark embedding\". The reader must infer that successful tracing means that the trigger successfully activates the backdoor. This already lacks clarity and should be specified in words in the papr. Secondly, even though the watermark success can be well defined for t5-flan (is the class output correct?), it is less clear how it should be defined for images.  The authors say \"A user study is conducted to assess WSR for Stable Diffusion\". This means that the presence of the watermark is decided on the subjectivity of a random user, not by an objective metric (e.g., a CLIP-score, a feature-space distance, or perceptual hash), which I find highly problematic. This leads me to the most important weakness.\n\n**Badly designed watermarking**: The authors assume success as soon as a single trigger successfully activates the backdoor on watermarked content.  This is very wrong. Since a probability of false-alarm is never  empirically quantified on non-watermarked LoRAs, we can't know if success is due to random chance or actual signal detection. The watermarking hypothesis testing problem is never formally defined, and as such, FPR and TPR metrics are naturally absent from the paper. Even worse, in the image case, the author propose no objectively measurable way to verify that the output of the trigger is what the model owner embedded.  In the current way the paper is written, it is unclear how a model owner would prove ownership using this method: since there is no counterfactual to the trigger's response (what is the expected image generated by \"rdc\" for a non watermarked model ?), how can i know this LoRA's response to this specific trigger actually demonstrate ownership of the model? The paper's \"proof\" of ownership relies entirely on the secrecy and assumed uniqueness of the (trigger, target) pair. The authors seem to miss a whole piece of the watermarking system, rendering the proposed method useless in the advertised setting."}, "questions": {"value": "Overall, I would respectfully ask the authors to review the entire paper after designing a correct formalization for the problem they are trying to solve. In its current state, this is not a watermarking paper, and the scientificity of its experiments is highly dubious. Specifically, I would advise the authors to:\n\n- Provide more meaningful experiments using more recent and relevant models, as well as using more samples to compute significant metrics.\n- Use objective measure to match the expected target with the trigger's response (e.g., a CLIP-score, a feature-space distance, or perceptual hash)\n- Formalize the watermarking problem as a hypothesis test and derive sound statistical measurement on the power and PFA of the test, in the same way as is done in all the papers the authors actually cite in the related works\n- Pay close attention to the replicability of their experiment\n- Why talk about content watermarking in the related works, when the authors actually only perform **model** watermarking?\n\n\nIn its current state, I can only recommend a strong rejection for this paper, and I doubt I could raise my score to an acceptance given how much work would be needed to match ICLR requirements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y6CWVtEEto", "forum": "reLSTmQg5G", "replyto": "reLSTmQg5G", "signatures": ["ICLR.cc/2026/Conference/Submission6479/Reviewer_N4E9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6479/Reviewer_N4E9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947776348, "cdate": 1761947776348, "tmdate": 1762918858266, "mdate": 1762918858266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of detecting unauthorized misuse of LoRAs by proposing LoRAGuard, a black-box watermarking technique. The method introduces two main contributions: (1) a \"Yin-Yang\" watermark design where the Yang watermark is triggered during addition operations and the Yin watermark during negation operations, and (2) a shadow-model-based training approach that integrates multiple unrelated LoRAs during watermark embedding to improve robustness in multi-LoRA scenarios. The authors evaluate their approach on Flan-T5-large and Stable Diffusion models, reporting nearly 100% watermark verification success rates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper identifies a relevant problem - tracing unauthorized LoRA misuse is indeed important as these models become more widely shared and deployed in various applications.\n2.\tThe experimental evaluation covers multiple aspects including effectiveness under different numbers of LoRAs, various weight parameters, and robustness against fine-tuning and pruning attacks.\n3.\tThe approach works across different model types (language models and diffusion models), demonstrating some generality.\n4.\tThe writing is generally clear in explaining the technical approach, and the figures help illustrate the main concepts."}, "weaknesses": {"value": "The paper tackles an interesting problem but falls short in several critical areas. The technical contribution feels incremental - essentially training two backdoors instead of one. While the shadow model training shows empirical benefits, the lack of principled justification makes it hard to understand when and why it works. The experimental evaluation, though covering multiple dimensions, remains limited in scope with only two base models tested. More concerning is the lack of comprehensive comparisons with existing watermarking methods and the incomplete treatment of adaptive adversaries. The stealthiness claims are overstated given the mixed results in Section 5.5. Details are as follows:\n1.\tThe core contribution essentially amounts to training two separate backdoors - one with positive weights (Yang) and one with negative weights (Yin). This is a relatively straightforward extension of existing backdoor-based watermarking methods rather than a fundamental innovation. The shadow model training, while helpful, also lacks deep technical insight into why this particular approach is optimal.\n2.\tThe evaluation is limited to only two base models (Flan-T5-large and Stable Diffusion). Given the diversity of model architectures and LoRA applications, this feels insufficient to make strong claims about the method's general effectiveness. More recent or diverse models (e.g., Llama, Mistral, SDXL variations) would strengthen the evaluation.\n3.\tThe paper primarily compares against BadNets and mentions one prior work (Liu et al., 2024b) but doesn't provide comprehensive comparisons with other watermarking or backdoor detection methods. The lack of ablation studies to justify design choices (e.g., why dropout over other regularization techniques) is concerning.\n4.\tThe Way2 approach of generating shadow LoRAs using random Gaussian noise based on statistics from other LoRAs seems ad-hoc. There's no theoretical or empirical justification for why this should produce meaningful shadow models. The fact that Way1 and Way2 perform similarly might actually suggest the shadow models aren't contributing much beyond regularization.\n5.\tThe threat model assumes adversaries only use simple addition/negation operations and doesn't consider more sophisticated attacks like adaptive adversaries who know about the watermarking scheme, or more complex LoRA composition methods. The fine-tuning defense evaluation uses only 1,500 samples, which seems limited.\n6.\tThe stealthiness experiments (Section 5.5) show mixed results but are presented somewhat optimistically. For instance, the RAP and ONION results show that detection is possible with reasonable FRR/FAR tradeoffs, which contradicts claims of strong stealthiness."}, "questions": {"value": "1.\tHow does the method perform on more recent and diverse model architectures? Have you tested on models like Llama-2/3, Mistral, etc?\n2.\tCan you provide theoretical justification or more detailed ablation studies on why the shadow model approach with dropout works? What happens if you use other regularization techniques?\n3.\tHow does computational cost scale with the number of shadow models used during training? What's the overhead compared to standard LoRA training?\n4.\tHave you considered adaptive attacks where adversaries know about your watermarking scheme and specifically try to remove the Yin-Yang watermark?\n5.\tThe transferability claim in Section 4.4 is interesting but underexplored. Can you provide more extensive evaluation of watermark transfer to other task-specific LoRAs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mYwKv9Ck69", "forum": "reLSTmQg5G", "replyto": "reLSTmQg5G", "signatures": ["ICLR.cc/2026/Conference/Submission6479/Reviewer_HEQD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6479/Reviewer_HEQD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762178067825, "cdate": 1762178067825, "tmdate": 1762918857916, "mdate": 1762918857916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}