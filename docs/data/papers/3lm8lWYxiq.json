{"id": "3lm8lWYxiq", "number": 8004, "cdate": 1758050720083, "mdate": 1763421400562, "content": {"title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs", "abstract": "Does continued scaling of large language models (LLMs) yield diminishing returns? In this work, we show that short-task benchmarks may give an illusion of slowing progress, as even marginal gains in single-step accuracy can compound into exponential improvements in the length of tasks a model can successfully complete. Then, we argue that failures of LLMs when simple tasks are made longer arise from mistakes in execution, rather than an inability to reason. So, we propose isolating execution capability, by explicitly providing the knowledge and plan needed to solve a long-horizon task. First, we find that larger models can correctly execute significantly more turns even when small models have near-perfect single-turn accuracy. We then observe that the per-step accuracy of models degrades as the number of steps increases. This is not just due to long-context limitations---curiously, we observe a self-conditioning effect---models become more likely to make mistakes when the context contains their errors from prior turns. Self-conditioning does not reduce by just scaling the model size. But, we find that thinking mitigates self-conditioning, and also enables execution of much longer tasks in a single turn. We conclude by benchmarking frontier thinking models on the length of tasks they can execute in a single turn. Overall, by focusing on the ability to execute, we hope to reconcile debates on how LLMs can solve complex reasoning problems yet fail at simple tasks when made longer, and highlight the massive benefits of scaling model size and sequential test-time compute for long-horizon tasks.", "tldr": "We measure long-horizon execution capability of LLMs, identify a failure mode where models self-condition on their own errors, and show benefits of increasing model size and sequential test-time compute", "keywords": ["Large Language Models", "Long Horizon", "Agents"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/892d77bc8fd82bbc1fe55714619ba607c25cdf81.pdf", "supplementary_material": "/attachment/827c95d15ad303bf2e0f3da164d6cee1a2269aae.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the scaling properties of Large Language Models (LLMs) on long-horizon tasks. The central thesis is that the perception of \"diminishing returns\" from scaling is an illusion created by short-task benchmarks. The authors argue that even small, diminishing gains in single-step accuracy can compound exponentially, leading to massive improvements in the total length of a task a model can successfully complete. To support this, the authors propose a distinction between \"reasoning/planning\" and \"execution.\" They argue that many failures on long tasks are not failures of reasoning, but of simple execution. They introduce a synthetic, multi-step key-value dictionary addition task designed to isolate this \"execution\" capability by providing the model with all necessary knowledge (the dictionary) and the plan (the keys to add). The authors conclude that scaling and sequential compute both provide massive, non-diminishing benefits for the long-horizon tasks that hold true economic value."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and the central arguments are clear to follow.\n2. The synthetic experiments are thorough. The design of the \"self-conditioning\" experiment, which injects errors into the model's history to prove a causal link, is particularly clever and provides clear support for this specific phenomenon."}, "weaknesses": {"value": "1.  The paper's entire set of conclusions about \"long-horizon execution\" is built upon an extremely synthetic and narrow task: repeatedly retrieving values from a dictionary and adding them to a running sum. The authors claim this isolates \"execution,\" but it is highly questionable whether this type of execution is representative of the complex, diverse, and adaptive execution required in real-world agentic tasks (e.g., navigating a new website, debugging code, or managing a multi-tool workflow). Real-world execution is not typically the N-th repetition of the exact same retrieve-then-compose operation. It involves varied actions, changing states, and error correction based on new observations. Because the task is so far removed from the real-world tasks it claims to model, it is impossible to know if the findings (especially \"self-conditioning\") would generalize. The paper's broad conclusions about the \"economic value\" of LLMs are therefore built on a very tenuous foundation. The limitations section acknowledges this, but I believe it is a more fundamental flaw that challenges the paper's entire premise."}, "questions": {"value": "1. If the problem is purely \"execution,\" why is the solution \"reasoning\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "18PSbydvQh", "forum": "3lm8lWYxiq", "replyto": "3lm8lWYxiq", "signatures": ["ICLR.cc/2026/Conference/Submission8004/Reviewer_f3Ec"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8004/Reviewer_f3Ec"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8004/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704847694, "cdate": 1761704847694, "tmdate": 1762920006834, "mdate": 1762920006834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies whether scaling LLMs or introducing thinking continues to yield benefits, for long-horizon tasks that requires many sequential steps. They design experiments with provided knowledge and plan to isolate execution and planning capabilities.\n\nTheir finds are:\n\n1. longer execution are more difficult\n2. larger LLMs significantly prolong the execution\n3. self-condition is the main reason aside pure long context\n4. larger LLMs do not affect self-condition\n5. thinking alleviates self-condition\n6. CoT is necessary\n7. GPT-5 > Claude 4 ~ Grok-4 > others"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and easy to follow.\n\n2. The idea is well motivated; isolating the long horizon from other factors makes sense.\n\n3. The experiment setting is well designed. (1) long horizon, knowledge, and execution are decoupled. (2) long context and self-condition are decoupled.\n\n4. Finds are valuable to the community."}, "weaknesses": {"value": "1. Is self-conditioning a bad behavior? From my understanding, a strong LLM with in-context reasoning **should** be self-conditioned.\n\nFor example, in this case:\n```\nOriginal number: 10\nSequence: [+3, -7, +1, +2]\nStep 1: 10 + 3 = 14\nStep 2: 14 - 7 = 8\nStep 3: 8 + 1 = 10\n...\n```\n\nIn this case, a better answer should be `Step 4: 10 + 2 = 13`\n\n2. The discussion about the relationship between self-conditioning and existing self-correction methods is necessary. The self-correction method includes agentic methods (self-refine and its following works) and reasoning behavior (aha moment)."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "e1PmWIlnpE", "forum": "3lm8lWYxiq", "replyto": "3lm8lWYxiq", "signatures": ["ICLR.cc/2026/Conference/Submission8004/Reviewer_mGG6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8004/Reviewer_mGG6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8004/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940015057, "cdate": 1761940015057, "tmdate": 1762920006390, "mdate": 1762920006390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the ability of LLMs to execute many-step plans or instructions in long-horizon tasks. While this consists of planning and then execution, the paper strips away the planning part via controlled tasks where the model is given detailed instructions and \"knowledge\", to only test the ability to execute. The paper studies three factors: (1) the impact of model strength/scaling, (2) the impact of correct executions of earlier steps in the context (called self-conditioning in the paper), and (3) the impact of thinking.\nFor (1), it starts with a simple formal analysis that assumes constant and independent accuracy per step, that indicates strong gains from improving accuracy per step. The paper itself later shows that the assumptions of this analysis are not always true. To obtain a controlled empirical setting, the work uses a simple lookup and prefix-sum task, where the \"dictionary\" is provided. Regarding (1), they find that first, even models that are very good at one lookup or addition step can perform much worse on the long-horizon task, possibly indicating problems with state tracking. The per-step accuracy diminishes with the length of the task. Second, still, better models indeed are better at long-horizon execution. For (2), the paper shows an experiment that helps distinguish between failure due to task length versus due to errors in the execution trace, by controlling the errors in the trace. All tested models are sensitive to the errors in their history. (3) For thinking models, this sensitivity appears to go away. The final experiment measures the influence of per-step complexity, and finds that without CoT or thinking, models cannot execute complex steps.\n\nOverall I thought this paper has some interesting findings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper shows interesting observations on the brittleness of LLMs to errors in context, and the increase in per-step error in long tasks, despite being able to execute single steps well.\n\n- The experiments use a controlled (albeit very simple task) setting with interesting setup. \n\n- Multiple models of different sizes are tested.\n\n- The paper is decently written."}, "weaknesses": {"value": "- The controlled task is rather simple, and it is a chain of always the same task (the paper mentions this already, though). I understand the controlled setting helps to get a controlled study, and that is also good. Possibly relating it to a more real-world task could have been nice.\n\n- Some things were not fully clear -- see my questions below.\n\n- it is possible that if the model designs a plan internally, it is different from the externally provided lookup and \"plan\". This is not a full weakness in that the model is not fully transparent, but it could be mentioned in the text.\n\n- In some points, the writing was not completely clear to me, e.g.:\n\nlines 87/88: \"the per-step error rate itself rises as the task progresses. This is in contrast to humans, who typically improve at executing a task with practice.\" The reasoning chain is usually not \"practice\", hence, I was not sure how this relates to humans practicing.\n\nlines 133/34: \"In fact, failures in execution have been misattributed to limitations in reasoning or planning capabilities\". If the model fails, how do you know it is planning correctly but failing to execute the plan? If you do not control for the plan like in this paper.\nThe description of \"turn accuracy\" compared to \"step accuracy\" is a bit confusing, as both use one state update.\n\nSec 3.1: the appendix explains the results with the difficulty of state tracking/management, whereas the main text talks about circuits. Why is the discussion split like that?"}, "questions": {"value": "- Sec 3: how exactly is correctness measured for the cunulative sum and addition? If e.g. the model makes a mistake in step 2, the result is wrong for the remaining steps. Is this counted as an error from step 2 onwards or just for step 2? Is this also how it is done for horizon length?\n\n- lines 260/261: where is the CoT in Figure 12?\n\n- You find that both state tracking and self conditioning are a problem. Do these interact, and if so, how (maybe I missed that)?\n\n- do Sec. 3.2 and 3.3 use the same task as Sec. 3.1? In Sec. 3.3, how exactly do you vary the turn complexity? A description and/or example would be helpful.\n\n\nOther minor comments:\n- legend of Figure 7b: both thinking and CoT color code look the same in my pdf\n- For proposition 1, add the independence assumption to the proposition statement. This may not hold in practice at all times, so good to mention it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YmZvik8TDN", "forum": "3lm8lWYxiq", "replyto": "3lm8lWYxiq", "signatures": ["ICLR.cc/2026/Conference/Submission8004/Reviewer_qfhB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8004/Reviewer_qfhB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8004/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944719496, "cdate": 1761944719496, "tmdate": 1762920005990, "mdate": 1762920005990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the idea that the common notion of \"diminishing returns\" on single-step tasks (as models are scaled up) should not be taken to mean that there is no value in pushing for those small increases, as these seemingly small increases can lead to an exponentially large benefit on long-range execution. In particular, the authors propose a synthetic tasks (of essentially computing \"prefix sums\" of a sequence of numbers revealed via a key-value mechanism) as well as setups/metrics to disentangle execution ability (the focus of this study) from the ability to properly plan and/or retrieve knowledge. With detailed studies on this benchmark, the authors provide evidence supporting their claims about diminishing returns being an illusion, LLM trends for long-horizon execution tasks, etc."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The question raised by the authors, namely whether the diminishing rate of improvement in single-step tasks is actually a real problem or not, is an interesting and practically valuable one. The authors' proposal of considering the max task execution length as a key metric is also of importance.\n\n* The study is clearly motivated and the empirical section is well-organized with clearly labeled setup and results section.\n\n* The authors raise a fairly large number of interesting questions and design specific experiments to test them.\n\n* The synthetic long-horizon execution task seems well-suited to the authors' desire to isolate away effects of knowledge retrieval and planning. The idea of treating planning and retrieval as \"key value\" problems is unique, and it allows the authors to \"provide\" the plan and knowledge at each step as appropriate.\n\n* Some of the results are quite surprising and creatively supported by the authors with explanation hypotheses. E.g., that single-step accuracy decreases with task length seems very unintuitive at first sight, but then the authors try out two hypotheses and find their combination to be a useful explanation of this phenomenon."}, "weaknesses": {"value": "* The synthetic task proposed here boils down to retrieving and summing up (all prefix subsequences of) $t \\times K$ numbers, where each number has 2 digits. While this task fits the authors' desire to isolate out planning and retrieval issues, it's **not exactly a sequential task** as referred to by the authors when discussing some experiments. This is important as, while sequential tasks are known to benefit from \"reasoning\" tokens or chain-of-thought, this is not really the case for adding $N$ numbers. Formally, this problem is well-known to lie in the complexity class $TC^0$ of *fully parallelizable* problems! This is a bit counter-intuitive, but happens to be true --- that one can add $N$ $N$-bit numbers completely in parallel in just a few steps. In fact, when a Transformer-based LLM compute attention over $N$ previous tokens, this sum is precisely what it computes. Thus, in a way, the synthetic task here isn't quite the perfect fit for what the authors really need. (An alternative task would be where 2-digit numbers are replaced with $5 \\times 5$ size *matrices*, and instead of sum, the task is to *multiply* the matrices; this is *not* known to be fully parallelizable, making it a better fit for some of the authors' narrative around CoT and reasoning tokens being helpful.)\n\n* That said, the experiments even with the proposed \"prefix sum\" task do seem to support the authors' claims in practice. I do find some details a little hard to believe/understand, though. For instance, Fig 7(a) seems to suggest that without CoT, strong models like Deepseek-V3 an Kimi-k2 can barely add 4-6 numbers of 2 digits each. I would have expected them to be able to handle longer sequences.\n\n* The discussion in section 2.1 (diminishing returns) feels not very convincing or deep. For one, Proposition 1 seems to follow directly from the observation that if each step *independently* succeeds (a strong assumption) with probability $p$, then $T$ steps will succeed with probability $s = p^T$. If this is the reasoning, it would be better to spell it out in the main text, as readers will readily connect with this common understanding of single-step errors compounding in multi-step scenarios.\n\n* Similarly, it is well-understood that single-step errors compound in multi-step uses. It would be helpful for the authors to discuss this. The common \"use\" of this observation is that a small *decrease* in single-step accuracy often leads to a large *drop* in multi-step accuracy. What the authors are saying is really the same statement, looked from the other direction: namely, a small *increase* in single-step accuracy often leads to a large *increase* in multi-step accuracy. These kinds of connections both help understand where the proposed observation is coming from and why it's reasonable. Yet, the discussion is not framed this way.\n\n* In the same section, in Fig 2, the claim that a small increase in step accuracy (x-axis) leads to a large increase in horizon length (y-axis) appears to really be only true when the slope of the curve is significantly larger than 1, which happens when step accuracy is already in the range of 90% to 100%. Note that for long-range tasks to be meaningfully solved, $H_{0.75}$ or $H_{0.9}$ are much more realistic measures than $H_{0.5}$. In other words, the exponential increase observation (with small increases in step accuracy) seem to \"work out\" only when single-step accuracy is already very high, in the 90%+ range. This seems important but is not discussed."}, "questions": {"value": "Could the authors comment on the points raised in the weaknesses section and how they might address it? I think the paper is thought-provoking and I am open to increasing my score contingent on the authors' willingness to address some of the points raised."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5GhUNK59NF", "forum": "3lm8lWYxiq", "replyto": "3lm8lWYxiq", "signatures": ["ICLR.cc/2026/Conference/Submission8004/Reviewer_42Ay"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8004/Reviewer_42Ay"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8004/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762212123749, "cdate": 1762212123749, "tmdate": 1762920005004, "mdate": 1762920005004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether the apparent diminishing returns from scaling LLMs are an illusion caused by evaluating them on short-horizon tasks. It identifies execution errors instead of reasoning failures as the main limitation in long-horizon task performance. The authors design a controlled synthetic setup that isolates execution by providing explicit plans and knowledge, enabling the measurement of how many steps a model can reliably execute. They discover that even marginal single-step accuracy improvements compound exponentially into longer horizon lengths, introduce the notion of self-conditioning and show that thinking models mitigate this effect. Experiments across models show non-diminishing gains from scaling."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper presents a well-structured experimental design using a keyâ€“value retrieval task.\n* The paper uncovers a new degradation source of models compounding their own mistakes, which is novel.\n* The authors evaluate multiple model families, showing consistent scaling trends."}, "weaknesses": {"value": "* The arithmetic-like setup may not capture the diverse error dynamics in natural environments. Extending to real-world planning or tool-use tasks could strengthen claims of generality.\n* The focus is primarily on model size and sequential test-time compute, without exploring complementary factors such as training curricula, architecture modifications, or memory-augmented mechanisms that may interact with long-horizon execution."}, "questions": {"value": "See suggestions in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YWq4IRZK5Y", "forum": "3lm8lWYxiq", "replyto": "3lm8lWYxiq", "signatures": ["ICLR.cc/2026/Conference/Submission8004/Reviewer_xdtj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8004/Reviewer_xdtj"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8004/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762891188410, "cdate": 1762891188410, "tmdate": 1762920004558, "mdate": 1762920004558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}