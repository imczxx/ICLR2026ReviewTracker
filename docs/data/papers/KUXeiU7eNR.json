{"id": "KUXeiU7eNR", "number": 19697, "cdate": 1758298514616, "mdate": 1759897025093, "content": {"title": "Absorbing Quantization Error by Deformable Noise Scheduler for Diffusion Models", "abstract": "Diffusion models deliver state-of-the-art image quality but are expensive to deploy. Post-training quantization (PTQ) can shrink models and speed up inference, yet residual quantization errors distort the diffusion distribution (the timestep-wise marginal over $\\vx_t$), degrading sample quality. We propose a distribution-preserving framework that absorbs quantization error into the generative process without changing architecture or adding steps.\n(1) Distribution-Calibrated Noise Compensation (DCNC) corrects the non-Gaussian kurtosis of quantization noise via a calibrated uniform component, yielding a closer Gaussian approximation for robust denoising.\n(2) Deformable Noise Scheduler (DNS) reinterprets quantization as a principled timestep shift, mapping the quantized prediction distribution $\\vx_t$ back onto the original diffusion distribution so that the target marginal is preserved.\nUnlike trajectory-preserving or noise-injection methods limited to stochastic samplers, our approach preserves the distribution under both stochastic and deterministic samplers and extends to flow-matching with Gaussian conditional paths. It is plug-and-play and complements existing PTQ schemes. On DiT-XL (W4A8), our method reduces FID from 9.83 to 8.51, surpassing the FP16 baseline (9.81), demonstrating substantial quality gains without sacrificing the efficiency benefits of quantization.", "tldr": "We propose a theoretically derived method to reduce quantization impact, which can be applied to diffusion and flow matching, and seamlessly integrate with a variety of other PTQs", "keywords": ["Diffusion Model", "Quantization", "Distribution-perserving", "Inference Effiency"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4b5e4f0fcd03b225c4a06131f2f149ea74c4463b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a post-training method to absorb quantization error in diffusion models with two main technologies: (1) Distribution-Calibrated Noise Compensation (DCNC). The authors point out that an assumption in previous work -- quantization error is Gaussian -- does not hold. Instead, they use DCNC to statistically correct the mismatch. (2) Deformable Noise Scheduler (DNS). The authors theoretically interpret the quantization noise as a timestep shift in diffusion process and use DNS to schedule the timesteps and compensate for the timestep shift."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The flow of the paper is smooth, from problem observation to the methods that precisely target the problems. \n2. The approaches are inspiring. I especially like the part reinterpreting the quantization noise with diffusion timestep shift. \n3. The mitigation of quantization noise kurtosis in Figure 5 is impressive."}, "weaknesses": {"value": "1. The evaluations need more clarification and improvement. \n2. Lacking comprehensive discussion compared to other outlier-aware methods. \n3. The quality of generated images is not substantially improved."}, "questions": {"value": "1. This paper tackles the quantization error problem -- however, it is anti-intuitive that the results are better than using 16 bits. This important to justify. Is it because quantization error is large even in 16 bits? What if using the plug-and-play approach on FP16 or BF16? \n2. Are the evaluation metrics reported averaged over sufficient number of generated images? Error bars should be included for more statistical insights. \n3. Mitigating kurtosis of quantization noise seems overlap with outliers handling, as both target distribution tails. Could you discuss and compare the two?\n4. Which timestep is Figure 4 at?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RzuINTkz43", "forum": "KUXeiU7eNR", "replyto": "KUXeiU7eNR", "signatures": ["ICLR.cc/2026/Conference/Submission19697/Reviewer_uUkK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19697/Reviewer_uUkK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761159616730, "cdate": 1761159616730, "tmdate": 1762931538634, "mdate": 1762931538634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author proposed a plug-and-play approach to improve the quantization\nperformance of diffusion models through introducing the distribution-Calibrated Noise Compensation (DCNC) that manages to calibrate the distribution to Gaussian, and Deformable Noise Scheduler (DNS) that is aware of the distribution shifts from the quantization. Together, the author claims that the approach preserves the distribution under ubiquitous samplers that fit most situations of multi-step diffusion models. This approach overcomes the limitations of current approaches (reshaping weight distributions, injecting approximated Gaussian noise to absorb residual errors) by ensuring the Gaussian assumption and distribution-awareness. The evaluation results indicate some improvements on strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. training-free, plugin-and-play\n\n2. Broadly applicable: works across UNet/DiT/FLUX (incl. deterministic / flow-matching setups) with light calibration and no architecture changes; tangible gains in low-bit regimes (e.g., W4A8)."}, "weaknesses": {"value": "1. The improvement seems incremental. The teasing image in Fig. 1 seems to give a rare indication of distributional preservation. In Fig. 2, the FID score of the method is higher than the baseline (22.64 v.s. 22.78).\n\n2. Missing metrics: for distributional matching tasks, LPIPS tells the perceptual distance and is also an important metric; the author should also report this metric in their paper. This metric has been exhaustively reported in SVDQuant, yet is missing in this article. The author should report this to strengthen their contribution.\n\n3. Limited robustness diagnostics: most component/variance-estimator ablations are on a small LDM-4 setup; sensitivity to $W_u$, variance estimator choice, and DNS mapping under different schedulers/backbones (DiT-XL, FLUX) isn't thoroughly characterized."}, "questions": {"value": "1. Is the timestep shift global or per-sample/timestep? Are there any failure cases where DNS degrades diversity or introduces artifacts?\n\n2. For DCNC, any theoretical/experimental justification for the Gaussian claim? Please report the distribution-bias measurement metrics, such as kernel inception distance (KID), across all backbones."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s0sNxUYzY2", "forum": "KUXeiU7eNR", "replyto": "KUXeiU7eNR", "signatures": ["ICLR.cc/2026/Conference/Submission19697/Reviewer_kcQd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19697/Reviewer_kcQd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761448706340, "cdate": 1761448706340, "tmdate": 1762931538026, "mdate": 1762931538026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposed a distribution-preserving approach that absorbs quantization-induced shifts directly into the sampling process of diffusion/flow models. It proceeds in two stages: \n(i) Distribution-Calibrated Noise Compensation, which adds a uniform component to the quantization residual to correct its heavy-tailed deviation; \nand (ii) Deformable Noise Scheduler reinterprets quantization as a fractional timestep shift, rewriting the noise schedule so the quantized conditional matches the full-precision marginal. \n\nThe method is plug-and-play and works with stochastic and deterministic samplers as well as flow-matching models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work is sufficiently innovative to me. The paper reframes PTQ for diffusion/flow models through a distribution-preserving lens, combining kurtosis-corrected residuals via a calibrated uniform component with treating quantization as a principled timestep shift, extending error absorption beyond stochastic sampling to deterministic samplers and flow-matching.  \n\n2. The results of this article are solid, proving the effectiveness of the proposed scheme. Almost all have achieved SOTA in the field of quantization without extra sampling steps, e.g. DiT-XL W4A8 FID 9.83→8.51, even surpassing FP16. \n\n3. The intuition of work seems reasonable to me."}, "weaknesses": {"value": "1. The connection and comparison with previous work is not clear enough.\n\n2. The method of correction does not have a strong theoretical foundation.\n\n3. No quantitative analysis of runtime was performed."}, "questions": {"value": "1. Please position more explicitly what is new beyond: (i) replacing Gaussian residuals with DCNC’s uniform-calibrated residual, and (ii) mapping to a fractional timestep and redesigning the schedule—ideally with a side-by-side derivation comparing your Eqs. (15–20) to PTQD’s update (Eqs. 9–10) and a conceptual table of assumptions/scope (stochastic vs. deterministic/flow).\n\n2. Uniform correction is heuristic; explore richer residual models. DCNC chooses a uniform component to cancel excess kurtosis (Eq. 13/Appendix A.1), which controls a single moment but ignores skew and higher-order structure.\n\n3. The paper states “no extra steps” and “no additional memory,” but there is no quantitative runtime profile for per-step cost or calibration cost. I'm expecting an end-to-end latency and memory for FP16/PTQ/PTQD/Ours on the same GPU, plus the calibration time once vs. reused across runs; include breakdowns to reassure deployability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OyEoMM19bU", "forum": "KUXeiU7eNR", "replyto": "KUXeiU7eNR", "signatures": ["ICLR.cc/2026/Conference/Submission19697/Reviewer_MnEb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19697/Reviewer_MnEb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663340557, "cdate": 1761663340557, "tmdate": 1762931537304, "mdate": 1762931537304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper addresses the challenge of post-training quantization (PTQ) for large diffusion models. Quantization reduces model size and speeds up inference but introduces errors that are particularly damaging to diffusion models, as these errors accumulate over the iterative denoising steps, leading to significant quality degradation.  \n- The authors identify a key limitation in prior work (like PTQD), which attempts to absorb quantization error as Gaussian noise but is restricted to stochastic samplers and fails for deterministic or modern flow-matching models (e.g., FLUX).  \n- The paper introduces a novel, plug-and-play framework with two core components:  \n  - Distribution-Calibrated Noise Compensation (DCNC): This component corrects for the fact that quantization error is not perfectly Gaussian but is often \"heavy-tailed.\" It analytically derives and adds a calibrated amount of uniform noise to the quantization residual, making its statistical properties (specifically, its kurtosis) much closer to a true Gaussian distribution.  \n  - Deformable Noise Scheduler (DNS): The paper shows that the effect of quantization on the model's output distribution can be interpreted as a timestep shift in the original diffusion process. Instead of adding extra noise, DNS dynamically adjusts the noise schedule (the $\\alpha$ values) for the quantized model so that its output distribution at each step aligns with the distribution of the original, full-precision model at a different, \"shifted\" timestep. This preserves the original generative distribution.  \n- The method is plug-and-play enhancement approach that works with existing PTQ techniques (e.g., Q-Diffusion, PTQ4DiT, SVDQuant) across various model architectures (LDM, DiT, FLUX.1). It consistently improves quantitative metrics without adding inference latency or memory overhead. Crucially, it works for both stochastic (DDPM) and deterministic (DDIM, Flow Matching) samplers."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written.  \n- The core idea of reinterpreting quantization error as a timestep shift is innovative and provides a principled, unified framework for the problem. It moves beyond heuristic corrections to a distribution-preserving solution.  \n- A major strength is its compatibility with various sampler types (stochastic, deterministic) and model frameworks (diffusion, flow-matching).  \n- The method requires no retraining (PTQ), adds no inference overhead, and can be seamlessly integrated with existing quantization methods.  \n- The paper provides extensive experiments on multiple model backbones (LDM-4, DiT-XL, FLUX.1), bit-precisions (W8A8, W4A8, W4A4), and under different sampling settings (steps, guidance scales).  \n- The evaluation of the approach shows promising results, outperforming other state-of-the-art works."}, "weaknesses": {"value": "- The method requires a one-time calibration step involving a dataset (3,000 images in the paper) to estimate the noise statistics.  \n- The method introduces a tunable weight \\$W_u\\$ for the uniform correction. While the paper selects a value of 0.2, its optimal value might be dataset- or model-dependent, requiring minor hyperparameter tuning for best performance.  \n- Missing comparison to [1]-[5].  \n- Minor typo at line 262 (\"is\" is written twice).  \n  \n[1] TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models (https://arxiv.org/pdf/2311.16503).   \n[2] Post-training Quantization on Diffusion Models (https://openaccess.thecvf.com/content/CVPR2023/papers/Shang_Post-Training_Quantization_on_Diffusion_Models_CVPR_2023_paper.pdf).  \n[3] PQD: Post-training Quantization for Efficient Diffusion Models (https://arxiv.org/abs/2501.00124).  \n[4] Towards Accurate Post-training Quantization for Diffusion Models (https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Towards_Accurate_Post-training_Quantization_for_Diffusion_Models_CVPR_2024_paper.pdf).  \n[5] BiDM: Pushing the Limit of Quantization for Diffusion Models (https://proceedings.neurips.cc/paper_files/paper/2024/file/44b61c5c0ba06d55ab5a1cfb9cfff763-Paper-Conference.pdf)."}, "questions": {"value": "How does this method perform when compared to the references mentioned in the weaknesses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NeOvkkGMJ4", "forum": "KUXeiU7eNR", "replyto": "KUXeiU7eNR", "signatures": ["ICLR.cc/2026/Conference/Submission19697/Reviewer_heeP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19697/Reviewer_heeP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987645568, "cdate": 1761987645568, "tmdate": 1762931536871, "mdate": 1762931536871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}