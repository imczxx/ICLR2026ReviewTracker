{"id": "aJJppqAm6r", "number": 13980, "cdate": 1758226429444, "mdate": 1763718524966, "content": {"title": "True Self-Supervised Novel View Synthesis is Transferable", "abstract": "In this paper, we identify that the key criterion for determining whether a model is truly capable of novel view synthesis (NVS) is transferability: Whether any pose representation extracted from one video sequence can be used to re-render the same camera trajectory in another. We analyze prior work on self-supervised NVS and find that their predicted poses do not transfer: The same set of poses lead\nto different camera trajectories in different 3D scenes. Here, we present XFactor, the first geometry-free self-supervised model capable of true NVS. XFactor combines pair-wise pose estimation with a simple augmentation scheme of the inputs and outputs that jointly enables disentangling camera pose from scene content and facilitates geometric reasoning. Remarkably, we show that XFactor achieves transferability with unconstrained latent pose variables, without any 3D inductive biases or concepts from multi-view geometry — such as an explicit parameterization of poses as elements of SE(3). We introduce a new metric to quantify transferability, and through large-scale experiments, we demonstrate that XFactor significantly outperforms prior pose-free NVS transformers, and show that latent poses are highly correlated with real-world poses through probing experiments.", "tldr": "The key criterion for determining whether a models is capable of NVS is transferability, and we present the first fully geometry-free and self-supervised model capable of it.", "keywords": ["Novel View Synthesis", "Self-Supervised", "Unsupervised", "Representation Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed211d63956e3d61d787d5af9a3a2883c0c97f32.pdf", "supplementary_material": "/attachment/7a1b35f238ba3cbe2af8cf4e9f928984818bb0e8.zip"}, "replies": [{"content": {"summary": {"value": "This paper aims to define \"true\" self-supervised novel view synthesis (NVS) by introducing transferability as the key criterion. The paper defines transferability as the ability for a pose representation extracted from one video to render the same camera trajectory in a different scene. The paper further argues that prior generalizable, self-supervised NVS methods fail this test, as they learn to interpolate context frames rather than reason geometrically.\n\nTo solve this challenge, the paper presents XFactor, a model claimed to be \"geometry-free\" and capable of this \"true\" NVS. XFactor's design is based on two main ideas: 1) bootstrapping from a stereo-monocular (two-view) model to force extrapolation rather than interpolation, and 2) a new transferability objective that uses pose-preserving augmentations (e.g., inverse masking ) to disentangle pose from scene content. The authors also introduce a new metric, True Pose Similarity (TPS), to quantify this transferability. Experiments show XFactor significantly outperforms re-implementations of prior work (RayZer, RUST) on this new metric."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies that previous generalizable self-supervised models (like RayZer and RUST) are prone to interpolating context frames when trained with a simple autoencoding objective, which is a valuable insight.\n\n2. The transerability objective, enabled through the pose-preserving augmentation scheme, is a novel and original formulation for a self-supervised task. It provides a new way to force the model to learn a representation that disentangles the camera motion from the appearance. \n\n3. The use of a stereo-monocular model to force extrapolation and avoid the interpolation failure mode is a clever, though costly, design choice.\n\n4. The paper's internal logic is sound. The experiments rigorously support the paper's central, albeit narrow, claim: that the XFactor model, trained with its specific transferability objective, is superior at the task of pose-trajectory transfer, as measured by the proposed TPS metric. The ablations (Table 3) are particularly strong, effectively demonstrating that the novel objective is the key ingredient for this success, and that adding multi-view information from scratch is detrimental to this specific goal. The counterintuitive finding that an explicit $SE(3)$ bottleneck is harmful is genuinely interesting.\n\n5. The paper is exceptionally well-written, clear, and logically structured. The authors do an excellent job of motivating their specific problem formulation, defining their terms (transferability, TPS), and explaining their method (XFactor). Figure 1 provides a very clear illustration of the novel training objective."}, "weaknesses": {"value": "1. The paper's central premise—that \"transferability\" is the key criterion for \"true NVS\" —is an overstatement. The paper defines \"true NVS\" in a way that conveniently excludes the most dominant NVS paradigms, such as single-scene optimization (NeRF, 3D Gaussian Splatting), which the paper dismisses as \"Oracle Methods\". The work's scope is limited to generalizable, unposed video models, and the abstract and introduction must be revised accordingly.\n\n2. The paper equates \"transferability\" with \"user-controllable\" NVS. The proposed XFactor model does not achieve this, though. A user cannot specify a novel viewpoint; they can only copy a viewpoint from another existing video sequence. The learned 256-dimensional latent pose is a black box. This is a \"pose-copying\" system, not a \"pose-controlling\" one, which is a significant gap between the stated problem and the delivered solution.\n\n3. The claim of being \"geometry-free\" is a bit misleading in my opinion. The entire transferability objective is built on a strong, expert-provided geometric prior: that augmentations like masking are pose-preserving, a fact the authors even formalize as $ORACLE[AUG[\\mathcal{I}]] = ORACLE[\\mathcal{I}]$. This is not a \"pure machine learning problem\"; it is a clever re-encoding of a geometric bias into the training data.\n\n4. The paper compares XFactor to re-implemented baselines (RayZer, RUST)  on its own novel metric (TPS), a metric for which XFactor is explicitly optimized. These baselines, unsurprisingly, fail spectacularly. The paper's own ablation (Table 3, \"Unconstrained\" baseline) shows that a standard stereo-monocular model with an autoencoding objective doesn't do much worse. This shows the win is not from a superior architecture but mainly from a training objective that is perfectly aligned with the evaluation metric, making the comparison to prior art feel circular and a bit unfair.\n\n5. The model's reliance on a stereo-monocular POSEENC to avoid interpolation is a major tradeoff. It sacrifices the core strength of multi-view 3D reconstruction: robust integration of information across many frames. This design choice may solve the interpolation proble,m but it creates a model that is likely less robust and has a poorer understanding of 3D geometry than a true multi-view system."}, "questions": {"value": "Given that your criterion of \"transferability\" does not apply to single-scene NVS (e.g., NeRF), will you concede that your paper is not about \"true NVS\" in general, but rather about the more specific sub-problem of generalizable NVS from unposed video?\n\nYou state your goal is a \"user-controllable viewpoint\". How can a user control the 256-dim latent pose vector of XFactor without first having a source video with the exact camera motion they desire? Isn't this a \"pose-copying\" system, not a \"pose-controlling\" one?\n\nYour training objective simulates two sequences with identical poses. How brittle is the learned representation to this perfect \"pose-matching\" assumption? Have you analyzed the performance degradation when the source and context trajectories are not identical, but merely similar?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3tyXjlLwRA", "forum": "aJJppqAm6r", "replyto": "aJJppqAm6r", "signatures": ["ICLR.cc/2026/Conference/Submission13980/Reviewer_9vfk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13980/Reviewer_9vfk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773703948, "cdate": 1761773703948, "tmdate": 1762924479529, "mdate": 1762924479529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of self-supervised novel view synthesis (NVS). The authors make a crucial and insightful observation: the key criterion for a \"true\" NVS model is transferability, i.e., a representation of a camera pose extracted from one video sequence should be usable to render the exact same camera trajectory in a completely different scene. They argue that prior self-supervised, geometry-free methods fail this test, as their learned \"pose\" representations are entangled with scene content, leading them to act as sophisticated frame interpolators rather than genuine view synthesizers.\n\nTo address this, the paper introduces XFactor, a novel self-supervised NVS model designed explicitly to learn transferable pose representations. The authors quantify transferability by proposing TPS (True Pose Similarity), and prevent the model from learning to interpolate by making XFactor bootstrapped from a stereo-monocular model and trained with a pose-preserving data augmentation strategy. Experimental results demonstrate the effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper asks a fundamental question in the field of self-supervised NVS. The proposed \"transferability\" is a powerful and original reframing of the problem. The paper is well-written and well-motivated. The design of the proposed solutions is elegant and principled, with its effectiveness validated by extensive experiments. Through the lens of transferability, the paper provides valuable insights to the community of self-supervised NVS."}, "weaknesses": {"value": "I appreciate the paper's main idea and find its core contribution to be insightful and significant. While the work is very strong, I have some concerns, primarily regarding the evaluation methodology for what the paper defines as a \"true NVS model\".\n\n\n### 1. The Challenge of Evaluating a True NVS Model\n\nMy main concern is how to fairly and comprehensively evaluate NVS performance under the new, important definition of \"true NVS model\". While I agree that standard autoencoding metrics like those in Table 4 are biased, I am not convinced that the proposed True Pose Similarity (TPS) metric (Eq. 9), used in Tables 1-3, provides a complete picture. My reasons are as follows:\n\n**1.1. Assessing Perceptual Quality in the Transfer Setting:** The TPS metric evaluates the *geometric consistency* of a transferred camera trajectory but does not directly measure the *perceptual quality* of the synthesized images. The paper itself compellingly argues that autoencoding reconstruction (seq-to-seq rendering on the same scene) is \"not equivalent to true NVS\" (lines 641-642) and is merely a measure of interpolation ability. This creates a critical gap in the evaluation: we can see from the TPS scores in Table 1 & 2 that XFactor is geometrically consistent, but we have no quantitative perceptual measure of the actual view synthesis quality, which should be a key point for NVS methods. This leaves the assessment of the model's core capability incomplete.\n\n**1.2. Fragility of the Oracle-based Metric:** The TPS metric's reliance on a post-hoc oracle (VGGT) makes it potentially fragile. As the paper notes (lines 199-202), the metric could be \"hacked\". More practically, it can fail in challenging extrapolation scenarios. For example, consider transferring a forward-motion trajectory from a driving scene (A) to a scene featuring an ego-centric rotation of an object (B). A model might produce meaningless, blurry artifacts for $\\text{Render}(\\mathcal{S}^B, \\mathcal{Z}_T^A)$, as it is a difficult extrapolation synthesis task (line 474-477). In such a case, an oracle like VGGT would likely fail to extract any meaningful pose from these artifacts, leading to a poor TPS score. This low score, however, would be an artifact of the evaluation tool's failure, not necessarily a failure of the model's transferability logic.\n\n**1.3. Inherent Noise in Oracle Tools:** It is well-documented that structure-from-motion oracles like COLMAP, and the feed-forward methods like VGGT, are not perfectly reliable and can produce noisy or incorrect poses. This issue has been analyzed in prior work such as RayZer [1] (arxiv version, Figure 4) and Less3Depend [2] (arxiv version, Appendix F). This unreliability introduces noise into the evaluation, making it difficult to disentangle the model's errors from the oracle's errors.\n\nGiven these points, I wonder if a well-defined perceptual measurement, independent of TPS, is necessary to complete the evaluation. For example, leveraging the **Pose-Preserving Augmentation** framework (Eq. 11) at **test time**. By applying two different augmentations to the same source sequence, the model could render a target frame from the second augmented sequence using the pose from the first. The resulting image could then be directly compared to the ground-truth target using standard perceptual metrics (PSNR, LPIPS, etc.). This would provide a direct, quantitative measure of rendering quality under the context of true transferability, which might complement the TPS metric.\n\nI believe a discussion of these evaluation limitations in the main paper or appendix would be highly beneficial. On this point, I am open to hearing the authors' clarification and am highly willing to raise my rating if this concern is addressed.\n\n### 2. Potentially Related Work\n\nThe paper astutely identifies the ill-posed nature of the autoencoding objective for NVS. This issue has also been noted by [3], which also addresses this problem, but proposes an alternative solution that involves leveraging explicit 3D representations for fine-tuning. Including a discussion that contrasts XFactor's geometry-free, transferability-based approach with this alternative 3D-aware solution would further strengthen the paper's contribution and contextualize it within the latest research landscape.\n\n\n---\n\n\n[1] RayZer: A Self-supervised Large View Synthesis Model (ICCV'25)\n\n[2] The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge (arxiv)\n\n[3] Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos (arxiv)"}, "questions": {"value": "1. The reported performance of the RayZer baseline in Table 4 appears to be a significant degradation when compared to the results presented in the original RayZer paper (Figure 1). I recognize and appreciate that the authors had to re-implement this baseline due to the lack of public code, and commend them for seeking confirmation from the original authors that their implementation is \"faithful\" (lines 312-314). However, the substantial performance gap is confusing. Are there key differences in the training dataset, evaluation protocol, or hyperparameter settings compared to those used in the original RayZer publication?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h9tHZg6ETP", "forum": "aJJppqAm6r", "replyto": "aJJppqAm6r", "signatures": ["ICLR.cc/2026/Conference/Submission13980/Reviewer_VrW1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13980/Reviewer_VrW1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955511388, "cdate": 1761955511388, "tmdate": 1762924478466, "mdate": 1762924478466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that the correct criterion for self-supervised NVS is transferability: a pose representation extracted from one sequence should produce the same camera trajectory when used to render another scene. The authors show that recent pose-free/self-supervised methods (RayZer, RUST) learn interpolation latents that fail this test. The authors propose XFactor, a geometry-free model using pairwise pose estimation, stereo-monocular bootstrapping, and pose-preserving augmentations (e.g., inverse masks) to disentangle pose from content. The authors also propose a new metric named True Pose Similarity (TPS) quantifies transferability using oracle poses.\n\nExperiments on datasets like RealEstate10K, DL3DV, MVImgNet, and CO3Dv2 demonstrate XFactor's superior transferability and pose probing accuracy, with ablations validating design choices."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper is insightful, which reframes NVS with the *transferability*. This is a fresh perspective, and the proposed TPS metric is a practical tool for evaluation the transferability of an NVS model.\nQuality of this paper is strong, with large-scale experiments on diverse real-world datasets supporting claims of transferability; ablations thoughtfully dissect components like stereo-monocular training and augmentations. Paper presentation is quite clear, from problem analysis to method derivation, making complex ideas accessible."}, "weaknesses": {"value": "1. The proposed TPS relies on an oracle (VGGT used in current experiments). VGGT is reasonable, but VGGT vs more conventional SfM (e.g., COLMAP) can differ—especially on difficult sequences (textureless regions, non-static content). The paper does not report robustness of TPS to oracle errors (noise, scale drift, missing frames), nor does it show if conclusions change with a different oracle.\n\n2. The authors note blurring/warping for large baseline transfers and suggest this stems from determinism. This is an important limitation for practical use and should be explored more: does a generative renderer (diffusion model) fix artifacts? What are tradeoffs with photorealism vs pose fidelity?\n\n3. The practical cost at inference (latency/memory) for rendering many target poses or long sequences is not reported."}, "questions": {"value": "1. How might TPS change if the authors evaluated it with COLMAP alongside VGGT. How sensitive would TPS be to small perturbations (translation/rotation jitter) in the oracle poses?\n\n2. Could a generative decoder plausibly reduce the blurring/warping seen at large baselines without degrading pose control — and if not, what trade-offs are expected between realism (e.g., FID/LPIPS) and pose fidelity (TPS)? Which probabilistic design choices seem most likely to preserve pose conditioning?\n\n3. What are resource costs: expected latency and peak GPU memory for rendering one target view on common GPUs, and how latency and memory scale as the number of context views increases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vhWeTwU8ga", "forum": "aJJppqAm6r", "replyto": "aJJppqAm6r", "signatures": ["ICLR.cc/2026/Conference/Submission13980/Reviewer_ZciQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13980/Reviewer_ZciQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979405729, "cdate": 1761979405729, "tmdate": 1762924477870, "mdate": 1762924477870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles self-supervised novel view synthesis by defining pose latent transferability as the key criterion for success. The proposed model, **XFactor**, achieves this through a geometry-free, self-supervised approach that combines pairwise pose estimation with curated data augmentations. By training on real-world videos without explicit geometric biases, XFactor learns transferable pose latents. To measure this capability, the authors introduce a new metric, True Pose Similarity (TPS). Experiments show that XFactor outperforms prior NVS transformers like RayZer and RUST on large-scale benchmarks. Furthermore, analysis confirms that its learned pose latents meaningfully correlate with actual camera poses, validating the approach."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well-written and has clear results/figures to present the results.  Datasets and baselines are well-described.\n\n2. This work defines \"transferability\" as a discriminative property for novel view synthesis (NVS) and introduces a new metric (TPS) for its quantification. It argues that view reconstruction alone is insufficient for genuine NVS without this property.\n3. The proposed model, XFactor, utilizes a pose-preserving, dual-masking data augmentation to disentangle the scene representation from the camera pose. This method achieves high transferability without relying on explicit geometric priors.\n4. Empirical results show that XFactor substantially outperforms baselines across multiple datasets. The paper supports its claims with a comprehensive evaluation, including pose-probing to validate the learned latent space and detailed ablation studies that provide insights into the model's design."}, "weaknesses": {"value": "- The novel views generated by XFactor, as seen on the project's homepage, suffer from noticeable blurring and distortion, even when the camera trajectory is accurately recovered. The paper would be stronger if it proposed a method to address these rendering imperfections. While the authors mention that blurring and warping occur for distant poses, a more in-depth analysis of these artifacts would add important nuance to the empirical results and better guide subsequent research.\n- The paper's experimental validation would be more convincing if it included a comparison against other self-supervised learning objectives, such as those based on contrastive learning or mutual information. Additionally, the evaluation of the ViT-based architecture is incomplete without benchmarking it against more recent paradigms, including video diffusion models like ReCamMaster, which have shown strong performance on related tasks.\n- The current experiments are limited to well-controlled datasets, leaving the performance of XFactor in more challenging, unconstrained real-world environments an open question. The paper does not provide insight into how the method would handle scenes with dynamic objects or significant photometric variations, such as changes in lighting or the presence of motion blur, which are common in real-world video."}, "questions": {"value": "1. Are there plans for open-sourcing code and pretrained models to facilitate reproducibility？\n\n2. Minor issue: The paper defers critical details of the training procedure to the appendix. For instance, the exact loss function is not defined in the main methodology. The description of the dual-masking augmentation is also incomplete, as it omits the probability of the no-masking case, making it unclear how frequently the transfer objective is applied during training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5IqcB14gIm", "forum": "aJJppqAm6r", "replyto": "aJJppqAm6r", "signatures": ["ICLR.cc/2026/Conference/Submission13980/Reviewer_QYSv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13980/Reviewer_QYSv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991535367, "cdate": 1761991535367, "tmdate": 1762924477450, "mdate": 1762924477450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}