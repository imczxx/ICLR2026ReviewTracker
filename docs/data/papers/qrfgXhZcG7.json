{"id": "qrfgXhZcG7", "number": 20885, "cdate": 1758311410533, "mdate": 1763000598124, "content": {"title": "ARCS: Agentic Retrieval-Augmented Code Synthesis with Iterative Refinement", "abstract": "We present Agentic Retrieval-Augmented Code Synthesis (ARCS), a system that improves LLM-based code generation without fine-tuning. ARCS operates through a budgeted synthesize–execute–repair loop over a frozen model: it retrieves relevant code context before generation, proposes candidates, executes them against tests, and repairs based on execution feedback. This retrieval-before-generation design reduces hallucination and accelerates convergence. We formalize ARCS as a state-action process with provable guarantees on termination, monotonic improvement, and bounded cost. A tiered controller (Small/Medium/Large) trades latency for accuracy predictably. On HumanEval, ARCS achieves up to 87.2\\% pass@1 with Llama-3.1-405B, surpassing CodeAgent (82.3\\%) while using simpler control than tree-search methods. On TransCoder, it achieves $\\geq 90\\%$ accuracy on most translation pairs. On a LANL scientific corpus, it improves CodeBLEU by +0.115 over baseline RAG. ARCS provides a practical, reproducible approach to reliable code synthesis using existing LLM checkpoints.", "tldr": "ARCS adapts a frozen LLM to local code by retrieving before generating and using an execute-and-repair loop, achieving competitive code synthesis and translation without fine-tuning.", "keywords": ["Agentic code generation", "Iterative refinement", "sandbox verification", "Chain-of-thought prompting", "Retrieval-Augmented Generation"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d4733b6dac207c7bf1dc04c5d9d8b9c805b22827.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies agentic retrieval-augmented code generation, introducing a synthesize–execute–repair loop in which the system retrieves relevant code contexts, proposes candidate solutions, executes them, and repairs errors iteratively. The framework is instantiated using LLM-based agents."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Timely topic: Agentic code generation is a trending and potentially impactful direction in AI4SE.\n- The ablation study is informative and contributes useful insights about component contributions."}, "weaknesses": {"value": "1. Technical originality and clarity\n\nThe proposed solution is technically straightforward. While agentic code generation and RAG-based code generation are each useful, their combination in this paper feels somewhat contrived rather than synergistic. The synthesis of these ideas does not lead to substantial new capabilities or theoretical insights.\n\nMuch of the technical content lacks depth. For example, Section 3.3 provides only trivial observations (e.g., termination due to an explicit upper bound), and it is unclear why such results need to be emphasized.\n\nThe reinforcement learning–like formulation (i.e., state–action process) is presented but not operational; it plays no meaningful role in the methodology or experiments.\n\n2. Experimental evaluation\n\nWhile using HumanEval is natural, the choice to include cross-language translation as an additional evaluation setting is poorly motivated. The use of CodeBLEU for the LANL dataset does not clearly demonstrate the advantages of the ARCS framework.\n\nThe observed improvements are modest—ARCS (87.2%) vs. CodeAgent (82.3%) on HumanEval—and are not particularly convincing, especially given the many uncontrolled factors that may influence the results.\n\nThe reliability of the results depends heavily on the quality and completeness of the test suites, which are not fully discussed.\n\n3. Presentation quality\n\nThe paper is generally poorly written. The formalization in Section 3.2 complicates simple ideas rather than clarifying them.\n\nSeveral technical terms are introduced without adequate explanation or citation — for example, “FAISS ANN index.” Such omissions unfortunately compromise readability."}, "questions": {"value": "1. Are there any genuine innovations within each component of the proposed framework, or are these primarily adaptations of existing techniques?\n2. What exactly does “frozen model” mean in this context? Does it refer to a model whose parameters are fixed during all stages (retrieval, synthesis, repair), or are there components that remain trainable?\n3. What role does the state–action pair (i.e., the reinforcement learning formulation) play in the current implementation? It is introduced in the text, but its function and impact on the overall framework are unclear.\n\nTechnical:\nEq(2) what does \\oplus exactly stand for?\nEq(5) what does \\emptyset exactly stand for?\nLine 072: ex post execution checking ? (a typo?)\nLine 501 and 526: double reference entries."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z5wNTHk0Jf", "forum": "qrfgXhZcG7", "replyto": "qrfgXhZcG7", "signatures": ["ICLR.cc/2026/Conference/Submission20885/Reviewer_MLDf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20885/Reviewer_MLDf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761782016622, "cdate": 1761782016622, "tmdate": 1762999981649, "mdate": 1762999981649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "IAV8xrLmZO", "forum": "qrfgXhZcG7", "replyto": "qrfgXhZcG7", "signatures": ["ICLR.cc/2026/Conference/Submission20885/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20885/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763000596200, "cdate": 1763000596200, "tmdate": 1763000596200, "mdate": 1763000596200, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Agentic Retrieval-Augmented Code Synthesis (ARCS) for improving LLM-based code generation through a synthesize–execute–repair loop. The authors show that their retrieval-before-generation design reduces hallucination and accelerates convergence, as well as improves accuracy. The authors approach of using chain of thought planning to decompose tasks into subgoals, and then retrieve code snippets for each subgoal separately is conceptually very sound. While many methods propose trading more compute for high performance, this paper includes provable bounds on termination, monotonic improvement, and cost. Lastly, the approach can keep the model frozen, and hence is more easily adaptable. The results show that on HumanEval, TransCoder, and a LANL scientific corpus, the ARCS technique matches or exceeds strong prompting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors present a clear formalization of the state-action-transition-reward iterative framework for their approach, as well as design guarantees based on inputs like the budget.\n- Figure 1, the system architecture is clear presentation of the approach.\n- The theoretical guarantees are relatively straight-forward, however the inclusion in the paper is a nice addition.\n- The approach is novel, particularly by pairing planning with conditioned treival and the verification-in-the-loop. The verification and feedback has both conceptual and in the ablation shows strong emperical advantages without (presumably) adding signficant computational overhead.\n- The experimental setup presented is thorough. Specifically, they include ablation studies looking at individual components in the system, include multiple benchmarks (TransCode, LANL, and HumanEval) for assessing performance under a variety of contexts, and compare against 3 baselines. Additionally, in Table 5 they compare against a basic RAG approach and consistently show superior performance."}, "weaknesses": {"value": "- It seems like the Medium ARCS tier often underperforms small. It's unclear what the value in the Medium approach would be or when it is a good choice.\n- In Table 1 why is the baseline model GPT-3.5-Turbo and not the same model as used in the ARCS backbone — this feels slightly unfair.\n- Lacks statistical significance testing. Since the approach relies on frozen models, it is reasonable to run multiple trials.\n- More could be added to the Related Work to distinguish the novelty of ARCS. Specifically, how the execution feedback differs from RethinkMCTS and how loop for ARCS differs from CodeAgent.\n- No analysis of the retrieval provided. It's unclear how much the system depends on accurate retrieval and the impact of the choice of budget k on performance.\n- Qualitative analysis to show failures and more insights into how the agentic loops like would be very valuable to readers.\n- Empirical calculations of the costs to use the approach (e.g., latency or wall clock time) compared with a baseline like RAG would be helpful."}, "questions": {"value": "- Why does the approach rely on RAG? Since you are already relying on a sandbox for execution, could the agents execute queries based on their plan. For example, if the agent needs a specific function, then it could grep within a python REPL over the code corpus. This type of approach would eliminate the need for a retriever and vector database."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yfv6WOs9ue", "forum": "qrfgXhZcG7", "replyto": "qrfgXhZcG7", "signatures": ["ICLR.cc/2026/Conference/Submission20885/Reviewer_D1WX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20885/Reviewer_D1WX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868398469, "cdate": 1761868398469, "tmdate": 1762999981406, "mdate": 1762999981406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper uses RAG to retrieve code snippets to generate candidate solutions and uses execution feedback to adjust the solution. They test their approach on 3 coding benchmarks. They also claim to contribute theoretical properties of their algorithm."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-The paper is generally well written and easy to follow.\n\n-To my knowledge, this is the first paper that I have seen that does RAG with code snippets so the application of retrieval is novel"}, "weaknesses": {"value": "-Table 1 shows higher performance with RethinkMCTS and the downside the authors mention that in Line 373 that it relies on “heavier exploration” does not satisfy me. That would suggest that there are situations where more exploration or better exploration strategies than what ARCS does is needed.\n\n\n-The theoretical section does not contribute anything in my opinion. All propositions and lemmas are trivial and just based on definition. For proposition 1, best-so-far tracking by definition is monotonic. For proposition 2, if you set a loop for at most B iterations, then it will undoubtedly be bounded. For proposition 3, if you defined upper bounds for each portion of the cost then Eq 18 is apparent. Proposition 4 is saying that if you have more controllers to choose from, then the probability of having the optimal one is higher. These are not insightful theoretical properties."}, "questions": {"value": "Suggestion:\nFor each portion of the method, you can remove the work “Specification”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Mb4eXx1IEf", "forum": "qrfgXhZcG7", "replyto": "qrfgXhZcG7", "signatures": ["ICLR.cc/2026/Conference/Submission20885/Reviewer_ysBL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20885/Reviewer_ysBL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996516265, "cdate": 1761996516265, "tmdate": 1762999981451, "mdate": 1762999981451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method called Agentic Retrieval-Augmented Code Synthesis (ARCS) for iteratively engineering prompts fed into a fixed, backbone LLM for generating code to solve benchmark coding tasks. The proposed method has four main steps: (i) use Chain-of-Though (CoT) techniques to produce an LLM-generated description of a high-level idea for the solution approach based on the current prompt and concatenate this description with the prompt; (ii) retrieve relevant code examples from an existing code database, eliminate redundancies, and concatenate these examples with the prompt; (iii) generate code using the backbone LLM and current prompt; (iv) run unit tests, collect feedback, and incorporate this feedback into the prompt. Formal description of steps (i)-(iv) is provided. The proposed method with various versions of Llama-3.1 as backbone is experimentally compared with existing baselines using non-Llama backbones. Ablations illustrating the effect of the various components of (i)-(iv) are provided."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is an intuitive and natural way to improve on existing retrieval-augmented methods for code generation. The experimental results indicate that the method is consistently able to solve challenging code generation tasks."}, "weaknesses": {"value": "The paper has the following weaknesses:\n1. The paper suffers from clarity and readability issues. It is unclear from the introduction and related works why previous methods for retrieval-augmented code generation fail and how the ARCS framework overcomes these issues. Throughout the paper the writing is very terse and full of undefined jargon, and several important claims lack justification or explanation (see **Questions** for examples). In addition, there are several points throughout the main body where important concepts and notation are inadequately defined or not defined altogether (see **Questions** for examples).\n2. Several key components of the proposed method are not described in sufficient detail for reproducibility and are not situated clearly (with appropriate references and discussion) in the relevant literature. In particular, the descriptions provided in the *Implementation* sections in Section 3.2 of how the core components of the method were implemented are insufficient for reproducibility, and no code is provided to mitigate this issue. In addition, it is unclear from the description of the method what existing methods (for CoT, encoding, redundancy filtering, etc., with appropriate references) are leveraged within each component.\n3. The experimental results do not compare ARCS against existing baselines with the same backbone LLM. This seriously undermines the meaningfulness of the experimental comparisons, since superior performance of ARCS against baseline methods could be due to the underlying backbone of ARCS (Llama-3.1-*) being better suited to the task at hand than the backbone used by the baseline, instead of inherent superiority of ARCS. From the description of ARCS, it is unclear why identical backbones were not used, seriously weakening the experimental contribution of the paper."}, "questions": {"value": "1. What are the specific drawbacks of existing methods that ARCS addresses?\n2. What are $x_i, y_i$ near lines 105-106?\n3. What is $\\mathcal{X}$ near lines 142-143?\n4. What is all-MiniLM-L6-v2, lines 143-144?\n5. What are $\\mathcal{E}$ and Encode in equation (2)?\n6. What does $(f_{<t} \\lVert f_t)$ mean in equation (3)?\n7. What are $\\pi_1, \\pi_2, \\mathcal{R}$ in the **Similarity and Filtering** section on page 4?\n8. What are Format(), signature(), doc(), and code() in equation (12)?\n9. How are $Enc_{env}, Enc_{plan}, Enc_{evid}, Enc_{inv}$ defined on lines 244-245?\n10. What is $P_{\\phi}$ in equation (16)?\n11. What do the fragments \"On context pressure, apply stable trunction\" and \"collapse $Enc_{plan}$ to headers\" mean in lines 263-264?\n12. What are the definitions of $\\mathcal{P}_{S}, \\mathcal{P}_{M}, \\mathcal{P}_{L}$ in Lemma 1?\n13. Does the fact that ARCS (Small) outperforms ARCS (Medium) on pass@1 contradict Lemma 1?\n14. What does the statement around lines 368-369 that \"scores are indicate rather than strictly comparable across different backbones\" mean?\n15. What does it mean around lines 372-373that \"ARCS (Large) achieves competitive pass@1 with substantially simpler control than tree-search methods\"? Specifically, what does \"simpler control\" mean in this context?\n16. Is there a concrete, quantitative meaning to the claim that \"RethinkMCTS...relies on heavier exploration\" around lines 373-374?\n17. What does it mean at the end of page 7 that \"the pattern matches the framework\"? What does \"the pattern\"  refer to here?\n18. Does the statement at the beginning of Section 4.5 that \"Medium often matches or slightly exceeds Large, consistent with translation benifiting [sic] from structured decomposition without repeated verification\" contradict Lemma 1?\n19. What does it mean around lines 421-422 that \"The most challenging direction is Python $\\rightarrow$ C++ due to the paradigm shift\"? What is \"the paradigm shift\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YaMn4m1QPN", "forum": "qrfgXhZcG7", "replyto": "qrfgXhZcG7", "signatures": ["ICLR.cc/2026/Conference/Submission20885/Reviewer_wbjf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20885/Reviewer_wbjf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762113142248, "cdate": 1762113142248, "tmdate": 1762937610940, "mdate": 1762937610940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}