{"id": "IlyesljaNb", "number": 20931, "cdate": 1758311778708, "mdate": 1759896951018, "content": {"title": "Intrinsic training dynamics of deep neural networks", "abstract": "A fundamental challenge in the theory of deep learning is to understand whether gradient-based training can promote parameters belonging to certain lower-dimensional structures (e.g., sparse or low-rank sets), leading to so-called implicit bias. As a stepping stone, motivated by the proof structure of existing intrinsic bias analyses, we study when a gradient flow on a parameter $\\theta$ implies an intrinsic gradient flow on a ``lifted'' variable $z = \\phi(\\theta)$, for an architecture-related function $\\phi$. We express a so-called intrinsic dynamic property and show how it is related to the study of conservation laws associated with the factorization $\\phi$. This leads to a simple criterion based on the inclusion of kernels of linear maps, which yields a necessary condition for this property to hold. We then apply our theory to general ReLU networks of arbitrary depth and show that, for any initialization, it is possible to rewrite the flow as an intrinsic dynamic in a lower dimension that depends only on $z$ and the initialization, when $\\phi$ is the so-called path-lifting. In the case of linear networks with $\\phi$, the product of weight matrices, the intrinsic dynamic is known to hold under so-called balanced initializations; we generalize this to a broader class of {\\em relaxed balanced} initializations, showing that, in certain configurations, these are the \\emph{only} initializations that ensure the intrinsic metric property. Finally, for the linear neural ODE associated with the limit of infinitely deep linear networks, with relaxed balanced initialization, we explicit the corresponding intrinsic dynamics.", "tldr": "", "keywords": ["gradient flow", "path-lifting", "intrinsic lower dimensional dynamic", "conservation laws", "implicit bias"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/67fb47b1b70fda54ea4034c1f3ef15a08d708469.pdf", "supplementary_material": "/attachment/77564e129ad008821f85f9bb495bcef0ba0f6ab9.zip"}, "replies": [{"content": {"summary": {"value": "This paper studied conditions under which the gradient flow in minimizing $\\ell(\\theta)$ can be rewritten as an autonomous flow in $z=\\phi(\\theta)$ for some reparametrization map $\\phi$. Based on conservation laws, the authors derived and analyzed two sufficient conditions: the intrinsic metric property and the intrinsic recoverability property. They showed for general ReLU networks that, parameter points in a dense set admit an intrinsic flow in a neighborhood. For linear networks, they showed that the map from all weight matrices to the end-to-end matrix satisfies the intrinsic metric property under relaxed balanced initializations, but not in general. The authors derived explicit forms of the intrinsic flow for linear networks and an infinitely deep linear network with relaxed balanced initialization, and a depth-three scalar ReLU network."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed framework provides a systematic way to study the intrinsic dynamics of gradient flow. It continues two lines of works:\n\n* The paper considered reparameterized flows that are not necessarily mirror flows, thereby extending the framework in Li et al. (2022), and providing an intermediate step for applying tools developed in Azulay et al. (2021). \n\n* It presented several descriptions of the intrinsic metric and intrinsic recoverability properties based on conservation laws, which I find natural and insightful. This connects with the framework in Marcotte et al. (2023). \n\nI therefore believe this work is valuable for the community studying optimization dynamics and implicit bias. \n\nAdditionally, the intrinsic dynamics derived for the depth-three ReLU network are interesting and have not been identified in prior work, to my knowledge."}, "weaknesses": {"value": "* The negative results for linear network are not strong to me. First, only the sufficient condition, intrinsic metric property, is declined, not the existence of intrinsic dynamic. Second, only a specific reparametrization map, $\\phi_{\\mathrm{lin}}$, is considered. It remains unclear whether $\\phi_{\\mathrm{lin}}$ induces an intrinsic flow, and if not, whether other reparameterization maps might. \n\n* The intrinsic flow for ReLU networks is established only locally, which makes its utility unclear. Current analyses of implicit bias, even those not relying on mirror flows, still require the reparametrized flow to be globally defined, for example those in Azulay et al. (2021).\n\n**Initial recommendation**\n\nDespite the above points, I find this paper valuable to the community and my initial recommendation is acceptance."}, "questions": {"value": "* Is Assumption 2.1 not satisfied by $\\phi_{\\mathrm{ReLU}}$? Consider $g(x,\\theta)= u \\sigma(v^\\top x)=\\mathbb{1}(v^\\top x)uv^\\top x$. When $\\theta$ lies on {$v^\\top x=0$}, $\\phi_{\\mathrm{ReLU}}(\\theta)=u v^\\top$ is not a reparametrization in any neighborhood of $\\theta$, since the indicator function $\\mathbb{1}(v^\\top x)$ is not determined by $u v^\\top$. \n\n* How is the gradient flow solution defined for ReLU networks, for example, the \"maximal solution\" in Definition 2.6 and in Proposition 3.10? In particular, how is the non-differentiability of the ReLU activation at zero addressed? Does a gradient flow solution exist on $t\\in[0,+\\infty)$ for any initialization, and is the solution unique?\n\n* (Following the previous question) Does Proposition 3.10 indicate that, regardless of how the derivative of ReLU at zero, $\\sigma'(0)$, is defined, the flow in $Z$ is always well-defined on $\\Theta$, and the definition of $\\sigma'(0)$ only affects $\\nabla f(Z)$ in the $Z$-flow? \n\n\n**Minor suggestions**\n\n* I find Equation (7) quite interesting. It would be helpful if the authors could provide a geometric interpretation of this condition, in particular an interpretation of the kernel of $\\partial M$. \n\n* In the abstract and the Contributions, there are claims similar to \"For general ReLU networks, we show that, for any initialization, it is possible to rewrite the flow as an intrinsic dynamic.\" According to Corollary 3.9, I would suggest using \"for a dense set of initializations\" instead of \"any initialization\", as these two notions can differ a lot.  \n\n* It might make more sense to group Sections 2, 3.1, and 3.2 together as they concern the general framework, and group Sections 3.3 and 4 together as they concern specific networks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4j3UlBvYyU", "forum": "IlyesljaNb", "replyto": "IlyesljaNb", "signatures": ["ICLR.cc/2026/Conference/Submission20931/Reviewer_xeEd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20931/Reviewer_xeEd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800813735, "cdate": 1761800813735, "tmdate": 1762939034101, "mdate": 1762939034101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The goal of this paper is to find whether a high-dimensional gradient flow can be written as an intrinsic Riemannian flows in lower-dimensional spaces. The authors analyzed three related properties: intrinsic dynamics, metric, and recoverability. They then connect these properties: if we have iinstrinsic recoverability (connected with conservation laws), then the intrinsic metric property is satisfied, which then \"verifies\" the intrinsic dynamic property. The application to ReLU networks and that to linear networks are also discussed."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper has demonstrated a high level of mathematical rigor. The authors clearly define the involved properties formally, and use a series of lemmas and theorems to formally establish their connections. I find the results provided in Theorem 3.3 technically sound, which provides a viable way to investigate the possibility of studying the low-dimensional intrinsic Riemannian flow. The scope of the framework is somewhat general.\n\nIn addition, the application to deep linear networks (the relaxed balanced initialization compared to prior works) is interesting, which explains why this kind of initialization is necessary rather than only a clever or convenient choice."}, "weaknesses": {"value": "1. However, in my view, this paper does not provide fundamentally new insight. The authors indeed broaden the scope of the connection between conservation laws and intrinsic metric for low-dimensional flow, however, this idea has been broadly discussed by prior works (e.g., Bah et al., (2022); Marcotte et al. (2023)) and the authors do not simplify the description. Hence, there lacks an adequate motivation for what this new framework can provide. As a result, I think the scope of contribution of this paper is limited (providing a new characterization for the path-lifting metric). \n\n2. Another major weaknesses is the presentation. I think this paper is very challenging to read. It is too dense. The chain of definitions, lemmas, and theorems is very long, yet the main flow is constantly interrupted by examples and propositions that have already been discussed by other works. This is even more confusing when I find that the authors in fact have one separate section for examples. In addition, the link between high-level concepts is difficult to follow, and there lacks sufficient intuitive explanation for, e.g., why these high-level properties should be equivalent and can be connected."}, "questions": {"value": "1. Can the authors provide some novel insights about neural network behavior that was unknown before applying this new framework?\n\n2. One motivation of studying the intrinsic flow suggested by the authors is the connection to implicit bias. Then how do the results appeared in this paper inspire the study of the implicit bias? As one advantage of the framework is the relaxation of the requirement of the mirror flow representation, which in fact helps finding the implicit bias, I'm a bit confused about this question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lO1M6oMoTe", "forum": "IlyesljaNb", "replyto": "IlyesljaNb", "signatures": ["ICLR.cc/2026/Conference/Submission20931/Reviewer_1Gdc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20931/Reviewer_1Gdc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847635327, "cdate": 1761847635327, "tmdate": 1762939033453, "mdate": 1762939033453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper defines a three increasingly stronger conditions that all imply the existence of intrinsic dynamics. These conditions require the knowledge of the set of invariants / conserved quantities, however a necessary and an equivalent condition that is more easily computable is given for two of these conditions.\n\nThe authors then use this framework to give a number of example of intrinsic dynamics, in linear networks and ReLU networks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a number of ideas that englobe many previous result in the same framework, and seems to also show that one cannot do better than these previous results (there are no more invariants than those already known).\n\nSome of the intrinsic dynamics provided are to my knowledge new, though they are all very close to already known works."}, "weaknesses": {"value": "This paper is a bit of a typical example of \"proof by abstract non-sense\", it mainly proves mainly already existing results using very abstract (and arguably quite complex) tools. I appreciate that this type of approach can tell us that there are not more invariants and therefore simplifications than the ones we already knew, but I am not convinced that I need all these tools to find the next invariants on a new model, because it seems that people have been able to identify these invariants naively without issues.\n\nFor linear networks, the relaxed balanced conditions has also been used in this paper https://arxiv.org/abs/2405.17580, which also proves that this condition arises naturally in the infinite width limit. The infinite depth linear network dynamics description is more novel, but it is very similar to the two layer case.\n\nFor ReLU networks, the invariants are just the differences between the norms of the incoming and outcoming weights, which were known for a while too."}, "questions": {"value": "- Are there any model type where you believe that these techniques could discover some yet unknown invariants and lead to simplified dynamics?\n- There are very few invariants in ReLU networks (the number of invariant is the squared root of the number of parameters), which suggests that there is no hope to just use invariants to obtain truly lower dimensional dynamics, do you still see hope? The biggest successes at finding simplified dynamics in DNNS is taking infinite width limits, e.g. NTK / mean-field limits. Can these fit in your framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gg2Q4DEAym", "forum": "IlyesljaNb", "replyto": "IlyesljaNb", "signatures": ["ICLR.cc/2026/Conference/Submission20931/Reviewer_E9V6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20931/Reviewer_E9V6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861998763, "cdate": 1761861998763, "tmdate": 1762939032899, "mdate": 1762939032899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a unifying theoretical framework to support implicit bias-type analyses of deep neural networks. The paper's goal is to establish when the gradient flow on the original parameters $\\theta$ implies a corresponding flow on a lifted variable $z = \\phi(\\theta)$ depending on initialization $\\theta(0)$ and the architecture --- this is defined formally through the \"intrinsic dynamic\" property. The authors define two further properties of the pair $\\theta(0)$ and $\\phi$, which can ensure intrinsic dynamics on $z$. These properties are related through a chain of implications, with necessary and/or sufficient conditions provided for parts of this chain to hold. Finally, the framework is applied to establish\n\n * that any initialization $\\theta_0$ of DAG ReLU networks of arbitrary depth satisfies the intrinsic dynamic property with respect to the path-lifting $\\phi$\n * that the relaxed balanced initialization for arbitrarily deep fully connected linear neural networks (LNN) with square weights satisfies the intrinsic dynamic property with respect to a commonly used reparametrization $\\phi$. This result is extended to infinitely deep LNNs under the same initialization for which the dynamics $\\dot{z}$ is made explicit.\n * the explicit dynamics of $\\dot{z}$ for a 3 layer ReLU MLP.\n * the explicit dynamics of $\\dot{z}$ for two-layer fully connected linear networks with relaxed balanced initialization"}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper makes a solid contribution to the line of work studying the implicit bias of NNs. The reasons are multifold:\n\n* The proposed framework is novel and provides a unified approach for establishing that a reparametrization of the weights admits intrinsic dynamics, which is essential to implicit bias analyses. This result is very useful as, to my knowledge, existing literature relies on ad-hoc approaches that come with a difficult-to-track fine-grained variation in assumptions. While the difficulty of making $\\dot{z}(t)$ explicit remains, this framework can provide shortcuts to validating potential architecture-initialization-reparametrization combinations. \n* To the best of my knowledge, the related literature is appropriately covered. Connections to prior work are precise and detailed, and paint a clear picture of the current state of theoretical progress in this area.\n* The proofs are presented clearly and, though I have not examined every detail, their main steps appear correct.\n* The paper is well-structured and thoughtfully written, mathematically sound, and with a good balance of grounding examples among the dense theoretical results. The authors did a great job at condensing a high amount of information in a way that is easy to follow and pleasant to read."}, "weaknesses": {"value": "A balancing discussion on the limitations of this framework / current results is mostly missing. For example,\n * What is the main difficulty in extending Theorem 4.3 to arbitrary $r$? \n * The ReLU architecture and $\\phi_{\\mathrm{ReLU}}$ pair seems to allow for stronger properties leading to intrinsic dynamics, i.e., intrinsic recoverability, compared to linear neural networks with relaxed balanced $\\theta_0$ and $\\phi_{\\mathrm{Lin}}$. Is it coincidental, or does it point to a deeper-rooted limitation of the factorization $\\phi_{\\mathrm{Lin}}$?"}, "questions": {"value": "Please see the above section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0c8EeP4IKc", "forum": "IlyesljaNb", "replyto": "IlyesljaNb", "signatures": ["ICLR.cc/2026/Conference/Submission20931/Reviewer_cmEp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20931/Reviewer_cmEp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762201499211, "cdate": 1762201499211, "tmdate": 1762939032502, "mdate": 1762939032502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}