{"id": "ykgeXlyWDU", "number": 1506, "cdate": 1756888183112, "mdate": 1759898205512, "content": {"title": "Adaptive Symmetrization of the KL Divergence", "abstract": "Many tasks in machine learning can be described as or reduced to learning a probability distribution given a finite set of samples. A common approach is to minimize a statistical divergence between the (empirical) data distribution and a parameterized distribution, e.g., a normalizing flow (NF) or an energy-based model (EBM). In this context, the forward KL divergence is a ubiquitous due to its tractability, though its asymmetry may prevent capturing some properties of the target distribution. Symmetric alternatives involve brittle min-max formulations and adversarial training (e.g., generative adversarial networks) or evaluating the reverse KL divergence, as is the case for the symmetric Jeffreys divergence, which is challenging to compute from samples. This work sets out to develop a new approach to minimize the Jeffreys divergence. To do so, it uses a proxy model whose goal is not only to fit the data, but also to assist in optimizing the Jeffreys divergence of the main model. This joint training task is formulated as a constrained optimization problem to obtain a practical algorithm that adapts the models priorities throughout training. We illustrate how this framework can be used to combine the advantages of NFs and EBMs in tasks such as density estimation, image generation, and simulation-based inference.", "tldr": "", "keywords": ["KL Divergence", "Generative adversarial networks", "Normalizing flow", "Energy-based models"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8e12babf42e3a1caa091a9b56db40add93ad9a70.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method to minimize the symmetric Jeffreys divergence (the sum of forward and reverse KL divergences) to learn a probability distribution from data. The key challenge is that the reverse KL term depends on the unknown data distribution. To address this, the authors introduce a proxy model to approximate the data distribution and formulate the problem as a constrained optimization task. A central contribution is an \"adaptive symmetrization\" mechanism, implemented via a resilient optimization framework (P-DYN), which dynamically adjusts the emphasis on the forward KL, reverse KL, and proxy model fidelity during training. The authors develop a primal-dual algorithm based on non-convex duality theory to solve this problem and propose a synergistic combination of a Normalizing Flow (NF) as the primary model and an Energy-Based Model (EBM) as the proxy. Experimental results on synthetic 2D data, latent-space image sampling (CelebA), and simulation-based inference (SBI) benchmarks are provided, claiming improved stability and performance over baselines like NF, WGAN, and a fixed-weight penalty method."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The core idea of using a collaboratively trained proxy model to enable the minimization of the Jeffreys divergence is novel and interesting. Moving away from the adversarial setup of GANs towards a collaborative, constrained optimization framework is a worthwhile direction."}, "weaknesses": {"value": "The paper lacks a critical discussion on the fundamental tension between the forward and reverse KL divergences. It is well-established that their minima can be contradictory (mode-covering vs. mode-seeking). The proposed adaptive symmetrization aims to balance them, but the theoretical conditions under which minimizing their sum leads to a desirable solution are not analyzed. The claim that the method avoids the issues of both extremes needs deeper justification.\n\nThe transition from the idealized problem (PI) to the dynamically constrained problem (P-DYN) is presented as a solution for infeasibility, but the approximation gap between these two formulations is not quantified or analyzed. It remains unclear how the solutions of (P-DYN) relate to the original goal of minimizing the Jeffreys divergence.\n\nThe transition from Eq. (P-DYN) to the empirical dual problem in Eq. ($\\hat P$-DYN) is flawed. The entropy of the data distribution is ignored. This oversight invalidates the equivalence claimed in this step.\n\nThe experiments are primarily conducted on low-dimensional, synthetic 2D datasets. While useful for illustration, they are insufficient to demonstrate the scalability and practical utility of the method for modern machine learning problems. The claim that the method is \"more accurate on a variety of datasets\" is overstated.\n\nThe comparisons, while including NF and WGAN, lack benchmarks against other state-of-the-art generative models (e.g., diffusion models, VAEs) or other methods for symmetric divergence minimization.\n\nThere are related work on combining forward and reverse KL, such as the $\\alpha$-bridge [1]. Discussions on the pros and cons of the proposed method over these existing ones are necessary.\n\n\n[1] Zhao, Miaoyun, et al. \"Bridging maximum likelihood and adversarial learning via α-divergence.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Nza0OR66FN", "forum": "ykgeXlyWDU", "replyto": "ykgeXlyWDU", "signatures": ["ICLR.cc/2026/Conference/Submission1506/Reviewer_Nftq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1506/Reviewer_Nftq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761368298315, "cdate": 1761368298315, "tmdate": 1762915787036, "mdate": 1762915787036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new way to minimize the Jeffreys divergence by introducing a \" proxy mode\". The final objective is weighted combination of three KL divgerneces. The proposed method is then applied to task like: density estimation, image generation, and\nsimulation-based inference."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The experiment section covers different potential use cases of the proposal method, which is good."}, "weaknesses": {"value": "1. **Motivation is unclear**\nThe motivation for using Jeffreys divergence is \"training on discrete samples may lead to a\nmismatch between the modelled distribution and the data distribution (illustrated in Figure 1a). Ordinarily, minimizing a symmetric divergence would alleviate this issue,\" which is unclear to me. I am not sure why minimising a symmetric divergence will make the mode and target distribution more matched. They are all valid divergences; any valid divergence will make two distributions equal when the divergence goes to zero. Need more explanation on the motivation, practical evidence or reference to illustrate this problem.\n\nThere are some benefits of combining forward KL and reverse KL for training, for example, in this paper https://arxiv.org/pdf/1907.11891, the motivation is that Reverse KL will lead to mode collapse but will get sharper mode estimation, forward KL will have better mode covering ability, so adding FKL to RKL will improve the diversity. This is one example of a valid motivation. This is the most important question to answer when starting research.\n\n2. **The experiment results are too bad** The FID for CelebA is too high, and only on low low-dimensional latent space is too out of date. A valid paper either has a new method with a good motivation or can improve some current best methods. This FID is too high, couldn't show the proposed method is effective in high dimensions. Other 2d experiments are too trivial.\n\n3. **Idea is not inspiring** The proposed method requires introducing another proxy model, which needs to be as powerful as the main model, which is unaffordable in current machine learning world."}, "questions": {"value": "See above for the weekness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jqfBTEWLLO", "forum": "ykgeXlyWDU", "replyto": "ykgeXlyWDU", "signatures": ["ICLR.cc/2026/Conference/Submission1506/Reviewer_sXKM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1506/Reviewer_sXKM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659915420, "cdate": 1761659915420, "tmdate": 1762915786894, "mdate": 1762915786894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new approach to generative modeling by minimizing the Jeffrey's divergence.  The forward KL $D(\\pi|p_\\theta)$ is trained in the usual MLE way using samples of $\\pi$ while the reverse KL $D(p_\\theta|\\pi)$ which is not directly accesible is trained by using a surrogate distribution $q_\\psi$.   Instead of minimizing $D(p_\\theta|\\pi)$ directly one minimizes instead $D(p_theta|q_\\psi)$ (which is then computed \"explictly\" or via MC) while keeping $D(\\pi|q_\\psi)$ small.  \n\nThe main idea is to avoid the adversarial (and sometime brittle) approach to GANs which is use the dual formulation of the KL divergences and neural newtowkr architectures. In the current approach the family $p_\\theta$ and $q_\\psi$ are parametrized directly a combination of neural flows and energy models to ensure expressivity. \n\nThe resulting objective functional is not convex so it is treated by dual optimization and a control on the difference bewteen solutions of the original and dual functional."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1) The paper is very well written, the main ideas and concepts are presented with clarity and in a nuanced manner.\n\n2) The ideas in the paper are novel and original.  As far as the reviewers knows,  this is a completely new approach to generative model and a new way to avoid the adversarial training of GANs.   In some way the introduction of a surrogate  replaces the back-forward training commong in flow models (such as diffusion models or normalizing flows)  by the introduction of the surrogate. The combined use of neural flows and energy model is also interesting.  \n\n3) The experiments are overall sufficient to demonstrate the effectiveness of the training.\n\n4) The reviewer appreciate the thoughtful discussion  about the limit of the methods in high-dimension, and maybe the need for other divergences"}, "weaknesses": {"value": "1)  The fact that generative/surrogate  divergence is handled via importance sampling  MC is a little bit worrying, especially if the target has a complex structure with metastable behavior.   \n\n2) The dual optimization framework seems super interesting.  The reviewer would have appreciated a bit more background and intuition about why this works and why this apply here.  In particular the assumption about closeness in total variation (which is a very strong norm) is not very likely to be true in practice."}, "questions": {"value": "My two questions would be to adress the two weakness noted above.\n\n1) Can you explain where your method starts to fail?\n\n2) How can one understand the theory behind the optimization problem better?  It seems from the experiments that that the gap between primal and dual solutions is very small."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I13155aqWK", "forum": "ykgeXlyWDU", "replyto": "ykgeXlyWDU", "signatures": ["ICLR.cc/2026/Conference/Submission1506/Reviewer_rxKB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1506/Reviewer_rxKB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852742908, "cdate": 1761852742908, "tmdate": 1762915786786, "mdate": 1762915786786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a collaborative alternative to adversarial training for minimizing the Jeffreys divergence. The key idea is to introduce a proxy model \\(q_ψ\\) that both fits the data and serves to approximate the reverse KL term \\(DKL(p_θ || π)\\) via \\(DKL(p_θ || q_ψ)\\)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This study proposes an adversarial training method for minimizing the Jeffreys divergence. The method is well motivated and theoretically supported. I find no critical flaws in the derivation. However, I am not very familiar with energy-based models and could not fully appreciate the significance of the contributions."}, "weaknesses": {"value": "See above."}, "questions": {"value": "- As I understand it, the KL divergence is preferred because it is connected to maximum likelihood estimation, which yields asymptotically efficient estimators. What is the statistical advantage of symmetrizing the KL divergence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "sw1WL0SVno", "forum": "ykgeXlyWDU", "replyto": "ykgeXlyWDU", "signatures": ["ICLR.cc/2026/Conference/Submission1506/Reviewer_j8Dk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1506/Reviewer_j8Dk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985843050, "cdate": 1761985843050, "tmdate": 1762915786650, "mdate": 1762915786650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}