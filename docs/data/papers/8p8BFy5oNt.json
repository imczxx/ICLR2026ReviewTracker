{"id": "8p8BFy5oNt", "number": 4215, "cdate": 1757639366845, "mdate": 1763752361880, "content": {"title": "DAWN: Dual Space Regeneration Attack", "abstract": "The growing use of generative models has intensified the need for watermarking methods that ensure content attribution and provenance. While recent semantic watermarking schemes improve robustness by embedding signals in latent or frequency representations, we show they remain vulnerable even under resource constrained adversarial settings. We present \\textsc{DAWN}, a training-free, single-image attack that removes or weakens watermarks without access to the underlying model. By projecting watermarked images onto natural priors across complementary representations, \\textsc{DAWN} suppresses watermark signals while preserving visual fidelity. Experiments across diverse watermarking schemes demonstrate that our approach consistently reduces watermark detectability, revealing fundamental weaknesses in current designs. Our code is available at \\url{https://anonymous.4open.science/r/DAWN-567A/}", "tldr": "", "keywords": ["watermarking", "attacks", "vision"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c33816c540ca509838eee15bad468a94f2c8cb33.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes DAWN, a training-free, single-image, model-agnostic watermark removal attack that sequentially (1) performs frequency-domain reconstruction, (2) applies diffusion-based semantic refinement, and (3) matches channel-wise mean/variance for color/tone correction. Experiments across pixel/frequency/latent watermark families report high attack success, though the authors acknowledge chroma/hue shifts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- __Practical adversarial setting__. The no-box, single-image threat model is realistic and clearly stated, and the pipeline is simple, fast (single pass), and model-agnostic.\n- __Clear pipeline design with insights motivated__. The paper articulates the role of each stage (spectral suppression → semantic restoration → color alignment), with an explicit color-correction formula."}, "weaknesses": {"value": "- __Perceptual quality degradation__ (visible color/tone shift).    \nAlthough the paper claims “perceptual and semantic consistency,” the qualitative figures indicate noticeable hue shifts; the authors themselves note that luminance is preserved while chroma deviates. In an attack intended for usable images, this level of color drift is a material drawback. A user seeking to remove a watermark typically still wants to use the resulting image. Current evidence suggests DAWN often achieves success by sacrificing chromatic fidelity (the images appear noticeably “purpler” in multiple visualizations). I recommend reporting color-sensitive metrics in addition to PSNR/SSIM/LPIPS. Further, a user-study on color acceptability (or thresholds) would make the “perceptual fidelity” claim more persuasive.\n\n- __Section §3 hypothesis is not fairly tested.__    \nThe stated hypothesis is that frequency-based reconstruction is more effective than pixel-only regeneration at weakening frequency-domain watermarks. However, the current experiment demonstrates higher success at the cost of worse perceptual quality (higher LPIPS, lower CLIP similarity), which is expected—stronger distortion can trivially improve removal. To validate the hypothesis fairly, match image quality across methods (e.g., tune the diffusion regeneration strength/steps and the frequency UNet noise/mask until PSNR/LPIPS/ΔE are aligned), then compare detector p-values. Otherwise the conclusion conflates attack strength with tolerated degradation.\n\n- __Novelty is incremental/assembly-style.__    \nThe approach is largely a sequential combination of a known frequency-space denoising/reconstruction, a standard img2img refinement, and simple channel-wise normalization. The paper’s primary contribution is empirical: showing this particular stacking is effective under the single-image threat model. The methodological novelty is modest.\n\n- __Evaluation design favors DAWN via luminance-heavy reporting.__    \nSuccess is reported alongside PSNR/SSIM/LPIPS and CLIP, but then SSIM_lum and CLIP_lum are emphasized—metrics that explicitly downweight color errors. This can systematically under-report DAWN’s most visible artifact (hue shift). Fairness requires quality-matched comparisons across attacks and inclusion of color-perception metrics\n\n- __Baselines not run at matched quality.__   \nSome baselines (e.g., semantic regeneration or “imprint-removal”) could plausibly improve success if allowed to trade perceptual quality for removal. The paper should retune competing attacks to reach comparable PSNR/LPIPS, then compare success rates, exactly as recommended for §3. This will clarify whether DAWN’s advantage persists when the “cost” (quality loss) is controlled. For instance, for SemRefine, in Zodiac Table 15, the authors tune the regeneration steps to controll the attack strength."}, "questions": {"value": "Please refer to the Weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "phvQ31ufXn", "forum": "8p8BFy5oNt", "replyto": "8p8BFy5oNt", "signatures": ["ICLR.cc/2026/Conference/Submission4215/Reviewer_HStc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4215/Reviewer_HStc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760467650543, "cdate": 1760467650543, "tmdate": 1762917232892, "mdate": 1762917232892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DAWN, a training-free, single-image, model-agnostic attack that removes or weakens generative watermarks by sequentially projecting watermarked images onto natural priors across frequency and semantic domains. DAWN achieves >95% success on classical pixel/frequency watermarking schemes and 70–90% on latent-space methods (TREE-RING, ZODIAC) while maintaining high perceptual fidelity. The paper highlights structural vulnerabilities of current watermarking approaches."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The paper introduces a novel attack formulation by introducing a practical single-image, no-box adversarial setting rarely explored in watermarking research (although related work and references could be extended, see below)\n* The method appears to be simple and generalisable; it's training-free at inference and adaptable across watermark types and domains\n* The benchmark is comprehensive, indlucing six waterarking schemes \n* Overall, the paper is clear and raises important concerns for the robustness of watermarking"}, "weaknesses": {"value": "* Related work and references are limited and should be extended to contextualise the work\n* The evaluations rely on stable diffusion-based backbones and it's unclear how it generalises to other architectures\n* While “training-free,” the method still relies on access to large pretrained generative models\n* A bit more depth of publishing watermarking-removal pipelines would be appreciated"}, "questions": {"value": "* How does DAWN perform on more recent watermarking systems that interleave multiple frequency bands or use cryptographic verification?\n* Could DAWN be adapted for video or multimodal watermarking schemes?\n* What safeguards do the authors suggest for responsible disclosure or controlled release of such attacks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "m4PnhmwirC", "forum": "8p8BFy5oNt", "replyto": "8p8BFy5oNt", "signatures": ["ICLR.cc/2026/Conference/Submission4215/Reviewer_gB8N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4215/Reviewer_gB8N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922984264, "cdate": 1761922984264, "tmdate": 1762917232652, "mdate": 1762917232652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DAWN, a single-image, training-free attack designed to remove watermarks from images. The method works without access to the generative model or binary messages. It combines three stages: (1) a frequency-domain UNet to suppress spectral artifacts, (2) a diffusion-based semantic refinement (img2img) to restore image structure, and (3) a perceptual color correction step to match the original image's statistics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "S1. Realistic Threat Model: The attack setup is practical and compelling. It operates under a highly constrained, realistic threat model: it is training-free (at inference time), model-agnostic, and only requires a single watermarked image."}, "weaknesses": {"value": "W1. Missing experimental details: Key experimental details are ambiguous. In Section 3, the paper describes \"a single pass of SD-v2 img2img\". It is unclear if this refers to using only the VAE autoencoder for reconstruction or applying a full diffusion-purification step. If it's the latter, the noise level and diffusion parameters are not specified.\n\nW2. Motivating experiment: The experiment in Section 3, which motivates the entire approach, is not very convincing. It claims the frequency-domain UNet is more effective at watermark removal than the diffusion model. However, it also reports that the UNet's output has significantly worse perceptual quality (LPIPS 0.53 vs. 0.10). The improved \"removal\" is likely just a byproduct of greater image degradation. A fair comparison would require evaluating both methods at a fixed level of perceptual quality.\n\nW3. Qualitative Results: The attack severely degrades image quality, rendering the \"attacked\" images unusable. The qualitative results in Figures 1, 4, and 6 show extreme color artifacts (strong purple and green tints). This is supported by the quantitative metrics in Table 2, which report PSNR values as low as 14.56 for the SDP dataset, which is very low.\n\nW4. Weak WM Baselines: The attack is primarily evaluated against weak or outdated watermarking methods (e.g., DWTDCT, DWTDCTSVD, Riavgan, SSL). More robust, state-of-the-art methods (e.g., TrustMark, Invismark, WAM) are not included.\n\nW5. Unfair baseline for attacks: The comparison to baseline attacks (imprint-removal, regeneration-based) in Section 6.2  is incomplete. The paper reports their attack success (Fig 3)  but omits their corresponding perceptual quality metrics (PSNR, LPIPS, etc.). This makes it impossible to evaluate the trade-off between removal success and image fidelity, which is the key metric for any attack.\n\nMinor. Missing citations:\n- https://arxiv.org/abs/2310.07726\n- https://proceedings.neurips.cc/paper_files/paper/2024/file/67b2e2e895380fa6acd537c2894e490e-Paper-Conference.pdf"}, "questions": {"value": "Q1: The paper states in Section 3 and Section 5  that it \"embed[s] TREE-RING watermarks\" or \"appl[ies] the target watermarking schemes\" to existing clean images. However, Treering is an in-generation watermark that cannot be applied post-hoc. How was this implemented?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cGjP5O6KVw", "forum": "8p8BFy5oNt", "replyto": "8p8BFy5oNt", "signatures": ["ICLR.cc/2026/Conference/Submission4215/Reviewer_198Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4215/Reviewer_198Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938169065, "cdate": 1761938169065, "tmdate": 1762917232412, "mdate": 1762917232412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **DAWN**, a training-free, single-image, model-agnostic attack that aims to suppress semantic and frequency-space watermarks by sequentially applying (1) frequency-domain reconstruction, (2) diffusion-based semantic refinement, and (3) tone/color correction. \nExperiments target multiple watermarking schemes and report relatively high attack success rate at the price of low image perceptual quanlity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper addresses a timely and relevant problem, that is practical single-image attacks on modern semantic/frequency watermarks with a clear threat model. \n- The two design principles (frequency-space projection for suppression; diffusion for semantic recovery) are motivated by an analysis highlighting the limits of pixel-only regeneration."}, "weaknesses": {"value": "* **Missing trade-off reporting.** Section 3 argues frequency-domain reconstructions suppress watermarks more effectively than pixel-based diffusion but at the cost of **perceptual quality**, which is a widely recognized trade-off in the watermark-attack literature. However, the experiments do not explicitly present **paired** trade-off curves/tables (e.g., **PSNR/LPIPS vs. attack success rate**) across methods to quantify this. This makes it hard to judge whether DAWN’s higher success is achieved at an acceptable perceptual cost. \n* **Perceptual degradation.** Even ignoring the trade-off framing, the reported image-quality are low for semantic watermarks (e.g., PSNR ≈ 14–16 dB on TREE-RING/ZODIAC), and visualizations show notable color-tone shifts relative to the originals. This suggests the method’s higher success rate is obtained by **substantial** degradation, weakening the fairness of comparisons that claim superiority over baselines without controlling quality levels. \n* **Visualization intent unclear.** The **luminance** comparisons in Figure 4 are not clearly tied to conclusions; moreover, DAWN does not appear consistently better than baselines in luminance-based views, and the qualitative benefit of including these panels is ambiguous. \n* **Ambiguity about “single forward pass”** The paper states it uses a *single forward pass* and “takes couple of seconds,” yet Step 2 is **diffusion-based semantic refinement** (img2img), which typically entails multiple denoising steps. The intended definition of “forward pass” is unclear, and end-to-end compute vs. imprint-removal is not reported in directly comparable units. \n* **Limited originality.** The three stages—frequency-domain reconstruction, diffusion regeneration, and simple channel-wise color statistics matching, are each close to existing methods; the novelty is modest in algorithmic terms."}, "questions": {"value": "1. Can you report **paired** PSNR/LPIPS vs. **attack-success** curves (or Pareto tables) across all compared attacks to enable *quality-controlled* comparisons? This would address the concern that higher success may come from larger perceptual changes.\n2. What exactly constitutes one “forward pass” in DAWN? How many diffusion steps are run in Step 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jYrynMW764", "forum": "8p8BFy5oNt", "replyto": "8p8BFy5oNt", "signatures": ["ICLR.cc/2026/Conference/Submission4215/Reviewer_PE2y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4215/Reviewer_PE2y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949694460, "cdate": 1761949694460, "tmdate": 1762917232160, "mdate": 1762917232160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}