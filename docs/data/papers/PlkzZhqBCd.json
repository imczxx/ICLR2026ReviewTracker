{"id": "PlkzZhqBCd", "number": 24318, "cdate": 1758355414822, "mdate": 1759896771582, "content": {"title": "Enhancing Language Model Reasoning with Structured Multi-Level Modeling", "abstract": "Inference-time scaling enhances a model’s reasoning by extending its chain-of-thought (CoT). However, existing approaches typically rely on a single policy trained with outcome-reward RL fine-tuning, which often suffers from CoT derailment, i.e., the reasoning trajectory drifts off-course due to compounding errors. This problem is particularly severe for smaller LMs with long CoTs due to their limited capacity. To address this, we propose Multi-Level Reasoning (MLR), a framework that augments single-policy methods with high-level policy modeling. Specifically, MLR incorporates two policies: a high-level planner that generates step descriptors (abstract sub-goals) and a low-level executor that produces detailed content conditioned on these descriptors. The planner then summarizes the content to guide the next sub-goal, forming an alternating plan–execute loop. To reduce cost, we adopt a minimal design, where the base model serves as the low-level policy and a lightweight LoRA module implements the high-level policy. We further observe that outcome-reward RL is inefficient for long trajectories (e.g., exceeding 4K tokens). To overcome this, we introduce online Step-DPO, a process-level preference optimization scheme that leverages Twisted Sequential Monte Carlo (TSMC) to provide scalable stepwise supervision. This yields more efficient training, improved stability, and higher accuracy. Extensive experiments on challenging math, science, and logical reasoning benchmarks demonstrate that, with only 10% SFT data and 5% of preference pairs, our method outperforms both the DeepSeek-R1 distillation baseline and the outcome-reward RL baseline across multiple base models and tasks.", "tldr": "", "keywords": ["Chain-of-Thought reasoning", "Direct Preference Optimization", "Process supervision", "Twisted Sequential Monte Carlo", "Large language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/479f6ae803334cf1f8d94f534202e1ff5ef5d50b.pdf", "supplementary_material": "/attachment/1abcef4d27503857b113377777b7d4ac4730c790.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Multi-Level Reasoning (MLR), a framework for improving long-horizon reasoning in language models by decomposing the chain-of-thought (CoT) into high-level abstract descriptors and low-level detailed contents. A lightweight LoRA-based high-level planner alternates with the base LM as the low-level executor, forming an iterative plan–execute loop.\n\nTo address the inefficiency of outcome-reward RL for long trajectories, the authors propose Online Step-DPO, a process-level preference optimization method that uses Twisted Sequential Monte Carlo (TSMC) to compute step-wise survivability utilities. This enables dense, on-policy supervision without training a separate process reward model.\n\nExperiments on math (MATH500, AIME24), science (GPQA-Diamond), and logical reasoning (BoardGameQA-Hard) benchmarks show that MLR outperforms distillation and RL baselines such as DeepSeek-R1-Distill and SimpleRL, often with substantially fewer preference pairs (5%) and SFT data (10%)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-motivated. Alternating plan-execute loop allows dynamic adjustment and improves interpretability.\n\n2. The experiments demonstrate consistent gains across several benchmarks."}, "weaknesses": {"value": "1. The writing can be further improved for clarity and completeness. For example, in the abstract, abbreviations such as RL (reinforcement learning) and LM (language model) should be defined upon their first appearance.\n\n2. The method contribution of the paper appears limited. However, I appreciate that the authors conduct sufficient ablation studies, which provide evidence for the empirical effectiveness of the proposed approach.\n\n3. It would be interesting to see how the model performs when only the high-level reasoning outputs are generated during inference, without the detailed low-level reasoning steps. This could offer insights into the interpretability and abstraction capability of the high-level policy.\n\n4. The paper should clarify the seed datasets used to generate the CoT samples. In addition, the comparisons could be made fairer by including baselines that directly apply SFT and RL on the same datasets."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "htWVZWy4Kx", "forum": "PlkzZhqBCd", "replyto": "PlkzZhqBCd", "signatures": ["ICLR.cc/2026/Conference/Submission24318/Reviewer_fiGr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24318/Reviewer_fiGr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814192914, "cdate": 1761814192914, "tmdate": 1762943040105, "mdate": 1762943040105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Multi-Level Reasoning (MLR), an SFT+DPO framework that addresses two critical limitations of single-policy long CoT approaches: CoT derailment and sparse reward guideline. MLR decomposes the reasoning process into a hierarchical structure with high-level step descriptors generated by a lightweight LoRA module and low-level detailed content produced by the base model, operating through an alternating plan-execute loop. The framework employs SFT to initialize both policies on multi-level decomposed data, followed by online Step-DPO that leverages Twisted Sequential Monte Carlo (TSMC) to generate process-level preferences for stepwise supervision without requiring a separate process reward model. Finally, the authors provide empirical results and ablation studies to show its effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The framework is clear and easy to understand, and the paper is generally well-written.\n\n2. The empirical experience is extensive. Authors test the proposed method in several areas (math, science and logic) with average scores from multi-individual runs to show a stable improvement. The ablation study in Table 2 clearly demonstrates the effectiveness of each component in MLR."}, "weaknesses": {"value": "1. One motivation, \"CoT derailment\" is unclear and hard to follow. \n\n2. To demonstrate the effectiveness of their approach, the authors compare MLR with several baselines. However, I think this comparison is somewhat unconvincing, as the models are trained on different datasets. It would be more meaningful to evaluate the methods on the same dataset (important) while varying only the training strategies — for example, comparing SFT+GRPO (baseline) and MLR (your proposed framework) under identical data conditions.\n\n3. The process of summarization from high-level abstraction to low-level details might also be an important factor, but the paper does not explore this aspect in depth. (Only said \"including them improves performance and facilitates training\" in the paper)\n\n4. The choice of K=4 rollouts in TSMC appears somewhat arbitrary, and the paper does not provide any preliminary analysis to justify this setting. I believe that an ablation study would be highly valuable for demonstrating the robustness of the framework in real-world scenarios."}, "questions": {"value": "1. I am wondering what the sources of your training dataset and prompts are, as this information is important not only for reproducibility but also for ensuring a fair comparison.\n\n2. The summarization step is underspecified. What model performs this? How long are summaries? \n\n3. What's the definition of \"Estimated survival accuracy\" in Figure 5 (a)?\n\n4. When computing the utilities (Equations (3) and (4)) in process preference modeling, do you sample only once or perform multiple sampling rounds to compute the average score? I guess that sampling only once could introduce too much randomness.\n\n5. Authors said they fine-tune a small LM on the low-level SFT data for \"fast rollout\", and this small model achieves comparable estimation accuracy while being much faster. How small is it? The paper lacks relevant details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZTLFgT29BX", "forum": "PlkzZhqBCd", "replyto": "PlkzZhqBCd", "signatures": ["ICLR.cc/2026/Conference/Submission24318/Reviewer_7Cso"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24318/Reviewer_7Cso"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836215228, "cdate": 1761836215228, "tmdate": 1762943039818, "mdate": 1762943039818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Multi-Level Reasoning (MLR), a framework that addresses limitations in single-policy long chain-of-thought (CoT) approaches for language model reasoning. MLR decomposes reasoning into alternating high-level step descriptors (abstract sub-goals) and low-level detailed content. The framework uses the base model as the low-level policy and a lightweight LoRA module for the high-level policy. To enable efficient training, the authors introduce online Step-DPO, which leverages Twisted Sequential Monte Carlo (TSMC) to provide process-level preference signals without requiring a separate reward model. Experiments on MATH500, AIME24, GPQA-diamond, and BoardGameQA-hard demonstrate improvements over both distillation baselines (DeepSeek-R1-Distill) and outcome-reward RL methods across three base models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel application of HRL concepts to reasoning tasks with a practical, efficient design (frozen base + LoRA)\n- TSMC-based process preferences are creative and avoid the overhead of training separate process reward models\n- Strong empirical results: MLR consistently outperforms DeepSeek-R1-Distill despite using only 10% SFT data and 5% preference pairs\n- Addresses a timely problem as reasoning models gain prominence"}, "weaknesses": {"value": "- 各个组件的创新性有限 - 主要贡献在于其组合方式而非从根本上采用新的技术"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FqkosSMyYR", "forum": "PlkzZhqBCd", "replyto": "PlkzZhqBCd", "signatures": ["ICLR.cc/2026/Conference/Submission24318/Reviewer_yNnM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24318/Reviewer_yNnM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884412342, "cdate": 1761884412342, "tmdate": 1762943039628, "mdate": 1762943039628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose Multi-Level Reasoning to enhance LLM reasoning with structured multi-level modeling. Specifically, a high-level planner first generates step descriptors as the roadmap, and then the low-level executor generates the detailed content based on the high-level planner, with the help of the online step-DPO algorithm to provide step-wise supervision. The proposed model achieves good performance on both math and science reasoning tasks based on different backbone models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tOverall, the idea is sound, and the proposed method achieved good results.\n-\tThe authors have conducted ablation tests to verify its essential components."}, "weaknesses": {"value": "-\tIn Section 3.2, the authors first fine-tune based on the low-level content via full SFT, and then learn from the high-level content via LoRA. The order of training, as well as the usage of full SFT/LoRA on different high-/low- level data, should be discussed (better with detailed experimental results).\n-\tIn the main experiment, although the authors have implemented their method on diverse backbone models, the compared baselines are not strong enough. Other popular preference optimization methods (e.g., RL methods such as GRPO) should be compared.\n-\tIn the Related work, there are some step-wise preference optimization methods or those that also adopt certain forms of overall planners, which should be discussed and even compared. The authors are suggested to highlight the technical novelty of the proposed model compared to existing efforts."}, "questions": {"value": "Refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6Ni6dBrOel", "forum": "PlkzZhqBCd", "replyto": "PlkzZhqBCd", "signatures": ["ICLR.cc/2026/Conference/Submission24318/Reviewer_bbdw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24318/Reviewer_bbdw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088913878, "cdate": 1762088913878, "tmdate": 1762943039431, "mdate": 1762943039431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}