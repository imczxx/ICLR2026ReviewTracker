{"id": "GuCkBGef6C", "number": 18489, "cdate": 1758288198112, "mdate": 1759897100009, "content": {"title": "MIABench: Full-Pipeline Evaluation of Membership Inference Attacks", "abstract": "Membership inference attacks (MIAs) are widely used to assess a model's vulnerability to privacy leakage by determining whether specific data instances were part of its training set. Despite their significance as a privacy metric, existing evaluations of MIAs are often limited to isolated and inconsistent scenarios, hindering comprehensive comparisons and practical insights. To address this limitation, we analyze the full pipeline of training models and conducting MIAs, and present a comprehensive benchmark for evaluating various MIA methods on deep learning models. We establish a reproducible benchmark suite with code and models, leaderboards, detailed insights into the mechanisms of different MIA approaches, and practical guidance for selecting and applying MIAs effectively. This work enhances the understanding and application of MIAs, providing a solid foundation for advancing privacy-preserving machine learning research.", "tldr": "", "keywords": ["Membership Inference Attack; Machine Learning Privacy"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/19f4a064bd4be1c4213cec08335cad2d8ea23a9c.pdf", "supplementary_material": "/attachment/d491d26c92b0b7c82cd36f65db68146515b99f19.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a benchmark suite that streamlines the application of diverse membership inference attacks (MIAs) based on the evaluation criteria (attack or audit) for the purpose of evaluating various MIA methods on deep learning models. MIA performance is used as a metric to assess privacy-preserving capabilities of machine learning algorithms, but there is a dearth of work which allows for fair and comprehensive comparison of diverse MIA approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The benchmark suite in this paper supports the evaluation of different MIAs within a common [black-box] threat model, thereby making any results obtained from using the code base fair and comparable.\n- In attack mode, the suite selects hyperparameters for MIA, such as the scaler threshold, using the shadow models.\n- The suite also allows for using MIAs to evaluate the performance of different machine unlearning methods.\n- It provides evidence in support of the argument that metric-based MIAs, which are tailored to be sensitive towards membership signal as compared to non-membership signal, might be a poor choice to judge the efficiency of machine unlearning methods. This is because the samples \"unlearned\" via these approaches ought to be confidently regarded as non-members by MIAs to claim efficient unlearning, and this is a task for which SOTA metric-based MIA methods might yield misleading estimates.\n- In section G1, they comprehensively detail how the utility of the model is affected in different experiments undertaken in the paper.\n- The author(s) have shared the code for review."}, "weaknesses": {"value": "- In lines 317-319, the authors mention that using DP-SGD to train the models does not compromise their utility, but they do not quantify the privacy guarantees using any conventional metrics of DP, such as $\\epsilon$ in $\\epsilon$-DP [1] or $\\mu$ in $\\mu$-GDP [2]. \n- LiRA is known to perform poorly with a low # of shadow models (such as the 5 used in the paper). Compared to that, RMIA offers much better results, as evident from your experiments. Using a weaker version of the attack does not make for a fair comparison against other MIAs in the experiments.\n- In the standard MIA threat model (the attack mode), the target model's training hyperparameters are presumed to be known and are repurposed for training the shadow models. If we go by the author(s)'s differentiation between attack and audit mode based on the knowledge of the training dataset, knowledge of training hyperparameters would also be a feasible assumption in the latter, but not always in the former. The author(s) do not account for this in their benchmark suite's MIA pipeline. \n\n[1] Dwork et al. “Calibrating Noise to Sensitivity in Private Data Analysis.” TCC 2006.\n\n[2] Dong, J. et al. “Gaussian Differential Privacy.” CoRR 2019."}, "questions": {"value": "**Questions**: This work is a commendable step towards creating a benchmark suite for evaluating different MIA configurations in a fair manner, allowing comparison of performance across multiple evaluation metrics. Although there are parts of it that need to be addressed, as listed in the weaknesses. I am amenable to revising my initial assessment provided that the authors can address the concerns highlighted in the weaknesses as listed above.\n\n**Suggestions**: The following minor corrections/ suggestions can help improve the presentation of the paper:\n\n- Minor Correction #1: Figure 2(a) says \"Epoch\" along the x-axis when it should be mislabel rate as stated in the caption.\n- Minor Correction #2: Line 291 mentions 2 sentences which imply the same thing. It is an unnecessary repetition.\n- Minor Correction #3: In line 678, it should be \"we then use\", not \"we then then use\".\n- Minor Correction #4: In line 705, missing parentheses around Bertran et al. (2024).\n- Minor Correction #5: Line 708 \"RMIA: RMIAZarifzadeh et al. (2024)\" is missing space indentation and parentheses around the reference. Also, in the same line, it's \"LiRA\", not \"LiRa\".\n- Minor Suggestion: Most results/ claims are made with respect to MIA accuracy, which papers like Carlini et al. (2022) have asserted is a metric that weighs non-members and members equally. Instead, using F1 or Recall (TPR) would be much more appropriate as metrics. For example, a high F1@P (or low F1@N) would imply that MIAs are more efficient at detecting members, which is significantly more detrimental as a privacy risk than correctly identifying non-members (@N)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3dZr4KhzJl", "forum": "GuCkBGef6C", "replyto": "GuCkBGef6C", "signatures": ["ICLR.cc/2026/Conference/Submission18489/Reviewer_GeVM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18489/Reviewer_GeVM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761478648630, "cdate": 1761478648630, "tmdate": 1762928184539, "mdate": 1762928184539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reproducible benchmark suite that standardizes the evaluation process of membership inference attacks. It introduces two distinct evaluation scenarios: an \"attacking mode\" (where the adversary lacks membership knowledge for hyperparameter tuning and must rely on shadow models) and an \"auditing mode\" (where a model owner has access to membership information to find the optimal attack hyperparameters). Using this benchmark, the paper evaluates a wide range of MIA methods across different datasets, model architectures, and training algorithms. The key findings highlight that a model's generalization gap (the difference between training and test accuracy) is a primary indicator of its vulnerability to MIAs. The authors also observe a high agreement among the predictions of the top-performing attacks, suggesting they exploit similar model properties. Finally, the paper provides a practical guide for selecting appropriate MIA methods based on the evaluation scenario."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Comprehensive Pipeline: The paper's main strength is its holistic approach, evaluating MIAs across the machine learning pipeline (data, model, training algorithm, and hyperparameter selection) rather than in an isolated step.\n*  The paper provides practical takeaways. The strong correlation found between the generalization gap and MIA vulnerability provides a good metric for developers to monitor.\n* Extensive Experimentation: The benchmark is tested across numerous variables: MIA methods, datasets, architectures, and training algorithms (including DP-SGD and machine unlearning), providing a robust empirical foundation for the paper's conclusions."}, "weaknesses": {"value": "* Scope Limited to Vision: As acknowledged by the authors, the benchmark is currently focused exclusively on computer vision datasets (CIFAR-10, CIFAR-100, ImageNet100) and classification models. The findings may not directly generalize to other domains, such as NLP, time-series, or generative models, which have different data characteristics and model architectures.\n* Focus on Black-Box Access: The study limits its scope to black-box attacks where the adversary only has access to the model's output logits. While common, this excludes more powerful white-box attacks (which have access to gradients or parameters) that are also a significant threat.\n* Overly Pessimistic Attacker Model: The \"attacking mode\" assumes an adversary with minimal knowledge of the training data distribution. This may not represent sophisticated, real-world attackers who often have strong prior knowledge (e.g., knowing a model was trained on public data like Common Crawl or Wikipedia), allowing them to create far more accurate shadow models and blur the distinction between the \"attacking\" and \"auditing\" scenarios."}, "questions": {"value": "- Could the benchmark be extended to include a scenario that models an adversary with partial, high-level knowledge of the training data distribution (e.g., \"trained on public web data\"), bridging the gap between the current \"attacking\" and \"auditing\" modes?\n\n- How much more effective would these MIAs be in a white-box scenario?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PoUtVAwOvs", "forum": "GuCkBGef6C", "replyto": "GuCkBGef6C", "signatures": ["ICLR.cc/2026/Conference/Submission18489/Reviewer_ff2B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18489/Reviewer_ff2B"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761519139506, "cdate": 1761519139506, "tmdate": 1762928182527, "mdate": 1762928182527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of membership inference attacks (MIAs), a key technique for evaluating a model’s vulnerability to privacy leakage by determining whether specific data samples were part of its training set. In particular, the paper attempts to conduct a systematic analysis of the entire MIA pipeline and introduces a benchmark for evaluating and comparing different MIA methods across standard deep learning models and benchmark datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Benchmarking privacy leakage is of practical importance, and this is a timely contribution that provides a useful benchmark.\n\n- The presentation is generally clear and the paper is well structured overall.\n\n- The experimental evaluation is extensive in quantity"}, "weaknesses": {"value": "- It is unclear what is new or novel in this work. The paper seems present very limited new insights or findings for the community, \nthe key observations that *MIA performance correlates with the generalization gap* and that *hyperparameter selection plays a significant role* are already well established in the privacy literature.\n\n- The data modality is limited to images and the models are restricted to CNN architectures. While this setup is standard in the MIA literature, it lacks diversity, which is limiting for a paper that aims to advance the frontier of MIA benchmarking in the current community.\n\n- The paper lacks a clear description of the threat model, i.e., it does not specify explicitly what information or access each attack assumes. For example, based on this submission alone, it remains unclear whether each method has knowledge of the full dataset, partial dataset, partial data distribution, or whether the attacks operate in a white-box or black-box setting (e.g., logit or label access). The current comparisons are therefore not entirely transparent, even though some settings (such as the distinction between attacking vs. auditing modes) are implicitly defined."}, "questions": {"value": "- Section 2 Table 1: It would be better to directly include references to the corresponding papers in the table (or clearly indicate the naming correspondence in the text). As it stands, it is not entirely clear which paper each entry or column in the table refers to. While some of this information is available in the Appendix, it would still be helpful to make the mapping more explicit within the main text or table itself.\n\n- The current distinction between attacking and auditing modes is somewhat confusing. In this paper, the difference appears to be limited to how hyperparameters are selected, whereas in the broader literature, auditing typically refers to a stronger attack assumption that often reflecting a worst-case scenario where the adversary can actively select or construct “hard examples” (e.g., canaries) to better probe model privacy. This conceptual misalignment makes the terminology here potentially misleading.\n\n- Related to the above point (see \"Weakness\" section): the paper is not fully self-contained. While this may be understandable for a large-scale benchmark, it would still be very helpful if the authors could provide clearer and more detailed descriptions of each method used in the experiments, e.g., summarizing the key ideas, main formulas, and assumed threat models for each attack under a consistent notation framework."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8ezis8SFvE", "forum": "GuCkBGef6C", "replyto": "GuCkBGef6C", "signatures": ["ICLR.cc/2026/Conference/Submission18489/Reviewer_Zs8w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18489/Reviewer_Zs8w"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923464765, "cdate": 1761923464765, "tmdate": 1762928180130, "mdate": 1762928180130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MIABench, a comprehensive benchmark for evaluating Membership Inference Attacks (MIAs) on classical deep learning classifiers under black-box access. MIABench includes a reproducible suite (including code, datasets, models, etc.) and defines two practical evaluation modes, which are Attacking mode and Auditing mode. The extensive experiments have explored the impact of mutiple different factors on MIAs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Comprehensive Coverage: evaluations span critical dimensions like data, architecture, training, hyperparameters and metrics.\n- Utility for the Community: rhe benchmark’s reproducibility and extensibility enable future MIA research."}, "weaknesses": {"value": "- The MIABench only targets traditional visual classification models, e.g., CNN models.However, most state-of-the-art MIAs have spread to more models and scenarios, like [1] and [2]. It does not address multimodal data (text, audio) or the state-of-the-art large generative models (e.g., GPT, diffusion models, image autoregressive models), which are increasingly relevant for privacy research. \n- The table formatting is overly crude and non-standardized. For instance, Table 3 lacks dividing lines. The author is requested to carefully review the entire manuscript for similar standardization issues.\n- The related work mentions MIA methods but does not explicitly compare MIABench to existing MIA evaluation frameworks to highlight unique advantages.\n\n[1] Yu H, Qiu Y, Yang Y, et al. Icas: Detecting training data from autoregressive image generative models[J]. arXiv preprint arXiv:2507.05068, 2025.\n\n[2] Kowalczuk A, Dubiński J, Boenisch F, et al. Privacy Attacks on Image AutoRegressive Models[C]//Forty-second International Conference on Machine Learning."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vv6oHj2s3X", "forum": "GuCkBGef6C", "replyto": "GuCkBGef6C", "signatures": ["ICLR.cc/2026/Conference/Submission18489/Reviewer_Ctoz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18489/Reviewer_Ctoz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996834362, "cdate": 1761996834362, "tmdate": 1762928179384, "mdate": 1762928179384, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}