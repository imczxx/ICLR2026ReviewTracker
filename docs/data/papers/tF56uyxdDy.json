{"id": "tF56uyxdDy", "number": 5582, "cdate": 1757921189989, "mdate": 1759897966557, "content": {"title": "Test-Time Scaling with Reflective Generative Model", "abstract": "We introduce a new Reflective Generative Model (RGM), which obtains OpenAI o3-mini's performance via a novel Reflective Generative Form. This form focuses on high-quality reasoning trajectory selection and contains two novelties: 1) A unified interface for policy and process reward model: we share the backbone network and use task-specific heads for reasoning trajectory predicting and scoring respectively, introducing only 50M extra parameters for trajectory scoring. 2) Eliminating the reliance on process-level annotation: we provide a self-supervised process reward model (SPRM), which can directly learn the high-quality reasoning trajectory selection from the outcome reward. Equipped with the reflective generative form, RGM is naturally suitable for test-time scaling based on the controllable thinking length. Experiments demonstrate that our RGM with 50M SPRM outperforms 72B reward models and enables QwQ-32B outperforms OpenAI o3-mini on AIME24 (84.2 vs 79.6) and HMMT25 (53.1 vs 53.0). Code will be available.", "tldr": "", "keywords": ["LLM Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/37735a2ca54d9fc886b8e41a710e3d381728513e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on parallel TTS techniques to enhance the reasoning capabilities of large models. The authors propose Reflective Generative Model (RGM), where the core idea is to have the policy model and the PRM share the same backbone network. This reduces the parameter and computational overhead associated with deploying a separate PRM. Experiments demonstrate that a 32B model equipped with just a 50M-parameter SPRM can outperform OpenAI's o3-mini on challenging reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  clear performance improvement is observed as the number of candidates increases, proving the method's effectiveness. \n2. The shared-backbone design adds minimal parameter overhead, significantly reducing computational costs.\n3. As claimed by the authors, RGM enables efficient reasoning selection while eliminating the need for expensive process-level annotation."}, "weaknesses": {"value": "1. Scalability Concerns: Like many PRM methods, this approach requires manually decomposing responses into \"Steps.\" This process relies heavily on human priors and empirical design, raising doubts about its scalability to new tasks.\n2. A core claimed contribution of the paper is removing reliance on process-level supervision. However, ImplicitPRM already claims they can fulfill this (learn PRM from ORM ). How do you position your work relative to this line of work, which suggests that a PRM can function without any extra parameters?"}, "questions": {"value": "Could you elaborate on how the \"Step-tokens\" are identified and inserted during the reasoning process?\n\nIs it possible to compare with the ImplicitPRM methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y8e2eftDwX", "forum": "tF56uyxdDy", "replyto": "tF56uyxdDy", "signatures": ["ICLR.cc/2026/Conference/Submission5582/Reviewer_uFte"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5582/Reviewer_uFte"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664187198, "cdate": 1761664187198, "tmdate": 1762918147442, "mdate": 1762918147442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Reflective Generative Models to enable test time scaling. Authors propose a unified interface where the policy model and PRM share the same backbone network, which saves parameter; they also introduce a Self-supervised PRM that learns to evaluate reasoning quality from only outcome-level supervision. Experiment results show that RGM with SPRM achieves performance comparable to OpenAI o3-mini on math benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The unified interface design is elegant and addresses a real deployment challenge in test-time scaling. Achieving comparable performance to 72B parameter reward models with only 50M parameters is a solid contribution\n* The evaluation is thorough and well-designed with multiple base models and diverse architectures\n* Paper writing is clear and readable. The formalization of different inference paradigms provides a clean framework for understanding the contribution\n* The \"aha moment\" analysis is very interesting and provides valuable insights for future works"}, "weaknesses": {"value": "* The SPR loss introduces a bootstrapping process where the current model quality decides the psudolabel quality. I am curious if there is any formal analysis on the convergence properties. \n* Since the authors are proposing a novel architecture, I would expect more ablation studies on model design, for example the SPRM head MLP structure, and the geometric mean aggregation. \n* The method relies on '.\\n\\n' tokens as semantic boundaries, which assumes the policy model naturally produces well-segmented reasoning. This heuristic may not transfer to models with different output conventions, non-English languages, or domains where reasoning doesn't follow paragraph-like structure."}, "questions": {"value": "* Can the authors provide some analysis on the model's robustness to initialization? \n* Is there a way to detect the aha moment online during training, or can you predict when it will occur based on model size or other factors?\n* Additional ablation studies or explanations of design choices regarding the model architecture would strengthen the paper. See weakness.\n* Given the dependency on training SPRM jointly with each policy model, have you explored whether SPRM trained on one model can transfer to similar architectures with fine-tuning? This would make the approach more practical for people who want to apply it to other base models without access to full retraining."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QYn4efgqW6", "forum": "tF56uyxdDy", "replyto": "tF56uyxdDy", "signatures": ["ICLR.cc/2026/Conference/Submission5582/Reviewer_65ZK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5582/Reviewer_65ZK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762199625593, "cdate": 1762199625593, "tmdate": 1762918147137, "mdate": 1762918147137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RGM (Reflective Generative Model), a test-time scaling approach that shares the backbone network between the policy model and process reward model (PRM), adding only 50M parameters for trajectory scoring. The key innovation is a Self-supervised Process Reward Model (SPRM) that learns to evaluate reasoning steps using only outcome-level supervision. Experiments show strong empirical results across models of different sizes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Model overlap in the policy and reward model means that the inference overhead is minimal.\n- SPRM can be trained with just the outcome labels, and from the ablation experiments seems to correspond to process-level correctness. For example, score of the last step performs worse than the score of the entire sequence. \n- Good empirical results\n- Generalization beyond math to coding \n- The experimental setup is thorough and tested across multiple model sizes."}, "weaknesses": {"value": "- Important baselines such as majority voting is missing. Moreover, there have been recent work such as \"GenSelect: A Generative Approach to Best-of-N\" (Toshniwal, 2025) and \"Learning to Reason Across Parallel Samples for LLM Reasoning\" (Qi, 2025) which demonstrate strong parallel reasoning performance. \n- The claims about beating 72B RM is somewhat misleading. The Qwen2.5-RM models were trained on short, non-reasoning CoT solutions, and are not suitable for scoring the long reasoning traces generated by models evaluated in this paper. The parameter count claim of 50M beating 72B RM in the abstract is also somewhat misleading because the RM is sharing the backbone.\n- MCTS results being slightly worse than Best-of-N suggests the RM is still not good enough to conduct search which begs the question if a process reward model is really buying any performance in this setup.  Ideally, an ORM trained on the same data as SPRM would be a more fair comparison."}, "questions": {"value": "- Is the \"Aha moment\" surprising? Isn't it just training dynamics?\n- Why did you not train a new ORM on the training data for SPRM?\n- Why are obvious baselines like majority voting/self-consistency missing from the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3kfV3wvXyr", "forum": "tF56uyxdDy", "replyto": "tF56uyxdDy", "signatures": ["ICLR.cc/2026/Conference/Submission5582/Reviewer_RgpY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5582/Reviewer_RgpY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762228857391, "cdate": 1762228857391, "tmdate": 1762918146866, "mdate": 1762918146866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to improve the test-time performance of policy models. Specifically, the focus is on allowing the policy to generate multiple candidates for a given query and then using a process reward model to select the best candidate from the pool. The key innovation of this paper is sharing the backbone network between the policy and the process reward model to reduce parameter overhead. Additionally, the authors propose a method that leverages the consistency between final answer correctness and the scores generated by the process reward model, in order to mitigate the negative effects caused by false positive and false negative samples during the training of the process reward model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- By sharing the backbone network, the proposed method reduces the inference cost of using the PRM to evaluate policy rollouts.\n\n- Experimental results show that the proposed SPRM achieves superior performance with the addition of fewer parameters.\n\n- The proposed method is simple and seems to be effective."}, "weaknesses": {"value": "- Missing related work on process reward models. Several studies [1-4] also incorporate outcome labels to train a process reward model, which is highly relevant to this paper.\n\n- Other work [5] has introduced '\\n' as a step token. What is the rationale and benefit behind selecting '\\n\\n' instead? A concern is that if the policy model does not generate '\\n\\n', how would this method remain applicable?\n\n- Regarding line 219, I have a concern about the clarification: \"Since the representation in the last layer mainly captures the logits prediction for a single token, we use the hidden representations from the second-to-last layer of the policy model to provide richer contextual information.\" Why does the second-to-last layer provide richer contextual information than the last layer? More theoretical or empirical justification is needed for this assertion.\n\n[1] From r to Q: Your Language Model is Secretly a Q-Function, COLM 2024.\n\n[2] Discriminative Policy Optimization for Token-Level Reward Models, ICML 2025.\n\n[3] DPO Meets PPO: Reinforced Token Optimization for RLHF, ICML 2025.\n\n[4] Free Process Rewards without Process Labels, ICML 2025.\n\n[5] Let's Verify Step by Step, Arxiv 2023."}, "questions": {"value": "- What does the variable c represent in the Linear(c, 2c) at Line 206?\n\n- In Equation 6, what do the terms $y_i$ and $Score_i$ with the index $i$ represent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w9Doze0pal", "forum": "tF56uyxdDy", "replyto": "tF56uyxdDy", "signatures": ["ICLR.cc/2026/Conference/Submission5582/Reviewer_sWYC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5582/Reviewer_sWYC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240445193, "cdate": 1762240445193, "tmdate": 1762918146634, "mdate": 1762918146634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces reflective generative models for test-time scaling, which use a shared network for policy and process reward models. This dramatically decreases the number of extra parameters to 50M. Additionally, the paper introduces a self-supervised loss (SPR Loss), which directly learns the quality of the reasoning trajectory from the outcome reward. The core idea is to have the same model both generate reasoning trajectories and score them with minimal extra parameters. The authors conducted a wide range of experiments across baseline models and demonstrated high performance (OpenAI o3-mini level), outperforming billion-scale reward models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Originality: The paper proposes a highly original idea of using the same backbone for policy and reward models. This idea is a novel and exciting extension of prior reward models, which are typically large and separately trained. This idea opens up a lot of exciting directions for enabling richer interactions between reasoning trajectory generation and evaluation. \n- Quality: The proposed framework is clearly defined and well-motivated. The experimental evaluation is comprehensive, including multiple models and benchmarks. \n- Clarity: The text and figures are clear and easy to understand. \n- Significance: The paper makes a highly significant contribution to test-time scaling."}, "weaknesses": {"value": "1. The design of the self-supervised process reward loss (SPR loss) could benefit from additional motivation and clarification. Specifically, the binary weight w_n only includes a step in the loss when the predicted step score aligns with the final outcome. Why choose a hard threshold (0.5) vs other alternatives? Could such a hard cutoff potentially discard a large fraction of training samples, particularly early in training? And could this selective inclusion behavior relate to later observations, such as the “aha” moment? Also, a minor point: y_n is the correctness of the final answer, which shouldn’t depend on n? Why use a subscript (which could be misleading)? \n2. The paper’s discussion of the “aha moment” (Sec. 5.4; Fig. 5) is vague and under-analyzed. The authors highlight a green dashed line to indicate where correct and incorrect trajectory scores begin to diverge, yet provide no quantitative criterion for identifying this point. Visually, the gap between curves appears to increase gradually rather than showing a discrete transition. If there is indeed a transition, could it be simply explained by the use of a hard 0.5 threshold in the SPR loss? The authors could add more quantitative analysis on the representation or gradient through learning if this “aha moment” is indeed an important finding.  \n3. The paper claims that SPRM generalizes across domains, but this claim is only supported by results on LiveCodeBench. Given that mathematics and coding reasoning tasks share very similar structures, this claim of generalization currently lacks sufficient evidence. This claim could be substantially strengthened by either including evaluation on more diverse tasks or adding a discussion on what types of domains the current approach is expected to generalize well to and where its limitations might lie. For example, the segmentation choice of using ‘\\n’ might not be as suitable for tasks involving natural language?\n4. The explanation for why MCTS underperforms relative to Best-of-N (BoN) is speculative and unsupported by evidence."}, "questions": {"value": "1. The final trajectory score is defined as the geometric mean of the single-step scores (Eq. 5). It might be helpful to add a sentence on the motivation/justification for this choice. I assume the geometric mean is chosen to push all step scores to be decent, since a single low score can substantially reduce the overall score. It might be interesting to add what scenarios this choice of final score does not yet capture. For example, right now, the final score treats each single step score equally and independently. In the case where the reasoning trajectory later corrects for its earlier mistakes, the earlier bad step might undesirably penalize the entire trajectory. There is an interesting question of how to incorporate the trajectory structure into the score. Have you tried any alternative form? \n2. Minor type: Eq. 6 (line 245), score_i and y_i should be score_n, y_n."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l0Pk8H5gBW", "forum": "tF56uyxdDy", "replyto": "tF56uyxdDy", "signatures": ["ICLR.cc/2026/Conference/Submission5582/Reviewer_jVpd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5582/Reviewer_jVpd"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission5582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762402526380, "cdate": 1762402526380, "tmdate": 1762918146411, "mdate": 1762918146411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a Reflective Generative Model that unifies a policy model and a process reward model through a shared backbone. It proposes a filtering mechanism for learning a process reward module (SPRM) based on outcome rewards, aiming to eliminate the need for process-level annotation"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Strong empirical validation: The experiments are extensive and include multiple benchmarks and LLMs."}, "weaknesses": {"value": "- SPRM is a filtering mechanism, not self-supervised: The model filters step-level data via a binary weight that retains only steps consistent with the outcome.\n- Limited novelty: The shared backbone between policy and reward model is an engineering optimization rather than a conceptual advance in test-time scaling. Moreover, the paper does not study whether shared parameters introduce bias.\n- Terminology and clarity issues: The formulation of LLMs lacks rigor  (e.g., “basic LLM”)"}, "questions": {"value": "- What is the reasoning behind using the same backbone for the reward model and policy? Could this introduce bias or reward hacking effects?\n- What is the multi-agent data cleaning framework used to obtain high-quality samples for the dataset?\n- Why is SPRM described as a self-supervised method, given that it relies on filtered outcome correctness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hNl6zAZG4H", "forum": "tF56uyxdDy", "replyto": "tF56uyxdDy", "signatures": ["ICLR.cc/2026/Conference/Submission5582/Reviewer_WqcA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5582/Reviewer_WqcA"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission5582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762446229730, "cdate": 1762446229730, "tmdate": 1762918145924, "mdate": 1762918145924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}