{"id": "NFJK96X82a", "number": 5078, "cdate": 1757844914473, "mdate": 1763553573931, "content": {"title": "Rethinking Reward Models for Multi-Domain Test-Time Scaling", "abstract": "The reliability of large language models (LLMs) during test-time scaling is often assessed with *external verifiers* or *reward models* that distinguish correct reasoning from flawed logic. Prior work generally assumes that process reward models (PRMs), which score every intermediate reasoning step, outperform outcome reward models (ORMs) that assess only the final answer. This view is based mainly on evidence from narrow, math-adjacent domains. We present the first unified evaluation of four reward model variants, discriminative ORM and PRM (dORM, dPRM) and generative ORM and PRM (gORM, gPRM), across 14 diverse domains. Contrary to conventional wisdom, we find that (i) dORM performs on par with dPRM, (ii) gPRM is not competitive, and (iii) overall, gORM is the most robust, yielding significant and consistent gains across every tested domain. We attribute this to PRM-style stepwise scoring, which inherits label noise from LLM auto-labeling and has difficulty evaluating long reasoning trajectories, including those involving self-correcting reasoning. Our theoretical analysis shows that step-wise aggregation compounds errors as reasoning length grows, and our empirical observations confirm this effect. These findings challenge the prevailing assumption that fine-grained supervision is always better and support generative outcome verification for multi-domain deployment. We publicly release our code, datasets, and checkpoints at this [anonymous repository](https://anonymous.4open.science/r/iclr2026-5078-7744) to facilitate future research in multi-domain settings.", "tldr": "Generative outcome reward models outperform process reward models, challenging the belief that fine-grained supervision is better for test-time scaling.", "keywords": ["reward model", "multi-domain", "test-time scaling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5c603af7a0cef58eced4d515867b29bba725efe0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper compares four types of external verifiers: dPRM, dORM, gPRM, and gORM. (d for discriminative, and g for generative)\nFindings are divided by domain and cot-type (short or long) and summarized on the last page of the paper. \n\nIn general, the paper suggests ORM-type verifiers to show the best (most robust performance), and to prefer gORM over dORM when compute allows. They hypothesize this is likely because PRMs are likely to accumulate errors or are not a suitable match with the current long-reasoning paradigm, where models can self-recover from errors. \n\nI have mixed feelings about this paper. While, after reading it, I think some of the results are obvious, I understand it's an important part of science for someone to conduct a well-controlled experiment to document and prove such obviousness."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "While the paper is not of the type that proposes a new method or dataset, it aims to broaden our understanding of external verifiers. And might be a good reference material for those in the industry who have to choose which verifier to use in certain scenarios.\n\nThe presentation is nice and I would say fancy."}, "weaknesses": {"value": "(1) I think we can summarize scenarios where we need external verifiers into two cases. (a) at test-time: where we want to boost the performance of a fixed language model. The paper may be helpful to practitioners who need to choose a generator-verifier combination. One thing that worries me is that the paper highly relies on empirical findings, and I'm not sure whether the same rules will apply to new datasets, domains, training datasets, etc. (b) at train-time: where we use external verifiers to guide training. For instance, see Figure 4 of [1], which provides an experiment on whether their analysis on verifiers can be applied to PPO-style training. The paper lacks analysis in this direction. In a nutshell, while this may appear a bit off-topic for the authors, I think the findings are heavily reliant on empirical observations on the test-time side and lack exploration in the train-time side.\n\n\n(2) The paper emphasizes that the limitations of past works are being narrowed down to math only. However, I think this paper still lacks exploration in non-math domains. The dataset used to train the verifiers for MMLU-Pro is automatically generated via Llama-3.1-8/70B. Evaluation is done only on MMLU-Pro, a very standardized MCQA benchmark. Especially with the diverse evaluation datasets flooding recently, it would have been better for the authors to collect a diverse set of benchmarks (even if they had to sample it).\n\n(3) An increasing usage of external verifiers is in training where we the community now aims for harder and harder datasets. It would have been interesting to include the results in harder datasets."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G4BznATUIr", "forum": "NFJK96X82a", "replyto": "NFJK96X82a", "signatures": ["ICLR.cc/2026/Conference/Submission5078/Reviewer_WbK9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5078/Reviewer_WbK9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799845260, "cdate": 1761799845260, "tmdate": 1762917859170, "mdate": 1762917859170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the belief that process reward models (PRMs) are superior to outcome reward models (ORMs). While PRMs excel in narrow math domains, this study conducts the first unified evaluation of four reward model variants (dORM, dPRM, gORM, gPRM) across diverse domains. The key finding is that Generative ORM (gORM) is the most robust and delivers significant and consistent gains across every tested domain. In this multi-domain setting, dORM performs on par with dPRM, and gPRM is not competitive. The authors demonstrate that PRMs fail because their stepwise error aggregation compounds errors as reasoning length grows. PRMs also struggle to evaluate long reasoning chains that involve self-correction (\"aha\" moments), a common feature in complex, non-math domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper directly challenges the widely held belief that Process Reward Models (PRMs) are superior to Outcome Reward Models (ORMs). It correctly identifies that this consensus was formed from studies on narrow, math-adjacent domains. By conducting the first unified evaluation of four model variants (dORM, dPRM, gORM, gPRM) across diverse domains, it provides a more generalizable finding: gORM (Generative ORM) is the most robust and yields significant and consistent gains across every tested domain.\n\n1. The authors don't just present what happens, but why. It offers a theoretical analysis showing that the log-error lower bound for PRMs (dPRM and gPRM) grows linearly with the length of the reasoning chain ($T$). In contrast, the error bound for ORMs is independent of $T$. This provides a formal explanation for why PRMs fail on the longer, more complex reasoning chains found in multi-domain settings.\n\n1. This paper diagnoses specific, practical failure modes of PRMs that dPRM is highly sensitive to label noise, which is prevalent in multi-domain datasets auto-labeled by LLMs. It also finds that gPRM suffers from a severe shift in its CoT-length distribution because its strict consensus filtering removes too many long CoTs, fatally mismatching its training data to the test set."}, "weaknesses": {"value": "1. The study's evaluations are confined to tasks with verifiable outcomes (i.e., math and multiple-choice questions). The authors also explicitly state that these findings may not generalize to open-ended generation tasks like dialogue or summarization.\n\n1. The authors do not perform exhaustive tuning, as mentioned in the appendix. This may have led to an unfair comparison. The poor performance of PRM-style models (dPRM, gPRM) on this new, noisy, multi-domain task might be because their inherited hyperparameters were not optimal, while the hyperparameters inherited by the ORM-style models just happened to be more robust.\n\n1. The paper strongly demonstrates that gPRM fails because its strict consensus filtering mechanism leads to a severe lack of long CoT samples in its training data, thereby creating a length distribution shift. This weakness might be caused by gPRM's data collection pipeline, not by the concept of gPRM itself. A gPRM that adopted a different data collection strategy (e.g., less strict filtering or sampling specifically for long CoTs) might still be very competitive."}, "questions": {"value": "1. The dPRM's failure is attributed to its sensitivity to label noise from the single Llama-3.1-70B annotator. Is this a fundamental flaw of dPRM, or an artifact of this specific annotator's noise profile? If regenerating the process labels with a more advanced model, can the dPRM's performance be recovered with higher-quality process labels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WpmRLnYHgc", "forum": "NFJK96X82a", "replyto": "NFJK96X82a", "signatures": ["ICLR.cc/2026/Conference/Submission5078/Reviewer_YvuS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5078/Reviewer_YvuS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918602729, "cdate": 1761918602729, "tmdate": 1762917858738, "mdate": 1762917858738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates several variants of RMs on math and general domains with controlled experiments. By unifying the setups of several previous works, the authors found out that math and multi domain settings might lead to different trends among those RM variants. Then intuitive understandings of these differences were proposed, namely error accumulation along CoT trajectories, label noise, and length distribution shift. Results suggest that PRM might not be always better than ORMs especially in multidomain setup, and provide practical guidelines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper presents controlled experiments on RM variants to address the debate around them on math and multi domain setup.\n* The paper is well written, with clear narratives and good figures. The main message is clearly conveyed with experiments, followed by in-depth analysis."}, "weaknesses": {"value": "* For math, process reward labels come from PRM 800k, which is typically believed to have good quality. For MMLU-pro, they are fully synthetic, from Llama 3 70b. Potentially the degradation of label quality on MMLU-pro can lead to the result that PRM underperforms ORM. In contrast, the ORM labels of both PRM 800k and MMLU pro should be good. The difference in label quality itself might undermine the validity of the conclusions   \n* Although MMLU pro is different from math, they are still both tasks with verifiable rewards. It is still unclear how ORM and PRM compare in a more open ended setup.\n* Weak theoretical results. It is well known that PRM error accumulates as the CoT length grows. In this sense, the results are of almost no significance. Besides, this point alone does not establish the advantage of ORM over PRM."}, "questions": {"value": "* The paper shows gPRM is robust to label noise in the math domain but fails in the multi-domain setting due to a distribution shift. Is this shift purely an artifact of consensus filtering on noisy labels? In other words, would gPRM still fail in a multi-domain setting even if it were trained on perfectly clean, human-annotated process labels (like PRM 800k)?\n* The paper states CoTs account for 15.3% of ProcessBench. What is the prevalence of \"aha\" CoTs in MMLU-Pro? Is it significantly higher, and could this (in addition to CoT length) be a primary factor of the PRM performance collapse?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HX98XmotSk", "forum": "NFJK96X82a", "replyto": "NFJK96X82a", "signatures": ["ICLR.cc/2026/Conference/Submission5078/Reviewer_ueA4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5078/Reviewer_ueA4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015707233, "cdate": 1762015707233, "tmdate": 1762917858514, "mdate": 1762917858514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a unified evaluation of four reward model variants for test-time scaling—discriminative and generative outcome reward models (dORM, gORM) and process reward models (dPRM, gPRM)—across 14 domains. While prior work favored PRMs (step-level supervision) over ORMs (final-answer supervision), the authors find that this assumption breaks down in multi-domain reasoning. Specifically, gORM consistently outperforms all others, dORM ≈ dPRM, and gPRM underperforms despite prior success in math. The paper attributes these results to compounded stepwise errors and label noise in long or noisy CoTs. Theoretical analysis establishes that PRM log-error grows linearly with reasoning length, while ORM error remains bounded. Empirical results (e.g., Figs. 5–8, 12, 15) confirm this, and the authors provide practical guidelines for model selection."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is very well written with lots of careful analysis. I appreciate all the figures and they clearly conveys the results. \n- The paper compares four reward model variants across 14 diverse domains (law, biology, philosophy, etc.), using multiple backbones and datasets, and the study the quite diverse and comprehensive."}, "weaknesses": {"value": "- Limited task diversity in form: All benchmarks are multiple-choice or verifiable; generalization to open-ended reasoning or generation remains untested."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OvvGwzw6Lp", "forum": "NFJK96X82a", "replyto": "NFJK96X82a", "signatures": ["ICLR.cc/2026/Conference/Submission5078/Reviewer_TwoX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5078/Reviewer_TwoX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142549030, "cdate": 1762142549030, "tmdate": 1762917858302, "mdate": 1762917858302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank all reviewers for their time and constructive feedback, which has significantly improved our paper.\n\nWe are especially grateful that the reviewers recognize several strengths of our work, including the **first unified evaluation of four RM variants across diverse domains** (TwoX, ueA4, YvuS), our **clear presentation** (TwoX, ueA4, YvuS, WbK9), the **practical value for selecting external verifiers** (TwoX, WbK9), and our **theoretical and empirical analysis of PRM failure modes** (TwoX, ueA4, YvuS).\n\nWe believe that we have successfully addressed all major concerns in this rebuttal. Below, we summarize **the changes** that have been included in the revised version (shown in blue):\n\n---\n\n- **`L446–449`** in **`§4.2`**: in response to reviewer **ueA4**, we expanded our discussion on the **limitations of the training data generator** (i.e., $p\\_\\mathtt{LLM-j}$, LLM-as-a-judge, or QwQ-32B [1] in our case) and how such **imperfections contribute to distribution shift**.\n\n- **`Table 3`** in **`Appendix G`**: in response to reviewers **ueA4** and **YvuS**, we added the Wasserstein distance of $\\texttt{gPRM}$ in the multi-domain setting with (i) **label refinement** using Gemini-2.0 Flash [2] and (ii) **relaxation** of the *consensus filtering*.\n\n- **`Table 5`** in **`Appendix G`**: in response to reviewers **ueA4** and **YvuS**, we included the **surviving proportion** (%) after *consensus filtering* of verification CoTs for $\\texttt{gORM}$, $\\texttt{gPRM}$, $\\texttt{gPRM}$ with label refinement, and $\\texttt{gPRM}$ with relaxed filtering.\n\n- **`Table 6`** in **`Appendix G`**: in response to reviewers **ueA4** and **YvuS**, we added best-of-$N$ results on MMLU-Pro using Llama-3.1-8B-Instruct with **label refinement**.\n\n- **`Table 7`** in **`Appendix G`**: in response to reviewer **YvuS**, we added best-of-$N$ results on MMLU-Pro using Llama-3.1-8B-Instruct with **relaxed filtering**.\n\n- **`Table 8`** in **`Appendix G`**: in response to reviewer **YvuS**, we added best-of-$N$ results on MMLU-Pro using Llama-3.1-8B-Instruct while varying the **learning rate** and **LoRA rank $r$** for $\\texttt{dPRM}$ and $\\texttt{gPRM}$.\n\n- **`Figs. 41 and 42`** in **`Appendix G`**: In response to reviewer **WbK9**, we added results on a **more challenging multiple-choice QA dataset**, GPQA-diamond [3] (biology, chemistry, and physics), using five different LLMs with $N=16$.\n\n- **`Final version`**: in response to **all reviewers**, we will include a **detailed discussion of the limitations on open-ended generation** and **our hypothesis regarding generalization** of our study to such settings.\n\n- **`Final version`**: in response to reviewer **ueA4**, we will further clarify the connection between the observations in `§4.1` and the practical guidelines in `§5`.\n\n\n---\n\n### Reference\n\n[1] Team, Qwen. \"Qwq-32b: Embracing the power of reinforcement learning.\" Mar. 2025,\n\n[2] Comanici, Gheorghe, et al. \"Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities.\" arXiv 2025.\n\n[3] Rein, David, et al. \"Gpqa: A graduate-level google-proof q&a benchmark.\" First Conference on Language Modeling. 2024\n\n---"}}, "id": "oYpgY2O1Tl", "forum": "NFJK96X82a", "replyto": "NFJK96X82a", "signatures": ["ICLR.cc/2026/Conference/Submission5078/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5078/Authors"], "number": 18, "invitations": ["ICLR.cc/2026/Conference/Submission5078/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763532086742, "cdate": 1763532086742, "tmdate": 1763532086742, "mdate": 1763532086742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}