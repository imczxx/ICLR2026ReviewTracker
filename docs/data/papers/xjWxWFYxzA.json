{"id": "xjWxWFYxzA", "number": 15984, "cdate": 1758258060085, "mdate": 1759897268958, "content": {"title": "BLOB-Q: Boosting Low Bit ViT Quantization via Global Optimization on Model Distortion", "abstract": "In this paper, we present a novel Mixed-Precision Post Training Quantization (PTQ) approach for Vision Transformers (ViTs). Our approach aims to minimize the output distortion caused by quantization, and thus can maximally maintain the accuracy of ViT models even quantized to low bit widths. Different with prior works which typically optimize the output error of current layer (layer distortion), when performing quantization, our approach directly minimizes the output error of the last layer of the model (model distortion). As model distortion is highly related to accuracy, our approach can maximally maintain the accuracy even when quantized to low bit widths. We formulate the quantization of ViTs as a model distortion optimization problem, given the constraint of size. By solving the optimization problem, the optimal bit allocation across layers, i.e., the optimal bit width of each layer, can be obtained, with minimized model distortion. Directly solving the optimization problem is an NP-hard problem. We propose to adopt the second-order term of the Taylor series expansion to approximate model distortion, where an important additivity property can be derived under the approximation. Utilizing the second-order additivity property, the optimization problem can be decomposed into sub-problems and solved efficiently in an iterative manner. Specifically, we propose a dynamic programming algorithm to solve the optimization problem and efficiently find the globally optimal solution with only linear time complexity. Extensive experiments on six ViT models demonstrate the effectiveness of our approach. Results show that our approach significantly improves state-of-the-art and can further reduce the size of ViT models to 4 bits to 6 bits without hurting accuracy.", "tldr": "", "keywords": ["Efficient Inference", "Vision Transformer Quantization", "Model Compression"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/665fd118016b3a565dc0d8a5e51017b3837d11d3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a Mixed-Precision Post-Training Quantization (PTQ) framework for Vision Transformers (ViTs). Unlike previous approaches that minimize layer-wise reconstruction error, the method directly minimizes the final-layer output error (i.e., model-level distortion). The authors formulate the problem as a global optimization task and introduce a dynamic programming algorithm that finds the globally optimal bit allocation in linear time. Experimental results demonstrate consistent improvements over baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The formulation based on second-order approximation and the additivity property provides a sound theoretical foundation for global distortion minimization.\nThe proposed dynamic programming solver is computationally efficient, achieving linear scaling and outperforming prior heuristic search methods.\nTheoretical analysis is thorough and well-supported by derivations in both the main text and the supplementary material.\nThe overall framework is efficient and well-motivated."}, "weaknesses": {"value": "The comparative analysis is limited. Most reported baselines use uniform-bit quantization, making the improvement less surprising. The few mixed-precision baselines included are outdated and insufficient to establish competitiveness against recent methods.\n\nThe fine-grained bit allocation across both weights and activations (as shown in Figures 7–8) may introduce nontrivial implementation overhead and complicate hardware deployment compared to uniform-bit schemes.\n\nThe assumption of inter-layer independence—and particularly the independence between activation and weight errors—is questionable. Figure 3 implies negligible inter-layer dependency, which seems unrealistic, especially within transformer blocks where correlations between layers of same block are expected. The figure also omits quantitative measures of dependency magnitude. It remains unclear whether the proposed method would remain effective without this assumption."}, "questions": {"value": "Please see Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BvHTX7NX82", "forum": "xjWxWFYxzA", "replyto": "xjWxWFYxzA", "signatures": ["ICLR.cc/2026/Conference/Submission15984/Reviewer_HeS4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15984/Reviewer_HeS4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761491286838, "cdate": 1761491286838, "tmdate": 1762926193503, "mdate": 1762926193503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes output-based model distortion as a metric of layerwise quantization impact on the output and additivity where the output distortion is a sum of layerwise quantization impacts. \nBased on this, the paper presents a dynamic programming based solution to determine per-layer precision while meeting the given bit constraint."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Compared with most of quantization methods which focus on layerwise distortion minimization or training loss minimization, the presented idea of output distortion minimization looks effective according to experimental results.\nAdditivity property, which I think is not new but very well studied in detail in the paper, is exploited in dynamic programming to explore the entire space of bit allocation.\nAblation studies (in the main paper and appendices) are quite extensive."}, "weaknesses": {"value": "Appendix D.6 reports a preliminary experiment on language model.\nIt would be nice if more detailed and quantitative analysis were provided."}, "questions": {"value": "The proposed method looks quite effective on ViTs.\nHow would it be applied to language models?\nIn other words, how can we address the basic assumptions of the proposed method, zero mean, inter-layer independence, linearity (to ignore higher order terms), ...?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Qu2gjlj6jr", "forum": "xjWxWFYxzA", "replyto": "xjWxWFYxzA", "signatures": ["ICLR.cc/2026/Conference/Submission15984/Reviewer_JeSs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15984/Reviewer_JeSs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761699343280, "cdate": 1761699343280, "tmdate": 1762926192838, "mdate": 1762926192838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a Mixed-Precision Post-Training Quantization (PTQ) framework for Vision Transformers that minimizes final-layer output error rather than layer-wise reconstruction error. The authors formulate bit allocation as a global optimization problem and develop a dynamic programming algorithm that finds the optimal solution in linear time."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written\n- Theoretical justification.\n- The overall framework is efficient and well-motivated."}, "weaknesses": {"value": "- Figure 3 in the appendix only indicates that the dependency of quantization error across layers is smaller than the squared quantization error of each layer. However, it is not clear whether the product of quantization errors between any two layers is significantly smaller or approximates zero. The fact that layers are quantized independently does not intuitively imply that the product of errors equals zero."}, "questions": {"value": "- Tables 14 and 15 demonstrate that the mixed-precision approach is faster and consumes less memory than the uniform-bit approach. How is this possible? What is the reason behind this?\n\n- Is it possible to replace the bit-width allocation of stronger mixed-precision methods (e.g., EMQ/OMPQ/...) to evaluate how much stronger the proposed mixed-precision approach is?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eqaUAJ0v0X", "forum": "xjWxWFYxzA", "replyto": "xjWxWFYxzA", "signatures": ["ICLR.cc/2026/Conference/Submission15984/Reviewer_qybZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15984/Reviewer_qybZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808789773, "cdate": 1761808789773, "tmdate": 1762926192441, "mdate": 1762926192441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Mixed-Precision Post Training Quantization (PTQ) method for Vision Transformers (ViTs) that minimizes model distortion (the final layer's output error) instead of layer-wise distortion to better preserve accuracy. By approximating model distortion using a second-order Taylor expansion and exploiting its additivity, a dynamic programming algorithm efficiently finds the optimal bit allocation, achieving 4–6 bit quantization without accuracy loss across multiple ViT models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing and theoretical derivation of this work are reasonable and easy to follow.\n2. The problem formulation and solution both take computational efficiency into account.\n3. Experiments demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The proposed method is not entirely new, similar ideas have been explored in prior works. For example, model output reconstruction instead of layer-wise reconstruction has already been proposed; the second-order optimization in Section 3.1 and the approximation method in Section 4 have also been widely used. If I have misunderstood, the authors are welcome to clarify.\n2. The compared methods are somewhat outdated, limited to baselines from 2023 or earlier. Moreover, the comparison might be unfair, as most existing methods perform uniform-precision quantization.\n3. Some mathematical derivations could be improved. For instance, in Property 1 (Equation 4), a more rigorous expression should be \nE <= E_W + E_A. This can be easily proven and makes more sense than the current “Second-Order Additivity” assumption. It would not affect the subsequent derivation or conclusions, as it can be regarded as optimizing the upper bound of the error."}, "questions": {"value": "0. Please first address the questions in the “weaknesses.”\n1. It would be helpful to further summarize the technical contributions. At present, most components appear to have been proposed before, so it would be good to highlight what is newly introduced or improved.\n2. When solving the mixed-precision quantization problem, could the authors elaborate on the advantages of using dynamic programming? More commonly used alternatives today maybe are Pareto or linear programming methods.\n3. The concept of Model-level Optimization is unclear. For example, although BLOB-Q formulates a global optimization problem, it also decomposes it into subproblems for solving. Similarly, HAWQ models the Hessian of the task loss with respect to model outputs, so why is that not considered “Model-level Optimization”?\n\nMinor comments:\n1. In Related Works, the subheadings “PTQ” and “MPQ” do not need to be abbreviated again since they have already been defined earlier.\n2. In Section 3, there is only one subsection (3.1). Consider adding an additional subsection for better structure."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qk7OSUppun", "forum": "xjWxWFYxzA", "replyto": "xjWxWFYxzA", "signatures": ["ICLR.cc/2026/Conference/Submission15984/Reviewer_VhYT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15984/Reviewer_VhYT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813104043, "cdate": 1761813104043, "tmdate": 1762926192028, "mdate": 1762926192028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}