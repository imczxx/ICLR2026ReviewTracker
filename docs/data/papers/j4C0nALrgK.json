{"id": "j4C0nALrgK", "number": 8236, "cdate": 1758075637167, "mdate": 1759897797189, "content": {"title": "Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction", "abstract": "Automated theorem proving (ATP)\n---the task of generating a proof that passes automated proof verification given a math question in formal language--- is a critical challenge at the intersection of mathematics and Artificial Intelligence (AI). We introduce Goedel-Prover-V2, a family of two language models that establish a new state-of-the-art (SOTA) in open-source ATP, using the Lean proof assistant. In addition to standard expert iteration and reinforcement learning, our approach incorporates three key innovations: (1) During training when improvement plateaus on human questions, the prover does {\\em scaffolded data synthesis} to generate synthetic questions of increasing difficulty for its own training; (2) The prover is trained to self-correct   using  Lean compiler feedback; (3) Improved test-time exploration through checkpoint averaging to balance accuracy and diversity. \n\nOur small model, Goedel-Prover-V2-8B, reaches 84.6\\% pass@32 on MiniF2F and outperforms DeepSeek-Prover-V2-671B despite being $80\\times$ smaller. Our flagship model, Goedel-Prover-V2-32B, achieves 88.1\\% on MiniF2F at pass@32 in standard mode and 90.4\\% in self-correction mode, outperforming prior SOTA by a large margin. Additionally, our flagship model solves 86 problems on PutnamBench at pass@184, securing first place among open-source models and surpassing DeepSeek-Prover-V2-671B's record of 47 problems by pass@1024 with about $20\\times$ smaller model size and significantly lower compute budget. To support community research, we have open-sourced the prover models and a SOTA statement formalizer, with all training datasets to be released in the near future.", "tldr": "", "keywords": ["Theorem Proving", "Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5c10423dc1d4c53d37ce52c946bfac1636d1772.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present Goedel-Prover-V2, a family of two models finetuned for automated theorem proving with Lean. The models were trained through a series of expert iteration and RL, starting with an initial seed generated by DeepSeek-Prover-V2. Additionally, the models were specifically finetuned to be able to incorporate Lean feedback and underwent a weight averaging scheme to increase diversity in the generations of the models. The results indicate that the models outperform prior open-weight alternatives on popular benchmarks like PutnamBench and Mini-F2F."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper present two new models that outperform prior state-of-the-art in an important field of mathematical applications for LLMs (namely, automated proof generation).\n- The training scheme (weight averaging, expert iteration with RL for Lean feedback, ...) is interesting and novel.\n- Goedel-Formalizer-V2 also outperforms prior formalizers.\n- Comparison with prior work is solid and appropriately uses the two most popular benchmarks in this field for the comparison."}, "weaknesses": {"value": "I am currently giving a 6, since I believe that while the paper presents a strong contribution, there are significant areas of improvement, especially regarding the paper. Most of these weaknesses can be improved during the rebuttal phase, and I therefore see myself increasing my score if they are appropriately addressed. However, for some weaknesses it could also be that their answer will indicate further problems with the paper (especially regarding contamination and the auto-formalizer). If this is the case, I will decrease my score.\n\nI have split the weaknesses in \"weaknesses\" and \"remarks\", where \"remarks\" are smaller points that only affected my rating in a minor way.\n\n**Weaknesses**\n- It is unclear why MathOlympiadBench is included. The results on this benchmark are only given in Figure 1 for three models and are never even discussed in the experimental section. I do agree that the three examples in the Appendix indicate that MathOlympiadBench is better formalized, but it is unclear what proportion of MiniF2F this constitutes and how difficult it was to find such examples. Furthermore, MathOlympiadBench has the distinct disadvantage that its theorems might have been included in training data of other models, since decontamination procedures for these models might not have included the non-overlapping problems with MiniF2F. Overall, I think the inclusion of MathOlympiadBench in its current state decreases the contribution of the paper, as it takes away the focus from Goedel-Prover-V2 and is missing a lot of details regarding experimentation, argumentation why it is necessary to have such a third dataset, and missing explanations regarding its construction (Appendix A is very unclear, as I have no idea how the issues occured, what kind of human evaluation was done, etc.). The authors should either remove the benchmark or make it a complete part of their paper by adding these details.\n- The comparison between feedback and no feedback version of the models is misleading. pass@n with 2 rounds of feedback allow the model to have $3n$ attempts at the problem, not $n$. As such, comparisons should be made between pass@n of the feedback setting with pass@3n of the non-feedback setting. I do note that this is a back-of-the-envelope calculation, as feedback rounds might spend less compute tokens (but it does have more input tokens), but at least the FLOPs between these settings should be matched for proper comparisons. If I look at the numbers, the feedback settings helps much less when one does this more proper comparison (but one cannot entirely tell from the numbers included in the paper alone).\n- It is not mentioned how the results in Table 1 were obtained. If, as I suspect, correctness was verified by the same LLM judge, how can we ensure that the Goedel-Formalizer did not simply learn to fool this LLM judge better? It was specifically finetuned to do better according to this LLM judge, automatically leading to more true positives and false positives. If these results were indeed obtained using the same LLM judge, it would be required to perform another (mabye smaller-scale) evaluation method to compare the formalizers.\n- It is never mentioned where the training data problems were sourced from. Furthermore, it is never mentioned how decontamination with respect to the test benchmark was performed.\n- Currently, no code is included. It would be appreciated if code could be included to verify some of the statements made etc.\n\n**Remarks**\n- I disagree that including CoT in Figure 7 has any influence on performance. The two lines are clearly within 1 standard deviation of each other and the difference is overall very minimal. Furthermore, including the CoT also leads to more compute that needs to be spent per task, indicating that it is likely the opposite case: It is probably better to not include the CoT because it is basically the same performance for less compute.\n- In Figure 8, it is unclear why we are looking at checkpoints rather than the final model. Also, the differences between the different $\\alpha$'s are tiny, and much less clear-cut than what is discussed in the text.\n- I find Section 2 confusing in places. Specifically, the data generation pipeline is very unclear. It should be more clearly mentioned how:\n  - How the data generation pipeline with scaffolding is used in each stage (is it used all the time, only in some stages, when is it used, how is the data generated to check whether it can be solved, ...). \n  - Where the problems were originally sourced from.\n  - Specifically, 2.4 only ever mentions what happens when you have the formalized statements ready, but it is not clear how these formalized statements are obtained in each stage due to the missing details above.\n- The motivation in the first paragraph of the introduction feels a bit off (especially L45-47): Kimina-Prover has the same number of parameters as Goedel-Prover-V2 so the argument does not really hold there. The motivation is much simpler than what presented there (\"we want to create even better models\"). It would be better to incorporate this argument instead.\n- Kimina-Prover should be included in Fig 1 center and right as well.\n- In L98: \"ultimately bridge the long-standing divide between intuitive human reasoning and formal proof verification.\" -> This feels a bit too grand. To do such a thing, one would at the very least need an informalizer as well, or somehow incorporate informal proofs in the formal verification process. I would remove this sentence.\n- L134-136 either needs a citation or an explanation how this evaluation was performed.\n- It is never mentioned which LLM is used to verify formalizations, not even in Appendix C.\n- The caption of Table 1 should be above the table.\n- There is an asterix in the caption of Table 2, but it is never explained why it is there.\n- It is never mentioned which $\\alpha$ was chosen for the final model.\n- The formatting consistency needs to be improved. In one section, the paper uses bullet points without bolded headers, in another it uses bullet points with bolded headers, or just no bullet points and no bolded headers, or just bolded headers. Some of these bolded headers are capitalized, others are not, ... Plots do not have a consistent format. The discrepancy is especially prevalent between Figure 6 and 7 (different background, label size, title vs no title, ...).\n- The writing in the appendix D needs a pass, it contains several typos and grammatical mistakes."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NmGfznOLQo", "forum": "j4C0nALrgK", "replyto": "j4C0nALrgK", "signatures": ["ICLR.cc/2026/Conference/Submission8236/Reviewer_nJcq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8236/Reviewer_nJcq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760526024804, "cdate": 1760526024804, "tmdate": 1762920181836, "mdate": 1762920181836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Goedel-Prover-v2, an open-source theorem-proving LLM , using the Lean proof assistant. Goedel-Prover-v2 incorporates RL with three key innovations: verifier-guided self-correction, scaffolded data synthesis, and model (checkpoint) averaging to recover output diversity late in training. Empirically, Goedel-Prover-v2-32B achieves a relatively high performance in existing benchmarks, such as MiniF2F and Putnambench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper introduces a novel method for generating synthetic data, potentially one of the strongest reasons. The model filters out bad autoformalisations, and integrates compiler-feedback self-repair directly into whole-proof generation. The model uses `extract_goal` to capture unsolved states from a proof attempt too. The model introduces an interesting method for improving test-time exploration through checkpoint averaging, potentially balancing accuracy and diversity. Last but not least, the model achieves strong empirical results, requiring fewer pass@n compared to other existing models to achieve greater performance."}, "weaknesses": {"value": "1. The model primarily focuses on MiniF2F and PutnamBench. While these are widely used, including more datasets in the comparison would be better, including ProofNet-test, AIME, AMC, IMO, and DeepSeek-ProverBench.\n\n2. The model averaging improves pass@n but introduces another hyperparameter without a principled selection rule. A more detailed explanation of this would be great.\n\n3. The data-synthesis filter uses LLM autoformalisation and judging; is there any manual annotation on the correctness of the statement?\n\n4. While the model relies on training reasoning instances, it would be great to provide the average length per dataset compared to the previous goedel-prover. Does including reasoning, while extending the length, improve the performance?"}, "questions": {"value": "1. One of the strengths within Goedel-Prover-v2 would be the quality of the dataset. Will the dataset be open sourced upon acceptance?\n\n2. Following on from Weakness 3, is there any manual annotation on the correctness of the filtering and judging?\n\n3. Are there any potential methods to reduce the bias towards components, as we observe that the model is generally weak at geometry, while stronger at some other components?\n\n4. Could integrating lemma generation/retrieval further boost performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0Lfc9hiMRD", "forum": "j4C0nALrgK", "replyto": "j4C0nALrgK", "signatures": ["ICLR.cc/2026/Conference/Submission8236/Reviewer_5oHs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8236/Reviewer_5oHs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760964913438, "cdate": 1760964913438, "tmdate": 1762920181165, "mdate": 1762920181165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper makes several contributions: \n\n- Releasing a LLM for ATP with impressive @32 accuracy on miniF2F and Putnam Bench.\n\n- Releasing another LLM for autoformalization though it remains unclear how well this model performs.\n\n- A training method that involves RL and model averaging.\n\n- Fine tuning the ATP model so that it can do self-correction on flawed proofs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper has the potential to be a high impact paper, in my view, if it releases the code for its training method, and releases its training set. If these two items are not released, my recommendation for the paper will be rejection.\n\nThe training algorithm, especially the model averaging method, is a useful contribution, assuming that code will be released with the paper.\n\nThe release of the ATP model is a useful and significant contribution for the community. However, the mere release of the models without releasing the code and training data does not make a direct contribution towards the CFP of ICLR in my view."}, "weaknesses": {"value": "Accuracy of the autoformalizer model is only evaluated on 300 Omni math problems. First, it is not clear what these problems are. Second, it is not clear how the 300 problems relate to the training set of this model. Paper should report the accuracy of its model on known benchmarks such as miniF2F. If the model is trained on the miniF2F, paper should report that to the reader. Evaluating a new model on a new dataset does not provide sufficient information.\n\nHow is the accuracy of autoformalizer LLM evaluated? Is the evaluation performed by humans familiar with Lean? Or is the evaluation performed automatically using other LLMs?\n\nAlthough Figure 8 provides some insights, it remains unclear to me how much of the gained accuracy is due to model averaging. For example, if there was no model averaging at all, what would the accuracy of the trained model be pass 32? Would it be close to the accuracy of other similar models such as DSPV2 8B model? A clear set of ablation studies may clarify this.\n\nThe effect of self correction module is not reported for other LLMs. It may be the case that the self correction method is only effective if the LLM is trained to take the Lean compiler feedback into account. I think the paper needs to clarify whether this is correct or not. If it has performed any systemic experiments, such results should be reported. Moreover, the self-correction method can be compared against the methods that repair the proofs and do not require fine-tuning the LLM.\n\nToken budgets should also be reported especially for the experiments that involve self-correction. It remains unclear to me how much computational cost is needed for self corrections.\n\nThe paper lacks any analysis of the training set. Given that the training set is largely extracted out of the rival model, DSP V2, one would expect a clear analysis of the training set. Specifically, a direct comparison against the testing set would be helpful. The abstract and introduction make a big point about the accuracy of GPV2 being higher than the accuracy of DSPV2. In my view, it would have made sense to acknowledge that this paper has used DSP V2 as the base to generate part of its training set, i.e., this paper can mention that it has built on the work of DSPV2.\n\nPaper does not contain supplementary material such as code.\n\nPlenty of abstract is repeated in the first two paragraphs of the introduction."}, "questions": {"value": "Please see comments above. I'd be happy to revise my score based on the authors' response.\n\nWhy is the release of training set postponed to the near future?\n\nWhat were the deciding factors behind choosing Lean 4.9? Was this choice influenced by the Lean version used by DSP V2? \n\nHave authors evaluated the accuracy of their model using higher versions of Lean? I would guess that if the model is fine tuned on a more recent version of Lean, its accuracy will increase."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Kmm0krVBEE", "forum": "j4C0nALrgK", "replyto": "j4C0nALrgK", "signatures": ["ICLR.cc/2026/Conference/Submission8236/Reviewer_BEpZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8236/Reviewer_BEpZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866653662, "cdate": 1761866653662, "tmdate": 1762920180794, "mdate": 1762920180794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Goedel-Prover-V2, a family of open-source LLMs for Lean4 theorem proving. The framework integrates three key components: (1) large-scale data generation method that enables large and high-quality data generation; (2) verifier-guided self-correction for proofs; (3) model averaging technique that improves the output diversity. The authors report impressive performance of the model, with the 32B version achieving 88.1% pass@32 on MiniF2F-Test and solving 86 problems (13.03%) on PutnamBench under pass@184. The authors commit to releasing all models, training recipes, and datasets to support future research and reproducibility."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Goedel-Prover-V2 achieves impressive performance, with the 32B model achieving 88.1% under pass@32 on MiniF2F-Test and solving 86 problems on PutnamBench at pass@184. The smaller 8B model also outperforms the larger previous model on MiniF2F.\n2. The experiment section is thorough and complete, covering not only core benchmark evaluations but also scaling laws, ablation studies on verifier-guided self-correction, and analysis of model averaging and reinforcement learning dynamics. This allows the readers to more thoroughly understand the effectiveness of each component of the model.\n3. The authors’ plan to release all trained models, the codebase, and the training datasets will be a valuable contribution to the research community."}, "weaknesses": {"value": "Despite being a thorough and complete work, I believe the paper could benefit from improvements in the following areas:\n\n1. The evaluated datasets primarily focus on domains where the underlying knowledge is relatively elementary but requires the application of many tricky techniques, for example, the PutnamBench. As a result, the model may achieve strong performance by simply memorizing those techniques from larger models, rather than demonstrating true improvements in cognitive reasoning. It would be valuable to evaluate the model on more complex datasets such as ProofNet or FormalIMO to better assess its generalization and reasoning capabilities.\n2. The paper does not disclose the cost of the training and data generation processes in detail, which limits the reproducibility and transparency of the work."}, "questions": {"value": "This work could be more solid and complete if the authors could address the following questions:\n\n1. Generalization to advanced domains: What is the model’s performance on datasets that require more advanced mathematical knowledge, such as FormalIMO and ProofNet? \n2. This paper could also benefit in the sense of reproducibility from a more detailed description of the training process. Specifically, what are the dataset sizes used for S1, S2, and S3 as shown in Figure 3? What are the GPU hours and API costs associated with dataset generation? Furthermore, what is the total GPU hour cost for the SFT and RL stages, respectively, and what hyperparameters were used for model averaging (e.g., the choice of α values)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SeUBRyl3OC", "forum": "j4C0nALrgK", "replyto": "j4C0nALrgK", "signatures": ["ICLR.cc/2026/Conference/Submission8236/Reviewer_sCYS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8236/Reviewer_sCYS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973236206, "cdate": 1761973236206, "tmdate": 1762920180324, "mdate": 1762920180324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}