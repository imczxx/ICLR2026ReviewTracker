{"id": "Xn6EnJZghu", "number": 15690, "cdate": 1758253970258, "mdate": 1759897288750, "content": {"title": "Randomized Antipodal Search Done Right for Data Pareto Improvement of LLM Unlearning", "abstract": "Large language models (LLMs) sometimes memorize undesirable knowledge, which must be removed after deployment. Prior work on machine unlearning has focused largely on optimization methods that adjust parameters to enforce forgetting while preserving retention. However, these approaches assume that the forget and retain sets are readily available, which rarely holds in practice. Unlearning is typically triggered by an undesired generation at inference time, making the retrieval of relevant data the central challenge. \nWe introduce the notion of \\emph{data Pareto improvement} for LLM unlearning, which formalizes how retrieval can expand the achievable trade-off frontier between forgetting and retention. To realize this principle, we propose Randomized Antipodal Search on Linearized Influence Kernel (RASLIK), a retrieval algorithm that combines permutation–projection hashing with randomized antipodal search. RASLIK reduces selection variance, achieves sublinear complexity, and yields a double gain in both quality and efficiency. Across multiple models, datasets, and unlearning algorithms, RASLIK consistently outperforms deterministic baselines and even oracle sampling, establishing randomized search as a principled and scalable solution for data-centric unlearning.", "tldr": "", "keywords": ["Unlearning", "Randomized Algorithms"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff54f8f31cc6994756800cecda3fae511babb241.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles an important but underexplored aspect of LLM unlearning: data retrieval. The authors argue that existing work focuses on optimization algorithms while assuming forget/retain sets are given, which doesn't reflect real-world scenarios where unlearning is triggered by problematic generations at inference time. They introduce the concept of \"data Pareto improvement\" and propose RASLIK (Randomized Antipodal Search on Linearized Influence Kernel), which uses permutation-projection hashing to efficiently retrieve both aligned samples (to forget) and anti-aligned samples (to retain) from training data. The method is evaluated on two models (OLMo-2-1124-7B and Pythia-2.8B) across two scenarios (trigger-based and domain-specific forgetting) with two unlearning algorithms (GA_GDR and GA_KLR), showing consistent improvements over baselines including oracle sampling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Problem formulation: Reframing unlearning as a data retrieval problem is insightful and addresses a real practical gap. The \"data Pareto improvement\" concept provides a principled way to think about retrieval quality.\n\nTheoretical contribution: Theorem 3.3 formally establishes variance reduction properties, which is more rigorous than purely empirical methods. The proof sketch makes intuitive sense (randomization smooths boundary decisions).\n\nComprehensive evaluation: Testing across 2 models × 2 algorithms × 2 datasets with multiple baselines shows thoroughness. The ablation study (RASLIK-F, CR-x variants) is informative.\n\nPractical efficiency: Achieving sublinear complexity O(|X|k) vs O(|X|d) is important for large-scale applications. The antipodal search via sign-flipping is an elegant computational trick."}, "weaknesses": {"value": "Missing critical baselines: The paper discusses RapidIn, DataInf, and Alinfik in related work (Section 5) but doesn't compare against them experimentally. These are the most relevant recent methods for influence-based retrieval in LLMs. Without this comparison, it's difficult to assess whether RASLIK truly advances the state-of-the-art or simply outperforms weaker baselines like BM25 and random selection.\n\nLimited scale validation: Only testing on 2.8B and 7B models is a significant limitation for a method claiming to be \"scalable.\" Modern deployed LLMs are often 13B, 70B, or larger. Will the sketch-based approach remain efficient as d grows to hundreds of billions of parameters? Even one experiment on a 13B+ model would strengthen the scalability argument considerably.\n\nOracle sampling paradox under-explored: The claim that RASLIK outperforms oracle sampling (Table 2, Table 3) is counterintuitive and potentially the paper's most interesting finding, but the explanation is insufficient. The CR-x ablation in Table 3 shows that some noise helps, but why does randomization beat having ground-truth labels? Is this about regularization, avoiding overfitting, or something more fundamental? This deserves deeper analysis rather than a brief mention in Section 4.4."}, "questions": {"value": "Why no comparison with RapidIn/DataInf/Alinfik? These are cited in your related work as recent advances in influence-based retrieval for LLMs. Were there technical obstacles to including them, or practical constraints? Even if full experiments aren't feasible, could you provide a conceptual comparison explaining how RASLIK differs from and improves upon these methods?\n\nCan you validate Assumption 3.2 empirically? The boundary mass assumption (Λ > 0, margin Γ > γ) is central to Theorem 3.3's variance reduction guarantee. Can you show that your experimental datasets actually satisfy this? For example, plot the distribution of ρ_x values around thresholds τ_F and -τ_R to demonstrate non-zero boundary mass.\n\nWhat explains the oracle paradox? Why does randomized retrieval consistently outperform oracle sampling? Is this result stable across different random seeds, or could it reflect favorable cherry-picking? Have you analyzed which specific samples RASLIK retrieves differently from Oracle, and why those differences lead to better forgetting-retention trade-offs? This finding challenges conventional wisdom and deserves thorough investigation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "42GitqXHJ6", "forum": "Xn6EnJZghu", "replyto": "Xn6EnJZghu", "signatures": ["ICLR.cc/2026/Conference/Submission15690/Reviewer_5Nif"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15690/Reviewer_5Nif"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761518413611, "cdate": 1761518413611, "tmdate": 1762925942074, "mdate": 1762925942074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on Large Language Models (LLMs) unlearning, especially the data perspective of achieving satisfactory unlearning. Sepcifically, this work assumes that the forget and retain sets are readily available but rarely holds in practice, so the critical question comes to retrival of relevant data for unlearning. To this end, this work propose randomized antipodal search on linearized influence kernel, a retrieval algorithm that combines permutation-projection hashing with randomized antipodal search. Various experiments are conducted to verify the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper considers one critical problem setting in unlearning, i.e., the data accessibility for forget and retain set. Which is important but rarely considered in LLM unlearning problem.\n2. The proposed RASLIK is novel and reasonable to tackle the targeted problem of effective retrieval of relevant data.\n3. The experiments including verfiication on different model and different retriving methods are comprehensive."}, "weaknesses": {"value": "1. Is there any experimental verification on the computational efficiency.\n2. It is unclear how we can decide the threshold in practice, and are there any practical implications for the derived theorem?\n3. In addition to the various related works on LLM unlearning, the authors should also consider discussing one work that first considered the partial accessibility of forgetting data in machine unlearning in decoupling the concept and target decoupling [1]. [1] Decoupling the Class Label and the Target Concept in Machine Unlearning. arXiv, 2025"}, "questions": {"value": "Please consider the questions in the weakness part for revision suggestions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7QbYpSmO0E", "forum": "Xn6EnJZghu", "replyto": "Xn6EnJZghu", "signatures": ["ICLR.cc/2026/Conference/Submission15690/Reviewer_TAgW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15690/Reviewer_TAgW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856361044, "cdate": 1761856361044, "tmdate": 1762925941513, "mdate": 1762925941513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the concept of data pareto improvement. Specifically, instead of assuming the forget set and retain set are available for an unlearning request, the authors assume access to only a target generation, which indicates the behavior that we want to forget. Based on this target, the authors retrieve a corresponding forget set and retain set from the training set of the LLM. For retrieval, the paper proposes randomized antipodal search, which first randomly projects the gradient into a lower-dimensional space and then retrieves data samples based on the cosine similarity between the query and training data in the projected space. Experiments on two benchmarks show that the proposed method is effective across different unlearning methods and base models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is novel in the sense that it touches on a new perspective of LLM unlearning. The proposed new setting, where the forget and retain sets need to be retrieved, is also more practical in real-world applications. In general, the reviewer thinks this is a good direction for LLM unlearning.\n2. The authors propose a method for this problem based on randomized projection, which is efficient in large-scale applications.\n3. The writing and presentation are clear."}, "weaknesses": {"value": "1. My main concern is the benchmarks used for evaluation. Based on my understanding, in both benchmarks, the knowledge we want to forget is not learned during the LLM's pre-training. Instead, they are behaviors that need to be fine-tuned on a poisoned dataset, or knowledge that needs to be learned from a synthetic dataset. In other words, the knowledge we aim to forget comes from the fine-tuning stage and is purely synthetic. The paper will be greatly improved if the experiments are done to forget the real-world knowledge in LLMs, and the forget and retain sets are retrieved from the actual pre-training corpus. For example, datasets like [1-3] target real-world knowledge for unlearning.\n\n2. The proposed method does not consistently outperform the baselines. For example, in Table 2, the proposed method is worse than the embedding similarity baseline on the Howdy-Alpaca Dataset for the OLMo model and GA_GDR unlearning. In other cases, the performance is worse in either forget rate or retain rate.\n\n3. The cost of the method is not reported. Based on my guess, the proposed method might be more expensive in time compared to baselines like embedding similarity because of the gradient calculation. Can the authors compare the actual time complexity and justify the use of the proposed method?\n\n[1] Shi et al., MUSE: Machine Unlearning Six-Way Evaluation for Language Models.\n\n[2] Li et al., The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning.\n\n[3] Liu et al., Revisiting Who’s Harry Potter: Towards Targeted Unlearning from a Causal Intervention Perspective."}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "agUUwX4aF1", "forum": "Xn6EnJZghu", "replyto": "Xn6EnJZghu", "signatures": ["ICLR.cc/2026/Conference/Submission15690/Reviewer_vAEx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15690/Reviewer_vAEx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953775867, "cdate": 1761953775867, "tmdate": 1762925941083, "mdate": 1762925941083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new LLM unlearning setting, where the forget data and retain data is not complete. It frames unlearning as a retrieveal problem where given an example generation for unlearn, the task is to identify suitable forget data and retain data from corpus to achieve effective unlearning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Novel data-centric perspective for LLM unlearning. The proposed retrieval setup to build best forget and retain data is novel and interesting.\n* The proposed randomized search method has a theoratical guarantee."}, "weaknesses": {"value": "* Incomplete evaluation setup. This paper synthetically constructs two new dataset for evaluating the performance of proposed method. While the experiment show promising performance, it's unclear how this setup approximates real-life unlearning request. For example, the trigger forgetting seems to be a easy pattern matching for retrieval, and the domain-specific forgetting seems also obvious for retrieval.\n* Constructing this setup in some commonly used LLM unlearning dataset like TOFU (one author information sentence and the retrieval can target at other factual knowledge of that author) and WMDP should be helpful.\n* Missing retrieval example. It's unclear that what kind of sentence or knowledge can be retrieved via the proposed method, raising some concerns about why the performance is better compared to other method."}, "questions": {"value": "* What's the fictional world knowledge is for Virtual-Alpaca? There seems not to be examples in the main paper or appendix.\n* What's the search run-time for proposed method?\n* The proposed synthetic dataset seems both on question answering or short textual response, can the framework be applied to large knowledge/concept unlearning like the one in RWKU paper about all information about a person?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Zf7CbkyCvp", "forum": "Xn6EnJZghu", "replyto": "Xn6EnJZghu", "signatures": ["ICLR.cc/2026/Conference/Submission15690/Reviewer_sPkh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15690/Reviewer_sPkh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971751930, "cdate": 1761971751930, "tmdate": 1762925940197, "mdate": 1762925940197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}