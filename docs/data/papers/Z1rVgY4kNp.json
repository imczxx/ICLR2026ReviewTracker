{"id": "Z1rVgY4kNp", "number": 18814, "cdate": 1758291062735, "mdate": 1759897079939, "content": {"title": "From Weak Data to Strong Policy: Q-Targets Enable Provable In-Context Reinforcement Learning", "abstract": "Transformers trained with offline expert-level data have shown remarkable success in In-Context Reinforcement Learning (ICRL), enabling effective decision-making in unseen environments. However, the performance of these models heavily depends on optimal or expert-level trajectories, making them expensive in various real-world scenarios. In this work, we introduce Q-Target Pretrained Transformers (QTPT), a novel framework that leverages Q-learning instead of supervised learning during the training stage. In particular, QTPT doesn't require optimal-labeled actions or expert trajectories, and provides a practical solution for real-world applications. We theoretically establish the performance guarantee for QTPT and show its superior robustness to data quality compared to traditional supervised learning approaches. Through comprehensive empirical evaluations, QTPT consistently outperforms existing approaches, especially when trained on data sampled with non-expert policies.", "tldr": "", "keywords": ["In-Context Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/04b334d6704208c73b6c3e73709c6e41737f57a8.pdf", "supplementary_material": "/attachment/7c5c5ab18e9a555da704fbd35998be2fa96b85cd.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a Q-learning-based Transformer pretraining framework, termed QTPT, which aims to reduce the reliance on high-quality expert data in in-context reinforcement learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The study introduces QTPT, which incorporates Q-learning objectives into Transformer pretraining to replace traditional behavior cloning, thereby reducing dependence on expert demonstrations. By embedding the Bellman update process within the Transformer architecture, QTPT enables end-to-end TD learning.\n\n2.Theoretical Strength:\nThe paper provides theoretical guarantees on the sub-optimality bounds of QTPT under finite-horizon MDPs and stochastic linear bandit settings. It further decomposes the total error into sampling bias and model bias, establishing a clear and rigorous theoretical framework for analysis.\n\n3.Empirical Validation:\nExtensive experiments conducted across multiple environments—including linear bandit, Darkroom, Miniworld, and mathematical reasoning tasks—demonstrate the effectiveness and robustness of QTPT."}, "weaknesses": {"value": "1.The combination of Q-learning and Transformer architectures is not novel. In fact, numerous studies in Offline RL have explored similar approaches, as evidenced by prior works such as [1, 2, 3].\n\n2.Lack of visualization and intuitive demonstration: Although the authors provide extensive assumptions and theoretical proofs to justify the effectiveness of their method, the paper lacks visualizations and intuitive empirical results that illustrate how QTPT functions in practice. Merely reporting cumulative reward performance is insufficient to demonstrate the learning dynamics, stability, or interpretability of the proposed approach.\n\n3.Mismatch between theory and experiments: The theoretical analysis assumes the offline dataset is generated by a single behavior policy $\\pi_\\beta$. However, the experiments use mixtures of trajectories (random + expert). The paper does not provide theory or discussion on how such policy-mixed datasets affect QTPT’s guarantees or performance.\n\n4.Lack of key baseline comparisons: The paper does not include comparisons with other Offline RL methods such as CQL or IQL under the same experimental settings. Moreover, it fails to compare against Q-learning Decision Transformer, which is a closely related approach. Although Q-SFT is mentioned in the related work section, no experimental results are provided to contrast its performance with QTPT.\n\n5.Limited experimental scale: The experimental settings are relatively small and simplistic. For instance, in the linear bandit task, the dimensions are limited to d=5, A=10, and T=200, which are insufficient to test scalability. Similarly, the Darkroom environment uses only a 10×10 grid with 100 steps, making it too simple to demonstrate the method’s effectiveness in more complex or large-scale scenarios.\n\n6.Failure of Q-learning in sparse-reward environments: The paper acknowledges that QTPT fails under the Dark Key-to-Door setting when using expert data, attributing this to the inability of Q-learning signals to propagate effectively in sparse-reward environments. However, such environments are common in reinforcement learning, meaning this limitation significantly restricts the method’s practical applicability. Furthermore, the paper does not propose any solution or mitigation strategy to address this issue.\n\n[1].Kim J, Lee S, Kim W, et al. Adaptive Q-aid for conditional supervised learning in offline reinforcement learning. NIPS, 2024.\n\n[2].Hu S, Fan Z, Huang C, et al. Q-value regularized transformer for offline reinforcement learning. ICML, 2024.\n\n[3].Wang Y, Yang C, Wen Y, et al. Critic-guided decision transformer for offline reinforcement learning.AAAI, 2024."}, "questions": {"value": "1.Compared with Q-SFT and Q-learning Decision Transformer, what are the main technical differences of QTPT? The paper claims that QTPT is “end-to-end”, yet its training process also appears to involve iteratively computing targets and updating parameters. Could the authors clarify this apparent inconsistency?\n\n2.How is Assumption 3.1(b) guaranteed to hold under a random policy? This seems especially problematic in large-state-space MDPs, where a random policy is exceedingly unlikely to cover trajectories visited by the optimal policy. Could the authors (i) provide a weaker assumption that still suffices for the result, or (ii) offer a dedicated analysis for random-policy data, e.g., conditions based on concentrability coefficients, coverage/mismatch metrics, or state–action visitation lower bounds that make the proposition valid in this setting?\n\n3.In Proposition 3.2, how is Assumption 3.1(b) ensured to hold under a random policy? In large state-space MDPs, a random policy is highly unlikely to cover the trajectories of the optimal policy. Could the authors provide either a weaker assumption or a dedicated analysis tailored to the random policy setting?\n\n4.In Figure 2a, the performance gap between QTPT and SPT on the LinUCB dataset appears small. Is this difference statistically significant? Could the authors provide confidence intervals or significance tests for all experimental results?\n\n5.In the mathematical reasoning experiment (Section D.3), the authors actually use QTPO (an actor–critic variant) rather than the original QTPT. Why does QTPT itself produce degenerate outputs in this setting? Does this issue limit the applicability of QTPT to language-related tasks?\n\n6.The experiments use a mixture of random and expert data, whereas the theoretical analysis assumes a single behavior policy. Could the authors extend the theoretical framework to account for mixed-policy datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ehwbExdorF", "forum": "Z1rVgY4kNp", "replyto": "Z1rVgY4kNp", "signatures": ["ICLR.cc/2026/Conference/Submission18814/Reviewer_X4wV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18814/Reviewer_X4wV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761144384240, "cdate": 1761144384240, "tmdate": 1762999989819, "mdate": 1762999989819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Q-Target Pretrained Transformers (QTPT), a framework that leverages Q-learning for pretraining to alleviate reliance on optimal action labels or expert trajectories. The authors provide theoretical upper bounds on the suboptimality of QTPT and present empirical results demonstrating that QTPT outperforms supervised pretraining baselines, particularly when trained on suboptimal or noisy datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a practically important problem: high-quality expert data are costly and scarce in real-world settings, making methods that learn effectively from suboptimal data highly valuable.\n\n2. The work provides a clear theoretical analysis with upper bounds on suboptimality, offering principled insights into when and why the approach should succeed.\n\n3. The overall writing is clear and well-organized, making the methodology and results easy to follow."}, "weaknesses": {"value": "1. Baselines and fairness of comparison: The paper compares QTPT only against SPT and behavior cloning. Please include stronger, closely related baselines such as Q-SFT and Q-Decision Transformer under matched model size, data, and compute to enable a fair and systematic performance evaluation.\n\n2. Counterintuitive trend in Figure 2(b): In Figure 2(b), the suboptimality of the proposed approach appears to increase as the expert ratio in the pretraining data increases. This seems counterintuitive—performance should generally improve with higher-quality data. Please analyze the underlying cause.\n\n3. Derivation details: Please provide a more detailed derivation for the comparison between the suboptimality of QTPT and SPT referenced in Lines 970–971, including all intermediate steps, assumptions, and how terms are controlled to obtain the final inequality."}, "questions": {"value": "Please see Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JDxZPTu4v5", "forum": "Z1rVgY4kNp", "replyto": "Z1rVgY4kNp", "signatures": ["ICLR.cc/2026/Conference/Submission18814/Reviewer_QZqr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18814/Reviewer_QZqr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636646580, "cdate": 1761636646580, "tmdate": 1762999989808, "mdate": 1762999989808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing in-context reinforcement learning (ICRL) methods (e.g., AD and DPT) require high quality pretraining data, which is difficult to obtain in practice. This work proposes Q-Target Pretrained Transformers (QTPT), which replaces existing ICRL methods' supervised learning objectives with a Q-learning objective. QTPT can directly estimate the optimal Q function from suboptimal data, addressing the challenging data requirement problem. The authors provided theoretical guarantees for QTPT. The regret upper bounds isolate the impacts of sample bias and model bias. Lastly, they illustrate the effectiveness of QTPT using standard ICRL benchmarks including Darkroom, Dark Key-to-door, and Miniworld."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work addresses one of the most important problems in ICRL. \n- The experiment benchmarks are comprehensive including bandits and all commonly used benchmarks (Darkroom, Key-to-door, Miniworld) and a novel Math benchmark (although there are many important baselines missing, see in Weaknesses)."}, "weaknesses": {"value": "## Related Work\n\nAs data requirement problem is one of the most important ICRL problems, many prior works have addressed this problem with different algorithmic attempts. To name a short list,\n\n1. Yes, Q-learning helps offline in-context RL. \n2. In-Context Reinforcement Learning From Suboptimal Historical Data.\n3. Meta-DT: Offline Meta-RL as Conditional Sequence Modeling with World Model Disentanglement\n\nThese prior works **all don't need** high-quality trajectories or optimal action labels in the pretraining dataset. Without discussing these works or comparing these work in the experiments, it is challenging to evaluate the value of this work. \n\n## Algorithm Soundness\n\nIt is a well-known problem that in practice, offline RL methods suffer significantly from the overestimation problem induced by function approximation and the Bellman (max) operator. As a consequence, they often rely on approaches such as pessimism to penalize the optimal Q functions' bootstrapped target values. In the ICRL, this problem is similar but QTPT has no such efforts. \n\n## Experiments\n\n- It is expected that QTPT outperforms SPT on suboptimal pretraining trajectories, as SPT is not designed for these scenarios. \n- It is more helpful if the authors compare QTPT using suboptimal data with DPT using optimal action labels and AD also using its required pretraining data. In this way, we can see how much performance we lose when moving away from high-quality pretraining data. \n- Most importantly, there are many important missing baselines all addressing the same challenge (see a short list above). \n\n## Regret Decomposition \n\n- The regret decomposition in Eq.5 is more than common for learning theory results. Thus, I personally find the claim that \"Such a clear separation builds a novel theoretical\nfoundation to isolate the impacts of sample bias and model bias and sheds light on how QTPT\nbalances leveraging available data and mitigating model bias during pretraining\" may over claim novelty of the results. \n- More importantly, the regret decomposition appears incorrect to me (not the regret upper bounds for the LHS of the regret decomposition, just the decomposition itself). In particular,  in finite sample results, we care the regret of data-driven solutions, that is, the suboptimal gap between the optimal policy and a policy learned from finite samples. But now the LHS of Eq 5 is not even sample-size dependent -- it only involves policies related to population loss $\\pi_{\\tilde{Q}}$. I believe the correct sample decomposition should be $Subopt(\\pi^\\star, \\pi_{\\hat{Q}_n}) \\le Subopt(\\pi^\\star, \\pi_{\\tilde{Q}}) +  Subopt(\\pi_{\\tilde{Q}},  \\pi_{\\hat{Q}_n})$. \n- What is $\\log |\\mathcal{Q}|$ in Proposition 3.2?"}, "questions": {"value": "See in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fvPE7CUidi", "forum": "Z1rVgY4kNp", "replyto": "Z1rVgY4kNp", "signatures": ["ICLR.cc/2026/Conference/Submission18814/Reviewer_qqhv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18814/Reviewer_qqhv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851355557, "cdate": 1761851355557, "tmdate": 1762999989579, "mdate": 1762999989579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the problem of in-context reinforcement learning. It proposes to formulate the in-context reinforcement learning problem as an MDP. Then use Q-learning to train a Q-function from offline data. It derives theoretical bounds for policy performance, and shows that it outperforms BC transformer policy on linear bandit problems and grid world environments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper studies an important problem of in-context reinforcement learning.\n\n- The reduction from in-context reinforcement learning to MDP is interesting.\n\n- The proposed algorithm is well-described and easy to understand."}, "weaknesses": {"value": "- The novelty is limited, since it simply combines Q-learning with Transformer architecture, which has been explored in existing work, e.g., [4].\n\n- The paper makes strong assumptions about the coverage of the behavioral policy $\\pi_{\\beta}$, in that it provides sufficient support for the optimal policy $\\pi^\\star$. This assumption is particularly strong because the state space $\\mathcal{X}$ covers the entire history. It is unclear how the proposed method can scale beyond simple environments.\n\n- A few papers in the offline RL literature, e.g., [1-3], make use of pessimism to avoid over-estimating the Q function when the behavioral policy does not provide good coverage over the state-action distribution visited by the optimal policy. It would be interesting to see if this can be combined with the proposed method.\n\n- A very related work is Q-transformer, which also uses the transformer architecture to learn Q function with a pessimistic version of Q-learning. It would be good to have a discussion on it.\n\n**References**\n\n[1] Jin, Ying, Zhuoran Yang, and Zhaoran Wang. \"Is pessimism provably efficient for offline rl?.\" International conference on machine learning. PMLR, 2021.\n\n[2] Fujimoto, Scott, and Shixiang Shane Gu. \"A minimalist approach to offline reinforcement learning.\" Advances in neural information processing systems 34 (2021): 20132-20145.\n\n[3] Xie, Tengyang, et al. \"Bellman-consistent pessimism for offline reinforcement learning.\" Advances in neural information processing systems 34 (2021): 6683-6694.\n\n[4] Chebotar, Yevgen, et al. \"Q-transformer: Scalable offline reinforcement learning via autoregressive q-functions.\" Conference on Robot Learning. PMLR, 2023."}, "questions": {"value": "- Technically the problem described in 2.1 is a POMDP, since $M$ is not given to the learner. What are the benefits and drawbacks of reducing it to an MDP where the state-space is the history of past experience?\n\n- What would be the benefits and drawbacks of the proposed method compared to estimating $M$ from the history and learning a $M$-conditioned policy, similar to how one would solve a POMDP problem?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4s11wLojJl", "forum": "Z1rVgY4kNp", "replyto": "Z1rVgY4kNp", "signatures": ["ICLR.cc/2026/Conference/Submission18814/Reviewer_pMt4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18814/Reviewer_pMt4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960293186, "cdate": 1761960293186, "tmdate": 1762930204914, "mdate": 1762930204914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}