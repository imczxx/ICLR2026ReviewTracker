{"id": "Yo7eG3lC3y", "number": 24444, "cdate": 1758356926529, "mdate": 1759896765714, "content": {"title": "LEGO-Eval: Towards Fine-grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation", "abstract": "Despite recent progress in using Large Language Models (LLMs) for automatically generating 3D scenes, generated scenes often lack realistic spatial layouts and object attributes found in real-world environments. As this problem stems from insufficiently detailed, coarse-grained instructions, advancing 3D scene synthesis guided by more detailed, fine-grained instructions that reflect real-world environments becomes crucial. Without such realistic scenes, training embodied agents in unrealistic environments can lead them to learn priors that diverge significantly from real-world physics and semantics, degrading their performance when deployed. Thus, verifying the alignment between the fine-grained instruction and the generated scene is essential for effective learning. However, current evaluation methods, such as CLIPScore and vision-language models (VLMs), often fail to reliably assess such alignment. This shortcoming arises primarily from their shallow understanding of 3D scenes, which often leads to improperly grounded scene components. To address this, we introduce LEGO-Eval, an evaluation framework equipped with diverse tools designed to explicitly ground scene components, enabling more accurate alignment assessments. We also present LEGO-Bench, a benchmark of detailed instructions that specify complex layouts and attributes of real-world environments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge by 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with LEGO-Bench reveals significant limitations in current generation methods. Across all evaluated approaches, success rates reached at most 10% in generating scenes that fully align with fine-grained instructions.", "tldr": "", "keywords": ["Text-Guided 3D Scene Synthesis", "Multimodal Large Language Models", "Automatic Evaluation", "Benchmark"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ef03cbc91a706ef64765e177b5d935abdfec0c6b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new metric, LEGO-Eval, for evaluating the 3D embodied environments.  LEGO-Eval uses multiple tools to extract the information from the scene and verify if they satisfy the constraints. The evaluation results demonstrate LEGO-Eval achieves much higher agreement with human judger than previous metrics like CLIP-Score."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed metric has much better agreement with human judgement compared to other metrics.\n2. The refinement experiments is a highlight that such metrics can be reliable rewards for improving systems."}, "weaknesses": {"value": "**Require dense scene annotations.** This is the major concern of the proposed evaluation metric which requires dense annotations of each assets (attributes, locations, etc). However, for some methods which generate entire scene in a single mesh (e.g., diffusion-based model), the proposed method cannot use tools to get those information. The results in Table 2 also validate this concern that, with textual reasoning on attributes and precise spatial information from rendering engine, the performance will drop significantly. Therefore, the usability of the proposed metric is quite limited."}, "questions": {"value": "1. How many human annotators are recruited for collecting human judgments?\n2. How many examples are used in Figure 7 to get the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "r7xrk1YjgT", "forum": "Yo7eG3lC3y", "replyto": "Yo7eG3lC3y", "signatures": ["ICLR.cc/2026/Conference/Submission24444/Reviewer_oMe6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24444/Reviewer_oMe6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974932226, "cdate": 1761974932226, "tmdate": 1762943083836, "mdate": 1762943083836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Hello authors, thank you for putting together this interesting submission. It is nice to see more work focusing on the evaluation gaps in this area. As I was reading, I had a few questions about both the conceptual choices and the implementation details.\n\n1. I am curious how much the tool-based approach actually contributes. In Appendix C3, some of the tools handle very simple lookups, for example retrieving the set of objects in a room. Could these be replaced with a single dictionary that contains all of this information about the scene, which recent LLMs should be able to parse directly, avoiding the need for extensive runtime tool selection?\n\n2. In Section 4.1.3 you note that text can capture fine-grained attributes, for example the color of small objects, and can also summarize information extracted from retrieved images. If I understand correctly, in LEGO-Eval a VLM first looks at rendered object images to produce this text, and then an LLM or VLM processes this text again downstream. In that case, is the intermediate textual step actually necessary, or could the system achieve similar performance by working directly with the images instead of converting them into text first, which would also reduce the amount of tool use?\n\n3. For relationships like left and right, the frame of reference can be ambiguous. How do you define the coordinate system or viewpoint when evaluating these relations?\n\n4. The textual descriptions in LEGO-Bench seem quite templated, for example sentences like \"there is ... in the room\" or \"\\<object\\> is or has \\<attribute\\>\". Is this style used throughout the dataset, and do the instructions in Figures 2 and 8 also follow a fixed order, such as rooms first, then objects, then relations in separate sentences? This seems easier to parse than natural descriptions of indoor scenes. Have you considered or tested more free-form descriptions, and how do you think that would affect the results in Table 4?\n\n5. In Section 4.2.1, could you clarify how you adapted I-Design, LayoutGPT, and LayoutVLM to work with Holodeck? On lines 361 to 362, you mention that LayoutGPT and LayoutVLM position a given set of objects, but both methods can also generate scenes from scratch. LayoutVLM provides the prompts they used to select objects in their supplementary material, and the LayoutGPT paper specifies that it is given the available categories and then decides what to place, rather than taking an explicit object list. How did you configure these methods in your experiments to fit the Holodeck setup?\n\nOnce again, it is encouraging to see more attention on the evaluation gap in this area. Thank you for the interesting work!"}}, "id": "Qe2GLpTxTz", "forum": "Yo7eG3lC3y", "replyto": "Yo7eG3lC3y", "signatures": ["~Hou_In_Ivan_Tam1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Hou_In_Ivan_Tam1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24444/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763509639525, "cdate": 1763509639525, "tmdate": 1763509639525, "mdate": 1763509639525, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles a pain point in text-guided 3D scene generation: we can now generate scenes from language, but we can't reliably tell if the scene actually matches the detailed instruction. Existing automatic evaluators (e.g., CLIPScore) don't really understand 3D layouts, and they crumble on constraints.\n\nTo address this, the authors propose LEGO-EVAL, a tool-augmented evaluation pipeline. The idea is: Take the long instruction, identify and break it into structured constraints (4 types so far); For each constraint, plan which tools to call (Unity environment interaction, textual reasoning, VLM reasoning), execute those tools to actually ground the entities, and then give a binary judgment with evaluation explanations, declare the whole scene valid only if all constraints pass.\n\nAlongside this, the authors build LEGO-BENCH, focusing on the attributes and spatial relationships of 3D scene generations, so that different evaluators can be compared."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Reframes 3D-scene evaluation as a tool-augmented reasoning task. Combining constraint extraction, planning, and multimodal tool calls for grounding is a novel and well-motivated contribution.\n- The pipeline and tool taxonomy are well-explained. Figures and examples make the method intuitive.\n- Strong experiments with fair baselines (e.g., CLIPScore, SceneEval). Clear metrics, ablations, and human alignment analyses."}, "weaknesses": {"value": "- Simulator dependency: LEGO-EVAL assumes access to the scene graph and Unity backend. This may not be available in many real settings like photorealistic assets.\n- Scene limination: LEGO-BENCH is limited to indoor scenes. Broader or more varied data would strengthen claims.\n- Failure analysis: It's unclear which constraint types cause most errors for baselines."}, "questions": {"value": "1. Can LEGO-EVAL operate without simulator access?\n2. What is the average tool-call cost per instruction and runtime per scene?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DIz1MystPM", "forum": "Yo7eG3lC3y", "replyto": "Yo7eG3lC3y", "signatures": ["ICLR.cc/2026/Conference/Submission24444/Reviewer_MKU2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24444/Reviewer_MKU2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994402651, "cdate": 1761994402651, "tmdate": 1762943083635, "mdate": 1762943083635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper builds on the core insight that existing evaluation methods (such as using VLMs as judges) do not adequately match fine-grained text instructions with 3D scenes; this becomes a problem for downstream use-cases such as text-to-scene synthesis. To address this, the paper introduces (i) LEGO-Bench, a manually annotated (n = 130) dataset of text-scene pairs, and (ii) LEGO-Eval, a tool-based evaluation method that drastically outperforms VLM-as-judges when compared to ground-truth."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. LEGO-Eval’s tool-grounded pipeline drives a striking jump in F1 versus the usual VLM-as-judge baselines, showing that explicit grounding leads to better alignment verdicts.\n\n1. LEGO-Bench is valuable: 130 instructions with roughly 1.2k hand-checked constraints covering both architectural makeup and object relations give the community a realistic, fine-grained stress test. The field of scene graphs, while tangential to this paper, _also_ incidentally lacks high-quality fine-grained annotations for scenes, despite it being a common drawback of VLMs. \n\n1. The paper is well-written and easy to follow. The experimental coverage is thoughtful; it has ablations over tool types, comparisons against four synthesis systems, and the Holodeck refinement loop (Fig. 7) all help illustrate the usefulness of LEGO-Eval/Bench."}, "weaknesses": {"value": "1. The paper does not provide conclusive evidence (or even a brief discussion) to the claim that finer-grained text-scene alignment leads to real embodied gains. The paper does provide _preliminary_ evidence via the Holodeck refinement vignette (Fig. 7); however there’s no “detect -> repair -> retrain” loop or even a pointer to existing sim-to-real failures. A minimal downstream study (or stronger citations) would make the story much more convincing.\n\n\n1. LEGO-Eval leans on several Unity-facing tools, so the comparison to image-only VLM judges risks being apples-to-oranges. Please add baselines that ingest similar structure (e.g., VLM + detector/scene-graph outputs, see weakness #2 and question #3 below) to show the lift truly comes from the proposed orchestration.\n\n\n3. The paper has a limited analysis of failure modes of VLM-as-judges. Figure 8 hints that VLM judges mostly hallucinate or misidentify objects, yet we never see how often that happens or how severe it is. Please tally the dominant error types (mis-identification, spatial mistakes, attribute mismatches) so we can tell whether cascading failures are the main culprit. If mis-identification dominates, test baselines that feed object detection outputs [1,2] or structured summaries from scene graph generators to see how much ground they recover. Likewise, benchmark 3D-language models (3D-LLM, Point-LLM) that already encode volumetric context. A quantitative breakdown plus these stronger baselines would clarify when LEGO-Eval is indispensable versus when richer perception priors nearly match it.\n\n[1] Grounding-DINO. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. https://arxiv.org/abs/2303.05499\n[2] YoloV8, https://yolov8.com/"}, "questions": {"value": "1. As mentioned above, do you have any downstream evidence (even a small detect -> repair -> retrain study, or at least documented cases in the literature) that tighter instruction-scene alignment boosts embodied performance?\n\n1. Can you quantify the cost tradeoff of LEGO-Eval: number of tool calls per instruction, duration (limitation currently mentions \"two hours\" for the 260 samples), and approximate compute cost per sample. How do these figures compare to the single-pass VLM judge?\n\n1. It seems that LEGO-Eval is specifically targeted at static, top-down scenes and requires careful tool curation. Can it theoretically cope with dynamic scenes? Furthermore, would it make sense to add scene graph generators [1,2], which produce fine-grained annotations from scenes, as a possible baseline? \n\nI am quite willing to raise my score if the above questions and weaknesses are addressed.\n\n[1] Gu et al, ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning. https://arxiv.org/abs/2309.16650\n[2] Huang et al, LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene Graphs with Weak Supervision. https://arxiv.org/abs/2304.07647"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1Xp5BhJa6O", "forum": "Yo7eG3lC3y", "replyto": "Yo7eG3lC3y", "signatures": ["ICLR.cc/2026/Conference/Submission24444/Reviewer_zAu4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24444/Reviewer_zAu4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240365781, "cdate": 1762240365781, "tmdate": 1762943083443, "mdate": 1762943083443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LEGO-EVAL, a new evaluation framework that uses a diverse set of tools (for environment interaction, textual reasoning, and multimodal reasoning). This tool-augmented approach allows it to explicitly ground scene components and accurately assess if the generated 3D scene aligns with complex, detailed instructions. \n\nThe authors also created LEGO-BENCH, a new benchmark of fine-grained instructions for 3D environments. Experiments show LEGO-EVAL dramatically outperforms VLM-as-a-judge (0.81 vs. 0.40 F1 score) in alignment with human judgments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Instead of relying on one AI model to just \"look\" at the scene, the paper introduces LEGO-EVAL, which acts more like a detective. It uses a set of specialized \"tools\" to check specific facts—one tool to find all the objects, another to check their color, and another to measure their spatial relationships.\n\nThe authors created their own difficult test (called LEGO-BENCH) full of complex instructions. They proved their new \"judge\" (LEGO-EVAL) is far more accurate than older methods.\n\nExperiments also show that current AI models for building 3D scenes are still very bad at following detailed instructions, failing most of the time."}, "weaknesses": {"value": "The paper introduces a new test set called LEGO-BENCH, but it only contains 130 instructions. This is a very small number, which might not be enough to prove the necessity of making such a benchmark. In fact, there are many indoor scene synthesis benchmarks and it is not even worthwhile to start a new language-instructure synthesis from scratch.\n\nIn LEGO-BENCH, the scenes used to test the evaluator were created \"manually.\" This process is very slow, expensive, and hard to scale. Utilizing a sequence call of LLM APIs to generate, verify, and refine, seemed to be costly and super inefficient in generating a simple contraints scene from natural language.\n\nThe best results come from using \"GPT-4.1.\" The paper shows that performance drops significantly when using smaller or different models. This means the system's success isn't just its smart design but also its reliance on a very powerful (and expensive) \"brain\" that not everyone can access or afford. For example, does a 7B or 4B model good enough to generate good results based on the method proposed?\n\nConsidering the downstream tasks, what can this method bring advantages to? e.g. robotic learning? navigation? gaming?  The dataset from \"manually collect instructions for 3D scene synthesis\" may not be super useful for other envs, tasks, game engines, simulations, e.t.c. In another language, the impact of this LEGO-BENCH is too small."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TQAmyiNnW7", "forum": "Yo7eG3lC3y", "replyto": "Yo7eG3lC3y", "signatures": ["ICLR.cc/2026/Conference/Submission24444/Reviewer_5Zma"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24444/Reviewer_5Zma"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762286440454, "cdate": 1762286440454, "tmdate": 1762943083241, "mdate": 1762943083241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}