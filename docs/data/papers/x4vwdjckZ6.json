{"id": "x4vwdjckZ6", "number": 24532, "cdate": 1758357698145, "mdate": 1759896761697, "content": {"title": "Sensitivity of Small Language Models to Fine-tuning Data Contamination", "abstract": "Small Language Models (SLMs) are increasingly being deployed in resource-constrained environments, yet their behavioral robustness to data contamination during instruction tuning remains poorly understood. We systematically investigate the contamination sensitivity of 23 SLMs (270M to 4B parameters) across multiple model families by measuring susceptibility to syntactic and semantic transformation types during instruction tuning: syntactic transformations (character and word reversal) and semantic transformations (irrelevant and counterfactual responses), each applied at contamination levels of 25\\%, 50\\%, 75\\%, and 100\\%. Our results reveal fundamental asymmetries in vulnerability patterns: syntactic transformations cause catastrophic performance degradation, with character reversal producing near-complete failure across all models regardless of size or family, while semantic transformations demonstrate distinct threshold behaviors and greater resilience in core linguistic capabilities. Critically, we discover a ``\\textit{capability curse}\" where larger, more capable models become more susceptible to learning semantic corruptions, effectively following harmful instructions more readily, while our analysis of base versus instruction-tuned variants reveals that alignment provides inconsistent robustness benefits, sometimes even reducing resilience. Our work establishes three core contributions: (1) empirical evidence of SLMs' disproportionate vulnerability to syntactic pattern contamination, (2) identification of asymmetric sensitivity patterns between syntactic and semantic transformations, and (3) systematic evaluation protocols for contamination robustness assessment. These findings have immediate deployment implications, suggesting that current robustness assumptions may not hold for smaller models and highlighting the need for contamination-aware training protocols.", "tldr": "Our objective is to gain insights into the robustness and adaptability of SLMs in handling data contamination, contributing to a deeper understanding of their learning mechanisms and potential limitations.", "keywords": ["Small Language Models", "Data Contamination", "Fine-tuning Sensitivity"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/62111446971b676a7103a7637798ac3377020179.pdf", "supplementary_material": "/attachment/27b558bfe81afbc00e8f3b93f1b888a743f9c372.zip"}, "replies": [{"content": {"summary": {"value": "This work provides an analysis of various small models' robustness to syntactic and semantic perturbations in fine-tuning data. The authors find that syntactic corruptions lead to larger degradations across all models, while semantic corruptions show more gradual, model-size-dependent effects. This work highlights that common data quality checks may have different implications for smaller models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This work is well written, with good experiments to support the authors' findings of the influence of specific types of fine-tuning data perturbations. The authors address a gap in the literature on robustness of small models during the fine-tuning process, and offers a strong analysis of the impacts on model output behaviour beyond just task accuracy."}, "weaknesses": {"value": "* While the types of transformations are well motivated by practical scenarios (as described in the Appendix), my main concern is with the \"syntactic\" perturbation. The syntactic transformation (especially the word ordering one) is not pure syntax, but actually breaks semantics as well. This makes it difficult to isolate the specific effect of syntactic structure from that of meaning disruption. The resulting performance degradation may reflect a broader semantic breakdown rather than a purely syntactic sensitivity.\n\n* The transformations described are applied broadly across data samples, but there is no quantification or variance of the degree of perturbation. Without this type of analysis, it is unclear how model robustness scales with perturbation strength. For example, a more realistic setup might involve partial or graded transformations (e.g., reversing only a few tokens rather than the entire sentence) to capture the sensitivity of models to incremental syntactic or semantic shifts."}, "questions": {"value": "1. Have the authors done any additional controlled analysis to verify that models fail due to syntactic structure rather than semantic distortion? For instance, could a meaning-preserving reordering yield different result (e.g., using a method similar to work in \"Tailor: Generating and Perturbing Text with Semantic Controls\" from Ross et al, 2022)?  \n\n2. Have the authors tested whether small, localized perturbations lead to proportional degradation, or whether the effect is mostly binary once the semantics are broken?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FJX8LlkMQQ", "forum": "x4vwdjckZ6", "replyto": "x4vwdjckZ6", "signatures": ["ICLR.cc/2026/Conference/Submission24532/Reviewer_4au4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24532/Reviewer_4au4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761521472083, "cdate": 1761521472083, "tmdate": 1762943113794, "mdate": 1762943113794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study how sensitive language models are to different types of data transformations. They consider four transformations: two syntactic (character reversal and word reversal) and two semantic (irrelevant responses and counterfactual responses). Each transformation is applied to 25%, 50%, 75%, or 100% of the training data. Their results show that semantic transformations are especially harmful. Models tend to adopt the transformed patterns almost entirely even when only 25% of the training data is affected. In contrast, syntactic transformations require a higher level of contamination before performance degradation becomes significant. The authors also find that larger models are more prone to learning semantic artifacts, and that prior alignment (e.g., instruction tuning) does not necessarily improve robustness to dataset contamination."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper examines contamination patterns across a wide range of language models, varying in size and family, to ensure the generalizability of its findings.\n- While it may seem intuitive that syntactic patterns are particularly harmful, the paper’s empirical demonstration of this is valuable. More broadly, the observed differences in how models react to various contamination patterns provide important insights."}, "weaknesses": {"value": "- It is unclear how realistic these transformations are in real-world settings, particularly the character and word reversal cases and the large-scale contamination levels (25%–100%). While code-switching may occur, especially in multilingual contexts, it represents a far less disruptive transformation. At such high levels of contamination, the primary concern may no longer be the model’s sensitivity or robustness.\n- The paper does not have a **Related Work** section."}, "questions": {"value": "- Table 1: To what extent can the lower accuracy of *gemma-3-270M-It*, *SmolLM2-360M-It*, and *Qwen-2.5-0.5B-It* be attributed to the general difficulty small language models face in following instructions?\n- Figure 2: What is the rationale for averaging performance across all models for each contamination strategy and ratio?\n- Are the **word** and **character reversal** contaminations reversible? In other words, can the original sentence be deterministically recovered from a shuffled one?\n- Line 259: Why do the **counterfactual transformations** plateau around 70%? Could this be partly due to limitations of the evaluation metric, which may be difficult to fully saturate?\n- Line 265: It is surprising that **character reversal** and **word reversal**, which show similar contamination adherence, diverge so sharply in task accuracy. Moreover, accuracy slightly increases for **word reversal** as contamination rises, why? Does the `task accuracy computation` unshuffle (implicitly or explicitly) model generations before evaluation?\n- Line 297: There is another large gap between **character reversal** and **word reversal**. If embedding scores are computed directly on the model outputs, the plateau for word reversal suggests that the metric may be insensitive to word order. Is it then appropriate in this context?\n- Line 352: Did you investigate the striking difference between *LLaMA 3.2 3B* and *LLaMA 3.2 3B It* at 25% contamination for **character reversal**? Similarly, *OLMO 2 1B It* performs much better than others, could this reflect stronger instruction-following capabilities?\n- Line 416: This conclusion may need to be tested at larger scales to be more robust.\n- Overall, it would be valuable to explore smaller contamination ratios (e.g., 0.1%, 1%, 5%, 10%), task-specific fine-tuning, and larger models (potentially through quantization or parameter-efficient fine-tuning). Varying the size of the training dataset could also yield additional insights."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iSa8nGho10", "forum": "x4vwdjckZ6", "replyto": "x4vwdjckZ6", "signatures": ["ICLR.cc/2026/Conference/Submission24532/Reviewer_9L7H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24532/Reviewer_9L7H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761560568977, "cdate": 1761560568977, "tmdate": 1762943113483, "mdate": 1762943113483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the robustness of Small Language Models (SLMs) to data contamination during the instruction fine-tuning phase. The authors evaluate 23 different SLMs across six model families, testing both base and instruction-tuned variants. The study introduces four types of data contamination: two syntactic (character reversal, word reversal) and two semantic (irrelevant responses, counterfactual responses), applied at different levels of 25%, 50%, 75%, and 100%.\n\nThe core findings reveal a stark asymmetry: SLMs are vulnerable to syntactic contamination, with character-level reversal causing near-complete performance collapse at just 25% contamination. Conversely, models are more resilient to semantic contamination, maintaining grammatical structure even while learning to reproduce flawed content. The paper introduces two novel observations: i) the capability curse, where larger, more capable models are more susceptible to learning and reproducing semantic corruptions; and ii) the alignment paradox, where instruction tuning provides inconsistent and sometimes negative robustness benefits against syntactic corruption. It conclude that robustness to data contamination for SLM deployment is not solved by scaling or standard alignment procedures."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and easy to understand.\n- This paper is, to my knowledge, the first to systematically investigate the impact of fine-tuning \ndata contamination on SLMs at this scale, providing insights for real-world deployment.\n- The authors experimented with full-finetuning instead of PEFT is worth noting."}, "weaknesses": {"value": "- The *irrelevant* dataset was constructed by pairing a question with a randomly selected answer from a different example in the clean dataset. While this tests for question-answer semantic correspondence, the irrelevant answers are still high-quality, well-formed, and grammatically correct responses, merely answers to the wrong questions. This may not fully represent other common types of data contamination, such as ‘garbage’ text, HTML tags, or unparseable noise, which might have a different (and potentially worse) impact on model stability and grammatical correctness than the clean irrelevance tested here.\n\n- Some related works [1,2] regarding how data similarity affects model finetuning would be worth discussing from the data contamination viewpoint in the revision.\n\n[1] Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets\n\n[2] When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment"}, "questions": {"value": "1. The paper shows larger models are better at learning counterfactual instructions. Is this simply a facet of them being \"better learners\" in general (i.e., they would also learn correct instructions from fewer examples)? Or does this suggest a specific trade-off where scaling improves logical instruction-following, which in turn makes models more vulnerable to logically flawed instructions?\n2. Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EO1lNJxcRJ", "forum": "x4vwdjckZ6", "replyto": "x4vwdjckZ6", "signatures": ["ICLR.cc/2026/Conference/Submission24532/Reviewer_2Ayj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24532/Reviewer_2Ayj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991065242, "cdate": 1761991065242, "tmdate": 1762943113267, "mdate": 1762943113267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the robustness of 23 SLMs (270M to 4B parameters) to fine-tuning data contamination. The study used two main categories of corruption: syntactic transformations (character/word reversal), which disrupt the structural form of the answer; and semantic transformations irrelevant/counterfactual responses, which disrupt the content and meaning of the answer.\n\nThis paper reached two main conclusions:\n1. Models exhibit catastrophic vulnerability to syntactic corruption (especially character reversal), where even $25 $ % contamination can cause near-total performance collapse, exposing an architectural weakness.\n2. A \"capability curse\" emerges with semantic corruption, where (larger,) more capable models are conversely more prone to learning harmful instructions, leading to greater accuracy degradation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  This paper is not limited to a single model family but systematically tests 23 Small Language Models (SLMs) from various families, with parameters ranging from 270M to 4B, making its conclusions broadly representative.\n\n2.  This paper points to the conclusion of a \"Capability Curse\", where more capable, larger-parameter models are paradoxically more prone to learning incorrect semantic instructions (For the discussion of the second conclusion, please see the weakness analysis).\n\n3.  The problem investigated by this paper (i.e., the sensitivity of SLMs to fine-tuning data contamination) has strong guiding significance for the deployment of models in real-world environments, as it directly relates to model reliability when data quality is uncontrollable.\n\n4.  This paper adopted a multi-dimensional evaluation framework, integrating metrics such as semantic similarity, accuracy, grammatical correctness, and \"LLM-as-a-judge,\" and ensured the reliability of its results through consistency checks with human evaluators."}, "weaknesses": {"value": "W1： There are some issues with the presentation of the chart results. Such as， Figure 1 lacks labels or captions that clearly explain the exact meanings of the horizontal and vertical axes, making it difficult to understand at the beginning of reading.\nW2: Although the authors claim \"Capability Curse\" is counterintuitive, it seems to be a common intuition that more complex models are less robust to training data contamination. [1] [2][3]\nW3: In the Abstract and the \"Alignment paradox\" section of Section 4 (Discussion), this paper draws a key conclusion: that \"Alignment\" does not guarantee (and may even reduce) the model's robustness to syntactic contamination. However, the weakness of this conclusion lies in its overly narrow definition of \"alignment.\" However, alignment also involves other methods such as RLHF or DPO, which this paper does not explore.\n\nI think the above weaknesses are minor issues. This does not affect my judgment of the score. For the core issue, please refer to the Question section.\n\n**References**\n\n\n[1] Overparameterized Linear Regression Under Adversarial Attacks\n\n[2] Fragile Giants: Understanding Susceptibility of Models to Subpopulation Attacks\n\n[3]Poisoning and Backdooring Multimodal Models"}, "questions": {"value": "Regarding one of the paper's main conclusions, \"that models are extremely sensitive to syntactic contamination (particularly character reversal),\" I have concerns about the soundness of the experimental design.\n\nThe \"character reversal\" operation adopted by the authors (i.e., completely reversing the entire answer string) is an extremely severe form of data corruption. This operation essentially transforms the original data $X_i$ into data with **extremely high noise content**, causing a single data point to retain almost no original syntactic or semantic information.\n\nTherefore, this experiment is less a study of \"model sensitivity to syntactic contamination\" and more a study of \"the impact of mixing different proportions of **pure noise** into the instruction fine-tuning dataset on the model.\"\n\nI raise the following questions:\n\n1.  **Lack of Precedent and Real-world Basis**: Can the authors provide prior work to justify that \"complete reversal\" is a recognized method for syntactic robustness testing? In real-world \"dirty data,\" this type of contamination is highly unlikely. Real-world character-level contamination typically manifests as **typos, misspellings, or random character replacements and deletions**.  Prior work on testing character-level robustness has generally employed milder, more realistic contamination methods. For instance,  [1] used random character **insertion, deletion, and replacement** to simulate misspellings, while  [2] also advocates for testing model behavior by generating random misspellings. These studies typically define a **contamination intensity** (e.g., **10% or 20% of characters in a single data point $X_i$ are perturbed**), rather than 100% reversing the entire data point as done in this paper. The extreme operation adopted by the authors creates a significant gap between this study and prior work, as well as real-world scenarios.\n\n2.  **Doubts about the Conclusion**: I suspect the primary reason for the \"catastrophic performance degradation\" reported is not that the model is sensitive to \"a small amount of data with character contamination,\" but rather that the model is sensitive to **a training dataset containing a small amount of pure noise.** (i.e., the data has been turned into almost pure noise). The paper's conclusion that \"**25% of the data being contaminated**\" causes collapse refers to 25% of the data being 80% noise. This does not prove that a model's performance would also degrade significantly if the training set contained a small amount of data with only *light* character contamination.\n\nI recommend the authors provide additional experiments evidence: If a more standard, milder character-level contamination were applied (**following the practice of prior work [1, 2]**, e.g., setting **10-20% random character replacement per data point**), would the model still exhibit the same \"catastrophic performance degradation\" when **25% of the dataset is contaminated** in this manner? If not, the authors' current conclusion regarding syntactic contamination may need to be substantially revised.\n\nIf the author can provide sufficient and convincing answers to these questions, I would be glad to revise my score.\n\n---\n**References**\n\n[1] Evaluating the Robustness of Neural Language Models to Input Perturbations\n\n\n[2] Beyond Accuracy: Behavioral Testing of NLP Models with CheckList"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DMe8CVnshF", "forum": "x4vwdjckZ6", "replyto": "x4vwdjckZ6", "signatures": ["ICLR.cc/2026/Conference/Submission24532/Reviewer_UPgc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24532/Reviewer_UPgc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762572389716, "cdate": 1762572389716, "tmdate": 1762943113086, "mdate": 1762943113086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}