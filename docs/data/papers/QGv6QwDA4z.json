{"id": "QGv6QwDA4z", "number": 19323, "cdate": 1758295376783, "mdate": 1759897045481, "content": {"title": "Can Vision–Language Models Assess Graphic Design Aesthetics? A Benchmark, Evaluation, and Dataset Perspective.", "abstract": "Assessing the aesthetic quality of graphic design is central to visual communication, yet remains underexplored in vision–language models (VLMs). We investigate whether VLMs can evaluate design aesthetics in ways comparable to humans. Prior work faces three key limitations: benchmarks restricted to narrow principles and coarse evaluation protocols, a lack of systematic VLM comparisons, and limited training data for model improvement. In this work, we introduce AesEval-Bench, a comprehensive benchmark spanning four dimensions, twelve indicators, and three fully quantifiable tasks: aesthetic judgment, region selection, and precise localization. Then, we systematically evaluate proprietary, open-source, and reasoning-augmented VLMs, revealing clear performance gaps against the nuanced demands of aesthetic assessment. Moreover, we construct a training dataset to fine-tune VLMs for this domain, leveraging human-guided VLM labeling to produce task labels at scale and indicator-grounded reasoning to tie abstract indicators to concrete design regions.Together, our work establishes the first systematic framework for aesthetic quality assessment in graphic design.", "tldr": "", "keywords": ["Vision Language Models; Design aesthetics"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a32bfb2b9eddffff3ecd5386dc6ed89a452ce0d0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes and constructs AesEval-Bench, a benchmark for systematically evaluating the ability of vision-language models (VLMs) in graphic design aesthetic assessment. The contributions include: formalizing aesthetic evaluation as a multi-granularity question-answering task involving global judgment, region selection, and precise localization; building a high-quality, multi-dimensional benchmark based on professional design datasets; revealing the limitations of current VLMs through comprehensive experiments; and proposing a data construction method that combines human-guided annotation with indicator-grounded reasoning to generate effective training data for model improvement."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper demonstrates significant novelty in the application of vision-language models (VLMs) by being the first to systematically explore their capability in assessing graphic design aesthetics. Unlike prior work focused on image captioning or general visual question answering, this study introduces AesEval-Bench—a multi-dimensional, multi-task benchmark that categorizes aesthetic evaluation into three granular levels: global judgment, region selection, and precise localization. It covers twelve specific design indicators across four core dimensions (layout, color, font, and graphics), grounded in professional design principles. This structured and fine-grained approach bridges a critical gap in AI support for creative domains, offering both theoretical insights and practical utility for automated design critique systems."}, "weaknesses": {"value": "1. Lack of Highlighting Best/Worst Performances and Missing Comparison with Fine-tuned Models\nThe result tables (e.g., Table 2–5) present raw scores without highlighting the best and second-best performances (e.g., via bold or underline), nor do they indicate the worst or second-worst results. This makes it difficult for readers to quickly identify performance leaders and laggards. More critically, the tables only include existing general-purpose VLMs, but do not report results from the fine-tuned VLMs trained on the proposed AesEval-Train dataset. This omission is a major flaw, as it fails to validate whether the proposed training method actually improves model performance, undermining the paper’s claimed contribution.\n2. Insufficient Discussion on Human Annotation Reliability and Rater Background\nAlthough human annotations are central to constructing the benchmark and training data, the paper does not report inter-annotator agreement metrics (e.g., Cohen’s Kappa, Fleiss’ Kappa, or ICC). It also lacks details on the annotators’ qualifications (e.g., professional designers), the number of raters, or the selection criteria. Given the subjective nature of aesthetic judgment, the absence of such analysis raises concerns about the reliability of the ground-truth labels and the validity of the reported model performance.\n3. Ambiguity in the Definition of \"Problem Regions\" and Lack of Clarity in Problem Injection\nThe paper does not clearly specify how the candidate regions for the region selection and bbox prediction tasks are generated. Are they derived from structured design elements (e.g., text boxes, image frames), or produced via sliding windows, superpixels, or other segmentation methods? Furthermore, while the paper mentions that aesthetic flaws are introduced through human intervention to create negative samples, it fails to describe the concrete procedures for modifying the original designs. It also lacks details on who performed these modifications and according to what guidelines.\n4. Limited Evaluation of Model Generalization\nAll experiments are conducted on the Crello dataset, with no testing on out-of-domain design data. As a result, the reported performance may not generalize to other design contexts or cultural aesthetics, limiting the practical applicability and robustness of the findings."}, "questions": {"value": "1. The authors are advised to improve the table formatting by clearly highlighting the best and second-best results, and to include a comparison of VLMs fine-tuned on AesEval-Train with other baseline models, in order to validate the effectiveness of the proposed training approach.\n2. The authors should add an analysis of inter-annotator agreement (e.g., Cohen’s Kappa or ICC) and provide annotations regarding the annotators’ professional backgrounds, number, and selection criteria, to enhance the credibility and interpretability of the benchmark.\n3. It is recommended that the authors clarify the method for generating \"problem regions,\" including the strategy for candidate region segmentation (e.g., based on design elements or sliding windows) and the detailed process of introducing aesthetic flaws, to improve transparency and reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yvORJv6g7R", "forum": "QGv6QwDA4z", "replyto": "QGv6QwDA4z", "signatures": ["ICLR.cc/2026/Conference/Submission19323/Reviewer_GAuh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19323/Reviewer_GAuh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915645563, "cdate": 1761915645563, "tmdate": 1762931270215, "mdate": 1762931270215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AesEval-Bench, a new benchmark and dataset for evaluating how well vision–language models (VLMs) assess the aesthetic quality of graphic designs. It systematically compares different VLMs and provides fine-tuning data, showing that while current models lag behind human-level aesthetic judgment, human-guided labeling and indicator-grounded reasoning significantly improve performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Introduces a comprehensive benchmark (AesEval-Bench) for aesthetic evaluation.\n2. Provides systematic comparison and fine-tuning data for VLMs.\n3. Shows that human-guided labeling and reasoning improve aesthetic assessment."}, "weaknesses": {"value": "1. Important details are missing. In section 4 TRAINING DATA CONSTRUCTION, if I did not miss anything in Sec.4, authors do not introduce most important details on training data, such as data scale, data source. Without those essential information, we cannot accurately evaluate the dataset and the following datasets. If it is sampled from the same source as the benchmark data, it may have have overfitting issues.\n\n2. Another big concern is, the source data is very limited, sampled from Crello dataset (Yamaguchi, 2021). Single source itself may not be a serious issue because the source dataset could be diverse. But authors fail to provide detailed analysis of the Crello dataset. I am not sure if it can be the qualitied source data. From examples showed in the submission, I feel like the design works are a bit similar to each other in terms of style. I am concerned about it."}, "questions": {"value": "1. Could you provide details on the training data?\n2. Could you provide justifications of using Crello dataset as data source?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b659fWeCJA", "forum": "QGv6QwDA4z", "replyto": "QGv6QwDA4z", "signatures": ["ICLR.cc/2026/Conference/Submission19323/Reviewer_bVZv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19323/Reviewer_bVZv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951834800, "cdate": 1761951834800, "tmdate": 1762931269423, "mdate": 1762931269423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission presents AesEval-Bench (a graphic design aesthetic assessment benchmark with 4 dimensions, 12 indicators, 3 tasks) and AesEval-Train (a training dataset via human-guided VLM labeling and indicator-grounded reasoning), alongside evaluations of 10 VLMs and fine-tuning tests. However, fundamental flaws in benchmark validity, insufficient novelty, unrigorous analysis, and unsupported claims invalidate its contributions, justifying rejection."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses the underexplored gap of graphic design aesthetics assessment, distinct from general image aesthetics benchmarks, with potential utility for designers and generative AI systems.\n2. AesEval-Bench's structure (4 dimensions, 12 indicators, 3 tasks) aligns with established design principles, covering both global (aesthetic judgment) and local (region selection, precise localization) aesthetic evaluations.\n3. The AesEval-Train pipeline offers practical solutions to scale domain-specific annotation, particularly via human-guided VLM labeling and indicator-grounded reasoning."}, "weaknesses": {"value": "1. Benchmark Data Lacks Authenticity: Flawed examples rely on synthetic perturbations (e.g., element repositioning, font alteration) of professional Crello designs. These artificial changes do not replicate real-world aesthetic defects, reducing the benchmark to testing sensitivity to synthetic manipulations rather than authentic aesthetic judgment.\n2. Insufficient Novelty: Core dimensions/indicators derive from existing design literature without novel taxonomical insights; tasks are standard in visual assessment; indicator-grounded reasoning is an incremental adaptation of existing grounded visual reasoning, not a transformative innovation.\n3. Unrigorous Evaluation: Claims about reasoning-augmented VLMs' poor performance lack controlled experiments (e.g., ignoring prompt/architecture differences); key design-focused baselines are omitted; performance metrics lack statistical significance tests, making model comparisons unreliable.\n4. Unsupported Fine-Tuning Claims: Ablation studies fail to rule out trivial factors (e.g., context length) for performance gains; the fine-tuned model's extremely low precise localization performance undermines claims of \"effective supervision\".\n5. Critical Limitations Unaddressed: Omits key design types (e.g., infographics) and subjective aesthetic aspects (e.g., creativity); no inter-annotator agreement data for human labels, compromising ground truth reliability; indicators are not disentangled, causing ambiguous evaluation signals."}, "questions": {"value": "1. How do you verify that synthetic perturbations align with real-world aesthetic flaws? Please provide data on human experts' recognition of the intended flaws in perturbed designs.\n2. What task-specific prompting strategies were used for reasoning-augmented VLMs, and how were they optimized for design aesthetics?\n3. How do you rule out overfitting to synthetic data for the fine-tuned model's performance? Please provide cross-validation on real-world flawed designs.\n4. How do you ensure indicator independence? Please provide correlation analysis of model performance across indicators."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jsgFUzalfv", "forum": "QGv6QwDA4z", "replyto": "QGv6QwDA4z", "signatures": ["ICLR.cc/2026/Conference/Submission19323/Reviewer_vL31"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19323/Reviewer_vL31"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982002636, "cdate": 1761982002636, "tmdate": 1762931268556, "mdate": 1762931268556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AesEval-Bench, a comprehensive benchmark for assessing graphic design aesthetics using vision–language models (VLMs). It covers four dimensions (typography, layout, color, graphics), twelve indicators, and three quantifiable tasks (aesthetic judgment, region selection, precise localization). The authors systematically evaluate a range of VLMs and propose a novel dataset construction pipeline using human-guided VLM labeling and indicator-grounded reasoning, showing that fine-tuning with this data improves performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Comprehensive Benchmark: AesEval-Bench advances the field by covering a wide range of design principles and providing quantifiable tasks.\nSystematic Model Evaluation: The paper compares proprietary, open-source, and reasoning-augmented VLMs, highlighting current limitations.\nInnovative Dataset Creation: The human-guided VLM labeling and indicator-grounded reasoning approach is scalable and interpretable.\nClear Experimental Protocols: Evaluation metrics and ablation studies are well described.\nPerformance Gains: Fine-tuning with the new dataset yields substantial improvements, even for smaller models.\nActionable Insights: The analysis of input components (image, explanation, metadata) is useful for future research."}, "weaknesses": {"value": "Binary Annotation Limitation: The evaluation dimensions are annotated in a binary fashion (yes/no), reducing the complexity of the task to simple classification. This raises the question of whether large VLMs/LLMs are necessary, as simpler models or rule-based systems might suffice.\nOverkill of VLM/LLM for Binary Tasks: The paper does not convincingly justify the use of large VLMs/LLMs for benchmarking binary data. There is little discussion on the trade-off between model complexity and task requirements, and no baseline comparison with lightweight models.\nDataset Creation vs. Model Utility: While the dataset creation pipeline is interesting, its practical value is unclear. The fine-tuning experiments are performed only on open-source LLMs, which are shown to be suboptimal in the benchmarking. The real utility of the dataset would be demonstrated by showing improvements in strong proprietary models (e.g., GPT-5) when fine-tuned with this data.\nUnclear Value for State-of-the-Art Models: The benchmarking shows that models like GPT-5 already outperform open-source models out-of-the-box. However, the paper does not test whether a competitive variant of GPT-5 (via in-context learning may be) with the new dataset leads to meaningful gains. Without this, it’s unclear if the dataset provides real value beyond what top models can already achieve.\nMissing Baseline Comparisons: There is no comparison with traditional computer vision models or simple classifiers trained on the same binary labels, making it difficult to assess whether the proposed approach is truly necessary or effective.\nIndicator Entanglement: The twelve indicators are not fully disentangled, which could affect the interpretability and granularity of the results.\nSubjectivity: Highly subjective aspects of design, such as creativity, are not addressed."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kEM3Zm1EWS", "forum": "QGv6QwDA4z", "replyto": "QGv6QwDA4z", "signatures": ["ICLR.cc/2026/Conference/Submission19323/Reviewer_8akP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19323/Reviewer_8akP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994686379, "cdate": 1761994686379, "tmdate": 1762931268217, "mdate": 1762931268217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}