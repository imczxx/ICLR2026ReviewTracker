{"id": "c461FB8VKD", "number": 9455, "cdate": 1758123112171, "mdate": 1763752244286, "content": {"title": "From Memory to Reasoning: Generative Models Enable Explainable Continual Learning", "abstract": "Class Incremental Learning (CIL) aims to enable models to acquire new knowledge over time without forgetting previous tasks. However, existing CIL approaches primarily rely on discriminative modeling, making them prone to catastrophic forgetting due to parameter expansion and lacking transparency in the prediction process, which limits their reliability in real-world settings. In contrast, human continuous learning is inherently generative and interpretable. Humans integrate new concepts by focusing on salient visual details and linking them to prior semantic structures, forming traceable chains of reasoning. Therefore, we propose the Generative Explainable Class-Incremental Learning (GECL), a pioneering generative and interpretable CIL framework designed to address these challenges. GECL employs soft-label-guided visual augmentation to focus attention on the most discriminative image regions and utilizes large language models (LLMs) to construct structured semantic attributes. This approach eliminates the need for expanding classification heads, preventing parameter overwriting and preserving previous knowledge. These semantic attributes achieve fine-grained alignment with visual features through entropy-regularized optimal distribution matching, where a cost matrix explicitly quantifies each attribute-region contribution, generating transparent attribute-region reasoning chains. Experiments across natural scenes and fine-grained datasets demonstrate that GECL balances high accuracy, low forgetting rates, and interpretability, marking a promising step toward safe and reliable continuous learning.", "tldr": "", "keywords": ["Class Incremental Learning", "Intepretable Classification", "Multi-model", "Generative Methods"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7f6d5194bba16d50926313b02af49c628a4b6ea6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Generative Explainable Class-Incremental Learning (GECL), a generative and explainable framework for Class-Incremental Learning (CIL/FSCIL). Key contributions include:\n\n1. Soft-label guided visual enhancement to highlight discriminative regions and suppress backgrounds (Sec. 3.2).\n\n2. Generation of structured semantic attributes using large language models (LLMs), which are encoded into semantic embeddings via a frozen CLIP text encoder (Sec. 3.3).\n\n3. Formulation of region-to-attribute alignment as an entropy-regularized optimal transport (OT) problem solved by Sinkhorn scaling, producing coupling matrix T*, which is leveraged for interpretable \"region × attribute\" reasoning during inference (Eq. 10–14; Fig. 2)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Proposes a generative, interpretable incremental learning approach using LLM-generated semantic attributes, thus preventing head expansion.\n\n2. Provides clear interpretability: OT coupling matrices explicitly match \"region-to-attribute,\" with insightful examples illustrating \"right for the wrong reasons\" (Fig. 3, p. 9).\n\n3. Comprehensive ablation studies examining sensitivity to hyperparameters $\\lambda$, sampling weight $\\alpha$, and number of attributes k, providing practical insights (Fig. 4, pp. 17–18).\n\n4. Well-defined formalization of the entropy-regularized OT problem, including clear derivations and convergence explanations (Eq. 10–13; Appendix B)."}, "weaknesses": {"value": "1. Limited novelty in theoretical contributions: entropy-regularized OT for cross-modal matching is established; using it directly for final decision-making and interpretability is innovative but under-discussed. The theoretical content primarily reiterates known results (Appendix B) and lacks comprehensive comparison and justification against other matching/alignment paradigms. Essential OT literature, such as https://arxiv.org/abs/1306.0895, is not cited.\n\n2. Insufficient quantitative evaluation of interpretability: Despite emphasizing joint improvement in accuracy, forgetting, and interpretability, the paper lacks objective metrics (deletion/insertion curves, AOPC, Sufficiency/Necessity, Pointing Game, TCAV/ACE, or CUB attribute alignment accuracy). Current reliance on visualization alone (Fig. 3) does not strongly substantiate the interpretability claims.\n\n3. Robustness concerns: Reliance on semantic quality generated by LLMs introduces potential hallucination risks, acknowledged by authors in limitations.\n\n4. Potential fairness issue: Class-name prompts in LLM might introduce prior knowledge leakage, especially in fine-grained datasets. Comparisons between \"class-name only\" and \"masked-class/visual context only\" prompts would better validate dependency on strong label priors.\n\n5. Computational and memory complexity concerns: The inference requires matching each input against attributes from all classes via OT (regions × attributes × classes × iterations), potentially problematic in long-sequence scenarios.\n\n6. Lack of knowledge-base governance strategies: No explicit mechanisms or evaluations provided for redundancy removal, conflict detection, attribute distillation, or mitigation of error accumulation and knowledge drift in incremental knowledge bases.\n\n7. Minor formatting and typographical errors, such as bracket mismatches (e.g., line 359 \"Entropy-reg(Liu et al., 2022a))\") and spacing (e.g., line 54 \"Cao et al. (2024)proposed\")."}, "questions": {"value": "1. How are attribute conflicts and redundancies detected and resolved in the knowledge base? Is there an implemented or evaluated attribute distillation or merging method?\n\n2. What strategies are proposed to mitigate robustness and fairness issues introduced by LLM-generated semantics, especially concerning hallucination risks in large-scale data scenarios?\n\n3. How does the framework handle conflicts, error accumulation, and knowledge drift inherent in incremental learning?\n\n4. Could standard incremental metrics (Forgetting, BWT, AUC across tasks) be included besides PD, and empirical results provided to justify whether learning minimal parameters effectively prevents catastrophic forgetting compared to frozen backbones or varied adapter capacities?\n\n5. Regarding interpretability faithfulness, would masking or replacing matched regions and observing prediction confidence degradation serve as a valid quantitative evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7y6r8T55Ob", "forum": "c461FB8VKD", "replyto": "c461FB8VKD", "signatures": ["ICLR.cc/2026/Conference/Submission9455/Reviewer_xvnP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9455/Reviewer_xvnP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747837791, "cdate": 1761747837791, "tmdate": 1762921048613, "mdate": 1762921048613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work aims to develop a novel Continual Learning methods called GECL which combines two worlds: generative modeling and explainable AI. Main experiments are conducted on CUB200, TinyImageNet and mini-ImageNet. Motivation is taken from how human learn new knowledge in novel tasks and how do they accommodate those information. LLMs/VLMs are used to generate semantic meaning of the features. Ablations on the method are provided. GECL itself generate foreground mask, then it does hierarchical sampling, and then there is encoder and trainable adapter which provides visual features which are then matched with a database features remembered from previous tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea is novel and interesting. \n\nThe problem to tackle is important, and inspiration from human behavior is a plus of this method. \n\nExperiments are thorough and convincing. \n\nAblations are present. \n\nWriting is clear and images readable."}, "weaknesses": {"value": "The contextualization of the work is properly done only from one angle, Class Incremental Learning, omitting a baseline work that combines explainability and CIL, namely ICICLE [1], this comparison is vital for this work. \n\nThe relation to generative modeling is not well described. I am not sure if usage of LLMs guarantees generative modeling in this case. I would expect to provide theoretical background on this process and discuss this with other generative classifiers such as [2]. \n\nAs this work also introduces novel explainability method, some evaluation of this would be beneficial. Either a user study or usage of FunnyBirds framework [3].\n\n[1] Rymarczyk, Dawid, et al. \"Icicle: Interpretable class incremental continual learning.\" Proceedings of the IEEE/CVF international conference on computer vision. 2023.\n\n[2] Mackowiak, Radek, et al. \"Generative classifiers as a basis for trustworthy image classification.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n[3] Hesse, Robin, Simone Schaub-Meyer, and Stefan Roth. \"Funnybirds: A synthetic vision dataset for a part-based analysis of explainable ai methods.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023."}, "questions": {"value": "Can you add ICICLE baseline to your results? \n\nCan you provide more details on generative aspect of your work and how does it correspond? Currently I am not convinced. \n\nIs it possible to provide evaluation of explanation? As this method is not basing CIL scenario on acclaimed XAI method as in [1], such evaluation would be important to understand if this type of explanations provide the users ability to understand them. \n\nAddressing comparison to [1] and providing evaluation of explanation would make me change my score to 6, and providing more details on the generation aspect of this methodology will result in score 8."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wPSvEfKl1g", "forum": "c461FB8VKD", "replyto": "c461FB8VKD", "signatures": ["ICLR.cc/2026/Conference/Submission9455/Reviewer_J8a8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9455/Reviewer_J8a8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860977897, "cdate": 1761860977897, "tmdate": 1762921048272, "mdate": 1762921048272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles two key issues in Class-Incremental Learning (CIL): catastrophic forgetting of old knowledge and opaque decision-making, which existing discriminative or LLM-driven generative CIL methods fail to fully solve . To address these, it proposes the Generative Explainable Class-Incremental Learning (GECL) framework, consisting of three core modules: a soft-label-guided visual enhancement module (highlights discriminative regions), a generative visual-semantic module (uses LLMs to generate structured semantic attributes for new classes, avoiding classification head expansion), and an entropy-regularized distribution matching module (establishes visual-semantic correspondences for transparent reasoning chains) . GECL optimizes a distribution-matching objective to prevent parameter overwriting, and experiments on Tiny-ImageNet (traditional CIL), CUB200, and mini-ImageNet (few-shot CIL) show it outperforms SOTA methods in balancing accuracy, forgetting resistance, and interpretability . Its contributions include: introducing GECL for explainable CIL, designing a pipeline for human-like reasoning, and validating its superiority via extensive experiments ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "# Strength 1: Dual Core CIL Pain Points Addressed Simultaneously  \nThis study specifically resolves the two long-standing core issues in Class-Incremental Learning (CIL): \"catastrophic forgetting\" and \"opaque decision-making,\" which are rarely tackled together by existing methods. Unlike discriminative approaches (e.g., CODA-Prompt) that focus on mitigating forgetting but remain \"black boxes,\" or early generative methods (e.g., GMM) that lack structured semantics, GECL eliminates the expansion of classification heads to avoid parameter overwriting, fundamentally reducing forgetting. Meanwhile, it generates quantitative \"visual region-semantic attribute\" reasoning chains (e.g., \"Region 4 matches 'thick white fur' with a weight of 0.018\") through entropy-regularized distribution matching, making the decision-making process traceable. This fills the gap where existing methods struggle to balance both aspects, which is crucial for high-stakes real-world applications requiring both accuracy and interpretability .  \n\n\n# Strength 2: Lightweight Design with Strong Compatibility  \nGECL adopts a lightweight and compatible architecture that avoids heavy parameter updates, a key advantage over resource-intensive CIL methods. It leverages a **frozen CLIP visual/text encoder** (ViT-B/32) to extract features, requiring only training of a lightweight adapter module for domain adaptation instead of fine-tuning large pre-trained models—this significantly reduces computational costs and memory overhead (experiments run on two NVIDIA 3090 GPUs, common in academic settings). Additionally, the framework is not tied to specific large language models (LLMs): it uses MiniGPT-4 for semantic attribute generation but can integrate other LLMs with minimal adjustments. This design not only lowers the barrier for practical deployment but also ensures compatibility with mainstream pre-trained models, distinguishing it from methods that rely on custom, hard-to-replicate architectures .  \n\n\n# Strength 3: Comprehensive and Rigorous Experimental Validation  \nThe research fully verifies GECL’s effectiveness through multi-scenario and multi-dimensional experiments based on the target dataset settings. It covers both traditional CIL (on Tiny-ImageNet, with 200 classes split into 5/10/20 incremental tasks) and few-shot CIL (on CUB200, 200 fine-grained bird classes; mini-ImageNet, 100 classes with 40 incremental classes), outperforming state-of-the-art methods in long-sequence tasks (e.g., 20 tasks on Tiny-ImageNet) where forgetting is more severe. Moreover, ablation studies confirm the necessity of key modules (e.g., adding the distribution matching module boosts Tiny-ImageNet accuracy from 54.34% to 75.96%), and visualizations demonstrate GECL avoids \"correct predictions for wrong reasons\"—forming a complete evidence chain to ensure conclusion reliability ."}, "weaknesses": {"value": "# Weakness 1: Limited Scenario Generalization and High Architectural Complexity  \nGECL is explicitly designed for **class-incremental image classification tasks** and lacks adaptability to other continual learning (CL) scenarios—especially non-visual or task-agnostic CL (e.g., LLM continual learning or unclassified visual tasks). The framework’s core modules are tightly coupled with visual data characteristics: the soft-label-guided visual enhancement module relies on image-specific operations (e.g., Sobel operator for edge detection, adaptive histogram equalization; ), and the distribution matching module is optimized for \"visual region-semantic attribute\" alignment, which is irrelevant to text-only or multimodal (non-visual) CL tasks. Additionally, the framework’s complexity—with three interdependent core modules (visual enhancement, generative semantic modeling, entropy-regularized distribution matching) and iterative optimization of coupling matrices—creates high barriers for migration. For LLM continual learning (which focuses on knowledge retention during parameter updates or task sequence expansion), GECL’s visual-centric components (e.g., frozen CLIP visual encoder; ) are redundant, and its semantic attribute generation logic (tailored for image class descriptions) cannot be directly reused, making it impractical to adapt to non-classification CL scenarios.  \n\n# Weakness 2: Lack of Comparison with Latest CIL Methods (e.g., InfoLora[1])  \nThe paper’s experimental comparisons are incomplete, as it fails to benchmark against **recently proposed CIL methods** (e.g., InfoLora) — casting doubt on whether GECL truly outperforms state-of-the-art discriminative methods. The study only compares GECL with methods published up to 2024 (e.g., GMM (Cao et al., 2024), CODA-Prompt (Smith et al., 2023b); ) and does not include newer discriminative CIL approaches like InfoLora (a method that leverages low-rank adaptation to enable efficient continual learning while preserving pre-trained knowledge). InfoLora, as a representative of updated discriminative methods, is designed to mitigate forgetting with lightweight parameter updates (similar to GECL’s adapter but optimized for pre-trained model fine-tuning), and its performance in long-sequence CIL tasks is widely recognized. Without comparing GECL with such latest methods, the paper cannot fully validate its claim of \"outperforming discriminative learning methods\"—it remains unclear whether GECL’s advantages over older methods (e.g., CODA-Prompt) can be maintained when facing more advanced discriminative alternatives. This gap weakens the persuasiveness of GECL’s performance superiority.\n\nReference:\n[1] InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning"}, "questions": {"value": "See my weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2a30jBadSY", "forum": "c461FB8VKD", "replyto": "c461FB8VKD", "signatures": ["ICLR.cc/2026/Conference/Submission9455/Reviewer_FvnW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9455/Reviewer_FvnW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873449485, "cdate": 1761873449485, "tmdate": 1762921047871, "mdate": 1762921047871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents GECL (Generative Explainable Class-Incremental Learning), a novel framework that integrates generative semantic modelling with interpretable reasoning for continual learning. Unlike conventional discriminative methods, GECL combines a soft-label-guided visual enhancement module, a generative visual–semantic modelling process (using large language models to construct structured semantic attributes), and an entropy-regularised optimal transport formulation for distribution matching between visual regions and semantic attributes. The proposed framework aims to achieve both knowledge retention and interpretability by generating explicit region–attribute reasoning chains for each prediction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths\n1. The integration of generative semantic modelling and interpretable reasoning into the continual learning paradigm is a distinctive and timely idea. It bridges the gap between performance-oriented and explainability-oriented approaches.\n2. The analogy to human reasoning — linking new visual details to structured prior semantic attributes — provides an intuitive conceptual foundation that enhances readability and motivation.\n3. The paper provides a well-structured framework, detailing each module (visual enhancement, semantic generation, and interpretable matching) with mathematical precision and algorithmic clarity.\n4.  Experiments are conducted across diverse datasets (both general and fine-grained), with comparisons against a wide range of baselines, including discriminative, generative, and prompt-based models.\n5. The explicit reasoning chain visualisations offer a tangible advantage over most continual learning methods, which typically lack transparency.\n6.  The manuscript is well-organised and written in clear academic English, with appropriate use of figures and tables to convey key insights."}, "weaknesses": {"value": "Weaknesses\n1. While the framework is well-motivated, its theoretical grounding remains limited. The paper would benefit from a deeper analysis of why entropy-regularised optimal transport provides a meaningful proxy for reasoning, beyond its mathematical formulation.\n2.  The method’s reliance on LLM-generated semantic attributes introduces significant variability. The quality, consistency, and domain relevance of these attributes are not systematically analysed or benchmarked.\n3.  Although visual reasoning chains are shown, there is no quantitative or human-evaluated metric assessing interpretability. Claims about “explainability” remain primarily qualitative.\n4. Some baselines (e.g., DualPrompt, CODA-Prompt) benefit from large-scale pretraining, but others could be tuned or extended for fairer comparison under similar constraints. More discussion on this issue is needed.\n5.  The computational overhead of the distribution matching (Sinkhorn iterations) and LLM-based attribute generation is not sufficiently addressed. Real-time or large-scale deployment may be challenging.\n6. Although the authors claim to provide code, key implementation parameters (e.g., prompt templates, attribute filtering strategy, and convergence settings for optimal transport) should be more clearly documented in the main paper rather than deferred to the appendix.\n7. Conceptually, the work draws upon existing generative incremental learning and CLIP-based frameworks. The unique contribution of GECL lies in combining these ideas under an interpretable matching perspective, but the originality relative to recent generative–semantic continual learning approaches could be made more explicit."}, "questions": {"value": "plz see my detailed commments above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7sfSlEwymR", "forum": "c461FB8VKD", "replyto": "c461FB8VKD", "signatures": ["ICLR.cc/2026/Conference/Submission9455/Reviewer_zNcY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9455/Reviewer_zNcY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964256062, "cdate": 1761964256062, "tmdate": 1762921047561, "mdate": 1762921047561, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}