{"id": "Mz98kwANpF", "number": 9340, "cdate": 1758119506204, "mdate": 1759897730544, "content": {"title": "Align, Don’t Divide: Revisiting the LoRA Architecture in Multi-Task Learning", "abstract": "Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large Language Models (LLMs). In practice, LLMs are often required to handle a diverse set of tasks from multiple domains, a scenario naturally addressed by multi-task learning (MTL). Within this MTL context, a prevailing trend involves LoRA variants with multiple adapters or heads, which advocate for structural diversity to capture task-specific knowledge. Our findings present a direct challenge to this paradigm. We first show that a simplified multi-head architecture with high inter-head similarity substantially outperforms complex multi-adapter and multi-head systems. This leads us to question the multi-component paradigm itself, and we further demonstrate that a standard single-adapter LoRA, with a sufficiently increased rank, also achieves highly competitive performance. These results lead us to a new hypothesis: effective MTL generalization hinges on learning robust shared representations, not isolating task-specific features. To validate this, we propose Align-LoRA, which incorporates an explicit loss to align task representations within the shared adapter space. Experiments confirm that Align-LoRA significantly surpasses all baselines, establishing a simpler yet more effective paradigm for adapting LLMs to multiple tasks. The code is available anonymously.", "tldr": "", "keywords": ["Large Language Models", "Low-rank Adaptation", "Multi-Task Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d42936ddda081978a4db7c2178bbb67250e9b604.pdf", "supplementary_material": "/attachment/8118b142cb0c25f073ffb2e4c4112f98a05c5413.zip"}, "replies": [{"content": {"summary": {"value": "This work revisits multi-task parameter-efficient fine-tuning (PEFT) from a representation-sharing perspective.\nThe paper challenges the common belief that multi-task generalization requires task-specific modularization (e.g., multiple Lora heads or adapters) and instead argues that learning a shared aligned representation is the key to improve multi-task generalization. Based on the empirical observations, the author propose the method Align-LoRA, which keeps a single high-rank LoRA adapter and introduces an additional representation alignment loss to encourage consistency among task representations. In the paper, the alignment loss is implemented via either KL divergence or MMD. Experimental results on LLaMA2/3 and Qwen2.5 models show that Align-LoRA achieves superior performance compared to multi-head baselines, while remaining simpler and efficient.\n\nThis paper offers an interesting perspective on multi-task LoRA design and provides clear empirical evidence supporting the potential benefits of representation alignment. The experiments are carefully executed and the analysis is generally convincing. However, the validity of the conclusions appears to hold only within a narrow setting (e.g., moderate model scales, limited rank ranges, and homogeneous reasoning tasks). The contribution, while insightful, is confined in scope and lacks general applicability"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper revisits PEFT under the multi-task setting and delivers a clear message that shared representations can substitute complex multi-head structures for multi-task learning. Clear empirical evidence is provided in the paper.\n- Across multiple reasoning and QA benchmarks, the proposed method consistently improves over prior lora variants while remaining efficiency.\n- The proposed alignment loss introduces no inference overhead and can be easily integrated into existing lora pipelines.\n- The paper is well organized and analyses are meaningful and clear."}, "weaknesses": {"value": "- The main finding of this paper is conceptually overstated. The paper treats the number of lora heads as a proxy for task separation, but in the evaluated R-LoRA and M-LoRA settings, heads do not correspond directly to tasks. Each head merely represents a low-rank subspace, and all tasks share the same set of heads with different weightings. Consequently, the claim that “increasing the number of heads brings no benefit” only reflects redundancy among feature components, not a true analysis of task-specific decomposition. The conclusion that “multi-component architectures are unnecessary” is therefore conceptually narrower than stated. It applies only to representation-factorized designs, not to task-wise multi-adapter frameworks.\n- The comparison between high-rank lora and multi-component variants is not thorough. The paper increases the rank of the single-lora (r=8–10) but keeps the per-head rank of R-LoRA and M-LoRA fixed (r=4). Since rank directly governs representational capacity, the observed improvement may stem from capacity scaling rather than architectural superiority. A fair comparison would require per-head rank tuning.\n- It has been well explored that LoRA performance is non-monotonic with rank that high rank often causes optimization instability or representation collapse. Align-LoRA implicitly assumes “higher rank = better performance,” an assumption valid only in a narrow range (r≤16). This limits the method’s stability and generality in larger-scale or higher-rank settings.\n- The paper infers that since a single high-rank adapter performs well, task representations should be thus aligned in a shared subspace.\nThis conclusion is heuristic and not logically necessary since multi-task performance could also result from implicit subspace partitioning.\nThe alignment hypothesis lacks supporting evidence such as gradient similarity or representation visualization.\n- Recent multi-task PEFT works (e.g., MixDA 2023, MixLoRA 2024, PEMT 2024) explicitly combine shared information and task-specific components and achieve stronger results. The proposed Align-LoRA enforces full sharing and ignores task-specific modeling, making its conclusions less general and its novelty limited relative to the current research trajectory.\n- In the paper, all training and evaluation tasks belong to the reasoning and QA domain. There are no experiments on heterogeneous tasks such as summarization, translation, or code generation, where task objectives and output distributions differ substantially.\nIt remains unclear whether the observed benefits of representation alignment hold under diverse or heterogeneous task settings."}, "questions": {"value": "- Did the authors tune the per-head rank for multi-component lora variants (e.g., R-lora and M-lora)?\n- How does the proposed framework perform on larger models (30~70B)?\n- Would aligning only certain shared components (rather than all) yield a better shared–specific trade-off?\n- Please address the key issues raised in the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G157iFTLfF", "forum": "Mz98kwANpF", "replyto": "Mz98kwANpF", "signatures": ["ICLR.cc/2026/Conference/Submission9340/Reviewer_BDgW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9340/Reviewer_BDgW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761300760785, "cdate": 1761300760785, "tmdate": 1762920972263, "mdate": 1762920972263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the prevailing multi-component LoRA paradigm for multi-task learning (MTL), arguing that architectural isolation of task-specific features is unnecessary. The authors demonstrate that a simplified multi-head LoRA (M-LoRA) with high head similarity outperforms complex variants, and that simply increasing the rank of a standard single-adapter LoRA achieves competitive performance. They propose a new hypothesis that learning task-shared representations is more effective, and introduce Align-LoRA, which uses an alignment loss (KL divergence or MMD) to explicitly encourage shared representations in the low-rank space."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "## strength\n\n1. The paper provides strong empirical evidence against the widely accepted multi-component LoRA paradigm, showing that simpler, shared representations can outperform complex, diversity-focused architectures.\n\n2. Align-LoRA introduces a novel alignment loss to explicitly encourage task-shared representations without adding inference overhead. The method is simple, effective, and retains the mergeability of LoRA, making it highly practical."}, "weaknesses": {"value": "## weakness\n1. The paper is not well-organized. It is difficult to follow the paper.\n2. The related work section can be divided into the preliminaries and related work. \n3. It is better to put the picture of the method in the main text instead of the appendix. \n4. The alignment loss introduces a new hyperparameter λ, and while a sensitivity analysis is provided, the paper does not offer clear guidelines for setting \nλ  in practice, which may hinder adoption."}, "questions": {"value": "1. Could you provide more intuition or theoretical insight into why high head similarity in M-LoRA leads to better multi-task generalization, especially given the common belief that diversity helps capture task-specific knowledge?\n2. Have you experimented with aligning representations in the up-projection matrix B or other layers? If so, what were the results? If not, why was the focus limited to the down-projection A?\n3. How does Align-LoRA perform in extreme multi-task scenarios with highly dissimilar tasks? Is the alignment strategy still beneficial when tasks have little in common?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "L99PE6j6oW", "forum": "Mz98kwANpF", "replyto": "Mz98kwANpF", "signatures": ["ICLR.cc/2026/Conference/Submission9340/Reviewer_UfuN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9340/Reviewer_UfuN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722670842, "cdate": 1761722670842, "tmdate": 1762920971739, "mdate": 1762920971739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper re-examines the role of structural diversity in multi-task PEFT for LLMs. The authors challenge the prevailing assumption that multi-adapter or multi-head LoRA architectures must isolate task-specific knowledge to achieve good multi-task generalization. they observe that simplified multi-head architectures (M-LoRA) with high inter-head similarity outperform more complex diversity-driven designs (e.g., R-LoRA). Building on this finding, they propose Align-LoRA, which introduces an explicit representation alignment loss (based on symmetric KL divergence) to encourage task-shared representation learning within a single LoRA adapter. Align-LoRA achieves good performance on multi-task benchmarks (e.g., BBH, multi-task reasoning) while maintaining zero inference overhead and fewer trainable parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The M-LoRA with high head similarity can outperform complex variants could influence the next generation of efficient fine-tuning strategies for LLMs. The proposed method also offers zero inference latency and fewer parameters, making it appealing for real-world settings.\n\n2. The paper provides thorough experimental validation, including comparisons across several recent LoRA variants on multiple model sizes and datasets. The ablation studies are convincing and well controlled. The proposed KL/MMD-based alignment loss is theoretically motivated.\n\n3. The paper is well written and logically structured. The argumentation from the paradox of diversity to the introduction of Align-LoRA is coherent. Mathematical formulations are clearly presented and self-contained.\n\n4. Experiments confirm that Align-LoRA significantly surpasses baselines, establishing a simpler yet more effective multi-task paradigm for LLMs."}, "weaknesses": {"value": "1. Lack of discussion on closely related prior work. The innovation of Align-LoRA lies in aligning the representation distributions across tasks. However, this idea was first proposed in VIP-MTL [1], which minimizes the distance between task-wise representation distributions to achieve impartial learning across tasks. Specifically, VIP-MTL constrains the mean and variance of pre-defined task distributions to align them within a unified probabilistic space, which conceptually overlaps with the total alignment loss (Eq. 5) in the present paper. The main difference is that Align-LoRA adopts a LoRA-based architecture and implements the alignment loss via pair-wise KL loss, while VIP-MTL achieves the same goal by mapping all task distributions to a common distribution scale (which arguably provides a more stable optimization objective). The paper does not cite or discuss this closely related work, which weakens the claimed novelty. A discussion clarifying the conceptual and methodological distinctions would significantly improve the paper’s contribution clarity.\n- [1] Impartial Multi-task Representation Learning via Variance-invariant Probabilistic Decoding. ACL 2025.\n\n2. While the empirical evidence strongly supports the hypothesis that representation alignment benefits multi-task learning, the paper lacks deeper theoretical analysis or information-theoretic justification for why such alignment improves generalization. Future version could formalize the connection between alignment and shared information sufficiency.\n\n3. Although both KL and MK-MMD variants are tested, the differences between them are not analyzed in depth. Understanding when one alignment metric performs better could make the approach more interpretable and generalizable.\n\n4. Most experiments focus on instruction-tuned LLMs (e.g., Qwen, LLaMA) under English NLP tasks. It remains unclear whether the observed benefits generalize to multilingual, multimodal, or non-text domains. Including a broader task spectrum or more diverse data distributions would further strengthen the claim of universality."}, "questions": {"value": "1. How sensitive is Align-LoRA to the task imbalance or heterogeneity among tasks? For instance, would alignment still help if tasks have highly divergent label spaces or modalities?\n\n2. Can the alignment loss potentially over-align tasks, leading to negative transfer or underfitting of domain-specific nuances?\n\n3. What are the theoretical intuitions behind why aligning task distributions in the low-rank subspace enhances generalization? Can this be linked to information bottleneck or mutual information preservation principles?\n\n4. How does Align-LoRA perform when combined with other recent PEFT advances, such as dynamic rank allocation (AdaLoRA) or direction-based decomposition (DoRA)?\n\n5. Can the authors share qualitative insights (e.g., t-SNE visualizations, mutual information metrics) to further support the claim that shared representations are better aligned across tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RkrldotJgn", "forum": "Mz98kwANpF", "replyto": "Mz98kwANpF", "signatures": ["ICLR.cc/2026/Conference/Submission9340/Reviewer_xs9V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9340/Reviewer_xs9V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050997128, "cdate": 1762050997128, "tmdate": 1762920971278, "mdate": 1762920971278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that, in multi-task learning, aligning task representations within a single (often high-rank) LoRA is more effective than dividing capacity across multiple routed adapters. As a consequence, it proposes Align-LoRA, which adds KL/MMD alignment in the LoRA A-space to promote a shared subspace, preserving mergeability and incurring zero routing overhead at inference. Across diverse tasks and base models, Align-LoRA consistently outperforms multi-component baselines, suggesting alignment-driven sharing improves generalization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper is clear and generally well-written. The core question of multi-component LoRA designs and pivoting to shared representation alignment is easy to follow. \n\nS2. Figures/Tables (e.g., inter-head similarity distribution and the BBH/eight-task results) are informative and support the claims.\n\nS3. The empirical comparisons are comprehensive and cover M-LoRA, HydraLoRA, R-LoRA. Also compared are single-adapter high-rank LoRA vs. multi-component baselines, which is great to see."}, "weaknesses": {"value": "W1. Insufficient mechanistic depth on “why” alignment helps. The paper convincingly shows that M-LoRA/high-rank LoRA/Align-LoRA work, but the internal mechanics remain under-probed. Beyond end metrics, please add targeted analyses that connect LoRA internals to generalization. For example, 1) Loss-component ablations. Currently, there is not much information regarding how losses contribute to fine-tuning. 2) Some mechanistic evidence that connects the LoRA’s internal structure to generalization would better convey the point. \n\n(minor) A suggestion would be to report the behavior of the spectral geometry of updates at each step. See below.\n\nW2. Impact of training on LoRA architecture: Right now, we only see the end result (accuracy numbers), but not how the adapters take shape while training. A simple set of training-time views will show when and where the adapters activate across layers, whether certain modules (Q/K/V vs. MLP) consistently carry the load, and if capacity (the chosen rank) actually gets used or quietly collapses to a few directions. \n\nW3. Compute & memory for LoRA is missing: The authors are requested to provide more analysis on parameter counts per layer/module and the total Q, K, V, out-proj, and MLP parameters. How is the training dynamics of the proposed LoRA? Can the authors comment on the wall-clock time, FLOPs, etc. as compared to standard architectures? Without this critical analysis, the paper is lacking in rigor."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IJTDkUA4re", "forum": "Mz98kwANpF", "replyto": "Mz98kwANpF", "signatures": ["ICLR.cc/2026/Conference/Submission9340/Reviewer_Ua6q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9340/Reviewer_Ua6q"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762911092023, "cdate": 1762911092023, "tmdate": 1762920970917, "mdate": 1762920970917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}