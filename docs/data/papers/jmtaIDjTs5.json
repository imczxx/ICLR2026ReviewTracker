{"id": "jmtaIDjTs5", "number": 21921, "cdate": 1758323585480, "mdate": 1759896896003, "content": {"title": "Predicting Weak-to-Strong Generalization from Latent Representations", "abstract": "AI alignment seeks to align models with human values such as helpfulness and honesty, yet humans may be unable to supervise on tasks exceeding human capabilities. Weak-to-strong generalization (WSG) has been proposed as a proxy for studying this problem, where a weaker model stands in for human supervision and alignment of a stronger model. While prior work provides evidence of WSG success, i.e. the strong model outperforming the weak supervision signal, prior tasks suffer from train-test contamination or rely on oversimplified linear models. We introduce a clean toy-testbed where transformer model pairs are pretrained on different rule variants of Othello and Tic-Tac-Toe, then the stronger model is finetuned on output from the weaker model. \nIt has been hypothesized that WSG works when the strong model learns how to leverage its superior features. While there has been prior theoretical support, we provide the first empirical evidence for this on transformers. In Othello, the strong student model surpasses the weaker teacher if and only if it has better board representations. Across 111 WSG pairs and 6 game rules, we find a 0.85 Spearman correlation between WSG success and superior board representations in the strong model as measured by linear probes. Our work is a proof-of-concept by analyzing a toy task. By open-sourcing our experiments, we hope to accelerate research on understanding when WSG succeeds.", "tldr": "We train generative models on board games and probe their internal representations to predict weak-to-strong generalization success.", "keywords": ["AI Safety", "Weak-to-strong generalization", "Mechanistic Interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/16014159f53f76da74f11478a2ab89f8a2afa03b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a clean, contamination-free toy testbed using variations of Othello and Tic-Tac-Toe rules for Transformer models. The authors empirically support the hypothesis that WSG success is directly tied to the strong model's ability to leverage its superior internal features, specifically its board representations. They quantify this mechanism by demonstrating a strong Spearman $\\rho$ of $0.85$ between WSG success and the superiority of the strong model's representations, as measured by linear probes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The experiments are interesting. The use of linear probing (or more broadly, mechanistic interpretability methods) in Weak-to-Strong Generalization (WSG) is novel. To my best knowledge, this is the first paper to investigate this direction.\n- The motivation is clear. The claim regarding the lack of explanatory power concerning representations in related work is, to my knowledge, accurate. Most previous WSG theories utilize linear models and the Gaussian feature assumption to derive conclusions, including the statement that \"WSG works when the strong model learns how to leverage its superior features.\" While this claim has appeared in many related WSG works, this paper's effort to address the absence of a mechanistic explanation is appreciated.\n- The paper is generally well-written with intuitive figures and explanations. For instance, highlighting \"train-test contamination\" as one of the paper's advantages is excellent. This specific limitation appeared in the original WSG paper but seems to have been neglected by much of the subsequent work in this field."}, "weaknesses": {"value": "**Firstly, let me say that if my concerns are addressed completely and satisfactorily, I would be happy to revise my review and increase my rating.** It is possible that I have not fully grasped the setting of the board game variations; if so, please feel free to correct my understanding. Aside from the paper's experiments, I genuinely appreciate the underlying thinking presented. If the paper could delve deeper into the questions it raises, I would strongly advocate for a high score. However, for now, I feel there are areas where the paper could be improved and explored more thoroughly. Therefore, I need to reach a mutual understanding with the authors on several points, which requires a more detailed discussion.\n\n## 1. Problem Setting\nThis setting appears to differ from the standard WSG setup. Figure 1 suggests that the weak model is trained with a basic representation, while the strong model is trained with a better representation. However, in a practical WSG setting, the weak model is typically trained on ground truth data before its predictions are used to fine-tune the strong model. This would imply that the weak model's training source is \"better\" than the strong model's fine-tuning data (which is derived from the weak model). I am slightly confused by the assignment of representation quality for the weak and strong models in your proposed testbed compared to the typical WSG data pipeline.\n\n## 2. How Does Linear Probing Support Your Claim?\n**I am not convinced that linear probing adequately explains the mechanism by which the strong model leverages superior features to achieve WSG.** I would argue that if the WSG phenomenon occurs, and if you are using linear probing to measure the quality of representations, it is almost inevitable that the model is utilizing better features. Therefore, your experiments appear logically constrained to showing that, under the two specific settings you consider, WSG can occur under certain parameter configurations. Intuitively, if WSG occurs, your experimental results are guaranteed to follow the observed trend. Conversely, if WSG does not occur, the trend will not appear. As a result, the strong correlation you found is expected and not surprising. To my knowledge, linear probing can only indicate whether a representation encodes useful information about the task, but it cannot explain why this is the case. Consequently, the paper does not clearly explain why the strong model acquires these beneficial features in the first place. **If your main contribution is dealing with the verification of the previous statement in WSG that \"WSG works when the strong model learns how to leverage its superior features,\" then I believe that the linear probing approach is insufficient to verify \"how.\"** This is because linear probing has been widely employed in different tasks, and it is well-established that the performance of the final task can be predicted by probing the representations. In this sense, this paper merely validates a widely-acknowledged conclusion in a specific setting—WSG—which seems trivial and superfluous. However, if the paper's main goal is to apply interpretability tools to WSG, then it is easier for a researcher in WSG to accept the contribution. \n\nWhat exactly constitutes a \"better representation\" or \"superior features\" in the context of your work? I strongly encourage the authors to delve deeper into this fundamental question. For instance, the related WSG paper by Ildiz et al. (2025) you cited characterizes the condition under which WSG occurs using eigenvalue analysis. Also, Dong et al. (2025) uses intrinsic dimension. Therefore, I suggest the authors broaden the scope of this paper beyond just accuracy and linear probing to offer a more profound definition and evaluation of representation quality (for example, eigenvalue and intrinsic dimension).\n\nFurthermore, I believe that generally, if the final task accuracy is high, the features are likely good, but the converse—if the features are good, the final task accuracy is not necessarily high—is a more subtle point. The paper's finding of a trend between representation superiority (linear probe accuracy) and WSG success (final task accuracy) effectively suggests that representation quality can predict the occurrence of WSG. This shifts the core question to: **what is the precise relationship between linear probing accuracy and the final task accuracy?** Verifying this relationship seems challenging because the linear probing literature involves subtle decisions regarding which layer's representation to use and how to extract representations (especially in NLP), potentially requiring extensive comparative experiments. Your conclusion would be significantly more convincing if you could demonstrate two points: (1) The correlation between linear probing accuracy and final task accuracy is typically much weaker in other tasks; and (2) This correlation is significantly strong under the specific WSG settings you consider.\n\n## 3. Minor Points\n\nTypos/Formatting:\n- Line 217: Reference should be to Appendix A.1.\n- Line 266: Reference should be to Figure 2.\n- Line 335: The variables round_down, n_layer, and d_model should be formally explained. While they are easy to understand, variables derived from code that appear in the main text must be defined.\n- Line 400: Change replicated \"5.1\" to something more formal. Also, use the LaTeX formula $N=111$ instead of N=111 for consistency.\n- Line 402: The phrase \"2.1\" here is unclear and too casual.\n\nRelated work: \n- Missing references [1-4] concerning the theoretical analysis of weak-to-strong generalization.\n- Although the paper mentions the concept of \"The Linear Representation Hypothesis,\" (Line 133) it does not even cite the \"Linear Representation Hypothesis\" paper [5]. I understand that the core idea behind the linear representation hypothesis has been investigated for some years before [5], which may explain why you did not cite it. However, to my knowledge, the specific term \"Linear Representation Hypothesis\" first appeared in [5]. \n- While linear probing has been extensively used in LLMs in recent years, Section 2.2 of the paper does not include any citations from 2024. I encourage the authors to update this subsection by citing some recent, relevant papers published beyond 2023 on linear probing or the Linear Representation Hypothesis in LLMs, such as [6-10], to better support the context of this work and demonstrate awareness of current literature.\n\n[1] Relating Misfit to Gain in Weak-to-Strong Generalization Beyond the Squared Loss. ICML 2025.\n\n[2] The Capabilities and Limitations of Weak-to-Strong Generalization: Generalization and Calibration. arXiv:2502.01458.\n\n[3] From Linear to Nonlinear: Provable Weak-to-Strong Generalization through Feature Learning. HiLD 2025.\n\n[4] On the Mechanisms of Weak-to-Strong Generalization: A Theoretical Perspective. arXiv:2505.18346.\n\n[5] The Linear Representation Hypothesis and the Geometry of Large Language Models. ICML 2024.\n\n[6] Representation Engineering: A Top-Down Approach to AI Transparency. arXiv:2310.01405\n\n[7] Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. NeurIPS 2023.\n\n[8] On the Origins of Linear Representations in Large Language Models. ICML 2024.\n\n[9] Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models. ACL Findings 2024.\n\n[10] When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers. ICLR 2025."}, "questions": {"value": "1. Given a fixed parameter for the strong model in Figure 2, a stronger weak model results in a lower PGR. The authors explain this phenomenon by stating that \"For large weak models (medium, huge), PGR is negative, because they are already playing close to perfect.\" I find the observation that PGR is negative counter-intuitive, even if the weak model is highly reliable. While I agree the improvement over the weak model should be smaller, it should not lead to further degradation (a negative gain). This observation is peculiar and requires further discussion.\n2. Why is there a very large variance in Figure 2?\n3. Why is $R^2 = 0.298$ such a small number that might not indicate a significant trend? I understand that other metrics may sufficiently indicate the trend, but why is the $R^2$ value so low?\n4. Line 71: The claim \"We analyse the toy task of Othello, because it is the first truly pretrain leakage free transformer environment for WSG\" requires justification. Why is it the first? The authors make this strong statement without proper explanation or citation.\n5. Line 188: The claim \"Our mechanistic interpretability approach provides the first direct empirical validation of these theoretical predictions using realistic transformer architectures.\" I want to challenge the word **\"first\"** in this sentence (and in some other positions in this paper) and remind you that the paper by Xue et al. (2025) you cited also developed a representation-based metric that strongly correlates with W2SG performance. Given that linear probing is also a representation-based metric, would you like to provide a more detailed discussion (or potential experimental comparison) between the two?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QGAc9MEdbS", "forum": "jmtaIDjTs5", "replyto": "jmtaIDjTs5", "signatures": ["ICLR.cc/2026/Conference/Submission21921/Reviewer_iwNH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21921/Reviewer_iwNH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21921/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760675428362, "cdate": 1760675428362, "tmdate": 1762941982440, "mdate": 1762941982440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper demonstrates weak to strong generalization for transformers in a clean game environment (Othello and Tic-Tac-Toe). Furthermore, empirically this work confirms the already known hypothesis that WSG occurs if and only if the strong model has better representations (or superior features) of the target task than the weak model itself.\n\n\nDecision: My actual score for this paper would be 5. I’m okay with either decision for this paper."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper provides a better empirical evidence of the following known hypothesis in a clean game environment: WSG occurs iff strong model has a better representation of the target task than the weak model. These empirical findings therefore give further confidence into our current primarily theoretical understanding of WSG. The paper provides empirical evidence of this hypothesis for transformers -- which was not done in the prior work. \n2) The paper is clean and very well written."}, "weaknesses": {"value": "1) My primary weakness of this paper is the motivation itself. There are various theoretical results already showcasing good understanding of WSG, these results work more generally including transformers. Although the results in the paper are interesting, I feel there is no new understanding that can be generated from this work which was not already known. \n\n2) Also for a completely new complicated task, it is unclear on how to decide whether the strong model has learnt the better representations of that task."}, "questions": {"value": "Let’s say I have a new task — how can we determine whether the model has actually learned better representations? Is there a principled way to assess whether the model has learned a good representation for a given task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CaKpXvIDk6", "forum": "jmtaIDjTs5", "replyto": "jmtaIDjTs5", "signatures": ["ICLR.cc/2026/Conference/Submission21921/Reviewer_niCS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21921/Reviewer_niCS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21921/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857530311, "cdate": 1761857530311, "tmdate": 1762941981975, "mdate": 1762941981975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies weak-to-strong generalization (WSG) in which a weak model produces labels used to train a strong model. It has been empirically shown in frontier LLMs that the strong student can surpass its weak teacher. Here the authors propose a new toy testbed, where they train transformers on different rule variants of two games, Othello and Tic-Tac-Toe, to define weak & strong models. They claim to provide the first empirical evidence that WSG success depends on the stronger model’s internal representation quality, reporting a high Spearman correlation of 0.85 between WSG success and linear-probe accuracy on board-state representations. The paper concludes a proof-of-concept that strong models outperform weak teachers if and only if they possess such superior features."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The problem has clear motivation, and most parts of the paper are well-written, making the presentation easy to follow. \n* Code is anonymously open-sourced with a clean and reproducible setup that shows strong correlational results. \n* Though investigated through a toy framework, the problem is important as AI models are becoming increasingly stronger, and understanding their internal representation potentially helps facilitate alignment."}, "weaknesses": {"value": "* **Overstated Novelty**: Most of the empirical evidence from this toy testbed is correlational in this paper, but at times the wording (e.g. the \"if and only if\" statement in the abstract) seems to imply that the superior features learned by the strong enable it to achieve WSG. The only insight from the study is that better WSG and better representations tend to co-exist, but the bi-directional sufficiency/necessity and causal relationships are **NOT** adequately justified (via ablations or other methods). With this being said, this proof-of-concept for \"when WSG succeeds\" appears weak, as real WSG involves much richer internal/latent dynamics than toy board states can model. While the authors did point out this limitation, I believe it ultimately makes this paper lack depth and rigor. \n\n* **Inconsistency with Existing Work**: The main conclusion only revolves around the **strong** representations of the strong model, but previous work [2] has demonstrated that in large models, WSG involves an intricate interaction between the representations of both the weak and strong models, and WSG will not be as effective if this interaction causes the strong model to replicate the weak model's inability. This nuance is entirely missing here possible due to oversimplified toy setting and shows an important gap. In practice, strong representations of certain features could cause overfitting and amplify the weak model's bias. This does not manifest in the considered testbed. \n\n* **Inaccuracy in Problem Setup**: While the framework proposed in the paper can prevent pretrain leakage and train-test contamination to some extent, I believe the following two issues persist. (1) The claim in Section 2.1 is inaccurate: most WSG studies focus on two models pretrained similarly (covering similar scope of tasks). For example, the original OpenAI paper [1] used GPT-2 and GPT-4, and the superiority of the strong model mainly derives from richer representations across general tasks. Here the two transformers are trained on different rule variants and less representative of practice. (2) The setup seems to reduce obvious contamination, but it may not fully rule out pretraining leakage at the representational level. Both models still share similar \"token spaces\", and more justifications might be needed here. \n\n[1] Collin Burns et al. Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision.  \n\n[2] Yihao Xue, Jiping Li, and Baharan Mirzasoleiman. Representations Shape Weak-to-Strong Generalization: Theoretical Insights and Empirical Predictions. ICML 2025."}, "questions": {"value": "Most questions are raised in the Weakness section. Regarding the setup, I have two short follow-up questions: \n\n1. Have you tested whether manually degrading the board representations reduces WSG success, which can potentially support some causality?\n\n2. As already pointed out in the paper, the setup may not directly generalize to frontier models. Yet, what could be some extensions from the toy framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k6dyJazYoN", "forum": "jmtaIDjTs5", "replyto": "jmtaIDjTs5", "signatures": ["ICLR.cc/2026/Conference/Submission21921/Reviewer_N2kT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21921/Reviewer_N2kT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21921/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894129232, "cdate": 1761894129232, "tmdate": 1762941981663, "mdate": 1762941981663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}