{"id": "QBZoAChPv5", "number": 9050, "cdate": 1758108676991, "mdate": 1759897746336, "content": {"title": "Forward Chaining Neural Network for Rule Induction", "abstract": "Inductive Logic Programming (ILP) learns logical rules from data, forming an interpretable machine learning model.\nEarly-stage symbolic ILP systems perform outstandingly on small-scale tasks but suffer from combinatorial explosion.\nEmerging neuro-symbolic ILP methods demonstrate a certain degree of scalability and are more robust to noisy data.\nHowever, existing neuro-symbolic ILP methods are limited to constrained language biases, hampering further scalability.\nIn this work, we propose Forward Chaining Neural Network (FCNN), a stochastic neural network that can learn logical rules under any language bias.\nFCNN relaxes all syntactically correct rules into continuous spaces and searches for the semantically correct solutions via gradient-based optimization.\nExperiments on standard evaluation tasks and recently proposed large-scale tasks show that FCNN outperforms existing methods.", "tldr": "", "keywords": ["Inductive Logic Programming", "Rule Induction", "Neuro-Symbolic"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/59fb9be771b0ef186582c94a3b550bfc0477b7e5.pdf", "supplementary_material": "/attachment/51669e2b416c9652d3034b648ac907f71bc6744d.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the Forward Chaining Neural Network (FCNN), a neuro-symbolic model for Inductive Logic Programming (ILP). Unlike earlier neural ILP approaches (e.g., ∂ILP, LRI, HRI, DFORL), FCNN introduces a universal meta-rule framework that allows learning Horn rules of arbitrary arity and body length, under both closed-world and open-world assumptions. The method relaxes symbolic unification into a continuous probabilistic framework, parameterizing head and body atoms with embeddings and Bernoulli random variables. Optimization is performed using nested REINFORCE estimators and entropy regularization. Experiments on both classic small-scale ILP benchmarks and the new large-scale GeoILP dataset show that FCNN outperforms previous neuro-symbolic systems and even recent reasoning LLMs on most tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The idea of fully relaxing Horn rule unification into a differentiable stochastic process is elegant and theoretically sound. The probabilistic modeling of atom and variable unification with Bernoulli and categorical distributions provides a flexible parameterization.\n\nThe paper is rigorous, with clear mathematical definitions, probabilistic modeling, and proofs (e.g., unbiased gradient estimator, completeness theorem). Algorithmic details for differentiable subset sampling are also well described.\n\nThe paper provides a concrete procedure (Algorithm 2) for extracting interpretable symbolic rules from the learned stochastic representations — an important feature for ILP research."}, "weaknesses": {"value": "Presentation and readability.\n\nLack of conceptual comparisons.\n\nLimited ablations on modeling choices.\n\n\ni will detail these points in the section below."}, "questions": {"value": "**Presentation and readability.** The paper’s exposition is overly dense and bottom-up. The intuition behind the model (e.g. idea of reparametrizing unifications) could have benefited from a clearer top-down narrative — motivating ideas first, then details. A graphical illustration could also have helped.\n\n**Limited ablations on modeling choices.** Although prior neuro-symbolic systems like ∂ILP, LRI, HRI, and DFORL are discussed, comparisons to alternative paradigms that feel closer in paradigm misses. For example, DiffLog [1] and AlphaILP[2] exploited templates and forward chaining for ILP. How does it differ? Also, DeepSoftLog[3] introduces the idea of soft unification into probabilisti logic programming. Soft unification is actually a different parameterization (kernel-like) of a probabilisticu unification. Also in that paper there was some ILP on automata. What are the links?\n\n**Limited ablations on modeling choices.** The ablations mainly address sample sizes and OWA vs CWA modes. Missing are experiments probing the necessity of specific design choices (e.g., embeddings vs discrete parameters, REINFORCE vs relaxation-based training, as many other models do).\n\n\n[1] Si, Xujie, et al. \"Synthesizing datalog programs using numerical relaxation.\" arXiv preprint arXiv:1906.00163 (2019).\n[2] Shindo, Hikaru, et al. \"Learning differentiable logic programs for abstract visual reasoning.\" Machine Learning 113.11 (2024): 8533-8584.\n[3] Maene, Jaron, and Luc De Raedt. \"Soft-unification in deep probabilistic logic.\" Advances in Neural Information Processing Systems 36 (2023): 60804-60820."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Sl6weSvAEE", "forum": "QBZoAChPv5", "replyto": "QBZoAChPv5", "signatures": ["ICLR.cc/2026/Conference/Submission9050/Reviewer_t1ZP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9050/Reviewer_t1ZP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818109436, "cdate": 1761818109436, "tmdate": 1762920764441, "mdate": 1762920764441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission investigates neural-symbolic inductive logic programming (ILP) with the goal of learning logical rules from data. The core mechanism employed is a forward-chaining neural network based on so-called meta-rules (in the form of Horn clauses). Its main advantage lies in relaxing syntactic constraints on rules—for example, allowing the arity of predicates to increase—and thereby supporting, to some extent, the open-world assumption."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper demonstrates some progress compared to prior work. In particular, while previous approaches typically impose strong restrictions on atoms, the current method can, in principle, handle general Horn clauses.\n\n- The paper is written in a formal and structured manner, which contributes to its clarity and readability."}, "weaknesses": {"value": "-  Limited novelty. The work follows a fairly standard approach to tackling ILP in a neuro-symbolic manner—namely, by neuralizing logic rules (in this case, via forward-chaining neural networks) and softening symbols through distributions, thereby effectively continuizing traditional discrete objects. The use of REINFORCE is also conventional. In this respect, the contribution appears to lie primarily in adapting existing techniques to the ILP setting, rather than introducing fundamentally new ideas.\n\n-  Overstated contributions. The framework remains template-based, albeit with a more relaxed form than prior work. Consequently, its ability to address the open-world assumption (OWA) is still limited. For example, it is unclear whether the proposed method can discover new predicates or formulate new symbolic concepts, rather than relying on predefined templates.\n\n- Limited impact. The experimental evaluation is restricted to ILP benchmark datasets, which are relatively small and arguably toy problems. Given that ILP represents a niche research area, the scope of the current paper appears narrow, as reflected in the limited breadth of related work. It remains uncertain—though IMHO unlikely—whether the proposed method can generalize to real-world applications."}, "questions": {"value": "- Line 059, without **strong** language bias. What does strong mean here? \n- Line 205-206, I do not really understand “ i.e., unifying the head atom according to a probability”, “Such a unification corresponds to the property that the Horn rule only allows one head atom.” \n- Line 295 “After optimizing, the distributions are supposed to collapse to deterministic distributions, […]” why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6ReuEdyPYD", "forum": "QBZoAChPv5", "replyto": "QBZoAChPv5", "signatures": ["ICLR.cc/2026/Conference/Submission9050/Reviewer_CJSz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9050/Reviewer_CJSz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855247038, "cdate": 1761855247038, "tmdate": 1762920763937, "mdate": 1762920763937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FCNN, a stochastic neural network that can learn logic rule. They introduce a universal meta-rule that serves as a general template for Horn rules, removing the strong language bias and manual variable assignment present in prior neural ILP systems. FCNN performs probabilistic forward-chaining reasoning and optimizes the expected reward via REINFORCE. The paper evaluates the method on both classical tasks and large-scale ILP tasks with open-world setting, where previous neural ILP systems fail to scale, and shows that FCNN outperforms prior neural ILP systems and LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper addresses the limited expressivity and strong language bias of prior neural ILP systems.\n2. This method connects inductive logic learning with stochastic gradient optimization via probabilistic rule sampling and the REINFORCE estimator.\n3. The paper demonstrates scalability and interpretability on both small and large-scale ILP tasks, outperforming prior neural ILP systems and LLMs."}, "weaknesses": {"value": "1. The overall algorithm is not clearly presented, algorithm 1 and 2 describe partial components (rule sampling and body-atom extraction), but the complete training loop is missing, which makes the method section difficult to follow.\n2. The theoretical part on probabilistic relaxation equivalence is intuitive but lacks formal assumptions (e.g., boundedness, convergence) and a detailed proof.\n3. The efficiency and scalability of the method are not discussed, while a simple complexity estimate is given for body-atom sampling, this method could be more computationally expensive than traditional neural ILP. Its efficiency should therefore be analyzed or at least empirically compared with other neural ILP systems and LLMs."}, "questions": {"value": "1. It is impressive that FCNN solves all tasks and outperforms other neural ILP systems, but the discussion is limited. Could the authors clarify which components (e.g., probabilistic rule sampling, variable unification, or optimization scheme) contribute most to the improvements?\n\n2. What is its sample and computational efficiency compared to other neural ILP or LLM-based reasoning systems? How does runtime scale with the number of predicates or body-atom length?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "76Eq8gCX0v", "forum": "QBZoAChPv5", "replyto": "QBZoAChPv5", "signatures": ["ICLR.cc/2026/Conference/Submission9050/Reviewer_Pcpa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9050/Reviewer_Pcpa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944055405, "cdate": 1761944055405, "tmdate": 1762920763560, "mdate": 1762920763560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FCNN, which addresses a fundamental limitation in neuro-symbolic ILP, namely that most existing methods are restricted to unary/binary predicates and ≤2 body atoms. The paper proposes universal meta-rules that probabilistically unify with candidate atoms and variables through learned embeddings, optimized via nested REINFORCE. Key contributions: (1) direct learning of variable-argument unification vs. manual specification, (2) linear-time constrained sampling algorithm, (3) support for arbitrary predicate arities. Results: 100% success on standard small benchmarks including the historically difficult Fizz/Buzz tasks; outperforms LLM on GeoILP with high-arity predicates (up to 8 arguments). Theorem 4.3 proves completeness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: Universal meta-rule framework removes restrictive templates; direct variable-argument learning vs. manual specification is novel approach. Symbol randomization for fair LLM comparison is methodologically sophisticated.\n\nQuality: Completeness theorem provides theoretical foundation; ablation studies examine key design choices; 100% success on standard benchmarks; handles 8-arity predicates where baselines fail.\n\nClarity: Motivation clear; problem formulation well-justified; experimental setup rigorous.\n\nSignificance: Improves on fundamental scalability barrier in neuro-symbolic ILP; enables learning of complex rules with arbitrary arities; GeoILP benchmark specifically designed to test claimed contributions."}, "weaknesses": {"value": "1. Figure 1 (Sensitivity of sample size) shows larger sample sizes increase variance with no explanation provided. This raises questions about RLOO baseline effectiveness and practical deployability.\n\n2. No time/space complexity analysis, runtime comparisons, or convergence speed discussion. Critical for assessing practical scalability beyond synthetic benchmarks.\n\n3. Symbolic hyperparameters (|U|, B, V, auxiliary predicates) replace template design bias with hyperparameter bias. No guidance for setting these; ablations only cover Na, Nv. \"Sufficiently large\" appears frequently without bounds.\n\n4. CWA/OWA comparison on only 4 tasks; missing ablations for embedding dimensions, number of meta-rules, entropy decay schedule.\n\n5. Extension from equality (Ahmed et al.) to inequality constraint claimed \"almost the same\" but is non-trivial technical step deserving explicit derivation.\n\n6. GeoILP limited to \"basic\" level despite scalability claims; no failure mode analysis; learned rules not shown/analyzed for interpretability."}, "questions": {"value": "How were symbolic hyperparameters chosen for GeoILP? Was domain knowledge used? What happens with significantly oversized B, V—does optimization fail or does entropy regularization compensate? \n\nBriefly elaborate the key technical step extending equality constraint (s=k) to inequality (s≤B) beyond \"almost the same\" citation.\n\nWhat are time/memory complexities? How do training times compare to HRI/DFORL on matched tasks?\n\nCan you show examples from GeoILP? Are they interpretable and geometrically meaningful?\n\nDid you test LLM with original (non-randomized) GeoILP predicates to quantify the \"reasoning gap\" closed by semantic knowledge?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EIPOhKLGVG", "forum": "QBZoAChPv5", "replyto": "QBZoAChPv5", "signatures": ["ICLR.cc/2026/Conference/Submission9050/Reviewer_WSE3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9050/Reviewer_WSE3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762755741453, "cdate": 1762755741453, "tmdate": 1762920763192, "mdate": 1762920763192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}