{"id": "tYeuz2LwVU", "number": 20018, "cdate": 1758301535243, "mdate": 1763753950471, "content": {"title": "Signal in the Noise: Polysemantic Interference Transfers and Predicts Cross-Model Influence", "abstract": "Polysemanticity is pervasive in language models and remains a major challenge for interpretation and model behavioral control. Leveraging sparse autoencoders (SAEs), we map the polysemantic topology of two small models (Pythia-70M and GPT-2-Small) to identify SAE feature pairs that are semantically unrelated yet exhibit interference within models. We intervene at four loci (prompt, token, feature, neuron) and measure induced shifts in the next-token prediction distribution, uncovering polysemantic structures that expose a systematic vulnerability in these models. Critically, interventions distilled from counterintuitive interference patterns shared by two small models transfer reliably to larger instruction-tuned models (Llama-3.1-8B/70B-Instruct and Gemma-2-9B-Instruct), yielding predictable behavioral shifts without access to model internals. These findings challenge the view that polysemanticity is purely stochastic, demonstrating instead that interference structures generalize across scale and family. Such generalization suggests a convergent, higher-order organization of internal representations, which is only weakly aligned with intuition and structured by latent regularities, offering new possibilities for both black-box control and theoretical insight into human and artificial cognition.", "tldr": "Using four interventions, we probe how polysemantic structure steers LLM behavior and find that these vulnerabilities transfer across models, reflecting shared polysemantic structure.", "keywords": ["Polysemanticity", "Sparse Autoencoders", "Interpretability", "Behavior Intervention"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/39c65b0a5b1342a2c8cfef2a0e730611996707a6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work investigates polysemanticity in language models using SAEs, and whether this polysemanticity translates to practical vulnerabilities, where vulnerabilities are characterised based on shifts in next token predictions following interventions. They identify pairs of features which interfere, but are semantically unrelated, and show that black and white box interventions (prompt injection and steering) made on one feature interfere with the other. Interestingly, they find that these interference pairs, which appear to have no interpretable semantic link, replicate across models of different sizes, as shown with steering experiments. Regarding vulnerabilities, they show that altering the activations of more polysemantic neurons leads to greater shifts in outputs.\n\nThis is an interesting and well conducted investigation into a known but minimally explored implication of superposition. The writing and Figures are unclear in places (I have made some suggestions to improve this) and this does detract from the contributions. Furthermore, some of the relationships observed in the results are unclear and merit more explanation and investigation, as I have raised in the weaknesses and questions. However, overall this is an interesting piece of work with potentially valuable practical implications."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper addresses a potentially impactful but minimally studied issue: the impact of polysemanticity on LLM vulnerability.\n\nGood and thorough analysis, both within the main text (e.g. running experiments with features sampled from different interference intervals) and in the additional results in the Appendices (e.g. the correlation results between interference and semantic similarity in Figure 8 are an interesting additional result). Multiple metrics are used to validate results, and results are reported for multiple models with confidence intervals.\n\nThe set of black and white box interventions used are well-designed and thorough. The gradient-based steering approach, in particular, is a clever way to run steering experiments on models where SAEs are not available. \nThe finding that polysemantic structures transfer across models, despite having no immediately interpretable meaning, is quite novel and opens up interesting directions for further investigation.\n\nWhere there is confusion around the results (e.g. the major gap between models in Figure 3), the authors are transparent and honest, which I appreciate."}, "weaknesses": {"value": "The introduction is long and seems to try and incorporate a large amount of related work (while the main RW section is in the Appendices). This makes it difficult to follow the purpose of the paper. I’d recommend using the additional page to change this to a condensed introduction followed by a separate related work.\n\nI found Figure 1 confusing, especially as it appears before the representation domains are defined. The text on the right is a useful summary, but it was not immediately clear to me what the top boxes were trying to show. Could you move these below the other graph (so the reader reads the text first) and/or add labels to these boxes? Please also increase the font sizes in Figures 2-4.\n\nThe SAE feature steering results (Figure 3) are extremely different between Pythia and GPT-2, and I don’t find the hypothesised cause - model depth - to be particularly convincing. It would be great to repeat this with a 3rd model to compare the results, or at least to discuss or investigate other possible reasons in more detail (e.g. it may be SAE quality, since the difference in effect size is much smaller with the gradient direction interventions).\n\nThis paper could also be improved by adding a realistic case study of how interference values can be used to extract examples of prompt injections which elicit dangerous responses. This would be particularly impressive if they transfer to closed weight models."}, "questions": {"value": "What is your intuition behind the weighted cosine similarity metric (Equation 1) and how what this measures differs to the weighted overlap? Why select these measures?\n\nWhy do you use token frequency instead of these metrics in the prompt injection experiments?\n\nPlease define interference when you first refer to it in the introduction - its somewhat ambiguous otherwise. Similarly for the ‘feature irrelevance thresholds’ introduced in Section 3.1 - it would be much clearer if you define and link this to the semantic similarity measure in the main text (its also not clearly described in the Appendix).\n\nCan you repeat the experiment in Figure 3 with a third model, to investigate the 100x difference in magnitude between the Pythia and GPT-2 effects? \n\nThe results on Figures 3 and 4 show a 100x difference between these methods for GPT-2 but the text in Section 3.2 says 10x - is this a typo or error on one of the graphs?\n\nCan you move the note for Table 1 into the caption? The trends may also be clearer if you report the change in % relative to the random token rather than the absolute values. Also increase the shading contrast as this is barely visible!\n\nCan you add interference and semantic similarity scores to Table 3?\n\nMinor comment - the reference on line 143 is misformatted."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ougJvtMCSe", "forum": "tYeuz2LwVU", "replyto": "tYeuz2LwVU", "signatures": ["ICLR.cc/2026/Conference/Submission20018/Reviewer_dp5x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20018/Reviewer_dp5x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761489362333, "cdate": 1761489362333, "tmdate": 1762932918518, "mdate": 1762932918518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores polysemantic interference in transformer representations. Using sparse autoencoders (SAEs) trained on GPT-2-Small and Pythia-70M, the authors identify pairs of features that are semantically unrelated yet exhibit high cosine similarity (“interference”) in the SAE latent space. They then test whether intervening on one feature influences another across four levels: SAE-feature, token-gradient, prompt injection, and neuron manipulation. Results suggest that such interference pairs can weakly steer model predictions and that these patterns partly transfer to larger black-box models."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper attempts to connect feature-level correlations to behavioral effects and cross-model transfer --- an ambitious angle.\n\n- The idea of probing “interference” across different loci (feature, gradient, prompt, neuron) is original and could, in principle, yield insights into distributed representations."}, "weaknesses": {"value": "Unclear necessity of SAEs: It is never justified why SAEs are needed for identifying interference.\nThe same cosine-overlap analysis could be performed directly in the model’s activation or embedding space, or between token clusters in the vocabulary embedding. By relying on pre-computed SAE features (and their textual glosses), the analysis may inherit annotation noise and input-centric biases (see Arad et al., SAEs Are Good for Steering - If You Select the Right Features, 2025).\n\nDefinition: The notion of “interference” and of “polysemantic neurons” are both defined through the SAE basis; this makes the subsequent results self-referential. There is no control showing that similar effects would not appear if random or non-SAE directions were used.\n\nBaselines: In most experiments, there is no test of how non-interfering or semantically similar features behave under the same interventions. Without such controls, it is hard to interpret the magnitude or specificity of the reported changes.\n\nSmall effects: Table 1 and related plots show minor shifts (typically just a tiny bit above random). While statistically significant, the effects are small enough to fall within what could be lexical or topical overlap rather than genuine structural transfer. The cross-model “transfer” results in particular feel underwhelming. In Figure 5, the neuron-masking results are not monotonic, peaking at moderate polysemanticity. This contradicts the narrative that highly polysemantic neurons are “interference hubs.”\n\nIn sum: Interesting question and creative setup, but weak and noisy evidence and unclear conceptual motivation for SAEs"}, "questions": {"value": "- Why not compute interference directly between token-embedding clusters or residual directions instead of through an SAE?\n\n- Can the authors provide baselines for unrelated concepts under identical interventions?\n\n- How robust are the results to SAE sparsity, random seed, or layer choice?\n\n\nMore related work:\n- Rosetta Neurons (Darvid et al. 2024) \n- Second-Order Effects in CLIP (Gandelsman et al. 2024)\n- Platonic Representations (Huh et al. 2024)\n- Token Entanglement (Zur et al. 2025)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8L6dc9wf87", "forum": "tYeuz2LwVU", "replyto": "tYeuz2LwVU", "signatures": ["ICLR.cc/2026/Conference/Submission20018/Reviewer_yfBh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20018/Reviewer_yfBh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762030843645, "cdate": 1762030843645, "tmdate": 1762932917731, "mdate": 1762932917731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies polysemanticity in LLMs, using SAEs to identify features which are semantically unrelated, but have unexpectedly entangled representations. They use these features to develop 4 methods of intervention (prompt, token, feature neuron), and show the effectiveness of interventions with semantically unrelated features, e.g. occurrences of specific surnames with geographic locations. They find cases where, surprisingly, semantically unrelated features transfer from very small models (e.g. 70M) to large models (e.g. 70B). The authors also study highly connected \"super neurons\" which, if steered, affect hundreds of distinct SAE features, as a case study in extreme polysemanticity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Well-motivated problem setup, and grounded in real world impacts of polysemantic features and vulnerability of black-box models to adverserial attacks.\n- Experiments and methods are comprehensive, thorough, and well-documented.\n- Interesting empirical results with clear impact, especially the transfer between models which are tiny by modern standards and full-size 70B models. The super-neuron results are also interesting.\n- Generally well-written."}, "weaknesses": {"value": "- One of the greatest strengths of this paper is the result with feature transfer across model scale. However, the effect size of these seems very small at only a few % points in most cases (table 1), which the authors don't seem to make note of. If this is right, this seems like an important limitation of this work which should be clear to the reader, although it also may diminish the significance of this work somewhat.\n\t- Relatedly, some of the interventions in figure 3 seem to have very modest effects compared with the random baseline.\n- Presentation in the figures was lacking, in my opinion. I had trouble understanding fig 1, and this could be improved. For many of the results figures, I had some confusion as to exactly what the point is in some cases. It might help readability if the captions clearly state what the takeaway is from reading these results.\n- There seems to be some missing glue between the introduction, and the formalization of e.g. the \"Human Symbolic Manifold\", and the rest of the text. These don't seem to be referenced later, except in 3.5, and so I wonder if this formalization is even necessary, or if instead the authors could utilize it more.\n- Too much reliance on the appendix, which really should be extra information which is not vital to the content of the work. One example is table 3 and some of the word clouds in the appendix - these were helpful to give me intuition of what kinds of particular semantically unrelated features are interfering, and something like this in the main text might improve readability.\n\nI might be willing to raise my score if these concerns were addressed, especially the first point."}, "questions": {"value": "- What do the authors think of the relationship between semantically unrelated polysemanticity and recent work on emergent misalignment and subliminal learning [1, 2]?\n- For table 1, I wonder what a \"semantically related\" baseline would look like, rather than a random baseline. Was this tested, or would this make sense to test?\n- I wonder if many of the significance tests in this table would hold up with multiple hypothesis correction (e.g. holm-bonferroni), given that the table is showing 45 test results.\n- Diving deeper into some of the cases with big effect sizes in cross-model transfer (e.g. science on llama-70B) might be interesting.\n- There is no reference to figure 5 in the text.\n\n\n[1] Betley, J., Tan, D., Warncke, N., Sztyber-Betley, A., Bao, X., Soto, M., ... & Evans, O. (2025). Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs.\n\n[2] Zur, A., Loftus, A. R., Orgad, H., Ying, Z., Sahin, K., & Bau, D. (2025). _It’s Owl in the Numbers: Token Entanglement in Subliminal Learning_."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yBMhJCJ7WG", "forum": "tYeuz2LwVU", "replyto": "tYeuz2LwVU", "signatures": ["ICLR.cc/2026/Conference/Submission20018/Reviewer_Zy8X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20018/Reviewer_Zy8X"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149383815, "cdate": 1762149383815, "tmdate": 1762932917218, "mdate": 1762932917218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores interference in trained SAE latents and polysemanticity thereof. Specifically, polysemanticity is defined as the scenario where the precise descriptions assigned to two features differ and yet they interfere (for which I'm not quite clear what the precise definition is, since, unless I missed something, there was no formula provided for how interference is measured). From hereon, the work is mostly a qualitative study to develop the broader phenomenology of polysemantic SAE latents, i.e., identifying interesting ways that these features interact (e.g., how steering one can affect model behavior along another one). The coolest result, which relates with the work by Lee et al. [1], is that these steering pathologies transfer across model, indicating there are consistent statistical signatures in the data distribution, which all models pick on. This result is especially cool because the results were derived using small models (order 100M parameters), but transfer to substantially larger models (order 7--9B parameters).\n\n[1] https://arxiv.org/abs/2504.14379"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "I really like this paper. It needs some improvement on presentation / writing to ensure results are easily legible, but conditioning on that minimal rewrite, the empirical characterization of interference pathologies (such as the super-neurons), the transfer of these pathologies across model scales, and the exhaustiveness of the experiments is awesome. To be clear, I believe these results were relatively expected, but getting a detailed account of them in a single paper is very helpful for the community."}, "weaknesses": {"value": "- Missing descriptions of relevant concepts: I tried to find the precise mathematical definition for how interference is defined or measured, but didn't see it anywhere in the paper. This really impacted my ability to onboard with the paper on a first reading pass, but eventually digging through the appendix and looking at results, assuming there's a reasonable mathematical definition for the notion of interference, I like this paper's goals and results. If the authors can fix these definitions (unless I missed something there are not provided at the moment), I support the acceptance of this work. \n\nSome other minor comments:\n- Writing style: I found some of the language in the paper unnecessarily complex. For example, the use of the term \"loci\" in the abstract doesn't make sense, both because it's unnecessarily complex, but also (I think) wrong in this case? The closest term that makes sense would be \"foci\", since a \"locus\" refers to a path.\n\n- On adversarial examples and superposition: a recent work makes a similar point as the authors, i.e., adversarial examples can be driven by feature superposition [1]. \n\n- Possible typo on L94--95: If I understand correctly, the line should orthogonality in symbolic manifold M is not does not persist in the activation space after projection. The subsequent lines (L95--96) only make sense to me with this rephrasing.\n\n- Typo in Eq. 1: I presume it should be O(t) instead of P(t), or P(.) should be defined to be an element from the set {$O, \\tilde{O}$}? \n\n- Analysis layer: For several experiments, I wasn't sure which layer SAEs are analyzed. For example, in the paragraph on L279, it is stated that due to greater depth of GPT-2, SAE interventions for interfering features might be less effective. While I intuitively buy this, the precise layer number information would help contextualize the claim.\n\n[1] https://arxiv.org/abs/2508.17456"}, "questions": {"value": "A question that came to mind while reading:\n\n- Instead of the use of agglomerative clustering, as done for analysis in this paper, I wonder if SAEs with hierarchical priors could have been used to directly elicit statistical interferences between model features. For example, see papers [1, 2, 3]. I'd love to gather authors' thoughts (or experiments if they get the chance) on how they would use such more structured SAEs for their analysis. \n\n\n[1] https://arxiv.org/abs/2503.17547\n\n[2] https://arxiv.org/abs/2506.03093\n\n[3] https://arxiv.org/abs/2506.01197"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nD4LCKyy7o", "forum": "tYeuz2LwVU", "replyto": "tYeuz2LwVU", "signatures": ["ICLR.cc/2026/Conference/Submission20018/Reviewer_72iw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20018/Reviewer_72iw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762369786496, "cdate": 1762369786496, "tmdate": 1762932916607, "mdate": 1762932916607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}