{"id": "uxi7YoZ13b", "number": 25368, "cdate": 1758367217694, "mdate": 1759896723411, "content": {"title": "Adversarial Robust Reward Shaping for Safe Reinforcement Learning in AI-Generated Code", "abstract": "We propose \\textbf{Adversarial Robust Reward Shaping (ARRS)}, a novel reinforcement learning framework for generating secure code that explicitly addresses vulnerabilities to adversarial evasion attacks. Conventional reward functions in code generation tasks often do not take into consideration how vulnerable detection mechanisms to subtle perturbations in syntax are which leads to brittle security guarantees. The proposed method integrates an \\textbf{Adversarial Robustness Module (ARM)} into the reward computation pipeline, which systematically identifies worst-case failure scenarios through gradient-based perturbation analysis and penalizes the policy for generating exploitable code patterns. ARM works by generating adversarial examples that are semantically preserving and degrade the performance of the code evaluation system to the utmost and then teaching the RL agent to build a solution that is intrinsically secure using a robustness penalty added to the reward signal.", "tldr": "", "keywords": ["Adversarial Robust Reward"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e3a6fbfe593154484f778dadb4de89cd18289b9c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Adversarial Robust Reward Shaping, which focuses on LLM generated code's vulnerability towards subtle syntax perturbations. The propose algorithm introduces perturbations to sample adversarial cases and train an model. The paper conducts a rudimentary experiment on the topic and shows a promising start."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The idea of using adversarial learning for code LLMs and security vulnerability is a field that still have questions yet to be explored. The paper is a promising start in this direction."}, "weaknesses": {"value": "While the paper is a sound start, I think the idea has to be developed further and more thoroughly experimented and validated to be accepted.\n\nFirst, using an gradient based adversarial learning against gradient based attacks is an interesting and sound approach. However, however, gradient based method is not the only way in which LLM's vulnerability has been approached. For example, PAIR (Prompt Automative Iterative Refinement, Chao et al, 2023) uses an LLM to attack another LLM. While is is not expected that a single paper should all kinds of LLM attacks, it would be better to more clearly define the scope and the boundaries of the research.\n\nIn a related note, I think the paper could cite more related works in the field. While the paper cites some notable papers in the Code LLM, there's a wider breadth of research going on in the neighboring jailbreaking field. For example, while the paper cited Computer Vision research for gradient based approaches, Gradient Coordinate Gradient, is a jailbreaking technique used to exploit the security weakeness of LLMs and therefore would provide a more relavant example to a LLM research. Examples like this would provide a more concrete foundation for the research topic of the paper.\n\nI also found the paper rather difficult to read and several details seemed to be missing. For example, it is not clear how ARM is updated during training to remain challenging and adversarial. Going as early as Robust Adversarial RL (Pinto et al, 2017), how to update the adversarial example generator has been an important part of adversarial robustness by worst-case scenarios. Also, it is difficult to reproduce the results of the paper from the main submission alone, as several details, including training hyperparameters and LLM model in question, seems to be missing.\n\nFinally, there stylistic improvements that can be made. For example, in line 193, most conference submissions prefer equations to be center-aligned with equation number."}, "questions": {"value": "For major changes to the scoring decisions, I would like need to see the following\n\n- More comprehensive comparison with related works\n- A better description & explanation on how the algorithm works\n- Clearing up experiment details"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PQtas6eveS", "forum": "uxi7YoZ13b", "replyto": "uxi7YoZ13b", "signatures": ["ICLR.cc/2026/Conference/Submission25368/Reviewer_82Rr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25368/Reviewer_82Rr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761352307302, "cdate": 1761352307302, "tmdate": 1762943416235, "mdate": 1762943416235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **Adversarial Robust Reward Shaping (ARRS)** for code-generation RL. A module ARM generates *semantics-preserving* perturbations $\\delta^*$  to a produced snippet (c) that maximally degrade a security detector (f), and adds a robustness penalty to the reward. Perturbations are claimed to preserve semantics via syntax-aware edits (renaming, whitespace, comments) validated by an AST-based checker, and experiments on CodeSearchNet/APPS/CodeXGLUE report lower ASR and ~18% overhead."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of *folding worst-case robustness directly into the reward* is conceptually appealing and well-motivated for code, where discrete, semantics-constrained perturbations matter."}, "weaknesses": {"value": "* My first concerin is there is **no usable definition or concrete examples of “adversarial examples”**. The paper lists categories (renaming, whitespace, comments) but does not show a single before/after code pair demonstrating how such an “adversarial” edit leads to bad reward signals, unsafe code, or detector evasion in practice; the connection between (L(f(c+\\delta),s)) and code quality/safety is not grounded with examples   .\n* The detector (f) is unnamed and its training/access assumptions are unclear; transfer to unseen detectors/attacks is not established; key settings ((\\epsilon,\\lambda), attack steps) and baseline configs are missing, limiting reproducibility and external validity.\n* The **presentation** need substaintial improvement. Duplicated subsections and a malformed sentence reduce clarity and trust (e.g., “cannot be used for knifes neckties”; duplicated Section 4.x material)."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7pLbfrK2zQ", "forum": "uxi7YoZ13b", "replyto": "uxi7YoZ13b", "signatures": ["ICLR.cc/2026/Conference/Submission25368/Reviewer_7uKR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25368/Reviewer_7uKR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817997327, "cdate": 1761817997327, "tmdate": 1762943415833, "mdate": 1762943415833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Adversarial Robust Reward Shaping (ARRS), a reinforcement learning framework designed to generate secure AI code by addressing vulnerabilities to adversarial evasion attacks. It introduces an Adversarial Robustness Module (ARM) that is integrated into the reward pipeline. This module employs gradient-based perturbation analysis to identify worst-case scenarios and applies a robustness penalty to the reinforcement learning policy. The goal of this approach is to optimize both functionality and inherent security, shifting from a reactive to a proactive approach. Experiments conducted on benchmarks such as CodeSearchNet, APPS, and CodeXGLUE were compared against baseline models, showing improvements in robustness metrics, including Attack Success Rate (ASR) and robust accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper is well structured.\n2. Experiments show the effectiveness of ARRS."}, "weaknesses": {"value": "1. The presentation is severely undermined by messy formatting issues, including poorly rendered equations (e.g., repeated and misaligned formulas in Section 4, such as the duplicated robustness penalty derivations), unclear architectural diagrams (e.g., Figure 1's simplistic and unlabeled components fail to clearly illustrate ARM integration), and tables/figures that are large in scope but convey minimal actionable information (e.g., Table 1's sparse ASR comparisons lack statistical significance tests or error bars, while Figures 2 and 3 use generic plots without detailed axes labels or explanations of convergence behaviors).\n2. Novelty is limited, as the approach builds heavily on existing adversarial training techniques (e.g., FGSM and PGD from computer vision) adapted to code without substantial innovation.\n3. Experimental validation is inadequate: baselines are not fully comparable (e.g., no details on hyperparameter tuning across methods), metrics like vulnerability rates are measured with CodeQL but lack breakdowns by perturbation type."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Nm8SxxZXh7", "forum": "uxi7YoZ13b", "replyto": "uxi7YoZ13b", "signatures": ["ICLR.cc/2026/Conference/Submission25368/Reviewer_tQpj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25368/Reviewer_tQpj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997086903, "cdate": 1761997086903, "tmdate": 1762943415623, "mdate": 1762943415623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes adversarial training for generating secure code that explicitly addresses vulnerabilities to adversarial evasion attacks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Please see below."}, "weaknesses": {"value": "In the background there is a subsection for reinforcement learning for code generation and adversarial robustness for code analysis. One thing came to my mind is that here it might be reasonable to visit research on adversarial and robust reinforcement learning and possibly a small section about it. Because there has been a long line of research that considered gradient based attacks, defenses and detection methods in the reinforcement learning domain [1,2,3,4,5].\n\nInterestingly, there have also been attempts in the reinforcement learning domain to obtain robustness through adversarial training [4]. Another thing is that, more recently some studies demonstrated that adversarial training is vulnerable to black-box adversarial attacks and natural perturbations exhibiting generalization issues in comparison to standard reinforcement learning [2,5].\n\nI am mentioning these concepts from reinforcement learning, because at several points in the paper the exact same concepts have been proposed/re-introduced in a renamed way. For instance, the section titled “Proactive Security by Design in RL Training” the following description is provided:\n“Unlike post-hoc filtering, ARM proactively shapes the policy’s behavior by exposing it to adversarial examples during training.” This is called adversarial training.\n\nGiven the prior work in reinforcement learning and robustness of it, I am also quite interested if the proposed adversarial training of the submission is robust against more natural perturbations or black-box attacks.\n\nAlso regarding experimental results, I have seen more established datasets on coding such as [6,7,8,9,10]. Is there a reason why the experiments did not include these benchmarks? I am only asking this, because prior work [11] seems to test including these mentioned benchmarks.\n\nI think the submission works in a direction that can possibly have promising results. But in its current form it might not be ready to be published yet. \n\n[1] Adversarial Attacks on Neural Network Policies, ICLR 2017.\n\n[2] Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness, AAAI 2023.\n\n[3] Detecting Adversarial Directions in Deep Reinforcement Learning to Make Robust Decisions, ICML 2023. \n\n[4] Robust Adversarial Reinforcement Learning, ICML 2018.\n\n[5] Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs, AAAI 2022.\n\n[6] Evaluating Large Language Models Trained on Code, 2021.\n\n[7] Swe-bench: Can language models resolve real-world github issues?, 2023.\n\n[8] Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x, 2023.\n\n[9] Program synthesis with large language models, 2021.\n\n[10] Competition-level code generation with alphacode, 2021.\n\n[11] CodeT: Code Generation with Generated Tests, ICLR 2023."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mxGuJCpRw6", "forum": "uxi7YoZ13b", "replyto": "uxi7YoZ13b", "signatures": ["ICLR.cc/2026/Conference/Submission25368/Reviewer_K8kQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25368/Reviewer_K8kQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023067542, "cdate": 1762023067542, "tmdate": 1762943415304, "mdate": 1762943415304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}