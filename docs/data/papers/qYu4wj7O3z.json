{"id": "qYu4wj7O3z", "number": 1563, "cdate": 1756892039054, "mdate": 1759898202170, "content": {"title": "Data Provenance for Image Auto-Regressive Generation", "abstract": "Image autoregressive models (IARs) have recently demonstrated remarkable capabilities in visual content generation, achieving photorealistic quality and rapid synthesis through the next-token prediction paradigm adapted from large language models. As these models become widely accessible, robust data provenance is required to reliably trace IAR-generated images to the source model that synthesized them. This is critical to prevent the spread of misinformation, detect fraud, and attribute harmful content. We find that although IAR-generated images often appear visually identical to real images, their generation process introduces characteristic patterns in their outputs, which serves as a reliable provenance signal for the generated images. Leveraging this, we present a post-hoc framework that enables the robust detection of such patterns for provenance tracing. Notably, our framework does not require modifications of the generative process or outputs. Thereby, it is applicable in contexts where prior watermarking methods cannot be used, such as for generated content that is already published without additional marks and for models that do not integrate watermarking.  We demonstrate the effectiveness of our approach across a wide range of IARs, highlighting its high potential for robust data provenance tracing in autoregressive image generation.", "tldr": "We introduce a framework for robust data provenance tracing of images generated by autoregressive models, requiring no modifications to the generation process or model outputs.", "keywords": ["data provenance", "image autoregressive models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e6aa567d915d12416a686575ca08a28ec59bfe6f.pdf", "supplementary_material": "/attachment/92cf2b10b6293f85922454e8c624ae5b5a3c19c7.zip"}, "replies": [{"content": {"summary": {"value": "The paper propose a post-hoc data provenance method for image auto-regressive models (IARs), containing two stages. The first stage is training a inversion decoder to transform the image back to the feature map. The feature maps corresponding to the generated image can correspond more accurately to the codebook. than other images. The second stages  is calculating the metric for distinguishing whether the  image is generated by IARs. The experimental result shows the effectiveness of the proposed methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The experimental result shows the strong effectiveness to distinguish whether an image is generated by a given VAE decoder.\n2. The EncLoss as a ratio of erros sounds interesting."}, "weaknesses": {"value": "1. According to the provided code, the $D^{-1}$ is fine-tuned from the encoder. However, this process is not clearly described in the experimental setup. \n\n2. The term “AR” (autoregressive) appears in the paper title and description, but no actual AR transformers are presented in the method. It seems the paper may intend to refer to *Data provenance for **Image VAE Generation*** rather than ***Image AR Generation.*** In the experiments, models with different VAEs are compared, but there are no results comparing AR models using the **same** VAE backbone—for example, VAR-d24 for belonging *vs.* VAR-d30 for non-belonging, or VAR *vs.* VAR-CCA [1]. Since the proposed method itself does not directly involve AR transformers, but only uses the images generated by the AR transformer when fine-tuning the inversion decoder, we cannot be sure whether this is effective in this case. Recent works on membership inference for IARs, such as [2] and [3], might provide relevant perspectives or complementary techniques. These methods calculate the metric using the loss of the AR transformers. It would further strengthen the paper if the authors could discuss their methods with in the Related Work section, and it is better to compare with them.\n\n3. The reported value is too high. The evaluation protocol should therefore adopt stricter criteria, such as TPR@0.1%FPR, to provide a more reliable assessment of model robustness and discriminative power.\n\n4. In line 812, the authors mention using the GPU with 48 GB of memory, while in line 856 it is stated as 40 GB. This inconsistency should be clarified.\n\n[1] Chen, Huayu, et al. Toward guidance-free AR visual generation via condition contrastive alignment. ICLR 2025.\n\n[2] Kowalczuk, Antoni, et al. Privacy attacks on image autoregressive models. ICML 2025.\n\n[3] Yu H, et al. Icas: Detecting training data from autoregressive image generative models. ACM MM 2025."}, "questions": {"value": "1. The difference between *Ours (EncLoss)* in Table 3 and Baseline *AEDR* is that the first one remove $Q$. Could the authors clarify why removing $Q$ leads to better performance?\n\n2. According to the provided code, the random seed is fixed in the script to generate images. Is it the same when generating the evaluation dataset and fine-tuning dataset? Please check if there is a data leakage. Also, could you provide the result on ImageNet, using the first 500 classes for fine-tuning and last 500 classes for evaluation? This can help avoid the data leakage and verify the generalization of the proposed method.\n\n3. Comparison of IAR for belonging and non-belonging with the same VAE. I provide several settings below, but you don’t need to run all of them. Just try one or two, or any other similar configurations you think make sense.\n\n+ Models with difference sizes. For example, VARs have 4 difference sizes, including $d16, d20, d24, d30$. It is possible to compare $d16$ and $d24$\n+ VAR and VAR-CCA [1]\n+ LlamaGen, PAR [2] and LlamaGen-CCA [1]\n+ ……\n\nI would like to verify whether using only the images generated by the AR transformer for fine-tuning the inversion decoder is sufficient to distinguish belonging from non-belonging samples with the same VAE. If this approach proves effective, I would be inclined to raise my score.\n\nFor some other issues, please see \"Weakness\"\n\n[1] Chen, Huayu, et al. Toward guidance-free AR visual generation via condition contrastive alignment. ICLR 2025.\n\n[2] Wang, Yuqing, et al. \"Parallelized autoregressive visual generation.\" CVPR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GvH6DCmXSo", "forum": "qYu4wj7O3z", "replyto": "qYu4wj7O3z", "signatures": ["ICLR.cc/2026/Conference/Submission1563/Reviewer_SetU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1563/Reviewer_SetU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760506942222, "cdate": 1760506942222, "tmdate": 1762915817096, "mdate": 1762915817096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a post-hoc provenance framework for detecting and attributing images generated by Image Autoregressive Models (IARs). The key insight is that IAR-generated images leave characteristic patterns in their token representations due to the quantization process - generated images have feature representations closer to codebook entries than natural images. The authors design two provenance signals: QuantLoss (measuring distance to codebook entries) and EncLoss (measuring encoder-decoder reconstruction error), combined with a finetuned inverse decoder. Experiments on six IAR models (LlamaGen, RAR, Taming, VAR, Infinity, VQ-Diffusion) demonstrate near-perfect detection rates (≈100% TPR@1%FPR) and reasonable robustness to image post-processing.\n\nWhile the empirical results are strong and the problem is timely, the work suffers from limited technical novelty, lacks theoretical depth, and makes strong assumptions (white-box access, per-model finetuning) that significantly limit practical applicability. The core contribution is primarily an engineering adaptation of reconstruction-based detection to the IAR setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses an important and timely problem. As IAR models gain traction for image generation, reliable provenance methods become critical for combating misinformation and ensuring accountability.\n\n2. The experimental evaluation is comprehensive, covering multiple state-of-the-art IAR architectures (next-token, next-scale, random-order prediction) and demonstrating consistently high detection rates across diverse test scenarios.\n\n3. The post-hoc nature of the approach is valuable - no modification to the generation process is required, making it applicable to already-published content.\n\n4. The paper includes thorough ablation studies (Table 3, Table 4) that validate the importance of different components, particularly the decoder inversion step.\n\n5. Robustness evaluation against common image perturbations (JPEG compression, resizing, noise, etc.) is extensive, with augmentation-based training showing improved robustness.\n\n6. The writing is generally clear and the paper is well-organized with comprehensive appendices."}, "weaknesses": {"value": "1. Limited novelty. The core idea of using reconstruction error for detecting generated images is well-established. The main contribution is applying this to IARs by exploiting quantization artifacts. While the domain-specific adaptation is useful, the conceptual advance is incremental. The optimized quantization algorithm (Algorithm 3) for multi-scale IARs is the most novel technical component, but receives insufficient analysis.\n\n2. Strong practical limitations undermine the claimed applicability:\n   - Requires white-box access (encoder E, decoder D, codebook Z) to the target model, which may not be available for commercial or closed-source IARs.\n   - Requires per-model finetuning (10-50 epochs, 10K-50K images) despite being called \"post-hoc\". This is computationally expensive and contradicts the claim of broad applicability to \"already published content\" without additional training data.\n   - High computational cost for some models (VAR: 8.249 sec/image, Table A3) limits scalability.\n\n3. Insufficient theoretical grounding:\n   - Why does finetuning the encoder to invert the decoder improve attribution? The paper provides intuition (Eq. 6) but no theoretical analysis of what properties D^{-1} should satisfy.\n   - Why are QuantLoss and EncLoss \"orthogonal\"? Only empirical observations are provided.\n   - The choice of multiplicative combination in Eq. 10 lacks justification. Why not additive or learned weighting?\n   - No analysis of failure modes or theoretical performance guarantees.\n\n4. Experimental design concerns:\n   - The setting is somewhat artificial: 1000 clean images at 256×256 resolution, binary classification against single sources. Real-world scenarios involve mixed resolutions, multiple processing steps, and multi-class attribution.\n   - Finetuning uses generated images from the same distribution as test images, which may lead to overfitting. Cross-distribution generalization (e.g., different prompts, generation hyperparameters) is not evaluated.\n   - Only single performance metric (TPR@1%FPR) is reported. Full ROC curves and AUC would provide better insight.\n   - No statistical significance testing (error bars, confidence intervals) despite stochastic generation processes.\n\n5. The augmentation-based robustness training has concerning properties. As acknowledged in Appendix H, augmentation training improves QuantLoss but degrades EncLoss performance. This suggests the approach lacks principled robustness and relies on dataset-specific overfitting. The explanation provided (training D^{-1} to \"remove\" augmentation) indicates a fundamental issue with the EncLoss design.\n\n6. Limited baseline comparisons. The paper only compares against LatentTracer (diffusion-specific), AEDR (also diffusion-focused), and naive reconstruction. Comparisons with general deepfake detection methods or universal AI-generated image detectors are missing.\n\n7. The AE attribution vs AR attribution discussion (Appendix I) reveals a conceptual issue: the method attributes to the autoencoder, not the autoregressive model. Multiple AR models sharing the same AE are indistinguishable, which is a significant limitation not adequately addressed in the main paper.\n\n8. Presentation issues:\n   - 24-page appendix is excessive; key results (Table A4, A5) should be in the main paper.\n   - Some claims are overclaimed: \"first post-hoc provenance framework\" ignores reconstruction-based methods like RONAN.\n   - Algorithm 3 lacks critical details (learning rate, convergence criteria, initialization strategy)."}, "questions": {"value": "1. How does the method perform when the test images come from different generation settings (different sampling temperatures, guidance scales, or prompts) than the finetuning data? This would test true generalization ability.\n\n2. Can you provide theoretical analysis or empirical evidence on what properties the inverse decoder D^{-1} learns? For example, does it learn to undo specific decoder operations, or does it learn a direct mapping to the codebook?\n\n3. Have you tested adversarial robustness? An adversary aware of your method could potentially add carefully crafted noise to increase QuantLoss for generated images or decrease it for natural images.\n\n4. For the multi-scale VAR model, how sensitive is Algorithm 3 to hyperparameter choices (number of iterations N_{iters}, learning rate, initialization)? The current description is insufficient for reproduction.\n\n5. Why not learn the combination weights between QuantLoss and EncLoss instead of using a fixed multiplicative combination? A learned weighted sum might be more principled.\n\n6. How does the method perform on images that have undergone multiple sequential transformations (e.g., JPEG compression, then resize, then Gaussian blur)? Table 2 only shows single transformations.\n\n7. Can you clarify the finetuning data generation process? Are images generated with the same prompts/settings as test data, or is there diversity? This is critical for understanding potential overfitting.\n\n8. In Table 1, why does the method achieve 100% TPR@1%FPR for most settings but drops significantly for Infinity vs LAION (85.6%)? What is special about this case?\n\n9. The overhead of finetuning is claimed to be \"relatively small\", but 50 epochs on 50K images is substantial. Can you quantify the total computational cost (GPU hours) and compare it to the cost of training the IAR itself?\n\n10. Have you considered zero-shot or few-shot scenarios where only a small number of generated samples are available for finetuning? This would be more realistic for detecting newly-released or proprietary models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rklHLcx7d8", "forum": "qYu4wj7O3z", "replyto": "qYu4wj7O3z", "signatures": ["ICLR.cc/2026/Conference/Submission1563/Reviewer_891z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1563/Reviewer_891z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761398080706, "cdate": 1761398080706, "tmdate": 1762915816825, "mdate": 1762915816825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a data provenance method for image auto-regressive generation models, based on two carefully designed losses."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1) Their method does not require modifications to the models or images, and is applicable to already released ones.\n1) Their insight of QuantLoss and EncLoss is intuitive.\n1) They propose an optimized algorithm to address multi-scale VAR.\n1) Their experimental results are excellent, achieving 100% success in most cases.\n1) Their writing is clear and fluent."}, "weaknesses": {"value": "1) They state that the idea of the two signals is based on their observations, but there is no quantitative presentation of their observations in the main text.\n1) Lack of the formalization of greedy search, as well as the comparisons with Algorithm 3 in terms of performance and cost.\n1) According to Table A3, the cost of Algorithm 3 is relatively high. Perhaps it should be accelerated.\n\nMinor revisions:\n1) The $D^{-1}$ in Line 253 and the $Q$ in Line 270 should be italic.\n1) What are the $R$ in Line 731 and the $t$ in Line 749?\n1) In Table 1, it is difficult to find the boundary between \"Natural\" and \"Generated\"."}, "questions": {"value": "1) Consider a scenario of adaptive attack, where the model owner is aware of your detector and intends to mislead you. How might he do, and how would you handle it? Could you give some discussion?\n1) Why is the 2-norm used for all distances? Will there be any difference if using other distances?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WvkPkmiu3K", "forum": "qYu4wj7O3z", "replyto": "qYu4wj7O3z", "signatures": ["ICLR.cc/2026/Conference/Submission1563/Reviewer_XMV2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1563/Reviewer_XMV2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921619202, "cdate": 1761921619202, "tmdate": 1762915816661, "mdate": 1762915816661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical need for data provenance in Image Autoregressive Models (IARs). The authors propose the first post-hoc, model-agnostic provenance framework that requires no modifications to IAR training/generation processes. Specifically, the paper proposes two nove signals for data provenance, which are QuantLoss-based and EncLoss-based, respectively. The QuantLoss-based signal compares the original feature map and re-quantized one of the input image, while the EncLoss-based siganl measures the input image and the re-encoded image. The experiments have demonstrated the excellent effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Effectiveness: the proposed method requires no modification to the model’s training or generation process and exhibits distinguished performance conpared to current baselines.\n- Writing: the paper is well-written and easy to follow.\n- Holistic Evaluation: the authors evaluate not only detection accuracy but also robustness."}, "weaknesses": {"value": "- This paper seems to transfer the technique of membership inference to the scenario of data provenance, and the authors need to further clarify the differences between the settings of this paper and those of membership inference.\n- The number of baseline methods used for comparison in the experimental section is relatively few. It is recommended to compare with more baselines, such as [1] and [2].\n- The framework assumes white-box access to the target models. For closed-source IARs, this access may not be available—limiting real-world applicability. The paper does not discuss potential workarounds.\n\n[1] Kowalczuk A, Dubiński J, Boenisch F, et al. Privacy Attacks on Image AutoRegressive Models[C]//Forty-second International Conference on Machine Learning.\n\n[2] Yu H, Qiu Y, Yang Y, et al. ICAS: Detecting Training Data from Autoregressive Image Generative Models[C]//Proceedings of the 33rd ACM International Conference on Multimedia. 2025: 11209-11217."}, "questions": {"value": "Refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c9YJLbtdOd", "forum": "qYu4wj7O3z", "replyto": "qYu4wj7O3z", "signatures": ["ICLR.cc/2026/Conference/Submission1563/Reviewer_Rqvo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1563/Reviewer_Rqvo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990766763, "cdate": 1761990766763, "tmdate": 1762915816528, "mdate": 1762915816528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}