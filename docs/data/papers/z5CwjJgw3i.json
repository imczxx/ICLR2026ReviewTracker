{"id": "z5CwjJgw3i", "number": 10938, "cdate": 1758185105992, "mdate": 1759897619915, "content": {"title": "FactNLI: Dynamic and Automated Fact-Enhanced Augmentation of NLI Benchmarks", "abstract": "Natural Language Inference (NLI) is a core task for language understanding, yet existing NLI datasets are static and no longer challenging, allowing current Large Language Models (LLMs) to perform well without truly revealing their capabilities and shortcomings. To address this problem, we propose a new data augmentation framework to automatically build more challenging NLI datasets based on existing datasets, by iteratively fusing rich facts into the premise and hypothesis of an NLI instance. We use a strict fact filter to ensure that fused facts are non-contradictory and non-redundant. Applied to SNLI and MNLI, our augmentation substantially increases data length and complexity, and the performance of a range of LLMs on the augmented datasets drops significantly (up to 30%). Ablation experiments and human quality checks confirm the high quality of the generated data.", "tldr": "", "keywords": ["Natural Language Inference", "Benchmark Construction"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/04dfba750cafe07726cb85fbaec064bd418d846e.pdf", "supplementary_material": "/attachment/7fd9df34659d087f71de826eccea5bf63ff414ff.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework for automatically augmenting NLI datasets by iteratively fusing verified facts from Wikipedia into premise-hypothesis pairs. The method uses retrieval, filtering, and controlled fusion to create progressively harder instances while preserving original labels. Experiments on SNLI and MNLI show consistent accuracy drops (up to 30%) across multiple LLMs. Human evaluation was also conducted to show the generated data are of high-quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed fact-augmented framework is carefully designed and can be used as a valuable data augmentation method."}, "weaknesses": {"value": "- The motivation for the paper is not sound. The authors framed this work as, because existing benchmarks are saturated, we need a way to dynamically update them. This framing is ok but adding Wikipedia facts to NLI samples seems weird: \n  + First, benchmarks need to reflect real-world usage. If a benchmark is saturated by current models, we need to identify what are the remaining challenges in order to construct a new one. Augmenting the samples with arbitrary facts from Wikipedia move the benchmark away from its intended purpose. Are we evaluating fact-checking capabilities or evaluating the inference relationship in NLI task?\n  + Looking at the augmented samples, it seems that the method just add irrelevant padding. If the goal is just to make the text longer and overload with more information, we can just ask the LLM to rewrite the samples to make them longer.\n  + More importantly, do the retrieved facts actually influent in the inference? The process of fusing and verifying facts is good but does it matter if the facts are not needed to infer the correct label.\n- The paper demonstrates difficulty increase but doesn't analyze what makes augmented instances harder, e.g., length, coherence, too many entities, multi-hop reasoning."}, "questions": {"value": "I would be more convinced about the claims if the author could do experiments to:\n- Show that facts participate in reasoning.\n- Compare with simple expansion baselines, such as simply prompt the LLMs to rewrite the NLI samples to make it longer without changing the label."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t888cvDCTV", "forum": "z5CwjJgw3i", "replyto": "z5CwjJgw3i", "signatures": ["ICLR.cc/2026/Conference/Submission10938/Reviewer_DJoJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10938/Reviewer_DJoJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761029057088, "cdate": 1761029057088, "tmdate": 1762922134794, "mdate": 1762922134794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a well-recognized and critical problem in NLP: the saturation of existing NLI benchmarks like SNLI and MNLI by modern LLMs. The authors argue that the simplicity and static nature of these datasets fail to truly challenge current models and reveal their shortcomings. To this end, they propose FACTNLI, a novel and well-designed framework for automatically augmenting existing NLI instances with verifiable facts from Wikipedia. The core idea is to iteratively retrieve, filter, and fuse external facts into the premise and hypothesis, thereby increasing their length, semantic complexity, and the required reasoning depth, all while preserving the original inference label."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem addressed is timely and of great importance. As LLMs continue to improve, the need for more challenging and dynamic evaluation benchmarks is paramount. This work offers a scalable and principled solution, moving beyond static, one-off dataset collection efforts.\n2. The proposed retrieve-filter-fuse pipeline is well-motivated and technically sound. It provides a robust mechanism to ensure logical consistency and prevent the introduction of contradictory or redundant information, which is a common pitfall of automated data generation. Grounding the augmentation in an external knowledge source (Wikipedia) adds a layer of verifiability that is often missing in purely synthetic generation approaches.\n3. The authors evaluate a diverse set of seven models, spanning different architectures and sizes, which demonstrates the general applicability of their findings. And the multi-level augmentation provides a clear, controllable knob for difficulty, and the results compellingly show a monotonic decrease in performance as complexity increases.\n4. The paper is well-written and easy to follow. Figure 1 provides an excellent intuitive example, and Figure 2 clearly illustrates the overall framework."}, "weaknesses": {"value": "1. The current implementation totally relies on Wikipedia. While Wikipedia is comprehensive, this limits the framework's applicability to specific domains not well-covered by it (e.g., specialized medical or legal texts). A discussion of how the framework could be adapted to other knowledge sources or specific domains would strengthen the paper.\n2. The fusion step uses GPT-4o to compose the new premise and hypothesis. This process may introduce stylistic biases or artifacts characteristic of the generator model. It is possible that models evaluated on this data could learn to exploit these stylistic cues. While the authors separate generation from filtering, a brief analysis of the linguistic style of the augmented text (e.g., using a classifier to distinguish it from human-written text) would be a valuable addition.\n3. The paper demonstrates that models fail more often on the augmented data, but a deeper analysis of why they fail would be more insightful. Are the failures primarily due to the increased context length, or do they stem from an inability to perform multi-fact reasoning? A qualitative or quantitative error analysis categorizing the types of reasoning required (e.g., temporal, spatial, numerical) and where models struggle most would provide a richer understanding of the challenges introduced."}, "questions": {"value": "1. The framework involves many calls to GPT-4o and other models for each sample at each level. Can you provide a brief comment on the computational resources and time required to generate the augmented datasets? This would be helpful for other researchers looking to apply this method.\n2. Have you considered the effect of the retrieved facts' quality? The retrieval step is based on semantic similarity. Is it possible that loosely related or slightly inaccurate snippets are retrieved, and how does the downstream filtering and extraction pipeline handle such noise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8vm7Uhit5N", "forum": "z5CwjJgw3i", "replyto": "z5CwjJgw3i", "signatures": ["ICLR.cc/2026/Conference/Submission10938/Reviewer_ta4B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10938/Reviewer_ta4B"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761186548820, "cdate": 1761186548820, "tmdate": 1762922134344, "mdate": 1762922134344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that traditional NLI benchmarks are no longer challenging leading to iterative development of more challenging benchmarks through (1) adversarial approaches or (2) domain-specific benchmark development. However, both these approaches are time intensive and require careful human annotation. Instead, this paper proposes an automated pipeline to generate challenging benchmarks that can be made more successively more difficult when NLI methods saturate the previous iteration.\n\nStage 0 corresponds to the existing benchmarks (SNLI and MNLI in this paper). Each successive stage adds meaning preserving facts to the premise and hypothesis pair from the previous stage such that the entailment label does not change. Results show that accuracy of models, achieving 80-90% in stage 0, drop by 10-20% with just one iteration of the FactNLI process (length of premises increases 3.5x on average).\n\nFactNLI increases difficulty of the task by adding relevant but distracting factual information to the premise in hypothesis. It applies the following steps to each (premise, hypothesis, label) triplet in the previous iteration of the benchmark.\n1. Retrieve introductory passages from Wikipedia based on the premise (using sentence embedding models)\n2. Extract atomic facts from the passages (using GPT-4o)\n3. Filter our atomic facts that are not neutral with respect to the premise (NLI judged by a small fine-tuned model based on DeBERTa-V3-Base)\n4. Keep the maximal clique of facts that are each pairwise neutral with each other (NLI judged by a small fine-tuned model based on DeBERTa-V3-Base)\n5. Fuse the clique of facts into the premise from the previous stage. Fuse a **subset** of facts from each iteration into the **original** (stage 0) hypothesis. The label is retained as is (using GPT-4o)\n\nAssumptions: GPT-4o can perform atomic fact decomposition and fact fusion without introducing errors. The small NLI model is accurate for judging single sentence hypotheses.\n\nThe quality of the benchmark is validated across several axes (most importantly label accuracy, fusion errors, introduced logical inconsistencies) with a stratified human annotation (60 examples per stage x 3 stages of the final benchmark). Results show that the label is preserved 95% of the time after 3 stages with less than 7% of examples with internal inconsistency."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is demonstrated to cause iterative decrease in accuracy of NLI classifiers\n2. The quality degradation is studied in detail using human annotations\n3. Ablations show the effectiveness of the filtering stage"}, "weaknesses": {"value": "1. (Not a reason to reject) The difficulty introduced is one-dimensional and this method results in a sanity check benchmark (akin to Needle-in-a-haystack for long-context LLMs and Checklists in past NLI benchmarks)\n    - High performance on the benchmark demonstrates the ability of NLI models to handle long noisy context where *most* atomic facts in the premise and hypothesis agree by construction\n2. (Necessary validation to characterize the difficulty) I believe that by design, FactNLI will induce more queries to be predicted as entailed by NLI models. This is because only one atomic fact differs between the contradictory premise and hypothesis. If models cannot detect this one misaligned fact, they are more likely to predict that the entailed label.\n    - A confusion matrix of the errors in each label category can help clarify my hunch\n    - If the authors disagree, can you characterize the errors made by the NLI models in Stages 1, 2, and 3?\n3. (Unclear behavior of neutral examples) It is unclear how the neutral label is maintained after the fact fusion. Can the authors provide examples from the benchmark to explain how neutrality is maintained?\n4. (Missing discussion) Ambiguity is a prevailing issue for NLI models and this discussion is missing in the paper. E.g. AmbiEnt (Liu et al, EMNLP 2023) show that models struggle to capture the ambiguity in human labelers and cannot successfully disambiguate entailment queries with ambiguity.\n    - This is also relevant to the sample discussed in the Appendix D. It can be argued that L1 Hypothesis is no longer entailed by the L1 Premise, because L1 Premise associates the street fashion with the city street while the hypothesis associates the fashion to the woman.\n    - The human annotation is thus necessary to show that such effects are not introduced very frequently"}, "questions": {"value": "Questions are asked in the previous section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yUCKOhjBve", "forum": "z5CwjJgw3i", "replyto": "z5CwjJgw3i", "signatures": ["ICLR.cc/2026/Conference/Submission10938/Reviewer_Yg7r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10938/Reviewer_Yg7r"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762096965145, "cdate": 1762096965145, "tmdate": 1762922133977, "mdate": 1762922133977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FactNLI, an automated framework designed to augment existing NLI benchmarks like SNLI and MNLI. The authors argue that current datasets are too simple and static for modern LLMs. The proposed method iteratively retrieves verifiable facts from Wikipedia and fuses them into the original premise and hypothesis pairs. This process is structured in multiple levels to progressively increase difficulty. A key component is a filtering pipeline that uses a truth set and an entailment graph to ensure fused facts are non contradictory and non redundant, thereby preserving the original NLI label. Experiments show this augmentation significantly increases text complexity and leads to large performance drops for several LLMs. The authors support their method with a human audit that confirms the high quality and label fidelity of the augmented data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The work addresses a well known and important problem. Static benchmarks like SNLI and MNLI are saturated, and model performance on them is no longer a reliable indicator of true language understanding.\n2. The proposed framework is systematic and well designed. The idea of using a multi level augmentation to control difficulty is sensible.\n3. The inclusion of a truth set and graph based filtering mechanism is a crucial strength. It directly tackles the most common failure mode of data augmentation, which is label drift and the introduction of artifacts."}, "weaknesses": {"value": "1. The primary weakness is the contribution's novelty. The framework is essentially a very well executed data generation pipeline. It relies heavily on an existing proprietary model (GPT-4o) for the core tasks of fact extraction and fusion, and on standard NLI models for filtering. This feels more like a strong engineering contribution for dataset creation rather than a novel method for NLI.\n2. The paper does not fully disentangle why the task becomes harder. The augmented examples are up to 10x longer. It is unclear if the performance drop is due to a failure in complex logical reasoning or simply a failure in long context processing and aggregation of (sometimes distracting) facts. The task may be testing retrieval and summarization more than inference.\n3. The framework's goal of strict label preservation seems like a missed opportunity. In a dynamic, fact-centric setting, adding new evidence could and perhaps should logically change the relationship. The current design seems to force the original label, which may not be the most realistic way to test evidence based reasoning.\n\nMinor:\n1. In Figure 1, \"great infulence\" should be \"great influence\".\n2. In Figure 2, there are several misspellings such as \"Retrival\" (Retrieval) and \"infulence\" (influence).\n3. In Section 4.5, the heading \"Reults and Analysis\" should be \"Results and Analysis\"."}, "questions": {"value": "1.The performance drop is significant, but is this because the models must perform deeper inference, or are they simply getting \"distracted\" by the large volume of added facts, which are sometimes only tangentially related to the original premise? How could you separate these two failure modes?\n\n2.Given the heavy reliance on GPT-4o, did you experiment with any open source models for the fact extraction and fusion steps? How much does the quality of the augmentation, particularly label preservation and coherence, degrade when using smaller models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "umdMSXczjK", "forum": "z5CwjJgw3i", "replyto": "z5CwjJgw3i", "signatures": ["ICLR.cc/2026/Conference/Submission10938/Reviewer_6CCZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10938/Reviewer_6CCZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762259390836, "cdate": 1762259390836, "tmdate": 1762922133402, "mdate": 1762922133402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}