{"id": "FPpIE5lCCM", "number": 3398, "cdate": 1757418436453, "mdate": 1759898092239, "content": {"title": "CoMo: Compositional Motion Customization  for Text-to-Video Generation", "abstract": "While recent text-to-video models excel at generating diverse scenes, they struggle with precise motion control, particularly for complex, multi-subject motions. Although methods for single-motion customization have been developed to address this gap, they fail in compositional scenarios due to two primary challenges:  motion-appearance entanglement and ineffective multi-motion blending. This paper introduces CoMo, a novel framework for $\\textbf{compositional motion customization}$ in text-to-video generation, enabling the synthesis of multiple, distinct motions within a single video. CoMo addresses these issues through a two-phase approach. First, in the single-motion learning phase, a static-dynamic decoupled tuning paradigm disentangles motion from appearance to learn a motion-specific module. Second, in the multi-motion composition phase, a plug-and-play divide-and-merge strategy composes these learned motions without additional training by spatially isolating their influence during the denoising process. To facilitate research in this new domain, we also introduce a new benchmark and a novel evaluation metric designed to assess multi-motion fidelity and blending. Extensive experiments demonstrate that CoMo achieves state-of-the-art performance, significantly advancing the capabilities of controllable video generation.  Our project page is at \\url{https://como6.github.io/}.", "tldr": "We introduces CoMo, a novel framework for compositional motion customization in text-to-video generation, enabling the synthesis of multiple, distinct motions within a single video.", "keywords": ["Computer Vision", "Text-to-Video", "Video Motion Customization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a50832a4fe784319090265f134d6716fb88a4f65.pdf", "supplementary_material": "/attachment/e3276be75286ea6e9572fbeba192f1096571e97d.zip"}, "replies": [{"content": {"summary": {"value": "This paper focuses on motion customization from multiple reference videos. The authors propose a two-stage LoRA training methodology to decouple motion and appearance information from the reference frames. Subsequently, they employ a latent composition technique to generate a smooth latent representation under given motion conditions. To quantitatively evaluate multi-motion customization, a new benchmark and a corresponding evaluation metric are also introduced."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clarity of Presentation:** The description of the proposed method is clear and easy to follow.\n    \n2. **Quantitative Performance:** The paper demonstrates strong quantitative results in comparison to the baseline methods."}, "weaknesses": {"value": "1. **Missing Key Information on a Core Contribution:** The paper claims the proposal of a new benchmark as one of its main contributions. However, crucial details regarding this benchmark are absent from both the main paper and the appendix, making it difficult to assess its validity and scope.\n    \n2. **Insufficient Experimental Comparisons:** In the evaluation of compositional motion customization, the paper only compares its method against VACE, which is a zero-shot approach. The experiments would be more comprehensive if they included comparisons with single-motion customization methods that use a simple linear merging of latent codes."}, "questions": {"value": "1. **Object Position Discrepancy:** In Figure 2, there is a noticeable difference in the positions of the woman and the monkey between the results of \"ours\" and the other methods. Could you please elaborate on why linear merging and joint training appears to cause errors in object positioning?\n    \n2. **Impact of Latent Merging on Denoising:** In Section 3.2, the paper merges latent representations via a weighted sum. This operation can alter the variance of the resulting latent code compared to the original distribution. Given that diffusion models are highly sensitive to the latent distribution, could you provide an analysis of how the proposed merging technique affects the original denoising process?\n    \n3. **Generalization to Overlapping Bounding Boxes:** The proposed latent merging method relies on the bounding boxes of characters in the target video. How does this method perform in scenarios where bounding boxes overlap, for instance, when two characters walk past each other, crossing from one side of the frame to the other?\n    \n4. **Clarification of Motion Fidelity Metric:** Could author(s) please provide a detailed description of how \"motion fidelity\" is calculated? While Section 3.3 mentions multiple references for this metric, its specific implementation and formula are not detailed in the paper, which is essential for reproducibility.\n\n5. **Construction of the Benchmark**: Author(s) should provide the detailed information about how the benchmark is constructed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v1jFHRiMeX", "forum": "FPpIE5lCCM", "replyto": "FPpIE5lCCM", "signatures": ["ICLR.cc/2026/Conference/Submission3398/Reviewer_kKY8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3398/Reviewer_kKY8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666975794, "cdate": 1761666975794, "tmdate": 1762916703913, "mdate": 1762916703913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CoMo, a compositional motion customization method for integrating multiple motions into video generation. The approach consists of two phases: a single-motion learning phase where each motion is customized separately, followed by a multi-motion composition phase during inference. The visual results demonstrate the method's capability to combine two or three motions within a single video, though the overall visual quality of the generated videos could be improved. Additionally, the paper proposes a new metric for evaluating multi-motion customization performance. The overall framework is functional, but the visual quality is not satisfactory for a customization method. Overall, I believe this is a borderline paper, and I am willing to see the authors' response."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed two-phase framework successfully synthesizes multiple motions into a single video. With the carefully designed merging strategy, the resulting videos demonstrate good harmonization, particularly in the boundary regions between different motions.\n\n2. The proposed C&C metrics provide a reasonable approach for evaluating multi-motion customization."}, "weaknesses": {"value": "1. The visual quality of the multi-motion customization is not entirely satisfactory given the significant training process might already overfit on a single input video. Additionally, there is insufficient evaluation of the visual quality of the generated videos.\n\n2. The evaluation lacks comparison with other motion-conditional generative models that use motion representations such as human skeletons. I am also curious whether providing those models with merged skeleton sequences would enable them to perform multi-motion transfer effectively."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ITRoWx5vvI", "forum": "FPpIE5lCCM", "replyto": "FPpIE5lCCM", "signatures": ["ICLR.cc/2026/Conference/Submission3398/Reviewer_xkaZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3398/Reviewer_xkaZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761674953796, "cdate": 1761674953796, "tmdate": 1762916703728, "mdate": 1762916703728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CoMo, a novel framework that enables the learning and composition of multiple distinct motions within a single video. The method addresses two major challenges in motion customization: motion-appearance entanglement and multi-motion blending, through a two-phase design. First, a static-dynamic decoupled tuning approach disentangles motion from appearance to learn motion-specific LoRA modules. Then, a plug-and-play divide-and-merge strategy composes these motions spatially during denoising, allowing different subjects to perform distinct actions simultaneously. The authors also propose a new benchmark and evaluation metric (C&C score) for assessing multi-motion fidelity. Experiments demonstrate that CoMo achieves state-of-the-art performance in both single- and multi-motion customization, offering a flexible and training-efficient solution for controllable video generation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well-written and structured, making it accessible to readers with varying levels of expertise in the field.\n2. The motivation and the design of method are both reasonable and innovative.\n3. Qualitative and quantitative results show clear improvements over baselines.\n4. The paper provides thorough experimental evaluations, and all results supports the claim"}, "weaknesses": {"value": "1. It is recommended to enhance the diversity of motion customization scenarios, not only translation, but also rotation and scaling. \n2. From my point of view, the authors should discuss (or compare with, if possible) more existing methods that achieve similar motion customization (both U-Net-based ones and DiT-based ones), including but not limited to:\n    1. MOFT: Video Diffusion Models are Training-free Motion Interpreter and Controller\n    2. MotionClone: Training-Free Motion Cloning for Controllable Video Generation\n    3. VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control\n3. This paper does not specify or thoroughly discuss the evaluation dataset, which may not provide a comprehensive view of CoMo’s effectiveness. Releasing more details of the evaluation dataset or incorporating videos from publicly available benchmarks would greatly enhance the credibility of the paper.\n4. The comparison regarding computational efficiency should be provided."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zELIDZc5b2", "forum": "FPpIE5lCCM", "replyto": "FPpIE5lCCM", "signatures": ["ICLR.cc/2026/Conference/Submission3398/Reviewer_w24q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3398/Reviewer_w24q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834493415, "cdate": 1761834493415, "tmdate": 1762916703531, "mdate": 1762916703531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CoMo, a framework for compositional motion customization in text-to-video generation. The key novelty lies in enabling the synthesis of multiple distinct motions within a single video. The method features two stages: (1) Single-motion learning, which disentangles motion and appearance through a static–dynamic decoupled LoRA tuning scheme. (2) Multi-motion composition, achieved by a plug-and-play divide-and-merge strategy to compose multiple motion patterns during denoising.\n The authors also introduce a new benchmark and a Crop-and-Compare metric to evaluate multi-motion fidelity and blending. Extensive experiments show that CoMo achieves state-of-the-art results over baselines such as MotionDirector, DeT, and DreamBooth."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a two-phase pipeline that effectively separates motion and appearance through sequential LoRA tuning, and demonstrates clear improvements in motion disentanglement compared with joint training baselines. Both quantitative metrics and visual comparisons indicate clear gains in motion fidelity and compositional accuracy. \n2. The introduction of benchmark for compositional motion and the Crop-and-Compare (C&C) metric provides systematic toolset for assessing multi-motion fidelity and blending, which likely have lasting impact for subsequent research in controllable video generation.\n3. This paper is well-structured with intuitive figures. And implementation details are transparently provided in the appendix."}, "weaknesses": {"value": "1. **Limited analysis of scalability complexity.**\n   The paper mainly demonstrates two- to four-motion composition in spatially separated regions. It remains unclear how the method performs when motions overlap or extend to longer temporal durations.\n2. **Benchmark validation is somewhat limited.**\n   While the introduced dataset and C&C metric are valuable, their correlation with human perceptual judgment is not quantified. More discussion or cross-validation with existing metrics would improve confidence in the evaluation.\n3. **Insufficient description and discussion of training and benchmark data.**\n   The paper does not quantize the data composition and diversity for the proposed benchmark, nor specify the scale and source of training videos used in training.  Without clearer dataset statistics and examples, the evaluation's representativeness and training reproducibility remain uncertain."}, "questions": {"value": "1. **Data composition and usage.** \n\n   a) **Training data**: What datasets are used for training the single-motion LoRA modules? Please specify the data sources, scale, as well as whether the videos were curated or filtered in any way.  \n\n   b) **Evaluation data**: Providing dataset statistics (such as the total scale, motion diversity and distribution, ..., in quantitative form) and representative examples would make the benchmark’s coverage and difficulty clearer.\n\n2. **Handling of overlapping or interacting motions.**\n   How does the divide-and-merge mechanism behave when spatial regions partially overlap or when two subjects physically interact?  Is there any mechanism to ensure temporal coherence across motion boundaries? \n\n3. **Analysis about robustness.** \n\n   How robust is CoMo when reference videos differ in viewpoint or temporal length? Section4.1 claims that evaluation data includes \"camera motion\", but it seems that this part was not involved in the subsequent analysis and visualization.\n\n4. **Analysis about the region partitioning**. \n\n   The region partitioning process (dividing global video into several rectangular regions) in the Divide-and-Merge stage seems predefined. Could the authors provide more information about the partition strategy and discuss whether an adaptive or learned partition could improve compositional quality or reduce artifacts at motion boundaries?\n\n5. **Evaluation reliability.**\n   Has the C&C metric been validated through human studies or correlation with existing metrics?\n\n6. **Generalization across base models.**\n\n   Can the learned motion LoRA modules trained on one base model (e.g., Wan) be transferred to another DiT-based backbone?   This would be important to evaluate the modularity claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GH2jy3Nyun", "forum": "FPpIE5lCCM", "replyto": "FPpIE5lCCM", "signatures": ["ICLR.cc/2026/Conference/Submission3398/Reviewer_p3ci"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3398/Reviewer_p3ci"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916589432, "cdate": 1761916589432, "tmdate": 1762916703270, "mdate": 1762916703270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}