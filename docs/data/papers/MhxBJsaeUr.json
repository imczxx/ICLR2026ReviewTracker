{"id": "MhxBJsaeUr", "number": 12344, "cdate": 1758207178511, "mdate": 1759897515810, "content": {"title": "Attention is Advantage: How the Weaker Defeats the Stronger Through Cooperation", "abstract": "The phenomenon of weaker groups overcoming stronger opponents through cooperation in nature has inspired our exploration of cooperative-competitive mechanisms in multi-agent systems. In this work, we investigate emergent coordination policies in asymmetrical confrontations, focusing on how weaker agents collectively counter stronger opponents. The challenge of modeling such intricate interplay with a multilayer perceptron led us to adopt a Transformer architecture, which excels at capturing the complex, dynamic relationships between agents. We develop a two-phase curriculum training, with an attention-based strategy that effectively addresses policy training challenges in high-dimensional state spaces, and construct a scalable arena task that validates their effectiveness. Motivated by the Transformer's cooperative advantage, we utilize integrated gradients to attribute the contribution of attentions for each action dimension—thereby bridging attentions and behaviors and revealing how attentional dynamics scaffold collective superiority. This research provides a paradigm and an analytic approach for distributed collaboration in mixed adversarial systems.", "tldr": "", "keywords": ["interpretability and explainable AI", "multiagent learning", "reinforcement learning", "adversarial learning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/50ff9338fa71dc7f33c27c0ec20de29373293260.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores cooperative-competitive mechanisms in multi-agent systems where weaker agents collectively counter stronger opponents. The authors propose a two-phase curriculum learning framework and also leverage transformer-based policy network. They construct a task involving intra-team collaboration and inter-team confrontation, and evaluate the effectiveness of their method. They also conduct some analysis on integrated gradients to attribute the contribution of attention."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written. The methods and the experiment details are explained clearly.\nThe curriculum learning framework is simple yet effective. The investigation on integrated gradients also suggests the explanability of utilizing attention-based architecture of policy network."}, "weaknesses": {"value": "1. One limitation I found is that the proposed curriculum learning method is highly specialized for the bug-ant task proposed in this paper. This makes it unclear whether such a learning strategy is generally applicable to other domains. I expect to see a general algorithm with less human prior knowledge. From this perspective, the current method is relatively restricted.\n\n    Besides, the transformer-based policy architecture seems not novel, which has been proposed and studied for several years.\n\n2. The title seems a bit biased. The authors claim that both curriculum learning and the transformer archiecture are contributions, but the title only highlight the advantage of attention. Besides, I'm not sure whether it is appropriate to use the terms \"weaker group\", \"weaker defeats stronger\", etc, since if it the \"weaker\" can defeat \"stronger\" in some cases, literally, it is not weaker?"}, "questions": {"value": "It is unclear to me why the authors choose to study \"weaker groups\" overcome \"stronger ones\", and why it is an important topic. Especially, the proposed method in the paper seems to work for general games involving cooperation within teams and competition between teams."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fIpGdAa2n3", "forum": "MhxBJsaeUr", "replyto": "MhxBJsaeUr", "signatures": ["ICLR.cc/2026/Conference/Submission12344/Reviewer_MfZZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12344/Reviewer_MfZZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760973304431, "cdate": 1760973304431, "tmdate": 1762923262047, "mdate": 1762923262047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers multi-agent reinforcement learning problems with two opposing team where one team is larger but with weaker individual members than the other team. The primary focus of learning is to find strategies that can effectively collaborate within the team to beat the other team. The authors focus on a locomotion arena setup with ants and bugs where the goal of the teams is to push the other out of a shrinking circle around a center point.\nThe authors highlight three main contributions:\n1. Transformer-based policy network that can efficiently learn and attribute actions between the team members\n2. 2-stage curriculum learning\n3. Attribution method to interpret actions\nTheir empirical results highlight the effects of the first two points through ablation studies while provide explanations of actions using the attribution framework for certain snapshots of games."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "I found the following points to be compelling of the paper\n1. Interesting problem to consider since this is a mixed heterogeneous system with both competitive and cooperative dynamics\n2. The authors combine insights from both learning and interpretability\n3. They provide an extended discussion of the experimental results to highlight their contributions"}, "weaknesses": {"value": "I had a hard time following the paper and might have missed certain points but I found the following to be weaknesses\n1. The writing is often imprecise making it hard to follow the paper.\n* Notations are not defined such as $\\Delta w_{i,j}^{(s,n)}$, $w_{i,j}$, $obs$, orf the difference between $k$ and $\\hat{k}$.\n* In Section 4.3.3, they introduce \"Stage 1\" as an Integrated Gradient method without providing citation or background on this method. As someone not familiar with this method, I could not tell if this is a novel method or an adaptation of already existing one.\n* It is only described in Section 5 that the goal of the game is to push the opponent out of a shrinking circle. It is only mentioned in previous sections that agents can be eliminated. This would have made understanding the problem and the definition of the reward function in the curriculum learning easier.\n* Many discussions (e.g. in Section 4.3.2) are deferred to the Appendix that would be crucial to understand the paper as a whole.\n* Most figures are small to read when the paper is printed or have little contrast in black-white.\n2. The methods and problem statement seem to be general yet the authors constrain the problem to a single environment that leads questioning the generality of the results.\n3. The interpretation method seems to be limited to models with a single attention layer and specific setup as shows on Figure 3. While such network seems to work well on the problem the authors consider, it is not clear whether this scales to larger problem with more agents or larger state/action spaces.\n4. The curriculum used for learning is tailor-made for the specific problem limiting the generality of results."}, "questions": {"value": "I have the following questions to the authors to clarify the contribution and results of the paper\n\nProblem Setting (Section 3)\n1. What are the observations? The problem statement poses states while in later sections the paper refers to observations. Do agents have full observations about the states and/or actions of all other agents?\n2. Are the states and actions continuous or discrete?\n3. Do agents in the same team use the same policy or different? The formulation suggests that the share a policy but the problem statement states that they have different action spaces which might not be compatible with a shared policy\n4. What is the notion of optimality in this case? Is the desired outcome a Nash Equilibrium or some other optimality?\n\nLearning Method (Section 4)\n1. What are the intuitions behind the reward functions defined in the curriculum? The authors state the reward functions but I miss some explanation why they choose them.\n2. What embeddings are used in the transformer architecture? It is depicted on Figure 3 but not discussed.\n3. What do the following notations denote: $o, \\hat{o}, obs, \\Delta w$\n\nExperiments (Section 5)\n1. The curriculum learning was ablated with MLP policies which later results show are clearly limited. Have the authors ablated curriculum learning with Transformer policies too? That would provide a much stronger evidence for the curriculum method.\n2. What is the composite reward function used in the non-curriculum training?\n3. Are all policies trained simultaneously regardless of the team?\n4. Figure 5 only shows rewards for the Ants. What are the rewards for the Bugs?\n5. Could the authors provide some evidence that the training has converged after 1000 epochs? For example on the second figure in Figure 5 shows that the orange line just started shifting when the training finished.\n6. How were the PPO hyperparameters tuned and why did the authors decide to use PPO for a multi-agent setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "d3RuILu15q", "forum": "MhxBJsaeUr", "replyto": "MhxBJsaeUr", "signatures": ["ICLR.cc/2026/Conference/Submission12344/Reviewer_h6CL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12344/Reviewer_h6CL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761219794844, "cdate": 1761219794844, "tmdate": 1762923261762, "mdate": 1762923261762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies multi-agent reinforcement learning problem with heterogeneous agents that engage in both cooperative and competitive coordination. It proposes a curriculum learning framework that progressively trains agents from basic coordination to complex adversarial scenarios. The approach uses a two-stage training pipeline with a multi-agent transformer model: Stage 1 employs a dense reward to help agents develop stable inward-directed locomotion, while Stage 2 introduces a target-alignment reward that encourages adversarial pursuit and team coordination. To enhance interpretability, the paper performs a post-hoc three-stage attribution analysis that traces causal influence from input observations through attention mechanisms to action outputs. The proposed method outperforms baseline approaches using conventional dense and sparse rewards, and ablation studies show that curriculum-trained agents achieve better coordination and more emergent behaviors. Transformer-based agents also demonstrate consistent superiority over MLP-based architecture."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper is well written and provides a clear and detailed overview of heterogeneous multi-agent coordination learning in both adversarial settings.\n2.\tThe proposed method introduces a posthoc interpretable pipeline that maps attention to actions, latent vector dimensions to attention weights, and attention scores to observation features, offering valuable insight into which factors drive agent decision-making and emergent behaviors.\n3. The visual demonstrations effectively show how Ant agents coordinate for team success over individual survival, realigning dynamically even after one agent is eliminated to continue competing against stronger opponents.\n4.\tThe results show that curriculum learning with a simple reward design can outperform classical methods that rely on more complex, domain-specific reward shaping, highlighting the efficiency of the approach."}, "weaknesses": {"value": "1.\tWhile the idea is conceptually interesting, the paper does not clearly state what specific research question it seeks to investigate, whether it is about improving training stability, understanding attention-driven cooperation, or demonstrating emergent coordination, making the main objective somewhat ambiguous.\n\n2.\tSome of the demo videos on the project webpage aren’t accessible. Please ensure that all linked demos are working and clearly associated with the corresponding figures or results.\n3.\tWhile the idea of curriculum learning is interesting, the current evaluation is quite limited, focusing only on simple scenarios with a few heterogeneous agents. Although the setup covers three configurations (2A–1B, 3A–1B, 3A–2B), they are all variants of the same arena task with hand-designed rewards (Stage-1: centering; Stage-2: target alignment). The claims would be stronger if the authors (a) tested task reversals (e.g., staying outside while pushing opponents inward), (b) explored different adversarial objectives, and (c) demonstrated whether Stage-1 pretraining transfers to qualitatively different tasks. The current experimental settings leave the generality unclear.\n\n4.\tStage-2 heatmaps highlight different dominant latent feature indices across configurations (e.g., in 3A–2B: features 26, 33, 58 for w₁₁, 11 and 26 for w₂₂, 26 and 58 for w₃₃; in 3A–1B: 29, 44, 60 for w₁₁ and 17 for w₂₂ and w₃₃). What do these differences signify? Are these indices stable up to permutation, or do they represent different functional roles or episode phases? A quantitative correlation or overlap analysis across configurations and episode stages, along with mapping to Stage-3 observation semantics, would make the interpretability much stronger.\n\n5.\tAll reported populations are small (up to 3 Ants vs. 2 Bugs), even though the text claims the setup “can be generalized to m vs. n.” Evaluating larger or mixed-size teams would test whether cooperative patterns and attention-based attribution signals scale effectively or degrade. \n\n6.\tThe paper compares curriculum learning (Stage-1 followed by Stage-2) against a non-curriculum composite reward baseline (dense + sparse shaping). A stronger baseline would be to train the non-curriculum baseline using both curriculum rewards (Stage-1 and Stage-2) simultaneously via a weighted sum or an annealing schedule. This would help isolate whether the improvement comes from the curriculum phasing itself or simply from combining multiple reward objectives."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AeJKkuOpk9", "forum": "MhxBJsaeUr", "replyto": "MhxBJsaeUr", "signatures": ["ICLR.cc/2026/Conference/Submission12344/Reviewer_BqUo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12344/Reviewer_BqUo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762221684768, "cdate": 1762221684768, "tmdate": 1762923261394, "mdate": 1762923261394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}