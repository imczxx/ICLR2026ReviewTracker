{"id": "Uvm5AvGe5l", "number": 7908, "cdate": 1758042415149, "mdate": 1759897823121, "content": {"title": "Uncovering Challenges of Solving the Continuous Gromov-Wasserstein Problem", "abstract": "Recently, the Gromov-Wasserstein Optimal Transport (GWOT) problem has attracted the special attention of the ML community. In this problem, given two distributions supported on two (possibly different) spaces, one has to find the most isometric map between them. In the discrete variant of GWOT, the task is to learn an assignment between given discrete sets of points. In the more advanced continuous formulation, one aims at recovering a parametric mapping between unknown continuous distributions based on i.i.d. samples derived from them. The clear geometrical intuition behind the GWOT makes it a natural choice for several practical use cases, giving rise to a number of proposed solvers. Some of them claim to solve the continuous version of the problem. At the same time, GWOT is notoriously hard, both theoretically and numerically. Moreover, all existing continuous GWOT solvers still heavily rely on discrete techniques. Natural questions arise: to what extent existing methods unravel GWOT problem, what difficulties they encounter, and under which conditions they are successful. Our benchmark paper is an attempt to answer these questions. We specifically focus on the continuous GWOT as the most interesting and debatable setup. We crash-test existing continuous GWOT approaches on different scenarios, carefully record and analyze the obtained results, and identify issues. Our findings experimentally testify that the scientific community is still missing a reliable continuous GWOT solver, which necessitates further research efforts. As the first step in this direction, we propose a new continuous GWOT method which does not rely on discrete techniques and partially solves some of the problems of the competitors.", "tldr": "", "keywords": ["Optimal Transport", "Gromov-Wasserstein", "Generative Modelling", "Benchmark"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/42ceff7d17b186453b994e027f306d72bfa8096f.pdf", "supplementary_material": "/attachment/56423d81f97ce330692ca6b53dc72d1b8be36585.zip"}, "replies": [{"content": {"summary": {"value": "In this paper the authors investigate the challenges of solving continuous\nGromov-Wasserstein (GW) problems from discrete high dimensional data. The\nauthors first do a quick review of the existing literature on GW problems and\nhighlight the different strategies using discrete+regression or Neural networks\nto solve them. They discuss their limits, in particular the challenge of finding\na good GW mapping when there is no one-to-one correspondances in the data\n(denoted as uncorrelated data splitting) and devise a benchmark to compare the\nmethods on words embeddings with bilingual vocabularies (which provides them\nwith a ground truth mapping). The benchmark show teh limits of the existing\nmethods whose accuracy greatly decreases with alpha the proportion of\n\"correlated\" data. Then they propose an Neural GW solver that requires to solve\na minimax optimization problem. They show that their method outperforms the\nexisting ones on their benchmark especially for low \"correlation\"  and large\nscale dataset when some competitors are failing."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "+ This paper addresses an important and challenging problem of solving continuous\n  Gromov-Wasserstein problems from high dimensional data. \n+ The paper is well written and easy to follow, the positioning in the literature\n  is clear and the proposed benchmark is well motivated.\n+ The question of \"correlated\" vs \"uncorrelated\" data is very relevant in\n  practice and the benchmark proposed is a good contribution to the community.\n+ The proposed method is novel and shows interesting empirical results on the\n  benchmark.\n+ The benchmark illustrates well the limits of existing and proposed methods with\n  no clear winner in all settings which is a good sign of a well designed and\n  honest benchmark.\n+ I really appreciated the clarity of the writing and the quality of the\n  scientific steps followed in the paper. Asking a question and experimenting on\n  it followed by a reasonable contribution is refreshing compared to many papers\n  that just throw a new method at the wall and see if it sticks."}, "weaknesses": {"value": "Note that these weaknesses are minor and do not impact my overall positive\nopinion of the paper.\n\n+ The choice of the word \"correlation\" to denote overlapping of the support of\n  the true aligned samples is not well chosen since it is universally used in\n  statistics for something different. Perhaps \"overlap ratio\" would be a better \n  wording.\n+ While the benchmark is interesting, it is limited to word embeddings and\n  bilingual vocabularies. It would be interesting to see how the methods\n  perform on other types of data and tasks, possibly simulated with known\n  solution.\n+ The choices of performance measures in Fig 4 could be better. All Top-k\n  accuracy basically look the same whereas other \"marginal quality\" measures\n  are provided in the appendix. It would be better to have a more diverse set of\n  measures in the main text.\n+ The comparison in the large scale setting to NeuralGW is a little unfair since\n  other solvers rely on subsampling (minibatches) that are known to be biased wrt\n  the full GW solution. Existing reference on minibatch OT show that there is a\n  bias induced by minibatching that could be de-biased (see e.g. \"Minibatch\n  optimal transport distances; analysis and applications\").\n+ The proposed NeuralGW woks clearly better than existing methods in the low\n  correlation setting but is is actually not as good as competitors trained on a\n  much smaller sample size when the correlation is high (alpha=0.9). It is\n  necessary to discuss this point a bit more in the paper."}, "questions": {"value": "+ Could you please address the weaknesses mentioned above?\n\n+ The Topk accuracy measures seem to nearly constant wrt K. It means that only\n  the closest points are (relatively) well mapped and the others are not? Could\n  you please comment on that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jkLqSsuWaH", "forum": "Uvm5AvGe5l", "replyto": "Uvm5AvGe5l", "signatures": ["ICLR.cc/2026/Conference/Submission7908/Reviewer_wskS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7908/Reviewer_wskS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664064049, "cdate": 1761664064049, "tmdate": 1762919934825, "mdate": 1762919934825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on establishing a benchmark for neural Gromov–Wasserstein (GW) problems. The authors identify key existing works addressing this task and propose a methodology for benchmarking these methods.\nThe main observation is that most neural GW papers operate in a correlated regime, which tends to favor their reported performance, and are (naturally) influenced by the batch size that can be processed. The proposed benchmark explicitly varies the level of correlation and evaluates the robustness of different methods under these conditions.\nIn addition to this benchmarking framework, the authors introduce a new neural GW approach in the inner-product setting, derived from a duality theorem. This method is designed to be more robust to correlation effects."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Overall, I find the paper well written. It is dense, but the problem is clearly formulated. The related work section is also well presented: the methods in Section 3 are described concisely yet with enough detail to understand their scope and relevance.\nIdentifying the correlated dataset setting is, in itself, an interesting and valuable contribution.\nI find the experimental results in Section 4.2 particularly compelling, as they demonstrate that the presence of a natural pairing or correlation plays a crucial role in achieving good performance for neural GW solvers."}, "weaknesses": {"value": "Overall impression:\n\nAlthough the contributions are interesting, I find that several aspects of the paper remain unclear. The main message is somewhat blurred by a set of experiments that sometimes appear contradictory. At first, the paper suggests that the main challenge for neural GW methods lies in the level of correlation in the data; later, however, the issue seems to shift toward their inability to handle large-scale problems.\nThis mixture of factors within the experiments makes the overall conclusion difficult to interpret, leaving the reader somewhat frustrated by the lack of a clear and unified takeaway message.\n\nUnclear points:\n\n- About \"Limitations of existing methods\"\n\nOne of the main contributions of the paper is to show that, in the correlated regime, existing neural GW methods tend to be favored. More precisely, the correlated setting corresponds to random variables $(X, Y) \\sim (i_d \\times \\sigma) \\cdot \\pi$, where $\\sigma$ is a permutation and $\\pi$ a coupling.\n\nHowever, the train/test procedure related to this definition is not entirely clear. From what I understand, the “correlated” regime essentially corresponds to the “paired” regime — that is, a setting where there exists a natural alignment or pairing between samples.\nTo “break” this pairing, one can simply rematch the data points, which is indeed what the proposed procedure does. Yet, such manipulation is only feasible on synthetic datasets or on data where the natural pairing is explicitly known (e.g., word-pair datasets in embedding problems). I think it would be worth emphasizing that this procedure can only be applied in these specific cases.\n\n- About AlignGW:\n\nIt is stated that “we train a Multi-Layer Perceptron on the barycentric mapping,” but the barycentric mapping can in principle be defined in both directions (source to target or target to source). Which direction is used here, and why was this specific choice made?\n\n- A propos des resultats 4.2:\n\n(a) Why were not all baselines included in Figure 3?\nIncluding all methods would help make the comparison more comprehensive.\n\n(b) The description of the matching metrics is somewhat confusing or hard to follow. What exactly is meant by the reference pool and reference space? A bit more formalism here would help clarify what these objects represent. Similarly, the notion of “optimal pair” could be defined more precisely.\nThe same applies to the marginal metrics: it seems they refer to divergences between $\\mathbb{Q}$ and $T(\\mathbb{P})$, but an explicit equation would make this clearer.\n\n- About section 5.2:\n\nThe main remark here is that Figure 4 is very difficult to read. The results are not visually clear, and the curves are hard to distinguish.\nWhere are regGW and FlowGW in the first subfigures? Who are the “other methods”?\nThis is unfortunate, as this figure seems central to the paper’s conclusions, but its current presentation makes it difficult to interpret.\n\n\n- About the proposed method:\n\nIt is not clear to me why the proposed method should be more robust to correlation levels. Do the authors have any intuition on this ?\n\n- Other remarks:\n\nThe citation style is somewhat unconventional and could be standardized to match common academic formatting.\nAlso consider citing [1] a very recent paper on the subject.\n\n[1] Unsupervised Learning for Optimal Transport plan prediction between unbalanced graphs, Sonia Mazelet, Rémi Flamary, Bertrand Thirion, NeurIPS, 2025."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iZTtkP6fqh", "forum": "Uvm5AvGe5l", "replyto": "Uvm5AvGe5l", "signatures": ["ICLR.cc/2026/Conference/Submission7908/Reviewer_kxyJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7908/Reviewer_kxyJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757710273, "cdate": 1761757710273, "tmdate": 1762919934463, "mdate": 1762919934463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper has a twofold purpose: to provide a benchmark for Gromov–Wasserstein (GW) solvers, and to propose a new continuous Monge–GW method that does not rely on discrete techniques. Regarding the first goal, the authors report that existing solvers perform well when correlated data is available but fail in the absence of correlation. To address this limitation, they introduce a new methodology (NeuralGW), which is designed to handle fully uncorrelated setups and is claimed to be practically useful in more realistic and challenging scenarios. However, this new solver requires large amounts of training data, and other remaining challenges leave the door open for further research."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper highlights the limitations of existing GW solvers: in the interplay between discrete and continuous approaches, current methods are said to rely more heavily on the empirical (discrete) side rather than the continuous perspective, which, in practice, should treat data as i.i.d. samples."}, "weaknesses": {"value": "- In general, the presentation is not clear, making it difficult to fully understand the core problem.\n\n- Five existing GW methods are listed in Section 3, but only three of them are analyzed in Section 4.2.\n\n- Although the authors reference the seminal paper by Dumont et al. on the existence of Monge-GW assignments, the distinction between finding an optimal coupling/plan and finding an optimal map (i.e., the Kantorovich vs. Monge formulations) is blurred.\n\n- The quality of Figure 2 is poor; it appears blurry.\n\n- The beginning of Section 4.1 (lines 222–246) is unclear.\n\n- The proposed solver appears to have several drawbacks."}, "questions": {"value": "- I suggest formalizing what is meant by a “continuous setup.”\n\n- In the classical OT problem, the Kantorovich formulation turns the optimization into a linear program, making it more tractable than the Monge formulation. Could the authors elaborate on how this comparison translates to the GW setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n6HC6V9Bfk", "forum": "Uvm5AvGe5l", "replyto": "Uvm5AvGe5l", "signatures": ["ICLR.cc/2026/Conference/Submission7908/Reviewer_vEr5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7908/Reviewer_vEr5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774202699, "cdate": 1761774202699, "tmdate": 1762919934152, "mdate": 1762919934152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The Gromov-Wasserstein distance is a metric commonly used in practice to compare two different metric measure spaces due to its nice geometric interpretation of computing the most isometric map between the spaces. While nice for its mathematical properties, the Gromov-Wasserstein distance is quite challenging to compute. Existing methods for computing the Gromov-Wasserstein distance assume discrete metric measure spaces, and therefore fail to capture the underlying maps between continuous distributions that samples come from. To extend the Gromov-Wasserstein distance to the continuous setting, algorithms then rely on a correlated sample between the two distributions.\n\nThe authors of this work experimentally suggest that existing methods for computing Gromov-Wasserstein distances between correlated samples, i.e. samples where the optimal pairs have some underlying association in the metric measure spaces, fail to extend to the continuous Gromov-Wasserstein distance when discrete samples are taken i.i.d. from the two distributions. They then design a neural network which computes continuous Gromov-Wasserstein distances without relying on discrete instances like in prior works and empirically verify that their neural network outperforms existing methods for computing continuous Gromov-Wasserstein distances."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The introduction to the Gromov-Wasserstein problem was pretty well written and easy to follow.\n\nThe overview of existing algorithms for continuous Gromov-Wasserstein distance in section 3 was very useful and also well written.\n\n The authors clearly describe a problem with existing algorithms for the Gromov-Wasserstein distance and design a neural network which outperforms existing algorithms."}, "weaknesses": {"value": "The conclusion \"existing algorithms therefore don't work for uncorrelated data\" from Section 4 cannot be made as a general statement. In particular, their experiments were conducted on only two data sets with very similar types of data (word to vector embeddings) and no proof is provided to justify the strong claims they make about existing work. To my understanding, provable guarantees of existing algorithms for Gromov-Wasserstein distance are lacking, i.e. existing algorithms serve as merely heuristics already. Why should one expect that the results  from these two data sets to extend to other data sets on Euclidean spaces? Why could it not be an issue with the structure of the word embedding data tested?\n    \nThe authors note on line 464 that the performance of their neural network is \"inconsistent with respect to initialization parameters\" and \"sees high standard deviation among repetitions\"."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cx69wdaXoX", "forum": "Uvm5AvGe5l", "replyto": "Uvm5AvGe5l", "signatures": ["ICLR.cc/2026/Conference/Submission7908/Reviewer_hWa4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7908/Reviewer_hWa4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762053576508, "cdate": 1762053576508, "tmdate": 1762919933763, "mdate": 1762919933763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}