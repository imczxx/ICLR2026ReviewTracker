{"id": "ffYd6uJpJE", "number": 3857, "cdate": 1757556319246, "mdate": 1759898065996, "content": {"title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "abstract": "The Model Context Protocol (MCP) has emerged as a transformative standard for connecting large language models (LLMs) to external data sources and tools, and it is rapidly gaining adoption across major AI platforms. However, existing benchmarks are overly simplistic and fail to capture real-world application challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs on realistic and difficult tasks through interaction with real-world MCP servers. Our benchmark spans 6 core domains and 11 different MCP servers: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching. To ensure rigorous evaluation, we carefully design execution-based evaluators, including format evaluators for compliance, static evaluators for time-invariant content matching, and dynamic evaluators that automatically obtain real-time ground truth for temporally sensitive tasks. Through extensive evaluation of more than 20 leading LLMs, we find that even frontier models such as GPT-5-High (44.16% success rate) and Grok-4 (33.33% success rate) exhibit significant performance limitations. In addition, our benchmark poses a substantial long-context challenge, as the number of input tokens increases rapidly with each additional interaction step. It also introduces an unknown-tools challenge, since LLM agents often lack familiarity with the precise usage of certain MCP servers. Notably, enterprise-level agents such as Cursor and Claude Code fail to achieve better performance than the ReAct framework. Beyond evaluation, we open-source our extensible evaluation framework, enabling seamless integration of new LLMs, agents and MCP servers.", "tldr": "We introduce MCP-Universe, the first comprehensive benchmark for evaluating LLMs on realistic, complex tasks through interaction with real-world MCP servers, revealing major performance gaps even in frontier models.", "keywords": ["MCP", "LLMs", "Agents", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7840b547636f6eeae820084eaacaad729aa37610.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MCP-Universe, a benchmark designed to evaluate Large Language Models (LLMs) on complex, realistic tasks using real-world Model Context Protocol (MCP) servers. The authors created this benchmark to address the limitations of existing evaluations, which they argue are too simplistic and fail to capture real-world challenges such as long-horizon reasoning and interaction with large, unfamiliar toolsets.\n\nThe benchmark consists of 231 tasks across 6 domains: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching.\nIt utilizes 11 live MCP servers, including Google Maps, GitHub, and Yahoo Finance, which provide 133 distinct tools for the models to use.\nTo ensure rigorous and objective results, the framework uses execution-based evaluators instead of relying on an \"LLM-as-a-judge\" approach. These are divided into format, static, and dynamic evaluators that can check for compliance, time-invariant content, and real-time ground truth.\nThe evaluation framework is open-source, allowing researchers to seamlessly integrate new LLMs, agents, and MCP servers.\n\nEven top-performing models like GPT-5-High and Grok-4 achieved low success rates of only 44.16% and 33.33%, respectively. This indicates a major gap between the general capabilities of LLMs and their effectiveness in real-world MCP environments.\nSpecialized, enterprise-level agents like Cursor and Claude Code did not achieve better performance than the standard ReAct framework.\nA model's performance can be highly sensitive to its interaction paradigm. For example, the success rate of GPT-OSS-120B increased dramatically from 11.26% to 25.54% when switching from the OpenAI Agent SDK to a direct function-calling approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's primary strength lies in its originality. It introduces MCP-Universe, which it posits as the first comprehensive benchmark for evaluating LLMs in real-world Model Context Protocol (MCP) environments. While other MCP-related benchmarks exist, this work distinguishes itself by moving beyond simplistic or derivative tasks (like adapting existing math or code datasets) and focusing on live, authentic MCP servers.\n\nThe research is of high quality, demonstrated by its methodological rigor. The benchmark's design is comprehensive, covering 6 diverse domains and 11 real-world MCP servers. A key strength is the development of a robust, execution-based evaluation framework that avoids the known biases of \"LLM-as-a-judge\" methods. By implementing distinct format, static, and dynamic evaluators, the authors ensure objective and reproducible assessments, which is especially critical for tasks involving time-sensitive data. The extensive experiments, testing over 20 leading LLMs with different agentic frameworks, further underscore the work's thoroughness.\n\nThe paper is exceptionally clear and well-structured. The authors effectively motivate the work by articulating the precise limitations of prior benchmarks. The architecture of the MCP-Universe framework is explained logically and supported by clear diagrams that illustrate the workflow from task configuration to evaluation. Results are presented in accessible tables and figures that cleanly summarize key performance metrics and comparisons across models and domains. The central arguments and contributions are stated explicitly, making the paper easy to follow and its conclusions straightforward to understand.\n\nThe work is highly significant as it addresses an urgent need within the AI community. With MCP rapidly becoming a standard for integrating LLMs with external tools, a reliable benchmark is crucial for measuring progress. The paper's findings are impactful, revealing that even state-of-the-art models like GPT-5-High (44.16% success rate) and Grok-4 (33.33% success rate) struggle significantly, highlighting that real-world tool use remains a major challenge. By identifying critical failure points such as long-context handling and unfamiliar tool usage, the research provides clear and actionable directions for future work in agent architecture and model development. The open-sourced, extensible framework is a valuable resource that can catalyze further research and standardization in the field."}, "weaknesses": {"value": "The tasks were \"manually designed\" by the authors. While curated for difficulty, this approach risks a lack of diversity in the underlying problem structures within each domain. For instance, several Location Navigation tasks focus on finding optimal stopping points or equidistant locations. This may not fully represent the breadth of real-world geographic reasoning tasks. A more systematic approach, perhaps incorporating tasks derived from real user queries or logs from applications that use these tools, could ensure greater diversity and real-world representativeness.\n\nThe methodology explicitly discards tasks that can be \"consistently completed within five retries\". While this ensures the benchmark is challenging, it may also filter out the more common, simpler tasks that form the bulk of real-world agent interactions. This selection bias could skew the benchmark's results, making it an assessment of performance on \"hard cases\" rather than a representative measure of general utility. Including a separate track for more common, moderate-difficulty tasks would provide a more balanced perspective.\n\nThe primary metric is a binary success rate (SR), which treats a near-perfect solution that fails on a minor detail the same as a complete failure. The paper includes an \"average evaluator score\" (AE), but the relationship between partial success (high AE) and outright failure (low SR) is not deeply analyzed. For example, the paper notes that Claude-4.0-Sonnet achieves a higher AE than Grok-4 but a lower SR. A deeper analysis of why this occurs—such as identifying specific \"killer\" evaluators that cause catastrophic failure—would be highly instructive.\n\nThe evaluation focuses on correctness and the number of steps but overlooks crucial efficiency metrics like API costs, total token usage, or latency. In practical applications, an agent's cost-effectiveness and speed are as important as its ability to complete a task. An agent that succeeds but requires an exorbitant number of tokens or expensive tool calls may be impractical. Incorporating efficiency metrics would make the benchmark more aligned with real-world deployment constraints.\n\nThe paper identifies broad challenges like \"long context\" and \"unknown tools\". However, it would be more impactful to provide a detailed, quantitative breakdown of error types. For instance, when a model fails, is it due to incorrect tool selection, faulty parameter generation, misunderstanding the tool's output, or flawed multi-step planning? While a specific error example is given for Yahoo Finance, a systematic error analysis across all domains would provide more actionable insights for improving model reasoning and agent design.\n\nThe experiments primarily compare the ReAct framework with a native function-calling mode. While these are foundational, the field of autonomous agents includes other prominent paradigms like planning-and-solving or reflection-based methods. Testing a broader range of agent architectures on the benchmark would provide a more complete picture of which reasoning structures are best suited for complex, real-world tool use and why."}, "questions": {"value": "Questions:\n\nCould you elaborate on the methodology used to ensure task diversity and real-world representativeness, beyond simply curating for difficulty? Was there a systematic framework guiding task creation to avoid potential homogeneity?\nHow does filtering out easier tasks (solvable within five retries) affect the benchmark's generalizability? Does this focus on \"hard cases\" risk misrepresenting a model's typical real-world utility?\nYou note that some models achieve high partial scores (AE) but low final success rates (SR). Could you provide a deeper analysis of this discrepancy? Are specific evaluation steps consistently causing catastrophic failures?\nCould you provide a quantitative, systematic breakdown of the most common error types (e.g., incorrect tool selection, faulty parameter generation, planning failures) for the top-performing models across all domains?\nWere efficiency metrics such as API costs, token usage, or latency tracked during experiments? If so, could you discuss their implications, as these are critical for real-world deployment?\n\nSuggestions:\nConsider reporting on the distribution of partial scores (AE) for failed tasks. This could offer a more nuanced view of model capabilities by showing which models get \"closer\" to a correct solution.\nThe paper would be strengthened by including experiments with a wider range of agentic frameworks beyond ReAct and function calling, such as planning- or reflection-based agents, to provide a more comprehensive analysis of which reasoning structures excel.\nTo maximize the benchmark's impact, please consider releasing detailed setup instructions and containerized environments (e.g., Docker files) for the custom MCP servers, which would significantly lower the barrier for other researchers to reproduce and build upon your work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Urc94Umb8G", "forum": "ffYd6uJpJE", "replyto": "ffYd6uJpJE", "signatures": ["ICLR.cc/2026/Conference/Submission3857/Reviewer_eTRd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3857/Reviewer_eTRd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3857/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760600855685, "cdate": 1760600855685, "tmdate": 1762917069531, "mdate": 1762917069531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a realistic benchmark for evaluating LLM agents with real MCP servers. The benchmark covers 6 everyday domains and 231 real-life tasks. Evaluation is human-written, rule-based, and execution-driven (format/static/dynamic), avoiding LLM-as-judge hallucination. Extensive experiments on current LMs show limited MCP-use and task completion capability. The authors further analyze long-context effects and irrelevant/unknown-tool issues, with ablations (e.g., summarization) revealing mixed improvements across domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The benchmark is timely and well designed: it uses real MCP servers and real-world, multi-turn agentic tasks rather than simulators.\n2. The rule-based evaluation is solid and labor-intensive, avoiding typical LLM-as-judge pitfalls.\n3. The evaluation dimensions are clear—domains and format/static/dynamic—and the study evaluates a broad set of models, yielding clear, differentiated results.\n4. The exploratory analyses are useful, including long-context growth, unknown-tool misuse, and the impact of irrelevant servers. Letting the model do a few turns of exploration is a good perspective.\n5. The writting is clear."}, "weaknesses": {"value": "1. Insufficient analysis.\na) Lack of systematic error analysis. The paper would benefit from a structured taxonomy of failure causes across models and domains, with representative error cases. Format errors are only one category; others such as include tool selection, parameter filling, state tracking/memory failures. Such analysis would clarify the specific capability deficits and limitations of current models.\nb) Unknown-tools exploration lacks case studies (Sec. 4.3). A focused set of interaction traces would help distinguish “tool unfamiliarity” from “API/argument misuse” or poor timing, and would ground the quantitative trends.\n\n2. Metric coverage is somewhat limited. The primary emphasis on success rate omits other useful axes. Consider reporting cost (tokens/API calls), test time, performance stratified by task difficulty/trajectory length, and known vs. completely unseen tools. These would make the results more actionable for both research and deployment.\n\n3. Prompt/MCP definition sensitivity. Different models may favor different prompt formats and tool-call schemas (e.g., GPT vs. Claude). It is unclear whether the authors tried alternative prompt templates or MCP definition variants to control for such biases. A brief sensitivity study—or at least documenting the chosen formats and any model-specific tuning—would strengthen fairness and interpretability."}, "questions": {"value": "1. Questions in the weakness section.\n2. Please report general testing time and cost, at least for one or two models.\n3. Sec. 4.3 Long-context challenges: Are there failures purely due to over-long context? In Finance you argue fine-grained details are critical, yet GPT shows little change with/without summarization—how should we interpret this contrast with the big gains in Location Navigation?\n4. Sec. 4.3 Unknown tools challenges: Why isn’t added exploration consistently helpful? In theory a few exploratory turns can be view as few shots for the tool and it should help model understand.\n5. Which MCP servers are public/known vs. previously unseen to the models? This distinction could clarify tool-use generalization."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SIu6FT8Oqm", "forum": "ffYd6uJpJE", "replyto": "ffYd6uJpJE", "signatures": ["ICLR.cc/2026/Conference/Submission3857/Reviewer_Wvi6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3857/Reviewer_Wvi6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3857/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760895420744, "cdate": 1760895420744, "tmdate": 1762917069196, "mdate": 1762917069196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers” introduces a comprehensive benchmark to evaluate large language models (LLMs) in realistic, tool-connected environments via the Model Context Protocol (MCP). Unlike prior works that rely on simulated tool-use or GUI interactions, MCP-Universe connects directly to 11 real-world MCP servers across six domains — including Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching — encompassing 231 tasks. The benchmark evaluates models based on execution outcomes using three evaluator types: format, static, and dynamic evaluators. Experiments across over 20 state-of-the-art proprietary and open-source LLMs reveal that even top models achieve moderate success rates highlighting substantial gaps in long-horizon reasoning, unfamiliar tool usage, and multi-turn context handling. The paper positions MCP-Universe as the first rigorous, extensible testbed for assessing real-world MCP performance and promoting research on robust LLM-agent design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a well-motivated and timely contribution that addresses a clear gap in LLM benchmarking — the absence of realistic, execution-based evaluation in real-world MCP contexts. The benchmark design is conceptually coherent and technically grounded: by using authentic servers such as Google Maps, GitHub, and Yahoo Finance, the authors ensure genuine interaction complexity and avoid the artificial constraints of GUI-based or synthetic environments. The inclusion of diverse evaluator types (format, static, dynamic) demonstrates methodological rigor and reduces reliance on subjective “LLM-as-a-judge” paradigms, enhancing reproducibility and fairness. The experimental coverage is broad and systematic, spanning 20+ leading LLMs and multiple agent architectures (ReAct, function calling, enterprise agents), producing a rich quantitative landscape of LLM performance limitations. The paper also provides insightful diagnostic analyses — identifying long-context, unknown-tool, and tool-overload challenges — and introduces preliminary mitigation strategies (e.g., summarization and exploration phases) supported by empirical results. Overall, the work is clearly structured, experimentally thorough, and highly relevant to both academic and industrial AI research on model-agent interoperability."}, "weaknesses": {"value": "the paper lacks statistical robustness i.e. results are presented as raw success rates without standard deviations, error margins, or multiple-run variance, leaving uncertainty about consistency. next, although it identifies critical challenges like long-context failure and tool misuse, the paper does not provide conceptual or theoretical analysis explaining why existing architectures fail — the discussion remains empirical and descriptive rather than explanatory. Similarly, mitigation strategies such as summarization and exploration phases are treated as brief ablations rather than systematically evaluated modules, despite strong quantitative results, the paper misses qualitative depth — it does not include concrete reasoning transcripts or interaction traces showing how model behaviors differ across success and failure cases."}, "questions": {"value": "check weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qlzJQhuEBw", "forum": "ffYd6uJpJE", "replyto": "ffYd6uJpJE", "signatures": ["ICLR.cc/2026/Conference/Submission3857/Reviewer_ooGm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3857/Reviewer_ooGm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3857/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761534779755, "cdate": 1761534779755, "tmdate": 1762917068796, "mdate": 1762917068796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MCP-Universe, a comprehensive benchmark designed to evaluate large language models (LLMs) interacting with real-world tools through the Model Context Protocol (MCP). As MCP rapidly gains adoption across major AI platforms, the benchmark addresses a critical gap in evaluating model performance beyond static text reasoning. It encompasses 231 tasks across six diverse domains such as Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching using 11 actual MCP servers. The benchmark features a rigorous, execution-based evaluation framework with format, static, and dynamic evaluators that verify results through real API responses rather than subjective judgments. Extensive experiments across more than twenty leading LLMs reveal that even frontier models such as GPT-5-High (44.16% success rate) and Grok-4 (33.33%) struggle considerably, exposing fundamental challenges like long-context reasoning, handling unfamiliar tools, and cross-domain generalization. Overall, the paper is timely, ambitious, and addresses an emerging need in the field of agentic AI evaluation.\n\nThe paper provides a highly valuable benchmark that the community will benefit from, especially given the rising importance of MCP-based tool ecosystems. Its engineering quality, clarity, and evaluation rigor are commendable, and it establishes an essential foundation for future research on grounded LLM agents. However, the current version emphasizes system construction and empirical reporting more than analytical or methodological advancement. The experimental results convincingly show that existing models perform poorly, but the paper stops short of offering deep insights or principled pathways for improvement.\n\nIn its current form, the work reads as an excellent benchmark and resource paper rather than a full research paper meeting the methodological innovation bar of a top-tier venue. Strengthening the causal analysis of failure modes, expanding the discussion on agent architectures, and providing statistically robust evaluations would significantly elevate its impact. Nonetheless, MCP-Universe is a well-executed and necessary step toward standardized, real-world evaluation of LLM agents, meriting serious consideration and likely strong influence in future benchmarking research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The work’s main strength lies in its timeliness and originality. MCP-Universe is the first benchmark to evaluate LLMs in realistic MCP settings using actual servers rather than simulated environments, making it highly relevant to the evolving AI ecosystem. The benchmark’s breadth and comprehensiveness, spanning multiple domains and including over two hundred tasks demonstrate significant engineering effort and clear understanding of real-world complexity.\n\nThe evaluation methodology is robust and thoughtfully designed. By replacing the increasingly common “LLM-as-a-judge” paradigm with execution-based evaluators, the authors achieve greater objectivity and reproducibility. The clear delineation between format, static, and dynamic evaluators ensures rigorous assessment across both static and time-varying tasks.\n\nThe paper also excels in clarity and presentation. The problem is well motivated, the related work positioning is thorough, and the figures and tables illustrate the framework and results effectively. The findings especially the gap between model sophistication and real world reliability are clearly articulated and carry high significance for researchers developing tool using agents."}, "weaknesses": {"value": "Despite its strong contribution as a benchmark, the paper’s methodological depth is limited. Its experimental analysis primarily employs standard agent frameworks such as ReAct and basic function calling, offering limited insight into why models fail. While the benchmark surfaces key challenges like long-context reasoning and unfamiliar tool usage, the subsequent analyses of these issues are descriptive rather than diagnostic. The mitigation attempts (summarization and exploration phases) are simple prompt-level strategies with mixed or inconclusive outcomes, lacking principled reasoning or theoretical grounding.\n\nThe work would also benefit from deeper failure-mode analysis for example, distinguishing between failures of reasoning, planning, or format compliance and from more detailed justification of certain design choices such as the binary success metric and task difficulty calibration. Additionally, the uneven domain distribution (e.g., heavier weighting toward Web Searching) and absence of human-performance or variance analyses raise questions about benchmark balance and statistical robustness.\n\nThese issues do not undermine the benchmark’s value but limit the paper’s standing as a research contribution advancing methodology rather than as a high-quality resource paper."}, "questions": {"value": "Could the authors provide a more detailed breakdown of failure types e.g., incorrect tool selection, parameter misuse, or reasoning lapses and clarify how much each contributes to the overall error rate?\n\n\nHow are dynamic evaluators validated for accuracy when external APIs or data sources change over time?\n\n\nHave the authors assessed whether some tasks might overlap with training data for large proprietary models, given that MCP servers and examples are public?\n\n\nWould more advanced agent architectures those incorporating planning, reflection, or persistent memory alter the conclusions about LLM limitations on MCP-Universe?\n\n\nHow might benchmark difficulty be calibrated with respect to human or expert-agent performance to contextualize the reported success rates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ocBU4IS1Uw", "forum": "ffYd6uJpJE", "replyto": "ffYd6uJpJE", "signatures": ["ICLR.cc/2026/Conference/Submission3857/Reviewer_ciBT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3857/Reviewer_ciBT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3857/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993086904, "cdate": 1761993086904, "tmdate": 1762917068558, "mdate": 1762917068558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}