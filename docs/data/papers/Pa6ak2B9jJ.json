{"id": "Pa6ak2B9jJ", "number": 23650, "cdate": 1758346778457, "mdate": 1759896803177, "content": {"title": "Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models", "abstract": "Automated red-teaming has emerged as an essential approach for identifying vulnerabilities in large language models (LLMs). However, most existing methods rely on fixed attack templates and focus primarily on individual high-severity flaws,limiting their adaptability to evolving defenses and their ability to detect complex, high-exploitability vulnerabilities. To address these limitations, we propose AUTO-RT, a reinforcement learning framework designed for automatic jailbreak strategy exploration, i.e., discovering diverse and effective prompts capable of bypassing the safety restrictions of LLMs. AUTO-RT autonomously explores and optimizes attack strategies by interacting with the target model and generating crafted queries that trigger security failures. Specifically, AUTO-RT introduces two key techniques to improve exploration efficiency and attack effectiveness: 1) Dynamic Strategy Pruning, which focuses exploration on high-potential strategies by eliminating highly redundant paths early, and 2) Progressive Reward Tracking, which leverages intermediate downgrade models and a novel First Inverse Rate (FIR) metric to smooth sparse rewards and guide learning. Extensive experiments across diverse white-box and black-box LLM settings demonstrate that AUTO-RT significantly improves success rates (by up to 16.63%), expands vulnerability coverage, and accelerates discovery compared to existing methods.", "tldr": "We propose Auto-RT, a reinforcement learning framework that efficiently uncovers diverse and complex LLM vulnerabilities by optimizing attack strategies with early termination and progressive reward tracking, outperforming prior methods by 16.63%.", "keywords": ["red-teaming", "llm safety", "attcking", "jailbreak"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed2bc4d97b2f4cdc389825a8958ba5cf09fbee30.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents an automated red team testing framework named AUTO-RT, which aims to automatically explore and generate attack strategies that can bypass the security defenses of large language models (LLMs) through reinforcement learning, thereby identifying and exploiting vulnerabilities within the models. Unlike conventional approaches, AUTO-RT does not rely on fixed attack templates or manually designed strategies. Instead, it leverages a reinforcement learning framework to automatically generate diverse attack strategies within a multidimensional attack space."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes effective strategy exploration, which deviates from traditional methods that rely on fixed and manually designed prompts. Instead, AUTO-RT employs a reinforcement learning-based framework to explore a variety of jailbreaking strategies. This enables it to discover not only efficient but also diverse attack strategies, thereby expanding the attack surface and transcending the limitations of static methods.\n2. AUTO-RT operates effectively in both white-box and black-box settings. It does not depend on the internal structure of the model, thus demonstrating high generalizability across various scenarios."}, "weaknesses": {"value": "1. The paper fails to delve into the defense strategies of the targeted models and their adaptability, especially in black-box settings. There is a lack of sufficient case analysis or detailed explanation, particularly concerning models with stronger defense mechanisms. It remains unclear whether AUTO-RT truly possesses sufficient competitiveness in such scenarios.\n2. Although the paper proposes sophisticated methods for generating and optimizing attack strategies, it lacks detailed descriptions of the implementation and application of these strategies. How are the outputs of the high-level strategy generation model translated into effective attacks? The specific strategy generation process and the optimization of each step are not clearly demonstrated. To improve, the authors could conduct relevant case analyses.\n3. Despite discussing the effectiveness of AUTO-RT, the paper does not adequately control various variables in the experimental environment. For instance, could AUTO-RT's performance vary due to changes in the training set at different training stages? The existing comparative experiments do not demonstrate comprehensive control over all potential variables, leading to conclusions that lack sufficient persuasiveness. The authors could set up more ablation experiments to further substantiate the effectiveness of AUTO-RT."}, "questions": {"value": "1. The authors may first address the issues raised in the Weaknesses section.\n2. The RL experiments in the paper are based on training with PPO. Have the authors tried other methods for RL? If not, could they provide an explanation or conduct experimental analyses with other methods?\n3. The introduction of the FIR metric is indeed innovative, and the authors have demonstrated in Figure 4 that the FIR metric can improve performance. I wonder if the authors have tried other metrics? Are there any cases in the experimental results where the FIR metric is not applicable?\n4. Although the goal of DSP is to accelerate the search process, this means that it abandons some attack paths that may seem useless early on but could show value later. Could this premature pruning strategy cause the model to overlook some potential vulnerabilities, thereby affecting the final attack effectiveness? Although the experimental results of AUTO-RT are still satisfactory, the pros and cons of this early pruning strategy are worth discussing in the paper, and the authors should pay more attention to this point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V5hfuanXcq", "forum": "Pa6ak2B9jJ", "replyto": "Pa6ak2B9jJ", "signatures": ["ICLR.cc/2026/Conference/Submission23650/Reviewer_qWc4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23650/Reviewer_qWc4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23650/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761401400051, "cdate": 1761401400051, "tmdate": 1762942747543, "mdate": 1762942747543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AUTO-RT, a reinforcement learning framework for automatically searching for jailbreak strategies. It aims to boost the efficiency and success rate of adversarial policy discovery by incorporating Dynamic Strategy Pruning and Progressive Reward Tracking. The authors claim that AUTO-RT overcomes the inefficiency and scalability issues of traditional manual templates, while also addressing the limited policy search space of other automated red-teaming methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well, I've got to say, this work puts forward a promising and scalable paradigm for jailbreaking. While I'm not exclusively an expert in this filed, the challenges the authors claim to address are definitely valid and well-motivated. I'm on board with the problem statement.\n- I was particularly impressed by the Progressive Reward Tracking technique. It tackles the challenge of extremely sparse success signals by using a downgraded model to create a smoother reward landscape, rather than relying on a binary success/fail signal. I found this approach quite refreshing.\n- The experimental evaluation is quite convincing. The authors have included a broad set of up to 18 models, even if they are primarily open-source or partially open-source."}, "weaknesses": {"value": "- I'm a bit skeptical about the claim of \"narrow, predefined strategy sets\" in previous automatic methods. Expanding the strategy search space has become a key consensus in the jailbreaking community that . For instance, recent works like CL-GSO [1] and GPTFuzzer  [2] have explored similar directions using evolutionary strategies to optimize jailbreak policies in an scalable search space. Notably, CL-GSO also introduces a non-sparse (4-fold) reward signal. I strongly encourage the authors to discuss how their work relates to and differs from these types of methods.\n- Building on my first point, the paper would be much stronger if it included these more recent, evolution-based methods [1] in the baseline comparisons. The current baselines are relatively dated, making it difficult to truly assess whether AUTO-RT represents a state-of-the-art advancement.\n- Effectiveness on commercial models? While including 72B LLMs, I'm still curious about the method's performance against top-tier commercial models with much stricter safety mechanisms, like the latest versions of Claude or Gemini. How would it fare in those scenarios?\n- What about the framework's complexity and computational cost? The whole pipeline for AUTO-RT seems quite intricate, involving multiple models (AMg, AMr, TM, TM'). I suspect the computational overhead could be substantial, potentially limiting its scalability. Of course, this might be a minor nitpick, but I think a more thorough discussion of this limitation would be beneficial.\n- Are there any case studies? I couldn't seem to find any concrete examples of the jailbreak prompts generated by AUTO-RT and the corresponding harmful responses from the target models, either in the main paper or the appendix. Showcasing a few examples is pretty crucial for evaluating the coherence and plausibility of the generated jailbreaks.\n\n[1] Huang et.al, Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space, ACL 2025\n[2] Yu et.al., Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts."}, "questions": {"value": "Overall, I believe this is a solid piece of work with valuable contributions. I'm open to being convinced otherwise and would appreciate the authors' response to the weaknesses I've pointed out. I also welcome a strong rebuttal. My final recommendation will also take into account the opinions of the other reviewers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QVWVHH5egO", "forum": "Pa6ak2B9jJ", "replyto": "Pa6ak2B9jJ", "signatures": ["ICLR.cc/2026/Conference/Submission23650/Reviewer_BnSr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23650/Reviewer_BnSr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23650/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826762009, "cdate": 1761826762009, "tmdate": 1762942747320, "mdate": 1762942747320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations of current automated red-teaming methods for Large Language Models (LLMs), which often rely on fixed templates and focus on high-severity flaws while neglecting exploitability. The authors propose AUTO-RT, a novel reinforcement learning (RL) framework that shifts from discovering individual jailbreak prompts to exploring generalizable jailbreak strategies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method shows improvement of ASR and diversity compared to simple baselines such as RL, few-shot in-context learning, and imitation learning.\n\n- Testing on 18 models from diverse families (Llama, Vicuna, Gemma, Qwen, etc.) provides strong evidence of the method's generalizability."}, "weaknesses": {"value": "- I do not agree with the authors' claim that most of previous methods rely on a predefined set of toxicity behavior $\\mathcal{T}$. For example, [1] trains a language model to generate diverse attack prompts without any such set. It is design choice of whether we train a RL policy conditioned on the toxic behavior $t\\in\\mathcal{T}$ or fully unconditional policy. Moreover, the proposed method still requires the predefined set $\\mathcal{T}$, which might limit exploration to other toxic behavior.\n\n- The motivation of using the rephrasing model is unclear. If we optimize only the strategy generation model, then the strategy generation overly gets penalized some strategies where the rephrasing model struggles to attack the target model. \n\n- How to get downgraded models is missing, which seems to be critical component of the proposed method. In the paper, it simply says they are obtained by fine-tuning or in-context learning.\n\n- How do we choose one downgraded model among $n$ models is unclear. As far as I understand, the FIR metric is a function of $k$, $n$ different models, and the prompt set $\\mathcal{A}$. Based on my understanding, it does not tell which model is better. \n\n- The proposed method requires extra computation due to introduction of the rephrasing model and downgrading the target model. \n\n\n\n## References\n\n[1] Lee, Seanie, et al. \"Learning Diverse Attacks on Large Language Models for Robust Red-Teaming and Safety Tuning.\" The Thirteenth International Conference on Learning Representations.\n\n[2] Wang, Ren-Jian, et al. \"Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models.\" arXiv preprint arXiv:2506.07121 (2025)."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "moEXE3A24W", "forum": "Pa6ak2B9jJ", "replyto": "Pa6ak2B9jJ", "signatures": ["ICLR.cc/2026/Conference/Submission23650/Reviewer_NmbX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23650/Reviewer_NmbX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23650/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927974157, "cdate": 1761927974157, "tmdate": 1762942746963, "mdate": 1762942746963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}