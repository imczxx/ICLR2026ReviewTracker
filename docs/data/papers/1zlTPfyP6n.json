{"id": "1zlTPfyP6n", "number": 17795, "cdate": 1758280575423, "mdate": 1759897153353, "content": {"title": "FedARC: Adaptive Residual Compensation for Data and Model Heterogeneous Federated Learning", "abstract": "Federated learning (FL) enables multiple clients to collaboratively train models without sharing private data, but practical FL is hindered by both data heterogeneity and model heterogeneity. Existing Heterogeneous FL (HtFL) methods often suffer from inadequate representation alignment and limited knowledge transfer, especially under fine-grained distribution shifts, thus limiting both personalization and generalization. To address these challenges, we propose FedARC, a novel HtFL framework with Adaptive Residual Compensation. FedARC adaptively fuses local and global representations through a trainable projector and applies dynamic residual correction to mitigate feature-level distribution mismatches. Moreover, FedARC incorporates semantic anchor alignment to further reduce inter-client feature divergence, thereby stabilizing knowledge transfer and aggregation. We theoretically prove FedARC converges with a non-convex convergence rate $O(1/T)$. Extensive experiments on four public benchmarks demonstrate that FedARC consistently outperforms nine state-of-the-art HtFL baselines, achieving superior accuracy while maintaining efficient communication and computation.", "tldr": "", "keywords": ["Federated learning", "Distributed Machine Learning", "Deep Learning Algorithms"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d5d06f7cc9b45d7e9fe1ffefad4ae2d182a3a86c.pdf", "supplementary_material": "/attachment/e75d3fb0c214390896765545e44653fda5392621.zip"}, "replies": [{"content": {"summary": {"value": "FedARC is a Heterogeneous Federated Learning (HtFL) framework that enhances both personalization and generalization by adaptively compensating for feature mismatches between clients. It fuses local and global representations through a trainable projector, applies dynamic residual correction, and aligns semantic anchors to stabilize knowledge transfer. Experiments on four benchmarks show consistent improvements over state-of-the-art baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Empirical results across various benchmarks demonstrate that the proposed method outperforms baselines.\n- Model heterogeneity in federated learning is a practical scenario that reflects real-world heterogeneity."}, "weaknesses": {"value": "- Lack of explanation in the overview figure (Fig. 2): Since Fig. 2 lacks sufficient explanation in the caption, it is difficult to understand the overall workflow of the proposed method.\n\n- Unrealistic data heterogeneity: In real-world scenarios, each client may specialize in distinct benchmarks even when performing the same task (e.g., classification). For example, client 1 may focus on ImageNet-1K, while client 2 focuses on CIFAR-10. Although splitting client data using a Dirichlet distribution is a common approach, evaluating the proposed method under more realistic data heterogeneity would better demonstrate its robustness.\n\n- Limited model diversity: While the paper includes various architectures (e.g., ResNet18/34/50/101/152 and MobileNet), these are all relatively simple convolutional models. To validate the generalizability of the proposed method, it would be beneficial to include experiments with heterogeneous ViT-based architectures (e.g., ViT-Base, ViT-Large).\n\n- Lack of detailed ablation study: Table 3 only shows the performance gap between with and without components. To strengthen the motivation of each component, a more detailed analysis is required. For example, what is the effect of applying residual? \n\n- Marginal improvements in large-scale benchmarks: As shown in Fig. 5, FedARC achieves only marginal improvements over the baselines on DomainNet. Since DomainNet is the largest benchmark used in the paper (with 345 classes), this raises concerns about the scalability of the proposed method. Moreover, additional experiments on larger-scale benchmarks would further strengthen the claim of scalability."}, "questions": {"value": "- Applying MSE loss can encourage anchor alignment, but it will not ensure that $\\bar{R}_i^{F_k}$ captures client-specific means as mentioned in L244–L246. Could you clarify this point?\n\n- Did you use the same hyperparameters listed in Table 4 across all benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W8hCyUjRsp", "forum": "1zlTPfyP6n", "replyto": "1zlTPfyP6n", "signatures": ["ICLR.cc/2026/Conference/Submission17795/Reviewer_74tW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17795/Reviewer_74tW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760506568360, "cdate": 1760506568360, "tmdate": 1762927639575, "mdate": 1762927639575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FedARC, a framework for heterogeneous federated learning (HtFL) that aims to handle both data and model heterogeneity. To mitigate dual heterogeneity, this work introduces adaptive residual compensation to fuse local and global feature representations using a trainable projector, and semantic anchor alignment to reduce inter-client feature divergence. In addition, the authors claim a theoretical convergence rate of O(1/T) and report consistency improvements across multiple datasets compared with several HtFL baselines under different heterogeneous settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper covers a broad range of related works and situates itself reasonably within the HtFL literature.\n\n2. The experimental section includes comparisons on multiple datasets and settings (cross-silo and cross-device).\n\n3. Ablation studies are provided, and communication/computation costs are analyzed quantitatively."}, "weaknesses": {"value": "1. The paper appears to contain some technical flaws. It assumes that splitting two feature extractors will inherently cause one to learn client-specific information and the other to capture global shared features. This assumption is speculative and not backed by theoretical analysis or empirical evidence. Without such justification, the proposed “adaptive residual compensation” lacks conceptual grounding.\n\n2. The definition and formulation of the residual term are unclear. The paper does not specify what the residual actually represents or how it is computed. It seems to be introduced as a learnable correction term, but the optimization process and its influence on convergence are not explained.\n\n3. The novelty of the work is limited. Both residual adjustment and feature mean alignment are standard techniques in related areas such as domain adaptation and prior HtFL studies (e.g., FedProto, FedTGP, FedGH). \n\n4. The reported performance gains are modest, and some absolute accuracy values are lower than commonly observed on these benchmarks. This raises concerns about the implementation details or the fairness of the experimental setup."}, "questions": {"value": "1. How can you verify that the two feature extractors indeed learn distinct (client-specific vs. shared) representations? Any quantitative evidence, such as similarity or diversity analysis?\n\n2. What exactly are the “residual vectors” in Eq. (6)? Are they parameters, differences between outputs, or additional modules?\n\n3. How sensitive is the method to the choice of $\\lambda$, $\\kappa$, and other hyperparameters? Can you report standard deviations over multiple runs?\n\n4. The “semantic anchor alignment” looks similar to prototype or mean alignment in existing works (e.g., FedProto, FedTGP). What is the essential difference?\n\n5. The convergence analysis seems largely adapted from FedAvg. How does it account for the coupled updates between heterogeneous branches and the projector?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9h5QJ0DaDj", "forum": "1zlTPfyP6n", "replyto": "1zlTPfyP6n", "signatures": ["ICLR.cc/2026/Conference/Submission17795/Reviewer_phMb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17795/Reviewer_phMb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843067605, "cdate": 1761843067605, "tmdate": 1762927638724, "mdate": 1762927638724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates both data heterogeneity and model heterogeneity in Federated Learning (FL), focusing on two well-motivated challenges: representation alignment and knowledge transfer. To address these issues, the authors propose FedARC, a framework that integrates local and global knowledge through adaptive residual compensation. The experimental evaluation is comprehensive, and the method achieves state-of-the-art performance. Additionally, the inclusion of a convergence proof strengthens the theoretical soundness of the approach. However, the paper omits comparisons with several relevant baselines that also aim to balance global and local model optimization, which somewhat limits the completeness of the evaluation. Overall, this is a solid work with clear motivation. If the authors can provide convincing clarifications and stronger experimental comparisons in the rebuttal, I would be inclined to raise my score."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-The paper clearly articulates the problem it aims to address and effectively identifies the key challenges associated with both data heterogeneity and model heterogeneity in FL. This clear problem framing provides a strong foundation for the proposed solution.\n\n-The paper proposes a novel FL framework, FedARC, which effectively fuses global and local knowledge to address data and model heterogeneity issues. This design provides a potential approach to improving local-global collaboration among heterogeneous clients.\n\n-The paper provides a convergence proof, enhancing the theoretical soundness of the proposed algorithm. The experiments comprehensively evaluate both model and label heterogeneity, demonstrating the framework’s effectiveness under diverse federated settings."}, "weaknesses": {"value": "-Although the paper aims to address data heterogeneity, it focuses primarily on label skew while neglecting other important aspects such as domain heterogeneity. The main experiments (main table) do not include evaluations on domain-skewed settings. Furthermore, since the authors acknowledge potential challenges under distribution shifts, it would be important to explicitly consider and analyze domain shift to strengthen the completeness of the study.\n\n-The figure captions are overly brief, making it difficult for readers to understand the intended message of the visualizations. Each figure should include clear explanations of symbols, notations, and key observations or conclusions to help readers interpret the results more effectively.\n\n-The global–local structure adopted in this paper is not novel, as similar designs have been explored in several prior works [1, 2, 3, 4]. The primary novelty of this paper seems to lie in the proposed semantic anchor alignment mechanism rather than in the overall framework design. However, the experimental section omits comparisons with key related methods that also address knowledge transfer between global and local models [1] and knowledge fusion strategies [2]. Including these two baselines would provide a more convincing evaluation of the proposed method’s contribution and effectiveness.\n\n[1] Fed-CO2: Cooperation of Online and Offline Models for Severe Data Heterogeneity in Federated Learning. (NeurIPS 2023)\n\n[2] On Bridging Generic and Personalized Federated Learning for Image Classification. (ICLR 2022)\n\n[3] Cd2-pFed: Cyclic Distillation-Guided Channel Decoupling for Model Personalization in Federated Learning. (CVPR 2022)\n\n[4] Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels. (ICCV 2023)"}, "questions": {"value": "-How does the proposed model perform under FL scenarios that involve both domain shift and model heterogeneity?\n\n-I think a key limitation of local–global structured methods is their limited ability to adapt to newly joined clients. In real-world federated learning scenarios, scalability is crucial since new clients continuously emerge. How does the proposed method handle or adapt to newly arriving clients without retraining the entire model?\n\n-How does the distinct difference between data heterogeneity and model heterogeneity show in your method? From the current presentation, the proposed design appears primarily aimed at addressing data heterogeneity, while the specific challenges and treatment of model heterogeneity are not thoroughly discussed. Could the authors clarify how their method explicitly tackles model heterogeneity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mM9WeO7gQL", "forum": "1zlTPfyP6n", "replyto": "1zlTPfyP6n", "signatures": ["ICLR.cc/2026/Conference/Submission17795/Reviewer_Ct8h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17795/Reviewer_Ct8h"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761853065505, "cdate": 1761853065505, "tmdate": 1762927638252, "mdate": 1762927638252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies heterogeneous federated learning (HFL) with aim to address the issues of the inadequate representation alignment and limited knowledge transfer with the current HFL methods, especially under distribution shifts, for improving personalization and generalization. The idea is to maintain two feature extractors on each client, one for global features common in all clients and the other for local features specific to the client. The two part of features are concatenated and then projected by a learnable projector for knowledge fusion. To correct distribution shifts within and across clients, concatenated features are compensated with residual vectors, and fed fully into local prediction header and partially into global prediction header. To mitigate semantic shift, the two prediction losses are regularized by the MSE error between the averaged local/global features on the client and a global consensus. The final loss is a weighted combination of the two losses, and is optimized locally. The global feature extractor on each client after optimization is sent to server for the aggregation over all clients which is the broadcast to all clients. The resulting algorithm is shown to converge at a rate of O(1/T). Extensive experiments are conducted to show the effectiveness and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Identify issues with existing HFL methods that affect personalization and generalization\n2. Project the concatenation of local and global features on each client for knowledge fusion\n3. Propose adaptive residual compensation to address the distribution shift\n4. Anchor each client's overall feature mean to a global semantic anchor to reduce semantic shift"}, "weaknesses": {"value": "1. It is hard to identify key differences between conventional HFL and the proposed one from Figure 1\n2. It is unknown why residual vectors could reduce distribution shifts. Residual vectors are learned, but they are missing in Eq. (12)\n3. It is unknown mean feature vectors in Line 247 are averaged over Eq. (4) or Eq. (5), also there is no definition for $\\bar{\\mathcal{Z}}\\_{k}^{g}$\n4. Presentation could be improved for better readability"}, "questions": {"value": "1. In Line 198, what's $\\varepsilon_{k}$? what does it mean by concatenating two losses there with $\\circ$?\n2. In Line 257, what's $n_{i}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9whMPD91nv", "forum": "1zlTPfyP6n", "replyto": "1zlTPfyP6n", "signatures": ["ICLR.cc/2026/Conference/Submission17795/Reviewer_21dT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17795/Reviewer_21dT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762279661140, "cdate": 1762279661140, "tmdate": 1762927637902, "mdate": 1762927637902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}