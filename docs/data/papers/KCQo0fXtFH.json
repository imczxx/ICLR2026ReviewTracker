{"id": "KCQo0fXtFH", "number": 1445, "cdate": 1756883423509, "mdate": 1759898208587, "content": {"title": "Integer-Centric Neural Video Compression", "abstract": "Cross-platform coding consistency is a fundamental prerequisite for neural video codecs (NVCs). Previous works address this by adopting a floating-point-centric perspective to quantize a pretrained floating-point NVC into an integer one. However, this often leads to suboptimal performance with a significant bitrate increase. In this paper, we propose a high-performance cross-platform NVC by designing a comprehensive integer-centric training pipeline for model integerization, which enables the training of an integer NVC from scratch. This approach avoids initialization from a floating-point model and allows for more flexible learning across the entire integer space. Observing that the division operations in previous integerization methods destabilize from-scratch training, we propose a multiply-twice integerization strategy to circumvent this instability. Furthermore, we introduce a memorized temporal modeling mechanism, leveraging a memory module to capture long-term dependencies and enhance model capacity. With these innovations, we implement in-loop decoding modules in integer to ensure cross-platform coding consistency, which is further validated across multiple platforms. As a result, our cross-platform NVC achieves an average 20% bitrate reduction compared to H.266/VTM while maintaining an encoding/decoding speed of 153.0/137.3 fps for 1080p video. The code will be released.", "tldr": "", "keywords": ["Video Compression", "Model Integerization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0038dd1f393dad6ffff22cb75cf15213e136313b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a high-performance cross-platform neural video codec (NVC) that operates fully in the integer domain, ensuring bit-exact consistency across different hardware platforms. Unlike prior “floating-point-centric” methods that quantize pretrained float models, this work introduces an integer-centric training pipeline, training the integer model from scratch. Furthermore, they introduce a memorized temporal modeling mechanism. The proposed model achieves an average 20% bitrate reduction compared to H.266/VTM while maintaining an encoding/decoding speed of 153.0/137.3 fps for 1080p video."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The cross-platform coding consistency issue is of great significance for end-to-end video compression. In this paper, the authors propose the idea of training an INT16 model from scratch. Unlike other model quantization tasks, LSQ cannot be directly and effectively applied to compression tasks. To address this, the authors introduce a novel method called Multiply-twice integerization to overcome the problem, which is simple yet highly efficient. With the help of multiply-twice integerization and another proposed memorized temporal modeling, their method exhibits good RD performance at a very fast speed."}, "weaknesses": {"value": "This paper is well-written overall. Nevertheless, it still has some weaknesses.\n1. The issue of training instability lacks sufficient analysis. It should provide statistical data on key variables in the training process and compare them to demonstrate that the training effectiveness has indeed improved with Multiply-twice Integerization.\n2. The correlation between Multiply-twice Integerization and the FP16 (or FP32) model requires further analysis. For example, is it only effective for INT8 models but not for FP16 (or FP32) models? Could this design also provide performance benefits when applied to an FP16 (or FP32) model trained from scratch? \n3. Figure 5 should also account for the variation in decoding speed or complexity.\n4. Lack of experiments with Intra Period (IP) 32, following the setting of other mainstream methods like DCVC-FM.\n5. Lack of detailed training costs, like the number and version of the GPUs, and the days of training time. \n6. Lack of an exact formulation of the loss function.\n7. Typos: Line 161, \"Here, v is is\"; Line 244, \"he gradient\"."}, "questions": {"value": "1. Is there any multi-stage training strategy similar to DCVC-TCM? Will the training code be released?\n2. It seems that INT8 acceleration boosts speed at the cost of RD performance, whereas in-loop integerization and memorized temporal modelling sacrifice speed for better compression efficiency. Could you provide a detailed breakdown of the specific gains and penalties introduced by each component?\n3. In Line 971, you remove one DCB in the FP16 model. This seems to be the only change relative to DCVC-RT (FP16) that can increase speed. How much speed-up does it actually yield, and what performance loss does it incur? Is that the main factor that makes the FP16 model faster than DCVC-RT in Table 5?\n4. During the training of the entropy model, is noise injection adopted to simulate quantization error? In integer model training, what distinct statistical characteristics would the quantization errors exhibit?\n5. Line 251: \"If it is applied in LSQ, the coupled gradient may cause an incorrect updating of s\". Please provide a more detailed explanation of \"incorrect updating\". \n6. Table 1 shows In-Loop Decoding obtains 3.4% gains, while Table 4 shows In-Loop integerization obtains (30.6-15.4)% gains in C3. What's the difference between In-Loop Decoding and In-Loop integerization? Are they the same concept?\n7. In Line 836, what is the \"extensive kernel fusions\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HsKJ8gaDMf", "forum": "KCQo0fXtFH", "replyto": "KCQo0fXtFH", "signatures": ["ICLR.cc/2026/Conference/Submission1445/Reviewer_RwNV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1445/Reviewer_RwNV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1445/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747923997, "cdate": 1761747923997, "tmdate": 1762915772362, "mdate": 1762915772362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the first fully 8-bit integer neural video codec (NVC) trained end-to-end without any floating-point initialization. The proposed framework achieves real-time 1080p compression and ~20% bitrate reduction compared to H.266/VTM, while ensuring bit-exact cross-platform consistency. A novel multiply-twice quantization strategy and a memory-augmented temporal module jointly mitigate the stability–accuracy trade-off inherent to low-precision video compression."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method completely abandons the conventional “float → quantize” pipeline and successfully trains an integer-only codec from scratch, representing a pioneering step for low-precision end-to-end video compression.\n2. The proposed integerization removes division operations, suppresses gradient explosion, and maintains stable optimization for long GOP sequences in 8-bit precision.\n3. The lightweight memory module effectively compensates for capacity loss due to quantization, providing up to 17% additional bitrate reduction for the integer model.\n4. The work demonstrates thorough engineering validation, including bit-exact verification across multiple GPUs and Intel iGPUs, and extensive runtime benchmarks from 720p to 4K.\n5. The codec achieves superior rate-distortion (RD) efficiency and runtime compared to both VTM and existing cross-platform NVCs, highlighting its strong practical significance."}, "weaknesses": {"value": "1. Limited theoretical justification. No formal convergence analysis or error bound is provided for the multiply-twice scheme; the “division-free ⇒ stability” claim remains heuristic.\n2. RD curves flatten at higher bitrates, suggesting limited representational capacity under strict 8-bit constraints.\n3. Ablation studies are insufficient. The impact of network width, memory channel count, and QP granularity is not individually evaluated; no direct comparison between 8-bit and 16-bit integer models under identical settings."}, "questions": {"value": "1. The current consistency verification is limited to desktop GPUs and a single Intel iGPU. Additional validation on ARM-based devices, mobile DSPs, or heterogeneous compiler stacks would be valuable to confirm true cross-platform reproducibility.\n2. The quantization-error bounds and convergence guarantees of the multiply-twice integerization remain unclear. A formal derivation or at least an empirical characterization of the error distribution would strengthen the technical soundness of the method.\n3. The paper could further explore more advanced temporal memory mechanisms, such as causal self-attention or LSTM-based recurrent modules, to assess whether they offer improved rate–distortion trade-offs or different complexity behaviors.\n4. Verification of bit-exactness on additional hardware backends—such as ARM NEON, Apple A-series, or Qualcomm Hexagon, where 8-bit multiply–accumulate overflow handling differs—would provide stronger evidence of platform independence.\n5. The observed high-rate degradation suggests limited representational capacity under strict 8-bit constraints. Discussion of potential remedies, including model scaling, mixed-precision schemes, or adaptive bit-width allocation, would enhance completeness.\n6. A controlled comparison against a re-implementation of DCVC-RT using the same 8-bit quantization-aware training setup would clarify the relative performance gap and further substantiate the claimed improvements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GZM0bZJvdS", "forum": "KCQo0fXtFH", "replyto": "KCQo0fXtFH", "signatures": ["ICLR.cc/2026/Conference/Submission1445/Reviewer_dbJf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1445/Reviewer_dbJf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1445/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796076348, "cdate": 1761796076348, "tmdate": 1762915772202, "mdate": 1762915772202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an integer-centric approach to building a cross-platform neural video codec, extending DCVC-RT. It introduces a multiply-twice strategy that avoids divisions by using decoupled step sizes inspired by Learned Step Size Quantization, and adds a memorized temporal modeling mechanism to recover capacity under strict integer arithmetic. As reported, the method achieves competitive coding efficiency while ensuring bit-exact cross-platform decoding, thereby addressing the cross-platform incompatibility issue."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The integer-centric training pipeline is a strong contribution. It departs from conventional workflows that rely on quantization-aware training (QAT) or post-training quantization (PTQ), treating integerization as a first-class objective during model design and optimization.\n\nCross-platform determinism for learned codecs is a key deployment barrier in real-world settings, and this work addresses it directly. Achieving real-time throughput further strengthens the practical relevance of the approach. Additionally, focusing on integerization not only mitigates cross-platform round-off errors but also points toward more energy-efficient dedicated designs, given the lower power consumption of integer versus floating-point operators. This is especially important for battery-powered devices, as edge devices account for a significant share of video consumption.\n\nThe memorized temporal modeling block is a well-motivated addition. Integrated into the DCVC-RT backbone, it provides a principled way to boost temporal prediction beyond motion compensation and, per the reported ablations, contributes materially to the observed rate–distortion gains over a no-memory baseline. Further, the multiply-twice strategy is a meaningful contribution that extends LSQ-style quantization. By separating these scales and enforcing a fixed, division-free compute path, the method improves training stability, preserves dynamic range where needed, and provides clearer fixed-point semantics for deterministic deployment. The reported ablations indicate that this design materially contributes to the observed rate–distortion gains.\n\nThe proposed method achieves competitive, and in several cases superior, rate–distortion performance compared to state-of-the-art DCVC-based frameworks under the reported evaluation protocol, indicating that the design choices translate into efficient coding gains."}, "weaknesses": {"value": "**Related work coverage.** The paper omits a key ICLR reference on *integer networks for compression* [R1], which directly targets bit-exact decoding. Beyond being a relevant citation, this work is foundational, since it explicitly surfaced the cross-platform decoding consistency problem for learned compression and demonstrated integerization as a viable solution to achieve bit-exact behavior across heterogeneous hardware. Please cite this paper and position the present contributions relative to it. Additionally, the experimental comparison is largely confined to DCVC's extensions. Please broaden the baselines to include representative non-DCVC NVCs and report head RD results under the same protocol. Note that there are NVCs in the literature that report rate–distortion performance meeting or surpassing DCVC-FM. Restricting comparisons to DCVC variants makes it difficult to assess competitiveness against the broader SOTA methods. If the scope is intentionally restricted, clarify the selection criteria and justify why comparisons outside the DCVC family are omitted.\n\n**Practical deployment.** As acknowledged in the conclusion, the proposed NVC is not yet suitable for edge devices with tight computational and energy constraints. A key factor appears to be the 384-channel design of the memorized temporal modeling block, which likely increases compute, activation size, and memory traffic. To clarify deployment implications, please report the computational cost as MACs per pixel (MACs/px) and, for low-precision arithmetic, bit-operations per pixel (BOPs/px), and break these out specifically for the memorized temporal module.\n\n**Quantization precision.** The paper largely fixes the integer path at 8-bit and does not examine how coding efficiency and complexity trade off under more aggressive (e.g., 6- or 4-bit) or less aggressive (e.g., 10-, 12-, 16-bit) precisions. Presenting experiments that vary bit widths and report RD impact alongside complexity metrics would enhance the paper. In addition, with a less aggressive precision, the memorized temporal modeling block might admit fewer channels while preserving most of the gains, yielding a better Pareto between complexity and coding efficiency.\n\n**CPU support rationale is too vague.** The claim that CPU support is excluded due to “significant engineering” needs specificity. This justification is difficult to follow, since integer operations are ubiquitous on CPUs. All modern processors using the x86 architecture natively support integer arithmetic, and even Intel’s 8086, released in 1978, implemented integer arithmetic. For reproducibility and scope clarity, please disclose what is missing and what is the specific challenge for implementing integer operations on CPU. As written, it is ambiguous whether the barrier is that you have not implemented a CPU kernel yet, or that you did but could not achieve bit-exact cross-platform consistency.\n\n**UHD comparisons practicality.** The claim that Class A1/A2 (UHD) would take “months” seems overstated, mainly for VVC/VTM. Additionally, running VVenC [R4] at an appropriate preset, or temporally sampling shorter sequence segments for ECM, would improve the experiments quality in this scenario.\n\n### Minor Issues\n**Typos, article/subject-verb slips, and style inconsistencies.** The paper is clear overall but would benefit from a careful proofread to fix numerous small slips and improve polish: clean up article/subject–verb agreement (“a/an”, plural vs singular), remove duplicated words (e.g., “is is the quantized”), and choose one spelling (“modeling” vs “modelling”).\n\n**Inadequate references.** The citations for H.264/AVC, H.265/HEVC, and H.266/VVC point to HM/VTM source repos [R2], [R3] (or for the Overview paper in the case of H.264/AVC) rather than the normative specs. For claims relative to “H.26x,” please cite the ITU-T/ISO standards [R5], [R6], [R7].\n\n**Figure 2 not referenced/discussed.** Figure 2 appears in the paper but is never cited or explained in the main text; please add an explicit reference (e.g., “see Fig. 2”) at first mention and briefly state its takeaway and relevance.\n\n## References\n- **[R1]** Ballé, J., Johnston, N., & Minnen, D. (2019). *Integer Networks for Data Compression with Latent-Variable Models.* International Conference on Learning Representations (ICLR).\n- **[R2]** HM: <https://vcgit.hhi.fraunhofer.de/jvet/HM>\n- **[R3]** VTM: <https://vcgit.hhi.fraunhofer.de/jvet/VVCSoftware_VTM>\n- **[R4]** VVEnc: <https://github.com/fraunhoferhhi/vvenc>\n- **[R5]** ITU-T Recommendation H.264 and ISO/IEC 14496-10 (MPEG-4 AVC): *Advanced video coding for generic audiovisual services*.\n- **[R6]** ITU-T Recommendation H.265 and ISO/IEC 23008-2 (MPEG-H Part 2): *High efficiency video coding*.\n- **[R7]** ITU-T Recommendation H.266 and ISO/IEC 23090-3 (MPEG-I Part 3): *Versatile video coding*."}, "questions": {"value": "**Question 1:** In your ablation, coding efficiency improves as the number of channels in the memorized temporal module increases. For the 0-channel setting, does this mean the memory path is disabled (i.e., $m_t = f_t$ as a direct bypass), or is a different configuration used? In the main model with 384 channels, how many channels do $m_t$ and $f_t$ have at the fusion point?\n\n**Question 2:** What exactly is included in the reported fps (model inference, entropy coding/decoding, host–device I/O, preprocessing)? Additionally, do the fps numbers reflect processing a single video sequence at a time, or multiple sequences in parallel?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V6jySCAHw5", "forum": "KCQo0fXtFH", "replyto": "KCQo0fXtFH", "signatures": ["ICLR.cc/2026/Conference/Submission1445/Reviewer_94Vq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1445/Reviewer_94Vq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1445/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950594568, "cdate": 1761950594568, "tmdate": 1762915772066, "mdate": 1762915772066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel integer-centric design philosophy for neural video compression (NVC), challenging the prevailing \"floating-point-centric\" paradigm that first trains a full-precision model and then quantizes it into an integer implementation. The core contribution is a one-stage integer training framework that directly optimizes the integer model from scratch, aligning the training process with the target integer hardware. Extensive experiments demonstrate that the proposed integer-centric NVC achieves significant improvements in compression efficiency (measured by BD-Rate) and coding speed compared to existing cross-platform integer NVCs and even outperforms some floating-point models."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1)\tThe paper effectively identifies a critical limitation in current NVC research,the performance gap introduced by quantizing floating-point models, which has a clear and compelling motivation.\n\n(2)\tThe one-stage integer training framework and the Clipping Gradient Pass (CGP) mechanism are innovative solutions that directly address the challenges of training low-precision models.  The focus on hardware-friendly operations enhances the practicality and deployability of the method.\n\n(3)   The paper provides valuable details on the integer implementation, which aids in understanding and potential reproduction of the work."}, "weaknesses": {"value": "(1) While the overall results are impressive, the paper lacks a detailed ablation study to quantify the individual contribution of each proposed component (e.g., the impact of CGP alone, the benefit of one-stage training vs. a modified QAT, the effect of the specific integer bit-widths chosen).  This makes it difficult to assess which aspects of the framework are most critical to its success.\n\n(2) The paper positions its work as enabling high-efficiency cross-platform NVC.  However, the experiments focus on performance metrics without explicitly demonstrating consistency or portability across different hardware platforms (e.g., CPU, GPU, mobile, FPGA).  More discussion or evidence on how the integer-centric design specifically facilitates cross-platform deployment would be beneficial.\n\n(3) The encoding and decoding speeds of HEVC and VVC are very slow. I think the author should compare the methods with the faster x.265 and VVEnc. Moreover, as mentioned in (2), the devices used for the two types of methods (traditional or learning-based) are different (CPU vs GPU), which leads to an unfair comparison. I hope to see the performance of this method on different platforms to confirm whether this paper is really of great help in promoting the ease of use and real-time performance of learning-based video coding. If the author can solve the above problems in the subsequent rebuttal stage, I am willing to increase my score."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fL9nrMuda5", "forum": "KCQo0fXtFH", "replyto": "KCQo0fXtFH", "signatures": ["ICLR.cc/2026/Conference/Submission1445/Reviewer_s31y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1445/Reviewer_s31y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1445/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978314456, "cdate": 1761978314456, "tmdate": 1762915771882, "mdate": 1762915771882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a neural video codec (NVC) that achieves both cross-platform compatibility and fast coding speed. To enable cross-platform deployment, the authors propose quantizing the NVC. Instead of following the conventional approach of converting a pretrained floating-point NVC into an integer model, they introduce a novel One-Stage Integer Training framework, which trains the integer model from scratch, that is, performing quantization-aware training (QAT) from the very beginning. To facilitate this process, they design a multiply-twice integerization method to stabilize training and propose a recurrent memorize modeling strategy to further enhance performance. Experimental results show that the proposed NVC achieves comparable rate-distortion performance to DCVC-FM, DCVC-RT, and ECM, while significantly outperforming them in terms of coding speed."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes an integer-centric training pipeline that enables training an integer NVC from scratch, which differs from prior works that quantize pretrained floating-point models. This design avoids potential mismatch between floating-point and integer representations and demonstrates that quantization-aware training can be effectively applied from the beginning of training.\n- The multiply-twice integerization with clipped gradient pass is a key contribution that improves training stability. This approach addresses training stability and resolves the issue of gradients becoming zero in certain situations.\n- The proposed temporal modeling strategy effectively captures historical context for video compression. By incorporating a recurrent memory mechanism, the method maintains temporal consistency and enhances inter-frame feature utilization, contributing to the overall compression performance."}, "weaknesses": {"value": "- The proposed technique, such as multiply-twice integerization, lacks a strong theoretical foundation and does not guarantee improved coding performance.\n- The primary performance gain appears to come from the memory model, which is simply a standard LSTM architecture. The quantization method reduces the performance significantly.\n- Performance in high-bitrate scenarios is relatively weak, which may reduce the method’s practical value in applications where visual quality is prioritized. The degradation at higher bitrates suggests that the model’s capacity or quantization precision might limit its ability to preserve fine details.\n- Complexity metrics (e.g., kMACs, model size) are not reported. Without these measurements, it is difficult to evaluate whether the speedup comes from architectural efficiency or simply a smaller or shallower model, making the performance claims less transparent.\n- The rationale for choosing a channel size of 384 for the memory buffer is unclear. The paper only mentions that this value “balances the rate-distortion-complexity trade-off,” but it never specifies what type of complexity is being considered. It remains uncertain whether this parameter affects the overall coding speed, model size, or computational load, making it difficult to interpret the claimed balance or reproduce the design decision.\n- The source of the speed advantage over DCVC-RT (int16) is ambiguous. It remains uncertain whether the improvement arises from the 8-bit integer operations, the network architecture, or implementation details. A fair comparison with a 16-bit integer version trained under the same pipeline would help isolate the key factor.\n- Table 4 shows that the multiply-twice strategy does not always help when quantizing a floating-point model. This inconsistency suggests that the proposed integerization method might be sensitive to specific training conditions, limiting its general applicability.\n- Comparing Table 3 and Table 6, the quantization still introduces around a 10% BD-rate increase while providing only a modest speed gain. This indicates that the proposed approach involves a nontrivial performance and complexity trade-off, rather than maintaining the same rate–distortion efficiency as the floating-point models."}, "questions": {"value": "- Will the authors release the training scripts? Considering that the main contribution of this paper is the one-stage integer training strategy, providing the complete training pipeline would be highly beneficial for reproducibility and for the research community to further build upon this work.\n- The hyperparameters K_{a, b, c, d} are empirically set, but the paper does not describe how they are determined. It would be helpful to know whether there is a systematic or data-driven way to choose these values, or if they are tuned specifically for this implementation.\n- Figure 5 is somewhat unclear. When the memory channel size is set to 0, is there still any temporal information propagated to the next frame? According to Figure 3, if m_t's channel dimension is 0, there should be no temporal modeling at all, so further clarification would be useful.\n- Since s_i​ and s_c​ are decoupled step sizes, is it possible for either of them to take negative values during training? If so, would a negative step size be meaningful or cause instability in the quantization process? It would be helpful if the authors could clarify whether any constraint or mechanism is applied to ensure that both step sizes remain positive.\n- The paper introduces QP-aware quantization step sizes for variable bitrate. Does this imply that each QP value corresponds to a distinct set of s_i​ and s_c parameters throughout the model?\n- In Table 3, for the traditional codec, is it running on the CPU, or is GPU acceleration available? If only the CPU is used, explicitly stating this in the paper would help clarify and better compare the coding speed differences among all methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gwpIjQDAa6", "forum": "KCQo0fXtFH", "replyto": "KCQo0fXtFH", "signatures": ["ICLR.cc/2026/Conference/Submission1445/Reviewer_2sVU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1445/Reviewer_2sVU"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission1445/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998330660, "cdate": 1761998330660, "tmdate": 1762915771617, "mdate": 1762915771617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}