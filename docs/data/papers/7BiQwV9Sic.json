{"id": "7BiQwV9Sic", "number": 4479, "cdate": 1757687227945, "mdate": 1763716204036, "content": {"title": "Advancing Multi-agent Traffic Simulation via R1-Style Reinforcement Fine-Tuning", "abstract": "Scalable and realistic simulation of multi-agent traffic behavior is critical for advancing autonomous driving technologies. Although existing data-driven simulators have made significant strides in this domain, they predominantly rely on supervised learning to align simulated distributions with real-world driving scenarios. A persistent challenge, however, lies in the distributional shift that arises between training and testing, which often undermines model generalization in unseen environments. To address this limitation, we propose SMART-R1, a novel R1-style reinforcement fine-tuning paradigm tailored for next-token prediction models to better align agent behavior with human preferences and evaluation metrics. Our approach introduces a metric-oriented policy optimization algorithm to improve distribution alignment and an iterative \"SFT-RFT-SFT\" training strategy that alternates between Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) to maximize performance gains. Extensive experiments on the large-scale Waymo Open Motion Dataset (WOMD) validate the effectiveness of this simple yet powerful R1-style training framework in enhancing foundation models. The results on the Waymo Open Sim Agents Challenge (WOSAC) showcase that SMART-R1 achieves state-of-the-art performance with an overall realism meta score of 0.7858, ranking first on the leaderboard at the time of submission.", "tldr": "A novel R1-style Reinforcement Fine-Tuning (RFT) paradigm for multi-agent traffic simulation in autonomous driving.", "keywords": ["Autonomous Driving", "Reinforcement Fine-Tuning", "Multi-agent Traffic Simulation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea3a442150e9082ba0c0d19ab803caace569a396.pdf", "supplementary_material": "/attachment/37bab3bf9bba9f93f6e8db9c3e83f4261c9093e9.zip"}, "replies": [{"content": {"summary": {"value": "The paper apply the R1 training style to the traffic model learning task to solve the distribution shift problem. It directly uses the waymo evaluation metrics to assign the reward. The method achives good result in the Waymo Sim Agent benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The performance of the method is good on the Waymo Sim Agent benchmark.\n2. The paper is good written and easy to follow.\n3. The ablation experiments are extensive."}, "weaknesses": {"value": "1. The contribution of the paper is limited. It looks like an engineering work to apply the existing R1 training style to finetune an existing model based on CATK. \n2. The metric-oriented policy optimization method directly uses the evaluation metric as the reward, which makes its strong performance less meaningful. It may simply overfit to the metric itself rather than improving genuine behavioral realism across other metrics.\n3. Taking the waymo sim agent score as reward is time consuming because the computation is slow which requires 32 rollouts."}, "questions": {"value": "1. How do you assign the rewad among the 32 rollout? Do all 32 rollouts use the same reward?\n2. What is the performance of using the original KL penalty?\n3. What is the performance of using more iterations of SFT and RFT or making them optimize together?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SjjZT4Q5B4", "forum": "7BiQwV9Sic", "replyto": "7BiQwV9Sic", "signatures": ["ICLR.cc/2026/Conference/Submission4479/Reviewer_PHPM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4479/Reviewer_PHPM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760669348777, "cdate": 1760669348777, "tmdate": 1762917392308, "mdate": 1762917392308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to use reinforcement learning to post-train the Next-token-prediction-based motion planning model. It uses a three-stage post-training method, SFT-RLFT-SFT, and finally achieves SOTA on Waymo Open Sim Agent Challenge (WOSAC)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Agent simulation is an important problem for self-driving, especially building agents with realistic behaviors.\n2. The paper proposes to use RL post-training to explore better behaviors to increase the realism and achieve SOTA on the commonly used benchmark WOSAC. Comprehensive experiment results, including ablations, demonstrate the effectiveness of the proposed method.\n3. The paper writing is good and easy to understand."}, "weaknesses": {"value": "1. The method is mainly designed for an NTP-based model, while new adaptations are needed for continuous prediction models like diffusion models, which may limit its future application.\n2. It would be better to evaluate the method on more end-to-end planning benchmarks like nuPlan to show its general capabilities, which can further increase its impact."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b1CCQLyk0i", "forum": "7BiQwV9Sic", "replyto": "7BiQwV9Sic", "signatures": ["ICLR.cc/2026/Conference/Submission4479/Reviewer_tSgk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4479/Reviewer_tSgk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761581547629, "cdate": 1761581547629, "tmdate": 1762917391971, "mdate": 1762917391971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper tries to address the problem of distribution shift in the learned (supervised learning) simulators affecting their generalization capabilities.\n- The authors propose SMART-R1 to align agent behavior with human preferences and relevant evaluation metrics for agent traffic simulation tasks.\n- The authors introduce an iterative (interleaving) SFT-RFT-SFT post-training pipeline to maximize the performance and minimize the skills learned in previous training runs (catastrophic forgetting).\n  - They introduce Metric-oriented Policy Optimization (MPO) to exploit the knowledge from priors and can try to optimize for collision, off-route cases, etc. that might not be there in the SFT data. \n- With the above recipe the proposed method was able achieve best performance on Waymo Open Sim Agents Challenge (WOSAC)\n\nGiven the comments related to weaknesses and limitations, I can have the rating as 2. Flexible to move this during or after rebuttal."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors presented the text, figures and tables nicely.\n- Converting the trajectories into motion tokens makes it easier for the sequence to be fed to the transformer and formulate it as a next-token-prediction (NTP) task.\n- The closed-loop SFT seems to reduce the covariate shift in NTP models leading to improved performance.\n- RFT stage aligns the model with metric preferences and enhances realism. \n- The paper does a good job in performing experiments and ablations to study the effect KL regularization, effect of different post-fine-tuning methods, effects of using different RL algorithms for RFT."}, "weaknesses": {"value": "The approach proposed has off-late been a pretty common fine-tuning strategy for many robotics tasks. It is appreciated that the authors have done research in incorporating the multi-step post-training pipeline serving a dual purpose of maximizing performance and minimizing forgetting. However, there has been some work done on dealing with forgetting as well in LLMs ([Towards a Unified View of Large Language Model Post-Training](https://arxiv.org/pdf/2509.04419)) ."}, "questions": {"value": "1. Was keeping a KL term in the SFT tried?\n\n2. Was there any KL term in the RL loss to minimize the divergence from the base policy while maximizing returns?\n\n3. Was any other regularizer tried to prevent forgetting (for example, Elastic Weight Consolidation)?\n\n4. Was there any KL term in the second SFT training (refer to [RL's Razor: Why Online Reinforcement Learning Forgets Less](https://arxiv.org/abs/2509.04259))?\n\n5. How many agents are considered in the scenario and how are selected or filtered, if any?\n\n6. Line 397: “Notably, applying two consecutive SFT phases” - isn't this just one big SFT stage with 2x epochs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5099haZglH", "forum": "7BiQwV9Sic", "replyto": "7BiQwV9Sic", "signatures": ["ICLR.cc/2026/Conference/Submission4479/Reviewer_uZAq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4479/Reviewer_uZAq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997673347, "cdate": 1761997673347, "tmdate": 1762917390955, "mdate": 1762917390955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a trajectory forecasting system that is based on SMART and adds DeepSeek-R1-style training, which ultimately leads to SotA performance on a difficult dataset."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **S.1:** The idea is simple and well-justified. It's not methodologically revolutionary but the results are very impressive.\n- **S.2:** The writing is clear, the illustrations are simple but good.\n- **S.3:** I really like your ablations, especially table 3-6. I think this is very good scientific practice."}, "weaknesses": {"value": "- **W.1:** Reproducibility. I really dislike when papers don't submit any code with their submission for a system that's quite complex. Especially since your method is built on top of an existing codebase, I think it would've been great if you could've included an anonymous github link or a zip file with the submission.\n- **W.2:** Performance/Train time? Along the same lines of the previous comment: how long was this trained, on which hardware? How fast is inference?\n- **W.3:** The writing is moooostly clear but there are some sharp edges that I think need to be improved. In the abstract, you already mentioned \"R1-style...\" without mentioning what this is referring to (e.g.\"the SFT-RL-SFT training regimen of DeepSeek-R1\") and you often refer to CAT-K rollouts, which I'm not familiar with and since it seems important to your method, it'd be nice to have a quick explanation in the main body of your paper."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2RWjpHbpSn", "forum": "7BiQwV9Sic", "replyto": "7BiQwV9Sic", "signatures": ["ICLR.cc/2026/Conference/Submission4479/Reviewer_YBue"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4479/Reviewer_YBue"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762403816073, "cdate": 1762403816073, "tmdate": 1762917389899, "mdate": 1762917389899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}