{"id": "foOyv8KTj7", "number": 22608, "cdate": 1758333479831, "mdate": 1763716441686, "content": {"title": "JIR-Arena: The First Comprehensive Benchmark Dataset for Just-in-time Information Recommendation", "abstract": "Just-in-time Information Recommendation (JIR) is a service that delivers the most relevant information precisely when users need it the most. It plays a critical role in filling users' information gaps during pivotal moments like those in learning, work, and social interactions, thereby enhancing decision-making quality and life efficiency with minimal user effort. Recent device-efficient deployment of performant foundation models and the proliferation of intelligent wearable devices have made the realization of always-on JIR assistants feasible. However, despite the potential of JIR systems to transform our daily life, there has been little prior systematic effort to formally define JIR tasks, establish evaluation frameworks, or propose a large-scale multimodal benchmark with high-quality multi-party-sourced ground-truth labels. To bridge this gap, we present a comprehensive mathematical definition of JIR tasks and their associated evaluation metrics. Furthermore, we introduce JIR-Arena, the first multimodal JIR benchmark dataset comprising 34 scenes (831 minutes) with oracle information needs covering 11 types with diverse and information-request-intensive scenarios, designed to evaluate JIR systems across multiple dimensions, including whether they can i) accurately infer user information needs, ii) provide timely and helpfully relevant recommendations, and iii) effectively avoid the inclusion of irrelevant content that might distract users. \n\nConstructing a JIR benchmark is challenging due to the subjectivity of user information needs and the difficulty of achieving reproducible evaluations. To overcome these, our benchmark approximates user need distribution by combining human and large AI model inputs, and enhances objectivity through a multi-turn validation framework. Additionally, we ensure assessment reproducibility by evaluating information recommendation outcomes against static knowledge bases. We also develop a baseline JIR system architecture, and instantiate it with several large foundation models. Our evaluation of the baselines on JIR-Arena reveals that while large foundation model-based JIR systems can simulate user needs with reasonable precision (72.4% average), they struggle with recall (34.7% average) and effective content retrieval. The analysis identifies dual bottlenecks in both user information need prediction and retrieval systems, with semantic mismatch of predicted information need (62.9% of failures) being the primary failure mode. Finally, to facilitate future development of JIR systems and exploration of more JIR application scenarios, we release our code and data in the supplementary materials.", "tldr": "We formally define Just-in-time Information Recommendation (JIR), introduce JIR-Arena—the first large-scale multimodal benchmark for JIR systems—and provide baseline evaluations, code, and data to enable future development in this emerging area.", "keywords": ["information access", "AI agent", "user simulation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/24df4ddcba4ffd197d6f2942751496187f0a76a4.pdf", "supplementary_material": "/attachment/a9e25c004836bc6f94dd19597f8bc42c2123b23e.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents JIR-Arena, the first comprehensive benchmark dataset for evaluating Just-in-Time Information Recommendation (JIR) systems. The JIR task aims to capture and satisfy users' immediate information needs in information-intensive scenarios (e.g., attending lectures, watching tutorials). The authors rigorously formalize the JIR task as a Partially Observable Markov Decision Process (POMDP) and design a multi-dimensional evaluation framework encompassing Precision, Recall, content Relevance (R_relevance), and Timeliness (R_timeliness).\n\nThe JIR-Arena dataset comprises 34 multimodal scenes (primarily academic lectures and conference talks) totaling 831 minutes. The dataset construction employs a multi-agent collaborative approach, combining multiple large language models (GPT-4O, DeepSeek-V3) with human annotators, using voting mechanisms to simulate the distribution of user information needs. The authors implement baseline JIR systems and conduct evaluations, revealing that current systems can identify user needs with reasonable precision but perform poorly in terms of recall and information retrieval quality, indicating that the retrieval component is the primary bottleneck and pointing the direction for future research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Pioneering Contribution and Theoretical Framework.** This is the first work to formally define the JIR task and provide a standardized evaluation framework, filling a long-standing gap in the field. The formalization of JIR as a POMDP framework establishes a solid theoretical foundation for subsequent research, enabling precise definition and systematic study of JIR tasks. This represents a crucial contribution for an emerging research direction.\n\n**Innovative Data Collection Methodology.** The paper employs a multi-entity collaborative strategy to construct the dataset, demonstrating methodological innovation. By combining multiple large language models with human annotators and utilizing voting mechanisms to simulate user information needs, it appropriately addresses the inherent subjectivity and diversity challenges in need annotation. The three-layer retrieval verification pipeline (traditional IR → LLM quality check → human verification) exhibits a clear design rationale and fully reflects awareness of data quality control.\n\n**Well-Designed Evaluation Framework.** The proposed evaluation dimensions (Precision, Recall, Relevance, Timeliness) are comprehensive and well-suited to the characteristics of JIR tasks. The many-to-many matching mechanism accounts for the decomposability of information needs, which is a key distinction from traditional question-answering systems. The use of nDCG to evaluate retrieval quality, adoption of Gaussian kernel functions for temporal matching, and introduction of likelihood scores as weights all align well with task requirements. Although specific implementation details require refinement, the overall framework provides an excellent starting point for JIR evaluation.\n\n**Systematic Baseline Experimental Study.** The paper tests multiple mainstream large models (GPT-4O, DeepSeek-V3, Claude-3-7, Gemini-2.0-Flash) as well as small models suitable for edge devices (Phi-4, Qwen3-4B), comparing text-only versus multimodal models and different retrieval methods (BM25, Dense Retriever, Reranker). These experiments provide valuable performance baselines and initial insights to the community, clearly identifying retrieval systems as the current primary bottleneck."}, "weaknesses": {"value": "### 3.1 Reproducibility Issues\n\nThe paper explicitly states that the complete static knowledge bases cannot be released due to space limitations. This is a significant issue for a benchmark dataset, as without access to the same knowledge bases, other researchers will be unable to fairly reproduce experiments or compare system performance. It is recommended that, prior to formal publication, the authors provide an access solution for the knowledge bases (e.g., hosting on Zenodo or Hugging Face Datasets), or at minimum provide complete scripts and indexing methods for constructing the knowledge bases.\n\n### 3.2 Insufficient Depth of Experimental Analysis\n\nThe paper shows that retrieval relevance performance is suboptimal but lacks in-depth analysis of the underlying causes. Is the issue with query generation quality, indexing granularity (5 sentences/chunk), knowledge base coverage, or the retrieval model itself? It is suggested to conduct an oracle study (directly using ground truth questions for retrieval) to pinpoint the bottleneck. Additionally, the counterintuitive phenomenon that multimodal models exhibit lower recall than text-only models (0.412 vs 0.429) requires deeper analysis, such as identifying in which scenarios visual information enhances performance and in which scenarios it may introduce noise.\n\n### 3.3 Evaluation Details Require Clarification\n\nWhile the evaluation framework design is sound in concept, the many-to-many matching algorithm lacks mathematical formulation or pseudocode definition. It is unclear how to handle cases when one ground truth matches multiple predictions and how to avoid duplicate counting. The selection of key parameters (such as similarity threshold 0.55, temporal balancing coefficient 0.9, etc.) also lacks sufficient justification and explanation. It is recommended to clearly define the matching logic and conduct parameter sensitivity analysis.\n\n### 3.4 Limited Dataset Scale and Coverage\n\nThe scale of 34 scenes is relatively small, and coverage is limited to academic scenarios (lectures and conferences). The application scenarios for JIR should actually be much broader, with scenarios such as daily work, programming tutorials, and product usage also holding significant value. Furthermore, while the paper defines 11 types of information needs, it does not report the distribution of each type or fine-grained performance. It is suggested to supplement this content to identify which types are most challenging."}, "questions": {"value": "1. **Retrieval Bottleneck Localization:** Retrieval relevance is the primary bottleneck. Could you conduct an oracle study? That is, directly use ground truth questions for retrieval to observe performance, which would help determine whether the issue lies in the need generation stage or the retrieval system itself.\n\n2. **Multimodal Anomaly:** Why do multimodal models exhibit lower recall than text-only models (0.412 vs 0.429)? The paper's explanation is overly brief and lacks quantitative analysis. Could you analyze in which scenarios visual information enhances performance and in which scenarios it may introduce noise?\n\n3. **Evaluation Parameter Selection Rationale:** How were parameters such as similarity threshold 0.55, deduplication threshold 0.75, and temporal balancing coefficient 0.9 selected? Was parameter sensitivity analysis conducted?\n\n4. **Need Type Distribution Analysis:** The paper defines 11 types of information needs but does not report the distribution or performance for each type. Could this content be supplemented? Which types are most challenging?\n\n5. **Annotation Coverage Explanation:** Human verification covers only 272 samples (approximately 20%). Is this proportion sufficient? How is the quality of the remaining samples ensured?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VIzx730uvV", "forum": "foOyv8KTj7", "replyto": "foOyv8KTj7", "signatures": ["ICLR.cc/2026/Conference/Submission22608/Reviewer_JomG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22608/Reviewer_JomG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915809836, "cdate": 1761915809836, "tmdate": 1762942301864, "mdate": 1762942301864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank the reviewers for their valuable feedback. We are encouraged that reviewers recognize this work as **the first to systematically define the JIR task** (TQLe, JomG), establishing a **solid theoretical foundation** (JomG) through POMDP formalization. Reviewers appreciate the **innovative multi-entity collaborative methodology** for data collection (TQLe, JomG), the **well-designed evaluation framework** with comprehensive metrics (JomG), and the **practical importance** of the problem (PV32). The human-validated **high-quality dataset** (TQLe) and **comprehensive documentation** supporting reproducibility (TQLe) were also positively noted. \n\nReviewers also provided constructive suggestions for further strengthening the work. In response, we have conducted additional experiments and analyses to address their questions and provide deeper insights. **The results have been incorporated into the manuscript update, with analysis scripts uploaded**. We provide responses to common questions below."}}, "id": "BVLZMuFTFJ", "forum": "foOyv8KTj7", "replyto": "foOyv8KTj7", "signatures": ["ICLR.cc/2026/Conference/Submission22608/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22608/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22608/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763716643211, "cdate": 1763716643211, "tmdate": 1763716643211, "mdate": 1763716643211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces JIR-ARENA, the first comprehensive benchmark for JIR systems. The authors formalize JIR as a POMDP, propose evaluation metrics, and construct a multimodal dataset with 34 video scenes across lectures and conferences. Ground truth is generated through multi-entity collaboration with multi-round validation. The dataset includes information needs, temporal annotations, and hierarchical reference documents retrieved from static knowledge bases. Baseline experiments on several foundation models reveal low recall and poor retrieval performance, establishing initial benchmarks for future research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents the first systematic formalization of the JIR task as a POMDP with well-defined components, filling a key gap in evaluation infrastructure for proactive assistant systems. The motivation is clear and grounded in realistic use cases across education and workplace settings where users encounter information gaps.\n\n2. The data collection framework is innovative, combining four human annotators with four LLM configurations to approximate the distribution of information needs. The inclusion of a three-layer hierarchical verification process for references enhances reliability, and the reported human validation scores (3.26–3.80/4) indicate good quality. The use of voting instead of single annotations effectively mitigates subjectivity.\n\n3. The documentation is comprehensive, with full prompts and construction methodology detailed in the appendices, providing strong support for reproducibility and future research.\n\n4. The experimental evaluation covers both state-of-the-art and smaller models suited for on-device deployment. The error analysis is detailed and insightful, identifying specific failure modes such as low recall on high-likelihood needs and context-free retrieval errors. The work establishes clear and useful baselines for comparison in future studies."}, "weaknesses": {"value": "1. This paper lacks empirical evidence that proposed metrics correlate with actual user satisfaction or learning outcomes. Paper dismisses prior user studies for \"generalization difficulty\" but offers no validation. Critical questions unanswered: Do users prefer high-Recall or high-Precision? What metric weights matter? Missing dimensions like cognitive load? Therefore, a user study is needed for a more reliable results evaluation.\n2. Although the paper claims multimodal capability, the video content is processed primarily via NVILA narrations converted to text, making the input effectively text-based. The reported multimodal models exhibit worse temporal performance (lower Rstart/Rend), yet the paper provides no analysis or explanation for this degradation. This suggests limited true multimodal reasoning. Moreover, potentially informative visual cues (e.g., gestures, equations on the board) are ignored, representing a missed opportunity to leverage the visual modality meaningfully.\n3. The evaluation is limited to lectures and conference videos, excluding other domains (e.g., meetings, coding sessions, medical consultations, or daily tasks) that are emphasized in the paper’s motivation. Even within these selected categories, the dataset appears biased toward high-view YouTube videos, which may not represent typical real-world use cases. Moreover, the large discrepancy in average view counts raises concerns about data quality and representativeness."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ngEgOroUaD", "forum": "foOyv8KTj7", "replyto": "foOyv8KTj7", "signatures": ["ICLR.cc/2026/Conference/Submission22608/Reviewer_TQLe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22608/Reviewer_TQLe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922334946, "cdate": 1761922334946, "tmdate": 1762942301544, "mdate": 1762942301544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces JIR-ARENA, a benchmark and baseline system for Just-in-time Information Recommendation (JIR). It formalizes JIR as a POMDP, proposes evaluation metrics (including relevance and timeliness), and curates a multimodal dataset of lectures and conference talks totaling 831 minutes. User information needs are simulated via multi-entity, multi-turn LLM/human pipelines and evaluated against static knowledge bases; baseline experiments with several large and small models report modest precision/recall and low retrieval utility, with code and data planned for release to support future work."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) The problem in study is practical and important. The paper articulates a timely vision for proactive, context-aware information assistance.   \n \n2) Formalization and metrics. Casting JIR as a POMDP and defining task-specific metrics (e.g., relevance and timeliness) offer a clear foundation for systematic evaluation and future method development.  \n  \n3) Baseline system and planned resource release. An extensible baseline pipeline (generative + retrieval), along with a commitment to release data/code, lowers the barrier to entry for the community.   (IN the paper the authors say \"Upon acceptance, we will release the code and dataset to the public\")."}, "weaknesses": {"value": "1) The title says \"COMPREHENSIVE\", but the dataset only covers academic lectures and conference talks, which makes the claim overreaching given the narrow scope of scenes and sources (mostly YouTube).   Do you have any plans to broaden coverage beyond lectures/conference talks to everyday, non-academic contexts (meetings, classrooms, collaborative work, consumer media)?\n  \n2) Scale is limited. \"JIR-ARENA includes 2 categories of scenes, totaling 34 of them and spanning 831 mins\", which is small for a benchmark intended to capture diverse information needs across settings; more scenes, speakers, domains, and formats are needed.  \n  \n3) Reliability of simulated data. The benchmark relies heavily on LLM/human simulations to approximate user need distributions; without validation against real user behavior, the ground-truth labels and likelihoods risk subjectivity and distributional mismatch. How do you validate that the simulated need distributions reflect real audiences? I expect to see evidence (e.g., agreement analyses, small-scale user studies, or correlation with in-situ user queries) to substantiate that multi-entity voting and likelihoods approximate actual behavior. \n \n4) Selection and evaluation biases. Scene selection by \"most-viewed videos\" and evaluation against static knowledge bases (Wikipedia, textbooks, arXiv) may bias content and retrieval toward canonical references, not the heterogeneous resources users actually consult; justification and ablations on these choices are missing.  \n  \n5) A broader range of open-sourced LLMs are expected to appear in Table 1 for a more rigorous study. Concrete case studies are also expected for us to have a deeper understanding of the capabilities of LLMs on the JIR scenario.\n\n6) The abstract is too subjective. The authors should include some objective summary (dataset statistics, core metrics)  so that readers can have a quick overview on the quality of the resource."}, "questions": {"value": "Please refer to detailed comments in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ssn7x6W6SV", "forum": "foOyv8KTj7", "replyto": "foOyv8KTj7", "signatures": ["ICLR.cc/2026/Conference/Submission22608/Reviewer_PV32"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22608/Reviewer_PV32"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962612754, "cdate": 1761962612754, "tmdate": 1762942301209, "mdate": 1762942301209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study presents the just-in-time information recommendation task and introduces a systematic benchmark for evaluation. Using this benchmark, this paper conducts error analysis and reveal potential opportunities for improvement."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The proposed task is interesting and potentially has wide applicability. It is also original.\n\n(2) The task is well defined.\n\n(3) The contributions of the study are well articulated."}, "weaknesses": {"value": "(1) Although a benchmark on this task is presented, there does not seem to be a clear pattern in model performance. Most analyses are not deep enough to provide insights. For example, it seems that no models dominant in all evaluation dimensions. It is unclear what the major takeaway is based on the current discussion.\n\n(2) Many observations are not well explained. The findings may be superficial without extensive insights into future work.\n\n(2a) Some smaller models perform the best in some metrics and it is unclear why.\n\n(2b) How was the error analysis performed? How to define matched needs that are highly likely to be raised by users?\n\n(2c) It is necessary to expand the discussions in Line 422-423. What contextual information is it?\n\n(2d) The gap between strong proprietary and publicly released LLMs does not seem the be large. Why is that?"}, "questions": {"value": "Please see questions in the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XEqS2hEroL", "forum": "foOyv8KTj7", "replyto": "foOyv8KTj7", "signatures": ["ICLR.cc/2026/Conference/Submission22608/Reviewer_G1RX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22608/Reviewer_G1RX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985824023, "cdate": 1761985824023, "tmdate": 1762942300861, "mdate": 1762942300861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}