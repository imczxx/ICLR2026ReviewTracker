{"id": "uFomU6UJHH", "number": 238, "cdate": 1756732235153, "mdate": 1759898270541, "content": {"title": "Data Assessment for Embodied Intelligence", "abstract": "In embodied intelligence, datasets play a pivotal role, serving as both a knowledge repository and a conduit for information transfer. The two most critical attributes of a dataset are the amount of information it provides and how easily this information can be learned by models. However, the multimodal nature of embodied data makes evaluating these properties particularly challenging. Prior work has largely focused on diversity, typically counting tasks and scenes or evaluating isolated modalities, which fails to provide a comprehensive picture of dataset diversity. On the other hand, the learnability of datasets has received little attention and is usually assessed post-hoc through model training—an expensive, time-consuming process that also lacks interpretability, offering little guidance on how to improve a dataset. In this work, we address both challenges by introducing two principled, data-driven metrics. First, we construct a unified multimodal representation for each data and, based on it, propose diversity entropy, a continuous measure that characterizes the amount of information contained in a dataset. Second, we introduce the first interpretable, data-driven algorithm to efficiently quantify dataset learnability without training, enabling researchers to assess a dataset’s learnability immediately upon its release. We validate our algorithm on multiple simulated and real-world embodied datasets, demonstrating that it yields faithful, actionable insights, enabling researchers to jointly improve diversity and learnability. We hope this work provides a foundation for designing higher-quality datasets that advance the development of embodied intelligence.", "tldr": "A Quality Evaluation Toolkit for Embodied Data", "keywords": ["Benchmark; Embodied AI; Data-centric AI;"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/47cc23ed83d35b2c2fad933a0a7e692cd728b396.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "For embodied datasets, the authors created multimodal representations that are used to quantify the amount of information within a dataset, while also developing an algorithm to assess its learnability without requiring training."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tries to come up with ways of quantifying the utility of embodied data which is an important problem to adress. Also some of this techniques could also be used to understand datasets from other domains.\n\n2. The authors of the paper seem to have a done a thorough comparison with the related works and the introduction and the related works section seem to be well done."}, "weaknesses": {"value": "1. Some of the design choices in the paper appear somewhat arbitrary (see questions for details).\n\n2. It is also unclear how strongly the dataset score correlates with the accuracy results (see questions for details).\n\n3. Some of the claims made in the paper may be insufficiently substantiated (see questions for details)."}, "questions": {"value": "1. Could you clarify why only three frames are selected? Wouldn’t it make more sense to process the videos using a dedicated video model (for example, Video-LLaVA: https://huggingface.co/docs/transformers/en/model_doc/video_llava)? Additionally, could you explain why longer videos are penalized, as shown in Equation 2? If a video consists of repeated or constant frames, it shouldn’t necessarily be harder to memorize simply because it’s longer. Furthermore, this approach might fail to capture the diversity of tasks or actions within a single video. Hence is there an implicit assumption on the number of tasks per video?\n\n2. In the conclusion the authors say: \"In this work, we presented two principled, data-driven tools for assessing embodied datasets. First, we introduced diversity entropy, characterizing the information richness of a dataset. Experiments on 21 large-scale embodied datasets, we observe that simply scaling the number of episodes yields diminishing returns in information gain\". However, I'm not sure if I saw such an experiment. Where was this experiment conducted?\n\n3. Could you clarify how Table 5 (Appendix F) should be interpreted? Specifically, what do “ground truth” and “predicted” refer to in this context? If I’m understanding it correctly, wouldn’t this suggest that the metric and the accuracy are not correlated?\n\n4. Could you explain why the learnability of one model would be influenced by the representations from CLIP? How can the learning dynamics of one model be meaningfully related to the representations of another?\n\n5. Why is the model’s learnability represented as a single scalar value (Equation 5)? What does this number signify conceptually, and how is it actually computed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M3FGgEvunC", "forum": "uFomU6UJHH", "replyto": "uFomU6UJHH", "signatures": ["ICLR.cc/2026/Conference/Submission238/Reviewer_KKhd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission238/Reviewer_KKhd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962650016, "cdate": 1761962650016, "tmdate": 1762915477466, "mdate": 1762915477466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces two data driven tools for embodied datasets, a continuous metric called diversity entropy to quantify the information content of the dataset and an algorithm to estimate dataset learnability. The authors demonstrate the effectiveness of their framework on both simulation and real-world datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This work introduces a means to evaluate the quality of the datasets that are central to learning based methods in embodied agents. This is very crucial since the agent performance depends on these datasets. I believe this work will provide insights for future work."}, "weaknesses": {"value": "Overall the paper is well written and easy to understand. I didn’t find any significant weaknesses. However, I have a few clarifying questions listed below."}, "questions": {"value": "I have a few clarifying questions for the authors:\n\n1. Since CLIP is not trained on robot trajectories, how sensitive do you think are your diversity and learnability estimates to the choice of encoder? Have you tried pretrained encoders trained on embodied datasets?\n2. How did you choose the sigma for H_data in eq. 1? How sensitive is H_data to this choice?\n3. The metric seems to be reflective of visual variance. Did you test whether H_data correlates with semantic diversity, such as object or task variety?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "6D2fvwxrzo", "forum": "uFomU6UJHH", "replyto": "uFomU6UJHH", "signatures": ["ICLR.cc/2026/Conference/Submission238/Reviewer_VCAi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission238/Reviewer_VCAi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974685624, "cdate": 1761974685624, "tmdate": 1762915477342, "mdate": 1762915477342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author proposed two algorithms for data valuation in multimodal embodied datasets. The first method, named diversity entropy, is calculated based on a unified multimodal representation for each data. This quantifies information content of the data. The second method utilizes two factors, ease of memorization and expressiveness, to measure the learnability of the data. The authors validate these metrics on simulated and real-world datasets, analyzing 21 popular embodied datasets totaling over 800GB."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- While data valuation has been an established area, I really like the author's systematic approach of dividing valuation into diversity and learnability, and analyze the two holistically to give a better picture of the quality of the data. The topic of data valuation is also relevant in building VLA foundation models, as VLA training data is often noisy and focus on a narrow range of tasks. \n- The diversity entropy based on Parzen window estimation and the learnability factors are rigorously defined\n- The valuation metrics are strongly correlated with downstream performance."}, "weaknesses": {"value": "- From my understanding, the contribution is primarily on developing a new data valuation method. If this is the case, the author should compare the method to other data valuation methods in the related works section. Currently, the related works section only covers Embodied datasets and VLA Models, which are not very useful to understand the contribution of the work. \n- While the abstract claims the model use \"unified multimodal representation\", this representation is in fact just video frames, as described in Sec. 3.1. It would make the work more interesting if the representation can leverage information across multiple modalities, as I'm not entirely convinced different modalities represent same information.  \n- While authors test the method on multiple datasets, they only used one model (OpenVLA-7B) to validate model's downstream performance. It's not clear whether the performance/correlation transfers to other models. \n- Figure 6 caption: multimodle -> multimodal. Please proofread for other spelling mistakes."}, "questions": {"value": "See weaknesses. I would consider raising my score if the author could provide a comprehensive comparison against other works in data valuation domain, and evaluate the method on models other than OpenVLA."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VM5Tsm4mVP", "forum": "uFomU6UJHH", "replyto": "uFomU6UJHH", "signatures": ["ICLR.cc/2026/Conference/Submission238/Reviewer_rkwR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission238/Reviewer_rkwR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762531750792, "cdate": 1762531750792, "tmdate": 1762915477175, "mdate": 1762915477175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the underexplored problem of quantitatively evaluating the diversity and learnability of embodied datasets. The authors propose two data-driven, training-free metrics: (1) Diversity Entropy, a continuous measure of dataset information richness derived from unified multimodal representations, and (2) a Learnability Estimator, an interpretable algorithm that predicts how easily a model can learn from a dataset without training. The authors validate their approach on 21 simulated and real-world embodied datasets, including Libero, BridgeData, and Fractal, and show strong correlations between predicted and empirically measured learnability. The study suggests that dataset diversity and learnability can be meaningfully assessed without costly model retraining."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors provide the first quantitative and interpretable metrics for assessing embodied dataset diversity and learnability without model retraining.\n- They demonstrate strong empirical validation across diverse datasets, suggesting robustness."}, "weaknesses": {"value": "- The reliance on CLIP as a universal multimodal encoder limits generality; alternative embeddings (e.g., OpenVLA latent space) could yield different results.\n- The proposed metrics are heuristic approximations, not theoretically guaranteed proxies for model learnability.\n- Real-world validations are limited (two UR5 datasets); broader experimental diversity would strengthen claims.\n- Diversity entropy depends heavily on bandwidth and kernel choice, yet sensitivity analysis is missing.\n- The learnability estimator, while interpretable, introduces many hyperparameters (β, σ_model, σ_center) that lack principled tuning or uncertainty reporting.\n- The discussion underplays potential biases in CLIP-derived latent spaces and how they may distort entropy measurements."}, "questions": {"value": "- How sensitive are diversity entropy and learnability metrics to the CLIP feature space choice or visual encoder type?\n- Could the authors compare their metrics to mutual information or intrinsic dimension estimators as alternative diversity measures?\n- How would the proposed framework scale to video-based embodied datasets exceeding terabyte scale?\n- Is the learnability estimator applicable to language-conditioned or 3D embodied tasks, where temporal semantics differ?\n- Can the authors provide a reproducible toolkit or open-source implementation for community use?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sqCeDTczjr", "forum": "uFomU6UJHH", "replyto": "uFomU6UJHH", "signatures": ["ICLR.cc/2026/Conference/Submission238/Reviewer_N6CJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission238/Reviewer_N6CJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762610549373, "cdate": 1762610549373, "tmdate": 1762915477025, "mdate": 1762915477025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}