{"id": "yHzCDP1tXw", "number": 2423, "cdate": 1757082284871, "mdate": 1759898149149, "content": {"title": "MMSU: A Massive Multi-task Spoken Language Understanding and Reasoning Benchmark", "abstract": "Speech inherently contains rich acoustic information that extends far beyond the textual language. In real-world spoken communication, effective interpretation often requires integrating semantic meaning (e.g., content), paralinguistic features (e.g., emotions, speed, pitch) and phonological characteristics (e.g., prosody, intonation, rhythm), which are embedded in speech. While recent multimodal Speech Large Language Models (SpeechLLMs) have demonstrated remarkable capabilities in processing audio, their ability to perform fine-grained perception and complex reasoning in natural speech remains largely unexplored. To address this gap, we introduce MMSU, a comprehensive benchmark designed specifically for understanding and reasoning in speech. MMSU comprises 5,000 meticulously curated audio-question-answer triplets across 47 distinct tasks. Notably, linguistic theory forms the foundation of speech language understanding (SLU), yet existing benchmarks have paid insufficient attention to this fundamental aspect and fail to capture the broader linguistic picture. To ground our benchmark in linguistic principles, we systematically incorporate a wide range of linguistic phenomena, including phonetics, prosody, rhetoric, syntactics, semantics, and paralinguistics. Through a rigorous evaluation of 22 advanced SpeechLLMs, we identify substantial room for improvement in existing models. MMSU establishes a new standard for comprehensive assessment of SLLU, providing valuable insights for developing more sophisticated human-AI speech interaction systems.", "tldr": "", "keywords": ["SpeechLLMs", "Multimodal", "Speech Processing", "Linguistics", "LLM"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c611e59531cdb5e1fac3875ea3c91310a626c7b9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed a comprehensive benchmark for evaluating Speech LLMs on perception and reasoning tasks. They end up curating a total of 47 distinct tasks with 5000 audio-question-answer triplets. Their taxonomy and definition of tasks are expert-grounded, including both paralinguistics (emotion, speed, pitch) and linguistics (phonetics, prosody, rhetoric, syntactics, semantics). They also evaluate 22 Speech LLMs on the proposed benchmark and provide some insights and shortcomings of current SoTA models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. I deeply appreciate the efforts the authors have made for curating such a comprehensive audio benchmark with a reasonable taxonomy, and I believe this is beneficial to the future speech/audio community.\n2. Compared to prior works on audio benchmarks, their evaluation tasks are more diverse and more organized. Also, it employs more real speech/audio for evaluation.\n3. They evaluated a massive amount of Speech LLMs on their proposed benchmark."}, "weaknesses": {"value": "1. I don’t fully understand why Speech LLMs are generally bad at perception tasks but good at complex reasoning tasks. Shouldn’t the model have to be able to perceive the acoustic cues and then do the reasoning? Or simply because Speech LLMs are good at “guessing” the answer? Or because the curated reasoning tasks are too easy?\n2. Following the first point, why does Qwen2.5-Omni-7B outperform humans by 6% on Reasoning Linguistic (Semantics)? Is there anything wrong with the curated eval data for this task?\n3. The selection of models for evaluation is somewhat less informative. I’m not sure why the author chose this selection of models. From Table 1, it seems like all models have similar size parameters (7~10B), and some of the models are quite old. I think readers would be more interested in recent models and also models that are diverse in different sizes. For instance, I think Voxtral Small 24B (https://arxiv.org/abs/2507.13264) or StepAudio 130B (https://arxiv.org/abs/2502.11946) might be a good option to include. Or other suitable open source models with larger size."}, "questions": {"value": "1. It would be helpful if the author could refer to the section of the Appendix rather than just refer to the entire Appendix in the main manuscript.\n2. For Table 1, I think it would be beneficial to know the text/speech tokens or instruction samples these models are trained on. I hypothesize that it is quite related to the performance, even though most models in the table have a similar number of parameters. It’s just a suggestion, I can fully understand that you might have some space constraints for the paper, and this is not the core information of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W3CSS22j8D", "forum": "yHzCDP1tXw", "replyto": "yHzCDP1tXw", "signatures": ["ICLR.cc/2026/Conference/Submission2423/Reviewer_oqyG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2423/Reviewer_oqyG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761233970159, "cdate": 1761233970159, "tmdate": 1762916232902, "mdate": 1762916232902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MMSU, a linguistics-grounded benchmark for evaluating the spoken language understanding and reasoning capabilities of Speech Large Language Models (SpeechLLMs), analogous to MMAU for general audio (e.g., environmental sounds). The benchmark comprises 5,000 audio–question–answer triplets across 47 tasks spanning both perceptual and higher-level reasoning abilities. The authors rigorously evaluate 22 open-source and proprietary SpeechLLMs/OmniLLMs, revealing systematic weaknesses relative to human performance and providing substantive, diagnostic discussion."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* The systematic incorporation of core linguistic dimensions (phonetics, prosody, syntax, semantics, paralinguistics) provides a solid theoretical foundation that underscores the importance of this work and highlights genuine gaps in existing benchmarks.\n* The hierarchical task organization (perception vs. reasoning × linguistics vs. paralinguistics) is well-motivated and grounded in human cognitive processes as well as established linguistic theory. This design enables interpretable evaluation and facilitates precise identification of strengths and weaknesses across SpeechLLMs.\n* The evaluation of 22 existing systems represents a non-trivial experimental effort, yielding valuable insights and actionable diagnostic analyses for future SpeechLLM development."}, "weaknesses": {"value": "* The paper aims to re-center the importance of speech-centric tasks—those that cannot be reduced to simple spoken versions of NLP tasks or solved purely through surface-level semantics—and thereby re-emphasize what truly distinguishes SpeechLLMs from text-based LLMs. However, several of the newly proposed tasks (e.g., disfluency detection, pause perception, syllable perception, consonant/vowel perception, and polysemy reasoning) still appear solvable by a cascade system (ASR + LLM). The current baseline comparison does not include such a cascade setup, which limits the interpretability of the reported results and weakens the contextual grounding of how challenging this benchmark truly is.\n* I have some concerns about the use of GPT-4o for generating candidate answer choices, as this may introduce stylistic regularities that certain models could exploit through elimination heuristics, especially under a multiple-choice question setting. Moreover, the paper does not clearly describe how the questions were finalized or refined. It only mentions that the materials were adapted from linguistic textbooks, leaving unclear whether human validation or linguistic polishing was performed.\n* For an ICLR publication, I would expect deeper analytical discussion on design factors influencing model performance across linguistic dimensions. For instance, the observed weakness in phonological tasks: does it stem from insufficient exposure to task-specific instruction-tuning data (a knowledge limitation), or from the audio encoder’s inability to discriminate low-level acoustic contrasts such as intonation or duration (a perceptual limitation)? Additionally, the paper could more explicitly examine cross-task correlations, such as the relationship between speech stress perception (a perceptual task) and stress-based reasoning (its reasoning task counterpart), to better illuminate hierarchical dependencies in speech understanding.\n* [Minor] The dataset size—5,000 examples across 47 tasks (~100 samples per task)—seems relatively small given the diversity of phenomena covered. Reporting statistical significance or confidence intervals for model comparisons would strengthen the validity of the conclusions drawn from such limited samples."}, "questions": {"value": "* The paper adopts a multiple-choice question answering framework as its primary evaluation method, which is reasonable given the precedents set by benchmarks such as MMAU and MMLU. However, certain tasks might be more appropriately evaluated using an open-form format that captures richer reasoning or perceptual nuances (for example, tasks involving prosody interpretation, pragmatic inference, or long-form summarization). Could the authors elaborate further on this design choice and clarify why multiple-choice was preferred over alternative formats?\n* I can also imagine several linguistic-related speech tasks that are not currently included in the benchmark. For instance, morphology (e.g., morphosyntactic agreement) and interactional linguistics (e.g., distinguishing floor-taking from backchanneling). Including a brief discussion section on potentially overlooked linguistic dimensions would strengthen the work and provide useful context for future dataset extensions.\n* It would be helpful to clarify whether and how the leaderboard will be made publicly available.\n* Some models may have been exposed to portions of the audio data used in this benchmark during pretraining. How might such overlap affect the reported performance, and are there any mitigation strategies to ensure fairness and validity in evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "njX7To37Lp", "forum": "yHzCDP1tXw", "replyto": "yHzCDP1tXw", "signatures": ["ICLR.cc/2026/Conference/Submission2423/Reviewer_J8HX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2423/Reviewer_J8HX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866497204, "cdate": 1761866497204, "tmdate": 1762916232770, "mdate": 1762916232770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MMSU, a new benchmark designed to evaluate the spoken language understanding and reasoning abilities of SpeechSpeechLLMs.\nThe authors argue that existing benchmarks fail to capture the complexity of real-world speech, as they often focus on semantic content (the words spoken) while ignoring crucial acoustic information (how they are said). MMSU addresses this gap with 5,000 audio-question-answer triplets across 47 distinct tasks. These tasks are grounded in linguistic theory (phonetics, prosody, semantics, etc.) and use high-quality, authentic audio rather than relying heavily on synthetic speech.\nThe benchmark tests 24 perception tasks (e.g., intonation perception, speaker identification) and 23 \"reasoning\" tasks (e.g., sarcasm detection, emotional context reasoning)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths of the MMSU benchmark includes:\n* It is the first benchmark to be systematically grounded in linguistic theory. This allows it to test nuanced areas that other benchmarks miss, including phonetics, prosody, semantics, and paralinguistics.\n\n* MMSU is reasonable in size, providing 5,000 audio-question-answer triplets across 47 distinct tasks. It uniquely categorizes these tasks into 24 \"perception\" abilities (e.g., intonation perception) and 23 \"reasoning\" abilities (e.g., sarcasm detection).\n\n* The benchmark emphasizes \"acoustic authenticity\" by using audio from real-world sources and professional studio recordings. This directly addresses a major gap left by other benchmarks that heavily rely on TTS-synthesized audio, which lacks real human variability."}, "weaknesses": {"value": "Key weaknesses identified in the MMSU benchmark from my perspective include:\n\n* Missing Quantitative Reliability Metrics (IAA): The paper does not report standard inter-annotator agreement (IAA) scores (e.g., Cohen’s Kappa) to validate its dataset. While it details a rigorous, multi-stage review process to force consensus, this is a procedural fix, not a quantitative measurement. By omitting the initial agreement score, the paper obscures the potential inherent ambiguity of its 47 tasks and makes the annotation scheme's objectivity and replicability difficult to verify.\n\n* Fragile Evaluation Format: The benchmark's reliance on a multiple-choice question answering (MCQA) format could be seen as a weakness. Research indicates this format can be fragile, as a model's accuracy can change \"substantially\" simply by reordering the answer options or slightly rephrasing the question. This suggests a high score may not correlate with robust, real-world understanding.\n\n* Limited Conversational Complexity: MMSU could be criticized for inadequately representing the complexity of realistic, dynamic conversations. Its tasks focus heavily on single-speaker or simple scenarios and lack real-world challenges like overlapping audio, long-form inputs, and, most notably, \"speaker-attributed reasoning\" (knowing who said what) in multi-participant dialogues."}, "questions": {"value": "1- Could you elaborate on why you opted for this procedural approach over reporting standard quantitative metrics, like Cohen’s Kappa, for the initial annotation pass? Reporting the initial agreement scores before reconciliation would be invaluable for the community to understand the inherent ambiguity and objective difficulty of these 47 novel tasks for human annotators.\n\n2- Given that multiple-choice benchmarks can be susceptible to fragility—where model scores change significantly based on option order or distractor choice—what steps were taken to validate that MMSU measures genuine understanding rather than format-solving? For instance, did you test model robustness by evaluating performance with shuffled option orders, and did you consider alternative formats like open-ended answers?\n\n3- The benchmark's tasks primarily focus on single-speaker audio. This omits real-world complexities like overlapping speech, interruptions, and speaker-attributed reasoning (knowing who said what). Could you explain the rationale for this scoping decision, and how do you suggest this significant gap in multi-participant conversational understanding be addressed in future work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Yx1IOlOVGC", "forum": "yHzCDP1tXw", "replyto": "yHzCDP1tXw", "signatures": ["ICLR.cc/2026/Conference/Submission2423/Reviewer_W1An"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2423/Reviewer_W1An"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887670730, "cdate": 1761887670730, "tmdate": 1762916232663, "mdate": 1762916232663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MMSU, a comprehensive benchmark for evaluating spoken language understanding and reasoning abilities of SpeechLLMs. It contains 5,000 audio–question–answer samples across 47 diverse tasks, designed with help from linguistics experts. MMSU emphasizes real human recordings, linguistic theory, and fine-grained acoustic phenomena like prosody, intonation, stress, disfluency, and emotion. The benchmark covers two major categories — Perception and Reasoning.\nThe paper evaluates open-source and commercial SpeechLLMs, showing that even top systems like Gemini-1.5-Pro and Qwen2.5-Omni 7B reach only around 60% accuracy, while human accuracy is ~90%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- It covers 47 distinct speech tasks which is huge and comprehensive for SLU\n- Use of real world recoding and voice actors to produce authentic audio\n- The paper gives detailed error analysis, highlighting that most models fail in phonological and paralinguistic reasoning, not just semantics which is an important insight for future SpeechLLM research.\n- Overall a well written paper."}, "weaknesses": {"value": "- MCQ-based tasks might not be reliable to judge a model capabilities as selecting the chances of selecting a right answers is 25% and the model might hallucinate."}, "questions": {"value": "- MMSU focuses solely on multiple-choice tasks, which could skew results towards models trained for MCQ-type question-answering and possibly even contrastive models. It would be beneficial to include an open-ended subset, even a small one, to contrast performance with the close-ended tasks.\n- It would be great if the authors could explain the evaluation strategy more? Choosing any option from A-D is not sufficient as the answers from different models might be correct but differ semantically."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KdbFk0M7eI", "forum": "yHzCDP1tXw", "replyto": "yHzCDP1tXw", "signatures": ["ICLR.cc/2026/Conference/Submission2423/Reviewer_654X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2423/Reviewer_654X"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976821033, "cdate": 1761976821033, "tmdate": 1762916232391, "mdate": 1762916232391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new benchmark, MMSU, that deeply focuses on evaluating speech understanding capabilities of speech LLMs. Existing benchmarks often focus on general audio capabilities and don’t explore the full space of paralinguistic and phonological aspects of speech. This paper targets these with 47 tasks categorized as perception (roughly surface-level tasks) and reasoning (more intelligent tasks). The authors use linguistic experts to create task definitions, curate audio from three sources: existing audio datasets, custom-recorded audio, and synthetic audio, and perform manual review. They evaluate 22 speech LLMs and reveal gaps in fine-grained acoustic perception and phonological perception."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The dataset looks extremely useful and focuses deeply on speech evaluation rather than general audio, which I appreciate. Its size is also reasonably large, allowing robust evaluation.\n2. Most of the audio is human-generated, which is closer to real-world scenarios than datasets that primarily use synthetic audio data.\n3. The experiments and evaluations are quite thorough, covering a good variety of speech LLMs. The insights are actionable as well e.g. phonology-based understanding is poor for LLMs. The error analysis also shows interesting patterns, like a significant proportion being due to refuse-to-answer for GPT-4o.\n4. I especially appreciate the human baseline, which shows a clear and attainable upper bound for models to reach. Very few benchmarks take the effort to create a human baseline."}, "weaknesses": {"value": "1. The dataset creation process involves GPT-4o in-the-loop to augment distractor options. This could potentially create biases in the dataset that might be exploitable by speech LLMs. What percentage of the final distractor options were human-written vs. LLM-generated? Could the authors perform some analysis to check whether this introduces a bias that favors/disfavors LLM-based speech models?\n2. The manual review process at the end of the dataset collection is not described completely. For example, the paper mentions that only the data that meets the required standards is kept, but it’s not entirely clear what these standards are to me. I’d appreciate a more elaborate description.\n3. The MCQ format might allow models to take shortcuts; if the distractors are not confusing enough, the correct option can be intelligently selected without listening to the audio. It would be good if there was a question-and-options-only baseline with a strong text LLM that shows how much the models can get without access to the audio."}, "questions": {"value": "1. The introduction mentions that many existing benchmarks have too much TTS-generated audio. This benchmark has about 10% TTS synthetic audio data as well. What was the rationale for doing this, and for what tasks is the data synthetic vs real?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xDHDnEaCM4", "forum": "yHzCDP1tXw", "replyto": "yHzCDP1tXw", "signatures": ["ICLR.cc/2026/Conference/Submission2423/Reviewer_jD81"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2423/Reviewer_jD81"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission2423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762378786088, "cdate": 1762378786088, "tmdate": 1762916232179, "mdate": 1762916232179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}