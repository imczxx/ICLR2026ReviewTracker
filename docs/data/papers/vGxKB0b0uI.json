{"id": "vGxKB0b0uI", "number": 17918, "cdate": 1758282006851, "mdate": 1759897145134, "content": {"title": "SIREN: Subgraph Isomorphism via Reinforcement Enhanced Graph Neural Networks", "abstract": "The subgraph isomorphism problem comprises two distinct objectives: (a) Existence determination: Verifying whether an input graph contains a subgraph isomorphic to another input graph; and (b) Complete solution enumeration: Outputting the exhaustive set of all isomorphic mappings when they exist. Solving this problem serves as a fundamental requirement for numerous application domains. However, as an NP-complete problem, existing mainstream solvers primarily rely on heuristic techniques, demonstrating limited efficiency when handling large-scale input graphs. To address this challenge, we propose SIREN - a graph neural network enhanced with deep reinforcement learning for subgraph isomorphism resolution. SIREN establishes graph embeddings through partial order-aware GNNs, while employing Deep Q-Networks with bidomain-based pruning to accelerate the graph matching process. Experimental results on real-world datasets demonstrate that SIREN achieves 100% precision with modest computational time, outperforming AI-based approximate matching methods. Compared to state-of-the-art exact solvers, SIREN delivers 52% faster execution than leading AI approaches and 21% acceleration over top heuristic methods.", "tldr": "Solving subgraph isomorphism problem via deep reinforcement learning and partial order-aware GNN model", "keywords": ["Subgraph Isomorphism", "Reinforcement Learning", "Graph Neural Network", "Partial Order"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3430a47306595a76e2640d880f964e9f0975c919.pdf", "supplementary_material": "/attachment/c91fe61e3658f345cae5229fd1058c7e4d9e749d.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents SIREN, a novel framework that integrates a Deep Q-Network (DQN) with a partial order-aware Graph Neural Network (GNN) to solve the subgraph isomorphism problem. The key innovation lies in combining reinforcement learning to guide a tree-search process with GNN embeddings that preserve subgraph partial order relationships. The authors claim that SIREN achieves 100% precision (exact solutions) while being significantly faster than both state-of-the-art heuristic and learning-based methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The integration of DQN with a partial order-aware GNN for subgraph isomorphism is innovative. The idea of using RL to learn a node selection heuristic, replacing hand-crafted rules in traditional solvers, is a promising research direction.\n\n2. The paper provides a theoretical discussion on probabilistic completeness, which is a crucial property for an exact solver and distinguishes it from many approximate learning-based methods."}, "weaknesses": {"value": "1. The paper seems to omit key metrics. It reports only accuracy (not runtime) on trivial small graphs where SIREN may be slower than heuristic methods, and only runtime (not accuracy) on large graphs where its precision is unproven.\n\n2. The method is designed and tested only on graphs with topological structure, ignoring the critical challenge of node and edge attributes. This severely limits its applicability to real-world problems where matching is defined by both structure and features (e.g., molecular graphs).\n\n3. The bidomain-based pruning, while theoretically sound, incurs an exponential cost that could easily become a computational bottleneck on larger or sparser graphs, potentially negating its intended benefits."}, "questions": {"value": "1. The training process involves three phases (pretraining, imitation, RL). How stable is this pipeline, and what is the relative contribution of each phase to the final performance? Is the RL phase essential, or does most of the performance come from imitating VF3?\n\n2. The partial order-aware GNN is central to the node embeddings. How does its performance compare to a standard, off-the-shelf GNN model on this task? Is the partial order constraint the key factor, or is the gain primarily from the GNN's general representation power?\n\n3. Given the complexity of the full SIREN pipeline, did you explore a simpler alternative? What is the evidence that the full RL-based search is necessary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vTeXcnEwHW", "forum": "vGxKB0b0uI", "replyto": "vGxKB0b0uI", "signatures": ["ICLR.cc/2026/Conference/Submission17918/Reviewer_8kgs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17918/Reviewer_8kgs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761542710142, "cdate": 1761542710142, "tmdate": 1762927735552, "mdate": 1762927735552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SIREN, combining Deep Q-Networks with partial order-aware GNNs to solve the subgraph isomorphism problem. The method uses DQN to learn node selection heuristics while maintaining provable completeness through bidomain-based pruning. The authors claim 100% precision on approximate matching tasks with modest computational time and speedups of ~52% over AI methods and ~21% over heuristics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The \"partial order-aware\" GNN pretraining is a clever and valuable contribution. Using a max-margin loss to enforce the geometric partial order is an elegant way to capture subgraph structure. \n(2) AI + Search Framework: The overall approach of integrating a learned (DQN) policy with a formal, complete search algorithm (enhanced with bidomain pruning) is a strong and practical direction for tackling NP-complete combinatorial problems. \n(3) The reported results are impressive. Achieving 100% MAP (Table 1) against approximate methods validates the \"exact\" nature of the solver. Furthermore, the claimed speedups against a wide range of strong baselines (Figure 2, Table 3), including GNN-PE, VEQ, and parallel methods like VF3p, suggest that the learned search policy is highly effective."}, "weaknesses": {"value": "(1) Contribution 2 says explicitly that “a partial-order-aware GNN pretraining strategy” \"eliminates dependency on solver-generated labels.\" While this is true for the GNN pretraining (Section 3.2), the DQN training (Appendix A.5) is heavily dependent on a traditional solver. The training process involves a 3-stage curriculum (Pretraining, Imitation Learning, RL) where the first two stages use VF3 to generate \"high-quality initial samples\" and \"demonstration trajectories.\" This re-introduces the very solver bottleneck the paper claims to eliminate and makes the novelty of Contribution 2 feel less impactful on the entire system.\n(2) The paper says “provable completeness” but what is actually proved is probabilistic completeness under a non-decaying ε-greedy exploration and with the assumption that the search can run long enough. That is not the same as the deterministically complete, bounded-depth, fully explored search in classic subgraph solvers. There is no rigorous proof that the ε-greedy DQN policy will visit all states in finite time.\n(3) Code is promised “upon acceptance,” so current results cannot be verified.\n(4) In Section 4.2, “In SIREN, we utilize 8 layers of Graph Isomorphism Networks (GIN) Xu et al. (2019) each with 64 dimensions for the embeddings.” However, from Table 1, it seems that all baselines use 3-5 layers. It is unclear how baselines perform with more parameters in this paper."}, "questions": {"value": "(1) Given that SIREN also fails on 10K+ node dense graphs, how is this a fundamental improvement over baselines that fail on the same instances?\n(2) Since GIN is WL-power-equivalent, how do you handle cases where WL cannot distinguish candidate nodes but SI still requires distinguishing them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8VBP26CCIS", "forum": "vGxKB0b0uI", "replyto": "vGxKB0b0uI", "signatures": ["ICLR.cc/2026/Conference/Submission17918/Reviewer_TGyc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17918/Reviewer_TGyc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872403252, "cdate": 1761872403252, "tmdate": 1762927735106, "mdate": 1762927735106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SIREN, a novel framework that integrates Graph Neural Networks (GNNs) with Deep Reinforcement Learning (DQN) to solve the subgraph isomorphism problem. The method addresses both existence determination and complete solution enumeration with provable completeness. SIREN uses a partial-order-aware GNN to encode graph embeddings and a DQN with bidomain-based pruning to guide the search process efficiently. Extensive experiments on real-world and synthetic datasets show that SIREN achieves 100% precision and outperforms both traditional heuristic methods and recent neural approaches in terms of speed and accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "i)，Novel and Deep Methodological Integration: The paper's primary strength lies not in simply replacing parts of a traditional algorithm with AI, but in a deep architectural fusion of three distinct paradigms: 1，Reinforcement Learning as a Search Guide: A Deep Q-Network (DQN) is used to learn an optimal policy for selecting the next node pair during the tree search. This moves beyond hand-crafted heuristics to a data-driven, adaptive strategy. 2，Graph Neural Networks as State Representers: A custom GNN is employed to generate meaningful embeddings for the state (the current partial mapping and the graphs) and actions (candidate node pairs). This allows the RL agent to reason about graph structure in a continuous space.\nii), Theoretically Grounded and Self-Supervised Pretraining: The introduction of the partial-order-aware GNN is a significant contribution that addresses a major bottleneck in learning-based approaches. It explicitly models the subgraph isomorphism relationship as a partial order in the embedding space. The loss function is designed to ensure that if G1 is isomorphic to a subgraph of G2, then each dimension of G1's embedding is less than or equal to that of G2's."}, "weaknesses": {"value": "i), Inherent Scalability Limitations of the Approach: While SIREN outperforms other methods, it does not overcome the fundamental NP-hard nature of the problem. The framework still relies on a tree search whose worst-case time complexity remains exponential. The experiments show that on \"ultra-dense, large-scale graphs\" (e.g., >10^4 nodes, density ρ>0.3), SIREN, like all other solvers, becomes impractically slow. The claim of \"polynomial-time behavior\" in practice is observed, but not proven, and is likely highly dependent on graph structure. The memory footprint could be a critical constraint. The use of GNNs for embedding generation and a DQN for action selection, especially with large action spaces that require partitioning, implies high GPU memory usage. This could limit its application on hardware with constrained resources.\nii), Complex and Computationally Expensive Training Pipeline: The three-phase training process (Pretraining, Imitation Learning, Reinforcement Learning) is a double-edged sword. It requires running a traditional solver (VF3) to generate initial demonstration data, which itself can be slow. The need to train multiple components (GNN and DQN) sequentially makes the overall training process complex, long, and computationally intensive. This high barrier to entry could hinder adoption and make hyperparameter tuning difficult.\niii), Narrow Focus and Unverified Generalizability: 1,The method is highly specialized for the exact subgraph isomorphism problem. Its performance on related but distinct tasks like graph similarity, graph edit distance, or inexact matching is not explored. 2, The promise to extend to other NP problems (like the maximum clique problem) remains future work. The architecture's dependency on the specific \"bidomain\" structure for subgraph isomorphism may not translate directly to other problem domains."}, "questions": {"value": "1，How does SIREN handle graphs with noisy or missing labels? The current method assumes clean graph structures; robustness to noisy data is not evaluated.\n\n2， Could the framework be adapted for inexact or fuzzy subgraph matching? The current focus is on exact isomorphism; extending to approximate matching would broaden applicability.\n\n3， What is the performance of SIREN on graphs with heterogeneous node/edge types? The experiments primarily use homogeneous graphs; generalization to heterogeneous graphs is unclear.\n\n4， The very recent and highly related work D2Match (Liu et al., ICML 2023) also addresses the subgraph matching problem by combining deep learning with theoretical degeneracy. Please provide a detailed comparison between SIREN and D2Match, highlighting your method's relative advantages and limitations in terms of methodological approach, theoretical guarantees, and computational complexity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0MF9nMwund", "forum": "vGxKB0b0uI", "replyto": "vGxKB0b0uI", "signatures": ["ICLR.cc/2026/Conference/Submission17918/Reviewer_qaH3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17918/Reviewer_qaH3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762135674536, "cdate": 1762135674536, "tmdate": 1762927734687, "mdate": 1762927734687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SIREN, a framework that leverages a poset-aware GNN to produce structurally consistent graph/state embeddings, combined with a DQN-based policy to learn node-pair matching order and backtracking strategies. Together with bidomain-based upper bounds and pruning, SIREN aims to accelerate subgraph isomorphism search. The authors report 100% MAP on approximate matching tasks and significant speedups over heuristic solvers and GNN-PE on exact matching/enumeration benchmarks. The training pipeline consists of a large-scale supervised pretraining stage (50k epochs), followed by reinforcement learning fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Well-structured and modular design. The decomposition into state embedding, action embedding, upper-bound–guided pruning, and action-block evaluation is clear and reproducible. The interaction between action embedding ( $W_a$ ) and bidomain constraints in the Q-function is explicitly defined.\n2. Comprehensive empirical evaluation across accuracy, efficiency, and scalability. The experiments cover approximate prediction, exact solving, and large/dense synthetic graphs.\n3. Discussion of (conditional) completeness. The appendix uses the notion of probabilistic completeness, linking it to sufficient time and persistent exploration, which avoids portraying RL search as a black-box heuristic."}, "weaknesses": {"value": "1. Ambiguity in the “completeness” claim.\n    \n    The main text repeatedly uses terms such as “provably complete” and “guarantees completeness,” whereas Appendix A.4 explicitly describes probabilistic completeness relying on assumptions such as (\\epsilon)-greedy exploration and persistent exploration over sufficient time.\n    \n2. The reported 100% MAP is not strong evidence of perfect isomorphism prediction.\n    \n    Table 1 reports MAP (=1.000) on TU datasets with small graphs ((|V_Q|\\le 15, |V_T|\\le 20)), but MAP in such settings does not guarantee perfect isomorphism decision. The result appears more like a special case of “approximate retrieval + small candidate sets,” and it is unclear whether the method maintains such performance on larger graphs, weaker labels, or more challenging structural variations.\n    \n    More robust metrics (precision/recall, false positive/negative analysis, Top-(k) success rates) are needed.\n    \n3. Fairness of comparisons with exact solvers.\n    \n    Efficiency comparisons fix (|V(q)| = 8), which may be unfavorable to certain baselines such as GNN-PE.\n    \n    A more systematic study—scaling (|V(q)|) from 4 to 64 and analyzing varying graph density/label entropy—is necessary to confirm consistent advantages beyond a single configuration.\n    \n4. Extremely high training cost and unclear generalization ability.\n    \n    The pipeline requires 50k epochs of supervised pretraining plus 10k RL steps (with neighborhood resampling every 50 epochs), which is computationally expensive. The paper does not demonstrate out-of-distribution generalization—e.g., when structural distributions, label vocabularies, noise levels, or missing attributes differ between training and test."}, "questions": {"value": "1. Completeness.\n    \n    Is the “provably complete” claim in the main text referring to deterministic completeness or probabilistic completeness?\n    \n    Do the assumptions in Appendix A.4 (such as (\\epsilon)-greedy, persistent exploration, and Markov chain ergodicity) hold during test-time inference?\n    \n    If the deployed policy is purely greedy, does completeness still hold?\n    \n2. Evaluation metrics.\n    \n    In MAP evaluation, how do candidate-set construction, the number of negative samples, and graph sizes affect the reported MAP (=100%)?\n    \n    Please clarify whether the conclusion is sensitive to these design choices.\n    \n3. Training cost and generalization.\n    \n    What is the total training time and compute budget for the 50k+10k training pipeline?\n    \n    Has the model been tested on large graphs of unseen types without retraining?\n    \n4. Fairness of comparison settings.\n    \n    Does the performance degrade at larger query sizes?\n    \n    Please provide (|V(q)|)-scaling experiments and memory/time curves to evaluate robustness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sk8AKQQVFQ", "forum": "vGxKB0b0uI", "replyto": "vGxKB0b0uI", "signatures": ["ICLR.cc/2026/Conference/Submission17918/Reviewer_8b3e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17918/Reviewer_8b3e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762521938333, "cdate": 1762521938333, "tmdate": 1762927733906, "mdate": 1762927733906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}