{"id": "nkm3lL8CQE", "number": 11759, "cdate": 1758203563797, "mdate": 1763676394893, "content": {"title": "Discrepancy-Aware Knowledge Distillation for Large Language Models", "abstract": "Knowledge Distillation (KD) is a key technique for enhancing the capabilities of student models by transferring knowledge from powerful teachers. In Large Language Models (LLMs), however, the effectiveness of this transfer is fundamentally limited by distributional mismatch. The generic data used for distillation often fails to reflect the specialized distribution underpinning core expertise of the teacher. This gap hinders the acquisition of the teacher's most valuable capabilities. The challenge is fundamental because the ideal corrective method, importance weighting, is intractable without access to the unknown target density.\nWe propose Discrepancy Aware Knowledge Distillation (DAKD), a framework that re-frames this problem. Instead of estimating the unknown distribution, DAKD approximates the ideal importance weights by measuring the predictive discrepancy between the full teacher and a pre-trained-only base teacher, which serves as a distributional probe. The DAKD framework is \"discrepancy aware\" in a dual sense. It leverages the teacher-base divergence for distributional correction while using the teacher-student divergence for adaptive learning focus. This re-weighting is applied across multiple granularities, from the sequence and position down to the vocabulary level. Extensive experiments show that DAKD substantially outperforms state-of-the-art methods, enabling student models to more effectively inherit the nuanced capabilities of more powerful teachers.", "tldr": "", "keywords": ["knowledge distillation", "Large Language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/95eb3be468c481c8df412682ec29239fd21d9fa6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose DAKD, which essentially is a re-weighting of the standard KL divergence. Specifically, the author calculate the weights by considering at three levels: sequence, position, and vocabulary. The \"importance\" scores are calculated based on both the discrepancy between a finetuned teacher and a student, as well as between a finetuned teacher and pretrained teacher."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method show improvements over existing distillation methods.\n2. The authors experimented with both instruction tuning and reasoning."}, "weaknesses": {"value": "1. Higher performance could be due to hyper-parameter efforts. The authors combine the two discrepancies to produce these weights, which involves a alpha mixing hyper-parameter. As Table 3 shows, performance at 0 is pretty good, and as alpha goes to 1, half the values of alpha is better and the other half being worse.\n2. Additional cost. The proposed method uses extra compute, because it requires an additional pass on the pretrained teacher model, which could be hard to justify if improvement is not significant."}, "questions": {"value": "The paper states that \" Ideally, the student would be optimized under the teacher’s true data distribution, p⋆... However, p⋆ is often inaccessible\".\n\nWhy is this? We have the teacher's distribution and sampling from the teacher is a standard technique."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "REVAE2BNbT", "forum": "nkm3lL8CQE", "replyto": "nkm3lL8CQE", "signatures": ["ICLR.cc/2026/Conference/Submission11759/Reviewer_DXgg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11759/Reviewer_DXgg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11759/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758498713, "cdate": 1761758498713, "tmdate": 1762922785310, "mdate": 1762922785310, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers"}, "comment": {"value": "We sincerely appreciate the reviewers' time and the valuable feedback they have provided for our paper. These constructive comments have been instrumental in enhancing the quality of our work. We are encouraged that reviewers recognized our method as “intuitively sound” (Reviewers xcEB and F4Uc), having “clear and well-justified motivation” (Reviewer F4Uc), and showing “superior experimental performance” (Reviewers xcEB and DXgg).\n\nBelow, we provide point-by-point responses to your comments and present the revisions made to the manuscript based on your suggestions. \n\n- New LLM Base: Heterogeneous models, and cross-tokenizer alignment.\n- Qualitative Analysis: Visualization results.\n- Hyperparameter Strategy: Annealing schedule for the parameter $\\lambda$.\n- Computational Cost Analysis: Comparison against baselines with 50% expanded data volume.\n- New Benchmark: AlpacaEval for preference alignment settings.\n- New Objectives: $\\alpha$-$\\beta$-divergence and $f$-divergence.\n\nAll revisions are highlighted in blue. We hope that our responses and additional experiments could address your concerns."}}, "id": "V40Lopv8M2", "forum": "nkm3lL8CQE", "replyto": "nkm3lL8CQE", "signatures": ["ICLR.cc/2026/Conference/Submission11759/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11759/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11759/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763672625625, "cdate": 1763672625625, "tmdate": 1763672625625, "mdate": 1763672625625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the suboptimal performance of knowledge distillation when applied to limited (and potentially suboptimal) datasets. Drawing on a similar rationale as importance sampling, the authors propose using the KL divergence between the instruction-tuned teacher model and its base version to quantify the importance of different samples/tokens/vocabulary items. Extensive empirical analysis validates the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method appears intuitively sound and effectively addresses the key research problem of performing knowledge distillation on limited (and potentially suboptimal) datasets.\n\n- The paper is well-structured and easily accessible to readers.\n\n- Experimental results demonstrate superior performance compared to baseline methods, with the effectiveness of different components being systematically validated."}, "weaknesses": {"value": "- **Potential Efficiency and Scaling Concerns**  \nThe proposed method requires running inference with both the instruction-tuned teacher model and the base model across the entire training dataset to compute importance weights. This additional computational overhead may be significant, particularly considering that similar computational resources could be allocated to expanding the dataset size - which might naturally address the core issue of limited and suboptimal distillation data as raised in the introduction.\n\n- **Insufficient Experimental Validation.**  \nThe distinction between the instructed model and base model largely originates from the RLHF process designed to align the base model's outputs with human preferences (as acknowledged in the introduction). However, the experimental framework predominantly examines enhancements in reasoning capabilities, despite base models having already undergone extensive exposure to mathematical and reasoning-specific data during pretraining. This approach appears somewhat misaligned with the stated motivation. To more convincingly demonstrate the method's significance, we suggest supplementing the evaluation with experiments conducted under preference alignment settings.\n\n- **Missing Baselines**  \nThe study demonstrates effectiveness primarily using KL divergence. Given the rich variety of divergence measures in distillation literature (e.g., α-β-divergence [1], f-divergence [2]), it would be insightful to examine whether the observed improvements persist across different divergence formulations, thus providing a more thorough understanding of the method's robustness.\n\n[1] ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via α-β-Divergence. ICML 2025\n\n[2] f-Divergence Minimization for Sequence-Level Knowledge Distillation. ACL 2023."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ULqOuZdEwI", "forum": "nkm3lL8CQE", "replyto": "nkm3lL8CQE", "signatures": ["ICLR.cc/2026/Conference/Submission11759/Reviewer_xcEB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11759/Reviewer_xcEB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11759/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879431718, "cdate": 1761879431718, "tmdate": 1762922784467, "mdate": 1762922784467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Motivation of the paper: To address the problem of distribution mismatch between the high-quality knowledge in the teacher model and the knowledge learned by the student model during distillation, especially in scenarios where access to the teacher model's high-quality aligned data distribution is not available.\nMethod: The paper proposes a \"Difference-Aware Knowledge Distillation\" (DAKD) framework. Its key innovation lies in introducing a pre-trained \"base version\" of the teacher model as a distribution probe to measure the relevance of data to the core knowledge of the teacher.\nExperiments: The paper systematically validates the effectiveness of the DAKD method in surpassing state-of-the-art approaches across various scenarios, as well as the rationality and synergy of its multi-level, dual-signal design mechanism, through comprehensive performance comparisons, data efficiency analysis, model scale evaluation, and detailed ablation studies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper presents a clear and well-justified motivation, focusing on the important challenge of effective knowledge distillation from large to small models. \n2. The paper presents a simple and intuitive method, and Section 4.1 provides some theoretical justification for the proposed approach."}, "weaknesses": {"value": "1. The method may has certain limitations, such as the issue of cross-tokenization in real-world scenarios. and the method may be highly dependent on having access to both the base and SFT models simultaneously.\n2. The method yields limited performance gains when the knowledge sources are the same.\n3. The paper presents a very comprehensive quantitative analysis, It will be better that could provide a more intuitive demonstration of the method by some qualitative examples."}, "questions": {"value": "1. If an exact base model cannot be found, or we have only to different model(Qwen/LLaMA), can the DAKD method still work? If yes, what is the performance like?\n2. Regarding the hyperparameter λ that controls the \"difficulty\" of the learning \"curriculum\". A fixed λ may not be optimal. Have the authors considered annealing or adaptive strategies? For example, using a small λ in the early stages of training (resulting in a smoother weight distribution and encouraging broader exploration by the student), and gradually increasing λ as training progresses (leading to sharper weights and focusing the student on harder examples). Would this lead to more stable or better performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bvWnUjlmpt", "forum": "nkm3lL8CQE", "replyto": "nkm3lL8CQE", "signatures": ["ICLR.cc/2026/Conference/Submission11759/Reviewer_F4Uc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11759/Reviewer_F4Uc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11759/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882020015, "cdate": 1761882020015, "tmdate": 1762922783821, "mdate": 1762922783821, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}