{"id": "cWczH8ontO", "number": 9310, "cdate": 1758118396938, "mdate": 1763091021348, "content": {"title": "Video Generators are Robot Policies", "abstract": "Despite tremendous progress in dexterous manipulation, current visuomotor policies remain fundamentally limited by two challenges: they struggle to generalize under perceptual or behavioral distribution shifts, and their performance is constrained by the size of human demonstration data. In this paper, we use video generation as a proxy for robot policy learning to address both limitations simultaneously.\nWe propose Video Policy, a modular framework that combines video and action generation that can be trained end-to-end. Our results demonstrate that learning to generate videos of robot behavior allows for the extraction of policies with minimal demonstration data, significantly improving robustness and sample efficiency. Our method shows strong generalization to unseen objects, backgrounds, and tasks, both in simulation and the real world. We further highlight that task success is closely tied to the generated video, with action-free video data providing critical benefits for generalizing to novel tasks. By leveraging large-scale video generative models, we achieve superior performance compared to recent VLAs and video-action models, paving the way for more scalable and data-efficient robot policy learning.", "tldr": "", "keywords": ["Behavior Cloning", "Video Generation", "Representation Learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/cdf5affcaceddafbd0c7100e9ca873e764edfd23.pdf", "supplementary_material": "/attachment/524fa64b2ccd106dcea68fc0fcfa7b106bff0d0c.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Video Policy, a framework that treats video generation as a proxy for robot policy learning. Instead of training visuomotor policies purely from demonstrations, the authors show that video diffusion models can implicitly encode generalizable robot behavior, and a lightweight action decoder can map generated video representations into executable robot actions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong empirical results, especially on RoboCasa and Libero10, showing improved success rates over baselines with a compact architecture.\n2. Clarity and reproducibility: the paper is clearly written, with transparent architecture figures, ablations, and hyperparameter details."}, "weaknesses": {"value": "1. While the video diffusion model can be pretrained without actions, the action model still requires action-labeled data for fine-tuning. Therefore, the framework does not eliminate the need for action supervision. The title and framing (“action-free video learning”) are somewhat overstated.\n2. The action decoder is trained on limited demonstrations and does not inherit the generalization ability of the video diffusion model. As a result, its performance still depends heavily on the diversity and scale of the action-labeled subset, meaning the method’s scalability is only partial.\n3. The design closely resembles previous models such as Video Prediction Policy. Both of which also use video-conditioned architectures followed by action decoders."}, "questions": {"value": "1. The paper describes the video pretraining as action-free, yet the action diffusion model still depends on ground-truth action data. Could the authors clarify to what extent the approach actually reduces the reliance on action supervision compared to prior video-policy methods?\n2. Could the authors specify what architectural or training innovations account for their significant performance improvement? Is it primarily due to larger pretrained video priors or improved two-stage optimization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZXxZMnNREa", "forum": "cWczH8ontO", "replyto": "cWczH8ontO", "signatures": ["ICLR.cc/2026/Conference/Submission9310/Reviewer_6Bm3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9310/Reviewer_6Bm3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897363480, "cdate": 1761897363480, "tmdate": 1762920946464, "mdate": 1762920946464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "gG31nUI6Dh", "forum": "cWczH8ontO", "replyto": "cWczH8ontO", "signatures": ["ICLR.cc/2026/Conference/Submission9310/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9310/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763091020104, "cdate": 1763091020104, "tmdate": 1763091020104, "mdate": 1763091020104, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Video Policy, a framework that treats video generation as a proxy for robot policy learning. The central insight is that a video generative model, implicitly encodes policy information. A lightweight action decoder can then translate the video model’s latent dynamics into executable robot actions. The architecture jointly trains a video U-Net (adapted from Stable Video Diffusion, SVD) and an action U-Net that predicts robot end-effector actions conditioned on intermediate video features. The paper further highlights that action-free video data can enhance generalization to unseen tasks, suggesting that generative video modeling itself serves as a policy."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written. The introduction and method are easy to follow.\n\n- The experiments are very comprehensive to support the main claims in the paper.\n\n- The video prediction results look similar to the real world.\n\n- The results show that Video Policy is superior to baselines.\n\n- Using action-free data provides a potentially scalable way for data-driven robot learning."}, "weaknesses": {"value": "- The computation cost is high. The inference speed of the video model could slow down the policy rollout. Can the authors provide a comparison between Video Policy and other policy baselines? Can the authors propose some ways to accelerate the policy FPS?\n- It remains unclear if Video Policy still performs well in tasks with higher dynamics. The video model may show physics-inplausible results. It’s interesting if the authors can explore the behaviour of the policy and the video prediction model in more dynamic tasks.\n- Using more recent art in video models may improve the policy performance.\n- The idea of treating video prediction as a robot policy has already been proven working by some earlier work such as (Shuang et al,”Unified video action model.”). Can the authors provide some comparisons with these previous works."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I1lAnDeSYV", "forum": "cWczH8ontO", "replyto": "cWczH8ontO", "signatures": ["ICLR.cc/2026/Conference/Submission9310/Reviewer_hZje"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9310/Reviewer_hZje"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950348090, "cdate": 1761950348090, "tmdate": 1762920945089, "mdate": 1762920945089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes \"Video Policy,\" a framework for learning robot policies by leveraging pre-trained video generation models. The method uses a dual U-Net architecture: a fine-tuned Stable Video Diffusion (SVD) model generates future video frames, and a smaller action decoder, conditioned on the video model's internal features, generates corresponding actions. The central claim is that a two-stage training process—where the video model is frozen before training the action decoder—is superior to end-to-end joint training. The authors present results on the RoboCasa and Libero10 benchmarks, claiming state-of-the-art performance and high sample efficiency with as few as 50 demonstrations. \n\nHowever, I think the overall novelty of the paper is largely diluted by many related works that guide robot policy via video generation, and the main finding is not original as well."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Strong reported performance on benchmarks**: The paper reports high success rates on the RoboCasa (63% average) and Libero10 (94% average) benchmarks, outperforming several baselines, including some large-scale Vision-Language-Action (VLA) models, while using significantly less demonstration data. These results provide initial evidence for the potential of leveraging powerful video priors.  \n\n**Good video generation quality**: The authors give a good demonstration of the capability of the video generation model, which works well in most cases.\n\n**Good representation**: The paper delivery is clear, and the details are available for understanding the proposed method."}, "weaknesses": {"value": "**Overstated Novelty and Lack of Meaningful Comparison to Concurrent Work:** The paper frames the use of video models for policy learning as a novel contribution. This is a significant overstatement. The 2024-2025 period has seen a proliferation of work on this exact topic, including but not limited to concurrent models like **UVA**[1] , **UWM**[2] , and **VPP**[3] , which explore unified video-action architectures. The paper fails to properly situate itself within this crowded landscape, and its core novelty is limited to the specific finding about two-stage training, which is not so surprising, as action finetuning from a  pretrained video backbone seems a natural idea.\n\n**Prohibitive Computational Cost:** The reported inference time of **9 seconds for a single action sequence on an A100 GPU** is a severe limitation that is understated by the authors. This latency makes the system completely unsuitable for real-time, closed-loop control, which is a prerequisite for robotics in any dynamic environment. Dismissing this as a general problem that future hardware will solve is insufficient; it is a fundamental constraint on the proposed method's viability.  \n\n**Over dependent on the Video Generation Results** The policy's complete dependence on the video generator is a critical design flaw. The authors admit that real-world failures are caused by \"unrealistic video predictions\"—where the model imagines a physically impossible future. This creates a brittle, open-loop system. The paper proposes no mechanism to detect or recover from these generative failures, making the approach unreliable for practical applications.  \n\n**Reference**:\n\n[1] Li, S., Gao, Y., Sadigh, D., & Song, S. (2025). Unified Video Action Model. *ArXiv, abs/2503.00200*.\n\n[2] Zhu, C., Yu, R., Feng, S., Burchfiel, B., Shah, P., & Gupta, A. (2025). Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets. *ArXiv, abs/2504.02792*.\n\n[3] Hu, Y., Guo, Y., Wang, P., Chen, X., Wang, Y., Zhang, J., Sreenath, K., Lu, C., & Chen, J. (2024). Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations. *ArXiv, abs/2412.14803*."}, "questions": {"value": "**On Architectural Choices:** Why is a modular, causal `Video -> Action` framework fundamentally better than more integrated approaches like UVA's joint latent space or UWM's unified transformer? \n\n**On System Reliability:** Given that policy failures are directly caused by failures in video generation, the current system appears fundamentally brittle. What concrete mechanisms could be implemented to assess the physical plausibility of generated videos and enable the policy to recover from generative failures?\n\n**On Practical Deployment:** An inference time of 9 seconds is prohibitive for real-world robotics. Beyond general optimism about future diffusion model acceleration, what specific architectural or algorithmic changes to *your proposed method* do you see as the most viable path to achieving real-time control?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kS39pdLgGa", "forum": "cWczH8ontO", "replyto": "cWczH8ontO", "signatures": ["ICLR.cc/2026/Conference/Submission9310/Reviewer_rAra"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9310/Reviewer_rAra"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999785157, "cdate": 1761999785157, "tmdate": 1762920944355, "mdate": 1762920944355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}