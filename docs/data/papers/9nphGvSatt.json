{"id": "9nphGvSatt", "number": 4890, "cdate": 1757785027795, "mdate": 1759898006920, "content": {"title": "ELViS: Efficient Visual Similarity from Local Descriptors that Generalizes Across Domains", "abstract": "Large-scale instance-level training data is scarce, so models are typically trained on domain-specific datasets. Yet in real-world retrieval, they must handle diverse domains, making generalization to unseen data critical. We introduce ELViS, an image-to-image similarity model that generalizes effectively to unseen domains. Unlike conventional approaches, our model operates in similarity space rather than representation space, promoting cross-domain transfer. It leverages local descriptor correspondences, refines their similarities through an optimal transport step with data-dependent gains that suppress uninformative descriptors, and aggregates strong correspondences via a voting process into an image-level similarity. This design injects strong inductive biases, yielding a simple, efficient, and interpretable model. To assess generalization, we compile a benchmark of eight datasets spanning landmarks, artworks, products, and multi-domain collections, and evaluate ELViS as a re-ranking method. Our experiments show that ELViS outperforms competing methods by a large margin in out-of-domain scenarios and on average, while requiring only a fraction of their computational cost.", "tldr": "a new model and an extensive evaluatiion benchmark for domain generalization of instance-level image retrieval re-ranking using local descriptors", "keywords": ["instance-level image retrieval", "image re-ranking", "local similarity", "generalization", "interpretability"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2e3062411094e2a733008b870863412d725d06ad.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ELViS (Efficient Local Visual Similarity), a novel model for instance-level image retrieval. The core problem addressed is single-source domain generalization: training a model on one domain (e.g., landmarks) and having it perform well on retrieval tasks in unseen domains.\n\n**Method Overview:**  \n- Input: Local descriptors are extracted from two images using a foundation model (like DINOv2).  \n - Similarity Matrix Refinement: The local descriptor similarity matrix is refined using Optimal Transport with a key innovation: descriptor-dependent dustbin gains. This allows the model to learn to ignore uninformative descriptors (e.g., from the background).  \n- Vote Aggregation: For each descriptor, the strongest correspondence (similarity) is selected as a `\"vote.\" A small, learned function \\$f\\$ transforms these vote strengths, and they are summed to produce a final, global image similarity score.  \n- Training: The model uses a modified Binary Cross-Entropy loss with a second learned function g to reshape the penalty curve during training, which is discarded at inference.  \n\n\n**Main Results:**  \nThe authors demonstrate that ELViS achieves state-of-the-art performance on a comprehensive benchmark of 8 datasets, showing superior generalization to out-of-domain data while being significantly faster and more parameter-efficient than competing transformer-based methods (RRT, R²Former, AMES)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and clear to follow.  \n- SOTA Cross-Domain Generalization: The primary strength of ELViS is its ability to perform robustly on domains unseen during training. It consistently outperforms all competitors on out-of-domain (OOD) datasets.  \n- The method is efficient on both parameters count and latency aspects.  \n- Ablation studies on the contribution of each component is provided and well demonstrated."}, "weaknesses": {"value": "- Justification for \\$g\\$: The use of the learned function \\$g\\$ in the loss, which is discarded at inference, is an unconventional and somewhat non-standard technique. While it works well empirically, a more rigorous theoretical explanation for why this is necessary and why discarding it is valid would strengthen the method.  \n- Performance on In-Domain Data: In some in-domain (ID) settings, ELViS is outperformed by other methods. This suggests that while its bias towards generalization is powerful, it might come at the cost for optimal performance on the training domain itself.  \n- Novelty: The method primarily relies on existing components, which somewhat limits its degree of novelty."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zdDN2glxvR", "forum": "9nphGvSatt", "replyto": "9nphGvSatt", "signatures": ["ICLR.cc/2026/Conference/Submission4890/Reviewer_2TKs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4890/Reviewer_2TKs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761135437306, "cdate": 1761135437306, "tmdate": 1762917742235, "mdate": 1762917742235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduced a image-to-image similarity model that promotes cross-domain transfer, namely Efficient Local Visual Similarity (ELViS). In order to facilitate faster and more explainable image retrieve in cross-domain scenarios, ELViS leveraged optimal transportation to refine the local description similarity matrix S’, and then aggregate a learnable voting process to transfer the local similarity to the global similarity for the further image retrieval. \nTheir major technical contribution of their work lies in (1) the construction of refined local-description similarity matrix S’, which contains traditional similarity matrix S, data-dependent gains (learned by parametric method) that suppress uninformative descriptors (dustbins) and the learnable scalars that stands for transportation mass for dustbins. (2) the voting mechanism that transfer local descriptions to global description. (3) The authors built a benchmarking protocol that unified 8 existing datasets across various domains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The construction of refined local description similarity matrix is novel and intuitive. Especially the introduction of dustbin that avoids the hard comparison between different image instance. And the implementation of optimal transportation to refine the similarity matrix is intuitive and reasonable. Additionally, compared to the former strategy, this paper used the parametric method to empower the model with the ability to adjust the dustbin with self-supervision, making the model more interpretative and flexible. \n(2) They introduced 8 benchmarks that contains various kinds of domains, and they are the first work to conduct such an extensive evaluation of single-source domain generalization in instance-level retrieval. And the model performed well on those datasets, especially on out-of-domain retrieval scenarios.\n(3) Most of the figures and narrations in this paper (except for Introduction) is good and logical, stating their motivation and insights.  \n(4) The experiments and further analysis is abundant and showed the model’s effectiveness."}, "weaknesses": {"value": "(1) Compared to their technical contribution, their narration in the introduction is less satisfying and cannot specify their motivations. \na. The authors should detailed the reason why the focus on local descriptors is better than the global descriptors for the cross-domain image retrieval. Even though the author mentioned the intepretability and time cost, they can delve deeper into the explanation in representation space, making their statement and motivation less trivial and more solid.\nb. It would be better for the authors to add a figure for this comparison, making their motivation more intuitive.\nc. It would be better for the author to itemize and highlight their contribution in end of the introduction, making their contribution more clear for the readers.\n(2) The authors should simplify the Section.2 (Related Work), instead explicitly detail their motivation, contribution and analysis on previous researches in the introduction.\n(3) Even though model’s in-domain performances didin’t achieve state-of-the-art performances, it is acceptable."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4FxvwaRwqu", "forum": "9nphGvSatt", "replyto": "9nphGvSatt", "signatures": ["ICLR.cc/2026/Conference/Submission4890/Reviewer_UYEL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4890/Reviewer_UYEL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666084757, "cdate": 1761666084757, "tmdate": 1762917741829, "mdate": 1762917741829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ELViS, a re-ranking technique which is robust across domains. Elvis can be applied on top of common vision foundation models like DINOv2, DINOv3 and SigLIP2, and uses the transformer's local features by applying a lightweight post-processing to output a similarity score between two images, which is then used to re-rank a shortlist of retrieved candidates."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper outlines, demonstrates, and tackles a clear problem, which is that re-ranking methods trained on one domain underperform on others.\nElvis shows overall improvement when used on OOD data.\nThe paper is well written"}, "weaknesses": {"value": "1. Can Elvis work when images are of different resolutions? Given that f is an MLP it seems like Elvis would require images at a fixed resolution.\n\n2. I don't fully understand Figure 3. A better caption would be helpful\n\n3. Splitting results on ROxford and RParis would make results clearer and more comparable with other literature. Also which sets of ROxford and RParis are used (easy, medium, hard)?\n\n4. The dataset table should report also the sizes of train/val/test sets for the two datasets used for training\n\n5. A couple of images per dataset would help the reader to understand the domain gap between any two datasets\n\n6. I believe the retrieval is performed with the same model of which the local features are used? I don't see this explicitly stated in the paper\n\n7. Most importantly, comparing with image matching methods would be really helpful for the reader. Are the presented methods relevant, or should methods like SuperGlue be used for re-ranking in these domains?"}, "questions": {"value": "See the weaknesses stated above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M0GCGQQlo5", "forum": "9nphGvSatt", "replyto": "9nphGvSatt", "signatures": ["ICLR.cc/2026/Conference/Submission4890/Reviewer_dvx4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4890/Reviewer_dvx4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761699000051, "cdate": 1761699000051, "tmdate": 1762917741428, "mdate": 1762917741428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Instance-level image retrieval struggles with poor cross-domain generalization—existing models overfit to domain-specific data and fail on unseen domains, due to scarce cross-domain training data and reliance on representation space.\nSpecifically,\n1.Refine the local descriptor similarity matrix using entropy-regularized optimal transport (OT) with descriptor-dependent dustbin gains to filter uninformative patches\n2.Aggregate global similarity: select strongest local similarities per descriptor, weight them via a learnable function f, and sum—trained with modified BCE loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper shifts instance-level image retrieval from representation space to similarity space for stronger cross-domain robustness, refines optimal transport (OT) with descriptor-dependent dustbin gains , and creates the first unified benchmark for single-source cross-domain retrieval—uniting 8 datasets across 5 domains to standardize generalization evaluation."}, "weaknesses": {"value": "Chowdhury (2022) also employed optimal transport (OT) to address the instance-level image retrieval task. Therefore, I have concerns about the technical innovation of this paper.\n•\tOptimal Transport (OT) Parameters: The paper uses 10 iterations of the Sinkhorn-Knopp algorithm and a regularization term \\(\\lambda = 0.1\\), but it does not explain: i) Why 10 iterations (not 5 or 20)? ii) Why \\(\\lambda = 0.1\\)? Cuturi (2013) shows \\(\\lambda\\) directly impacts OT’s accuracy-efficiency tradeoff\n•\tExtreme Domains: The paper does not evaluate on domains with radical visual differences from natural images (e.g., infrared images, underwater photos, remote sensing imagery)—scenarios where cross-domain retrieval is highly valuable (e.g., satellite image matching for disaster response).\n•\tSmall-Sample Training: The paper uses large training sets (GLDv2 has 762K images, SOP has 60.5K), but many real-world domains have only 100–1000 labeled samples. It is unknown if ELViS’s small parameter count (96K) translates to good small-sample performance.\n\n \nCuturi, M. (2013). Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26.\nChowdhury, P. N., Bhunia, A. K., Gajjala, V. R., Sain, A., Xiang, T., & Song, Y. Z. (2022). Partially does it: Towards scene-level fg-sbir with partial input. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2395-2405)."}, "questions": {"value": "For the questions , please refer to the above summary of weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Br369ioQiY", "forum": "9nphGvSatt", "replyto": "9nphGvSatt", "signatures": ["ICLR.cc/2026/Conference/Submission4890/Reviewer_6e19"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4890/Reviewer_6e19"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898728297, "cdate": 1761898728297, "tmdate": 1762917740853, "mdate": 1762917740853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}