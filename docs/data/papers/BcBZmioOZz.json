{"id": "BcBZmioOZz", "number": 3702, "cdate": 1757498679919, "mdate": 1763649942300, "content": {"title": "InfoAgent: Advancing Autonomous Information‑Seeking Agents", "abstract": "Building Large Language Model agents that expand their capabilities by interacting with external tools represents a new frontier in AI research and applications. In this paper, we introduce InfoAgent, a deep research agent powered by an innovative data synthesis pipeline and orchestrated web search tools. To construct challenging, hard-to-find queries, we build entity trees and apply sub-tree sampling with entity fuzzification to systematically increase question difficulty. Unlike prior work that relies heavily on commercial search tools, we develop a dedicated self-hosted search  infrastructure, enhancing transparency of agent environments and facilitating further advancement of agent capacity. We evaluate the effectiveness of our data pipeline by measuring the average number of tool calls required to correctly answer a question, and also show that our agent yields better performance when equipped with our tools. Our InfoAgent is post-trained from Qwen3-14B using a two-stage recipe: cold-start supervised finetuning to instill long-horizon search behaviors, followed by reinforcement learning which significantly improves reasoning-driven tool use. With our methods, InfoAgent achieves 15.3% accuracy on BrowseComp, 29.2% on BrowseComp-ZH, and 40.4% on Xbench-DS, outperforming prior open-source deep research agents such as WebSailor-72B and DeepDive-32B.", "tldr": "This paper proposes a novel data synthesis pipeline for Deep Research Agents and a complete implementation of high-quality self-hosted search tools, achieving SOTA on deep research benchmarks.", "keywords": ["Agent", "RL", "Deep Research", "BrowseComp"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cf17d9565a3973f519e7ed2e7ffe88de11308955.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces InfoAgent, a 14B-parameter deep research agent trained via supervised fine-tuning (SFT) and reinforcement learning (RL) to perform multi-hop web search and information retrieval. The authors make three main contributions: (1) a data synthesis pipeline that constructs challenging multi-entity questions through entity tree construction, fuzzification, and sub-tree sampling; (2) a custom self-hosted search infrastructure with enhanced snippet generation to replace commercial APIs; and (3) a two-stage training recipe (SFT + GRPO) applied to Qwen3-14B. InfoAgent achieves 15.3% accuracy on BrowseComp, 29.2% on BrowseComp-ZH, and 40.4% on Xbench-DS, establishing state-of-the-art performance among open-source models under 15B parameters."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Originality in data synthesis**: The entity tree construction with three-stage fuzzification (entity names, dates/numbers, semantic rephrasing) is a creative approach to systematically increase question difficulty.\n2. **Quality of engineering**: The custom search infrastructure with multi-stage retrieval (BM25 -> embedding -> reranker -> LLM snippet generation) demonstrates good systems engineering. The Redis caching and performance optimizations show attention to practical RL training requirements.\n3. **Thorough empirical analysis**: The ablation studies provide valuable insights:\n   - SFT cold-start is essential (Figure 5 shows non-SFT model fails to learn)\n   - Tool quality significantly impacts performance (Table 2)\n   - Trajectory length matters (Table 3)\n   - Process rewards don't help (Appendix D)"}, "weaknesses": {"value": "1. **Missing critical baselines**: The paper cites ChatGPT Deep Research (OpenAI, 2025a), Gemini Deep Research (Google, 2025), Perplexity Deep Research (Perplexity, 2025), and Grok Deep Search (xAI, 2025a) as motivating examples but does not evaluate against any of them. This is a critical omission that makes it impossible to assess the true contribution relative to deployed deep research systems.\n2. **Limited performance and overstated claims**: The title claims to \"advance\" autonomous information-seeking agents, but InfoAgent achieves only 30% of o3's performance and lags significantly behind DeepSeek-V3.1 (71.2% vs 40.4% on Xbench-DS) and GLM-4.5 (68% vs 40.4%). The contribution is better characterized as advancing *small* open-source agents, not the field overall.\n3. **Limited novelty**: The paper primarily combines existing techniques (ReAct framework, GRPO, entity-based QA generation, multi-stage retrieval). While the integration is competent, there is insufficient algorithmic innovation."}, "questions": {"value": "1. **GLM-4.5 and DeepSeek-V3.1 setup**: Were these large open-source models evaluated with search capabilities? If so, what search infrastructure did they use? This affects interpretation of whether InfoAgent's contribution is the model training or the search tool."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cQufGCmokW", "forum": "BcBZmioOZz", "replyto": "BcBZmioOZz", "signatures": ["ICLR.cc/2026/Conference/Submission3702/Reviewer_7jmx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3702/Reviewer_7jmx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664683938, "cdate": 1761664683938, "tmdate": 1762916933414, "mdate": 1762916933414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to All Reviewers for Common Questions"}, "comment": {"value": "We sincerely thank all reviewers for their thorough reviews and insightful comments on our paper. We are delighted to see that the reviewers have identified several strengths in our paper, including:\n\n- The paper proposes a novel and creative data synthesis method that systematically scales task difficulty while maintaining solvability. (D7o1, HTWm, cXeW, 7jmx)\n- The proposed search tool delivers strong performance and can be a useful resource to the community if released. (D7o1, HTWm, cXeW, 7jmx)\n- The paper gives many valuable insights into the training recipe for developing strong search agents. (D7o1, HTWm, 7jmx)\n\nWe appreciate all reviewers' feedback and recognition of InfoAgent's contributions to the deep search agent community. We have updated the manuscript according to the feedback. Here we respond to the common questions raised by the reviewers. \n\n# Release of our dataset and tool (Response to Reviewer D7o1, cXeW)\n\nWe promise to open-source our training dataset and the search tool once this paper is accepted.\n\n# Our search tool is beneficial for model training, improving its general search ability. (Response to Reviewer HTWm, cXeW)\n\nWe thank the reviewers for raising this insightful point about evaluating training with our tool but inferring with the standard Wiki Retriever. This ablation would help clarify whether the model’s learned reasoning capabilities generalize to a weaker retrieval tool.\nHowever, the Wiki Retriever accesses only a fixed, offline Wikipedia dump, which prevents its use on our main open-web benchmarks (e.g., BrowseComp, X-Bench). For this reason, we initially excluded the cross-tool ablation to ensure fair comparison across all methods. We appreciate the reviewers’ observation that omitting this result may obscure the impact of our tool for training, and we have now included this analysis.\n\nSpecifically, we conduct the requested ablation on several widely used wiki-based benchmarks, where the Wiki Retriever can be properly applied, including (1) General QA: NQ [1], TriviaQA [2], and PopQA [3]; and (2) Multi-hop QA: HotpotQA [4], Musique [5], and Bamboogle [6].\n\n|  Training Tool | Inference Tool | Bamboogle |   PopQA  |    NQ    | TriviaQA | HotpotQA |  Musique |\n|:--------------:|:--------------:|:---------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n| Wiki Retriever | Wiki Retriever |    66.4   |   55.9   |   64.8   |   82.8   |   67.9   |   31.3   |\n|      Ours      | Wiki Retriever |  **70.4** | **56.6** | **66.1** | **83.6** | **68.9** | **33.1** |\n| Wiki Retriever |      Ours      |    79.2   |   58.0   |   65.3   |   89.0   |   70.1   |   31.7   |\n|      Ours      |      Ours      |  **86.4** | **59.7** | **67.2** | **90.3** | **72.7** | **34.9** |\n\n\nAs shown in the table, when inference is performed with the same Wiki Retriever, the model trained using our tool consistently outperforms the model trained directly with the Wiki Retriever across all these benchmarks. This suggests that our tool provides richer training dynamics, leading to more effective reasoning strategies.\n\nFor completeness, we also evaluated both tools on the original open-web benchmarks included in the paper. Because the Wiki Retriever cannot access open-web information, the corresponding results are very low and included mainly to illustrate the limitation of closed retrieval systems in the open-web setting.\n\n|  Training Tool | Inference Tool | BrowseComp |   BrowseComp-ZH  |    X-Bench    | \n|:--------------:|:--------------:|:---------:|:--------:|:--------:|\n| Wiki Retriever | Wiki Retriever |    1.0   |   4.5   |   8.0   |\n|      Ours      | Wiki Retriever |  1.1 | 4.3 | 9.0 |\n\n\nWe appreciate the reviewers’ helpful feedback and believe this additional analysis strengthens the empirical understanding of our framework’s contribution.\n\nReferences:\n\n[1] [Natural Questions: A Benchmark for Question Answering Research - ACL Anthology](https://aclanthology.org/Q19-1026/)\n\n[2] [TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension - ACL Anthology](https://aclanthology.org/P17-1147/)\n\n[3] [When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories - ACL Anthology](https://aclanthology.org/2023.acl-long.546/)\n\n[4] [HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering - ACL Anthology](https://aclanthology.org/D18-1259/)\n\n[5] [♫ MuSiQue: Multihop Questions via Single-hop Question Composition - ACL Anthology](https://aclanthology.org/2022.tacl-1.31/)\n\n[6] [Measuring and Narrowing the Compositionality Gap in Language Models - ACL Anthology](https://aclanthology.org/2023.findings-emnlp.378/)"}}, "id": "zX8gFFKbT8", "forum": "BcBZmioOZz", "replyto": "BcBZmioOZz", "signatures": ["ICLR.cc/2026/Conference/Submission3702/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3702/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3702/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763641053710, "cdate": 1763641053710, "tmdate": 1763642206519, "mdate": 1763642206519, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces InfoAgent, a deep research agent built upon the Qwen3-14B model. It proposes an innovative data synthesis pipeline that constructs entity trees, uses sub-tree sampling, and applies \"fact fuzzification\" to generate challenging information-seeking problems with long-range dependencies. Second, the authors developed and deployed a self-hosted search and browsing infrastructure to replace opaque commercial APIs, enhancing research transparency and reproducibility. InfoAgent is trained using a two-stage \"SFT cold-start + RL tuning\" paradigm. \n\nExperimental results show that InfoAgent (14B) achieves SOTA performance in its parameter class across multiple benchmarks, outperforming larger models like WebSailor-72B and DeepDive-32B."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies the \"shallow search\" problem in current benchmarks. Its proposed data generation method, based on entity trees, sub-tree sampling, and systematic \"fact fuzzification,\" is rational and effective.\n\n- The search tool's design, which combines BM25 filtering, embedding and reranker models, and uses an LLM (GPT-4o-mini) to generate snippets, is in itself a good engineering practice for building a RAG system."}, "weaknesses": {"value": "- The paper's core training framework (SFT + RL) is a standard paradigm in the agent domain (e.g., Search-R1, WebSailor). While the data synthesis component is well-executed, the idea of \"generating QA pairs based on knowledge graphs/entity trees\" is not entirely novel and is similar to the construction philosophy of benchmarks like WebWalkerQA.\n\n- Critical Dependency on the Custom Search Tool: To what extent is InfoAgent's high performance attributable to,  the trained model itself, or the highly-optimized, custom search tool that uses GPT-4o-mini for summarization? The ablation study in Table 2 (using Wiki Retriever) shows a catastrophic performance drop (e.g., 10.0 → 1.0 on BrowseComp), which strongly suggests the model's capabilities are deeply entangled with this powerful tool, supporting the latter concern.\n\n- The authors admit in the \"Reproducibility Statement\" that reproducing the custom search tool \"presents greater challenges\" and seems there are no plans to open-source it. Given the tool's decisive impact on performance, this severely hinders the community from building upon this work or making fair comparisons.\n\n- In Table 1, the performance gap between InfoAgent and its SFT-only version is enormous (e.g., a jump from 4.7 to 15.3 on BrowseComp). Such a massive improvement from the RL stage alone is unusual and requires a more detailed explanation from the authors as to what specific capabilities the RL stage is imparting."}, "questions": {"value": "- Can the authors provide a key ablation: *training* with the \"Ours\" tool but *inferring* with the standard \"Wiki Retriever\"? This would help assess whether the model's learned reasoning capabilities can generalize to a weaker tool.\n\n- The performance gap between the SFT-only model (4.7 on BrowseComp) and the SFT+RL model (15.3) is very large. Can the authors elaborate on why the RL stage brings such a dramatic improvement? Is RL teaching the model entirely new search strategies?\n\n- The paper attempts to use a process-based reward in Appendix D, but the experimental results (Table 4) show it provided no performance benefit and even led to a performance decrease on BrowseComp. How do the authors conclude the entity-recall-based process reward (Appendix D) failed? Does this suggest that the process reward signal itself was flawed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6E8epon3uP", "forum": "BcBZmioOZz", "replyto": "BcBZmioOZz", "signatures": ["ICLR.cc/2026/Conference/Submission3702/Reviewer_cXeW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3702/Reviewer_cXeW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836013534, "cdate": 1761836013534, "tmdate": 1762916933097, "mdate": 1762916933097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents InfoAgent, an information seeking agent built on top of strong search tools and optimized through training on synthetic data as well as reinforcement learning. InfoAgent includes a carefully designed search tools (search and browse) which performs multiple retrieval, re-ranking, and summarization steps for improved search quality. The data synthesis pipeline leverages tree structures to compose tasks of different difficulties. Experiments on several deep research benchmarks show that InfoAgent achieves the best performance among open-source methods. Further analysis delivers insights into the importance of the high quality search tool and several design choices in training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The overall writing of the paper is clear.\n2. The proposed search tool combines several existing techniques and delivers strong performance compared to the baseline (Wiki Retriever). The tool, once released, can be a useful resource to the community.\n3. Based on the experiment results, the data synthesis and the training pipeline seem to work well. Also it is interesting that the benefit of training on English data can generalize to Chinese datasets.\n4. The authors present several analysis and ablation study to present insights into developing stronger deep research agents, including the importance of tool quality, the critical role of SFT, and impact of trajectory length."}, "weaknesses": {"value": "1. The paper highlights the need for \"high-concurrency web search tool\" for web training, however, there's no discussion on the proposed tool's relevant attributes, such as throughput and latency. \n2. The data synthesis pipeline is rather complex. Since it is based on entity tree structures, I would like to see some discussion on the distribution of sub-tree sizes and how the synthesized tasks look like. More ablation studies on the individual steps will also help justify the design choices.\n3. Based on Table 2, it seems the major performance improvement comes from the proposed search tools, which includes many components and involve the use of commercial search APIs as well as LLM (GPT-4o-mini) for summarization. This potentially results in an unfair comparison to other open source methods. I would like to see more ablations and discussion on this to better determine the contribution of tool design."}, "questions": {"value": "1. Missing a reference to Mind2Web2 [1] in related work, which is a recently published deep research benchmark.\n2. line 118-120, could explain more on how RAG treats the retrieved passages as \"latent variables\" rather than input to the generator?\n3. I am confused about the role of constraint set K(v). How does it prevent shallow pattern matching?\n4. It is nice to see a discussion on tool call number in different datasets. Do you have further insights in the resulted qualitative difference between your data and existing ones? e.g. does your pipelines synthesize more balanced and comprehensive training trajectories?\n5. In Section 4.3, quality of search tool, how about training on your dataset and test with Wiki Retriever? I am trying to understand whether the training pipeline improves the tool-use capability or the success of InfoAgent is more dependent on the high-quality tools."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jg91XOL8Lh", "forum": "BcBZmioOZz", "replyto": "BcBZmioOZz", "signatures": ["ICLR.cc/2026/Conference/Submission3702/Reviewer_HTWm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3702/Reviewer_HTWm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975876056, "cdate": 1761975876056, "tmdate": 1762916932825, "mdate": 1762916932825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a deep research agent, InfoAgent. This work approaches the challenges of building a deep research agent through the following three aspects:\n(1) a data synthesis pipeline to construct entity trees from Wikipedia and perform sub-tree sampling with fuzzification, which will be further leveraged to generate complex, multi-entity reasoning questions for model training. \n(2) self-implemented search tools without reliance on commercial search APIs for reproducibility. The tools include BM25 + embedding + reranker + LLM snippet pipeline. \n(3) Two-stage training with both SFT and RL. \nEmpirically, InfoAgent achieves 15.3 % on BrowseComp, 29.2 % on BrowseComp-ZH, and 40.4 % on Xbench-DS, showing SOTA results among models with the same size and outperforming models at larger sizes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The entity-tree construction with multi-level fuzzification is a clever way to increase question complexity and encourage genuine multi-hop reasoning. Compared to earlier heuristic generation methods, this design systematically scales task difficulty while maintaining solvability.\n(2) The self-implemented search/browse setup (BM25 + embedding + reranker + LLM snippet) is a clear step forward in reproducibility.\n(3) The experiments with SFT and GRPO are helpful with detailed ablations. And the shown results are strong across multiple benchmarks."}, "weaknesses": {"value": "(1) All experiments and synthesized data are from Wikipedia. The current setup doesn’t test how well the pipeline or the trained model generalizes to open-web or non-encyclopedic content (news, technical sites, etc.).\n(2) The paper describes the tool infrastructure well, but rebuilding it from scratch would require major engineering. Even partial open-sourcing or more pseudo-code details would help the community reproduce results."}, "questions": {"value": "(1) How do you ensure the fuzzified questions remain solvable with a single correct answer? Did you measure ambiguity or validate questions with a strong verifier (e.g., o3 or human checks)?\n(2) What happens when the self-hosted search/browse system fails—e.g., missing content or timeouts? Is there a fallback or retry mechanism?\n(3) Are there plans to release the data synthesis code or a lightweight version of the custom search tool? That would significantly improve reproducibility and downstream benchmarking."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zhURzocmaZ", "forum": "BcBZmioOZz", "replyto": "BcBZmioOZz", "signatures": ["ICLR.cc/2026/Conference/Submission3702/Reviewer_D7o1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3702/Reviewer_D7o1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762237778933, "cdate": 1762237778933, "tmdate": 1762916932557, "mdate": 1762916932557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}