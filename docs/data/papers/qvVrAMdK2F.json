{"id": "qvVrAMdK2F", "number": 20351, "cdate": 1758305007063, "mdate": 1759896982215, "content": {"title": "On the Predictive Power of Representation Dispersion in Language Models", "abstract": "We show that a language model’s ability to predict text is tightly linked to the breadth of its embedding space: models that spread their contextual representations more widely tend to achieve lower perplexity. Concretely, we find that representation dispersion—the average pairwise cosine distance among hidden vectors—strongly and negatively correlates with perplexity across diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia, news, scientific abstracts). Beyond illustrating this link, we show how dispersion can be leveraged for a range of practical tasks—without requiring labeled data. First, measuring dispersion on unlabeled text allows us to predict downstream accuracy in new domains, offering a data‐efficient tool for model selection. Next, we find that identifying layers with higher dispersion pinpoints the best representations for retrieval‐based methods such as kNN‐LM, bypassing exhaustive layer‐by‐layer searches. Finally, we integrate a simple “push‐away” objective into training, which increases dispersion in both single‐domain and cross‐domain scenarios and directly improves perplexity in each.", "tldr": "", "keywords": ["Embedding geometry", "Unsupervised evaluation", "Mechanistic interpretability", "Large Language Models", "Label-free metrics"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4cf37480c560ed3d754252faa38e3ea8c8811521.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a simple geometric statistic of language model representations, called \"dispersion,\" defined as the mean pairwise cosine distance between hidden states (or between token embeddings). Higher dispersion corresponds to representations that are more spread out in embedding space. The authors argue that dispersion tracks model quality: it is strongly and consistently negatively correlated with perplexity, and can therefore be used for practical purposes such as predicting downstream accuracy in new domains and ranking or selecting models. They further propose a \"push-away\" auxiliary loss that explicitly increases dispersion during training, and report that this improves perplexity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper takes long-standing intuitions about representation geometry and turns them into actionable tools. The proposed dispersion metric is inexpensive to compute and is presented as useful for several practical tasks (checkpoint selection, layer selection, etc.). The authors also provide fairly broad empirical evidence across multiple model families and domains to support these claims."}, "weaknesses": {"value": "## Weakness 1. \"Predicting downstream performance without labeled data\" is oversold\nSection 3.1 claims to predict downstream performance without labels. What the method actually provides is a relative hardness ranking: low-dispersion examples are more likely to be wrong than high-dispersion examples. Given a new dataset with zero labels, computing dispersion yields only that ordering, not an absolute expected accuracy. The paper does not discuss how dispersion on a new task would be mapped to accuracy, and it does not test whether the dispersion–accuracy monotonicity continues to hold under distribution shift. Moreover, the monotone relationship is clearly tied to a specific model; for example, in Figure 8, if the computed mean pairwise distance is 0.08, it suggests a high correct rate for the 1B model but a lower correct rate for the 3B model.\n\n## Weakness 2. The usefulness for model selection is overstated. \n\nSection 3.2 only demonstrates results within very specific model families. The paper never shows that the same metric can compare across unrelated families (e.g. Qwen vs Llama), nor does it explicitly warn against that use. Even within one family, monotonicity with skill is not clean across scales: Distill-Qwen-14B scores higher on MATH than Distill-Qwen-1.5B, yet it has lower digit–digit and digit–non-math dispersion. Therefore, the claimed usefulness for model selection breaks when you move across parameter sizes. This suggests dispersion is mainly a heuristic for ranking sibling checkpoints of similar size, not a universal capability score. Without clearly stating these limitations, the paper may lead readers to overgeneralize.\n\nEven within one family and one parameter size, the usefulness remains in doubt. For example, Qwen2.5-Math-7B has D–D equal to 0.27 with a MATH score of 55.4%, while Distill-Qwen-7B has D–D 0.28 with a MATH score of 92.8%. It is unclear whether 0.28 is meaningfully larger than 0.27, because the paper does not provide standard errors for these summaries; and if 0.27 and 0.28 are effectively similar, then the large change in performance actually suggests that the D–D gap is not useful for predicting performance, which contradicts the paper's claim. \n\n## Weakness 3. Unclear benefits over existing alternatives\n\nThe paper claims that dispersion can rank models for a domain and identify hard inputs without labels, but perplexity on unlabeled domain text already gives a label-free measure of adaptation. The paper does not compare dispersion against that baseline, so we cannot tell whether dispersion provides genuinely new information or is just a rephrasing of model confidence."}, "questions": {"value": "Q1: Can perplexity on unlabeled in-domain text replace dispersion in Sections 3.1 and 3.2? If perplexity already produces the same ranking or predictions, then dispersion does not add unique value beyond possible computational savings. If dispersion performs better, please quantify that improvement.\n\nQ2: In Section 3.2, can the proposed model-ranking method generalize across different model families (e.g., Qwen vs Llama), or is it only intended for ranking closely related checkpoints within one family?\n\nQ3: Can you provide standard errors for Tables 3 and 4 to clarify whether the reported dispersion gaps (e.g., D–D = 0.27 vs 0.28) meaningfully differ?\n\nQ4: For Section 3.1, can you provide evidence that dispersion can yield a calibrated accuracy estimate on truly unseen data? In particular, how should a user interpret a computed dispersion value to make an actionable inference about expected accuracy?\n\nQ5: For Section 3.1, have you tested whether the dispersion–accuracy relationship you observe on ARC/MMLU for one model persists on a different dataset drawn from a different domain (i.e., under distribution shift)? If so, please report it. If not, please clarify that the current result is model- and dataset-specific, and should not yet be treated as a universal performance predictor.\n\nQ6: In Section 3.2 (training with an auxiliary dispersion objective), can you clearly define all notation? In particular, $d$ appears with different meanings, and $\\mathcal{L}_{\\mathrm{CE}}$ and $\\mathcal{L}_{\\mathrm{aux}}$ are referenced without a full specification. Please also explain how this auxiliary objective relates to prior work on contrastive/repulsive representation learning, so that readers do not interpret the loss as entirely new."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KV0YtF8XNt", "forum": "qvVrAMdK2F", "replyto": "qvVrAMdK2F", "signatures": ["ICLR.cc/2026/Conference/Submission20351/Reviewer_4BDa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20351/Reviewer_4BDa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761732684590, "cdate": 1761732684590, "tmdate": 1762933809611, "mdate": 1762933809611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a comprehensive empirical study establishing a strong negative correlation between representation dispersion and perplexity in language models. The authors demonstrate this relationship across multiple model families and domains . Beyond this core correlation, the paper explores several practical applications of this insight: (1) predicting downstream accuracy on unlabeled data, (2) a \"dispersion gap\" metric for label-free model selection, (3) unsupervised layer selection for kNN-LM, and (4) an auxiliary training loss that increases dispersion and improves perplexity. The work is well-supported by extensive experiments and offers both conceptual insights and practical tools for the community.\n\nThis is an excellent paper that makes a substantial contribution to our understanding of representation learning in language models. It identifies a robust and generalizable phenomenon and, most importantly, derives a set of practical, effective, and efficient applications from this insight. The work is empirically sound, clearly presented, and has immediate utility for both researchers and practitioners. The minor weaknesses do not detract from the overall significance and quality of the work. I recommend acceptance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The core finding—higher representation dispersion correlates with lower perplexity—is simple, intuitive, and empirically robust. The paper does an excellent job of moving beyond mere observation to demonstrate a range of practical applications, making the finding directly useful for practitioners.\n\nThe empirical analysis is thorough. The authors validate their claims across a wide range of models and datasets, which significantly strengthens the generalizability of their conclusions.\n\nThe proposed applications are compelling and well-executed, making strong and practical contribution. The \"dispersion gap\" for model selection (Section 3.2) is a particularly elegant and efficient method that requires no forward passes, only the model's output embedding matrix. The layer selection for kNN-LM (Section 3.3) provides a simple, unsupervised heuristic that could save significant computational resources. The auxiliary training objective (Section 3.4) successfully translates the observational finding into an actionable method for improving model performance, especially in the cross-domain setting.\n\nThe paper goes beyond global averages to perform a fine-grained analysis of dispersion within semantic clusters (Section 2.3). This is a crucial experiment that convincingly shows the effect is not merely about pushing dissimilar contexts apart but also about separating semantically similar ones, reinforcing the core hypothesis.\n\nThe paper is well-written, clearly structured, and the figures effectively illustrate the key trends. The appendices provide substantial additional detail and results.\n\nThe paper is highly reproducible. The methodology for computing dispersion is clearly defined. The appendices provide extensive details on datasets, models, hyperparameters (for fine-tuning and training with the auxiliary loss), and the uniform binning algorithm. The use of standard, publicly available models and datasets further aids reproducibility."}, "weaknesses": {"value": "The conclusion slightly lacks for causation. The core evidence is primarily correlational. The auxiliary loss experiment in Section 3.4 is the strongest argument for causality, but it is limited to the GPT-2 architecture. A more nuanced discussion of this distinction would strengthen the paper. Is dispersion a fundamental driver of performance, or is it a side-effect of a model learning better, more discriminative features?\n\nThe experimented domain/language coverage is slightly limited. As acknowledged in the limitations (Appendix C), the experiments are primarily on English text from standard benchmarks. While the inclusion of code is a positive step, the claim of \"diverse domains\" could be more strongly supported by including data from truly different distributions (e.g., low-resource languages, highly technical manuals, social media). The multilingual MMLU results in the appendix are a good start but are still based on translated versions of a known benchmark.\n\nThe discussion of the most related work (Viswanathan et al., 2025) is not sufficient. The authors state their work is more \"actionable,\" but a more direct comparison of the findings (e.g., do they observe similar dispersion patterns?) would better situate this contribution within the existing literature."}, "questions": {"value": "(1) Have you considered or attempted any ablation studies that directly reduce dispersion (e.g., by adding a loss that encourages representations to cluster) to see if perplexity increases? This would provide even stronger evidence for a causal link.\n\n(2) In Section 2.2, you note the negative correlation strengthens in deeper layers. Do you have a hypothesis for why this is the case? Is it simply that deeper layers are more task-specific and directly linked to the final prediction?\n\n(3) The auxiliary loss requires computing pairwise distances within a batch, which is O(B²). Did you find this to be a significant computational bottleneck during training, and were any approximations (e.g., sampling pairs) considered for larger batch sizes?\n\n(4) A Minor Issue. On Page 8, Table 1: The standard deviations for GPT2-Large (FFN, N=10) are `0.68±0.06`, which is very large compared with other values in the same column. Is this expected? A brief comment might be useful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QM0FMymwqa", "forum": "qvVrAMdK2F", "replyto": "qvVrAMdK2F", "signatures": ["ICLR.cc/2026/Conference/Submission20351/Reviewer_uSU9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20351/Reviewer_uSU9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827304337, "cdate": 1761827304337, "tmdate": 1762933809327, "mdate": 1762933809327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that one can tie the predictive quality (i.e. through next token prediction) of a language model by a geometric quantity of the language model. The define the representation dispersion as the cosine similarity between vector embeddings produced by the language model between different inputs in the dataset. They argue that batches with high dispersion tend to be ones the model is more accurate on, as measured by the perplexity on the next token prediction. They propose several use cases, such using this for layer-selection, model-selection, or as a training objective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality: This paper presents a novel metric which states several interesting correlations. They then showcase some natural and interesting use cases for this.\n2. Quality: The empirical study in this paper is extensive, and convincing. \n3. Clarity: Everything is presented clearly, except for in section 3.2 (as mentioned below)\n4. Significance: There are several actionable applications presented that showcase how this framework could be helpful."}, "weaknesses": {"value": "1. I think it is not obvious what to take from Figure 9. It seems like the relationship between the gap and the performance is not as obvious as the results section made it out to be. I am also not convinced that the Spearmans correlation is useful here on such a low number of data points."}, "questions": {"value": "1. It might be of interest to compare the layer-wise patters you observe here to those that have been observed in prior work, such as \"Layer by Layer ...\" by Skean et al. They also propose a metric called \"dataset entropy\" which seems in some sense similar to what is measured here."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YB2ggiZudl", "forum": "qvVrAMdK2F", "replyto": "qvVrAMdK2F", "signatures": ["ICLR.cc/2026/Conference/Submission20351/Reviewer_CstH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20351/Reviewer_CstH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949875603, "cdate": 1761949875603, "tmdate": 1762933808906, "mdate": 1762933808906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies a simple geometric measure on representation dispersion (the average pairwise cosine distance among hidden vectors and shows it negatively correlates with language-model perplexity across model families and text domains. The authors report strong negative correlations between dispersion and perplexity at the sequence level and also for last-token perplexity across context lengths. They further observe the correlation strengthens in deeper layers and is amplified by (full) fine-tuning and that within-cluster as well as between-cluster distances grow during training. Finally, the paper shows how dispersion can be used for many practical tasks, such as predicting downstream performance, identifying best layers for retrieval, and introducing dispersion objective during pretraining."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Dispersion is very simple to compute and model-agnostic. Consistent trends are observed across many families and datasets. \n2. The paper demonstrates that dispersion increases even among highly similar contexts.\n3. More importantly, the observations can be used for many downstream tasks. Adding domain dispersion loss for pretraining seems a very simple technique to improve model performance."}, "weaknesses": {"value": "1. My main concerns is that whether the correlation between dispersion and lower perplexity is due to memorization or generalization. Since LLMs are pre-trained on so many text in the web, the query data used in the experiments are seen by LLMs. Therefore, the resulting of dispersion is not that surprising as model tend to produce better representation on the data it has been seen. Could you provide a clear train / val split set up for GPT-2 to see why the dispersion can also happen for hold-out samples from the same domain. This would be the results more solid.\n2. Also, how would one come up with domain tag for dispersion loss proposed for pretraining LLM? Can we apply a domain classifier on web-scale pretraining data to label a text into a domain? Then apply dispersion loss on that. This would make the method useful actual LLM pretraining rather than a contrived case.\n3. Another question is whether this idea generally is applicable to vision encoder or multimodal applications? In fact, vision encoders are often trained with positive-negative samples regularization and therefore naturally encourages dispersion across pre-training data."}, "questions": {"value": "1. The average cosine distance does not indicate the spread-out or diversity of the embedding. Can you also try out Vendi Score [1] to see if the observations in the paper still holds? \n\nReference:\n1. Friedman & Dieng. The Vendi Score: A Diversity Evaluation Metric for Machine Learning. TMLR 23'."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PP3yEMb4MQ", "forum": "qvVrAMdK2F", "replyto": "qvVrAMdK2F", "signatures": ["ICLR.cc/2026/Conference/Submission20351/Reviewer_3MuU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20351/Reviewer_3MuU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971598469, "cdate": 1761971598469, "tmdate": 1762933808618, "mdate": 1762933808618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}