{"id": "L7Z5wBMPrW", "number": 6831, "cdate": 1757997282830, "mdate": 1763736350506, "content": {"title": "COSA: Context-aware Output-Space Adapter for Test-Time Adaptation in Time Series Forecasting", "abstract": "Deployed time-series forecasters suffer performance degradation under non-stationarity and distribution shifts.\nTest-time adaptation (TTA) for time-series forecasting differs from vision TTA because ground truth becomes observable shortly after prediction. \nExisting time-series TTA methods typically employ dual input/output adapters that indirectly modify data distributions, making their effect on the frozen model difficult to analyze. \nWe introduce the Context-aware Output-Space Adapter (COSA), a minimal, plug-and-play adapter that directly corrects predictions of a frozen base model.\nCOSA performs residual correction modulated by gating, utilizing the original prediction and a lightweight context vector that summarizes statistics from recently observed ground truth.\nAt test time, only the adapter parameters (linear layer and gating) are updated under a leakage-free protocol, using observed ground truth with an adaptive learning rate schedule for faster adaptation.\nAcross diverse scenarios, COSA demonstrates substantial performance gains versus baselines without TTA (13.91$\\sim$17.03\\%) and SOTA TTA methods (10.48$\\sim$13.05\\%), with particularly large improvements at long horizons, while adding a reasonable level of parameters and negligible computational overhead.\nThe simplicity of COSA makes it architecture-agnostic and deployment-friendly.\nSource code: https://anonymous.4open.science/r/linear-adapter-A720", "tldr": "", "keywords": ["Test-Time Adaptation", "Time-Series Forecasting", "Simple Adapter"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c6994691d7f63eef8caa4e6909c7958b36520a91.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces COSA (Context aware Output-Space Adapter), an adapter-based model that adjust predictions of a frozen base model. COSA achieves this via by learning a gated linear transformation on the original prediction and a context vector of summary statistics of the input. The model was tested on standard time series forecasting benchmarks and showed better performance with faster inference time. In addition, authors showed the utility of different components via an ablation study."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Although the idea is simple and one may argue that it is not novel per se, I think the empirical results show strong adequate performance gain in a much shorter inference time compared to the other baselines. \n\n- I think authors have done a good job in finding relevant (albeit recent) works.\n\n- Authors have reported hyper-parameters and also provided code. \n\nThe current results support the claims in the paper and look convincing to me and overall I am in favour of acceptance."}, "weaknesses": {"value": "- The input of the gating mechanism and its transformation by parameters g is a bit unclear to me in the formulation and figures. I think authors should elaborate this part similar to other components. \n\n- Authors conducted experiments on the standard time series benchmarks. I think the message of the paper will be stronger if also test the method on the Monash benchmark [1]. Just to be clear, I do not expect authors to repeat all their experiments on this benchmark.  \n\n- The benefit of the proposed method is only visible in long-term forecasting which one may argue it is expected as in the current setup (including baselines) there is a delay period to observe some ground truth. I think the paper will be stronger if authors highlight the applicability domain of the work i.e., what problems COSA-F (-P) is the most suitable for? \n\n- Standard error or confidence interval for numerical results and error bars for the bar plots are not reported. Also, it would be nice to have average+se to summarize each method/dataset.  \n\n- The baseline normalization is not clearly defined in Section 4.2.2, overall this section seems a bit disconnected to the rest of the paper. \n\n- Looking at the reported results in TAFAS and PETSA paper, there are significant differences with those reported in this paper (Exchange, ETTm2, ETTh2 on iTransformer).  \n\n[1] Godahewa et al. 20221 https://arxiv.org/abs/2105.06643"}, "questions": {"value": "Can you please comment on the performance of COSA-F (-P) with only a single update (S==1) either verbally and/or empirically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JcaZKkyjtn", "forum": "L7Z5wBMPrW", "replyto": "L7Z5wBMPrW", "signatures": ["ICLR.cc/2026/Conference/Submission6831/Reviewer_jjeA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6831/Reviewer_jjeA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761609867113, "cdate": 1761609867113, "tmdate": 1762919094599, "mdate": 1762919094599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper, titled “COSA: Context-Aware Output-Space Adapter for Test-Time Adaptation in Time Series Forecasting”, addresses the problem of distribution shifts and non-stationarity that cause performance degradation in deployed time-series forecasting models. The main objective is to develop a lightweight, architecture-agnostic test-time adaptation (TTA) mechanism that can improve model performance post-deployment without retraining the base model.\n\nThe proposed method, COSA, is a context-aware output-space adapter that performs direct residual correction on the model’s predictions. It introduces a linear correction layer modulated by a learnable gating mechanism and informed by a context vector summarizing recent ground-truth statistics. During deployment, the frozen base model’s predictions are corrected in real-time, and only the adapter parameters (weights, bias, and gate) are updated using an adaptive learning rate schedule under a leakage-free protocol."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles an important and timely problem—test-time adaptation for time-series forecasting—which has received comparatively less attention than its vision counterparts.\n- The proposed output-space-only adapter (COSA) is a conceptually clean and lightweight solution, addressing the design complexity and interpretability challenges of prior dual-adapter approaches.\n- The idea of context-aware residual correction, where recent ground-truth statistics inform a gating-based correction, is intuitive. It provides a plausible mechanism for leveraging the sequential observability unique to forecasting."}, "weaknesses": {"value": "- Although the paper proposes a simplified, output-space-only adapter, the underlying idea of residual correction or adapter-based adaptation is not fundamentally new. Similar concepts—such as residual adapters, calibration layers, and lightweight correction modules—have appeared in various works.\n\n- While the simplicity of a linear output-space adapter is appealing, it may also limit the method’s expressive power, particularly under nonlinear or abrupt regime shifts.\n\n- The context vector construction relies on fixed aggregation (mean or median) and fixed length K, which might not generalize across datasets with different periodicities or nonstationary characteristics.\n\n- The paper positions the cosine–adaptive learning rate (CALR) as theoretically motivated but does not provide formal derivation or convergence analysis."}, "questions": {"value": "- The context vector $C_t$ is built from recent batch statistics (e.g., means) with fixed length $K$. Could a learned context encoder (e.g., lightweight RNN/SSM/attention) or an adaptive $K$ selected online (error-aware rather than rule-based) improve robustness across different periodicities and drift patterns without eroding the latency advantages? Please discuss the accuracy–overhead trade-off.\n\n- COSA applies a learnable gate $g$ with $\\tanh(g)$ to modulate correction. Do you observe interpretable gate trajectories during level shifts, scale drift, and seasonal phase changes (e.g., stronger gating when residuals spike)? Would horizon-specific or channel-specific gating improve control or stability relative to a single scalar gate?\n\n- Experiments decompose multivariate data into per-variable univariate tasks. In settings with strong cross-variable dependence, would COSA benefit from structured $W$ (e.g., block-sparse or low-rank shared components) or shared context to capture cross-covariances? What happens to parameter count, memory, and latency as dimensionality grows?\n\n- You describe a cosine–adaptive learning-rate schedule with thresholds motivated by stability. Can you provide formal reasoning or empirical evidence for convergence/safety (e.g., avoidance of error amplification) under short adaptation steps $S$, and comparisons against simpler schedules (fixed/one-cycle/standard AdamW) with statistical significance?\n\n- In cases where short-term perturbations revert (e.g., transient scale spikes), how quickly does COSA roll back adaptation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3k7HPcUvDD", "forum": "L7Z5wBMPrW", "replyto": "L7Z5wBMPrW", "signatures": ["ICLR.cc/2026/Conference/Submission6831/Reviewer_htEG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6831/Reviewer_htEG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655287566, "cdate": 1761655287566, "tmdate": 1762919094000, "mdate": 1762919094000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a simple yet effective method to adapt time-series forecasting models under distribution shifts. Instead of dual adapters that modify both inputs and outputs, COSA introduces a single output-space adapter that directly corrects predictions from a frozen base model using a linear residual correction modulated by a learnable gate. The adapter leverages a lightweight context vector summarizing recent ground-truth statistics and updates only its parameters during inference under a leakage-free streaming protocol. COSA achieves consistent accuracy gains across six datasets and multiple model types, while remaining architecture-agnostic and computationally efficient for deployment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a clear and elegant solution to the problem of test-time adaptation in time-series forecasting through a single output-space adapter, offering originality by simplifying previous dual-adapter frameworks. The methodology is technically sound, with a well-motivated design and a careful leakage-free adaptation protocol. Empirical results are extensive, covering multiple architectures, horizons, and datasets, demonstrating consistent and significant improvements. The paper is well written, with strong clarity in the problem formulation and experimental design. Its simplicity, effectiveness, and deployment-friendliness make it a valuable contribution with practical and theoretical significance for the TTA and time-series communities."}, "weaknesses": {"value": "The context construction relies on simple aggregation statistics, which may not capture complex temporal dependencies or handle irregular or missing data effectively. The experiments focus mainly on standard LTSF datasets without testing on more challenging non-stationary or real-world drift scenarios. The computational analysis could better clarify the trade-offs between adaptation time and inference latency. Additionally, statistical significance reporting or variance analysis would strengthen the reliability of the reported performance gains."}, "questions": {"value": "1. Can the authors clarify under which conditions an output-only adapter is sufficient, and when input-side calibration would still be necessary for stable adaptation?\n\n2. How sensitive is COSA to the construction of the context vector? Would richer temporal encodings or normalization statistics improve its robustness and generalization?\n\n3. Could the method be extended to jointly handle multivariate forecasting instead of decomposing into independent univariate tasks?\n\n4. How would COSA perform under partial label availability or delayed target feedback, which are common in real-world streaming scenarios?\n\n5. Would the proposed adaptive learning rate schedule still remain stable when applied to models with much longer adaptation windows or non-periodic datasets?\n\n6. Could you fix the code link? It seems to have expired."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jDiwjjV9xJ", "forum": "L7Z5wBMPrW", "replyto": "L7Z5wBMPrW", "signatures": ["ICLR.cc/2026/Conference/Submission6831/Reviewer_hY4L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6831/Reviewer_hY4L"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922907731, "cdate": 1761922907731, "tmdate": 1762919093659, "mdate": 1762919093659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces COSA (Context-aware Output-Space Adapter), a Test-Time Adaptation (TTA) framework designed to address the performance degradation of time series forecasting models caused by distribution shifts after deployment. In contrast to existing TTA methods that often employ complex dual input-output adapters, COSA proposes a simpler and more direct strategy that operates solely in the output space. The core mechanism utilizes a lightweight context vector, which summarizes statistics from recently observed ground truth, along with the base model's original prediction. This information is fed through a linear layer, modulated by a gating mechanism, to compute a residual correction. During the test phase, only the adapter's parameters are updated, while the backbone model remains frozen. Extensive experiments demonstrate that COSA improves prediction accuracy while reducing inference latency.\n\n---"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  **Simplicity and Novelty:** The \"output-space-only\" design is a great innovation. It directly corrects predictions, offering a simpler, more intuitive, and safer alternative to complex dual-adapter methods that indirectly modify data distributions.\n2.  **Inference Efficiency:** The method is quite fast, achieving a nearly order-of-magnitude speedup in inference time compared to prior TTA methods. This high efficiency makes it well-suited for practical, low-latency deployment scenarios.\n3.  **Empirical Performance:** The method's effectiveness is validated by empirical results, where it consistently achieves best performance across six benchmark datasets and multiple backbone architectures, with particularly gains in long-horizon forecasting.\n4.  **Comprehensive Validation:** The paper includes a thorough experimental analysis, featuring detailed ablation studies and sensitivity analyses on key hyperparameters. This evaluation validates the method's design choices and strengthens the credibility of the findings."}, "weaknesses": {"value": "1. **Unconventional Experimental Settings:** The experimental setup has some questionable aspects: (1) The lookback window of 96 is relatively short. Most contemporary work uses longer history windows, such as 336 or 512. Predicting a long horizon of 720 from a short 96-step history is a somewhat impractical scenario and may weaken the persuasiveness of the method's effectiveness. (2) For multivariate time series, the mainstream approach (even for channel-independent models) is to process all variates concurrently. Splitting multivariate series into univariate ones for individual forecasting is inefficient, especially for widely-used datasets with a large number of variates like Electricity and Traffic.\n2. **Missing Comparison with a Key Baseline and Motivation:** The authors motivate their work by claiming that existing TTA methods for time series forecasting use a dual-adapter architecture, and theirs is novel for being output-space-only. However, a recent paper, [SOLID](https://arxiv.org/abs/2310.14838), is also a TTA algorithm for time series that performs adaptation solely on the output prediction layer and involves an analysis of residuals. This paper is missing experimental comparison and analysis against SOLID, which weakens the novelty claim and leaves a gap in the evaluation.\n3. **Potential Risk of Over-Correction and Instability:** The paper mentions that the adaptive learning rate scheduler, CALR, adopts an \"aggressive\" strategy for fast convergence. While the tanh(g) gating mechanism bounds the correction, an aggressive learning policy, when faced with noisy or anomalous batches, could still lead the adapter to learn unstable or oscillating correction patterns. This over-correction might amplify noise in the short term rather than capturing the true distribution shift, especially with a small batch size B."}, "questions": {"value": "1. **Design of the Context Vector:** The context vector C is constructed by aggregating the K most recent batches, which implicitly assumes temporal locality. However, this assumption may not always hold. For series with strong periodicity, data points from the same phase in previous cycles might be more informative contexts. Conversely, for series with abrupt concept drifts, a context vector based on recent history could provide outdated or even detrimental information. Is a design that relies solely on recent locality optimal, or would it be more robust to consider other relationships such as periodicity?\n2. **Clarification on Parameter Efficiency:** Given that COSA has far more parameters than PETSA, could the authors clarify the rationale behind choosing a linear layer with a relatively large parameterization (i.e., the weight W is L x (L+K))? Furthermore, could more parameter-efficient structures, such as low-rank factorization, be explored to potentially achieve similar performance while reducing the parameter count?\n3. **Practical Use of Adaptive Batch Size B in COSA-P:** The paper proposes COSA-P, which uses the PAAS strategy to adapt the batch size B, and shows its superiority on the ETTh1 dataset, while COSA-F performs better on others. In a real-world deployment without clear periodic patterns, the PAAS strategy may not be effective. This raises two practical questions: 1) How should we as practitioners choose between COSA-F (fixed B) and COSA-P for a new real-world dataset? 2) For COSA-F, are there any guidelines or heuristics for selecting an optimal fixed value for B?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "psVyvMP91V", "forum": "L7Z5wBMPrW", "replyto": "L7Z5wBMPrW", "signatures": ["ICLR.cc/2026/Conference/Submission6831/Reviewer_8Mzj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6831/Reviewer_8Mzj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982341244, "cdate": 1761982341244, "tmdate": 1762919093183, "mdate": 1762919093183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}