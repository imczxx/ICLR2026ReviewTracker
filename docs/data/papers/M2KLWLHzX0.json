{"id": "M2KLWLHzX0", "number": 9398, "cdate": 1758121027564, "mdate": 1763728582882, "content": {"title": "Towards Reliable Detection of Empty Space: Conditional Marked Point Processes for Object Detection", "abstract": "Deep neural networks have set the state-of-the-art in computer vision tasks such as bounding box detection and semantic segmentation. Object detectors and segmentation models assign confidence scores to predictions, reflecting the model’s uncertainty in object detection or pixel-wise classification. However, these confidence estimates are often miscalibrated, as their architectures and loss functions are tailored to task performance rather than probabilistic foundation. Even with well calibrated predictions, object detectors fail to quantify uncertainty outside detected bounding boxes, i.e., the model does not make a probability assessment of whether an area without detected objects is truly free of obstacles. This poses a safety risk in applications such as automated driving, where uncertainty in empty areas remains unexplored. In this work, we propose an object detection model grounded in spatial statistics. Bounding box data matches realizations of a marked point process, commonly used to describe the probabilistic occurrence of spatial point events identified as bounding box centers, where marks are used to describe the spatial extension of bounding boxes and classes. Our statistical framework enables a likelihood-based training and provides well-defined confidence estimates for whether a region is drivable, i.e., free of objects. We demonstrate the effectiveness of our method through calibration assessments and evaluation of performance.", "tldr": "Conditional marked Poisson point processes for object detectors with well-calibrated void confidences.", "keywords": ["Computer Vision", "Confidence Calibration", "Object Detection", "Spatial Point Processes"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5f1ee2d7661a7a92fd08fd18dc5df5f41a9828de.pdf", "supplementary_material": "/attachment/3716758802a9ad9d6f1de514dc07c1b11883ce11.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel object detection framework based on spatial statistics, specifically conditional marked point processes, to address the critical problem of quantifying uncertainty in empty space regions. Unlike traditional object detectors that only provide confidence scores for detected objects, this approach can assess the probability that a given region is truly free of obstacles - a crucial capability for safety-critical applications like autonomous driving. The method models bounding box data as realizations of marked point processes and derives a likelihood-based training objective that enables well-calibrated confidence estimates for empty space regions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) Novel Problem Formulation and Theoretical Foundation: The paper addresses a fundamental gap in object detection by providing uncertainty quantification for empty space regions, which is crucial for safety-critical applications. The approach is mathematically rigorous, grounded in spatial point process theory, and represents the first application of such methods to deep object detection.\n2)Clear Presentation and Methodology: The paper is well-written with clear mathematical exposition, good visualization of results, and comprehensive experimental evaluation across multiple network architectures."}, "weaknesses": {"value": "1) The paper lacks comparison with other uncertainty quantification methods in object detection (Bayesian approaches, ensemble methods, Monte Carlo dropout) and existing calibration techniques. Evaluation is restricted to only two datasets, and there's insufficient analysis of baseline comparisons beyond semantic segmentation models.\n2) The Poisson assumption may not hold for real object distributions which often exhibit clustering or repulsion. The factorization in Eq. (3) assumes independence between spatial location and object properties, which may be unrealistic. The method also has scale issues, assigning square patches to detected peaks that conflict with objects of varying sizes."}, "questions": {"value": "1)How realistic is the Poisson assumption for object distributions in real scenes with clustering or mutual exclusion?\n2)Have you considered hybrid approaches combining high-performance detectors with your calibration framework?\n3)How does computational overhead compare to standard detectors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4O84jSA9nQ", "forum": "M2KLWLHzX0", "replyto": "M2KLWLHzX0", "signatures": ["ICLR.cc/2026/Conference/Submission9398/Reviewer_dzBf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9398/Reviewer_dzBf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761473398773, "cdate": 1761473398773, "tmdate": 1762921007627, "mdate": 1762921007627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper formulates object detection as a conditional marked Poisson point process (CMPPP): box centers are points; widths/heights/classes are marks; an image-conditioned intensity λ and mark distribution p(m|ξ,I) yield a likelihood-based loss and, crucially, closed-form probabilities that arbitrary regions are empty (“drivable”). The loss emerges from the Radon–Nikodym derivative of the (marked) PPP relative to a homogeneous PPP reference, giving a principled alternative to heuristic detection losses. Implementations use segmentation backbones (DeepLabv3+, HRNet, SegFormer) to predict dense maps for intensity, size, and class. Experiments report calibration of void probabilities on Cityscapes and VisDrone and compare mAP to baseline detectors (lower mAP, better void calibration)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Principled probabilistic formulation. \n- Clear derivation of a likelihood (negative log-RN) for (marked) point processes → a coherent objective for detection and void confidence, instead of ad-hoc objectness + CE/L1. The derivation and discretization details are explicit.\n\nOperational “empty-space” probability. \n- Two definitions (no centers in A; no boxes intersecting A) lead to computable expressions, incl. a practical Laplace-based integral for box intersection (Eq. 10–12). This directly targets a safety-critical question standard detectors don’t answer.\n\nCalibration protocol & results. \n- Expected Calibration Error (ECE) for randomly sampled boxes across scales; PPP/CMPPP show orders-of-magnitude lower ECE than segmentation-product or standard detectors. Plots and tables are convincing for the posed metric.\n\nHonest positioning. \n- Authors explicitly do not claim SOTA mAP and frame the contribution as a probabilistic foundation enabling calibrated emptiness estimates.\n\nFor me, this paper look like an \"old dish but with a completely new taste.\" I personally think we need more paper like this."}, "weaknesses": {"value": "Modeling assumptions (independence / PPP). \n- A PPP ignores interactions (e.g., repulsion/occlusion between objects). Authors note this in limitations; nonetheless, it undercuts realism and may bias void probabilities in crowded scenes. Extending to Gibbs/repulsive processes or Cox processes would strengthen claims. \n\n\nEmpirical scope is narrow. \n- Only two datasets (Cityscapes, VisDrone), limited classes; no distribution-shift tests, no multi-seed variance, and little analysis of sensitivity to discretization (H×W) or the single inference hyperparameter (crop size) beyond an appendix note. Also, more experiments on more standard detection datasets will be helpful, e.g., COCO.\n\nmAP lags standard detectors. \n- Reported CMPPP mAP is worse than common baselines like Faster R-CNN/CenterNet; the paper argues calibration is the goal, but many venues expect Pareto curves (mAP vs calibration vs speed) to contextualize trade-offs. \n\nCalibration baseline for segmentation may be weak. \n- Their segmentation “void” probability multiplies per-pixel “road” probabilities (independence assumption). Modern segmentation calibration (temperature scaling, Dirichlet, focal-calibration) could shrink the reported gap—this isn’t explored. \n\n\nEvaluation choices. \n- Random box sampling is simple but may not reflect planner-relevant regions (e.g., near obstacles/curbs). Lack of PDQ reporting (they cite PDQ) misses a natural probabilistic detection metric to compare against probabilistic baselines. \n\nUncertainty taxonomy. \n- The method yields aleatoric void probabilities; epistemic uncertainty (e.g., via ensembles/MC-Dropout) is mentioned as combinable but not evaluated. For “reliable” navigation, both matter. \n\nTODOs:\n- Trying to extend the model with more advanced architectures, e.g., DINO (the detection one) or latest YOLO. \n- Add more experiments on COCO or other larger datasets that covers more diverse objects."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r8kQ7W0W0V", "forum": "M2KLWLHzX0", "replyto": "M2KLWLHzX0", "signatures": ["ICLR.cc/2026/Conference/Submission9398/Reviewer_EoWf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9398/Reviewer_EoWf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948780620, "cdate": 1761948780620, "tmdate": 1762921007286, "mdate": 1762921007286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Reponse on Comparison with Re-Calibration Methods"}, "comment": {"value": "We thank all reviewers for their valuable feedback. Two points came up more than once, so we decided to address them as common responses to all reviewers.\n\nWe concede that a better description of the problem and corresponding experimental validation is in order: \n\nOur motivation for applying spatial point process models is that void confidences from pixel classifiers such as semantic segmentation or objectness cannot be rectified by re-calibration techniques. For empirical validation of this claim, we provide results for semantic segmentation and objectness post-hoc calibration which show that satisfactory re-calibration of the obtained confidences is not possible, even when the area of test regions is fixed. Below, we provide an intuitive conceptual explanation of this phenomenon.\n    \nIn the following table, we tested temperature scaling (ts) calibrations. Although this slightly improved void calibration of the baseline models, they remain at least an order of magnitude worse calibrated than our model. \n    \n| Model      | Metric   | s=1    | 250    | 500    | 750    | 1000   | 1500   | 2500   | 5000   | 10000  |\n| ---------- | -------- | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ |\n| DeepLabv3+ | ECE_S    | 0.0591 | 0.1102 | 0.1741 | 0.2033 | 0.2245 | 0.2521 | 0.2667 | 0.2417 | 0.1948 |\n|            | ECE_S_ts | 0.0514 | 0.1286 | 0.1411 | 0.1525 | 0.1618 | 0.1653 | 0.1797 | 0.1837 | 0.0922 |\n|            | ECE_P (ours)    | 0.0000 | 0.0012 | 0.0017 | 0.0029 | 0.0029 | 0.0046 | 0.0062 | 0.0109 | 0.0164 |\n| HRNet      | ECE_S    | 0.0682 | 0.0413 | 0.0859 | 0.1142 | 0.1443 | 0.1785 | 0.2206 | 0.2295 | 0.1939 |\n|            | ECE_S_ts | 0.0609 | 0.0378 | 0.0596 | 0.1245 | 0.1329 | 0.1397 | 0.1520 | 0.1597 | 0.1641 |\n|            | ECE_P (ours)    | 0.0000 | 0.0008 | 0.0012 | 0.0014 | 0.0019 | 0.0022 | 0.0041 | 0.0053 | 0.0071 |\n| SegFormer  | ECE_S    | 0.0755 | 0.0621 | 0.0585 | 0.0609 | 0.0593 | 0.0588 | 0.0582 | 0.0626 | 0.0713 |\n|            | ECE_S_ts | 0.0863 | 0.0434 | 0.0378 | 0.0379 | 0.0338 | 0.0344 | 0.0333 | 0.0364 | 0.0481 |\n|            | ECE_P (ours)    | 0.0000 | 0.0006 | 0.0008 | 0.0014 | 0.0018 | 0.0027 | 0.0046 | 0.0053 | 0.0082 |\n    \n**Semseg re-calibration**: We fit a temperature scaling to $20\\%$ of the Cityscapes validation data and evaluate on the complementary set. For each value of $s$, we fit and evaluate on test boxes of that size exclusively as this is a best case scenario for the re-calibration of pixel classifiers. No generalization to other (significantly larger or smaller) test box sizes is required. Realistically, calibration would be performed at the pixel level and then tested on other sizes, which yields poorer results than scaling for the respective test box size as shown in the table.\n\n**OD re-calibration**: We perform the same re-calibration scheme for our object detection baselines (Faster R-CNN and CenterNet). The ECE value for both models tested for a sample box size of $s=1000$ improves from $0.9915$ when uncalibrated to $0.5435$ with temperature scaling. We observe that our method is significantly better calibrated than segmentation or object detection models.\n    \n\n**Explanation**: The fact that this is the case can be understood by the following thought experiment. For test region $A$, a semseg or objectness model predicts some void confidence $\\widehat{p}(A)$ as a product of confidences/objectness values over (super-)pixels within $A$ as described in our paper. We show that, and argue why, these values are vast under-estimations of the measured void frequencies. Increasing the area by a factor of 2 to an area $A'$ will also increase the number of factors in the confidence multiplication by 2, so we expect a behavior of $\\widehat{p}(A') \\approx [\\widehat{p}(A)]^2$, i.e., we have exponential scaling with respect to the increasing factor of the area.\nOne the one hand, this means that it is impossible to re-calibrate well across significantly differing test region sizes. On the other hand, even for fixed areas of test regions $A$, the confidences obtained from a pixel classifier by multiplying per-pixel confidences cannot even be re-calibrated to a satisfactory degree. This is because the model is not designed to yield meaningful confidences across image neighborhoods.\n\nWe believe that making this argument more explicit and plastic in our introductory section strengthens the motivation for our model design and experimental evaluation. We will include the experimental validation of this intuition in section 4.2."}}, "id": "fQFOsXXPOV", "forum": "M2KLWLHzX0", "replyto": "M2KLWLHzX0", "signatures": ["ICLR.cc/2026/Conference/Submission9398/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9398/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9398/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763727392789, "cdate": 1763727392789, "tmdate": 1763728345752, "mdate": 1763728345752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a Conditional Marked Poisson Point Process (CMPPP) model for object detection. The core motivation is to provide a probabilistically sound framework that can accurately estimate the confidence of \"empty space\" (drivable areas), addressing the lack of such uncertainty measures in standard object detectors. The method is derived from spatial statistics, using a negative log-likelihood loss for end-to-end training. Experiments on Cityscapes and VisDrone compare the proposed method's calibration against semantic segmentation and older object detection baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Theoretical Novelty: The derivation of the object detection task from the theory of marked point processes is mathematically grounded and offers a different perspective compared to standard heuristic-based loss functions.\n\n- Addressing an Overlooked Problem: attempting to quantify the uncertainty of regions without detections is a relevant topic for safety-critical applications."}, "weaknesses": {"value": "- Questionable Problem Formulation: The paper heavily prioritizes calibration over standard accuracy metrics. However, calibration does not mean high accuracy. The premise that drivable area requires such a complex probabilistic setup is not entirely convincing; in many standard applications, drivable area is effectively treated as a discrete distribution for decision-making. The experimental setting for calibration appears somewhat contrived to highlight the proposed method's strengths while ignoring standard operational requirements (high mAP).\n\n- Missing Standard Calibration Techniques in Baselines: The paper compares its intrinsically calibrated method against standard DNNs (like DeepLabv3+) that are known to be miscalibrated out-of-the-box. A fair comparison requires these standard models to be evaluated with common post-hoc calibration techniques applied, most notably temperature scaling. It is possible that a standard detector with simple temperature scaling achieves comparable empty-space calibration to the proposed complex CMPPP method, which would significantly diminish the core contribution.\n\n- Unfair Baselines (Segmentation Task): The comparison with semantic segmentation models regarding \"drivable area\" calibration is unfair. The baselines were trained in a multi-class setting. For a fair comparison, the semantic segmentation baselines should be trained specifically in a binary classification setting (road vs. non-road).\n\n- Outdated Baselines (Detection Architectures): The chosen object detection baselines (Faster R-CNN, CenterNet) are outdated for ICLR 2026. The field has moved to transformer-based architectures. The authors should compare against DETR or its more recent variants to demonstrate if the proposed CMPPP really offers advantages over modern state-of-the-art detectors."}, "questions": {"value": "- Why did you not include temperature scaling (or other standard post-hoc calibration methods) for the baseline models? Comparing against uncalibrated raw logits is a weak baseline.\n\n- Why did you not train the semantic segmentation baselines on the binary \"road vs. non-road\" task for a fairer comparison of empty space calibration?\n\n- Can you provide results comparing your method to modern detection architectures like DETR?\n\n- Given that calibration is not a substitute for accuracy, how does the downstream planner benefit from a well-calibrated but less accurate detector?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rtwLxeFHGQ", "forum": "M2KLWLHzX0", "replyto": "M2KLWLHzX0", "signatures": ["ICLR.cc/2026/Conference/Submission9398/Reviewer_hbUL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9398/Reviewer_hbUL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961453597, "cdate": 1761961453597, "tmdate": 1762921006928, "mdate": 1762921006928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors tackles the problem that modern object detectors provide confidence only for detected objects but not for empty regions. The authors propose a probabilistic approach based on Conditional Marked Poisson Point Processes (CMPPP) that are able to model both detections and confidently predict empty spaces. By treating object centers as spatial points with marks for size and classes, the model can estimate the probability that any region is truly object-free. Trained with corresponding likelihood loss, the authors demonstrate that their approach results in well-calibrated aleatoric uncertainty and achieves competitive detection accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper contributes to the important study of uncertainty-based object detection, which is highly relevant for autonomous driving and robotics applications.\n\n* It is clearly written and easy to follow; the main idea of the proposed method is intuitive, and the limitations of prior approaches are well described.\n\n* The proposed probabilistic framework is principled and mathematically grounded, providing a coherent way to quantify uncertainty for both detected objects and empty regions."}, "weaknesses": {"value": "* The experimental evaluation is relatively narrow, focusing mainly on the Cityscapes dataset; testing on additional datasets (e.g., KITTI, BDD100K) would better demonstrate generalization to diverse environments and scene layouts. The same experimental protocol could also be extended to 3D object detection tasks using datasets such as nuScenes or Waymo, which would show whether the proposed probabilistic modeling scales to spatially richer domains.\n\n* The claimed improvement in calibration would be more convincing with comparisons to strong post-hoc calibration baselines (e.g., temperature scaling) applied to existing detectors with center-prediction segmentation heads (CenterNet-style), where re-calibration could be performed pixel-wise. While such methods may require a separate calibration set, this limitation can often be mitigated in practice. To strengthen the claim, the paper could include an analysis of how much calibration data would actually be needed to achieve comparable performance with standard post-hoc approaches.\n\n* While the method enables probabilistic estimation of empty-space confidence, its practical relevance remains unclear. The paper does not demonstrate how this “emptiness calibration” translates to downstream tasks such as planning, risk estimation, or control. To make the contribution more impactful, the authors could connect the calibrated emptiness probabilities to decision-making metrics — for instance, by integrating them into a planner or trajectory evaluation module. Extending the framework to 3D object detection and testing on datasets like nuScenes or Waymo would also allow assessing how such uncertainty information might affects predicted vehicle trajectories and safety-related metrics."}, "questions": {"value": "* Have you evaluated how well the method generalizes beyond Cityscapes, for instance on KITTI or BDD100K, or considered extending it to 3D datasets like nuScenes or Waymo?\n\n* How would your approach compare to standard post-hoc calibration methods such as temperature scaling or pixel-wise calibration applied to CenterNet-style detectors?\n\n* How could the proposed emptiness calibration be integrated into downstream tasks like motion planning or trajectory evaluation to demonstrate its practical value?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xJPzRKROi9", "forum": "M2KLWLHzX0", "replyto": "M2KLWLHzX0", "signatures": ["ICLR.cc/2026/Conference/Submission9398/Reviewer_PjVJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9398/Reviewer_PjVJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762124230310, "cdate": 1762124230310, "tmdate": 1762921006554, "mdate": 1762921006554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}