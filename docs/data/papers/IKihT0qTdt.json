{"id": "IKihT0qTdt", "number": 16992, "cdate": 1758270994963, "mdate": 1763048339353, "content": {"title": "Investigating Self-Supervised Representations for Audio-Visual Deepfake Detection", "abstract": "Self-supervised representations excel at many vision and speech tasks, but their potential for audio-visual deepfake detection remains underexplored. Unlike prior work that uses these features in isolation or buried within complex architectures, we systematically evaluate them across modalities (audio, video, multimodal) and domains (lip movements, generic visual content). We assess three key dimensions: detection effectiveness, interpretability of encoded information, and cross-modal complementarity. We find that most self-supervised features capture deepfake-relevant information, and that this information is complementary. Moreover, the models attend to semantically meaningful regions rather than spurious artifacts. Yet none generalize reliably across datasets. This generalization failure likely stems from dataset characteristics, not from the features themselves latching onto superficial patterns. These results expose both the promise and fundamental challenges of self-supervised representations for deepfake detection: while they learn meaningful patterns, achieving robust cross-domain performance remains elusive.", "tldr": "Self-supervised features capture useful and complementary patterns for audio-visual deepfake detection, yet struggle to generalize across datasets, more likely due to failing to capture broader artifacts rather than relying on spurious correlations.", "keywords": ["deepfake", "deepfake detection", "audio-visual", "self-supervised representations", "video forensics"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/763b5739e5ef4ce3972ce21ee56e7eb71ca72db1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper evaluates self-supervised representations (SSL) for audio-visual deepfake detection through a systematic, cross-modal study. Using linear probing on frozen backbones from diverse modalities (audio, visual, and audiovisual), the authors analyze what information these representations encode, how interpretable their decisions are, and how complementary different modalities become when combined. Experiments across four datasets (AV-Deepfake1M, FakeAVCeleb, AVLips, and DeepfakeEval-2024) show that SSL features such as from CLIP, Wav2Vec2, and AV-HuBERT capture semantically meaningful deepfake cues and focus on manipulated facial regions rather than spurious artifacts. However, none of the models generalize reliably to unseen domains, showing that while SSL features can be strong and complementary, robust cross-dataset generalization remains a key open challenge."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper compares a sufficient spectrum of SSL models across modalities, providing a well-rounded analysis of their suitability for multimodal deepfake detection.\n\nThe linear-probe setup, log-sum-exp aggregation, and controlled training pipeline enable fair comparisons and isolate the representational power of the backbones.\n\nThe inclusion of temporal and spatial explanations, as well as quantitative comparison to human click annotations, goes beyond conventional performance reporting (e.g., simply AUC results) and provides some insight into model behavior.\n\nThe observation that SSL models encode meaningful yet non-transferable information, and that generalization failures stem largely from dataset characteristics rather than spurious feature reliance, can be useful for future work.\n\nThe writing is clear, and the paper is easy to understand."}, "weaknesses": {"value": "The paper offers limited practical novelty in its insights. The main conclusion that self-supervised representations perform well in-distribution but fail to generalize is not very surprising, as this pattern is well established in the deepfake detection literature (see [1] for a popular earlier work), where most methods struggle under domain shift. It is thus largely expected that self-supervised models trained in a similar way would show the same behavior. While the analysis is systematic, it largely confirms known challenges rather than providing new conceptual understanding or solutions.\n\nThe related work section overlooks some prior studies exploring self-supervised learning for deepfake detection (e.g., [2], [3]). Moreover, Auto-AVSR is presented as a self-supervised method, whereas it is in fact a supervised audio-visual speech recognition model trained with labeled transcriptions. Additionally, earlier work such as LipForensics [4] already demonstrated that lipreading-based representations can be leveraged for deepfake detection.\n\nThe study relies on linear probes to evaluate representational quality for supervised learning. However, it is well known that Transformer-based SSL models often exhibit limited linear separability and can perform substantially better when fine-tuned in higher layers (e.g., [5]). This raises questions about how representative the reported results are of each model’s true potential, particularly with respect to generalization.\n\nThe paper does not compare its results to state-of-the-art deepfake detection methods. Without such baselines, it is difficult to interpret the absolute performance of the SSL features and to assess their practical relevance beyond relative comparisons.\n\n[1] Li, Lingzhi, et al. \"Face x-ray for more general face forgery detection.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.\n\n[2] Haliassos, Alexandros, et al. \"Leveraging real talking faces via self-supervision for robust forgery detection.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n\n[3] Zhang, Daichi, et al. \"Learning natural consistency representation for face forgery video detection.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024.\n\n[4] Haliassos, Alexandros, et al. \"Lips don't lie: A generalisable and robust approach to face forgery detection.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n\n[5] He, Kaiming, et al. \"Masked autoencoders are scalable vision learners.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022."}, "questions": {"value": "Beyond confirming the well-known gap between in-distribution and out-of-distribution performance, what specific new insights do the authors believe this study provides about why self-supervised representations fail to generalize?\n\nSince Transformer-based SSL models often exhibit weak linear separability but improve with fine-tuning, could the authors justify focusing just on linear probes (for supervised training)? Would partial fine-tuning change the generalization trends observed?\n\nWould it be possible to include or discuss results against recent deepfake detectors, to contextualize the absolute performance of the evaluated SSL features?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PPQKmU91Va", "forum": "IKihT0qTdt", "replyto": "IKihT0qTdt", "signatures": ["ICLR.cc/2026/Conference/Submission16992/Reviewer_MybE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16992/Reviewer_MybE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761591952074, "cdate": 1761591952074, "tmdate": 1762927008512, "mdate": 1762927008512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We would like to withdraw this submission as we are preparing a significantly improved version."}}, "id": "wl1JZ4AoN8", "forum": "IKihT0qTdt", "replyto": "IKihT0qTdt", "signatures": ["ICLR.cc/2026/Conference/Submission16992/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16992/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763048338582, "cdate": 1763048338582, "tmdate": 1763048338582, "mdate": 1763048338582, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates audio–visual deepfake detection using self-supervised representations. Unlike prior works that apply pretrained features within complex pipelines, the authors conduct a systematic evaluation across modalities and domains to analyze effectiveness, interpretability, and cross-modal complementarity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is clearly written and easy to follow.\n\n- The flow from problem definition to empirical observation is natural and coherent, allowing readers to easily grasp the motivation and key findings."}, "weaknesses": {"value": "- The novelty and contribution are rather limited. The proposed methods are straightforward, and the paper focuses more on analysis than on developing new algorithms.\n\n- While readability is good, the paper could be made more compact; for example, information such as Table 1 (basic model descriptions) belongs in the appendix rather than the main text.\n\n- The comparisons are not fully appropriate for a deepfake detection study. The paper contrasts self-supervised models (e.g., AV-HuBERT vs. AV-HuBERT + proposed algorithm) rather than comparing against dedicated deepfake detection baselines.\n\nOverall, while the motivation and storyline are strong, the technical content and experimental design do not yet meet the ICLR standard for methodological contribution."}, "questions": {"value": "Please refer to Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hOrq5fwDa8", "forum": "IKihT0qTdt", "replyto": "IKihT0qTdt", "signatures": ["ICLR.cc/2026/Conference/Submission16992/Reviewer_ooUb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16992/Reviewer_ooUb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921408819, "cdate": 1761921408819, "tmdate": 1762927007765, "mdate": 1762927007765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the use of self-supervised representations for audio-visual deep fake detection. Multiple audio and visual encoders are used and evaluated on multiple datasets and an attempt to identify the regions where these models focus on is made."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is well written\n- The study presented is very interesting and novel (to the best of my knowledge)\n- Results on multiple datasets are presented, both in-domain and out-of-domain\n- The authors attempt to identify the regions the models attend to, which is very interesting.\n- Overall, the conclusions are interesting."}, "weaknesses": {"value": "- The auto-AVSR model performs better than AV-Hubert for visual and audio-visual speech recognition. Is there a hypothesis why it performs so much worse than AV-Hubert for deep fake detection.\n- Table 2, the performance of audio and visual AutoAVSR features is much lower than the audio-visual AutoAVSR features. This looks a bit weird, since the gap for AV-Hubert is much smaller.\n- Have the authors tried to combine features from multiple layers instead of using the last layer's features?\n- One of the conclusions is that the proxy tasks (AV synchronisation and next-token prediction) do not work well for deep fake detection. Have the authors evaluate the performance of these models? i.e., how well they perform on AV synchronisation and next-token prediction. If they perform well, then the conclusion drawn is sound, if not, then no conclusion can be drawn."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YAK5hc2GDh", "forum": "IKihT0qTdt", "replyto": "IKihT0qTdt", "signatures": ["ICLR.cc/2026/Conference/Submission16992/Reviewer_ofoL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16992/Reviewer_ofoL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009920517, "cdate": 1762009920517, "tmdate": 1762927006933, "mdate": 1762927006933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an empirical study of using pretrained self-supervised audiovisual representations as inputs to a simple deepfake detection head on top. The work uses standard audiovisual models trained on audiovisual speech datasets of human talking heads, and evaluates deepfake detection performance on standard downstream benchmarks.\n\nWhile the paper is a detailed study of how these methods perform on the task, given the gaps in novelty and soundness (detailed below), I am leaning towards rejecting the paper."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The chosen self-supervised audiovisual representation are appropriate, as are the evaluation datasets. The results presented on the tested datasets seem reasonable, and are accompanied by qualitative analysis of temporal and spatial explanations.\n\n- The paper is well written and is easy to read and understand\n\n- The reader could benefit from understanding how the chosen self-supervised method does on the standard datasets, which could help them not redo the same results in their own work."}, "weaknesses": {"value": "- The paper uses linear probing on top of pretrained self-supervised audiovisual models to evaluate on multiple downstream datasets. To me, this is akin to running a bunch of baselines on the tested datasets with no particular technical novelty.\n\n- A crucial technical drawback of the evaluation setting to me is the fact that individual frames are encoded separately by the audio/visual backbones before being aggregated. While frame-level information does provide useful information, the essence of audiovisual deepfake detection (for the more complex synthetic examples) is usually tied to the temporal patterns of the data (both synchrony between lip/face movements with the speech content and its consistency and realness over time). Instead of embeddings extracted from one frame alone, at the bare minimum I would have liked to see an ablation study using a sliding window or being modeled temporally using a token mixing Transformer or recurrent layer.\n\n- Given that this work was more focused on evaluating existing representations, I found the lack of methods like AVData2Vec and RAVEN to be a gap. These methods are similar to AV-Hubert but presented better performance on related audiovisual understanding tasks.\n\n- The paper does not effectively introduce seminal related work in important areas. For example, for audiovisual synchronization, works like SyncNet (Chung et al), Multisensory Networks (Owens et al) were important. Similarly, I found no introduction of work related to synthetic talking head generation / speech-driven facial animation (even though it is mentioned in the fake video generation portion of the chosen datasets).\n\n- Line 1 of the abstract is “Self-supervised representations excel at many vision and speech tasks, but their potential for audio-visual deepfake detection remains underexplored” -> For many of the related papers that the work cites, the baselines seem to be self-supervised representations!\n\n- I think the overall writing can be made clearer to indicate that the focus is on human-centric “talking head” videos as opposed to general deepfake videos. Given this, some of the qualitative analysis showing heightened contributions from the facial regions seems unnecessary to me (and the spurious detections in the non facial regions are potentially problematic)."}, "questions": {"value": "- Did the authors try to preprocess the datasets to only have the face crops used as input? If so, did they try to do any preprocessing (e.g. face alignment) that is commonly used in these pipelines?\n\nAlso, it would be helpful to directly address the weaknesses I mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "71wupdtCmV", "forum": "IKihT0qTdt", "replyto": "IKihT0qTdt", "signatures": ["ICLR.cc/2026/Conference/Submission16992/Reviewer_vUMH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16992/Reviewer_vUMH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762150833065, "cdate": 1762150833065, "tmdate": 1762927006376, "mdate": 1762927006376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}