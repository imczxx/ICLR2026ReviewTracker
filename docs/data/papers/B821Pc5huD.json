{"id": "B821Pc5huD", "number": 22291, "cdate": 1758329095979, "mdate": 1759896874565, "content": {"title": "SOMA: Efficient Multi-turn LLM Serving via Small Language Model", "abstract": "Large Language Models (LLMs) are increasingly deployed in multi-turn dialogue settings where preserving conversational context across turns is essential. A standard serving practice concatenates the full dialogue history at every turn, which reliably maintains coherence but incurs substantial cost in latency, memory, and API expenditure, especially when queries are routed to large proprietary models. Existing approaches often struggle to balance the trade-off between response quality and efficiency. We propose a framework that exploits the early turns of a session to estimate a local response manifold and then adapt a smaller surrogate model to this local region for the remainder of the conversation. Concretely, we learn soft prompts that maximize semantic divergence between the large and surrogate small language models' responses to surface least-aligned local directions, stabilize training with anti-degeneration control, and distill the mined cases into localized LoRA fine-tuning so the surrogate runs without prompts at inference. A simple gate enables a one-time switch with rollback on drift. We further provide a theoretical analysis for key components in SOMA. Extensive experiments show the effectiveness of SOMA. The source code is provided at: https://anonymous.4open.science/r/SOMA-D377.", "tldr": "", "keywords": ["Small Language Model", "multi-turn conversations", "local manifold approximation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b6bbcf2fc30cecdb850de2d3ed80f23ebcd281a5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "To address the substantial latency and memory costs in multi-turn conversations with LLMs, this paper proposes a method to estimate a local response manifold and adapt a smaller surrogate model to replace the original large model. It first optimizes a set of learnable soft prompts that generate responses most divergent from the responses generated by the original model using three losses from both token level and the distribution level; then, the surrogate smaller model is fine-tuned on the learned and fixed soft prompts to minimize the NLL loss between the generated and ground-truth responses. A theoretical analysis is provided to guide hyperparameter selection. Experiments are conducted with several LLMs across six datasets, achieving superior performance with lower token consumption."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper identifies a key observation of the number of tokens in multi-turn conversions using LLMs, which motivates the use of smaller surrogate models to replace the original large models in the remaining rounds of conversions.\n\n2. This paper proposes the concept of $\\textit{semantic neighborhood}$ in the latent space of LLMs such that the generated tokens by smaller models not only exactly match the tokens generated by the original models, but also satisfy the semantic distributions.\n\n3. The experiments show both performance and efficiency superiority of the proposed method across different LLMs and several datasets and tasks."}, "weaknesses": {"value": "1. The authors mention $\\textit{manifold}$ many times in this paper and claim the divergence on a local manifold. However, the proposed method does not formally investigate and define the structure of the manifold in the latent space, and also does not utilize tools and concepts widely used in manifold, such as tangent space, log and exp operations, geodesic distances, and so on. The “manifold” used in this paper is a subspace of high-dimensional Euclidean space, which is indeed a manifold by definition, but it is a \"trivial\" manifold. The cosine distance used in this paper verifies that this manifold is just a linear subspace.\n\n2. The Section of \"Expectation-weighted Semantic Divergence Loss\" (Line 267 - 296) is hard to follow. It is unclear why this defines a distribution-level alignment. It may be an issue of presentation, not methodology.\n\n3. In Line 204 - 205, the authors mention \"a fast semantic closeness test\", but it seems this test is not described in the following sections in the main text, which makes the inference process vague when using the fine-tuned smaller surrogate models. Correct me if I was wrong."}, "questions": {"value": "1. Why do we need to find the optimal soft prompts that drive the smaller surrogate model to generate the most divergent responses, and then fine-tune the smaller models on these optimized soft prompts? It looks similar to adversarial learning, but the motivation and correctness are not clearly described.\n\n2. In the final loss formula in Line 322 - 323, why does it not contain the Semantic Divergence Loss defined in Eq. (1)? Is this a typo, or did I miss something?\n\n3. In the fine-tune loss defined in Line 339 - 340, why does it contain the cosine similarity between the encoded features of the response? Does it mean a single NLL loss is not enough?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "V2loRWvgQd", "forum": "B821Pc5huD", "replyto": "B821Pc5huD", "signatures": ["ICLR.cc/2026/Conference/Submission22291/Reviewer_X3Gv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22291/Reviewer_X3Gv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895877477, "cdate": 1761895877477, "tmdate": 1762942153084, "mdate": 1762942153084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work proposes SOMA (Soft-prompts for lOcal Manifold Approximation), a framework for efficient multi-turn LLM serving that leverages a small surrogate model to replace a large, expensive language model after the initial dialogue turns. Observing a long-tail token distribution—where early turns are information-dense and later turns are short yet context-dependent—the authors frame the problem as approximating the large model’s local response manifold within the conversation-specific region. SOMA first uses differentiable soft-prompt tuning to identify directions of maximal semantic divergence between the large and small models, enhanced by an expectation-weighted semantic loss and an anti-degeneration regularizer to ensure meaningful exploration. It then distills these mined cases into localized LoRA fine-tuning, enabling prompt-free inference with the adapted small model. A lightweight cosine-based gate decides when to switch to the surrogate and triggers rollback if topic drift is detected. Theoretical analysis provides bounds on switching reliability and prompt coverage, while experiments across six datasets show SOMA achieves higher response fidelity to the original model than strong baselines while significantly reducing token usage and latency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. SOMA exploits the empirically observed long-tail token distribution in multi-turn dialogues, where early turns are information-dense and later turns are short yet context-dependent.\n\n2. SOMA introduces an effective local manifold approximation framework grounded in soft-prompt tuning to identify directions of maximal behavioral divergence between the large and small models. This targeted exploration surfaces the most informative failure cases for adaptation, ensuring that the surrogate model is fine-tuned precisely."}, "weaknesses": {"value": "1. SOMA relies on a warm-start phase that requires multiple initial turns to be processed by the expensive large language model before the surrogate can take over. This upfront cost may be prohibitive in settings where conversations are typically short, limiting the framework’s applicability and overall efficiency gains. The method assumes that most dialogues are sufficiently long to amortize this initial overhead, which may not hold across all real-world use cases.\n\n2. This work assumes that the local manifold induced by early dialogue turns remains stable and representative for the remainder of the conversation. If the dialogue undergoes significant topic shifts or introduces new complex reasoning demands later on, the fine-tuned surrogate—trained only on early context—may fail to adapt adequately, even with the rollback mechanism. While the cosine-based gate attempts to detect drift, it may not reliably capture subtle or gradual semantic shifts that degrade response quality without triggering a rollback.\n\n3. SOMA’s effectiveness depends heavily on the quality and capacity gap between the original large model and the small surrogate. As shown in the Qwen experiments, when the surrogate is significantly weaker (e.g., 0.6B vs. 8B), even localized fine-tuning may not fully bridge the behavioral gap, leading to lower absolute fidelity. This suggests that SOMA may not scale well to extreme model size disparities or to tasks requiring deep reasoning that small models fundamentally cannot replicate, regardless of adaptation.\n\n4. The soft-prompt mining stage, while theoretically grounded, introduces non-trivial computational and implementation complexity. It requires careful tuning of multiple hyperparameters (e.g., neighborhood size, temperature, anti-degeneration weight) and assumes access to the surrogate’s embedding space and tokenizer. This limits its plug-and-play usability, especially in black-box or API-only settings where internal model details are inaccessible, reducing its practicality for end users without deep technical resources."}, "questions": {"value": "Please refer to the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2DKNxwxZ2W", "forum": "B821Pc5huD", "replyto": "B821Pc5huD", "signatures": ["ICLR.cc/2026/Conference/Submission22291/Reviewer_oZsj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22291/Reviewer_oZsj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940423686, "cdate": 1761940423686, "tmdate": 1762942152867, "mdate": 1762942152867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge of efficient multi-turn LLM serving. This paper proposed SOMA, a framework that use the early turns of a conversation to construct a “local response manifold.” SOMA identifies least-aligned response directions between a large model and a smaller surrogate using soft prompt mining, stabilizes training via anti-degeneration regularization, and performs localized LoRA fine-tuning. At inference, a lightweight cosine-based gating mechanism performs one-time switching and rollback upon topic drift."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This topic is very interesting and meaningful.\n2. Addresses a genuine bottleneck: multi-turn inference cost in LLM serving, by introducing a pipeline compatible with existing inference engines (vLLM, FlashAttention).\n3. Integrates semantic divergence mining, local manifold theory, and LoRA fine-tuning into a unified and reproducible system.\n4. Theoretical bounds (Thm. 1–3) guide hyperparameter selection, which are empirically validated.\n5. Tests across model families and diverse datasets, outperforming strong baselines."}, "weaknesses": {"value": "1. During soft-prompt mining, the large model receives a verbalized discrete prefix, while the small model uses continuous embeddings. This asymmetry may distort outputs and bias divergence measurement. The paper lacks ablation study about it.\n2. The primary metric, similarity to the large model’s responses, measures stylistic alignment rather than factual correctness. No human evaluation or task-grounded accuracy (e.g., exact match on MATH) is provided. This undermines claims of “no quality loss.”\n3. Sampling and normalization methods (e.g., “normalized by Turn 1”) are unspecified. The observation may simply restate known conversational trends.\n4. Theoretical Assumptions Too Strong:\n\n(1) The discrepancy Fisher matrix is assumed locally smooth and low-rank without empirical verification (e.g., spectral decay plots).\n\n(2) The switching theorem presumes sub-Gaussian improvement, but no diagnostic supports this."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XORIOYP9Oq", "forum": "B821Pc5huD", "replyto": "B821Pc5huD", "signatures": ["ICLR.cc/2026/Conference/Submission22291/Reviewer_DGr6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22291/Reviewer_DGr6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984232881, "cdate": 1761984232881, "tmdate": 1762942152653, "mdate": 1762942152653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SOMA—a multi-turn LLM serving framework that adapts a small “surrogate” model to locally approximate a large “original” model within the context of an ongoing dialogue. The motivation is an empirical long-tail token pattern: early turns are long and information-dense, later turns are shorter but still depend on early context, suggesting a path to improve serving cost without hurting quality (Figure 1) . SOMA runs a three-stage pipeline: (1) soft-prompt tuning to mine directions of maximal behavioral divergence between the small and large models; (2) localized LoRA fine-tuning on those mined cases so the surrogate no longer needs prompts at inference; and (3) efficient inference with an extractive summary and a cosine-gate switch/rollback when drift is detected . The paper also gives theory: a directional-recovery guarantee for the mining objective (Theorem 1), a detection bound for switching (Theorem 2), and a coverage guarantee for the number of soft-prompt candidates (Theorem 3)"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Cohesive framing + pipeline: The long-tail observation motivates a clear local-approximation approach; the soft-prompt mining → localized LoRA → switch/rollback story is well-scoped and practical for serving systems\n\n2. Evidence of effectiveness:  Across six datasets SOMA attains the highest similarity to the original while reducing tokens per dialogue"}, "weaknesses": {"value": "1. Experimental section is thin and indirect with respect to theory: \nThe main experimental evidence is essentially one page (Table 1 + two figures). While results are positive, they do not directly validate the theoretical claims. there are no measurements of manifold discrepancy, empirical calibration of the switching bound\n\n2. Evaluation targets fidelity to the original, not task quality: \nThe metric is LLM-as-judge similarity to original outputs (and the judge prompt is provided in the appendix), which conflates “matching F” with “being good.” This risks circularity and judge bias; there’s no human evaluation or task-specific metrics\n\n3. Baseline and setting details could be tighter : \nModel pairings and fairness. The “original” and “surrogate” sometimes differ by both size and family (e.g., LLaMA-3.1-70B vs LLaMA-2-7B), which introduces style/tokenization gaps that may either help or hurt specific methods; please justify this choice and include within-family ablations where possible.\n\n4. Datasets and filtering: \nThe paper notes filtering to keep context-dependent dialogues, but there’s little analysis of how this impacts difficulty and whether conclusions hold without filtering; details on the MATH/MT-Bench multi-turn construction would help reproducibility and interpretation"}, "questions": {"value": "See weakness for more details.\nQ1. can you also add more relevant baselines: e.g. Speculative decoding and FrugalGPT (LLM cascades)? Or at least discuss to contextualize the contributions of SOMA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zeh9QoB569", "forum": "B821Pc5huD", "replyto": "B821Pc5huD", "signatures": ["ICLR.cc/2026/Conference/Submission22291/Reviewer_ejFa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22291/Reviewer_ejFa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762488754379, "cdate": 1762488754379, "tmdate": 1762942152425, "mdate": 1762942152425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}