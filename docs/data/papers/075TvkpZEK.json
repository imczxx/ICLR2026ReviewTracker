{"id": "075TvkpZEK", "number": 15935, "cdate": 1758257347389, "mdate": 1759897272046, "content": {"title": "SMARAN: Closing the Generalization Gap with Performance Driven Optimization Method", "abstract": "Optimization methods have evolved significantly by introducing various learning rate scheduling techniques and adaptive learning strategies. Although these methods have achieved faster convergence, they often struggle to generalize well to unseen data compared to traditional approaches such as Stochastic Gradient Descent (SGD) with momentum. Adaptive methods such as Adam store each parameter's first and second moments of gradients, which can be memory-intensive. To address these challenges, we propose a novel SMARAN optimization method that adjusts the learning rate based on the model's performance rather than the objective function's curvature. This approach is particularly effective for minimizing stochastic loss functions, standard in deep learning models. Traditional gradient-based methods may get stuck in regions where the gradient vanishes, such as plateaus or local minima. Therefore, instead of only depending on the gradient, we use the model's performance to estimate the appropriate step size. We performed extensive experiments on standard vision benchmarks, and the generalization trends observed with SMARAN demonstrate compelling distinctions relative to adaptive and non-adaptive optimizers.", "tldr": "", "keywords": ["Optimization", "Gradient decent", "Learning rate scheduler", "Regularization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/51bea55276c3d93f4d63af8d07abe5cf281cc86d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an optimization algorithm SMARAN for deep learning. SMARAN has two main characteristics, the first is that it normalizes the gradient before updating the first-order momentum, and the second is that it adopts the objective function value to update the second-order momentum. Then this work provides the analysis of the regret bound for SMARAN based on common assumptions. Finally, SMARAN is compared with several classical or adaptive optimizers in the experiments of CV tasks. SMARAN achieves great test accuracies on CIFAR datasets, and obviously a low generalization gap on Tiny-Imagenet.\n\nThe main contribution of this work is that it adopts the function value in the adaptive learning rate, which could reduce the memory cost of optimizer states."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This paper proposes an algorithm SMARAN which adopts the function value in the second-order momentum. This is a novel technique in the optimizer studies. The writing of this paper is clear. Most contents of this work are easy to understand."}, "weaknesses": {"value": "The “Introduction” part lists a series of drawbacks of previous methods. However, the proposed method SMARAN seems not to overcome all these drawbacks, except for the large memory of Adam, which is not mentioned in the following parts. This work needs to emphasize the motivation for using the function value to calculate the learning rate in SMARAN. It is also not explained or discussed in the work why replacing the gradient with the function value could close the generalization gap.\n\nSome statements in the work lack the support of references. For instance, it states that “Previous methods … because gradients give the curvature of the landscape. However, for a nonconvex setting, steep curvature results in slow learning, whereas in our approach”. I think some related works should be provided for these assertions.\n\nThe setting of the experiments is relatively simple, i.e. only conducting the CV tasks. SMARAN adopts an adaptive learning rate, and the common adaptive optimizers are good at training a transformer-based model. More experiments on this kind of model should be included.\n\nThe presentation of the experimental results needs to be improved. It would be better to summarize the specific values of the test accuracies in one list to make the results clearer. This work states that SMARAN is a memory-efficient optimizer. However, this point is not shown in the experiments.\n\nIn addition, the organization of this work is also poor. The formulas in the proof of theorems leave too many blanks in the paper."}, "questions": {"value": "It is mentioned at the end of page 4 that “Since the learning rate scheduler is based on the objective function value over training data, if the optimizer tries to overfit the training data, the same proportion of regularization prevents the model from overfitting”. Could you give a more detailed explanation of why adopting the adaptive regularization factor, and what advantage it has over the constant regularization factor."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z6KDCNGnvE", "forum": "075TvkpZEK", "replyto": "075TvkpZEK", "signatures": ["ICLR.cc/2026/Conference/Submission15935/Reviewer_bjWs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15935/Reviewer_bjWs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849234450, "cdate": 1761849234450, "tmdate": 1762926150054, "mdate": 1762926150054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SMARAN, a novel optimization method for deep learning that adjusts the learning rate based on the model's performance (i.e., the objective function value) rather than the gradient's curvature, aiming to close the generalization gap often seen in adaptive optimizers. Unlike Adam, which uses exponential moving averages (EMAs) of gradients, SMARAN uses the EMA of past loss values to scale the learning rate and incorporates a form of adaptive weight decay to prevent overfitting. Experiments on vision benchmarks (CIFAR, Tiny ImageNet) show that SMARAN achieves better generalization, with lower test loss and smaller generalization gaps, compared to state-of-the-art optimizers like Adam, AdamW, and SGD with momentum."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core strength is its demonstrated ability to achieve superior generalization performance, as evidenced by consistently lower test loss and significantly smaller generalization gaps across multiple datasets and architectures compared to popular baselines.\n2. The key innovation of using the EMA of the loss value (rather than gradients) to adapt the learning rate is conceptually distinct from most existing methods. This approach allows the optimizer to be cautious when losses are high and accelerate when losses are low and decreasing, potentially avoiding pitfalls like vanishing gradients.\n3. SMARAN is more memory-efficient than adaptive methods like Adam because its adaptive learning rate component is a scalar (based on the overall loss) rather than a vector (requiring storage of per-parameter moment estimates)."}, "weaknesses": {"value": "1. The experiments are confined to image classification tasks on standard vision datasets (CIFAR, Tiny ImageNet). The paper lacks evaluation on more complex tasks (e.g., language modeling, object detection) or larger-scale datasets (e.g., ImageNet), making it difficult to assess the method's broader applicability and scalability.\n2. While the paper provides a regret bound in the online convex setting, deep learning involves highly non-convex optimization. The analysis does not fully address the behavior of SMARAN in this more relevant non-convex landscape, which is the primary context for its use.\n3. The learning rate adaptation depends directly on the absolute value of the loss. If the loss function has a very different scale (e.g., due to different architectures or tasks), the hyperparameters (especially the global learning rate η) might need significant retuning, potentially reducing its claimed \"adaptive\" advantage in practice. The paper uses a fixed γ=0.9 and λ=0.01, but a more thorough ablation study on these hyperparameters would strengthen the claims."}, "questions": {"value": "How does SMARAN’s performance-driven learning rate adaptation theoretically behave in non-convex landscapes, particularly near saddle points or flat regions where the loss value may remain nearly constant for many iterations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XAtfGekxmD", "forum": "075TvkpZEK", "replyto": "075TvkpZEK", "signatures": ["ICLR.cc/2026/Conference/Submission15935/Reviewer_7Jd3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15935/Reviewer_7Jd3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923406994, "cdate": 1761923406994, "tmdate": 1762926149007, "mdate": 1762926149007, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel optimization method which the authors term as SMARAN, that aims to bridge the generalization gap often seen with adaptive optimizers and improve memory efficiency. SMARAN uniquely adjusts its learning rate based on the model's performance (loss values), utilizing exponential moving average (EMA) of normalized gradients to determine the update direction and an EMA of squared loss values to dynamically scale the learning rate.\n\nThe authors argue that such a strategy based on loss performance allows for cautious learning with high losses and accelerated convergence in the regime with low and decreasing losses, thus preventing stagnation in flat regions. \n\nSMARAN also integrates an adaptive weight decay regularization, whose strength is tied to the performance-based learning rate, to address overfitting. Theoretical analysis provided along with extensive experiments on image classification problems using multiple model architecture types."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I find a few key strengths in this work:\n- The most significant strength is SMARAN's innovative approach to adjust the learning rate based on the objective function value (model or loss performance) rather than solely gradient information. This represents a fresh and unique direction in optimizer design.\n- Proposed method SMARAN directly tackles two critical issues in deep learning: the generalization gap associated with adaptive methods and their memory intensiveness due to per-parameter moment storage. SMARAN's scalar learning rate is a clear win for memory efficiency, and the empirical results also seem to strongly support improved generalization.\n- The integration of an adaptive weight decay mechanism, which is dynamically controlled by the performance-based learning rate, is a clever design. This dynamic regularization is well-suited for mitigating overfitting as the model converges.\n- Proposed method consistently shows compelling empirical superiority across diverse vision benchmarks (CIFAR-10, CIFAR-100, Tiny ImageNet) and architectures (ResNet50, DenseNet121) when compared to a comprehensive set of baselines, including SGD, Adam, AdamW, RAdam, DecGD, and Prodigy."}, "weaknesses": {"value": "Some weaknesses in the paper still remain undressed at this point:\n- The use of a normalized gradient (Equation 2) could introduce numerical instability if the gradient norm approaches zero even when a reasonable epsilon value is used.\n- While the paper states it's \"performance-driven,\" the \"performance\" metric is consistently defined as the loss function value. Although loss is a direct indicator, further discussion on whether other performance metrics (e.g., validation accuracy, task-specific metrics) could be effectively incorporated into the adaptive learning rate calculation would be interesting. Wonder if the results would change in any way if we had looked these alternative performance metrics?\n- Would have been good to see some theoretical study or explanations for how method performs in the non-convex setting.\n- While future work mentions extending to text and video, the current experiments are primarily on vision tasks., it would be good to obtain empirical insights on language models / tasks as well."}, "questions": {"value": "Some qns for the authors:\n- How robust is the proposed SMARAN method to extremely noisy gradients, particularly concerning the normalized gradient? Could situations arise where the gradient norm is very small but not precisely zero, leading to an amplified noisy direction?\n- The adaptive weight decay is a key feature. Could the authors elaborate on scenarios where a fixed weight decay (like in AdamW) might still be preferred, or where SMARAN's adaptive weight decay might have limitations?\n- Could the authors offer more insight into the specific cases where SMARAN showed the largest performance gains or where its generalization gap was most significantly reduced compared to other optimizers? Are there particular types of datasets or model complexities where SMARAN particularly shines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VD5MgrCKC7", "forum": "075TvkpZEK", "replyto": "075TvkpZEK", "signatures": ["ICLR.cc/2026/Conference/Submission15935/Reviewer_4A8R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15935/Reviewer_4A8R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938795917, "cdate": 1761938795917, "tmdate": 1762926148594, "mdate": 1762926148594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an optimizer that adaptively changes its effective learning rate based on the an EMA of losses, in addition to using a EMA on the gradient history. Modulation of the learning rate as a function of the loss, is their main contribution. They provide a regret analysis in the online convex optimization setting. Empirically, across standard vision benchmarks they report competitive convergence with stronger generalization than Adam-style methods and SGD, while avoiding per-parameter second-moment storage."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper is easy to follow and is presented in a clear manner.\n2) They propose a loss-driven gain that avoids per-parameter second-moment buffers reduces memory and implementation complexity."}, "weaknesses": {"value": "1) Missing reference and comparison to [1].\n\n2) I am afraid the hyper parameters with which SGD experiments are run are not optimal. For instance in Fig 1(a) SGD on ResNet-50 does less than 70% test accuracy, however in this [2] github implementation's default values SGD achieves 93.5%. Providing some clarity on this would be helpful in gauging the proposed methods efficicacy.\n\n\n[1] Rolinek, Michal, and Georg Martius. \"L4: Practical loss-based stepsize adaptation for deep learning.\" Advances in neural information processing systems 31 (2018).\n\n[2] https://github.com/kuangliu/pytorch-cifar"}, "questions": {"value": "In line 128, it is mentioned that when the recent losses are low the learning rate is increased leading to faster convergence. However, looking at the update equation 13,$$\nx_{t+1} = x_t - \\eta \\left( \\frac{f(x_t)}{\\sqrt{v_t} + \\varepsilon} \\right)\\,(m_t + \\lambda x_t)\n$$\nlooking at this equation, I am afraid with lower loss the learning rate is decreased and not increased as stated in the paper. Can you provide clarity on line 128."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pICgwEUkNO", "forum": "075TvkpZEK", "replyto": "075TvkpZEK", "signatures": ["ICLR.cc/2026/Conference/Submission15935/Reviewer_Jv5d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15935/Reviewer_Jv5d"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971664556, "cdate": 1761971664556, "tmdate": 1762926148166, "mdate": 1762926148166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}