{"id": "hCy2mld5DK", "number": 17358, "cdate": 1758275004666, "mdate": 1762927775745, "content": {"title": "Towards Training One-Step Diffusion Models Without Distillation", "abstract": "Recent advances in training one-step diffusion models typically follow a two-stage pipeline: first training a teacher diffusion model and then distilling it into a one-step student model. This process often depends on both the teacher’s score function for supervision and its weights for initializing the student model. In this paper, we explore whether one-step diffusion models can be trained directly without this distillation procedure. We introduce a family of new training methods that entirely forgo teacher score supervision, yet outperforms most teacher-guided distillation approaches. This suggests that score supervision is not essential for effective training of one-step diffusion models. However, we find that initializing the student model with the teacher’s weights remains critical. Surprisingly, the key advantage of teacher initialization is not due to better latent-to-output mappings, but rather the rich set of feature representations across different noise levels that the teacher diffusion model provides. These insights take us one step closer towards training one-step diffusion models without distillation and provide a better understanding of the roles of teacher supervision and initialization in the distillation process.", "tldr": "", "keywords": ["diffusion model", "one-step generative model", "distillation"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/ad61de27de5c95950dfeec153a27159d8011d05a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the possibility of training one-step diffusion models without teacher supervision, i.e., training on the score matching gradient. Instead, it proposes a GAN-like method that trains a classifier to distinguish between data from noisy real data or noisy synthetic data. Performances on CIFAR-10 and ImageNet are comparable with those trained under teacher supervision. In summary, this paper proposes a novel one-step diffusion method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed method is a simple yet effective method. It gets rid of teacher supervision but can achieve comparable performances. From the efficiency perspective, the contribution of this method is significant."}, "weaknesses": {"value": "The paper is missing some details and lacks experimental analysis. The major weaknesses are listed below. Please also see the questions.\n\n1. How is the class-ratio estimator trained? I didn't find details of this paper, and I hope the author can clarify.\n2. Based on the performances in Tables 1 and 2, I would say the major advantage of this method is its efficiency, since it doesn't need the teacher's output. Some comparison of the efficiency can make this paper more convincing.\n3. The author only performs experiments on class-conditional image generation. How about the text-to-image task?\n\nMinors: Figure 1 has overlapping legends, and there's a typo in Table 1 on Line 393.\n\nI'll consider raising the score if all conerns are addressed."}, "questions": {"value": "1. Is the class-ratio estimator pretrained, or is it trained jointly with the diffusion model? I'd like to see some analysis on the design choice of training the class-ratio estimator.\n2. Can your method scale up to more complex tasks, such as video generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oOaM3817Hj", "forum": "hCy2mld5DK", "replyto": "hCy2mld5DK", "signatures": ["ICLR.cc/2026/Conference/Submission17358/Reviewer_weJd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17358/Reviewer_weJd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17358/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792046630, "cdate": 1761792046630, "tmdate": 1762927273183, "mdate": 1762927273183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Thank you for the useful feedback. We have decided to withdraw the paper for further improvement."}}, "id": "cvd4tmTcoq", "forum": "hCy2mld5DK", "replyto": "hCy2mld5DK", "signatures": ["ICLR.cc/2026/Conference/Submission17358/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17358/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762927774581, "cdate": 1762927774581, "tmdate": 1762927774581, "mdate": 1762927774581, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript proposes to train an one-step inference model not trained by classical diffusion process and distillation. It highlights the initialization of well-trained diffusion models, and claims that it exactly matter that features cover varied levels of noise."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- A thorough background introduction spanning at least four pages is provided without appendix statements, and only one page of methodology in the main paper and two pages of experiments to support main claims with tremendous and pure visualization in the appendix, catering to readers unfamiliar with these foundational concepts."}, "weaknesses": {"value": "- Limited experiments on only small-scale datasets. The experimental parts of initialization just propose two hypotheses, but limited additional experiments, generalization ablation and analyses are provided to support them.\n- It would be better to compare the proposed methods on similar settings with GANs, where the exact framework is very similar to GANs', that of one-step inference and without classic diffusion model designs of distillation, but a score-parameterization GAN.\n- The incomplete post-ablation model serves as the baseline, e.g., EDM-2-XL of 13.0 FID. And the one of the main claims says that one-step inference beats most of the other diffusion models under these settings."}, "questions": {"value": "- Even if the author claims that this is not a GAN that need converged discriminator in page 7, it's still hard to convince. Isn't it a better statement that it's a score-parameterization GAN initialized by diffusion model's weights under Bayesian formulation, where the so-called classifier is the discriminator to learn from the teacher models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "V663d7rqu2", "forum": "hCy2mld5DK", "replyto": "hCy2mld5DK", "signatures": ["ICLR.cc/2026/Conference/Submission17358/Reviewer_vXEj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17358/Reviewer_vXEj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17358/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875675339, "cdate": 1761875675339, "tmdate": 1762927272714, "mdate": 1762927272714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel diffusion model training method that directly trains a one-step student model without supervision from a teacher model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors adopt a likelihood ratio estimation approach to directly estimate the density ratio between the student and teacher models, thereby avoiding explicit supervision from the teacher model. This is a novel training strategy."}, "weaknesses": {"value": "1. The authors claim in the introduction that existing methods cannot achieve high-quality generation within five steps. However, to my knowledge, methods such as DMD2, SenseFlow, Hybrid SD, and Consistency Trajectory Model have already demonstrated high-quality generation within four steps or even a single step.\n\n2. Although the proposed method does not require training two models simultaneously like VSD, it still requires a pre-trained classifier before training the student model. Therefore, in total, the proposed approach still involves training two models—a classifier and a student model—which does not seem to offer a clear efficiency advantage over the original VSD.\n\n3. Typically, the related work section is placed either after the introduction or after the experiments but positioning it between the method and experiments sections seems inappropriate.\n\n4. The experiments are only conducted on CIFAR-10 and ImageNet, which are relatively simple image datasets. More complex tasks, such as text-to-image generation, are not explored. Given that text-to-image synthesis is currently the most widely applied domain for diffusion models, it is necessary to include experiments on this task and compare with state-of-the-art models such as SD 3.5, SDXL, and FLUX."}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FWpORSx01q", "forum": "hCy2mld5DK", "replyto": "hCy2mld5DK", "signatures": ["ICLR.cc/2026/Conference/Submission17358/Reviewer_Anmi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17358/Reviewer_Anmi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17358/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003468470, "cdate": 1762003468470, "tmdate": 1762927272216, "mdate": 1762927272216, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}