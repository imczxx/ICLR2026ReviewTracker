{"id": "U9meoc0Sau", "number": 18953, "cdate": 1758292301020, "mdate": 1759897071061, "content": {"title": "SceneCOT: Eliciting Chain-of-Thought Reasoning in 3D Scenes", "abstract": "Existing research of 3D LLMs still struggles to achieve efficient and explainable reasoning, primarily due to the under-exploration of the mechanism of human-like scene-object grounded reasoning. This paper bridges the gap by presenting a novel framework. We first introduce a Chain-of-Thought reasoning framework in 3D scenes (SceneCOT), decoupling a complex reasoning task into simpler and manageable problems, and building corresponding visual clues based on multimodal expert modules. To enable such a framework, we build the first large-scale 3D scene Chain-of-Thought reasoning dataset, SceneCOT, including more than 190k high-quality data instances. Extensive experiments across various complex 3D scene reasoning benchmarks demonstrate that our new framework achieves state-of-the-art with clear interpretability. To our knowledge, this is the first attempt to successfully implement the COT technique for achieving human-like step-by-step reasoning for 3D scene understanding, where we show great potential in extending it to a wider range of 3D scene understanding scenarios.", "tldr": "A step-by-step reasoning framework for 3D scene understanding", "keywords": ["3D scene reasoning", "chain-of-thought reasoning", "multimodal LLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1cea4ba40be4fd32e8f0151e28dd570514035769.pdf", "supplementary_material": "/attachment/78bdf388d332aa7364d6ceb396e4498bd9b74373.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces grounded Chain-of-Thought reasoning into 3D large language models, decoupling complex reasoning tasks into simpler and more manageable problems, and constructing corresponding visual cues through multimodal expert modules. To achieve this, the authors focus on dataset curation. By training on the proposed dataset, the model attains state-of-the-art performance on several 3D understanding benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The first to introduce CoT reasoning into 3D understanding.\n2. Proposes a large-scale dataset to support the study.\n3. The model achieves leading performance across multiple benchmarks."}, "weaknesses": {"value": "1. **Limited evaluation:** The authors primarily evaluate on MSQA and Beacon3D, while several widely used benchmarks such as ScanQA and SQA-3D are not considered.\n2. **Limited data sources:** The annotations mainly originate from Nr3D and MSQA. Incorporating larger and more diverse datasets and scenes, such as MMScan[3], could enhance generalization.\n3. **Object-centric input:** The method mainly relies on object-centric input, which requires an additional segmentation model during inference and thus limits broader applicability. The ablation results show that performance is highly dependent on segmentation labels. Since object-centric input already provides a strong spatial prior for grounded reasoning, it remains unclear whether the proposed method can generalize to video-based 3D LLMs such as Video-3D LLM[1] or LLaVA-3D[2].\n\n[1] https://arxiv.org/abs/2412.00493 (CVPR 25)\n[2] https://arxiv.org/abs/2409.18125 (ICCV 25)\n[3] https://arxiv.org/abs/2406.09401 (NIPS 24)"}, "questions": {"value": "1. Can grounded large-scale datasets like **3D-GRAND** be integrated into your pipeline?\n2. What is the purpose of introducing probability during reasoning? Are there any ablation studies to justify its effect?\n3. The **LEO** model adopts a similar object-centric input as the proposed method. Why does the performance on MSQA decrease after training on your dataset? The effectiveness of the proposed reasoning mechanism remains questionable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TI9M1gnHNc", "forum": "U9meoc0Sau", "replyto": "U9meoc0Sau", "signatures": ["ICLR.cc/2026/Conference/Submission18953/Reviewer_rnc2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18953/Reviewer_rnc2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828240895, "cdate": 1761828240895, "tmdate": 1762931006749, "mdate": 1762931006749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SceneCoT, a 3D scene understanding framework that employs step-by-step chain-of-thought (CoT) reasoning to enhance spatial reasoning performance on benchmarks such as MSQA and Beacon3D. To facilitate 3D CoT reasoning, the authors construct a large-scale dataset, SceneCoT-185K. Experimental results further demonstrate that SceneCoT effectively improves grounding–QA coherence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. SceneCoT demonstrates strong performance in spatial reasoning, particularly on counting and grounding questions. Moreover, its step-wise grounded reasoning provides a transparent and interpretable rationale.\n\n2. The construction of the CoT steps is reasonable and aligns well with how humans approach spatial question answering.\n\n3. The paper is well-written, clearly organized, and easy to follow."}, "weaknesses": {"value": "1. SceneCOT is designed around specific reasoning tasks—namely Situated Reasoning and Object-Centric Reasoning—and limited question types such as counting, attribute, and spatial relationship queries. This task-specific design may restrict the model’s ability to generalize to unseen question types encountered in more complex 3D world.\n2. In Table 1, the overall performance of LEO and MSR3D on MSQA drops noticeably after fine-tuning on the SceneCoT-185K dataset. Could the authors provide analysis on the possible reasons behind this degradation?\n3. Since SceneCOT is designed based on LLaVA-1.5, it would be helpful to show the performance comparison of these two models.\n4. It would be helpful to evaluate SceneCoT on out-of-domain datasets (e.g., SQA3D [1], Hypo3D [2], VSIBench [3]) to verify whether CoT fine-tuning improves spatial reasoning beyond the training domain.\n\n[1] Ma, Xiaojian, et al. \"Sqa3d: Situated question answering in 3d scenes.\"\n\n[2] Mao, Ye, et al. \"Hypo3D: Exploring Hypothetical Reasoning in 3D.\"\n\n[3] Yang, Jihan, et al. \"Thinking in space: How multimodal large language models see, remember, and recall spaces.\""}, "questions": {"value": "I have concluded all my questions in the weakness sections."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MaAezHKmmb", "forum": "U9meoc0Sau", "replyto": "U9meoc0Sau", "signatures": ["ICLR.cc/2026/Conference/Submission18953/Reviewer_tikH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18953/Reviewer_tikH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847360778, "cdate": 1761847360778, "tmdate": 1762931006309, "mdate": 1762931006309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of poor grounding in 3D vision-language models, where models often generate plausible-sounding answers that are not factually connected to the 3D scene. The authors propose SCENECOT, a novel framework that introduces step-by-step, Chain-of-Thought (CoT) reasoning to 3D question answering. The method explicitly decomposes a complex 3D reasoning task into four manageable stages: task recognition, task-relevant region localization, entity/attribute grounding using expert modules, and final grounded reasoning. To train this framework, the authors also developed SCENECOT-185K, the first large-scale dataset containing 185,000 grounded CoT reasoning traces for 3D scenes. Experiments demonstrate that SCENECOT achieves competitive performance on the general MSQA benchmark and, most notably, significantly outperforms all baselines on the Beacon3D benchmark, which is specifically designed to measure grounding-QA coherence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work directly targets the critical and well-documented problem of poor grounding-QA coherence in 3D-VL models. Instead of just aiming for better QA accuracy, it focuses on ensuring the answers are correctly derived from the scene's visual context.\n2. The framework's multi-stage design (task recognition, region localization, grounding, reasoning) is intuitive and inherently interpretable. This transparency makes it easier to diagnose failure cases, as shown in the qualitative examples.\n3. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. The SCENECOT framework is a complex, multi-stage pipeline rather than a simple end-to-end model. Its performance is heavily reliant on a cascade of specialized, pre-trained modules (e.g., Mask3D for object proposals, PQ3D for grounding). This introduces multiple potential points of failure, and the overall performance is strongly coupled to the quality of these \"expert\" modules.\n2. The dataset, while large, is constructed from existing benchmarks (MSQA, Nr3D) that are primarily based on the ScanNet dataset. This limits the diversity of scenes, objects, and tasks. The paper acknowledges that the framework does not yet extend to more complex, long-horizon embodied tasks.\n3. The paper introduces a \"grounded CoT\" framework, but it lacks experiments on standard 3D visual grounding benchmarks (e.g., Nr3D, Sr3D, or ScanRefer). While Beacon3D measures coherence (grounding + QA), evaluating the grounding module's performance in isolation on these tasks seems essential to fully validate the \"grounded\" aspect of the CoT."}, "questions": {"value": "1. The generation of the SCENECOT-185K dataset relies on rule-based methods and LLM (GPT-4O) generation. What was the extent of manual verification to ensure the quality and correctness of the intermediate reasoning \"thoughts\"?\n2. In the 4-stage pipeline, how does the model handle error propagation? For instance, if the initial \"Task Recognition\" step fails, or the \"Region Localization\" identifies the wrong area, does this inevitably lead to a final failure, or are there mechanisms for recovery in the later stages?\n3. The multi-step inference process, which involves calls to symbolic engines and expert grounding modules, seems computationally more intensive than an end-to-end model. What is the comparative inference latency of SCENECOT versus baselines like Chat-Scene or LLaVA-3D? Is the framework practical for real-time applications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9BBSZ0wR1n", "forum": "U9meoc0Sau", "replyto": "U9meoc0Sau", "signatures": ["ICLR.cc/2026/Conference/Submission18953/Reviewer_WaKT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18953/Reviewer_WaKT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761853704920, "cdate": 1761853704920, "tmdate": 1762931005815, "mdate": 1762931005815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. The paper proposes SCENECOT, a grounded Chain-of-Thought (CoT) framework for interpretable 3D scene reasoning. It decomposes complex reasoning into four explicit steps—task recognition, region localization, entity grounding, and grounded reasoning—each supported by symbolic and multimodal expert modules.\n\n2. It introduces SCENECOT-185K, a dataset of 185K stepwise reasoning traces covering Situated (MSQA) and Object-Centric (Beacon3D, GQA3D) tasks.\n\n3. Experiments on MSQA and Beacon3D demonstrate improved grounding–QA coherence (34.7% vs. 19.5%) and validated gains from question-type recognition, region filtering, and grounding loss."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel application of CoT in 3D reasoning: Introduces an interpretable step-by-step framework that explicitly grounds each reasoning stage in scene elements. Visualization of reasoning chains (Fig. 6) makes the model’s decision process transparent and easier to diagnose.\n\n2. Large-scale dataset: SCENECOT-185K is the first dataset pairing CoT traces with 3D scene data, supporting both situated and object-centric reasoning.\n\n3. Clear empirical validation: Comprehensive experiments (MSQA + Beacon3D) and ablations demonstrate consistent improvement in 3D VQA task."}, "weaknesses": {"value": "1. Lack of Human Verification. All QA pairs in the SCENECOT-185K dataset are generated by GPT-4o, which can introduce factual or logical errors. The paper does not mention any human validation or quality control process to ensure the correctness of the generated reasoning traces.\n\n2. Limited Baseline Coverage. Although GPT-4o is included as a comparison model, the experiments omit stronger recent multimodal baselines such as Gemini 2.5 Pro or Claude Opus, which would provide a more comprehensive evaluation of reasoning capability."}, "questions": {"value": "1. In the Grounded Reasoning, how is the \"Object Image Tokens\" generated? Are they derived from cropped RGB patches, projected 3D features, or another visual encoding process?\n\n2. What exactly is the symbolic engine mentioned in the architecture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lluy8qz5tt", "forum": "U9meoc0Sau", "replyto": "U9meoc0Sau", "signatures": ["ICLR.cc/2026/Conference/Submission18953/Reviewer_rALF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18953/Reviewer_rALF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016999369, "cdate": 1762016999369, "tmdate": 1762931005234, "mdate": 1762931005234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}