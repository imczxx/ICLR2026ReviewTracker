{"id": "uU96luC4dp", "number": 10701, "cdate": 1758179943793, "mdate": 1763709107781, "content": {"title": "STAR: Similarity-guided Teacher-Assisted Refinement for Super-Tiny Function Calling Models", "abstract": "The proliferation of Large Language Models (LLMs) in function calling is pivotal for creating advanced AI agents, yet their large scale hinders widespread adoption, necessitating transferring their capabilities into smaller ones. However, existing paradigms are often plagued by overfitting, training instability, ineffective binary rewards for multi-solution tasks, and the difficulty of synergizing techniques. We introduce STAR: Similarity-guided Teacher-Assisted Refinement, a novel holistic framework that effectively transfers LLMs' capabilities to super-tiny models. STAR consists of two core technical innovations: (1) Constrained Knowledge Distillation (CKD), a training objective that augments top-k forward KL divergence to suppress confidently incorrect predictions, ensuring training stability while preserving exploration capacity for downstream RL. STAR holistically synergizes these strategies within a cohesive training curriculum, enabling super-tiny models to achieve exceptional performance on complex function calling tasks; (2) Similarity-guided RL (Sim-RL), a RL mechanism that introduces a fine-grained, similarity-based reward. This provides a robust, continuous, and rich signal for better policy optimization by evaluating the similarity between generated outputs and the ground truth. Extensive experiments on challenging and renowned benchmarks demonstrate the effectiveness of our method. Our STAR models establish SOTA in their size classes, significantly outperforming baselines. Remarkably, our 0.6B STAR model achieves the best performance among all open models under 1B, surpassing even several well-known open models at a larger scale. STAR demonstrates a training framework that distills capabilities of LLMs into super-tiny models, paving the way for powerful, accessible, and efficient AI agents.", "tldr": "", "keywords": ["Language Models", "Knowledge Distillation", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7bec739d9b99e68457a1228bde92fc0771521aa5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores training extremely small language models for function calling. The authors introduce the STAR framework, which extends the traditional Knowledge Distillation (KD) + Reinforcement Learning (RL) pipeline with two innovations. First, Constrained Knowledge Distillation (CKD) mitigates key shortcomings of existing KD methods. Second, during the RL phase, SimRL employs a similarity-based reward function aligned with ground-truth outputs. Through evaluations on standard benchmarks, the authors demonstrate that STAR models surpass state-of-the-art KD and RL approaches within the super-tiny model regime."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n\n1. The paper is well written and easy to follow, the experimental results are also well presented.\n\n2. The CKD loss is well motivated, and the gradient analysis is particularly insightful.\n\n3. The final performance of the STAR models are impressive.\n\n4. The paper shows that current SOTA  RL / distillation methods struggle in the super tiny models regime, which is interesting."}, "weaknesses": {"value": "The two main contributions of this paper are 1) the CKD loss function and 2) the sim-rl reward function, and the impact of both has not been well studied.\n\n1. In Table 4, CKD’s performance does not appear significantly stronger than other loss functions. For instance, CKD outperforms AKL by only a small margin (~0.1–0.5) on the BFCLv3 benchmark but performs worse than AKL on ACEBench without RL. Were the improvements of CKD tested for statistical significance?\n\n2. The proposed reward function has not been evaluated against prior approaches—for instance, the similarity-based reward function in ToolRL or other variants based on ASTs and PRMs. As a result, its effectiveness remains unvalidated.\n\n3. There is no ablation on the effect of distilling from a teacher; what if STAR is applied to D directly?\n\n4. (Line 418) “standard metrics are unreliable..” – what are the standard metrics? I also didn’t follow how sim-rl is better suited to handling the stochasticity of the teacher?\n\n5. (Minor) Since CKD is applied first in the pipeline, the paper reads better if it’s introduced first.\n\n6. (Minor) A lot of citations are missing a space after the text, for ex. Line 38: “function calling(Patil et al., 2024; Jin et al., 2025)”"}, "questions": {"value": "1. Is the improvement of CWD on baselines (such as AKL) statistically significant?\n\n2. How does the Sim-RL reward function compare to other reward functions in the literature (ex: ToolRL)?\n\n3. What is the impact of using the teacher’s generations D^T v/s D?\n\n4. In KD, the asymmetry of the divergence seems to be leading to poor performance. What if you replace it with a symmetric divergence? Ex: Jensen Shannon"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XUzH28Y2PZ", "forum": "uU96luC4dp", "replyto": "uU96luC4dp", "signatures": ["ICLR.cc/2026/Conference/Submission10701/Reviewer_Qjs5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10701/Reviewer_Qjs5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558425449, "cdate": 1761558425449, "tmdate": 1762921941966, "mdate": 1762921941966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a training pipeline to adapt tiny-scale language models for function calling. Specifically, the introduced pipeline is composed of a RL method, Sim-RL, with rewards crafted based on the similarity between generated function calls against the ground truth and a knowledge distillation method, Constrained Knowledge Distillation (CKD), that uses forward KL-divergence as the divergence metrics with an additional component to penalize the confident-but-wrong tokens. The combination of these two approaches, STAR, shows the best performance among models under 1B and closes the performance gap with larger models on two benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is relatively well-written and easy to follow.\n- The performance gains on the 0.6B model scale is consistent over most metrics in the two evaluation benchmarks, and the performance gap with larger is significantly reduced.\n- The paper includes discussion on the comparison between KD+RL and SFT+RL besides the empirical results that might be insightful for future work.\n- The paper includes analysis on the comparison among different KD methods."}, "weaknesses": {"value": "- Sim-RL looks highly dependent on the generated function calls’ format that the author defines based on the Qwen tool calling template. It might be important to show the generalizability of this method for alternative formats. \n- STAR requires RL training on the teacher model, which introduces additional non-trivial compute cost compared to SFT+RL.\n- More potential analysis studies could improve the persuasiveness of the paper in showing the advantages of CKD over SFT. For example, a comparison between these methods in a larger model (e.g. 1.7B), an ablation on the teacher model’s size, etc."}, "questions": {"value": "- It seems that the methodology of CKD is not specific to the task of generating function calls. Has the author considered applying this method to other tasks? If not, what constrains CKD to this specific task?\n- The paper lacks some explanations on the categories of the benchmarks shown in Table 1 and 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gB62MtJ8WL", "forum": "uU96luC4dp", "replyto": "uU96luC4dp", "signatures": ["ICLR.cc/2026/Conference/Submission10701/Reviewer_u15k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10701/Reviewer_u15k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031813990, "cdate": 1762031813990, "tmdate": 1762921941442, "mdate": 1762921941442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a framework to effectively imbue tiny-LMs with tool calling capabilities of their larger models. The proposed method consists of two stages: (1) Constrained Knowledge Distillation to prevent highly confident incorrect predictions by the student model, and (2) a similarity-based RL refinement on top of the trained student that computes a fine-grained similarity-based reward between the ground-truth functional calls and the model prediction (more suitable for multi solutions problems). The authors verify their approach by comparing it to several baselines evaluated on BFCL and Acebench (for testing generalization). Results showed improved performance across all benchmarks with the more profound boost on Acebench showing better generalization to unseen function call formats."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is generally well-written and well motivated\n- Experimental results are promising and show strong generalization compared to baselines\n- The method seems to simple and effective at mitigating overfitting problem especially when compared to conventional approach of SFT+RL\n- Results on closing the performance gap with much stronger models in Table 3 is pretty interesting\n- Also appreciate additional theoretical explanation for their Top-K truncation with FK"}, "weaknesses": {"value": "- There’s no direct comparison with existing RL-based methods. It’s not clear how the proposed reward is different from those proposed in related prior works for example one in [1]. In general more comparison with existing RL rewards would be nice. (see more in questions)\n\n[1] Anna Goldie, Azalia Mirhoseini, Hao Zhou, Irene Cai, and Christopher D Manning. Synthetic\ndata generation and multi-step reinforcement learning for reasoning and tool use."}, "questions": {"value": "1- Line 057: typo: the -> the\n\n2- Minor suggestion for structuring Sec 3: chronologically, it would have made sense to start with distillation and then talk about refinement (sim-RL)\n\n3- [line 106] Add citation for RLVR: Lambert, Nathan, et al. 2025. “Tulu 3: Pushing Frontiers in Open Language Model Post-Training.” In Proceedings of the Second Conference on Language Modeling\n\n4- [Line 270]: If I understand correctly, you have two iterations of Sim-RL? you refine both the teacher and the student using Sim-RL.  It would make sense to try to clarify this both in text and figure to avoid confusion.\n\n5- Baslines: which baseline is representing a simple binary RL reward? This is specially important and relevant to you analysis section explaining your reward design"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2sHLYzqN6N", "forum": "uU96luC4dp", "replyto": "uU96luC4dp", "signatures": ["ICLR.cc/2026/Conference/Submission10701/Reviewer_949b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10701/Reviewer_949b"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762209128440, "cdate": 1762209128440, "tmdate": 1762921940963, "mdate": 1762921940963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response to all reviewers"}, "comment": {"value": "Dear Reviewers,\n\nWe sincerely thank you for your detailed, insightful, and constructive feedback. We have now uploaded a revised version of our paper, with the changes highlighted for your convenience. While we have provided responses to each reviewer individually, we wish to highlight the key improvements and additions made in this revision:\n\n- **Deeper Analysis of Constrained Knowledge Distillation (CKD) (Section 4.3, Figure 3, Figure 4):** To better elucidate the unique contributions of CKD, we have significantly expanded our analysis in Section 4.3. We now provide a detailed discussion on CKD's advantages over other distillation strategies, its crucial role in preparing a robust policy for the subsequent Sim-RL phase, and its overall significance within the STAR framework. This analysis is supported by new experimental results, including `Pass@k` scores and policy entropy comparisons, which quantitatively demonstrate how CKD preserves vital exploratory capacity for effective RL refinement.\n- **Ablation Study on Sim-RL Reward Design (Section 4.3, Table 5):** To rigorously validate our novel reward mechanism, we have introduced a new ablation study comparing our Sim-RL reward design against alternative approaches, such as binary rewards and other specialized methods. The results, presented in the new Table 5, and the accompanying analysis in Section 4.3, empirically confirm that our fine-grained, similarity-based reward provides a more effective and robust signal for policy optimization in complex, multi-solution tasks.\n- **Methodological Clarifications and Reorganization (Section 3, Abstract, Introduction):** For improved clarity and logical flow, we have refined the presentation of our methodology. In our revised manuscript, we have reordered the chronological flow of our framework to introduce Constrained Knowledge Distillation (CKD) before Similarity-guided Reinforcement Learning (Sim-RL). We have also clarified in Section 3.2.1 that the use of the Qwen template is an implementation example, underscoring the general applicability of our formatting principles.\n- **Enhanced Clarity on Evaluation (Section 4.1, Appendix A.9):** To improve the clarity and reproducibility of our evaluation, we have added a reference in Section 4.1 directing readers to a detailed breakdown of all evaluation categories for the BFCL and ACEBench benchmarks in Appendix A.9.\n\nThank you again for your valuable input. We believe these revisions have substantially strengthened the paper and addressed the main concerns. We are available for any further questions or discussion."}}, "id": "7WrlYWkMdY", "forum": "uU96luC4dp", "replyto": "uU96luC4dp", "signatures": ["ICLR.cc/2026/Conference/Submission10701/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10701/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10701/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763709560824, "cdate": 1763709560824, "tmdate": 1763709560824, "mdate": 1763709560824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-stage training framework—a knowledge distillation phase followed by a reinforcement learning (RL) phase—to enable a compact 0.6B Qwen model to achieve strong tool-calling capabilities. For each phase, the authors introduce targeted improvements. In the distillation phase, they use a forward KL divergence variant that augments the standard top-k forward KL to suppress confidently incorrect predictions. In the RL phase, they design a heuristic reward function that evaluates the similarity between rollouts and ground truth with more fine-grained criteria, providing richer reward feedback compared to conventional binary rewards. By combining these techniques, the authors demonstrate that their method effectively trains a small model with strong tool-calling performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper provides targeted and practical improvements for enhancing tool-calling capability, offering insights that could be useful for others working in this area. The performance gains are solid and well-demonstrated through experiments"}, "weaknesses": {"value": "* The framework is sound and the empirical results are solid; however, the methodological contribution is not particularly significant."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "usEFckFLk8", "forum": "uU96luC4dp", "replyto": "uU96luC4dp", "signatures": ["ICLR.cc/2026/Conference/Submission10701/Reviewer_ovcd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10701/Reviewer_ovcd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762237602260, "cdate": 1762237602260, "tmdate": 1762921940517, "mdate": 1762921940517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}