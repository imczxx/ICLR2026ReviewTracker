{"id": "tDwJ2gX0Mf", "number": 2031, "cdate": 1756978341720, "mdate": 1763078667694, "content": {"title": "Global-Aware Edge Prioritization for Pose Graph Construction in SfM", "abstract": "The pose graph is an essential component of Structure-from-Motion (SfM) pipelines, where images form the nodes and edges encode relative poses between them. These graphs are typically sparse to reduce the cost of geometric verification required for each candidate edge. In this paper, we focus on robust pose graph initialization, performed at the very beginning of the SfM pipeline. Traditionally, this step relies on image retrieval methods applied independently to each image, connecting it to the $k$ most similar ones according to, e.g., embedding similarity. While effective in practice, this greedy approach does not allow communication across image pairs during graph construction. We address this limitation through the novel concept of edge prioritization, which ranks edges by their utility for SfM. We achieve this through the following two contributions. First, we propose an image representation network combined with a graph neural network (GNN), trained with SfM-derived supervision to predict edge ranks. The GNN exploits global context from the entire image set to guide pair selection. Second, we introduce an edge selection strategy based on minimum spanning trees, which uses predicted ranks to identify the most promising pairs. By integrating global information at both stages, our approach substantially improves SfM reconstruction in the high-speed regime, particularly when operating with very sparse pose graphs. Code will be released.", "tldr": "", "keywords": ["pose graph construction; structure from motion"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/f3ae53e95d02a0dee502b1a1717b5370caef34d1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a GNN-based approach for constructing pose graphs in SfM, replacing the traditional k-NN image retrieval step. The GNN ranks image pairs by their global importance for reconstruction, using geometry-derived supervision. The final pose graph is built from multiple minimum spanning trees. Experiments on MegaDepth, IMC23, and VisymScenes show improved reconstruction accuracy and robustness compared to existing retrieval-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and well-structured, making the technical ideas easy to follow. Introducing a GNN for global reasoning in pose graph construction is an original and conceptually appealing idea. Overall, the work presents an interesting direction with a solid methodological contribution, though the empirical validation is limited."}, "weaknesses": {"value": "There are several aspects that limit the strength of the contribution:\n\n1. The claimed benefit of incorporating global context, particularly for handling duplicates as illustrated in Fig. 1, is not convincingly supported by the quantitative results. The method still relies on external Doppelganger filtering, which suggests that global reasoning alone does not effectively resolve such ambiguities.\n2. The experimental evaluation is relatively limited in scope and size, considering that the main technical novelty is a new heuristic. Including more diverse and large-scale datasets such as MegaScenes (ECCV 2024) would provide stronger evidence of generalization.\n3. Important ablation is missing, the number of GNN message-passing iterations, as well as an analysis of computational complexity and runtime scalability are missing.\n4. The reported performance improvements over baselines like MegaLoc are relatively modest, hence the practical significance of the proposed approach is somewhat limited."}, "questions": {"value": "- Can the authors provide quantitative evidence that the proposed global reasoning helps disambiguate duplicates, beyond the qualitative examples in Fig. 1?\n\n- What is the computational overhead of the GNN-based ranking compared to standard k-NN retrieval, and how does it scale with large image sets?\n\n- Would it make sense to measure the quality of the constructed graphs directly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NAsVwjaDij", "forum": "tDwJ2gX0Mf", "replyto": "tDwJ2gX0Mf", "signatures": ["ICLR.cc/2026/Conference/Submission2031/Reviewer_3Lad"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2031/Reviewer_3Lad"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761584099244, "cdate": 1761584099244, "tmdate": 1762915997920, "mdate": 1762915997920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "tJ5cbx17yk", "forum": "tDwJ2gX0Mf", "replyto": "tDwJ2gX0Mf", "signatures": ["ICLR.cc/2026/Conference/Submission2031/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2031/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763078666905, "cdate": 1763078666905, "tmdate": 1763078666905, "mdate": 1763078666905, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims at enhancing the robustness of SfM methods via ranking edges in view graphs. It proposes to use a GNN to learn globally connected information, where edges are ranked via the trained GNN. During inference, the edge ranks are used to construct k-MSTs, which can be useful in discarding potentially wrong edges. The GNN is trained on the MegaDepth datasets. The method is evaluated on the Visym-Scenes and PhotoTourism datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The task and idea to rank edges in view graph is interesting and novel.\n(2) The method provide a simple yet (seems) promising method to discard potentially wrong matches in view graph, thus can enhance the robustness of SfM methods.\n(3) The paper provides quantitative results on the Visym-Scenes dataset and the PhotoToursim dataset to verify the effectiveness of the method."}, "weaknesses": {"value": "(1) The implementation details of the model is unclear. e.g. the feature dimension for the node features, edge features and global feature, which makes the reimplementation of the method difficult.\n(2) Important baselines are missing in the paper (COLMAP, GLOMAP, etc.).\n(3) While the paper claims it can used for enhancing the robustness of SfM under ambiguous scenes, no qualitative results are provided in the paper.\n(4) Qualitative results are missing in the paper. Moreover, though the camera pose accuracy provides quantitative metrics for reference, ground-truth camera poses for the evaluated datasets could be inaccurate. Another choice is to provide novel view synthesis quality as complementary metrics (as is done in ACE0 [ECCV 2024])."}, "questions": {"value": "1. At line 320, the authors said they compare their method to SOTA global retrieval baselines. But it is confusing to me how these baselines are used in the experiments (e.g. Do they simply used for constructing view graphs?) It must be made clear in the paper and explained in the rebuttal.\n\n2. I would like to know the generalizability of the GNN for ranking edges in view graphs. To me, the training dataset is not large enough, and the validated scenes are quite limited. More results on more diverse scenes should be provided (e.g. 7scenes, tanks-and-temples, etc)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YbEp7xmHVh", "forum": "tDwJ2gX0Mf", "replyto": "tDwJ2gX0Mf", "signatures": ["ICLR.cc/2026/Conference/Submission2031/Reviewer_XCTu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2031/Reviewer_XCTu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761685998955, "cdate": 1761685998955, "tmdate": 1762915997786, "mdate": 1762915997786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a global-aware edge prioritization framework to improve pose graph initialization in Structure-from-Motion (SfM). Traditional methods rely on local, pairwise image retrieval, which often misses critical connections. The authors propose a Graph Neural Network (GNN) to predict edge ranks by leveraging global context from the entire image set, supervised by 3D geometric data. This is combined with a multi-minimum spanning tree (MST) selection strategy to build sparse yet robust pose graphs. Experiments on benchmarks like MegaDepth and IMC23 show significant gains in reconstruction accuracy, especially under sparse graph conditions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear problem reframing & conceptual novelty. Recasting pose-graph initialization as global edge ranking (not independent retrieval) is a crisp, consequential shift. The GNN aggregates set-level context; multi-MST selection enforces global connectivity while promoting structural diversity.\n- Grounded supervision & principled objective. The supervision signal combines RANSAC inliers with common 3D points from an SfM run‚Äîself-supervised, label-free, and task-aligned‚Äîoptimized via a differentiable NDCG surrogate (LambdaLoss/NDCGLoss2++). This is a thoughtful fit to ranking.\n- Strong results in the sparse regime. The method notably improves AUC and registered cameras when k is small (e.g., k=1‚Äì3), precisely where sparse graphs are most fragile and practical speed demands are highest.\n- Challenging disambiguation study. On VisymScenes, the approach complements DG++, increasing correctly localized cameras and demonstrating robustness to doppelgangers, a real failure mode for SfM."}, "weaknesses": {"value": "- Scope of baselines at the graph construction stage. While retrieval backbones (CosPlace, AnyLoc, DINOv2-SALAD, MegaLoc) are compared as sources of scores, it remains unclear how the proposed global ranker stacks up against pairwise overlap predictors / re-rankers explicitly designed to suppress non-matchable pairs before verification (e.g., overlap-based voting or pairwise matchability models). The related-work section cites such directions (e.g., VOP/overlap prediction), but end-to-end comparisons at graph-building time are limited.\n- Dependence on backbones. The best results use MegaLoc (DINOv2 + SALAD) features. Although the method is backbone-agnostic in principle, the practical gains may attenuate under lighter encoders; the ablation with SALAD suggests some degradation. A more systematic latency/accuracy trade-off across encoders would be valuable.\n- High-speed / Sparse Regime. The paper highlights improvements under budget-limited, very sparse pose-graph initialization, instantiated by varying the number of MSTs k. Using k as a proxy conflates sparsity with compute/time budgets and lacks a standardized, method-agnostic definition; as a result, cross-method fairness and external validity remain unclear.\n- Policy & disclosure nit. Appendix notes that ‚ÄúLLMs were used for polishing the text,‚Äù which is fine as disclosure, but ICLR typically expects clarity on what content (if any) was machine-generated and whether any evaluation artifacts were affected (they were not, per the paper). A brief compliance note would remove any ambiguity.\n- Citation standards. To further improve readability, please consider the citation formatting adjustments. (e.g. line 051: ‚Äúbut only pruned Wilson & Snavely‚Äù; line 192: ‚Äúthe approach of (Turkoglu et al., 2021)‚Äù)"}, "questions": {"value": "- k vs. budget guidance. In practice, how should users set the number of MSTs k versus a total edge budget? Could you provide a simple heuristic mapping from desired geometric-verification budget to k (or a stopping rule based on rank margin / connectivity)? \n- Rank calibration & stability. The weight uses ùë§ùëñùëó=1‚àíùëüùëñùëó. How sensitive is MST construction to monotone transforms of ùëüùëñùëó? Have you considered pairwise uncertainty (e.g., predictive variance) to down-weight brittle edges during tree construction? \n- Broader baselines at the construction stage. Could you include overlap-prediction / matchability baselines that re-score candidate pairs prior to RANSAC, run under the same MST framework (i.e., replace ùëüùëñùëó)? This would disentangle benefits from (a) global reasoning and (b) MST selection itself. \n- Runtime composition. The time-accuracy plots are helpful; could you break runtime into (rank prediction + MST building + COLMAP) to quantify how much of the gain comes from fewer wasted verifications vs. cleaner graphs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ybpRlZmsUe", "forum": "tDwJ2gX0Mf", "replyto": "tDwJ2gX0Mf", "signatures": ["ICLR.cc/2026/Conference/Submission2031/Reviewer_XC7n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2031/Reviewer_XC7n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761736128308, "cdate": 1761736128308, "tmdate": 1762915997654, "mdate": 1762915997654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Large-scale SFM retrieves images based on image similarity, computes epipolar geometry among the retrieved images for downstream reconstruction tasks. This paper argues that this traditional method overlook the global cues at the time of retrieval and can lead to loss of information for correct initialisation.\nHence this paper proposes a GNN based edge ranking method on the SfM derived supervision to predict the edge ranks. These edge ranks are used for further downstream objective of SFM to select the correct pairs required for reconstructions. Such pairs are geometrically more relevant for the reconstruction task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a key insight on the initialisation problem of SfM and gives a fresh thought on what lacks in the initial view graph construction.\nThe paper attempts to solve the said problem and shows the improved matching results in complicated pairs of images which consist of large view point changes.\nThe paper isn easy to follow."}, "weaknesses": {"value": "There are number of weaknesses in this paper.\nThe major weakness is the method is not tried on creating large-scale SfM results which was the initial problem from where the sub problem is derived. Hence the utility of this method in the context of SfM is not clear.\nIn line 53‚Äùlimiting the quality of later refinement‚Äù is a statement which is not evaluated by this method. Generally global SfM produces better reconstruction in divide and conquer based SfM methods and the refinement is possible if any images are missed. The paper does not show what is the significance of this problem in the context of SfM and how much this method is contributing to that.\nIf we use SfM supervision then how we can generalise to other dataset? RANSAC also a random process.\n\nWith no results on SfM, the other weaknesses and the paper current condition leads to significant need of improvement before publishing the paper."}, "questions": {"value": "1. It is not clear how fl is trained in equation 1.\n2. Does e_{ij} is fixed over iteration?\n3. Eqn 7,8,9 will produce different values for different runs. How to select which run output to take?\n4. Line 244-245 is not clear"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dpuIU8Ir6H", "forum": "tDwJ2gX0Mf", "replyto": "tDwJ2gX0Mf", "signatures": ["ICLR.cc/2026/Conference/Submission2031/Reviewer_2DZW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2031/Reviewer_2DZW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840711635, "cdate": 1761840711635, "tmdate": 1762915997394, "mdate": 1762915997394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the method of Edge Prioritisation or ranking to initialise the Pose Graphs for the global structure-from-motion pipeline, moving beyond per-image ranking used traditionally. The key idea is to assign a rank to edges or image pairs based on its utility or expected contribution to the SfM process. They use a Graph Neural Network (GNN) to assign the ranks, which is trained on SfM-derived dataset. Th‡¥æ‡¥¨ also uses multiple spanning trees in the initialisation process."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Creating a global rank to prioritise edges (between pairs of images) seems to be a useful, new idea. This eliminates the use of similarity-based selection of images to initialise the pose graph. A representation learned from the training set of MegaDepth is employed using GNNs."}, "weaknesses": {"value": "- The method is not motivated or explained well. What is the intuition behind defining the GNN as they are defined. I don't get the basis of defining the graph structure with $f_l$, $f_{edge}$, $f_{update}$, $f_{MLP}$, etc.? Why those and why not something else?\n- Results do not appear to be significant compared to prior work."}, "questions": {"value": "I have a number of questions/doubts/comments/observations on the work as presented. Each may not require a response from the authors, but represent the basis of my decision/apprehensions.\n- While GNNs are good tools in this setting, can this method be called self-supervised (L235)? Yes, $u_{ij}$ & $v_{ij}$ are available using prior SfM methods on the standard dataset, but then those become the effective \"ground truth\", which will define an upper-bound on performance, won't they?\n- Equations 1, 2, 3, 5, 6 define the components of the Graph Neural Networks used in this work. I certainly want to know the motivation and intuition behind these designs. Neural networks are black-boxes by themselves; the best we can do in practice is to explain the choices made and establish them using appropriate evaluations of the chosen parameters through ablations. This work falls short on that count and appears ad-hoc overall.\n- Cosine similarity is all that is used by traditional methods (L188). In L197, $f_l$ uses the individual embeddings and their cosine as input. How does this add to the traditional method?\n- How did the dimnesonality of 256 come for $e_{ij}$? What is the dimensionality of the embeddings $d_i$?\n- Does $f_{MLP}$ directly predict the edge rank $r_{ij}$ as given in Eq 6, with lower numbers representing better matches?\n- For $r_{ij}$ given in Eq 9, it would seem higher is better (more inliers is good) which will be like an edge-score and not an edge--rank. Is there some confusion here, which may be just notational? Particularly in the light of the para in L240-242.\n- The loss used is not clear to me. What is $\\hat{r}_i$ in Eq 10? Is it computed by sorting r values from Eq 9?\n- IDCG iis computed from ground truth ranking, using  $r_i$ in place of  $\\hat{r}_i$, if I understand correctly. However, $v_i$ and $r_i$ are related. That would make IDCG a constant, won't it? Is that the intention?\n- Eq 11 will suggest higher the better for $r_{ij}$ as MST uses minimum of $w_{ij}$. Is that correct?\n- Use of Multiple MSTs: If I get it right, Xiao (2021) and Gan (2024) also used them. The novelty of this work is in using them with learned signals. Why is this so significant (L293)?\n- Is it true that the multi-MST approach guarantees k disjoint paths? Later MSTs may involve the infinite weight edges given non-dense connectivity, won't they? What is the impact of that?\n- The use of graph clustering is mentioned (L295). What is its impact? Does the procedure need to construe the $n^2$ graph first before partitioning? Do they need all $e_{ij}$ values?\n- Fig 3: Unfortunately, I don't see a clear win for the proposed method over the SOTA. Performance seems adequate at k=3 also. Results here are mixed at best. Most methods are very similar at k=3. Why is k=5 required at all? All seem to plateau there.\n- Regarding COLMAP time: Why is there a big reduction in the proposed method for k=5 from k=3? Explanation around L400 isn't satisfactory.\n- Table 1 results are also mixed, big advantage is only with k=5 and DG++, but not in run time!\n- Fig 4: 1-NN as a baseline for comparison is not acceptable as nobody will use it.\n- I didn't understand the relevance of the Oracle scores in Fig 4 and Tab 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X0bQMr9XX4", "forum": "tDwJ2gX0Mf", "replyto": "tDwJ2gX0Mf", "signatures": ["ICLR.cc/2026/Conference/Submission2031/Reviewer_xzzm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2031/Reviewer_xzzm"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission2031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983421439, "cdate": 1761983421439, "tmdate": 1762915997141, "mdate": 1762915997141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}