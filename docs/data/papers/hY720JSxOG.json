{"id": "hY720JSxOG", "number": 6954, "cdate": 1758003278106, "mdate": 1759897882034, "content": {"title": "Reranker Helps, but Not Enough: Towards Strong Poisoning Attacks Against RAG", "abstract": "Retrieval-Augmented Generation (RAG) augments Large Language Models with timely, external information, making their retrieval corpora a prime target for data poisoning. \nHowever, existing targeted poisoning attacks exhibit limited effectiveness against RAG equipped with a reranker to enhance retrieval quality.\nRemarkably, this defensive benefit comes at no additional cost: a reranker fine-tuned only on benign, in-domain documents can effectively filter malicious content without any adversarial training. \nTo realistically evaluate RAG and strengthen red-teaming efforts, we conclude practical prompt design principles that reveal reranker blind spots.\nBuilding on these insights, we introduce the $\\textbf{P}$rompt-$\\textbf{P}$erturbation $\\textbf{P}$oisoning $\\textbf{A}$ttack ($\\mathbf{P}^3 \\mathbf{A}$), a novel framework for generating sophisticated poisoned documents. \n$\\text{P}^3\\text{A}$ first employs rule-based prompt engineering to craft initial poisoned texts designed to evade reranker filtering.\nIt then injects subtle character-level perturbations into these texts, which promotes their ranking by the reranker while maintaining their adversarial effectiveness. \nThese perturbations introduce only about 1\\% textual change, ensuring the poisoned texts remain natural and readable.\nExtensive experiments demonstrate that our methods achieve effective attack performance, compromising reranker-enhanced RAG pipelines.\nFurthermore, our method exhibits strong transferability, proving equally effective against vanilla RAG—offering a more realistic and challenging benchmark for evaluating defense mechanisms.\nCode is available in the supplementary material.", "tldr": "", "keywords": ["LLM", "RAG", "Data Poisoning Attacks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a2f0d89777dd539ecfad918964f802d0fef4a80.pdf", "supplementary_material": "/attachment/43578ff1de3e1863e274ba5f6db4ae5761d3cd92.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies data poisoning attacks on Retrieval-Augmented Generation (RAG) systems and asks whether a reranker—a common module in realistic pipelines—can mitigate these attacks. The authors first show that adding a reranker indeed reduces the attack success rate of several existing poisoning methods, implying that prior work overestimates the vulnerability of RAG. To expose the remaining weaknesses, they propose a two-stage Prompt-Perturbation Poisoning Attack (P3A) that uses (1) rule-based prompt generation and (2) character-level perturbations guided by reranker gradients.  Experiments on multiple datasets and models demonstrate that P3A restores high attack success even with a reranker in place."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a practically important and underexplored problem: how reranking layers influence RAG’s robustness against poisoning. This perspective is novel and clarifies a gap in the current evaluation practice.  \n\n2. The paper is well written, easy to follow, and presents its experimental evidence clearly with good ablations and visualization."}, "weaknesses": {"value": "1.  While the paper reports that rerankers lower attack success rates, it does not dig into the mechanism behind this effect. Conceptually, a reranker is still a learned retrieval model; why should it resist poisoning while the base retriever fails? A deeper analysis would make the finding more illuminating rather than purely empirical.\n\n2. The P3A method assumes white-box access to the reranker in order to compute gradients and optimize perturbations. In realistic deployment, the attacker rarely knows which reranker a system uses. Therefore, the evaluation mainly reflects an upper bound on attack strength, not a feasible real-world threat. A discussion or experiment on black-box or transfer settings would be necessary to validate practicality.\n3. Overall, I find this work meaningful and well-motivated, and I lean toward accepting it, provided the authors address the issues regarding insufficient explanation of why reranking helps and limited realism of the proposed attack."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uY96TbgNYe", "forum": "hY720JSxOG", "replyto": "hY720JSxOG", "signatures": ["ICLR.cc/2026/Conference/Submission6954/Reviewer_ktoJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6954/Reviewer_ktoJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6954/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761142437560, "cdate": 1761142437560, "tmdate": 1762919181894, "mdate": 1762919181894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows that rerankers inadvertently defend against existing targeted RAG poisoning attacks, and proposes a trageted attack P3A—a two-phase attack combining rule-based prompt engineering with character-level perturbations—that helps the attack survive in presence of rerankers with high attack success rates (>70% ASR)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Authors identify that existing targeted poisoning attacks on RAG are brittle when a re-ranker is present in the RAG system and consequently propose an attack that is effective in the presence of a re-ranker.\n2. Comprehensive Evaluation with comparison against multiple baselines."}, "weaknesses": {"value": "1. Description of the methodology is unclear.\n2. No description on why existing techniques such HotFlip, GCG or other search based techniques can't be used.\n3. Does this attack translate to production RAG systems."}, "questions": {"value": "1. Character Level Perturbation: How are the initial set of n candidates chosen before PGD. Why go through two step process as described. Why can't we use existing techniques such as Gradient based techniques: HotFlip, GCG or Search based techniques eariler used for jailbreaking, to be repurposed in this scenario.  \n\n2. It would be good to test out your attack by connecting the RAG database with your poisoned documents to see if you can get non-zero attack success against production RAG systems to show real world impact."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TE17DiWUHM", "forum": "hY720JSxOG", "replyto": "hY720JSxOG", "signatures": ["ICLR.cc/2026/Conference/Submission6954/Reviewer_m2V5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6954/Reviewer_m2V5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6954/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796495092, "cdate": 1761796495092, "tmdate": 1762919181296, "mdate": 1762919181296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the Prompt-Perturbation Poisoning Attack (P3A) framework, which integrates a rule-based prompt generation phase guided by reranker-oriented prompt engineering with a character-level perturbation phase that refines texts to improve their ranking, thereby enabling effective attacks against RAG systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a new attack method targeting RAG systems.\n\n2. Experimental results demonstrate the effectiveness of the proposed approach."}, "weaknesses": {"value": "1. The threat model is unrealistic.\n\n2. Injecting five poisoned documents into the system is impractical.\n\n3. The paper does not consider recent defense methods."}, "questions": {"value": "1. The proposed P3A attack relies on a white-box assumption where the attacker has full access to the reranker’s parameters and gradients. However, in practical RAG systems, rerankers are usually proprietary or access-restricted. Therefore, this assumption is unrealistic, and the authors should evaluate the attack under a more practical black-box setting.\n\n2. In the experiments, the attack injects five malicious documents per query into the corpus. This setup is impractical because, as shown in [a], the number of truly relevant texts among the top-5 retrieved documents per query is typically fewer than five (e.g., in the NQ dataset). Consequently, injecting five malicious documents means that the number of poisoned texts exceeds the number of relevant ones, trivially increasing the attack success rate. The authors should instead adopt a realistic constraint, allowing only one poisoned document per query.\n\n3. The paper evaluates only basic defenses such as perplexity-based filtering and query paraphrasing, while neglecting more advanced and robust defenses proposed in recent studies [b][c][d].\n\n4. The current experiments are conducted only on small-scale datasets such as NQ, MS-MARCO, and HotpotQA. To validate the generalizability and scalability of the proposed method, the authors should further evaluate its performance on large-scale datasets.\n\n\n[a] Practical Poisoning Attacks against Retrieval-Augmented Generation.\n\n[b] TrustRAG: Enhancing Robustness and Trustworthiness in RAG.\n\n[c] Certifiably robust rag against retrieval corruption.\n\n[d] Who Taught the Lie? Responsibility Attribution for Poisoned Knowledge in Retrieval-Augmented Generation. In IEEE Symposium on Security and Privacy, 2026."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FR8fxW9nhs", "forum": "hY720JSxOG", "replyto": "hY720JSxOG", "signatures": ["ICLR.cc/2026/Conference/Submission6954/Reviewer_6xRo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6954/Reviewer_6xRo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6954/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879373841, "cdate": 1761879373841, "tmdate": 1762919180883, "mdate": 1762919180883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the vulnerability of RAG)systems to data poisoning and shows that while rerankers, offer a surprising “free defense” against existing attacks, they are not sufficient. To expose these weaknesses, the authors propose the Prompt-Perturbation Poisoning Attack (P3A), which first uses prompt engineering to create realistic, authoritative poisoned documents and then applies tiny character-level tweaks (about 1% of the text) to boost their ranking while keeping them natural. Experiments across multiple datasets and models demonstrate that P3A significantly outperforms prior methods, effectively compromising even reranker-enhanced RAG systems and transferring well to vanilla ones. The study concludes that rerankers help but cannot fully defend RAG pipelines, highlighting the urgent need for stronger, more adaptive defenses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Proposes a smart new attack, P3A, that mixes prompts and tiny text tweaks.\n2. Works in both black-box and white-box settings.\n3. Tested on many datasets, rerankers, and LLMs."}, "weaknesses": {"value": "1. A key limitation of P3A is that its full-power version requires white-box access to the reranker, the character-level PGD and position-selection steps depend on seeing reranker scores/gradients, so the fine-grained perturbation phase can’t be executed in a strict black-box setting. The paper does offer a black-box variant (P2A) that relies only on rule-based prompt engineering to produce “reranker-friendly” poisoned texts, and that variant performs well in experiments, but it generally lacks the precision of white-box P3A. In practice an attacker might try to optimize against a publicly available or proxy reranker and hope the poisoned samples transfer to the target system; this proxy to target transfer often works but is an empirical assumption that can fail when architectures, pre-processing, or retrieval configurations differ. \n\n2. The paper provides limited concrete mitigation strategies or operational deployment recommendations; a more thorough discussion of defenses, detection tradeoffs, and ethical considerations would increase practical impact.\n\n3. Experiments focus on three QA datasets and targeted factoid queries; it is unclear how the attack generalizes to other RAG applications (multi-turn dialogue, summarization, multimodal retrieval, or knowledge bases).\n\n4. The paper mostly relies on injecting multiple poisoned docs (they run with 5), so its big wins may overstate real-world risk, flooding a corpus is noisier and easier to spot than a single stealthy page. They show P3A can work with one doc, but I’d like to see more results and discussion about the minimum poisons needed and how detectable bulk injections are."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y6ZmnwXzqv", "forum": "hY720JSxOG", "replyto": "hY720JSxOG", "signatures": ["ICLR.cc/2026/Conference/Submission6954/Reviewer_FgvL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6954/Reviewer_FgvL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6954/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972535902, "cdate": 1761972535902, "tmdate": 1762919180196, "mdate": 1762919180196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}