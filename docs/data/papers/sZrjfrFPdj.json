{"id": "sZrjfrFPdj", "number": 9019, "cdate": 1758107411062, "mdate": 1759897747826, "content": {"title": "DySL-VLA: Efficient Vision-Language-Action Model Inference via Dynamic-Static Layer-Skipping for Robot Manipulation", "abstract": "Vision-Language-Action (VLA) models have shown remarkable success in robotic tasks like manipulation by fusing a language model's reasoning with a vision model's 3D understanding. However, their high computational cost remains a major obstacle for real-world applications that require real-time performance.\nWe observe that the actions within a task have varying levels of importance: critical steps demand high precision, while less important ones can tolerate more variance. Leveraging this insight, we propose DySL-VLA, a novel framework that addresses computational cost by dynamically skipping VLA layers based on each action's importance. DySL-VLA categorizes its layers into two types: informative layers, which are consistently executed, and incremental layers, which can be selectively skipped. To intelligently skip layers without sacrificing accuracy, we invent a prior-post skipping guidance mechanism to determine when to initiate layer-skipping.\nWe also propose a skip-aware two-stage knowledge distillation algorithm to efficiently train a standard VLA into a DySL-VLA. Our comprehensive experiments indicate that DySL-VLA surpasses the state of the art, achieving a 2.1\\% improvement in success length over Deer-VLA (NeurIPS'24) on the Calvin dataset, while simultaneously reducing trainable parameters by a factor of 85.7 and providing a 3.75$\\times$ speedup relative to the RoboFlamingo baseline at iso-accuracy. Our code is available on Anonymous Github.", "tldr": "We propose DySL-VLA, which accelerates vision-language-action models via dynamically applying computation for different action predictions", "keywords": ["Vision-language-action Model", "Layer Skipping", "Robot Manipulation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/020da3ac66aba08fad5413937702ed5518db8431.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces DySL-VLA, a framework that improves the efficiency of VLA model inference by dynamically skipping model layers based on the importance of robot actions. The method distinguishes between static (informative) and dynamic (skippable) layers and leverages trajectory continuity as an indicator of action importance. Through dynamic-static layer skipping, pre- and post-skip guidance, and skip-aware two-stage knowledge distillation, the model achieves acceleration with minimal loss in accuracy. Experiments on the CALVIN and LIBERO benchmarks show that DySL-VLA improves inference speed and even slightly enhances performance over existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper leverages the continuity of the trajectory as an indicator of the importance of current actions, which is intuitive and aligns with robotic motion characteristics.\n2.\tThe proposed dynamic-static layer skipping is a well-motivated and technically sound idea to balance accuracy and efficiency by preserving informative layers while skipping redundant ones."}, "weaknesses": {"value": "1.\tThe evaluation falls short of demonstrating real-world applicability. Given the claim of improved efficiency for robotic deployment, a real-robot experiment on edge hardware (e.g., Jetson Orin) with latency and performance measurements would strongly support the paper’s claims. The current evaluation is limited to simulations and lacks evidence of generalization to diverse real-world scenarios.\n2.\tSeveral method components rely on heuristics that may limit generality. For instance, defining static layers by average cosine similarity or selecting thresholds ($\\eta_1, \\eta_2$) and the number of static layers to keep are not clearly justified or analyzed for transferability across models or tasks.\n3.\tThe definition of “important actions” could be better clarified. While the link between low trajectory continuity and higher action importance is plausible, it is not rigorously demonstrated. Small errors in supposedly “unimportant” actions can still accumulate and degrade performance, so the assumption that such actions are safely skippable requires more discussion.\n4.\tContinuity may serve as a useful heuristic for identifying important actions but may not be sufficient on its own. Other factors, such as task semantics or environmental variability, might also influence action importance.\n\nMinor Points:\n\n1.\tPlease use **\\citep** properly instead of **\\cite** .\n2.\tUse proper quotation marks (**``''** ) in LaTeX."}, "questions": {"value": "1. The paper notes: “However, in the forward path, if we use the controller itself to decide the skipping place, we find that the model will continuously skip at the same dynamic layer.” Could the authors explain why this occurs and why it is undesirable?\n2.\tIn the evaluation, could the authors clarify why performance improves when layers are skipped? Is the improvement due to reduced overfitting or more effective training under the two-stage strategy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sXiF57d2SY", "forum": "sZrjfrFPdj", "replyto": "sZrjfrFPdj", "signatures": ["ICLR.cc/2026/Conference/Submission9019/Reviewer_aNrR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9019/Reviewer_aNrR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622937894, "cdate": 1761622937894, "tmdate": 1762920743702, "mdate": 1762920743702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces three mechanisms to improve inference latency and reduce the number of trainable parameters in vision-language-action (VLA) models. First, model layers are partitioned into static and dynamic groups based on their empirically determined importance. Second, an adaptive algorithm uses the continuity of predicted actions computed from a running average to ensure enough layers are kept during critical moments. Third, a two-stage training procedure stabilizes learning: adapters are first trained to summarize skipped layers, then controllers and adapters are jointly optimized using skip-aware knowledge distillation, enabling efficient layer skipping without degrading task performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written and presents extensive empirical evidence, such as activation similarity and layer significance in VLA settings to support the investigation of adaptive layer skipping.\n- To the best of my knowledge, the introduced algorithms for pre-skip prediction and two-stage knowledge distillation are novel in the context of VLA training. Both mechanisms are clearly motivated, theoretically sound, and directly address the challenges of inference speed, parameter efficiency, and maintaining high success rates.\n- The results on the LIBERO benchmark are strong, with DySL-VLA achieving competitive success rates while reducing the number of trainable parameters by over 30x compared to the state-of-the-art OpenVLA-OFT model.\n- By applying the method to two base models (OpenVLA-OFT and RoboFlamingo) across two different benchmarks (LIBERO and Calvin), the generality of the proposed approach is well supported."}, "weaknesses": {"value": "- The method seems slightly convoluted and introduces several interacting components which come with their own set of hyperparameters respectively (e.g. static layer selection, continuity thresholds, trajectory window for continuity calculation, adapter architecture, controller thresholds, moving stride and number of training steps for each stage). While it was demonstrated to work on two different base VLA architectures and benchmarks, I am doubtful about the ease of adaptability to new tasks or architectures.\n- The experiments on Calvin are only evaluated on the D->D and ABC->D settings, where performance of the base RoboFlamingo model is generally low. It remains unclear if DySL-VLA still performs well in the ABCD->D setting, where RoboFlamingo performs more consistently."}, "questions": {"value": "- Can you elaborate on how static layers are chosen? The process seems data-driven, but I could not find an exact procedure to determine them.\n- Could you comment on the general sensitivity of hyperparameters aside from those already mentioned in the ablation studies?\n- How does the number of trainable parameters compare to e.g. LoRA finetuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1wvIopX4iX", "forum": "sZrjfrFPdj", "replyto": "sZrjfrFPdj", "signatures": ["ICLR.cc/2026/Conference/Submission9019/Reviewer_zimE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9019/Reviewer_zimE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761670461031, "cdate": 1761670461031, "tmdate": 1762920743413, "mdate": 1762920743413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents DySL-VLA, a framework for speeding up VLAs dynamically skipping layers during inference. The method uses a hand-crafted continuity metric to decide when to skip, and lightweight adapters/controllers to maintain compatibility between layers. Experiments on simulation benchmarks show moderate speedups."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **Motivation**: the approach tackles the interesting problem of improving VLA model latency and computational cost, which is relevant for robotics.\n* **Latency improvements**: Provides speedup and some accuracy improvements, with ablation studies and reproducibility details.\n* **Presentation** : the work is clearly presented and motivated. The observations and analysis in Section 3.1 provide useful insights into previous approaches and the proposed method."}, "weaknesses": {"value": "* **Generalizability**: The decision to skip layers is based on a non-learned continuity calculation from action outputs, which may not generalize or be optimal.\n* **Worsens performance?**: while the authors show an improved latency of the model, it looks like the proposed skipping actually damages performance. It is only a 1% reduction on LIBERO, but it is unclear what's the impact on CALVIN, as the authors don't report OpenVLA-OFT number (base VLA model adopted) in the Calvin table."}, "questions": {"value": "* What is OpenVLA-OFT performance on CALVIN?\n* State-of-the-art claims should be removed. OpenVLA-OFT, which the method is based on, is not shown for CALVIN. Furthermore, with the constantly evolving field, the values reported are already surpassed by many other approaches, e.g. SEER [1]\n\n\n[1] Tian et al, Predictive Inverse Dynamics Models Are Scalable Learners for Robotic Manipulation"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cIfKzgN0dq", "forum": "sZrjfrFPdj", "replyto": "sZrjfrFPdj", "signatures": ["ICLR.cc/2026/Conference/Submission9019/Reviewer_uX9j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9019/Reviewer_uX9j"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954118641, "cdate": 1761954118641, "tmdate": 1762920743002, "mdate": 1762920743002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an algorithm to speed up VLA inference by skipping layers in the VLA architecture. The paper posits that some layers in the VLA are more important than the other and should be kept in the architecture at all times whereas other layers may be skipped. The paper also observes that some actions are more “important” than others and the VLA shows high sensitivity to skipping those important actions and therefore presents a “prior-post skipping guidance mechanism” to determine when to skip those layers. To compensate for the skipped layers, the method trains adapters using a 2 staged knowledge distillation algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a simple algorithm for layer skipping based on an observation that some layers in the VLA are much more important than others and some actions during the trajectory are important and require being processed by the full VLA whereas other actions are more amenable to layer skipping. \n2. The method shows good performance on 2 benchmarks on 2 different VLA architectures.\n3. The paper also presents extensive ablations on every part of the method showing individual performance improvements."}, "weaknesses": {"value": "***1. Details on the thresholds***\n\nHow are $\\eta_1$ and $\\eta_2$ computed? If they are hyper-parameters, how do they change between models and simulators / tasks?\n\nIn general, the paper lacks a bit of detail on how do they come up with the hyper-parameters used in the experiments and it would be nice to include that.\n\n***2.Real world experiments?***\n\nThe paper does not show any real-world performance. It would be nice to see how it affects the inference speed in the real-world.\n\n\n***3. Writing.***\n\nThe writing in this paper can be much improved. Overall, the paper reads well and main method is well motivated but some parts of the text are weirdly phrased which hurt the readability. Noting some of those instances below (there might be more):\nA. L147: “we conduct the following observations”? -> maybe rephrase to “we observe”\nB. L150: “which layers we should skip to” -> which layers to skip ?\nC. L198: very weird phrasing. Maybe simplify by just saying dynamically skip unnecessary layers.\nD. L285: “layers are forcibly kept”\nE. most of the quotation marks are wrong."}, "questions": {"value": "Please see the weaknesses section for questions.\nIn general, I like the paper as it starts with an interesting observation and comes up with a simple fix to layer skipping leading to good performance improvements. I’m leaning towards a weak accept rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oQBzrCsyX2", "forum": "sZrjfrFPdj", "replyto": "sZrjfrFPdj", "signatures": ["ICLR.cc/2026/Conference/Submission9019/Reviewer_Q8hS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9019/Reviewer_Q8hS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987086358, "cdate": 1761987086358, "tmdate": 1762920742170, "mdate": 1762920742170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}