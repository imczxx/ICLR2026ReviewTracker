{"id": "cUAhqSUfeK", "number": 13301, "cdate": 1758216207757, "mdate": 1759897446977, "content": {"title": "Progressive Coarse-graining and Deep Neural Networks (DNNs)", "abstract": "We try to provide an overarching perspective on some of the research done in the last few years explaining the behaviour of deep neural networks (DNNs) when they are used to complete a variety of classification and prediction tasks. We start by providing an overview of several noteworthy papers on the fundamental properties of DNNs across different architectures and data regimes. We then forward our own integrated perspective of DNNs as progressive coarse-graining systems inspired by Erik Hoel's Causal Emergence 2.0 framework.", "tldr": "We provide an overview of various theories about deep neural network function, and forward an overarching summary inspired by Erik Hoel's Causal Emergence 2.0 framework.", "keywords": ["Coarse-graining", "deep neural networks", "rank estimation", "linear separability", "information bottleneck", "Bayesian learning", "lottery ticket hypothesis"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4de7c885ec129617e60e39d85be52800511873f0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The manuscript is a overview on the working principles of deep neural networks, with a focus on the networks for classification tasks.  The overview is focused on the hypothesis that DNNs perform a progressive coarse-graining of the data, an hypothesis that is for sure viable and relevant."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The material described in Section 5 is, to the best of my knowledge, novel."}, "weaknesses": {"value": "1. The perspective ignores a relevant part of the literature. For example the section 4.3, on overparametrization, ignores the fact that the phenomenon is now well understood also analytically, with hundreds of papers devoted to a rigorous analysis of the double descent phenomenon (see for example https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/cpa.22008 ).\n2. Also the hypothesis that DNN perform a progressive reduction of the ranks has been severely challenged by the empirical evidence that in convolutional NNs for image classification the intrinsic dimension does decrease in the last layers, but in the first layers it systematically grows: https://papers.nips.cc/paper_files/paper/2019/file/cfcce0621b49c983991ead4c3d4d3b6b-Paper.pdf\n3. I am not sure that conference proceedings are the appropriate venue for reviews and perspectives"}, "questions": {"value": "How can the theoretical derivation presented in Section 5 be applied to the analysis of activation of real networks? What would be the added value with respect to the analysis performed in Shwartz-Ziv & Tishby 2017?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "URCH7GehAo", "forum": "cUAhqSUfeK", "replyto": "cUAhqSUfeK", "signatures": ["ICLR.cc/2026/Conference/Submission13301/Reviewer_rxiF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13301/Reviewer_rxiF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761220159391, "cdate": 1761220159391, "tmdate": 1762923967909, "mdate": 1762923967909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a unifying theoretical perspective, regarding DNNs as a process of \"progressive coarse-graining.\"  The authors synthesize evidence from several lines of recent research, including rank diminution, the information bottleneck, data separation laws, conceptual hierarchies, and the lottery ticket hypothesis. This paper argues that a DNN can be described as a principled simplification process with the goal of procedurally removing irrelevant information from the input, resulting in regular and predictive features that can be used to minimize loss."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "See weakness"}, "weaknesses": {"value": "1. Lack of originality. If I understand correctly, the main core idea of this manuscript, Namely, \"A deep neural network can be described as a principled simplification process with the goal of procedurally removing irrelevant information from the input, resulting in ...” This seems to be widely acknowledged, not surprisingly.\n\n2. The justifications for the claims in this paper are very brief. For example,\n\n-*\"Therefore, we extend the results in Goldfeld et al. (2019) to mean that the description length of the\nrepresentations is also decreasing, leading to a local decrease in complexity. This formalises the\nnotion from Shi et al. (2025) about ”transforming the data to a regular low-dimensional geometry”.\"*\n\n-*\"Finally, the increase in classification accuracy is given by the law of data separation proposed in\nHe & Su (2023) and also by the information bottleneck results.\"*\n\nWithout clearly stating the logic, it is difficult for the reader to follow.\n\n3. This paper is composed of previous works and plain claims. **There is no any theoretical proof and experimental verification**\n\n4. Writing logic needs to be polished. It seems that a large portion of this manuscript was generated with LLM assistance. If not, please correct me."}, "questions": {"value": "1. What‘s is Erik Hoel’s Causal Emergence 2.0 framework. This paper mentions it multiple times but does not explain it at all\n2. Can the author provide any theoretical proof or experimental verification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "88tNpCr1yD", "forum": "cUAhqSUfeK", "replyto": "cUAhqSUfeK", "signatures": ["ICLR.cc/2026/Conference/Submission13301/Reviewer_ALXz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13301/Reviewer_ALXz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761615084758, "cdate": 1761615084758, "tmdate": 1762923967577, "mdate": 1762923967577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper suggests an idea: Deep Neural Networks (DNNs) are a kind of progressive coarse-graining system. The authors believe that DNNs improve their high-level prediction by getting rid of small, low-level details layer by layer."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The author combined the main ideas from previous papers and summarized them in an intuitive way, including Information Bottleneck Theory, Rank Diminishing,    Hierarchies of Features, Over-parameterization and Under-parameterization, Lottery Ticket Hypothesis."}, "weaknesses": {"value": "1. No new ideas: The paper mostly brings together existing research. While the way it connects these ideas is easy to understand, it doesn't offer any truly new concepts.\n2. No proof or experiments: The paper does not provide any mathematical proofs or practical experiments. This makes it hard to know if its ideas are correct."}, "questions": {"value": "Since this paper mostly summarizes existing ideas and doesn’t add new theories, methods, or experiment results, I don’t have specific questions about its content."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Xy4u5Jdhy9", "forum": "cUAhqSUfeK", "replyto": "cUAhqSUfeK", "signatures": ["ICLR.cc/2026/Conference/Submission13301/Reviewer_zhBV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13301/Reviewer_zhBV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922976972, "cdate": 1761922976972, "tmdate": 1762923967234, "mdate": 1762923967234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a conceptual perspective that views deep neural networks as progressive coarse-graining systems.\nThe authors survey a wide range of existing studies on information bottleneck theory, rank reduction, feature hierarchies, Bayesian interpretations, and the lottery ticket hypothesis.\nThey argue that these diverse findings can be unified under a single narrative in which each layer of a network reduces effective rank and mutual information with the input while improving class separability with respect to the output.\nTo formalize this idea, the paper restates known relationships between rank, dimensionality, and mutual information as a set of qualitative inequalities.\nThe work is primarily conceptual and does not include new theoretical derivations, proofs, or empirical results."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper is written clearly and covers relevant literature accurately."}, "weaknesses": {"value": "1. The submission does not contain new theoretical or empirical results. It mainly summarizes and rephrases existing ideas.\n2. The proposed “progressive coarse-graining” perspective remains qualitative and does not lead to any testable prediction or measurable formulation.\n3. Overall, the work reads as a conceptual survey rather than a research contribution."}, "questions": {"value": "1. What specific new understanding or predictive claim is introduced by this framework?\n2. How could the proposed perspective be verified or falsified in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Bgbhdkyzkr", "forum": "cUAhqSUfeK", "replyto": "cUAhqSUfeK", "signatures": ["ICLR.cc/2026/Conference/Submission13301/Reviewer_zCb3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13301/Reviewer_zCb3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998122163, "cdate": 1761998122163, "tmdate": 1762923966746, "mdate": 1762923966746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}