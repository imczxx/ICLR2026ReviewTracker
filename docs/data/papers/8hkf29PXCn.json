{"id": "8hkf29PXCn", "number": 14054, "cdate": 1758227673452, "mdate": 1759897393298, "content": {"title": "Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning", "abstract": "We present EHRMIND, a practical recipe for adapting large language models (LLMs) to complex clinical reasoning tasks using reinforcement learning with verifiable rewards (RLVR). While RLVR has succeeded in mathematics and coding, its application to healthcare contexts presents unique challenges due to the specialized knowledge and reasoning required for Electronic Health Record (EHR) interpretation. Our pilot study on the MEDCALC benchmark reveals two key failure modes: (1) misapplied knowledge, where models possess relevant medical knowledge but apply it incorrectly, and (2) missing knowledge, where models lack essential domain knowledge. To address these cases, EHRMIND applies a two-stage solution: a lightweight supervised fine-tuning (SFT) warm-up that injects missing domain knowledge, stabilizes subsequent training, and encourages structured, interpretable outputs; followed by RLVR, which reinforces outcome correctness and refines the model’s decision-making. We demonstrate the effectiveness of our method across diverse clinical applications, including medical calculations (MEDCALC), patient-trial matching (TREC CLINICAL TRIALS), and disease diagnosis (EHRSHOT). EHRMIND delivers consistent gains in accuracy, interpretability, and cross-task generalization. These findings offer practical guidance for applying RLVR to enhance LLM capabilities in healthcare settings.", "tldr": "", "keywords": ["Large Language Model", "EHR", "Reinforcement Learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2f6865149d0fcddf33f4ed8e34aaaf7e6807fd8a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a recipe, called EHRMind, to adapt a large language model (LLM) to the electronic health record (EHR) domain.  The paper explores multiple alternative solutions and found out the most effective one is a supervised fine-tuning (SFT) followed by a stage of reinforcement learning with verifiable rewards (RLVR).  They also did analysis on when SFT is necessary and why RLVR can boost the model performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The writing of  this paper is excellent, with clear methodology explanation and necessary discussions.\n2. The method proposed in this paper is effective on the tasks of EHR prediction, clinical trial matching, and clinical event prediction. Many of its results on 3B models can exceed the performance of much larger propriatery models.\n3. The discussion of this paper is complete, with thorough experiments on relationship between data characteristics and training objectives. These findings are valuable for similar works in the future."}, "weaknesses": {"value": "1. Lack of methodological novelty: Though promising on the EHR dataset, the recipe adopted by this paper is a subset of the well-known pipeline to train general large language models (e.g. pre-training + supervised fine-tuning + reinforcement learning).\n2. The findings are not fed back to improve the performance of EHRMind: The paper proposes metrics to quantify if a specific task of clinical event prediction, which is valuable. However, it is just an analysis of the model behavior. It would be better if the authors could utilize these findings to improve the model training recipe.\n3. The authors claimed this is a reasoning model, but not sufficient evidence showed that the model benefits from reasoning. A non-reasoning EMRMind baseline could be introduced, where the model is asked to directly answer the question without the thinking tokens."}, "questions": {"value": "1. In section 4.3, you mentioned that Reliable Pass @ 12 is used in replacement of pass @ 12 because that model can get correct answer by guessing.  It is confusing to me: How do you calculate pass @ 12 when asking the model to \"consistently produce correct predictions?\" How many generations are done for each input?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MxM764TIbi", "forum": "8hkf29PXCn", "replyto": "8hkf29PXCn", "signatures": ["ICLR.cc/2026/Conference/Submission14054/Reviewer_tB34"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14054/Reviewer_tB34"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761711863073, "cdate": 1761711863073, "tmdate": 1762924539317, "mdate": 1762924539317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents EHRMIND, a two-stage recipe for adapting small Large Language Models (LLMs) to complex EHR-based reasoning tasks. The authors identify two primary failure modes for models on these tasks: \"misapplied knowledge\" and \"missing knowledge.\" They propose that Reinforcement Learning with Verifiable Rewards (RLVR) is effective for the former, but a \"lightweight\" Supervised Fine-Tuning (SFT) warm-up is required for the latter. The paper's key contribution is the use of Pass@k on the training set as a simple, effective diagnostic to determine which training strategy (pure RLVR or SFT+RLVR) is necessary."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper's primary strength is its practical, diagnostic approach. The framing of the problem as \"missing\" vs. \"misapplied\" knowledge is insightful and clearly demonstrated. The proposal to use Pass@k as a simple, compute-cheap indicator to predict the success of pure RL is a novel and valuable contribution. The strong empirical support for this diagnostic (e.g., $R^2=0.91$ in Fig 2, and validated in Sec 4.2 & 4.4) makes this a very convincing and useful \"recipe.\""}, "weaknesses": {"value": "1. The claims of reasoning over complex, noisy EHRs are undermined by a critical data filtering step detailed in Appendix F.3.2. To fit the context window, the authors discarded all patient data from the EHRSHOT benchmark except for two event types: condition_occurrence and procedure_occurrence. This means all lab results (measurement), medications (drug_exposure), and clinical notes (note) were ignored. The model is reasoning over a tiny, heavily pre-processed fraction of the EHR, not the full, complex record. This is a severe limitation that weakens the paper's central premise.\n\n2. The paper fails to discuss the more subtle and dangerous risk: a rationale that appears plausible, structured, and clinically aligned but is still clinically incorrect and therefore misleading to a doctor or patient. In particular, the model is trained with outcome supervision, without proper process supervision, and the SFT data is from proprietary GPT-4o, the paper does not address my concern of a \"good-looking\" rationale may being misleading or suboptimal.\n\n3. While the approach is well-motivated and self-contained, one concern is that the paper seems to be of interest to a limited audience, and I am not sure whether ICLR is the proper venue for publication. \n\n**Therefore my true rating for this paper is 5, i.e., I am neutral for its acceptance and rejection.**"}, "questions": {"value": "1. Does SFT lead to forgetting of knowledge in other domains? \n\n2. How much does the quality of the teacher's rationale matter for the SFT warm-up to be effective?\n\n3. The filtering of EHRs in the EHRSHOT task to only two event types is a major limitation. How confident are you that the model is learning \"clinical reasoning\" rather than just pattern-matching on these two specific data streams? Could you quantify the performance drop from excluding labs, medications, and notes?\n\n4. Since the model is trained with outcome supervision, without proper process supervision, and the SFT data is from proprietary GPT-4o, how can we trust the rationales given by the model?"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "In a high-stakes clinical setting, a rationale that looks coherent but leads to a suboptimal conclusion is arguably more dangerous than one that is obviously missing (the \"reasoning collapse\" they criticize). \n\nI am not sure whether this is a violation but I choose to flag it."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kSZuBMYyLb", "forum": "8hkf29PXCn", "replyto": "8hkf29PXCn", "signatures": ["ICLR.cc/2026/Conference/Submission14054/Reviewer_AAc4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14054/Reviewer_AAc4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935581505, "cdate": 1761935581505, "tmdate": 1762924538695, "mdate": 1762924538695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an empirical study of EHR-based clinical reasoning where answers are verifiable but require domain knowledge and multi-step reasoning. It observes “misapplied” vs. “missing” knowledge as key failure modes. It applies an existing RL-with-verifiable-rewards setup (GRPO-style) with simple, rule-based clinical rewards, preceded by a lightweight SFT warm-start; Pass@k is used as a heuristic to decide when SFT is needed. On MEDCALC, TREC Clinical Trials, and EHRSHOT, a 3B open model trained with SFT→RLVR shows sizable gains and sometimes surpasses larger proprietary models on verifiable metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The paper presents a Clear, reproducible training recipe (light SFT to RLVR) that practitioners in healthcare could adopt quickly.\n* Empirical breadth. Sensible evaluation across multiple EHR tasks with granular analyses (seen vs. unseen formulas, class-wise metrics, rationale structure)."}, "weaknesses": {"value": "* Limited novelty (major). No new RL algorithm or learning objective; contribution is primarily applying known RLVR with domain-specific, verifiable rewards and providing practical caveat/insights for EHR domain.\n* Comparative fairness. Unclear compute-/sampling-matching against strong proprietary baselines; no comparison on open-source RL baselines. The message delivered seems to be that \"RL finetuning on medical reasoning tasks beats pre-training only closed-sourced big models\" but that is well expected given extensive existing literature[1,2,3]. \nOverall, I believe journals like TMLR would be a better venue for this style of in-depth empirical analysis. \n\n[1] Ouyang, Long, et al. \"Training language models to follow instructions with human feedback.\" Advances in neural information processing systems 35 (2022): 27730-27744.\n\n[2] Chen, Zixiang, et al. \"Self-play fine-tuning converts weak language models to strong language models.\" arXiv preprint arXiv:2401.01335 (2024).\n\n[3] Belcak, Peter, et al. \"Small Language Models are the Future of Agentic AI.\" arXiv preprint arXiv:2506.02153 (2025)."}, "questions": {"value": "* Novelty/positioning. Can you clarify what is methodologically new beyond domain reward design and a training heuristic? Any theoretical or algorithmic advance, besides GRPO adaption to medical reasoning problems?\n* Can you evaluate other RL-trained baselines on more open-source models, to confirm this pipeline is generalizable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iqXYjdquhv", "forum": "8hkf29PXCn", "replyto": "8hkf29PXCn", "signatures": ["ICLR.cc/2026/Conference/Submission14054/Reviewer_NJoZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14054/Reviewer_NJoZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966916744, "cdate": 1761966916744, "tmdate": 1762924538355, "mdate": 1762924538355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EHRMind, a methodology for adapting LLMs to process Electronic Health Records, optimized with reinforcement learning with verifiable rewards (RLVR). Their primary research questions examine the effect of RLVR and SFT on medical reasoning. They found that a small model, specifically LLAMA-3-3B, fails in two cases. In the first case, the model fails to correctly apply medical knowledge despite the fact that it has the correct knowledge within itself. In the second case, the model lacks the related medical knowledge to complete the given task. On the evaluated benchmarks, the proposed approach improves the performance significantly over the zero-shot baseline model, and even commercial models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Strong results. The proposed approach improves performance over baselines including both open-weights and commercial LLMs, with a small 3 billion parameters model.\n- Simple methodology. Overall, the methodology is straightforward and easy to understand/implement, which significantly improves the performance.\n- Detailed analysis on several benchmarks considering different scenarios (e.g., SFT, RLVR, SFT+RLVR) with practical findings."}, "weaknesses": {"value": "- Limited experimentation: Llama-3-8B is used as the initial backbone, yet no other LLMs including same backbone with different scale, or a different LLM with the same scale. Qwen3 models were announced this year May, which should be considered for further evaluating the proposed approach. Although the analyses are detailed, it is necessary to perform these analyses again with different LLMs to make sure that the findings are not specific for Llama-3-8B.\n- The choice of Llama-3-8B needs to be justified. Why is this model chosen? For instance, one could have also experimented with Phi-3.5-mini-instruct, instead this model.\n- Limited novelty: This work applies SFT and RLVR to medical domain, considering EHR-based tasks that require some level of reasoning."}, "questions": {"value": "- Is the used model Llama-3-8B base pretrained autogressive model or instruction-tuned model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H3bc9z7QzR", "forum": "8hkf29PXCn", "replyto": "8hkf29PXCn", "signatures": ["ICLR.cc/2026/Conference/Submission14054/Reviewer_MWTs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14054/Reviewer_MWTs"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015425192, "cdate": 1762015425192, "tmdate": 1762924537986, "mdate": 1762924537986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}