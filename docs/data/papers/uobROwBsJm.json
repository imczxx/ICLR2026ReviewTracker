{"id": "uobROwBsJm", "number": 20592, "cdate": 1758308054759, "mdate": 1759896969030, "content": {"title": "MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use", "abstract": "The MCP standardizes how LLMs interact with external systems, forming the foundation for general agents.\nHowever, existing MCP benchmarks remain narrow in scope: they focus on read-heavy tasks or tasks with limited interaction depth, and fail to capture the complexity and realism of real-world workflows.\nTo address this, we propose \\texttt{MCPMark}, a benchmark designed to evaluate realistic and comprehensive MCP use, comprising $127$ high-quality tasks collaboratively created by human experts and AI agents.\nSpecifically, each task starts from a curated initial state and incldes a programmatic script for automatic verification.\nMoreover, these tasks require richer and more varied interactions with the environment, involving diverse create, read, update, and delete (CRUD) operations. \nWe conduct comprehensive evaluation of cutting-edge LLMs using a minimal agent framework that operates in a tool-calling loop. Empirical results show that the best-performing model, \\texttt{gpt-5-medium}, reaches only $52.56$\\% pass@1 and $33.86$\\% pass^4, while other widely regarded strong models, including \\texttt{claude-sonnet-4} and \\texttt{o3}, fall below $30$\\% pass@1 and $15$\\% pass^4.\nOn average, LLMs require $16.18$ execution turns and $17.38$ tool calls per task, substantially exceeding those in previous MCP benchmarks and demonstrating the stress-testing nature of \\texttt{MCPMark}.", "tldr": "MCPMark is a comprehensive benchmark for stress-testing agents and models in realistic MCP-based scenarios, with 127 tasks across Notion, GitHub, Filesystem, PostgreSQL, and Playwright.", "keywords": ["Large Language Models", "Agent", "Tool Use", "Benchmark", "Model Context Protocol"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a4162e51a758076a0dcaf249ce4c8a362b6588e5.pdf", "supplementary_material": "/attachment/42c4e8b521a0959837eff8f49c569496ff8481ff.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces MCPMark, a benchmark designed to evaluate the realistic and comprehensive use of MCP. It includes 127 tasks across five MCP environments: Notion, GitHub, Filesystem, PostgreSQL, and Playwright. It uses a minimal agent that has a tool-calling/llm-calling loop to evaluate LLMs against these tasks. The authors evaluated popular models and found even the best performing model only achieves 52% at pass@1 and 33% at pass^4."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The proposed benchmark contains realistic and diverse tools and tasks\n- Each task includes automatic verification scripts.\n- It uses a simple but standardized agent framework to just evaluate LLM tool calling capabilities\n- It evaluates many different models and draw some interesting conclusions"}, "weaknesses": {"value": "None. I like the paper quite a lot."}, "questions": {"value": "None. The paper is clearly written."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "POV1knTiGq", "forum": "uobROwBsJm", "replyto": "uobROwBsJm", "signatures": ["ICLR.cc/2026/Conference/Submission20592/Reviewer_NsiE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20592/Reviewer_NsiE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897069040, "cdate": 1761897069040, "tmdate": 1762934001591, "mdate": 1762934001591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new benchmark MCPMark for evaluating more relaistic use of Model Context Protocol (MCP) in context of LLM agents. Model Context protocol (MCP) standarizes how LLMs connect with external systems and tools. However, the paper argues that existing MCP bencmarks are often too narrow or not realistic. The proposed benchmark proposes to stress-test MCP use across 127 tasks in more realistic environments and larger interaction depth. The paper provides some interesting analysis on implicit vs explicit errors for state-of-the-art LLMs on MCPMark. Finally, they also show that best LLMs often require much more interactions (tool calls and execution turns) than those required by previous benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The paper highlights an interesting problem of evaluating realistic use of MCP in context of LLM agents.\n\n* The analysis on implicit vs explicit errors and failure cases in Sec. 4.2 is interesting.\n\n* The paper shows results on state-of-the-art LLMs on MCPMark (Tab. 3), showing the need for better stress testing for MCP use."}, "weaknesses": {"value": "* Can the authors please provide details on the scalability of the current data curation pipeline? \n       *  As mentioned in Sec. 2.1 and L164, the data curation pipeline requires a human expert. Thus, it will be important to also discuss the scalability of the data curation pipeline.\n\n* Also in Sec. 2.1, the authors mention that for each task \"including computer science PhD students, front-end designers, full-stack & AI infra engineers, and AI investorsâ€”each task takes 3~5 hours of focused expert effort\". While not a major concern, providing more details on impact of human expertise on the task quality will be interesting.\n\n* Which scaffold is used for MCPMark-Agent?\n\n* Also can the authors have some intuition for effect of the used agent scaffold on the numbers reported in Tab. 3?\n\n* Finally, in Sec. 4, in the discussion on failure cases it is noted that most errors are implicit. Could this be fixed by better prompting or more precise problem statements or does this point to a more fundamental limitation? If possible it will be interesting to see some qualitative examples of these implicit errors."}, "questions": {"value": "* The authors mention in Sec. 3, that more turns or cost does not equal better performance. This is actually consistent with findings in SWE agents or coding tasks. Have the authors verified if the trend also holds with test-time scaling and parallel rollouts?\n\nPlease see the weaknesses section for some additional questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WBIjL45mye", "forum": "uobROwBsJm", "replyto": "uobROwBsJm", "signatures": ["ICLR.cc/2026/Conference/Submission20592/Reviewer_VEYJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20592/Reviewer_VEYJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947661624, "cdate": 1761947661624, "tmdate": 1762934000827, "mdate": 1762934000827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper adds to the collection of MCP benchmarks. The key distinguishing aspects of this benchmark relative to existing MCP benchmarks are the following:\n- It introduces more complex tasks requiring larger number of turns\n- It covers more operations (read/update/delete) relative to other benchmarks that are only read-heavy.\n\nThe benchmark ensures repeatable and reliable evaluation by setting up resettable initial states and the tasks are co-desgined by a human-LLM agent team, with a significant effort in reviewing and cross-reviewing the tasks to ensure quality control. \n\nThe evaluations illustrate the key challenges of todays LLM-based agents in addressing the tasks. The analysis provide useful categorization of the failure modes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper improves the existing MCP benchmarking space by contributing a more complex collection of tasks with rigorous programmatic evaluation. \n- The benchmarking and analyses provide a useful empirical assessment of the state-of-the-art models (both closed and open) in tackling complex tasks using MCP servers."}, "weaknesses": {"value": "I see two areas that can be improved:\n\n- While I fully appreciate that creating these benchmarks take significant time and effort, the number of instances created overall and the pathway for creating more future instances dont appear to scale. Some justification of why 127 tasks are adequate (in a statistical sense) and representative (I do think has been argued for reasonably in the paper) needs to emphasized more clearly in the paper. \n\n- The choice of a simple agent for comparative model evaluation is a valid argument. However, it would still be useful to know what is the best performance that at least one single model could have achieved given more sophisticated agent formulations would be useful to know. In practice, this is how agents are going to be deployed. In some sense, one would like to what is the best performance that can be achieved on this benchmark with the best known techniques today.\n\n- The arguments for why MCP specific benchmarks are necessary can be stronger. There are multiple multi-tool use benchmarks that exist (e.g. AppWorld, WebArena, and SWEBench). For example, benchmarks like AppWorld provide a rich set of problems defined over multiple applications covering nine different applications and have tasks that are much longer in terms of turns than proposed in this paper. the Why shouldn't we consider MCP wrapping around these multi-tool and multi-turn agentic benchmarks? Why invest effort into creating new benchmarks."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BJzfLltOzz", "forum": "uobROwBsJm", "replyto": "uobROwBsJm", "signatures": ["ICLR.cc/2026/Conference/Submission20592/Reviewer_ZaUC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20592/Reviewer_ZaUC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762124590828, "cdate": 1762124590828, "tmdate": 1762934000193, "mdate": 1762934000193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}