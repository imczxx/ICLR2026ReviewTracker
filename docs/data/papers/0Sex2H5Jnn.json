{"id": "0Sex2H5Jnn", "number": 13435, "cdate": 1758217800099, "mdate": 1763487931685, "content": {"title": "CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density", "abstract": "Current benchmarks for long-context reasoning in Large Language Models (LLMs) often blur critical factors like intrinsic task complexity, distractor interference, and task length. To enable more precise failure analysis, we introduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load Theory (CLT). CogniLoad generates natural-language logic puzzles with independently tunable parameters that reflect CLT's core dimensions: intrinsic difficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\\rho$) regulates extraneous load; and task length ($N$) serves as an operational proxy for conditions demanding germane load. Evaluating 22 SotA reasoning LLMs, CogniLoad reveals distinct performance sensitivities, identifying task length as a dominant constraint and uncovering varied tolerances to intrinsic complexity and U-shaped responses to distractor ratios. By offering systematic, factorial control over these cognitive load dimensions, CogniLoad provides a reproducible, scalable, and diagnostically rich tool for dissecting LLM reasoning limitations and guiding future model development.", "tldr": "CogniLoad offers a novel approach to LLM evaluation through natural language logic puzzles with independently tunable parameters (length, intrinsic difficulty, distractor density) grounded in Cognitive Load Theory", "keywords": ["benchmark", "LLM", "reasoning", "long-context reasoning", "Cognitive Load Theory", "CLT", "synthetic benchmark", "natural language benchmark", "intrinsic difficulty", "extraneous load", "needle-in-a-haystack"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3405b36fcda293169a204d395b4e29d844ea5de0.pdf", "supplementary_material": "/attachment/d897271fd574f85993e85d4d07b6d17e502b4c4f.pdf"}, "replies": [{"content": {"summary": {"value": "The paper introduces a benchmark for long-context reasoning problems based on Cognitive Load Theory, which categorizes cognitive demands into three categories: intrinsic, extrinsic, and germane. Using this framework, they create puzzles in deductive reasoning and state tracking. They describe a generation procedure to change the difficulty of the benchmark by changing each of these dimensions separately. They generate 14,000 (4x5x7x100) puzzles to evaluate 22 LLMs across the dimensions. They fit a GLM and show the performance of the LLMs with respect to variations in each of the three dimensions, with task length still dominating."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Originality:** This paper offers a new lens for dividing up the factors which contribute to LLM performance using cognitive loading.\n\n**Quality:** The results in the paper are sound and thorough. I am particularly pleased that the authors used statistical methods to perform their analysis of benchmark results, and have a large sample size. \n\n**Clarity:** The description of the approach is described in detailed mathematical notation, making it highly precise. \n\n**Significance:** Meaningful dimensions for the analysis of LLM results are important to guide research, and I find comparisons to cognitive science to be particularly interesting, as they draw on well-established concepts (though of course with the caveat that LLMs do not have human minds...). The algorithm for procedurally-generated benchmark items is also useful."}, "weaknesses": {"value": "I find the paper to be well-executed, and have minimal concerns about the quality of the results. The concerns that I do have come from unclear or missing details in the paper, grounding in cognitive science, and from uncertainty about the broader impact of the work. \n\n**Soundness:** As a sense check, I do want to know that the task is possible within the context lengths provided. As with another recent paper (https://arxiv.org/abs/2506.06941) models running over the context window seems like a poor justification for calling a performance a \"reasoning\" failure.\n\n**Clarity:** \n\n1. The mathematical notation of the paper was comprehensive, but not always helpful. I think some pieces were missing, such as the use of k_t and m_t after their initial definition on line 199-200. I would also suggest distinguishing between \"=\" as a logical operator and \"=\" as a definition (probably by using \":=\"). v_{c,t} and u_{c,t} were a bit confusing along this lines, when moving between the definitions and their use validating statements.\n\n2. I am not certain how the 14000 problems were actually finally generated. Was this done with an LLM? Did you create formal logical sets and then translate them into plain language? The latter certainly seems more reliable.\n\n(I will note that although there were references to supplementary material, I only found a zip file with code and data, not further text. In any case, I do think that these details should be in the main body of the paper.)\n\n**Cognitive Science:** I am not a cognitive scientist, but I felt that the grounding in this theory was relatively light. I would appreciate more explanation/justification about why d, N, \\rho are the right way to operationalize these dimensions, beyond what appears in Table 1. In particular, my intuition is that d does not capture \"difficulty\" in the way I would usually conceive of it, since higher values require more working memory, but not more complex deductions. Similarly, I would imagine that instruction following is a subset of ECL, but seems to be described as outside of the three dimensions in the error analysis. One way to make this argument would be by comparison to a human exam that varies along the same cognitive dimensions, to show an analogy between how those variations are made and the variations made in this puzzle.\n\n**Significance:** I am open to being wrong here, but I worry that this paper does not show us things we didn't already know. State tracking is a known weakness of LLMs (https://arxiv.org/abs/2404.08819, https://proceedings.neurips.cc/paper_files/paper/2024/hash/e6231c5f46598cfd09ff1970524e0436-Abstract-Conference.html). It is also a relatively simple task to be testing relative to the more complex reasoning benchmarks described in related works. The variation of performance along context length (most previous works), task size (https://arxiv.org/abs/2506.06941) and number of needles (https://arxiv.org/pdf/2404.06654) have also been shown previously. Being able to track these three dimensions separately is probably new, but I would like to see a clearer discussion of why this matters. \n\nI also find the error analysis to be somewhat unhelpful. Finding that the models often run out of context is described as as failure to use tokens efficiently, but efficient token use is not something that the benchmark describes itself as measuring. Why shouldn't the task be adjusted to avoid these issues in order to ensure a pure assessment of the target reasoning abilities?\n\nIncidentally, I think addressing this can be tied to the previous comment about grounding in cognitive science. To the extent that the dimensions are well-justified, papers like (https://arxiv.org/html/2503.06378v2) have shown useful predictive results from cognitive science dimensions. \n\n**Overall**: I have netted out to a weak accept because I believe the work is methodologically sound, and want to be charitable about the significance, but I am very open to having my opinion swayed, particularly in how to weight the different elements of the papers."}, "questions": {"value": "1. What is the distribution of lengths of the puzzles in tokens? What fraction of the context window is consumed by the inputs? This task seems to require tracking a considerable amount of information. How much context would actually be required to just write out the states after each step?\n\n2. What do the findings of the error analysis mean for people trying to improve LLMs? Would training models via RL to perform state tracking tasks using the context window be a valuable thing to do?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OCGJftslbj", "forum": "0Sex2H5Jnn", "replyto": "0Sex2H5Jnn", "signatures": ["ICLR.cc/2026/Conference/Submission13435/Reviewer_aNzL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13435/Reviewer_aNzL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13435/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643950718, "cdate": 1761643950718, "tmdate": 1762924059886, "mdate": 1762924059886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CogniLoad introduces a synthetic benchmark grounded in Cognitive Load Theory (CLT) to evaluate LLMs' long-context reasoning capabilities. This benchmark independently controls three dimensions: intrinsic difficulty, task length, and distractor density.\n\nKey contributions include: (1) operationalizing CLT for LLM evaluation, (2) revealing task length as the dominant performance constraint, (3) identifying U-shaped sensitivity to distractors, and (4) providing capacity thresholds (e.g., ECL50, ID50) for 22 models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality​**\n\nIntroduces the first benchmark grounded in Cognitive Load Theory (CLT) for LLMs, disentangling intrinsic difficulty (d), distractor density (ρ), and task length (N) to diagnose reasoning failures.\n\n​**Quality​**\n\nRigorous synthetic puzzle generation with independent parameter control, validated through large-scale evaluation and GLM-based capacity thresholds (ECL50, ID50).\n\n​**Clarity​**\n\nWell-structured with explicit CLT-LLM mapping. The operationalization of task length (N) as a proxy for germane load (GCL) could be further justified; visualizations (e.g., U-shaped ρ sensitivity) enhance interpretability.\n\n​**Significance​**\n\nSets a new standard for interpretable LLM evaluation, revealing task length (N) as the dominant bottleneck and providing actionable \"cognitive fingerprints\" for model improvement."}, "weaknesses": {"value": "**Limited Generalizability Beyond Deductive Reasoning**\n\nThe benchmark exclusively tests ​deductive logic puzzles, neglecting inductive/abductive reasoning or real-world tasks (e.g., math, code debugging). This restricts insights into broader reasoning capabilities."}, "questions": {"value": "1. The factorial design isolates each dimension, but are there ​interaction effects​ (e.g., high d+ high ρexacerbating errors)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "744ZzQFDnp", "forum": "0Sex2H5Jnn", "replyto": "0Sex2H5Jnn", "signatures": ["ICLR.cc/2026/Conference/Submission13435/Reviewer_kvXx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13435/Reviewer_kvXx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13435/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756775118, "cdate": 1761756775118, "tmdate": 1762924059369, "mdate": 1762924059369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CogniLoad, a synthetic long‑context reasoning benchmark grounded in Cognitive Load Theory (CLT). It factorizes task stressors into three independently tunable dimensions: (1) intrinsic difficulty d (element interactivity and rule complexity), (2) distractor density ρ (needle‑to‑hay ratio), and (3) task length N (as an operational proxy for sustained “germane‑like” processing). The benchmark generates natural‑language logic‑grid puzzles with sequential state updates, evaluates exact‑match accuracy, and provides an error taxonomy. They have run factorial evaluations for 22 LLMs and fit a per‑model binomial GLM to derive interpretable “capacity thresholds” (ECL50, NT50, ID50). The main empirical findings are that length N is the dominant constraint, that distractors exhibit a characteristic U‑shaped performance curve with worst accuracy at intermediate ρ, and that models differ meaningfully in their sensitivity profiles."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The design isolates three often‑confounded aspects of long‑context reasoning and does so in a way that scales to arbitrarily long sequences. The generator is cleanly specified with validation constraints and produces semantically coherent, non‑trivial distractors. The regression view and derived thresholds offer compact, model‑specific “cognitive fingerprints” that go beyond aggregate accuracy; this is a useful diagnostic interface for both research and engineering. The error taxonomy and supplementary analyses (e.g., AIC comparison for quadratic ρ, confidence intervals) increase interpretability and make the empirical claims credible. The reproducibility is strong: the code repository and dataset release enable immediate verification."}, "weaknesses": {"value": "The CLT analogy is motivating but only partially validated. Using sequence length N as a proxy for germane load conflates at least three mechanisms in LLMs - attention budget, recurrent state tracking over updates, and compounding interference from earlier updates. The paper acknowledges this, but the empirical analysis does not probe which of these mechanisms dominates or whether N’s effect interacts mechanistically with d and ρ in predictable ways.\n\nThe evaluation mixes performance and infrastructure limits. The max‑context failures are treated as an error category, but 32K token limits, tokenization differences, prompt overhead, and vendor defaults create avoidable confounds between “reasoning under load” and “serving limits.” Several models show many “long‑context budget overflow” errors at N=250; this is partly a systems artifact rather than a cognitive limitation. The paper would be stronger if it normalized prompt and decoding budgets across models, used a shared short instruction template, and pre‑tested that all N regimes fit for at least a subset of items per model.\n\nEvaluation relies on an exact match with a final one‑sentence answer. The authors mitigate formatting drift via post‑processing and categorize “last‑logic” failures, but the metric remains coarse. Without step‑wise verification, it is hard to separate state‑tracking mistakes from late‑stage overwrites or local misparses. Given that the generator has full ground‑truth state trajectories, a step‑fidelity metric or a probabilistic grading of intermediate updates would add substantial value and make the “germane‑like” interpretation more concrete."}, "questions": {"value": "1. Please report, in the main paper, at least a compact interaction analysis among d, N, and ρ (one table with key coefficients or ANOVA‑style significance and effect sizes). The appendix suggests clear qualitative patterns; quantifying them would greatly help interpretation.\n2. How sensitive are the main conclusions to the instruction format? If the final answer is allowed to be free‑form (no strict one‑sentence requirement), do “last‑logic” errors diminish while overall accuracy remains similar?\n3. What is the average token length of puzzles at each N and d? Reporting token distributions per condition and per model tokenizer would help separate system limits from reasoning load.\n4. Can you provide size‑matched comparisons with harmonized decoding settings (temperature, top‑p, max tokens) across representative models, to reduce variance induced by vendor defaults?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4REYE7cOEm", "forum": "0Sex2H5Jnn", "replyto": "0Sex2H5Jnn", "signatures": ["ICLR.cc/2026/Conference/Submission13435/Reviewer_xtvk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13435/Reviewer_xtvk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13435/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761761220613, "cdate": 1761761220613, "tmdate": 1762924058852, "mdate": 1762924058852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new benchmark, CogniLoad, that leverages concepts from the Cognitive Load Theory to design a logic grid puzzle reasoning benchmark that systematically evaluates LLMs on their *intrinsic* load (ICL), *extrinsic* load (ECL), and *germane* cognitive load (CGL), with the last one being approximated via task length $N$. The authors evaluate 22 SOTA LLMs and present several trends and insights on the scaling of each of these dimensions of reasoning ability, for different classes of LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* CogniLoad is a systematic benchmark and the first to make proper distinction between the 3 studied kinds of cognitive load LLMs have to deal with when solving hard reasoning problems.\n\n* The work is grounded in a formalism from cognitive science, namely Cognitive Load Theory. Although some aspects (in particular, the connection to germane load) is a bit of a stretch, the notions of intrinsic and extrinsic load align very well with the notions of actual task and distracting information.\n\n* The study is quite detailed, with many LLMs of various kinds evaluated on the proposed benchmark. The authors discuss several trends. While some of these (e.g., increasing the intrinsic difficulty reduces LLM accuracy) are both intuitive and, to some extent, reported before, the present paper does it carefully while keep all (or most) other variables constant. This, thus, provides a cleaner evidence. Additional analyses, like failure modes and differential sensitivity, are useful.\n\n* The paper was easy to read and understand."}, "weaknesses": {"value": "* The paper can use a clearer description of the *germane* load from the CLT theory -- what it means, how it manifests, etc. -- along with some more discussion of why the task length parameter, $N$, is a good proxy for it.\n\n* (stronger concern) The needle-to-hay parameter, $\\rho$, is not as clean cut as intended, and this might even explain the U-shaped performance the authors observe. Specifically, the $\\rho$ parameter mixes two distinct aspects: higher $\\rho$ means more statements about the PoI (person of interest), which has two competing effects: (a) it means fewer distractor statements, which makes the problem easier to solve; and (b) it means a higher number of state changes for PoI, which makes the problem harder to solve. As an example, as one moves from $\\rho = 5\\%$ to $\\rho = 10\\%$, the number of distractor statements drop by (only) $5\\%$, but on the other hand, the number of state transitions for PoI go up by 2x, i.e., double!! The authors don't discuss this important aspect and its implications. Note that this provides a reasonable explanation of the U shaped curve observed -- at very low values of $\\rho$, the number of state transitions dominate the accuracy (the relative change in distractors isn't much), while at very high values of $\\rho$, the relative change in the number of distractors is the one that's significant. I would ask the authors to look more deeply into this and see how it affects the message of and explanations in the paper.\n\n* While it's good to have a formal puzzle definition in section 2, it is also important to verbally convey what each of the constraints is achieving. Can you add some explanatory text?\n\n* For task length $N$, and especially at relatively higher $\\rho$ values (which together means many state transitions for PoI), there is good reason to expect models to struggle without chain-of-thought reasoning, i.e., \"non-reasoning models\", based on theoretical results showing that transformers (without CoT) are inherently parallel (in complexity class $TC^0$ [1,2]) while state-tracking is a sequential task (in general well-known to be complete for the stronger complexity class $NC^1$). It's also known formally that using CoT allows models to perform state tracking and other sequential tasks [3,4]. This might explain some of the observed behavior, e.g., GPT-5 and o3 being resiliant to increase in $N$. Including some discussion of this connection would be valuable.\n\n[1] The Parallelism Tradeoff: Limitations of Log-Precision Transformers, Merrill et al., 2022\n\n[2] Transformers in Uniform TC0, Chiang, 2024\n\n[3] The Expressive Power of Transformers with Chain of Thought, Merrill et al., 2024\n\n[4] Chain of Thought Empowers Transformers to Solve Inherently Serial Problems, Li et al., 2024"}, "questions": {"value": "* Any thoughts on the $\\rho$ parameter not quite being as clean as anticipated, as increasing it makes the problem both easier and harder in different ways? (see note above)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gpZCuQ1qrd", "forum": "0Sex2H5Jnn", "replyto": "0Sex2H5Jnn", "signatures": ["ICLR.cc/2026/Conference/Submission13435/Reviewer_je6v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13435/Reviewer_je6v"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13435/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932650778, "cdate": 1761932650778, "tmdate": 1762924058275, "mdate": 1762924058275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}