{"id": "6ycBM1nsS3", "number": 8388, "cdate": 1758081035966, "mdate": 1759897788301, "content": {"title": "CoDA: From Text-to-Image Diffusion Models to Truly Training-Free Dataset Distillation", "abstract": "Prevailing Dataset Distillation (DD) methods leveraging generative models confront two fundamental limitations. First, despite pioneering the use of diffusion models in DD and delivering impressive performance, the vast majority of approaches paradoxically require a diffusion model pre-trained on the full target dataset, undermining the very purpose of DD and incurring prohibitive training costs. Second, although some methods turn to general text-to-image models without relying on such target-specific training, they suffer from a significant distributional mismatch, as the web-scale priors encapsulated in these foundation models fail to faithfully capture the target-specific semantics, leading to suboptimal performance. To tackle these challenges, we propose Core Distribution Alignment (CoDA), a framework that enables effective DD using only an off-the-shelf text-to-image model. Our key idea is to first identify the ``intrinsic core distribution'' of the target dataset using a robust density-based discovery mechanism. We then steer the generative process to align the generated samples with this core distribution. By doing so, CoDA effectively bridges the gap between general-purpose generative priors and target semantics, yielding highly representative distilled datasets. Extensive experiments suggest that, without relying on a generative model specifically trained on the target dataset, CoDA achieves performance on par with or even superior to previous methods with such reliance across all benchmarks, including ImageNet-1K and its subsets. Notably, it establishes a new state-of-the-art accuracy of 60.4\\% at the 50-images-per-class (IPC) setup on ImageNet-1K.", "tldr": "", "keywords": ["Dataset Distillation", "Text-to-Image Diffusion Model", "Core Distribution Alignment"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/24a0e498481d8eb4a6f36609ee91e305f9cbf957.pdf", "supplementary_material": "/attachment/9b8f4aee2c84a3fe0102585102f2b78e5daf6a34.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a two-stage approach. First, the Distribution Discovery stage employs HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) to identify all valid high-density clusters and subsequently select a set of representative samples. Second, the Distribution Alignment stage then utilizes the guidance of these representative samples to generate synthetic images, aligning the synthetic data distribution with the real data distribution."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors discover that the VAE latent space features exhibit poor separation due to the mixing of inter-class features, which severely degrades inter-class separability and intra-class compactness. To mitigate this issue, they leverage UMAP to preprocess the latent embeddings from the VAE.\n2. The proposed HDBSCAN method, combined with several split strategies, successfully identifies and selects well-distributed, representative samples from the original ImageNet dataset.\n3. The proposed approach demonstrates consistent performance improvements across the ImageNet-1k dataset and its 7 subsets."}, "weaknesses": {"value": "1. Insufficient visual validation of UMAP: While the paper employs UMAP to preprocess VAE latent embeddings, the effectiveness of this dimensionality reduction for improving feature separation is not visually demonstrated.\n2. Ambiguity in distribution alignment and scope violation: The Distribution Alignment stage involves a mechanism that appears to directly compensate for the difference between generated images and real samples. The strong dependence on a lambda hyperparameter suggests this step might be manually compensating for the difference, or potentially regressing to the exact real samples if lambda=1. This approach contradicts the principle of dataset distillation, which requires the synthesis of a small, artificial dataset, as its reliance on near-real samples aligns more closely with coreset selection rather than true distillation, making the paper's claimed scope questionable.\n3. The claimed achievement of a new state-of-the-art accuracy of 60.4% in the 50-images-per-class (IPC) setup on ImageNet-1K in the abstract or contribution section is only a 0.1% improvement over the baseline of 60.3%. This minimal gain is highly likely to be within the margin of experimental randomness."}, "questions": {"value": "1. In Table 5, why does the performance achieved by the \"Ours (R)\" configuration (utilizing real samples selected by HDBSCAN) yield lower accuracy compared to the \"Ours (G)\" configuration (which are synthetic images)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RRhAgIqHl4", "forum": "6ycBM1nsS3", "replyto": "6ycBM1nsS3", "signatures": ["ICLR.cc/2026/Conference/Submission8388/Reviewer_cWUx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8388/Reviewer_cWUx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761333665119, "cdate": 1761333665119, "tmdate": 1762920293515, "mdate": 1762920293515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Core Distribution Alignment (CoDA) — a dataset distillation framework that enables large-scale dataset distillation, relying solely on an off-the-shelf text-to-image diffusion model (SDXL).\n\nCoDA introduces two phases:\n- Distribution Discovery: Identifies the core distribution via UMAP + HDBSCAN clustering in the VAE latent space, selecting representative samples.\n- Distribution Alignment: Guides diffusion sampling using a core-distribution-based energy term, aligning generated images with selected samples.\n\nExperiments on ImageNet-1K and its subsets show CoDA achieves SOTA performance. The method is also computationally efficient and generalizes to new domains like Places365."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Clear formulation**: the paper presents a clean DD framework that combines the clustering-based representative discovery (such as D$^4$M and MGD$^3$) and energy-based diffusion guidance (such as MGD$^3$ and IGD). Even though both components exist in prior works, CoDA articulates them under a consistent probabilistic formulation (Eq. 4–7), which makes it easy to follow and reproduce.\n\n- **Strong empirical performance and generalization**: CoDA achieves SOTA accuracy on multiple benchmarks, and the results generalize across both architectures and datasets. The experiments are extensive and well-controlled.\n\n- **Efficient implementation**: CoDA’s distribution discovery phase runs entirely on CPU using VAE features, and the guidance process uses a well pre-trained T2I SDXL with no further training or SFT."}, "weaknesses": {"value": "- **Overstated “truely training-free DD” claim and misleading title**: The title “From Text-to-Image Diffusion Models to Truly Training-Free Dataset Distillation” is somewhat exaggerated and conceptually inconsistent. The authors argue that prior generative DD methods are paradoxical because they depend on diffusion models pretrained on the target dataset like imagenet-1k, whereas CoDA avoids this by using a general text-to-image diffusion model (SDXL). However, this reasoning is misleading: using a pretrained generative model regardless of its training data is a standard and acceptable practice in DD research, not a paradox. In fact, CoDA still relies on a large diffusion model that was pretrained on an even larger and more expensive dataset (LAION) than ImageNet. Thus, the method is not “training-free” in any absolute sense. It merely shifts reliance from a domain-specific model to a more general pretrained one. The paper’s framing unfairly overemphasizes this distinction with a whole para and risks confusing readers about what “training-free” actually means in the context of DD. I do agree with your distributional mismatch claim, but not the paradoxical pretrained model claim that you described in both abstract and introduction.\n\n- **Incremental method**: The authors a) replace the K-means algorithm with HDBSCAN and design a IPC Matching algorithm to obtain sufficient samples (similar to the prototype conception in D4M). Then, they b) use these samples to guide the denosing process (similar to the guidance mechanism in MGD3). Although the authors claim that the proposed method is distinct from D4M and MGD3,  it is still somewhat incremental based on the prior works."}, "questions": {"value": "- **distribution mismatch**: Figure 1a is confusing. The authors claim that the general T2I LDM “generates samples confined to a limited region of the data distribution,” implying an intrinsic coverage limitation. Which algorithm do you use to generate the visualization? T-SNE or UMAP? Do you have quantitative results to support your observation? Since the pre-trained DMs have better IS-FID score among generative models, they generate high fidelity and diversity samples. I know the dataset domain is different, but another reason for raising such distribuition mismatch may be the hyper parameters you used in your visualization algorithm. Are the results come from the same settings?\n- **Cross-architecture results**: What's the settings of table4? The results are come from a single eval network or the avg results of several eval networks? Are the eval networks be the same?\n- **Lack of experiments**: How about apply coda to the DiT or other DMs pre-trained on the target (such as Imagenet) dataset? Will it augment the SOTA results? It is also important to explore the ultimate capacity of coda.\n- **Lack of datasets**: Since the authors claim that the intrinsic core distribution is important in DD, I think the authors should provide the results on ImageWoof, which is a common dataset used in DD with more inter-class similarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dycKI919fn", "forum": "6ycBM1nsS3", "replyto": "6ycBM1nsS3", "signatures": ["ICLR.cc/2026/Conference/Submission8388/Reviewer_ptsN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8388/Reviewer_ptsN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761394195136, "cdate": 1761394195136, "tmdate": 1762920292960, "mdate": 1762920292960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Core Distribution Alignment (CoDA), a \"training-free\" framework for Dataset Distillation (DD) that solves a key problem: prior methods either paradoxically require models pre-trained on the target dataset or fail due to a \"distributional mismatch\" when using general models. CoDA works in two stages: first, a Distribution Discovery pipeline uses VAE, UMAP, and HDBSCAN to find the \"intrinsic core distribution\" of the target data. Second, a Distribution Alignment stage guides an off-the-shelf text-to-image model (like SDXL) to generate samples matching this core distribution. This approach achieves state-of-the-art results, surpassing even target-trained methods on benchmarks like ImageNet-1K."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.⁠ ⁠The paper tackles a fundamental, conceptual flaw in a popular research area and provides a complete, working solution. This \"truly training-free\" framework is what generative DD should have been from the start.\n\n2.⁠ ⁠The 2-stage design is a key strength. The paper identifies that both discovering the right distribution and aligning to it are essential. The Distribution Discovery pipeline is a major contribution on its own.\n\n3.⁠ ⁠⁠The discovery that the generated set Ours (G) can outperform the real set Ours (R)  is a highly significant finding. It suggests that the generative prior is not just a source of bias to be overcome, but a powerful tool that can be harnessed to \"denoise\" and \"improve\" real-world data for a specific task."}, "weaknesses": {"value": "1.⁠ ⁠The authors disclose that the key hyperparameters for the Distribution Discovery stage (n_neighbors for UMAP and min_cluster_size for HDBSCAN) exhibit \"drift\" across different datasets. While the method remains robustly above the baseline, achieving peak performance requires a new, dataset-specific grid search. This somewhat undermines the \"plug-and-play\" nature of the \"truly training-free\" framework.\n\n2.⁠ ⁠This entire chain is extremely brittle. A small, insignificant change in the initial UMAP embedding could cause a cluster to be \"un-splittable\" by Strategy 1, triggering a completely different logic path (e.g., Strategy 3 vs. Strategy 2), leading to a radically different set of representative samples. The paper does not analyze the stability or variance of the Distribution Discovery stage itself, which is a significant methodological gap given its complexity."}, "questions": {"value": "1.⁠ ⁠The \"intelligent refinement\" shown in Figure 9 is a key finding. But does this always work? You use the polysemous word 'bonnet' as an example of T2I failure in Figure 7. What happens in a high-disagreement scenario where the guiding sample s_j is a hat, but the SDXL prior (activated during PIS) is strongly biased towards cars? Does your guidance successfully correct the model, or does the \"refinement\" step cause the image to revert to a car, making the generated dataset worse than the real one?\n\n2.⁠ ⁠Why did you choose to build this highly complex, heuristic-driven pipeline on a feature space, rather than using a different off-the-shelf feature extractor that is also \"training-free,\" such as a CLIP encoder, which is known to provide a much more semantically separate latent space? Did you experiment with this?\n\n3.⁠ ⁠What analysis did you perform to measure the stability of this Distribution Discovery stage? How do we know this cascade of heuristics reliably converges to a high-quality set of samples, rather than just being a fragile mechanism that happened to work for your tested datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QHOMwNUFi2", "forum": "6ycBM1nsS3", "replyto": "6ycBM1nsS3", "signatures": ["ICLR.cc/2026/Conference/Submission8388/Reviewer_ugRR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8388/Reviewer_ugRR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643317383, "cdate": 1761643317383, "tmdate": 1762920292611, "mdate": 1762920292611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CoDA, a training-free dataset distillation framework that leverages an off-the-shelf text-to-image diffusion model to generate distilled data that is better aligned with the target dataset. The key idea is to first discover an intrinsic, high-density “core” distribution in the latent space of the target data and then guide the diffusion process so that the synthesized samples follow this core distribution. Experiments on ImageNet-1K and its subsets indicate that this distribution-aware generation can match or outperform several existing, diffusion-based dataset distillation methods that require training or fine-tuning on the target data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly identifies a practical gap in current diffusion-based dataset distillation: many approaches rely on a diffusion model trained on the very dataset they aim to compress. CoDA directly addresses this by using only an off-the-shelf model.\n\n2. The two-stage pipeline—(i) discovering a core distribution in latent space and (ii) aligning the diffusion generation to that distribution—matches the problem formulation and makes the contribution transparent.\n\n3. The use of latent embedding, dimensionality reduction, and density-based clustering allows the method to pick representative, high-density samples instead of relying on naïve per-class sampling.\n\n4. The method is tested on multiple ImageNet variants, different IPC regimes, and different evaluation settings, and it consistently shows advantages over other training-free or general-model baselines.\n\n5. The ablation studies demonstrate that most of the gains come from closing the distribution gap rather than from task-specific training, which supports the main claim of the paper."}, "weaknesses": {"value": "1. Although the approach does not train a diffusion model, the discovery stage (encoding, dimensionality reduction, clustering) plus guided sampling still incurs noticeable computational cost. A more explicit comparison of wall-clock time and memory with key baselines would make the “training-free” claim more convincing.\n\n2. The quality of the discovered core distribution may depend on settings for the dimensionality reduction and clustering steps. The paper suggests that different datasets may require slightly different configurations, which could limit robustness.\n\n3. The method is closely tied to the latent geometry of the chosen text-to-image model. It is not yet clear how portable the approach is to other diffusion architectures or to future models with different encoders."}, "questions": {"value": "1. Can you report end-to-end runtime (discovery + generation) per dataset and per IPC, and compare it with both target-trained diffusion distillation methods and recent training-free approaches?\n\n2. How robust is the distribution discovery stage when the off-the-shelf model’s latent space is not well aligned with the target domain (e.g., domain shift, fine-grained categories)?\n\n3. Is it possible to reuse or amortize the discovered core distribution across related datasets or class subsets to reduce the discovery cost?\n\n4. How does the method behave when labels are noisy or incomplete during the discovery stage?\n\n5. For higher-resolution generation, do you require per-class retuning of guidance strength, or is there a single configuration that works across classes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WKwBGWVD44", "forum": "6ycBM1nsS3", "replyto": "6ycBM1nsS3", "signatures": ["ICLR.cc/2026/Conference/Submission8388/Reviewer_NgTi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8388/Reviewer_NgTi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827142784, "cdate": 1761827142784, "tmdate": 1762920292241, "mdate": 1762920292241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}