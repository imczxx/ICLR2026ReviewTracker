{"id": "Zpfet7xJPo", "number": 17738, "cdate": 1758279990897, "mdate": 1759897157260, "content": {"title": "CODA: Learning to Guide Constraint-Aware Optimization for Hardware Accelerators", "abstract": "Designing specialized hardware accelerators is crucial for sustaining the rapid progress of deep learning, yet it remains a costly offline data-driven optimization problem. Evaluating accelerator performance typically requires expensive simulations, while a vast portion of the design space is infeasible due to strict area constraints. Existing data-driven optimization methods often lack feasibility guarantees and suffer from premature convergence on limited offline data. In this paper, we propose the Constrained Offline Design of Accelerators (CODA), a framework for constraint-aware optimization from offline data. CODA tackles data sparsity through a unified surrogate equipped with a cascaded prediction architecture. The model is jointly trained to first predict feasibility and subsequently performance, encouraging the learning of a shared representation that ensures reliable constraint-performance prediction from the limited data. Further, a constraint-aware evolutionary search guided by the surrogate balances exploration and exploitation, accelerating convergence toward feasible high-performance solutions. Extensive experiments on real-world accelerator tasks demonstrate that CODA consistently outperforms both online optimization by 1.11× and surpasses state-of-the-art offline methods by 1.42×. These results highlight CODA as a scalable and robust approach toward automated accelerator design in offline settings. The code is available at https://anonymous.4open.science/r/CODA-38F4/.", "tldr": "", "keywords": ["Hardware Accelerator Design", "Data-Driven Optimization", "Offline Optimization", "Evolutionary Computation"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a98a60b253576cc93e3f4c197736f88c63ee85d0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces CODA, a constrained offline optimization framework for DNN hardware accelerator design. It assumes access to an offline dataset of simulated designs and focuses on minimizing inference latency while satisfying feasibility (primarily area constraints). Latency is evaluated by the analytical model MAESTRO; feasibility is treated as a binary indicator.\n\nThe core is a Constrained‑Performance Cascade (CPC) surrogate: a feasibility classifier (first gate) and a latency predictor (second stage), trained jointly with uncertainty‑weighted multi‑task learning to cope with data sparsity. Search uses a constraint‑aware evolutionary algorithm where predicted infeasible candidates are filtered, but a small fraction can be retained to preserve diversity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Cascading feasibility and performance with uncertainty‑weighted multi‑task learning is an intuitive and sensible adaptation to avoid wasting search on invalid points while still learning a performance‑useful representation from limited data. \n* Combining it with a diversity‑preserving evolutionary loop is practical. Retaining a small fraction of predicted‑infeasible designs is a simple yet effective diversity knob as analyzed in the paper.\n* The empirical gains are notable on the chosen benchmarks."}, "weaknesses": {"value": "* The formulation Eq. (1) assumes a per‑layer pair and evaluates latency/feasibility on that basis. This implies that a one‑layer–to–PE‑resource attribution that does not capture:\n    1. Mapping of multiple layers per PE array, \n    2. Tiling of a single layer across many PEs and time steps, and \n    3. Temporal multiplexing across layers/an operator fusion pipeline. \n* None of this utilization/partitioning complexity is explicit in the representation, yet it critically affects latency and feasibility in real accelerators.\n* The paper explicitly focuses on area as the feasibility condition, noting that infeasibility could also result from memory/power or mapping failures, but these are not modeled in the experiments; NoC connectivity, bandwidth, off‑chip DRAM traffic, on‑chip SRAM bank conflicts, and load balance do not appear in the feasibility labels or the objective. This limits external validity for practical accelerator design. \n* Latency is measured by MAESTRO (which is analytical), not cycle‑accurate simulation or silicon measurements. Conclusions depend on its fidelity for the chosen dataflows. In comparative evaluations, CODA enjoys 6,000 pre‑labeled designs per case offline, whereas online baselines are limited to the same query budget but without that offline corpus. This raises fairness concerns.\n* The zero‑shot transfer is across three canonical dataflows with a hand‑crafted 37‑D context. It is unclear whether this generalizes to novel dataflows (e.g., different unrolling axes, GEMM‑centric systolic arrays, sparsity‑aware engines) or new hardware knobs."}, "questions": {"value": "* On Eq. (1), how does CODA account for (a) multi‑layer sharing of a PE array, (b) spatial/temporal tiling of a single layer over many PEs, and (c) load balance across PEs? If these are only in the dataflow context vector, can you show that the context features uniquely determine the mapping‑relevant degrees of freedom for latency and feasibility?\n* Can feasible(x) be extended to include other constraints such as NoC/SRAM/DRAM bandwidth and port/banking conflicts?\n* Could you test zero‑shot on a new dataflow family (e.g., GEMM‑systolic arrays with array size/tiling knobs), or inject sparsity/activation compression features into the context and measure transfer?\n* Can CODA handle multi‑head prediction (optimize latency, energy, and area jointly) with Pareto‑front search?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UPpY77nu1I", "forum": "Zpfet7xJPo", "replyto": "Zpfet7xJPo", "signatures": ["ICLR.cc/2026/Conference/Submission17738/Reviewer_TbNx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17738/Reviewer_TbNx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761351912499, "cdate": 1761351912499, "tmdate": 1762927573906, "mdate": 1762927573906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CODA, a framework for constraint-aware, surrogate-guided optimization of hardware accelerator designs from offline data. CODA leverages a cascaded surrogate neural architecture (CPC network) to first classify design feasibility (constraint satisfaction) then predict performance, and trains with an uncertainty-weighted dual-task learning approach to combat data sparsity. During search, a constraint-aware evolutionary algorithm exploits the surrogate for population filtering and ranking, balancing retention of feasible and informative infeasible solutions for exploration. Empirical results on several accelerator design cases demonstrate that CODA outperforms both online and leading offline optimization baselines in terms of performance and feasible solution ratio, and generalizes across unseen dataflows in zero-shot settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses an important problem of hardware accelerator design optimization, which has practical significance in computer architecture and hardware design.\n\n2. The experiments are thorough, covering multiple hardware accelerator design tasks (including AlexNet, MnasNet, MobileNetV2, ResNet50, and ShuffleNetV2). And the results demonstrate the effectiveness of CODA."}, "weaknesses": {"value": "1. Extension to multi-constraint scenarios. CODA focuses almost exclusively on area constraints. While this is a practical starting point, most hardware accelerator design problems are simultaneously subject to multiple constraints such as power, memory bandwidth, etc. The extension to multi-constraint scenarios is claimed to be “natural”, but this remains unproven either empirically or algorithmically. Could the authors provide more discussion on the potential issues this may cause and possible solutions to address these issues?\n\n2. Limited impact of some components. Certain parts of the method do not seem to contribute significantly. For example, why does CODA-w/o CE show only marginal improvement (0.0027)? Does this suggest that these components are not essential contributions?\n\n3. Lack of qualitative analysis on failures. While Figures 5–7 show aggregate predictive correlations, the paper does not present qualitative examples (e.g., design configurations that CPC misclassifies or ranks incorrectly) that could help practitioners better understand the strengths and limitations of CPC. Could the authors provide such analyses?\n\n4. The mysterious CPC network architecture. Overall, the design is good. However, could the authors elaborate more in the main text on the architectural design and underlying rationale of the CPC surrogate? For instance, what is the exact form of the input “solution”? How do positional encoding and multi-head attention function work in this context? How is the output of the Validation Encoder connected to the Evaluation Encoder?\n\n5. The related work section does not compare this paper with other studies but merely lists existing works. How do the authors position this work among others?\n\n6. Some typos: line 232 multi-tak. line 239 te."}, "questions": {"value": "1. During the Offline Optimizing process, how much do different search algorithms (e.g., random search with filtering) affect the results? I understand that the baseline compares with online random search, but I think this is not equivalent to replacing the optimizing module in CODA with random search, right?\n\n2. What happens if the final solution is infeasible? Moreover, since CODA only achieves around 60% feasibility, does this lead to significant computational waste?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ktr1EkqQjx", "forum": "Zpfet7xJPo", "replyto": "Zpfet7xJPo", "signatures": ["ICLR.cc/2026/Conference/Submission17738/Reviewer_w4kV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17738/Reviewer_w4kV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905531719, "cdate": 1761905531719, "tmdate": 1762927573387, "mdate": 1762927573387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper discusses an important problem in hardware accelerator design and DSE. It presents a framework for constraint-aware offline optimization of hardware accelerators which can predict both feasibility and performance. The surrogate model introduced in this paper incorporates uncertainty-weighted multitask learning to address data sparsity, and the evolutionary optimizer uses feasibility filtering to balance exploration and exploitation. It has an extensive experimental setup and claims to be performing better than online methods (1.11x) and SOTA offline methods (1.42x)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem discussed in the paper is timely and an important problem, especially in the current era of novel accelerators. And, it follows up with a good motivation with 4 clear reasons.\n2. The surrogate that predicts both the feasibility and performance is an interesting choice and a direction worth exploring in this landscape.\n3. Although there are limitations (explained below) in terms of the experiments, the paper has a sufficiently enough number og baselines to be compared with. Overall, a good experimental setup.\n4. The paper claims to have better performance compared to both online and SOTA offline methods.\n5. The paper is written well and easy to understand."}, "weaknesses": {"value": "1. I think this paper might fall under \"applications of ML\" rather than \"optimizations\".\n2. The model for the accelerators does not seem realistic. I could not find more information about the implementation of these accelerators or whether they are synthesizable, etc. I think this needs to go beyond an analytical model.\n3.  The selection of the dataset sizes and choices is random and not well justified. \n4. Paper introduces the difficulties in collecting data for the accelerators, which is indeed a very critical problem that needs to be addressed. However, it does not discuss about the time it took for their data collection effort and overheads.\n5. It is questionable whether this solution is scalable beyond NVDLA, ShiDianNao and Eyeriss. If we look into the novel accelerator development landscape, the dataflow has become more complex.\n6. It seems this approach relies a lot on the feasibility classifier. That might prevent it from reaching the global optimal."}, "questions": {"value": "1. How are these accelerators generated? What is the end result here? Do we get the HDL implementation of these? Are these accelerators discussed here cycle-accurate? Are these synthesizable?\n2. Can the introduced techniques in the CODA scale be applied in real-world scenarios where high dimensional hardware codesign problems involve memory hierarchy or NoC parameters?\n2. I'm concerned about the size of the training dataset. Does this cover the search space well? The search space can be exponentially large. How did you come up with the number 6000 architectures for training? Can you also provide more context about how long it took to collect data?\n3. How sensitive is this technique when it comes to the architecture of the encoders? Like Transformers and MLPs?\n4. How does CODA handle misclassified feasible points? Could it reject globally optimal designs if the feasibility classifier errors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HTf1RRjpN7", "forum": "Zpfet7xJPo", "replyto": "Zpfet7xJPo", "signatures": ["ICLR.cc/2026/Conference/Submission17738/Reviewer_mCrF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17738/Reviewer_mCrF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979551786, "cdate": 1761979551786, "tmdate": 1762927569167, "mdate": 1762927569167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to design hardware accelerators efficiently while complying with a hardware constraint. \n\nMany prior works for designing hardware accelerators have suffered from a feasibility problem that cannot guarantee the feasibility of searched solutions. It's because feasible solutions are distributed sparsely in the whole search space.\nAlso, it is costly to evaluate architectures because of the simulation costs.It leads to limited offline data, which is a major barrier for offline search. Besides, those huge simulation costs also hinder online search.\n\nTo resolve this problem, the paper proposes a CPC network (Constraint-Performance Cascade network) that can help both evaluate feasibility and predict hardware metrics of architectures. It consists of a feasibility classifier and a performance predictor. The feasibility classifier judges whether candidate architectures are feasible or not. The performance predictor predicts the latency of candidate architectures.\n\nAlso, to train the above framework well, the paper proposes uncertainty-weighted dual-task learning that balances feasibility classification and performance prediction. With the trained CPC network, optimized hardware accelerator architectures that are feasible and comply with the given constraint can be searched by evolutionary search."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper justified itself by pointing out the limitations of prior works for designing hardware accelerators.\n- The paper executed experiments well that could deliver what the paper contends.\n- It is worth mentioning that verifying not only the validity of searched hardware architectures but also the performance of proposed submodules."}, "weaknesses": {"value": "- There seems to be a lack of theoretical support in general.\n- It is inevitable that a model trained with limited offline data suffers from a bias problem. There is insufficient explanation of how this problem can be overcome by the proposals.\n- About uncertainty-weighted dual-task learning, the paper claims that 'the proposed mechanism alleviates data sparsity by extracting complementary information'. However, the support for this claim is not provided well.\n- Typo: In page 5, multi-tak -> multi-task / te -> the"}, "questions": {"value": "- The proposed CPC network cannot help but be trained with limited offline data. When an out of distribution architecture that is infeasible comes as an input, the feasibility classifier may misclassify the architecture in some cases because it hasn't ever seen an architecture like that. Also, it is difficult for the performance predictor to predict the performance of the candidate architecture well, for the same reason. Please give the authors' opinion on whether the above problem can be solved by the proposed method.\n\n- The review think that the explanation about uncertainty-weighted dual-task learning and equation (7) is insufficient. Can the authors provide additional explanation about them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E3WCXhITyr", "forum": "Zpfet7xJPo", "replyto": "Zpfet7xJPo", "signatures": ["ICLR.cc/2026/Conference/Submission17738/Reviewer_yHeB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17738/Reviewer_yHeB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996785536, "cdate": 1761996785536, "tmdate": 1762927568405, "mdate": 1762927568405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}