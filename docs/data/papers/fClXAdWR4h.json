{"id": "fClXAdWR4h", "number": 4428, "cdate": 1757679311495, "mdate": 1759898032974, "content": {"title": "Stochastic Sample Approximations of (Local) Moduli of Continuity", "abstract": "Modulus of local continuity is used to evaluate the robustness of neural networks and fairness of their repeated uses in closed-loop models. Here, we revisit a connection between generalized derivatives and moduli of local continuity, and present a non-uniform stochastic sample approximation for moduli of local continuity. This is of importance in studying robustness of neural networks and fairness of their repeated uses.", "tldr": "We present a non-uniform stochastic sample approximation for moduli of local continuity.", "keywords": ["stochastic sample approximation", "moduli of local continuity"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4830acb6c6ebe7b81ddc717b3dd9f404e94be536.pdf", "supplementary_material": "/attachment/11fd39b2387534c1f6f1dade9a45539c271680f5.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes non-uniform stochastic sampling methods, inspired by UCB bandit policies, to estimate local moduli of continuity (Lipschitz constants) of neural networks. The authors revisit the connections between generalized derivatives and Lipschitz constants in the o-minimal framework, claiming improved sample efficiency and scalability over standard uniform sampling, LipMIP, and LipSDP."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The topic is relevant to robustness and verification. The idea of casting Lipschitz estimation as a pure-exploration bandit problem is natural and can reduce unnecessary sampling in flat regions. The comparison with LipSDP/LipMIP is appreciated, and the experimental section shows improvements in some settings. The theoretical discussion linking Clarke Jacobians and definability is mathematically sound."}, "weaknesses": {"value": "The novelty is limited. The core contribution is an adaptation of standard UCB sampling to the well-known stochastic Lipschitz estimation baseline. The theoretical section is lengthy but does not yield new guarantees of interest to the ML community beyond established facts (consistency, asymptotic unbiasedness, and asymptotic optimality from standard bandit theory). The o-minimal component feels disconnected from the algorithmic contribution and is unlikely to be valued by ICLR readers.\n\nExperiments are insufficient to justify significance. Evaluations are conducted on small MLPs and a binary MNIST setting. There is no evidence that the method scales to modern architectures (CNNs, Transformers, diffusion models) or yields practical benefit in certified training or downstream robustness tasks. Reported gains, while present, remain modest. The computational overhead of the adaptive strategy is not fully analyzed.\n\nThe paper lacks a compelling narrative demonstrating why this improvement matters for robustness, certified training, or verification workflows in practice. As written, it reads as an incremental technical refinement rather than a contribution that changes understanding or capabilities."}, "questions": {"value": "- Can you clearly highlight the theoretical contribution that is unique to this work?\n- Does the method scale to larger architectures (e.g., Transformers, ViTs, LLMs)?  \n- Are there new bottlenecks when moving to large-scale models?\n- Can we show measurable improvements in robustness-aware training or certified robustness?  \n- Does the approach improve performance on standard certification benchmarks? \n- What is the computational complexity relative to baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wNaccqddwT", "forum": "fClXAdWR4h", "replyto": "fClXAdWR4h", "signatures": ["ICLR.cc/2026/Conference/Submission4428/Reviewer_fW2q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4428/Reviewer_fW2q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811431310, "cdate": 1761811431310, "tmdate": 1762917358629, "mdate": 1762917358629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors investigate the connection between generalized derivatives and local continuity, and further propose an algorithm for estimating the modulus of local continuity. Building on existing methods for evaluating local Lipschitz continuity, they extend the formulation to more general notions of local continuity. The authors also partition the input space of dimension \n$ d $ into $k^{d} subregions to support structured sampling. Finally, they incorporate a non-uniform UCB-based sampling strategy that concentrates samples in regions with higher estimated importance. The paper provides both theoretical analysis and empirical results demonstrating that the proposed approach yields more consistent and accurate estimates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1: The authors propose a non-uniform sampling strategy based on UCB scores. This approach allows the sampling process to better adapt to different regions of the input space compared to uniform sampling.\n\n2: The authors provide solid theoretical analysis demonstrating that their algorithm is more dynamic and consistent."}, "weaknesses": {"value": "1: The proposed non-uniform sampling method relies heavily on hyperparameters such as the exploration coefficient \"c\" and the subdivision threshold. A detailed discussion or sensitivity analysis of these parameters would be important to assess the robustness and practical usability of the method.\n\n2: For high-dimensional input spaces, partitioning the domain into more subregions may introduce significant computational overhead. The paper does not address the scalability implications of this partitioning strategy in high-dimensional settings.\n\n3:Although the paper focuses primarily on theoretical contributions, evaluating the method on larger datasets or across different modalities, such as natural language processing or time-series data, would strengthen the empirical evidence and demonstrate broader applicability."}, "questions": {"value": "Based on the identified weaknesses, I would appreciate clarification from the authors on the following points:\n1: Could the authors discuss how the key hyperparameters (e.g., exploration constant c and subdivision threshold) affect the performance of the proposed method? Providing sensitivity analysis or practical guidance on selecting these values would help assess the robustness of the approach.\n\n2: Could the authors analyze or comment on the computational efficiency of the method, particularly in high-dimensional settings?\n\n3: While the focus is theoretical, demonstrating the method on more diverse datasets or tasks (e.g., larger vision benchmarks, NLP models, or time-series data) would strengthen the empirical validation. Can the authors provide results on additional datasets or comment on expected performance in such settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FIoLrXCZKx", "forum": "fClXAdWR4h", "replyto": "fClXAdWR4h", "signatures": ["ICLR.cc/2026/Conference/Submission4428/Reviewer_Z5Co"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4428/Reviewer_Z5Co"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946132828, "cdate": 1761946132828, "tmdate": 1762917358243, "mdate": 1762917358243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel method for estimating the Lipschitz constant in neural networks, addressing the known scaling limitations of methods like LipMIP and LipSDP. The core contribution is Algorithm 3 in the paper, which frames the problem in an infinity-armed bandit setting without commonly considered assumptions, using an Upper-Confidence-Bound (UCB) policy to partition the input domain and concentrate samples in useful regions. The paper compares with LipSDP and LipMDP on RELU-activation based neural networks trained on synthetic data and on binary MNIST."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is written very well. The results are also clearly presented.\n\nThe results on binary MNIST clearly depict the improvements over LipSDP and LipMDP.\n\nI am glad that the paper does not limit itself to simple two layer neural networks and instead, in Figure 5, compares with the aforementioned methods across a number of hidden layers."}, "weaknesses": {"value": "A potential related work missing: In [1], a extreme value analysis method is used to compute Lipschitz constants and could be relevant to this paper.\n\n[1] Improving Neural Network Robustness via Persistency of Excitation, Sridhar et al, ACC 22\n\nScale: I am uncertain how easily this method can scale up to neural networks in use today from imagent scale models to LLMs, VLMs, and VLAs. Perhaps the authors can address the question of further scale?"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pjwEGPwQs1", "forum": "fClXAdWR4h", "replyto": "fClXAdWR4h", "signatures": ["ICLR.cc/2026/Conference/Submission4428/Reviewer_pafa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4428/Reviewer_pafa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762184176325, "cdate": 1762184176325, "tmdate": 1762917358003, "mdate": 1762917358003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}