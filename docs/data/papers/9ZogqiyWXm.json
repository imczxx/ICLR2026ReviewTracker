{"id": "9ZogqiyWXm", "number": 5904, "cdate": 1757944851142, "mdate": 1759897945992, "content": {"title": "Token-based Audio Inpainting via Discrete Diffusion", "abstract": "Audio inpainting seeks to restore missing segments in degraded recordings. Previous diffusion-based methods exhibit impaired performance when the missing region is large. We introduce the first approach that applies discrete diffusion over tokenized music representations from a pre-trained audio tokenizer, enabling stable and semantically coherent restoration of long gaps. Our method further incorporates two training approaches: a derivative-based regularization loss that enforces smooth temporal dynamics, and a span-based absorbing transition that provides structured corruption during diffusion. Experiments on the MusicNet and MAESTRO datasets with gaps up to 750 ms show that our approach consistently outperforms strong baselines across range of gap lengths, for gaps of 150 ms and above. This work advances musical audio restoration and introduces new directions for discrete diffusion model training.", "tldr": "", "keywords": ["Audio inpainting", "Discrete diffusion models", "Transformer-based diffusion", "Audio tokenization", "Generative modeling", "Music restoration"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/75594bbd553fb2b87a5e8aca0947effe81fd8e2c.pdf", "supplementary_material": "/attachment/f271ecbded3e183d92cd4bacd67ac5f474f6a82c.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a novel approach for restoring missing audio segments, using tokenized audio representations—specifically, pretrained WavTokenizer—and discrete diffusion modeling to achieve more effective inpainting of longer gaps. Training involves span-based masking as a structured corruption strategy and incorporates a derivative-regularized reconstruction loss.\n\nExperiments conducted on two music datasets demonstrate that the method outperforms baseline approaches across three objective metrics, especially when filling gaps exceeding 200ms."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Well-designed system capable of handling inpainting gaps up to approximately 500ms."}, "weaknesses": {"value": "The quality is heavily depends on the tokenizer or codec used.\n\nThe method lacks evaluation outside the music domain and does not consider additional conditions for music restoration.\n\nNo subjective measurements are provided, and demo samples show noticeable boundary artifacts."}, "questions": {"value": "Audio inpainting can leverage diffusion models either on continuous latent spaces or discrete tokens. It would be beneficial to directly compare these two strategies—using VAE for continuous representation and a neural codec for discrete tokens—at the same frame rate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qlARDb887S", "forum": "9ZogqiyWXm", "replyto": "9ZogqiyWXm", "signatures": ["ICLR.cc/2026/Conference/Submission5904/Reviewer_1vML"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5904/Reviewer_1vML"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761187493773, "cdate": 1761187493773, "tmdate": 1762918338595, "mdate": 1762918338595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Audio Inpainting via Discrete Diffusion (AIDD), a novel method for restoring missing segments in audio, particularly long gaps. The core contribution is being the first to apply discrete diffusion for audio inpainting, which enables more stable and semantically coherent generation compared to previous continuous-domain methods. The paper also proposes two new training techniques: 1) a span-based masking strategy for structured corruption and 2) a derivative-based regularization loss to ensure temporal smoothness. Experiments on the MusicNet and MAESTRO datasets demonstrate that AIDD outperforms strong baselines for gaps of 150 ms and longer, significantly advancing the state of the art in musical audio restoration."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper is the first to apply the discrete diffusion on tokenized representations for audio inpainting. \n\n2.The method achieves state-of-the-art results on the long-gap audio inpainting task.\n\n3.The code will be open-sourced."}, "weaknesses": {"value": "1.The evaluation lacks a subjective listening study, which is essential to validate the perceptual quality and musical plausibility of the results.\n\n2.The paper should quantify information loss from tokenization by reporting metrics on both the original audio (as a reference ceiling) and the reconstructed audio (audio passed through the tokenizer's encoder-decoder). This would clarify the tokenizer's impact and establish the method's practical upper bound.\n\n3.The audio sampling rates are not reported. It is unclear if the source audio is downsampled to match the WavTokenizer's 24 kHz reconstruction bandwidth, which would be a critical confounding factor affecting task difficulty.\n\n4.There is a potential training-inference mismatch. During training, the tokenizer processes the complete audio signal before tokens are masked (\"tokenize-then-mask\"). At inference, it processes a signal that already contains gaps (\"mask-then-tokenize\"). It is unclear if the long gap introduced at inference interferes with the tokenization of other regions. This discrepancy should be discussed.\n\n5.Key inference hyperparameters (e.g., diffusion steps, temperature, top-k) are missing, which hinders reproducibility. A latency analysis would also be beneficial to assess the method's practical usability."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Yg5xuoMOQQ", "forum": "9ZogqiyWXm", "replyto": "9ZogqiyWXm", "signatures": ["ICLR.cc/2026/Conference/Submission5904/Reviewer_jyTi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5904/Reviewer_jyTi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761421600715, "cdate": 1761421600715, "tmdate": 1762918337550, "mdate": 1762918337550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AIDD (Audio Inpainting via Discrete Diffusion), a model that performs audio inpainting directly in the discrete token domain instead of waveform or spectrogram space. Audio is first tokenized using a pretrained WavTokenizer, and a discrete diffusion model learns to predict masked token spans. The method introduces two main ideas: (1) Derivative-based regularization to ensure smooth temporal transitions between predicted tokens. (2) Span-based masking that masks contiguous token spans following a diffusion noise schedule, aligning the corruption process with the inpainting objective. AIDD is evaluated on MusicNet and MAESTRO, showing improved perceptual quality (FAD, ODG, LSD) on medium-to-long gaps (200–750 ms) compared to prior methods like CQT-Diff+, GACELA, and bin2bin, while being smaller and faster to train. The work contributes an efficient and conceptually clear token-level diffusion approach for long-gap audio restoration."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper presents a token-based diffusion model for audio inpainting (AIDD) that operates directly in the discrete token space rather than waveform or spectrogram domains. The idea is original in its formulation and addresses a practical limitation of prior work—difficulty maintaining long-range temporal and semantic consistency when filling large gaps. The proposed span-based masking and derivative regularization are intuitive yet effective design choices that align well with the inpainting objective. Experiments on MusicNet and MAESTRO demonstrate gains on medium- and long-gap scenarios, especially under limited computational resources. The method’s simplicity and the decision to train on single-GPU hardware make it appealing for future research and reproduction."}, "weaknesses": {"value": "(1) Codec choice not sufficiently justified.\n- The method relies entirely on WavTokenizer, but there are other single-codebook codecs such as UniCodec [1] that could equally serve this purpose. The paper does not explain why WavTokenizer was chosen or whether the improvements are specific to that tokenizer. A small ablation with an alternative codec would help isolate the contribution of the proposed diffusion mechanism.\n\n(2) No human evaluation.\n- The paper claims that AIDD produces perceptually natural and semantically coherent audio, yet only objective metrics (FAD, ODG, LSD) are reported. For a perceptual task like inpainting, even a small-scale human listening test would strengthen the claim substantially.\n\n(3) Fairness and completeness of baselines.\n- The paper compares AIDD against CQT-Diff+, GACELA, and bin2bin, but the comparison is not entirely fair or complete:\n- Different training steps and data splits: AIDD was trained for 100 k steps on MusicNet, while CQT-Diff+ used 400 k in its original setup. On MAESTRO, AIDD trained on a private subset (not released), whereas baselines used the full dataset. This makes direct comparison difficult.\n- Different modeling domains: AIDD operates in the token domain, while CQT-Diff+ and GACELA work in the spectrogram or waveform domain. Since metrics like FAD depend on the reconstructed waveform and decoder quality, comparing across such domains may not reflect pure modeling differences.\n\n(4) Limited scope of evaluation.\nThe model is only tested on central silent gaps. More realistic cases—multiple gaps, noisy or partially masked regions—are not examined, leaving generalization unexplored.\n\n[1] Jiang, Yidi, et al. \"UniCodec: Unified Audio Codec with Single Domain-Adaptive Codebook.\" arXiv preprint arXiv:2502.20067 (2025)."}, "questions": {"value": "(1) Why was WavTokenizer selected over newer or higher-quality single-codebook codecs like UniCodec?\n\n(2) Have you tested whether your derivative regularization still helps when using a different tokenizer?\n\n(3) Could you include a small human listening study (e.g., MOS or AB preference) to verify that objective gains correlate with perceived quality?\n\n(4) How stable are your FAD results given the relatively small sample size (~600 clips)? Any confidence intervals or bootstrap analysis?\n\n(5) Can your span-masking strategy handle multiple or non-silent gaps, or does it assume fully silent regions only?\n\n(6) You mention AIDD is smaller and faster than CQT-Diff+. Could you report the parameter count and inference speed to quantify this claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "This paper presents AIDD (Audio Inpainting via Discrete Diffusion), a model that performs audio inpainting directly in the discrete token domain instead of waveform or spectrogram space. Audio is first tokenized using a pretrained WavTokenizer, and a discrete diffusion model learns to predict masked token spans. The method introduces two main ideas: (1) Derivative-based regularization to ensure smooth temporal transitions between predicted tokens. (2) Span-based masking that masks contiguous token spans following a diffusion noise schedule, aligning the corruption process with the inpainting objective. AIDD is evaluated on MusicNet and MAESTRO, showing improved perceptual quality (FAD, ODG, LSD) on medium-to-long gaps (200–750 ms) compared to prior methods like CQT-Diff+, GACELA, and bin2bin, while being smaller and faster to train. The work contributes an efficient and conceptually clear token-level diffusion approach for long-gap audio restoration."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "224670yrWt", "forum": "9ZogqiyWXm", "replyto": "9ZogqiyWXm", "signatures": ["ICLR.cc/2026/Conference/Submission5904/Reviewer_G7ho"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5904/Reviewer_G7ho"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761784282581, "cdate": 1761784282581, "tmdate": 1762918337218, "mdate": 1762918337218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}