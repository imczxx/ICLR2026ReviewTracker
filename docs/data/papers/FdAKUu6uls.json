{"id": "FdAKUu6uls", "number": 9607, "cdate": 1758129840529, "mdate": 1759897709380, "content": {"title": "Efficient Dynamic Structured Sparse Training with Learned Shuffles", "abstract": "Structured sparsity accelerates training and inference on modern GPUs, yet it still trails unstructured dynamic sparse training (DST) in accuracy.  The shortfall stems from a loss of \\emph{expressivity}: whereas a dense layer can realise every possible mask obtained by choosing any $w$ active weights out of $n$, a fixed block, or $N{:}M$ layout explores only a subset of those possibilities. We propose to close this gap by learning, for each layer, a single permutation matrix jointly with the structured weight matrix. Applied to three canonical structures—block, $N{:}M$ and diagonals—we show that permutation-augmented DST (PA-DST) matches unstructured baselines (RigL, SET) at 90–95\\% sparsity on ImageNet-1K (ViT-B/16) and WikiText-103 (GPT-2), yet trains upto 1.21$\\times$ and infers up to $2.9\\times$ faster. The results position \\emph{structure + learned permutation} as a sweet-spot between accuracy and efficiency.", "tldr": "Training structure sparse neural networks with permutations to bridge the gap with unstructured sparsity.", "keywords": ["Foundation models; Structured Sparsity; Sparse Neural Networks; Dynamic Sparse Training"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0f903c4cf088e10b0503a85b88c42677678a6e98.pdf", "supplementary_material": "/attachment/c6e37f68734f1e064d9bc988b4bf03b2c714a971.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Permutation-Augmented Dynamic Sparse Training (PA-DST), which augments structured dynamic sparse training with per-layer learned permutations to recover expressivity while preserving structured kernels at inference via index reordering. The authors provide extensive analysis and experiments, showing that the proposed PA-DST approaches unstructured DST accuracy at high sparsity (90–95%) while retaining the structured implementation’s performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors show that adding a permutation matrix to the structured sparse layers can greatly restore the loss of expressive power caused by sparsity while maintaining hardware friendliness.\n\n\n2. The experiments span vision and large language models, multiple structured DST methods, and compare both accuracy and execution time overhead.\n\n\n3. The paper is easy to follow. The introduction and motivation are clear and well-organized."}, "weaknesses": {"value": "1. Lack of explanation of how the permutation matrix works in the backward phase and how the soft permutation is transformed to the permutation matrix.\n\n\n2. The authors propose a method to improve model accuracy by making the sparse weights denser. However, in some cases, the values of these weights may differ significantly from the original dense weights. Should more rigorous and comprehensive experiments (larger datasets? Larger models? CNN?) be conducted to demonstrate the generalizability of the proposed method?\n\n\n3. Figure 3 should be emphasized; it is hard to compare the performance improvement between the original method and the proposed PA-DST method."}, "questions": {"value": "1. For the block DST, what does the permutation matrix look like? If a new DST sparse structure is proposed, what changes would be needed in PA-DST to adapt to the new structure?\n\n\n2. The authors only discuss the ablation study of accuracy with row and column permutations in Table 10. However, accessing the weight matrix is typically done in either row-major or column-major. The impact of different access orders on computation time needs to be considered during implementation. It is hoped that the authors can provide an explanation for it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "crPkUwrO0c", "forum": "FdAKUu6uls", "replyto": "FdAKUu6uls", "signatures": ["ICLR.cc/2026/Conference/Submission9607/Reviewer_uGDU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9607/Reviewer_uGDU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858473463, "cdate": 1761858473463, "tmdate": 1762921148994, "mdate": 1762921148994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PA-DST, a structured dynamic sparse training method by learning a permutation at each layer to restore lost expressivity. The approach maintains the efficiency of structured sparse kernels at inference by implementing permutations through lightweight index reordering. Experiments indicate that PA-DST can narrow the accuracy gap to unstructured DST at high sparsity levels while preserving the performance benefits of existing structured methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors present PA-DST, a general layer formulation that combines structured sparsity with a single learnt permutation matrix.\n\n2. The authors prove tight combinatorial bounds showing that the added permutation recovers the expressivity lost by structured sparsity as compared to dense and unstructured sparse models.\n\n3. Experiments demonstrate that with learnt permutations, structured sparse methods can achieve unstructured level accuracy across both vision and language tasks, while achieving significant speedup."}, "weaknesses": {"value": "1. It isn't easy to compare the inference and training time of these structured sparse methods before and after using PA-DST. \n\n2. Comparing the execution time on only one model (ViT-B/16) makes the experiment seem insufficient. Although Table 5 lists the execution time of GPT-2 Medium, it only shows the diagonal sparsity of 60% and 80%. What would it be like to compare inference and training time on larger models?\n\n3. The author mentions in the paper that the proposed permutation-structured family can be accelerated in both forward and backward passes, but why is the difference in acceleration between inference and training so significant?\n\n4. The subsection number should be corrected. e.g. 6.1.1 PPL vs Sparsity."}, "questions": {"value": "1. In the ablation study, the authors show that there is no significant difference in accuracy between row and col permutations for ViT-B/16 model on ImageNet-1K. What are the results on other models and datasets? Are there differences in execution time for row and col permutations?\n\n2. Section 6.3 shows the distance of permutations. What is the significance of doing this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BmKsIn68Ld", "forum": "FdAKUu6uls", "replyto": "FdAKUu6uls", "signatures": ["ICLR.cc/2026/Conference/Submission9607/Reviewer_BguH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9607/Reviewer_BguH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860901924, "cdate": 1761860901924, "tmdate": 1762921148649, "mdate": 1762921148649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PA-DST, a method to augment dynamic sparse training by jointly learning permutations along with semi-structured sparse weight tensors. The input is multiplied by the permutations to enable semi-structured sparse topologies to be more expressive as measured by the number of linear regions in the functional output space of the sparse neural network (SNN). During training, a doubly-stochastic soft permutation matrix is used to learn the optimal per-tensor permutation. During inference, the soft permutation can be converted to a relatively efficient indexing operation applied to each layer's input. The empirical results suggest modestly improved model accuracy over existing semi-structured DST methods; however, the learned permutations do result in memory and latency overhead increases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed method is novel and does not appear to have a direct corollary in the DST literature. \n* The theoretical performance benefits of weight sparsity cannot be realized on commodity accelerators without imposing some form of structure on the sparse topology. As such, methods such as PA-DST which improve the expressibility of semi-structure sparse networks are crucial to creating practical SNNs for real-world tasks. \n* The proposed method is well motivated through an analysis of the number of linear regions (NLR) present in dense, unstructured sparse, and semi-structured sparse neural networks. \n* PA-DST universally improves the accuracy of existing semi-structured DST methods. \n* Experimental results are conducted on both image classification and language modelling tasks. \n* The paper includes all necessary hyperparameters required to reproduce the results.\n* The proposed method is closed under transposition, enabling acceleration of backwards passes when the underlying structure allows it."}, "weaknesses": {"value": "# Major concerns\nThe following represent key weaknesses that must be addressed to increase the rating:\n* Potential for overfitting on WikiText: WikiText is a relatively small dataset. Based on Table 9, it appears that the GPT-2 models were trained for 100 epochs for a total of ~10B tokens. Generally, LLMs are pretrained on more diverse data for fewer (1-2 epochs, if any). Further, DST often achieves the highest accuracies when trained for more training steps than dense (i.e., RigL x5 steps ~= Dense x1). Pretraining on a larger dataset (eg., c4) for fewer epochs (0-1 ideally) would be more indicative of real-world pretraining runs and help establish whether PA-DST remains accurate when more diverse data is used. \n* Memory overhead: At practical sparsities (50, 60%) that do not incur substantial PPL increases for GPT-2, the memory overhead of PA-DST is ~10-13%. Assuming the analysis was conducted with 16-bit weights and activations, the actual overhead in a deployment scenario may be double the value listed as W8A8 and even lower precision types become the norm for inference. \n* Modest accuracy gains: While PA-DST provides significant gains in high-sparsity regimes (e.g., +1.32% for ViT-L/16 SRigL at 90%), for many other practical scenarios, the accuracy gains are more modest (<= 0.5%), which may not justify the additional memory and latency overhead. \n* Additional training complexity: The learned permutations introduce a new hyperparameter, $\\delta$ for the  permutation learning schedule soft/hard transition threshold. It is unclear if tuning this value may be required for other models and datasets. \n* Lack of downstream task language evaluations: PPL has previously been shown to be a misleading metric when evaluating compressed LLMs.\n\n# Minor concerns\nThe following are minor concerns, typos, etc. which would improve the work but do not affect the final rating: \n* Scratch training vs. SFT: The proposed method is intended for training from scratch. Extending the experimental results to include supervised fine-tuning of pretrained models may improve the impact of the work. \n* Clarify backwards pass acceleration: On L51, the authors note that the permutation-structure class is closed under transposition, thus “enabling sparse-to-sparse acceleration in both the forwards and backwards passes”. My understanding is that such acceleration would only be possible only if $S_l$ is in the set of sparse structures that offer exploitable structure for acceleration for both $S_l$ and $S_l^T$. I.e., SRigL’s fixed fan-in constraint allows for acceleration in the forwards, but not backwards pass, unless the constraint is also applied to the fan-out. As such, it may be more clear to state that the permutations do not prohibit acceleration of the backwards pass provided that the underlying sparse structure is itself amenable to transposition."}, "questions": {"value": "* Do the benefits of PA-DST hold when pretraining is conducted on a more diverse dataset for less epochs? \n* What dtypes are used for weights and activations for the training and inference benchmarks? How would lower bit-width types change the overhead analysis?\n* Are the accuracy gains of PA-DST stable under multiple seeds? For instance, what is the standard deviation of select DST with and without PA-DST augmentation on ImageNet? \n* How was the permutation learning schedule threshold tuned? How was the value of 0.22 selected? How sensitive is PA-DST to this value? Was 0.22 used for both the GPT-2 and image classification tasks? How many tokens does it take for the GPT-2 permutation to reach the penalty threshold?\n*  When the sparse GPT-2 models are evaluated on a standard mutli-shot QA benchmark such as MMLU or OpenBookQA, are the benefits of learned permutations still apparent? \n*  The authors note that they were unable to reproduce the SRigL inference benchmarks “due to lack of support in PyTorch”. SRigL’s code base includes CUDA kernels with pytorch bindings. Could the authors expand on the difficulties they encountered while trying to reproduce these results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CN5Ubf3rGB", "forum": "FdAKUu6uls", "replyto": "FdAKUu6uls", "signatures": ["ICLR.cc/2026/Conference/Submission9607/Reviewer_aUjB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9607/Reviewer_aUjB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930534283, "cdate": 1761930534283, "tmdate": 1762921148378, "mdate": 1762921148378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a novel structured dynamics sparse training (DST) method that uses both a permutation matrix and structured weight matrix jointly to learn an expressive representation that is also amenable to acceleration in both the backwards and forwards pass on real-world hardware, and can exploit a variety of structured formats, e.g. N:M, block and diagonal. The authors claim their method improves generalization over existing structured DST baselines significantly at 90-95% sparsity. The authors evaluate their method on vision and language models, includes ImageNet-1K and WikiText-103 with models such as ViT-B/16 and GPT-2 respectively."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "* The authors claim to accelerate both the backwards and forward pass, unlike all existing structured DST methods I'm aware of, which only accelerate the forward pass.\n* The authors evaluate their method on a wide array of structured sparsity patterns, as opposed to only a fixed and specialized N:M sparsity pattern as in SRigL\n* The authors evaluate on strong models/dataset baselines, comparable to what has been used in prior work.\n* The proposed methodology is interesting and novel, in particular the permutation shuffling.\n* The authors address concerns over the efficiency of implementation of their proposed methodology.\n\nThe authors also claim to have addressed two of the most significant gaps in the literature in my opinion: accelerating the backwards pass/training and accelerating DST with arbitrary N:M sparsity patterns. These are in my mind the strength of this paper and really piqued my interest in their work."}, "weaknesses": {"value": "* The motivation the authors have chosen to write their story around, and indeed background/abstract is misleading: structured dynamic sparse training (DST) methods such as SRigL demonstrate that they can match the accuracy of unstructured DST methods except perhaps at extremely high sparsity (higher than the authors demonstrate here), and I believe most researchers in the field would not claim this is a significant gap. \n* The results presented appear to be very different than those in the SRigL baseline paper (which is confusing as they later claim they can't run SRigL inference), and without any explanation as to why. In fact they appear to match the SRigL *without ablation* results instead (i.e. the results used to demonstrate why neuron ablation is required), and since the authors don't mention how they have implemented SRigL or found these numbers, I'm concerned this is where the claim of a large generalization gap is coming from, and perhaps why the introduction reads as so misleading if one is aware of the SRigL results.\n* As has been established by much of the literature over the past 1-2 years, perplexity is a very poor evaluation alone of the relative performance of a language model performance post-pruning or compression, or indeed in any context. Unfortunately it is not sufficient to look at perplexity at this point, instead there do have to be some task evaluation benchmarks in the results. See for example \"Compressing LLMs: The Truth is Rarely Pure and Never Simple, Jaiswal et al., 2024\"\n* The theoretical justification was hard to follow and not very convincing to me. More importantly it comes off to the reader as disconnected from the story which frankly is a very empirical claim (improved generalization) or engineering claim (accelerating backwards pass/arbitrary n:m sparsity). I don't understand why so much of the main paper was devoted to this, I would much rather have seen the more important details of the proposed methodology that are in the appendix - a section frankly most readers of your work are not required to read.\n* As someone who is somewhat familiar with the SRigL codebase, I know it's written in *PyTorch*, so this statement is extremely worrying and confuses me as to how the authors are evaluating SRigL, which is the strongest baseline method, and main result in their improved generalization claim: \"We estimate the execution times for SRigL as per their methodology and are unable to run end-to-end training and inference due to lack of support in PyTorch\". It is generally expected that you are either replicating the results of the baseline or explicitly using comparable results from the baseline paper, I don't see how you can estimate real-world execution times, or give generalization results without being able to do inference.\n* The method proposed suffers a significant memory and compute overhead unlike existing structured DST methods\n* It is unclear what hardware (even if CPU or GPU or other) the timings in figure 3 are run on, or if these are indeed real-world timings at all given the author's comments in the paper, and if they include the compute overheads discussed for the proposed method. Given this I can't evaluate the claims on inference/training times at present.\n\nGiven the story of the paper is increased generalization, I have to evaluate the paper based on the claims around increased generalization rather than some of the more impressive aspects of the proposed algorithm, and the generalization results are unfortunately not well supported by the evidence, in particular the main (SRigL) baseline and confusion as to where those results come from.\n\nAdditional Feedback on Improving Paper:\n* The authors also claim to have addressed two of the most significant gaps in the literature in my opinion: accelerating the backwards pass/training and accelerating DST with arbitrary N:M sparsity patterns. I can't understand why these aren't the main motivation and story of the paper, these are much more interesting than claiming a minor increase in generalization. Also the evidence to support these claims would be clear to evaluate. \n* Figure 2 is extremely hard to read due to the very few colours and symbol overlap, especially on a printed version, but even on screen. Being such an important figure for the claims being made this is a big problem."}, "questions": {"value": "* Are you using SRigL *with ablation* in your experiments? Is this the original codebase or your own code replicating the results? Where do the SRigL results in Figure 1 come from if you are unable to run training or inference for SRigL?\n* Does your algorithm require neuron ablation as used in SRigL to match generalization at high sparsity?\n* \"We estimate the execution times for SRigL as per their methodology and are unable to run end-to-end training and inference due to lack of support in PyTorch\". Explain this statement, why were you unable to use or replicate the results of SRigL (especially considering it is written in PyTorch)?\n* Exactly where do the timings in Figure 3 come from? What hardware were they run on? GPU, CPU? Are they all timed the same way? Do the PA-DST timings include the compute overhead?\n* Do you have any further evaluation that perplexity for the language results, e.g. on task benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lergC3HMIu", "forum": "FdAKUu6uls", "replyto": "FdAKUu6uls", "signatures": ["ICLR.cc/2026/Conference/Submission9607/Reviewer_KXyo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9607/Reviewer_KXyo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762021893264, "cdate": 1762021893264, "tmdate": 1762921147917, "mdate": 1762921147917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}