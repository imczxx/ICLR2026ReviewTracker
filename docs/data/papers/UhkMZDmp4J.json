{"id": "UhkMZDmp4J", "number": 4885, "cdate": 1757783520460, "mdate": 1759898007307, "content": {"title": "Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models", "abstract": "Many reasoning techniques for large multimodal models  adapt language model approaches, such as Chain-of-Thought (CoT) prompting, which express reasoning as word sequences. While effective for text, these methods are suboptimal for multimodal contexts, struggling to align audio, visual, and textual information dynamically. To explore an alternative paradigm, we propose the Multimodal Chain of Continuous Thought (MCOUT), which enables reasoning directly in a joint latent space rather than in natural language. In MCOUT, the reasoning state is represented as a continuous hidden vector, iteratively refined and aligned with visual and textual embeddings, inspired by human reflective cognition. We develop two variants: MCOUT-Base, which reuses the language model’s last hidden state as the continuous thought for iterative reasoning, and MCOUT-Multi, which integrates multimodal latent attention to strengthen cross-modal alignment between visual and textual features. Experiments on benchmarks including MMMU, ScienceQA, and MMStar show that MCOUT consistently improves multimodal reasoning, yielding up to 8.23\\% accuracy gains over strong baselines and improving BLEU scores up to 8.27\\% across multiple-choice and open-ended tasks. These findings highlight latent continuous reasoning as a promising direction for advancing LMMs beyond language-bound CoT, offering a scalable framework for human-like reflective multimodal inference.", "tldr": "Exploring the Continuous Thought in Latent-Space Reasoning for VLMs", "keywords": ["Efficient Reasoning", "Multimodal Chain of Continuous Thought", "Latent Reasoning"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fafd8518db87981274d84708215d72f7d54ec360.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper propose MCOUT, a method to incoporate model hidden representation iteratively to simulate the process of chain-of-thought in the latent space. The paper additionally proposes MCOUT-multi to use an attention module to merge the visual embeddings and the intermediate hidden representations. Evaluation on vision-language benchmarks demonstrate the advantage of the intermediate latent reasoning compared to the baseline model without it."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "N/A"}, "weaknesses": {"value": "* The comparison of results is very unclear. For example, MCOUT-Base (Nt = 10) is written to have \"up 4.79%\" with 58.86% accuracy, while the baseline has 56.17. It is unclear what the method is comparing against. In addition the comparison with other baseline models is unfair because all the MCOUT models are finetuned on the benchmarks.\n\n* It is unclear what the purpose of the Multimodal Latent Attention module is. The discussion in Section 4.3 introduces many statistical results but are largely unrelated in justifying the module. Based on the experimental results, it seems that the MCOUT-Base consistently outperforms MCOUT-Multi.\n\n* The Training and evaluation setting is strange. It used only VQAv2 for pretraining as opposed to the standard LLaVA-style 2 stage training. Then, the model is finetuned only on the downstream benchmarks only. This raises question on the scalability and generalizability of the approach. In addition, the method doesn't include any CoT data during training, making it unclear how or whether the model can gain CoT capabilities at all."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "knksVv8fdn", "forum": "UhkMZDmp4J", "replyto": "UhkMZDmp4J", "signatures": ["ICLR.cc/2026/Conference/Submission4885/Reviewer_kxzQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4885/Reviewer_kxzQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761006513786, "cdate": 1761006513786, "tmdate": 1762917737670, "mdate": 1762917737670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel reasoning framework for Vision-Language Models (VLMs), termed \"Multimodal Chain of Continuous Thought\" (MCOUT). Its core contribution lies in shifting the reasoning process from discrete natural language sequences to iterative refinements within a continuous latent space. This represents a significant, exploratory breakthrough from the traditional \"Chain-of-Thought\" (CoT) paradigm, aiming to address inefficiencies and insufficient multimodal alignment that arise when forcing heterogeneous modalities into a single language sequence in multimodal contexts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel paradigm for vision language model reasoning, shifting from discrete natural language sequences to iterative refinements within a continuous latent space. This approach effectively challenges the inherent limitations of language centric Chain of Thought methods, presenting a promising avenue for more intrinsic and efficient reasoning.\n\n2. MCOUT achieves substantial performance improvements on several multimodal benchmarks using only a one billion parameter vision language model. These results indicate significant efficiency, allowing smaller models to attain powerful reasoning capabilities that outperform larger baselines, which is particularly valuable for resource constrained environments.\n\n3. The paper's rigorous analysis, especially the latent space dynamics, provides critical diagnostic insights into the current limitations of multimodal fusion mechanisms. By empirically revealing issues like modality collapse, the study not only identifies a key challenge for the field but also offers a clear, interpretable, and readily extendable foundation for further research into advanced reasoning."}, "weaknesses": {"value": "1. The paper proposes multimodal latent attention to enhance cross-modal alignment. However, the conducted latent space analysis reveals that the mixed embeddings exhibit remarkably low variance across iterations. This observation strongly suggests that the multimodal attention mechanism fails to extract and integrate discriminative visual context effectively, leading to MCOUT-Multi’s performance being almost indistinguishable from MCOUT-Base. Consequently, the claimed advantage in dynamic multimodal alignment is not robustly supported by empirical evidence.\n\n2. While the authors identify and acknowledge the issue of modality collapse, particularly evidenced by the static nature of mixed embeddings and initial norm imbalances, the proposed solutions, such as normalization layers, appear to be primarily engineering fixes rather than fundamental architectural or algorithmic advancements. A more profound exploration and resolution of this critical weakness in dynamic multimodal integration are absent.\n\n3. The ablation studies demonstrate that the framework's performance is notably sensitive to the number of latent reasoning steps (Nt) and the auxiliary loss weight (µ). Achieving optimal results necessitates careful tuning of these hyperparameters, which implies a potential lack of robustness or increased deployment complexity in varied application scenarios.\n\n4. The paper posits improved efficiency by circumventing token generation. Nevertheless, a detailed quantitative analysis comparing the incremental computational overhead of multiple forward passes inherent in the iterative process against the costs associated with traditional discrete token generation (CoT) is not provided. This omission leaves the actual efficiency gains less substantiated."}, "questions": {"value": "1. Did the authors investigate alternative attention mechanisms or different integration points for visual information to mitigate the observed modality collapse in MCOUT-Multi?\n2. Could the authors provide a more detailed comparative analysis of MCOUT's inference time and memory footprint against traditional Chain of Thought methods, especially across varying input lengths and iteration counts?\n3. Was a comprehensive error analysis performed to identify specific types of multimodal reasoning tasks where MCOUT underperforms, potentially with illustrative failure examples to elucidate underlying limitations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ELKTnhmddR", "forum": "UhkMZDmp4J", "replyto": "UhkMZDmp4J", "signatures": ["ICLR.cc/2026/Conference/Submission4885/Reviewer_YEkc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4885/Reviewer_YEkc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639502849, "cdate": 1761639502849, "tmdate": 1762922525289, "mdate": 1762922525289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Multimodal Chain of Continuous Thought (MCOUT), a framework for vision-language models that enables reasoning directly in a joint latent space (instead of discrete natural language CoT) by representing \"thought\" as an iteratively refined continuous hidden vector, mimicking human reflective cognition. It presents two variants: 1) MCOUT-Base, which reuses the model’s last hidden state; and 2) MCOUT-Multi, which integrates multimodal latent attention to enhance cross-modal alignment. Experiments are conducted on benchmarks including MMMU and ScienceQA using the Llama3.2 1B model, with MCOUT achieving up to 8.23% accuracy improvement over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The work makes a pioneering effort to shift reasoning for multimodal tasks from discrete natural language (traditional CoT) to a continuous latent space, representing an innovative exploration of latent-space reasoning in multimodal scenarios."}, "weaknesses": {"value": "1.\tLimited Novelty and Inadequate Comparison\nThe core method largely transfers latent CoT from COCONUT to a multimodal context, lacking sufficient novelty. Additionally, it fails to adequately compare against relevant existing methods (e.g., Mirage) and omits key baselines (e.g., COCONUT adapted to multimodal scenarios), making its distinct advantages unclear.\n\n2.\tAmbiguity in Method and Presentation\n\nThe paper lacks clarity: the introduction fails to clearly articulate specific contributions, and the claim of \"improved efficiency\" (cited as a motivation) is not experimentally validated.\n\nMethodological details are unclear, including the training process, calculation of L_{aux}, and the rationale for introducing MCOUT-Base and MCOUT-Multi.\n\n3.\tFlaws in Experimental Design\n\nThe experimental setup is ambiguous: baselines for MCOUT are not clearly defined, and the inclusion of scores from unrelated models is confusing (serving no clear comparative purpose).\n\nThe performance gain of MCOUT-Multi over MCOUT-Base is marginal, with no in-depth analysis of this phenomenon to justify MCOUT-Multi’s value.\n\nExperiments are limited to a single small model (Llama3.2 1B), lacking validation across multiple model sizes or series."}, "questions": {"value": "1.\tHow is the auxiliary loss L_{aux} calculated?\n\n2.\tHow are the parameters of the MultiHeadAttn module initialized?\n\n3.\tGiven the insignificant performance difference between MCOUT-Base and MCOUT-Multi, what is the rationale for introducing these two settings? Further, why is Section 4.3 significant under this circumstance?\n\n4.\tIn Table 4, when the auxiliary weight is 0, performance is comparable to the best setting (weight = 0.3). Does this imply L_{aux} is useless? And why is performance with an auxiliary weight of 0 much higher than the baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Q188P0fOC9", "forum": "UhkMZDmp4J", "replyto": "UhkMZDmp4J", "signatures": ["ICLR.cc/2026/Conference/Submission4885/Reviewer_BxVz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4885/Reviewer_BxVz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797398071, "cdate": 1761797398071, "tmdate": 1762917736985, "mdate": 1762917736985, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MCOUT (Multimodal Chain of Continuous Thought), a framework for reasoning in vision-language models (VLMs) that operates in continuous latent space rather than generating discrete reasoning tokens. The method iteratively refines hidden state representations by feeding them back into the model as \"thought tokens\" over N_t iterations. Two variants are proposed: MCOUT-Base, which directly reuses the last hidden state, and MCOUT-Multi, which fuses the hidden state with visual embeddings via multimodal attention. Experiments on ScienceQA, MMMU, and MMStar show improvements of 3-8% accuracy over baselines using a 1B parameter model (CLIP + Llama 3.2 1B)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper articulates a compelling case for continuous latent reasoning over token-based CoT. By generating N_t=5-10 hidden state vectors instead of 50-100 reasoning tokens, the method achieves computational efficiency while maintaining interpretable iteration counts.\n\n2. The paper is generally well written with a clear motivation, good writing, and informative figures.\n\n3. Section 4.3's analysis of modality collapse in MCOUT-Multi is commendable. The authors identify low-variance mixed embeddings and connect this to recent work on attention sinking, providing genuine insight into when/why their multimodal fusion struggles."}, "weaknesses": {"value": "Here's the review with unified, simpler formatting:\n\n1. **MCOUT-Base is essentially COCONUT applied to VLMs**: The paper positions MCOUT-Base as a novel contribution, but it appears methodologically identical to COCONUT (Hao et al., 2024) - taking the last hidden state and feeding it back iteratively. The only difference is applying it to a VLM architecture (which is just an LLM with a vision encoder). This should be acknowledged more directly. The true novel contribution is MCOUT-Multi, but...\n\n2. **MCOUT-Multi underperforms MCOUT-Base, undermining the paper's core thesis**: Unfortunately, the paper's actual novel contribution (multimodal latent attention) generally performs worse than or comparable to the baseline COCONUT (MCOUT-Base) approach. The paper essentially demonstrates that COCONUT works for VLMs, but that adding multimodal fusion doesn't help (and often hurts). This fundamentally undermines the premise that multimodal latent reasoning requires special cross-modal alignment.\n\n   More critically, if the paper's contribution is showing that multimodal information isn't effectively utilized in continuous latent CoT, then the experimental investigation is insufficient. ScienceQA, MMMU, and MMStar suffer from significant language bias - many questions can be answered from text alone without visual grounding. This has been documented by the literature NaturalBench (Lin et. al.). If the datasets don't require deep visual integration, MCOUT-Multi's fusion mechanism may appear ineffective simply because it's not needed. The paper should evaluate on benchmarks that require visual grounding such as Winoground, NaturalBench, or BLINK.\n\n   Without this investigation, we can't distinguish between: Hypothesis A - multimodal fusion in continuous latent space is fundamentally flawed; Hypothesis B - the specific fusion mechanism needs refinement; Hypothesis C - the datasets don't require multimodal reasoning, so fusion provides no benefit; or Hypothesis D - training was insufficient for the fusion module to learn effective alignment.\n\n3. **Custom model limits generalizability**: The authors train their own 1B VLM from scratch rather than applying MCOUT to existing models (LLaVA, Qwen-VL, InstructBLIP, etc.). This is problematic because the method is model-agnostic and could be applied to any VLM without retraining, comparisons are unfair since their baseline is their own undertrained model rather than SOTA VLMs, we can't assess whether improvements come from the method or their specific training recipe, and the approach would be more convincing if it improved existing strong models.\n\n4. **Limited baselines**: No comparison to standard text-based CoT prompting on the same model, self-consistency or other CoT variants, other test-time scaling methods, or the computational cost comparison (inference time, memory) versus token-based reasoning.\n\nWhile the core idea and approach may have merit, it needs more distinction from prior work, more thorough baseline comparions, and better justification of the main motivation for multimodal fusion."}, "questions": {"value": "Can you explicitly clarify the methodological differences between MCOUT-Base and COCONUT? If they are identical, should MCOUT-Base be framed as an application/extension of COCONUT rather than a separate contribution?\n\nI would also appreciate if you can address the open questions left open in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I8rzf91z7D", "forum": "UhkMZDmp4J", "replyto": "UhkMZDmp4J", "signatures": ["ICLR.cc/2026/Conference/Submission4885/Reviewer_yJPg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4885/Reviewer_yJPg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762360819529, "cdate": 1762360819529, "tmdate": 1762917727823, "mdate": 1762917727823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}