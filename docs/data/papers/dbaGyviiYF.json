{"id": "dbaGyviiYF", "number": 15885, "cdate": 1758256579144, "mdate": 1759897275523, "content": {"title": "A Dynamic Low-Rank Fast Gaussian Transform", "abstract": "The Fast Gaussian Transform (FGT) enables subquadratic-time multiplication of an $n\\times n$ Gaussian kernel matrix $\\mathsf{K}_{i,j}= \\exp ( -  \\lVert x_i - x_j \\rVert_2^2 ) $ with an arbitrary vector $h \\in \\mathbb{R}^n$, where $x_1,\\dots, x_n \\in \\mathbb{R}^d$ are a set of fixed source points. This kernel plays a central role in machine learning and random feature maps. Nevertheless, in most modern data analysis applications, datasets are dynamically changing (yet often have low rank), and recomputing the FGT from scratch in (kernel-based) algorithms incurs a major computational overhead ($\\gtrsim n$ time for a single source update $\\in \\mathbb{R}^d$). These applications motivate a dynamic FGT algorithm, which maintains a dynamic set of sources under kernel-density estimation (KDE) queries in sublinear time while retaining Mat-Vec multiplication accuracy and speed.  \n\nAssuming the dynamic data-points $x_i$ lie in a (possibly changing) $k$-dimensional subspace ($k\\leq d$), our main result is an efficient dynamic FGT algorithm, supporting the following operations in $\\log^{O(k)}(n/\\varepsilon)$ time: (1) Adding or deleting a source point, and (2) Estimating the \"kernel-density\" of a query point with respect to sources with $\\varepsilon$ additive accuracy. The core of the algorithm is a dynamic data structure for maintaining the projected \"interaction rank\" between source and target boxes, decoupled into finite truncation of Taylor and Hermite expansions.", "tldr": "", "keywords": ["Fast Gaussian Transform", "Theory", "kernal-density estimation"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a21b1a3a2bd3ae1f9b187e7dbc21411107db77ab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies dynamic algorithms for high-precision Gaussian kernel density estimation/Fast Gaussian transform. In particular given a set of points $x_1,\\ldots,x_n\\in \\mathbb{R}^{d}$ and associated charges $q_1,\\ldots, q_n \\in \\mathbb{R}$, given a point $x\\in \\mathbb{R}^d$ the goal is to output an $\\epsilon$ approximation to \n\n $\\sum_{i=1}^n q_i e^{-\\|x-x_i\\|_2^2}$\n\n faster than linear in $n$ time. The famous fast multipole method achieves this in $log^{O(d)}(n/\\epsilon))$ time but is not is a dynamic algorithm that supports updates to changing points $x_1,\\ldots,x_n$.\nThe contribution of the paper is to present a dynamic algorithm for this problem under the assumption that the data points live in a $w$ dimensional space. In particular their algorithm supports two operations in $log^{O(w)}(n/\\epsilon)$ time, the first is to accomodate updates to a point $x_i$ and associated charge $q_i$, and secondly output an $\\epsilon$ approximation to \n\n$\\sum_{i=1}^n q_i e^{-\\|x-x_i\\|_2^2}$ \n\nfor any $q$ in the same subspace. The main idea in their algorithm is a dynamic datastructure for maintaining the projected interaction rank between source and target boxes, decoupled into finite truncation of Taylor and Hermite expansion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strength of the paper is to study dynamic algorithms for fast Gaussian transform that is an important algorithm for various fundamental tasks in machine learning and data science."}, "weaknesses": {"value": "Perhaps a minor weakness of the paper is to study the problem under the assumption that the points and the query live in a low dimensional subspace. I further ask specific questions regarding this in the questions section."}, "questions": {"value": "Regarding the weakness, it would be interesting to know more motivations behind why the assumption is a natural assumption. Moreover which parts of the proof breakdown when for eg. the query is not in the same subspace as the points. Furthermore are there any other natural assumptions under which dynamic algorithms could be developed ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "q9SjPfDuPf", "forum": "dbaGyviiYF", "replyto": "dbaGyviiYF", "signatures": ["ICLR.cc/2026/Conference/Submission15885/Reviewer_GhF7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15885/Reviewer_GhF7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761495482938, "cdate": 1761495482938, "tmdate": 1762926103208, "mdate": 1762926103208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript outlines a scheme to make the FGT dynamic (in the sense that points are allowed to move). The method is also tailored to work with data that lies in a low-dimensional subspace and a scheme is sketched out that allows the subspace to (slowly) evolve."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The manuscripts contributions are technically sound (albeit quite dense). Moreover, the problem addressed (updating points) is of practical interest."}, "weaknesses": {"value": "The main weakness of the manuscript is that its focus is on a highly practical algorithm (the FGT) and, yet, it contains no experiments or illustrations that highlight if the proposed scheme is actually practical. Absent such analysis, it is hard to advocate for the contribution of the manuscript. Even basic experiments would really help to strengthen the manuscript. Given the context of the manuscript I consider this a very significant weakness.\n\nWhile purely theoretical papers can certainly be valuable, this work directly builds on the FGT and focuses the presentation as such---that makes it feel like an evolution of that algorithm that takes it from practical to (potentially) impractical. If there is a claim that the theoretical developments are of sufficient interest then there needs to be a stronger case made that they are broader or have particularly interesting technical developments (relative to prior work). The current presentation focuses so strongly on the FGT that it is hard to see this.\n\nLastly, while the specialization to low-dimensional data is good to have, it is not clear how interesting the result is in the exactly low-dimensional case. Perhaps I am missing something about the goals, but if the data is exactly low-dimensional then after an orthonormal basis $Q$ is computed for the subspace (which can be done \"cheaply\") isn't it just a matter of running all the FGT machinery (including the update scheme) on $x_i = Q^Ts_i$ since the FGT only depends on pairwise distances and $\\lVert x_i - x_j\\rVert_2 = \\lVert s_i - s_j\\rVert _2$? This is actually the scheme advocated for, but somehow lemma 3.5 and the surrounding text makes it seem more complicated than \"invoke your existing FGT code on $x_i$\" and that leads me to believe I may be missing something."}, "questions": {"value": "Does the manuscripts perspective on low-dimensional data provide a path to situations where the data nearly lies on some low-dimensional subspace (i.e., not exactly) or some more general low-dimensional manifold?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4TptKfbUGU", "forum": "dbaGyviiYF", "replyto": "dbaGyviiYF", "signatures": ["ICLR.cc/2026/Conference/Submission15885/Reviewer_rjDX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15885/Reviewer_rjDX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864054676, "cdate": 1761864054676, "tmdate": 1762926102760, "mdate": 1762926102760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the problem of efficient matrix-vector multiplication in the case where the matrix is a kernel matrix (first in the Gaussian kernel case and then in the more general case of fast decay kernels) and the kernel points and the target vector may be dynamic over time. The authors builds on the fast Gaussian transform (FGT), which considers the static case where all points in the kernel matrix and the target vector are known in advance, and extend it towards a dynamic FGT algorithm. The algorithm leverages a novel rewriting of the original FGT approximation procedure, which reveals how certain structures can be pre-computed, thus lowering the cost of adding new points and producing an efficient procedure to adapt the data structure. The authors discuss several extensions towards the case of low-dimensional state/dynamic spaces."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* New efficient FGT algorithm for matrix-vector multiplication in the dynamic case for Gaussian kernels.\n* The algorithm can add/remove source points and can perform kernel density estimation in logarithmic time exponential in the dimensionality of the subspace that all points below to.\n* The algorithm can be adapted to a larger family of fast-decay kernels and to the general case of points belonging to a static or dynamic subspace.\n* The derivation of the algorithm builds on a (at the best of my knowledge) novel treatment of the truncations of the kernel and Taylor expansions that allows for an efficient pre-computation of structures that remain constant through the include/delete operations."}, "weaknesses": {"value": "* While the paper is theoretical in nature, it lacks of any numerical validation of the proposed method. Unfortunately, the theoretical analysis of these numerical methods often hide operations and implementation details that \"in practice\" tend to dominate performance in many real-world regimes. Furthermore, while the theory does not require a precise tuning of the parameters (eg, delta, k, r, eps) their choice can have a significant impact on the overall performance. While the authors provide some guidance in Remark 3.1, even a simple numerical validation would help in strengthening the current contribution, in particular given the focus on machine learning applications.\n* In the literature review, the authors do not seem to discuss randomized linear algebra approaches to kernel learning. While this may be difficult to compare in practice (randomized vs deterministic), an extensive theoretical as well as practical literature is available following this approach and an extensive review would help. See e.g.\n\"Randomized Numerical Linear Algebra: Foundations & Algorithms\" https://arxiv.org/abs/2002.01387\n\"Random Fourier Features for Kernel Ridge Regression: Approximation Bounds and Statistical Guarantees\" https://arxiv.org/abs/1804.09893\n\"oASIS: Adaptive Column Sampling for Kernel Matrix Approximation\" https://arxiv.org/pdf/1505.05208\n\"Fourier Sparse Leverage Scores and Approximate Kernel Learning\" https://arxiv.org/abs/2006.07340\n\"Distributed Adaptive Sampling for Kernel Matrix Approximation\" https://proceedings.mlr.press/v54/calandriello17a.html"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0gkrEcGK7H", "forum": "dbaGyviiYF", "replyto": "dbaGyviiYF", "signatures": ["ICLR.cc/2026/Conference/Submission15885/Reviewer_Kcvf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15885/Reviewer_Kcvf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900247181, "cdate": 1761900247181, "tmdate": 1762926102393, "mdate": 1762926102393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the well-known Fast Gaussian Transform technique for computing kernel density estimates, and for approximating matrix-vector multiplication with a kernel matrix. The paper describes a method to create a dynamic version of the algorithm with poly-logarithmic update and query times. As with the static Fast Gaussian Transform, the techniques require an exponential dependency on the dimension $s$. This paper additionally discusses the case where the data lies in a low-dimensional subspace and shows that the complexity scales with the intrinsic dimension of the data, rather than the ambient dimension."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Problems of matrix-vector multiplication with a kernel matrix, and of kernel density estimation are important and relevant to the machine learning community, and the Fast Gaussian Transform is an important technique. The contributions of this paper to create a dynamic version of the FGT are therefore relevant and significant. While the techniques are largely direct generalizations of the standard FGT, this is nonetheless an important problem and this paper is the first (to my knowledge) to consider the FGT in the dynamic setting. The paper is generally well presented."}, "weaknesses": {"value": "Given that the FGT is fast in practice for low-dimensional data, the key weakness of this paper is the lack of empirical evaluation. A fast implementation of the described method would be of significant value.\n\nTypos and minor points\n- In the introduction, the notation changes unnecessarily from using $x_i$ to using $s_i$ for the source points.\n- In the related works comparison with LSH-based KDEs, you should be aware of recent work which directly address the question of dynamizing these data structures [1, 2]. Note that these methods achieve sublinear update time, but not polylogarithmic.\n- In the references, I could not find the paper 'Josh Alman, Gary Miller, Timothy Chu, Shyam Narayanan, Mark Sellke, and Zhao Song. Metric transforms and low rank representations of kernels. In arXiv preprint, 2021.'. What is the correct reference for Definition B.1?\n\n[1] Laenen et al. \"Dynamic Similarity Graph Construction with Kernel Density Estimation\", ICML 2025.\n[2] Liang et al. \"Dynamic maintenance of kernel density estimation data structure: From practice to theory.\", arXiv."}, "questions": {"value": "- Does Theorem B.5 follow as a direct corollary of Theorem F.2, with $w = d$?\n- What is the practical implication of this work? Can the algorithm described here be made to work fast in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DIEf5fJteg", "forum": "dbaGyviiYF", "replyto": "dbaGyviiYF", "signatures": ["ICLR.cc/2026/Conference/Submission15885/Reviewer_cz3u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15885/Reviewer_cz3u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907008481, "cdate": 1761907008481, "tmdate": 1762926102008, "mdate": 1762926102008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The reviewed work provides a data structure and algorithm for \"dynamicising\" the fast Gauss transform. The FGT allow allows approximating kernel density estimators at a cost $\\log(\\epsilon)$ with the key insight being to Taylor approximate the function $x \\mapsto \\exp((x - s)^2 / (2 \\sigma)$ at a set of output cell centers and precompute the sums of taylor coefficients for different source locations $s$.\n\nThe reviewed work allows for the dynamic addition of new evaluation locations x and source locations s by also taylor expanding the maps  $s \\mapsto \\exp((x - s)^2 / (2 \\sigma)$ and summing coefficients over sources locations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed algorithm is natural and solves a conceivably important problem\nThe paper is well written: I had not studied the FGT before and the FGT was a valuable resource in understanding both the static FGT and the dynamic extension proposed by the authors."}, "weaknesses": {"value": "Some numerical examples and code for an implementation would have been helpful to appreciate practicality of the proposed algorithm, although it does seem practical to me."}, "questions": {"value": "Typos:\n\nIn line 123, \"match the statistic FGT algorithm\" should be \"match the static FGT algorithm."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GOXCIJWlgn", "forum": "dbaGyviiYF", "replyto": "dbaGyviiYF", "signatures": ["ICLR.cc/2026/Conference/Submission15885/Reviewer_mERe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15885/Reviewer_mERe"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946255958, "cdate": 1761946255958, "tmdate": 1762926101524, "mdate": 1762926101524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}