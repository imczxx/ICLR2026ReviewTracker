{"id": "NCLjpR2MDq", "number": 25232, "cdate": 1758365533568, "mdate": 1759896728797, "content": {"title": "From Broad Exploration to Stable Synthesis: Entropy-Guided Optimization for Autoregressive Image Generation", "abstract": "Combining Chain-of-Thought (CoT) with Reinforcement Learning (RL) improves text-to-image (T2I) generation, yet the underlying interaction between CoT's exploration and RL's optimization remains unclear. We present a systematic entropy-based analysis that yields three key insights: (1) CoT expands the generative exploration space, while RL contracts it toward high-reward regions; (2) final reward is strongly negatively correlated with both the mean and variance of image-token entropy, highlighting the need to reduce uncertainty and instability; and (3) the entropy of the textual CoT directly governs downstream image quality, with lower-entropy CoTs leading to better generations. Motivated by these findings, we propose Entropy-Guided Group Relative Policy Optimization (EG-GRPO), a fine-tuning strategy that reallocates optimization budget by uncertainty: low-entropy tokens are excluded from reward-driven updates to preserve stability, while high-entropy tokens receive an entropy bonus that encourages structured exploration without collapse. Experiments on standard T2I benchmarks demonstrate that EG-GRPO achieves state-of-the-art performance.", "tldr": "", "keywords": ["Language Models", "Autoregressive Image Generation", "Chain-of-Thought"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5bb4ab9e362b7e472faf33ee928f045bec5eb290.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an approach to balance exploration via CoT vs exploitation using RL for improving the text to image models and ultimately proposes a modified version of GRPO to combine these two insights."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The logic behind the paper makes a lot of sense, the combination of exploration vs exploitation.\n\nThe objective of the paper is clear and well communicated."}, "weaknesses": {"value": "Missing references GRPO in autoregressive models: \n\n'Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization' \n'Simplear: Pushing the frontier of autoregressive visual generation through pretraining, sft, and rl.'\n\n\nThe relative improvement with respect to T2I-R1 is small. \n\nExperiments could be more comprehensive:\n- Can you apply the same method to improve aesthetic score like in 'Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization' ?\n- Can you try it with a more diverse set of text-to-image models? (I expected to work too, but its good for completeness even if its just on one benchmark to see how different base models improve with your technique)"}, "questions": {"value": "- Is Figure 6 representative across multiple prompts or is it a curated set of images? please add a disclaimer if so.\n- In Figure 2 why does T2I-R1 translate the whole distribution rather than reshaping it around the original Janus distribution, the shift seems extreme. The entropy image mean goes down, could we include an appendix showing real images so we can qualitatively compare the diversity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PRMJxgSL79", "forum": "NCLjpR2MDq", "replyto": "NCLjpR2MDq", "signatures": ["ICLR.cc/2026/Conference/Submission25232/Reviewer_6gZ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25232/Reviewer_6gZ9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761025340505, "cdate": 1761025340505, "tmdate": 1762943374518, "mdate": 1762943374518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of autoregressive text to image generation, using chain of thought before generation. The authors devise three conclusions about the relationship between entropy and reward, before going on to devise an algorithm EG-GRPO. This algorithm nulls out what is called low entropy tokens to instead focus on high entropy tokens. The authors offer a theoretical explanation for \"reinvesting\" in high entropy tokens. The paper experimentally verifies this on T2I-CompBench and WISE. Where on most tasks it shows superior performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- this paper is well justified and for the most part I agree with the three points flagged to instantiate this algorithm.\n- this paper does a good job justifying the motivation for the algorithm"}, "weaknesses": {"value": "- I am curious about the effect of EG-GRPO on diversity of the reward/image. Indeed, I worry that the by further optimizing high entropy tokens, the diversity of the rewards and images generated is lowered. \nAs seen in fig 5, it seems that average entropy is lower, and thus the model is generating less diverge images. Some analysis and discussion would be preferred.\n- The algorithms performance seems better than the other methods, but I worry this is at the cost of sample diversity.\n- It is unclear to me why even entropy minimization in this case is the correct method? To me, this seems like reducing exploration?"}, "questions": {"value": "see weaknesses for questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iwsAqbb4R9", "forum": "NCLjpR2MDq", "replyto": "NCLjpR2MDq", "signatures": ["ICLR.cc/2026/Conference/Submission25232/Reviewer_7HWk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25232/Reviewer_7HWk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761534078704, "cdate": 1761534078704, "tmdate": 1762943374046, "mdate": 1762943374046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the relationship between CoT prompting and RL in autoregressive text-to-image generation models. The analysis shows that CoT broadens exploration, while RL narrows it toward high-reward regions. Both the mean and variance of image-token entropy are negatively correlated with final rewards, emphasizing stability and reduced uncertainty. The authors propose Entropy-Guided Group Relative Policy Optimization (EG-GRPO), which allocates optimization based on token-level entropy. Experiments show the proposed method balanced exploration and stability in RL fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors provide a thorough analysis of the proposed method to show how CoT and RL are balanced to reduce uncertainty and preserve knowledge, which seems convincing.\n\nPaper is well organized. writing is clear and easy to follow."}, "weaknesses": {"value": "1. The proposed approach adds nontrivial computational overhead due to token-level entropy computation, percentile thresholding, and per-batch bonus recalibration. However, the paper lacks a quantitative analysis of the resulting scaling, memory, and wall-clock costs, particularly for large-scale models or datasets.\n2. While entropy is treated as a measure of uncertainty, the method primarily focuses on entropy reduction, potentially at the expense of output diversity and creative expressivity. A discussion of this trade-off is missing."}, "questions": {"value": "1. Have the authors benchmarked the wall-clock or memory overhead of EG-GRPO (with batch bonus calibration) compared to vanilla GRPO or diffusion-based models, especially at scale? \n2. Are there cases where the additional computation outweighs the quality gains?\nCould the authors provide a quantitative assessment of diversity (e.g., via FID/IS for diversity, or human preference rating) to determine whether entropy suppression negatively impacts creative diversity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "APYmtInu5l", "forum": "NCLjpR2MDq", "replyto": "NCLjpR2MDq", "signatures": ["ICLR.cc/2026/Conference/Submission25232/Reviewer_Wsoe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25232/Reviewer_Wsoe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974984794, "cdate": 1761974984794, "tmdate": 1762943373680, "mdate": 1762943373680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}