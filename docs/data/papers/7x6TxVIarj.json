{"id": "7x6TxVIarj", "number": 4788, "cdate": 1757768719596, "mdate": 1759898012978, "content": {"title": "MME-Unify: A Comprehensive Benchmark for Unified Multimodal Understanding and Generation Models", "abstract": "Unified Multimodal Large Language Models (U-MLLMs) have garnered considerable interest for their ability to seamlessly integrate generation and comprehension tasks. However, existing research lacks a unified evaluation standard, often relying on isolated benchmarks to assess these capabilities. Moreover, current work highlights the potential of ``mixed-modality generation capabilities'' through case studies—such as generating auxiliary lines in images to solve geometric problems, or reasoning through a problem before generating a corresponding image. Despite this, there is no standardized benchmark to assess models on such unified tasks. To address this gap, we introduce MME-Unify, also termed as MME-U, the first open and reproducible benchmark designed to evaluate multimodal comprehension, generation, and mixed-modality generation capabilities. For comprehension and generation tasks, we curate a diverse set of tasks from 12 datasets, aligning their formats and metrics to develop a standardized evaluation framework. For unified tasks, we design five subtasks to rigorously assess how models’ understanding and generation capabilities can mutually enhance each other. Evaluation of 12 U-MLLMs, including Janus-Pro, EMU3, and Gemini2-Flash, reveals significant room for improvement, particularly in areas such as instruction following and image generation quality.", "tldr": "", "keywords": ["Unify Model; Multi-Modal Language Model; Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c936e3a836f00c4559072319dd2e891976531586.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MME-Unify, a new benchmark to evaluate Unified Multimodal Large Language Models. It solves the problem that existing studies lack a unified way to test MLLMs’ understanding, generation, and mixed-modality abilities. MME-Unify uses data from 12 datasets, unifies task formats (like multiple-choice questions) and metrics, and designs 5 mixed-modality “unify tasks”. The authors tested 12 MLLMs, finding Gemini2.0-flash-exp performs best, but all models still struggle with balance between understanding and generation, and mixed-modality tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is generally well-written and easy to follow.\n2. This benchmarks evaluates the comprehensive abilities of unifed multimodal models, including multimodal understanding, generation, and mixed-modality integration.\n3. The 5 designed “unify tasks” (e.g., drawing auxiliary lines for geometry problems) effectively test how unifed multimodal models combine understanding and generation, targeting their most unique feature."}, "weaknesses": {"value": "1. My biggest concern about this paper is the way the paper evaluates image generation—using CLIP score and multiple-choice questions. CLIP score only checks overall semantic similarity, not whether the generated image truly follows the prompt. SEED-Bench (a 2023 work) adopted this metric for evaluation of unified mutlimodal models, and by 2025, there should be more suitable metrics.\n2. When using multiple-choice questions for image generation evaluation, the paper does not check if the generated image’s details match the prompt—only if it is similar to the option images. This means a model might \"pass\" the test without actually following the task’s requirements.\n3. The paper does not evaluate newer models like Gemini-2.5-Pro or Bagel, so we cannot know how these new models perform on the MME-Unify benchmark."}, "questions": {"value": "A minor issue: The layout of Figure 3 needs to be adjusted. The images in this figure are too small, making it difficult to clearly see the details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ItJmXedMTC", "forum": "7x6TxVIarj", "replyto": "7x6TxVIarj", "signatures": ["ICLR.cc/2026/Conference/Submission4788/Reviewer_QMnN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4788/Reviewer_QMnN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760846268164, "cdate": 1760846268164, "tmdate": 1762917577435, "mdate": 1762917577435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MME-Unify (MME-U), an open and reproducible benchmark targeting Unified Multimodal LLMs (U-MLLMs). It evaluates three capability axes: multimodal understanding, multimodal generation, and “unify” (mixed-modality generation that couples understanding with generation). To enable cross-task comparability, the authors convert understanding tasks into multiple-choice QA and standardize diverse generation metrics onto a common (0,100) scale. A key contribution is five unify subtasks—Image Editing & Explaining, Common-Sense QA + image generation, Auxiliary Lines for geometry, Spot-the-Difference, and Visual Chain-of-Thought (maze navigation)—intended to measure how understanding and generation reinforce each other. The benchmark aggregates 12 datasets into 30 subtasks and evaluates 22 models (open/closed, understanding/generation specialists, and U-MLLMs). Results show that even the strongest systems struggle on unify tasks and that instruction following and visual detail alignment remain open problems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Clear problem motivation**: The benchmark directly targets the unique mixed-modality generation capability that prior benchmarks do not quantify. The unify tasks are not a simple concatenation of existing tasks but require genuine coupling of understanding and generation.\n\n**Breadth and coverage**: The suite spans single/multi-image and video inputs, plus text-to-image/video, editing, image-to-video, and prediction on the generation side. Table 1 convincingly positions MME-U as broader than existing benchmarks.\n\n**Transparent pipeline and reproducibility**: The attribute unification pipeline, domain-wise metrics with standardized scoring, rule-based output matching, option randomization, and model capability adapters (e.g., key-frame sampling for video) make the evaluation reasonably clear and replicable."}, "weaknesses": {"value": "**Evaluation robustness (core concern)**: The scoring relies heavily on CLIP-based similarities and hand-crafted negative samples for both generation and unify tasks. This risks score hacking via feature-space proximity or style artifacts, and may not reflect human judgments on aesthetics, editing faithfulness, or geometric correctness. While the paper acknowledges this limitation, the current version lacks human evaluation or adversarial analyses to quantify the bias or alignment with human preferences.\n\n**Unify task share vs. weighting**: Unify subtasks comprise 546 QA items out of 4,104 total (≈13%) but constitute one of the three equally weighted top-level scores (≈33% of the final MME-U score). This mismatch can increase variance and reduce reliability, especially given the small per-subtask sizes (e.g., AL=52, VCoT=90). The paper does not report confidence intervals or stability analyses to mitigate this concern.\n\n**Diagnostic resolution on challenging unify tasks**: The authors provide stepwise accuracies for VCoT (action/coordinate/image), which is helpful, yet overall success is near zero. Without difficulty stratification (maze sizes/horizons) or milestone scoring, it remains hard to pinpoint where reasoning breaks down and to guide targeted improvement.\n\n**Missing baselines for calibration**: No random-guess baselines (e.g., 25% for 4-way MCQ), simple heuristic baselines, or human upper bounds are reported. This makes it harder to interpret absolute scores."}, "questions": {"value": "**On the Trustworthiness of Scores: How do you prevent \"benchmark hacking\"? ** Your evaluation relies heavily on proxy metrics like CLIP scores and handcrafted negative sample matching. \n\nWhat evidence can you provide that a high score on MME-U strongly correlates with the generation of high-quality, high-fidelity outputs that strictly follow complex instructions?\n\nHave you considered incorporating evaluation dimensions that are harder to \"hack,\" such as human preference scores or more robust automated metrics (e.g., object-level editing consistency checks), to anchor your benchmark in real-world performance?\n\n**On the Diagnosability of Failures: How do your results help us locate and fix problems?**  A benchmark should not just assign a score; it should provide a diagnosis. Currently, a 0% success rate on a task like VCoT tells us that \"all models fail,\" but it offers little insight into where they fail in the process, limiting its value for model developers seeking actionable feedback. Could you introduce milestone-based scoring for multi-step tasks like VCoT, reporting metrics such as \"path recognition accuracy for step 1\" and \"visualization quality for step 1\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KStX51BX7R", "forum": "7x6TxVIarj", "replyto": "7x6TxVIarj", "signatures": ["ICLR.cc/2026/Conference/Submission4788/Reviewer_VRFA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4788/Reviewer_VRFA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830799630, "cdate": 1761830799630, "tmdate": 1762917577221, "mdate": 1762917577221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MME-Unify (MME-U), the first open benchmark for Unified Multimodal Large Language Models (U-MLLMs). It evaluates U-MLLMs across three core capabilities: understanding, generation, and hybrid (Unify) multimodal generation. MME-U integrates 12 datasets, standardizes conventional tasks, and designs five unique hybrid generation sub-tasks (e.g., Visual Chain-of-Thought and geometry problem-solving with auxiliary lines). Evaluations on 12 U-MLLMs reveal that current models exhibit significant deficiencies in balancing different capabilities, following complex instructions, and performing multi-step unified tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) MME-U is the first benchmark for standardized unified multimodal generation tasks (unify capability). These tasks require models to collaboratively integrate reasoning with multimodal outputs, which is quite novel and also fills certain gaps in existing evaluations.\n(2) MME-U provides a unified scoring framework for comprehension, generation, and unified tasks. It standardizes complex metrics to a [0, 100] scale, offering an intuitive and comparable overall MME-U score."}, "weaknesses": {"value": "(1) In the unified tasks, image evaluation relies on multiple-choice questions based on CLIP similarity, which may allow models to exploit the evaluation. This simplification reduces the rigor in assessing generation quality and precise adherence to instructions. Additionally, if the options are very similar, such an evaluation may also be inaccurate.\n(2) On multi-step reasoning tasks like Visual CoT, none of the models succeeded, with Acc results at 0%. This indicates that the task is overly difficult and may not provide discriminative evaluation value."}, "questions": {"value": "Given the potential risk of exploitation in CLIP-I-based evaluation, do the authors plan to combine text generation metrics or use an LLM judge to perform a holistic assessment of generated images and text, in order to achieve a more rigorous evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JnmvRbCWSZ", "forum": "7x6TxVIarj", "replyto": "7x6TxVIarj", "signatures": ["ICLR.cc/2026/Conference/Submission4788/Reviewer_uqqm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4788/Reviewer_uqqm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869695220, "cdate": 1761869695220, "tmdate": 1762917576831, "mdate": 1762917576831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MME-Unify (MME-U), a new and comprehensive benchmark designed to evaluate Unified Multimodal Large Language Models. The authors identify a critical gap in existing evaluations: the lack of a standardized benchmark that assesses comprehension, generation, and mixed-modality generation capabilities simultaneously. MME-U addresses this by integrating tasks into three domains: (1) Multimodal Understanding, (2) Multimodal Generation, and (3) a novel set of unify tasks. The Unify tasks are specifically designed to test how a model's understanding and generation capabilities can mutually enhance each other, featuring five new subtasks like Visual CoT and Auxiliary Lines . The authors evaluate 12 unified MLLMs and find that current models have significant room for improvement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper's primary contribution is the introduction of five unify subtasks. This is the first standardized benchmark designed to rigorously assess the synergistic mixed-modality generation capabilities of U-MLLMs (e.g., reasoning before drawing), which has been a major gap in the field.\n- The benchmark curates and unifies a wide array of tasks from 12 existing datasets for understanding and generation. By reformatting all understanding and unify tasks into a standardized multiple-choice format and normalizing generation metrics, MME-U provides a consistent and reproducible framework for model comparison.\n- The evaluation of 22 models provides a clear snapshot of the current U-MLLM landscape. The results effectively highlight the performance gap between models and demonstrate that even top-performing models struggle with complex instruction following and multi-step unified tasks ."}, "weaknesses": {"value": "- The main weakness is the evaluation strategy for image generation in the \"Unify\" tasks. Using CLIP-I similarity to match a generated image against multiple-choice image options can be hacked. \n- While the Unify tasks are novel, the benchmark's Understanding and Generation sections are primarily a \"benchmark of benchmarks,\" curating tasks from many existing sources. This makes the overall contribution feel more incremental than groundbreaking.\n- The paper reports findings but fails to sufficiently discuss how its conclusions diverge from or challenge those of existing benchmarks. The analysis would be significantly stronger if it highlighted unique insights or model ranking differences revealed only by MME-U. This discussion is crucial for demonstrating the distinct value of this new comprehensive benchmark.\n- The benchmark appears to rely heavily on synthetic data generation, using models like GPT-4o to create QA pairs, explanations, and negative samples. The methodology for quality control and ensuring these synthetic data points are accurate, unbiased, and sufficiently challenging is not well-detailed, raising concerns about data quality and potential artifacts."}, "questions": {"value": "- The evaluation is missing several key sota models (e.g., Gemini 2.5-Pro, Bagel, Qwen-image). \n- See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8PdwlYlrU8", "forum": "7x6TxVIarj", "replyto": "7x6TxVIarj", "signatures": ["ICLR.cc/2026/Conference/Submission4788/Reviewer_xuiz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4788/Reviewer_xuiz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937518440, "cdate": 1761937518440, "tmdate": 1762917576472, "mdate": 1762917576472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summarization of the Responses"}, "comment": {"value": "Thank you for reviewing our submitted manuscript, \"MME-Unify: A Comprehensive Benchmark for Unified Multimodal Understanding and Generation Models.\" We sincerely appreciate the reviewer's insightful and constructive comments and suggestions. We believe that by addressing every issue raised by the reviewers, the quality and clarity of MME-Unify have been significantly enhanced. The main feedback and our corresponding actions are summarized below:\n\n* (1) We supplemented the justification for the rationality of the CLIP-I evaluation strategy and added consistency experiments comparing it with human assessment and LLM-Judge evaluation. (See Reviewer xuiz.W1, Reviewer uqqm.W1, Reviewer VRFA.W1&Q1&Q2, Reviewer QMnN.W1&W2)\n* (2) We supplemented the detail of data filtering process to ensure the quality of the final dataset. We also added Split-Half experiments to verify the stability and challenging nature of our data. (See Reviewer xuiz.W4)\n* (3) We further supplemented the difficulty stratification and diagnostic analysis of the experimental results for the VCoT task. (See Reviewer uqqm.W2, Reviewer VRFA.W3)\n* (4) We systematically clarified the differences from and contributions over existing multimodal benchmarks, along with the unique insights derived compared to existing benchmarks. (See Reviewer xuiz.W2&W3)\n* (5) We supplemented the evaluation results using LLM-Judge based metrics and additional evaluation dimensions. (See Reviewer uqqm.Q1, Reviewer VRFA.Q3, Reviewer QMnN.W1)\n* (6) We discussed the sample size, weight settings, and evaluation robustness for the unified tasks. (See Reviewer VRFA.W2)\n* (7) We supplemented the random guess baseline and human-set ceiling to help calibrate the interpretability of absolute scores. (See Reviewer VRFA.W4)\n* (8) We expanded and updated the evaluation of recent SOTA models, analyzing the unified capabilities of the latest U-MLLMs. (See Reviewer xuiz.Q1, Reviewer QMnN.W3)\n* (9) We improved the presentation of figures and tables to enhance the paper's readability. (See Reviewer VRFA.Q1)"}}, "id": "K7LsvpKIjD", "forum": "7x6TxVIarj", "replyto": "7x6TxVIarj", "signatures": ["ICLR.cc/2026/Conference/Submission4788/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4788/Authors"], "number": 31, "invitations": ["ICLR.cc/2026/Conference/Submission4788/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763739219960, "cdate": 1763739219960, "tmdate": 1763739219960, "mdate": 1763739219960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}