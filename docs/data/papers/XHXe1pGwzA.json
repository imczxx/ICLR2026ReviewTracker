{"id": "XHXe1pGwzA", "number": 21917, "cdate": 1758323548514, "mdate": 1759896896077, "content": {"title": "Correction of Decoupled Weight Decay", "abstract": "Decoupled weight decay, solely responsible for the performance advantage of AdamW over Adam, has long been set to proportional to learning rate $\\gamma$ without questioning. Some researchers have recently challenged such assumption and argued that decoupled weight decay should be set $\\propto \\gamma^2$ instead based on orthogonality arguments at steady state. To the contrary, we find that eliminating the contribution of the perpendicular component of the update to the weight norm leads to little change to the training dynamics. Instead, we derive that decoupled weight decay $\\propto \\gamma^2$ results in stable weight norm based on the simple assumption that updates become independent of the weights at steady state, regardless of the nature of the optimizer. As an example, we generalize our findings to constrained Scion and show that decoupled weight decay $\\propto \\gamma^2$ leads to stable weight and gradient norms and improved model performance.", "tldr": "Decoupled weight decay proportional to LR^2 results in stable weight & grad norms for both AdamW and Scion.", "keywords": ["AdamW", "weight decay", "Scion"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/59a7108958260863d72a9c207fefcbd2e214a570.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the effects of different forms of decoupled weight decay. Similar to recent work on weight decay (Defazio 2025) they argue that in order to keep the weight norms and gradient norms stable throughout training, the total strength of weight decay has to be proportional to the square of the learning rate. They use a different argument than prior work but arrive at the same form."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* Experiments on neural networks of a decent scale (ViT-S/16 on ImageNet, 124M parameter language model).\n* Manuscript is decently written."}, "weaknesses": {"value": "* I believe this paper is largely built upon some fundamental misunderstanding of prior literature. The update and the weights are not orthogonal when momentum is used, even for a random walk. Prior works (e.g. Kosson 2024) do not claim this. The fact that eliminating the perpendicular contribution of the update (after momentum is applied) doesn't affect the dynamics of the weight norm is not surprising if one understands this.\n* The notation is somewhat messy and inconsistent. For example it is not clear which symbols are vectors and which are scalars. A symbol^2 is used to both refer to an elementwise square and apparently the square vector norm.\n* The experimental comparison is insufficient. Comparing two optimizer forms at a single arbitrary hyperparameter configuration does not tell us anything definitive about their performance.\n* The generalization to Scion feels like a minor contribution. The final form is the same as for other optimizers and the analysis does not cover the matrix orthogonalization case which would be the most theoretically interesting portion. It is also not justified well why this matters as Scion is largely insensitive to the gradient norm and the overall phenomenon is already known."}, "questions": {"value": "Further clarifications of weaknesses:\n* The update IS NOT perpendicular to the weights when momentum is used, even when the gradients are completely independent from the weights or when normalization layers are used. This is falsely claimed multiple times throughout the paper including L46, L78, L140. A gradient $g_{t-k}$ is included both in the weights $w_t$ at time $t$ and in the update due to the momentum, causing correlation.\n* Kosson 2024 does not look at the update, seemingly for this specific reason. Rather they use the total update contribution of a gradient, which is independent from the current weights and/or orthogonal due to the use of normalization layers.\n* Notation. L32 uses $g_t^2$ to presumably refer to the elementwise square. L42 uses $\\theta_t^2$ to refer to $\\|\\theta_t\\|^2$. Algo1-L14 seemingly subtracts a scalar from a vector. Algo1-L9 has an extra parenthesis. I suggest using bold symbols for vectors and denoting norms with $\\|\\cdot\\|$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sy6E7TBcWv", "forum": "XHXe1pGwzA", "replyto": "XHXe1pGwzA", "signatures": ["ICLR.cc/2026/Conference/Submission21917/Reviewer_M9jm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21917/Reviewer_M9jm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760533493048, "cdate": 1760533493048, "tmdate": 1762941981323, "mdate": 1762941981323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenge the common practice in decoupled weight decay for optimizers like AdamW. Authors argue that instead of setting weight decay proportional to learning rate γ without question, it should be set to $\\propto\\gamma^2$ based on some orthogonality at steady state, but they find that perpendicular component have little effect. Instead, they derive stable weight norm assuming updates independent of weights at steady state. They generalize to Scion optimizer and propose ScionC with corrected decay $\\propto\\gamma^2$, showing stable norms and better performance in experiments on NanoGPT and ViT."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The derivation of steady-state weight norm is simple and general, not relying on specific optimizer details. It apply to many like SGDM, Lion, and Scion, which is nice.\n\n2. Experiments show clear improvement: lower val loss in NanoGPT (2.838 vs 2.846) and more stable norms. For ViT, AdamC and ScionC have better accuracy at longer epochs, like 77.9 vs 77.4 at 300ep for AdamC.\n\n3. Challenging prior works like Kosson et al. and Defazio with \"Renormalized\" AdamW experiment is good, showing negligible difference, cast doubt on geometry arguments.\n\n4. Reformulation of Scion in terms of η, γ_l, λ_l make it easier to understand role of decay."}, "weaknesses": {"value": "1. The assumption of independence at steady state seem strong. In practice, for short training like ViT 30-90ep, model may not reach steady state, as authors note AdamC not steady even at 300ep. How robust is this?\n\n2. Experiments are limited: only small models (124M NanoGPT, ViT-S), no large scale like Llama. Also, why double λ for AdamC/ScionC? Feel like hyperparameter tuning favor the proposed method.\n\n3. No ablation on momentum scheduling, just fixed α=0.1. Prior work like Pethick show momentum important for Scion, maybe correction interact with it.\n\n4. Some derivations ignore higher order terms (O(η²)), but for large η it might matter? Not tested."}, "questions": {"value": "1. Why not test on larger models or other optimizers like Lion?\n2. In ScionC alg, how determine if E[<θ, u>]=0 for each layer? You say except output, but is this always true?\n3. Any downside to corrected decay, like slower convergence early?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ioBXdiDhPV", "forum": "XHXe1pGwzA", "replyto": "XHXe1pGwzA", "signatures": ["ICLR.cc/2026/Conference/Submission21917/Reviewer_zgL2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21917/Reviewer_zgL2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725178156, "cdate": 1761725178156, "tmdate": 1762941980852, "mdate": 1762941980852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the dynamics of weight norms for LLM training and the impact of weight decay hyperparameters. It proposes a weight decay correction for Scion that improves performance for standard language and image model training tasks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The method proposes a weight decay correction for Scion, which seems to improve its performance. Despite its rather confusing presentation, the correction itself comes down to a rather simple correction for lerning-rate scheduling which was proposed before for AdamW by Defazio et al., 2025."}, "weaknesses": {"value": "* It remains unclear what is exactly the goal of this paper, and how the findings are new compared to previous works. To quote from the conclusion:\n> A priori there is no reason to believe that it is helpful to induce rapid changes of weight and gradient norms with hyperparameters, intentional or not, and we have not seen evidence to the contrary. [...] weight decay should default to $\\gamma^2$.\n\nHow is this finding any different/new to the ones from for example Defazio et al., 2025?\n\n* For the proposed correction of Scion, it remains unclear what is exactly the goal of the correction, and why exactly this correction is proposed. \nAfter some simplification of Algorithm 2 for the setting of the experiments (alpha_t is constant), it basically comes down to correcting $\\lambda_{t,l} = \\lambda_{0,l} * \\gamma_{t,l} / \\gamma_{max,l}$ for the LR schedule. Altogether, it seems that this is simply the idea of Defazio et al., 2025 applied to Scion. Looking at the experiments in Defazio et al., 2025, this also explains the slightly faster convergence of the corrected method.\n\n* It should be noted that when $\\alpha_t$ is constant, then all momentum-related terms in the correction cancel out. No experiments with time-dependent momentum are provided, which seems odd given the strength of claims of the conclusion (*\"can be considered a major step in resolving their interactions\"*)\n\n* The paper contains many mathematical derivations, where it is unclear if they follow from some assumptions made previosuly, or whether they are true in general. Further, for some arguments, no proofs/derivations are made, and hence it is rather unclear why/when these arguments can be applied due to the lacking mathematical rigor. A list of examples below:\n\n1) Line 045: why is E[u_t^2] constant for large t? Why are $\\theta_{t-1}$ and $u_t$ independent (or is this an assumption)?\n\n2) How exactly is steady-state defined? Is this an asymptotic argument for $t\\to \\infty$? In this case, how does it not conflict with the fact that practical training runs have a finite-horizon LR schedule, and so training always stops after finitely many steps anyway?\n\n3) In Algorithm 1, line 14 the nominator subtracts a scalar $u_{t,l||}$ from a weight matrix/vector. How is this possible? Also, it uses sometimes absolute value notation for a vector, and sometimes norm notation. Please clarify this.\n\n4) line 187: besides the fact that this jumps from Scion to AdamW notation again, how exactly do you arrive at the fact $E[\\langle u_{t-1}, u_t\\rangle] = 0$?\n\n5) line 189: how can the vector $m_{t,l}$ \"decay\"? Does this mean its norm decays, and if yes, to what value, and based on which assumptions?\n\n6) Algorithm 2, line 13: if $t$ is the iteration counter, how should one read $t \\to \\infty$? How can we compute the expectation in the if-clause in practice?\n\n7) line 307: \"For the purpose of our experiments, we believe [...]\". What do you mean with *believe*? Is this an assumption or an observation from the experiment?\n\n\n**In summary**, the contributions of this paper seem too incremental to recommend acceptance, as the main idea is to transfer a technique from previous work to a different base optimizer.\nFor future submissions, I would strongly recommend the authors to revise the motivation and derivations in terms of mathematical rigor."}, "questions": {"value": "* Line 098: \"we expect this change to be significant\". I did not understand why this renormalization step should have a big impact: if the projection term is small, then it just multiplies with the ratio of norms before and after the usual update of a single step, which should be close to one (in line with the steady-state weight norm assumption). Could the authors clarify what the goals of this experiments are, and what exactly is the conclusion from it and why?\n\n* When reusing the optimal LR from Scion, but extending training length and halving the batch size, this does not longer ensure that the learning-rate for the baseline is chosen optimally. Did you test whether the improvement could be due simply to suboptimal baseline tuning (e.g. by running exactly the same setup as in the Scion paper)?\n\nMinor:\n\nTypo in Alg 2, line 18: should be $\\lambda_{t,l}$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GjOuH8qHMm", "forum": "XHXe1pGwzA", "replyto": "XHXe1pGwzA", "signatures": ["ICLR.cc/2026/Conference/Submission21917/Reviewer_wtvG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21917/Reviewer_wtvG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742005319, "cdate": 1761742005319, "tmdate": 1762941980252, "mdate": 1762941980252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, existing insights on decoupled weight decay are challenged. The authors claim that decoupled weight decay proportional to $\\gamma^2$ results in a stable weight norm, independent of the optimizer choice. They then apply this insight to propose a corrected weight decay for the Scion optimizer."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The authors investigate how decoupled weight decay leads to stable weight (and gradient norms). A better understanding of optimization dynamics and training stability, which this paper aims to address, is a relevant and important topic for improving neural network optimization."}, "weaknesses": {"value": "- The writing is convoluted and hard to follow. The main message of the paper, including the title, is unclear and potentially misleading, leaving it ambiguous whether the main contribution is the proposition of a weight-corrected Scion optimizer or the claimed new insights about weight decay. The main section, however, focuses on the Scion optimizer. \n- Claims in the introduction appear to be misleading. In line 096, the authors state: “If the scalar projection $u_{t\\parallel}$ is small or zero and the subsequent balanced rotation (Kosson et al., 2024) [...] are important to the training dynamics, we expect this change to be significant.” However, *balanced rotation* in Kosson et al. is a concept of update speed compared across neurons and does not appear to relate to the parallel/perpendicular decomposition the authors use.\n- The authors' claim that the perpendicular component's effect on the weight norm is insignificant is poorly validated experimentally. Particularly, they do not explicitly investigate different cases, such as when the weight norm is small and the perpendicular update is large.\n- The claim of *improved model performance* is based on insufficient experimental evidence, i.e., they show a single run on ViT-S/16 on ImageNet and a Modded-NanoGPT run on Fineweb-edu-100B. For the latter, they do not include downstream evaluation to show how lower validation perplexity translates to downstream performance.\n- Kosson et al. is cited several times as important context, yet the authors do not properly discuss their own work against it in the related work section."}, "questions": {"value": "- Could the authors please clarify the primary contribution of this work? The current framing makes it difficult to assess the main claim.\n- Could the authors please address their potential misinterpretation of the *balanced rotation* concept from Kosson et al. (2024), which appears to be a cross-neuron phenomenon, and explain why this work was not discussed in the related work section?\n- Regarding Section 1.1: Can the authors provide more evidence that the $u_{t\\perp}$ component is *insignificant*? Have they tested other architectures or, as suggested in the review, specific regimes where this component might dominate (e.g., small weight norms and large updates rates)?\n- Could the authors provide more evidence for improved model performance, including downstream evaluation of LLM pretraining tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "da5VP4S6Lq", "forum": "XHXe1pGwzA", "replyto": "XHXe1pGwzA", "signatures": ["ICLR.cc/2026/Conference/Submission21917/Reviewer_hUyc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21917/Reviewer_hUyc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998080714, "cdate": 1761998080714, "tmdate": 1762941979680, "mdate": 1762941979680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}