{"id": "lRBfkbrLzu", "number": 21475, "cdate": 1758317984167, "mdate": 1763144913882, "content": {"title": "Beyond the Efficiency-Performance Trade-off: Semantic Foundation Attention", "abstract": "The quadratic computational complexity of self-attention presents a significant challenge for scaling Transformer architectures to longer sequences. While existing approaches pursue efficiency through sparse approximation or hardware optimization, they operate under the assumption that the input token sequence remains immutable. We propose Semantic Foundation Attention (SFA), which introduces semantic reconstruction—a paradigm that dynamically reconfigures the computational structure based on semantic relationships during attention computation. SFA employs two complementary strategies: similarity merging consolidates semantically aligned tokens through vector addition to preserve and amplify signal strength, while difference merging exploits orthogonality properties in high-dimensional embedding spaces to efficiently integrate complementary information. We implement custom CUDA compute kernels for SFA that decompose the generated dynamic attention patterns into diagonal and rectangular computation domains, enabling efficient execution without explicitly storing the sparse matrix. Comprehensive evaluation on OLMoE architectures demonstrates that SFA consistently improves performance across multiple downstream benchmarks while reducing computational requirements. These results show that computational efficiency and model performance can be jointly optimized through semantically-aware attention computation, establishing semantic reconstruction as a viable paradigm for attention mechanism design.", "tldr": "", "keywords": ["Semantic Foundation Attention", "Attention Mechanism", "Computational Efficiency", "Semantic Reconstruction", "Token Merging"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f05d50d1c22df9afb381c8d44a5d95119ee3c572.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "While this paper introduces an interesting concept—\"semantic reconstruction\" via dynamic token merging integrated within the attention mechanism—and demonstrates impressive engineering with a custom CUDA implementation, its overall contribution is incremental and the experimental evaluation is insufficient to support its claims in the current landscape of efficient Transformer architectures. The work positions itself as a paradigm shift beyond the efficiency-performance trade-off, yet it largely boils down to a sophisticated token merging technique combined with an efficient implementation similar to FlashAttention. The evaluation framework is missing critical comparisons against state-of-the-art hardware-efficient baselines and fails to benchmark on established long-context evaluations, making it difficult to assess the true value and novelty of the proposed Semantic Foundation Attention (SFA)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Novel Integration of Merging and Attention:** The core idea of performing semantic merging *during* the attention computation, rather than as a static preprocessing step, is compelling. The proposed similarity and difference merging strategies, which leverage the geometric properties of the embedding space, are theoretically sound and offer a more nuanced approach than simple averaging.\n* **Significant Engineering Effort:** The development of custom CUDA kernels that decompose the dynamic sparse attention pattern into diagonal and rectangular computation domains is a non-trivial engineering achievement. This allows the method to avoid explicit sparse matrix materialization and leverage the dense compute capabilities of GPUs, which is crucial for achieving practical speedups."}, "weaknesses": {"value": "1.  **Insufficient and Outdated Baseline Comparisons:** The paper's central weakness is its failure to compare against the current state-of-the-art in hardware-efficient attention mechanisms. While comparing against FlashAttention is necessary, it is far from sufficient. FlashAttention optimizes the *exact* full attention computation, whereas a large body of recent work has demonstrated that moving beyond the quadratic-cost paradigm entirely yields superior efficiency. The authors claim SFA is a new paradigm, yet they fail to benchmark against the true contemporary paradigms:\n* **Linear Attention & State Space Models (SSMs):** The field has seen significant advances from models with linear or near-linear complexity that are highly optimized for hardware. Prominent examples include **Gated Linear Attention (GLA) [1]**, **DeltaNet [2]**, and the highly influential **Mamba** architecture (including its recent iteration, **Mamba-2**) [3]. These models excel at long-sequence modeling with impressive hardware efficiency and have become the de-facto baselines for any new efficient architecture. The absence of any comparison to this class of models is a major omission.\n    * **Relevance:** These architectures challenge the very premise that attention, even sparse attention, is the only path forward. A thorough comparison is needed to understand where SFA truly stands. Does it offer better performance for its complexity class? Is it faster for a given level of quality than Mamba-2 or GLA on modern hardware? The paper provides no answers.\n\n2.  **Lack of Robust Long-Context Benchmark Evaluation:** For a method that claims to solve the scaling challenges of attention for longer sequences, the evaluation on long-context capabilities is inadequate. The maximum sequence length tested is 8192, and the downstream tasks are standard NLP benchmarks, not ones specifically designed to test long-range dependency modeling.\n* **The Ruler Benchmark:** A standard and expected benchmark for such a paper would be **Ruler** [4], which is specifically designed to evaluate a model's performance on tasks requiring long-range reasoning and understanding (e.g., variable tracking, multi-document question answering). Without results on Ruler, the claims about SFA's effectiveness on \"longer sequences\" remain unsubstantiated. It is unclear if the semantic merging of adjacent tokens can even preserve the long-range information necessary to succeed on these tasks.\n\n3. **Limited Scope of Merging Strategy:** The proposed merging mechanism is restricted to adjacent tokens only. This is a significant limitation, as semantic redundancy and relationships in language are often non-local. This myopic approach may be effective for compressing localized, repetitive patterns but is likely to fail at capturing more complex, long-distance dependencies, which is precisely the key challenge in long-sequence modeling. This further raises doubts about its potential performance on benchmarks like Ruler.\n\n4.  **Unexplored Synergy with Other Efficiency Techniques:** The paper misses the opportunity to discuss and explore the compatibility of SFA with other orthogonal optimization techniques.\n    * **Quantization:** For instance, hardware-efficient quantization methods are critical for deployment. Could SFA be integrated with something like **SageAttention** [5], which uses data-free quantization and hardware-friendly matrix factorization to accelerate attention? Exploring such synergies would significantly strengthen the paper's contribution by positioning SFA within a practical, end-to-end optimization pipeline. The current work exists in a vacuum, ignoring these crucial aspects of model efficiency.\n\n\n[1] Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. Gated linear attention transformers with hardware-efficient training. ICML 2024.\n\n[2] Yang, S., Kautz, J., & Hatamizadeh, A. Gated delta networks: Improving mamba2 with delta rule. ICLR 2025.\n\n[3] Dao, T., & Gu, A. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. ICML 2024.\n\n[4] Zhang, J., Wei, J., Huang, H., Zhang, P., Zhu, J., & Chen, J. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. ICLR 2025.\n\n[5] Hsieh, C. P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia, F., ... & Ginsburg, B. RULER: What's the Real Context Size of Your Long-Context Language Models? COLM 2024."}, "questions": {"value": "1.  Could you justify the decision to omit comparisons to prominent linear-complexity architectures like Mamba-2, GLA, or other SSM-based models? How do you hypothesize SFA would perform in terms of speed and accuracy against these models on a long-context benchmark like Ruler?\n2.  The core mechanism relies on merging adjacent tokens. Have you considered the impact this has on tasks requiring the preservation of precise positional information or long-range dependencies? Does your method risk losing critical information that non-local attention mechanisms would otherwise capture?\n3.  The performance gains of SFA are highly dependent on the learned compression ratio. What is the typical range of compression ratios you observe on standard language corpora, and how much does this vary across different domains (e.g., narrative text vs. source code)? Is there a risk of negative speedup (i.e., being slower than FlashAttention) on dense, non-redundant text?\n4.  How do you see SFA co-existing with other efficiency methods? For example, could the merging logic be beneficially combined with hardware-aware quantization techniques to achieve compound gains in performance?\n\nIn summary, while the paper presents a well-engineered solution to a known problem, it feels like an incremental improvement over existing sparse attention and token merging ideas. The evaluation is not comprehensive enough to support the strong claims being made, and the work fails to engage with the most relevant and recent advancements in efficient sequence modeling. Without addressing these major gaps, the paper is not ready for acceptance at ICLR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ugn1GLQoBt", "forum": "lRBfkbrLzu", "replyto": "lRBfkbrLzu", "signatures": ["ICLR.cc/2026/Conference/Submission21475/Reviewer_Bs6a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21475/Reviewer_Bs6a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760511929624, "cdate": 1760511929624, "tmdate": 1762941796723, "mdate": 1762941796723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Semantic Foundation Attention, a attention mechanism that aims to simultaneously improve computational efficiency and model performance by introducing semantic reconstruction within the attention computation process. Instead of treating token sequences as immutable, SFA dynamically merges semantically similar or complementary tokens during attention calculation using two strategies: similarity merging  and difference merging."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors provide detailed theoretical justification, mathematical derivations, and a full CUDA implementation, including forward and backward passes and kernel fusion details, demonstrating engineering depth.\n\n2. SFA is designed to be orthogonal and compatible with other efficiency-oriented attention variants (e.g., Linformer, Performer, FlashAttention), which increases its integration potential in large-scale systems."}, "weaknesses": {"value": "1. The paper is verbose and sometimes repetitive, with many long theoretical sections that could be condensed. Figures are not vectorized, resulting in poor visual clarity. A more concise and better-edited presentation would improve readability.\n\n2. Although the method is termed “semantic foundation,” no clear semantic analysis or visualization (e.g., clustering, attention heatmaps, interpretability metrics) supports that the merges indeed reflect semantic structure.\n\n3. While the framing as “semantic reconstruction” is interesting, most of the actual mechanisms (vector addition, orthogonal merging, cosine-similarity gating) resemble known techniques in token merging and compression. The novelty lies mainly in terminology and integration rather than fundamentally new algorithms."}, "questions": {"value": "Can SFA be integrated with existing sparse attention libraries (xFormers, FlashAttention-2) without significant overhead?\n\nIs there empirical evidence that the difference, merging mechanism truly captures orthogonal or complementary semantics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5spECT9u3m", "forum": "lRBfkbrLzu", "replyto": "lRBfkbrLzu", "signatures": ["ICLR.cc/2026/Conference/Submission21475/Reviewer_TH3W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21475/Reviewer_TH3W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644711993, "cdate": 1761644711993, "tmdate": 1762941796494, "mdate": 1762941796494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose Semantic Foundation Attention, a framework for establishing both performance and complexity gains of the attention mechanism, by following two general principles: (1) similarity merging: consolidate semantically aligned tokens and (2) difference merging: exploit orthogonality properties to integrate complementary information. By aligning similar tokens and decomposing orthogonal tokens, the framework can potentially avoid computing the full attention matrix, instead decomposing the attention matrix into diagonal and block matrices which admit more efficient computation. \n\nThe authors provide experiments, illustrating the performance of SFA versus standard Attention (with the FlashAttention implementation) comparing performance across a variety of benchmarks and model scale. They conduct ablation studies, comparing four model variants: full SFA, similarity-only, difference-only, and FlashAttention baseline. Finally, they provide a computational performance analysis, showing that with stronger compression ratio, SFA outperforms FlashAttention at high compression ratios, but underperforms at low compression ratios.\n\nThe work proposes an interesting new avenue to optimizing the efficiency of attention, and it seems to perform well in some special cases. The paper is fairly well written, and the ideas are well motivated. I lean towards accept."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The idea of merging similar tokens makes intuitive sense, as vectors pointing in similar directions should behave similarly under attention. \n\nThe experiments show that SFA exhibits higher (albeit very marginally higher) performance over many benchmarks, and at high compression ratios, is more efficient than FlashAttention."}, "weaknesses": {"value": "While SFA may function efficiently in special cases, it does not seem to give improved performance in the general case. Looking at the theoretical analysis, if k = n/2, then (n - k)k = n^2 is still quadratic. In practice, what guarantees can you place on compression ratio? If compression ratio is weak in practice, then the overhead of SFA is less efficient than FlashAttention."}, "questions": {"value": "Why should SFA improve model performance at all? You are only compressing token information, so Attention should strictly be less expressive over the compressed tokens. It is understandable that this may improve efficiency, but why should it improve the quality of the model?\n\nIt seems in both similarity and difference merging, you are just adding vectors together. It seems somewhat strange that both cases are treated in the same way. Is the key difference in the objective on which the attention heads are trained? i.e. similarity heads map similar tokens to the same direction, difference heads on the other hand aim for orthogonality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "GZBAdhufjQ", "forum": "lRBfkbrLzu", "replyto": "lRBfkbrLzu", "signatures": ["ICLR.cc/2026/Conference/Submission21475/Reviewer_9Y2t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21475/Reviewer_9Y2t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801756268, "cdate": 1761801756268, "tmdate": 1762941796214, "mdate": 1762941796214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Semantic Foundation Attention (SFA), an attention mechanism that dynamically consolidates tokens based on semantic relationships during computation. The approach employs similarity merging (combining aligned tokens via vector addition) and difference merging (exploiting orthogonality in high-dimensional spaces) to reduce computational complexity from O(n^2d) to O(2nd + (1-p)n^2d), where p is the compression ratio. The authors implement custom CUDA kernels that decompose dynamic attention patterns into diagonal and rectangular computation domains. Experiments on OLMoE architectures demonstrate performance improvements across multiple benchmarks while achieving computational efficiency gains,"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel paradigm: The paper reframes attention optimization from static pattern approximation to dynamic semantic reconstruction, which is new.\n2. Good implementation: The custom CUDA kernels with diagonal-rectangular decomposition demonstrate serious engineering effort to make the approach practical without explicit sparse matrix storage.\n3. Consistent experimental validation: Results across two different scale configurations and multiple benchmarks show improvements."}, "weaknesses": {"value": "1. Incompatibility with key aspects of modern LLM training: The approach has fundamental tensions with current best practices. (1) Causal masking: The paper shows causal attention (Figure 2) but doesn't address how merging token i and i+1 interacts with the causal constraint that token i shouldn't see token i+1. If you merge K_i and K_{i+1} via addition, isn't information from future tokens leaking into past positions? (2) KV caching for inference: Modern LLMs heavily rely on KV caching during autoregressive generation. How does SFA work with KV caching when merge decisions might change as new tokens are generated? Does the cache need to be recomputed? (3) Batching variable-length sequences: Different sequences will have different compression ratios, creating ragged tensors that are inefficient for batch processing. How is this handled?\n\n2. Questionable premise that input sequences are \"semantically flat\": The paper's central motivation claims existing methods treat sequences as \"static, semantically flat collections of tokens\". However, this characterization is misleading. Self-attention explicitly computes semantic relationships through the attention mechanism itself. The attention weights dynamically reflect semantic similarity via Q*K computations. Modern architectures already capture rich semantic structures through multi-layer attention. The paper hasn't demonstrated that the input sequence being \"immutable\" is actually a problem that needs solving. Why is modifying the token sequence during attention fundamentally better than letting attention weights handle semantic relationships? The paradigm shift seems predicated on a strawman characterization of existing methods....\n\n3. The \"semantic reconstruction\" framing may currently presented like lossy compression: Stripping away the terminology, SFA performs lossy compression of the input sequence based on learned heuristics. Lossy compression always involves trade-offs, you're discarding information and hoping the model can compensate. The paper hasn't proven that this particular compression strategy is superior to simpler alternatives like: (1) just training with shorter sequences and relying on the model's learned compression in its representations, (2) using hierarchical position encodings to naturally reduce effective sequence length, (3) applying standard dimensionality reduction techniques to embeddings. Why is dynamically merging tokens during attention better than these alternatives?"}, "questions": {"value": "Please address questions embedded in the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "f4Y2yTfcSl", "forum": "lRBfkbrLzu", "replyto": "lRBfkbrLzu", "signatures": ["ICLR.cc/2026/Conference/Submission21475/Reviewer_VFEA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21475/Reviewer_VFEA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985701009, "cdate": 1761985701009, "tmdate": 1762941795930, "mdate": 1762941795930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}