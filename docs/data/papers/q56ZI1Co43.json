{"id": "q56ZI1Co43", "number": 15991, "cdate": 1758258213730, "mdate": 1759897268501, "content": {"title": "ReVeal: Self-Evolving Code Agents via Reliable Self-Verification", "abstract": "Reinforcement learning with verifiable rewards (RLVR) has advanced the reasoning capabilities of large language models. Howerer, existing methods rely solely on outcome rewards, without explicitly optimizing verification or leveraging reliable signals from realistic environments, leading to unreliable self-verification and limited test-time scaling. To address this, we widen the verification–generation asymmetry by explicitly optimizing self-verification, making it a reliable driver of deeper test-time scaling. We introduce ReVeal, a multi-turn Reinforcement learning framework that evolves code generation through self-Verification and tool-based evaluation. ReVeal structures long-horizon reasoning as iterative generation–verification turns and incorporates TAPO for turn-level credit assignment, fostering the co-evolution of code and test generation. At inference, this strengthened self-verification enables the model to use self-constructed tests and tool feedback to continuously evolve code for 20+ turns on LiveCodeBench despite training on only three. It also significantly improves Pass@k, indicating stronger exploration that expands the reasoning boundaries of the base model. These findings highlight the promise of ReVeal as a scalable paradigm for RL training and test-time scaling, paving the way for more robust and autonomous AI agents.", "tldr": "", "keywords": ["Large Language Model", "Reinforcement Learning", "Code LLM", "multi-turn RL"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/26454bc8fd3c8b63c83c91bd5d0390d887602578.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes ReVeal, a multi-turn RL method for code generation explicitly optimizing for self-verification, i.e., generating unit test cases in this context, for which the authors adapt PPO into TAPO (Turn-Aware Policy Optimization) through deliberate reward shaping for the \"generator\" turn and the \"verifier\" turn. The authors train DAPO-Qwen-32B on a processed subset of TACO and eval on LiveCodeBench (LCB) v6 and CodeContest. The reported Pass@1 = 38.7% on LCB v6 at 25 turns. They also show higher pass@k compared to single-turn RL."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "About the originality, the problem of getting more signal than existing public test cases is crucial to multi-turn code generation in the competitive programming problems domain. The paper proposes to use model to self-generate test cases, and explicitly optimizes for this in the training time using Reinforcement Learning.\n\nTo train a single model end-to-end with the ability of both generating code solutions and unit test cases, the paper details the design of different reward for generation turn and verification turn, which the authors show that it leads to better performance empirically, and its effectiveness is shown via the ablation by comparing to RL training with outcome only reward.\n\nI appreciate the authors' effort on getting results with mean and std deviation with running multiple experiments to reduce the noise of the numbers. Besides reporting the pass rate, the reported metrics of corrections and degradations are also interesting."}, "weaknesses": {"value": "Despite the problem the manuscript tries to tackle and the approach it takes are interesting: the generation-verification gap and the method to train a single model trained for both verification and generation, there're several points that I feel less convincing and the manuscript could be of better form and positioned if these points are properly addressed:\n\n1. Novelty & Contextualization\n\n1.1 I see the core idea of the manuscript as explicitly optimizing self-verification ability, and better turn-level credit assignment, going beyond existing multi-turn RL work RLEF. Therefore, the algorithmic novelty sits largely in the reward shaping design and the TAPO construction, which is consistent with standard PPO variants and MC returns with custom shaping at turn level. \n\nSince the design of TAPO involves multiple component. Ablation of outcome only reward and single-turn RL seems not sufficient for me. These reward shaping necessites better motivation and comparison. The manuscript would benefit from more empirical evidence of the choice of the specific design of reward beyond motivation for justification. To name a few, for example:\n- the format reward and the pass rate reward: is the format reward shaping essential and removing it could lead to different performance.\n- generation reward: did the author try settings other than `abs = 0` and `imp = 1`?\n\n1.2 Apart from the concrete algorithmic design. The baseline the paper compares to, ReAct is a prompt-only baseline, CTRL is another approach where the verifier is trained against a fixed generator. If the claimed novelty is to explicitly optimize for self-verification, comparing to stronger baseline or alternative design provides stronger evidence of the proposed framework: For example, it's still not clear the gain of the proposed framework comes from the framework design or the RL loss design, which is not totally decoupled. \n\nThis is my biggest concern when reading the manuscript: it tries to center around these 2 novelties but provides under-substantial ablation to decouple and justify each of the both. For example, for the framework design thing, the manuscript could compare (under the same classical RL loss such as PPO, or turn-level PPO as shown in the previous work RLEF) the following alternative designs that gradually leads to the current design:\n\n- the verification step is coming from a fixed model, similar to https://arxiv.org/abs/2306.09896 or the mirroring setting of CTRL, either from the same starting policy or from another model, while the generation step is normally RL trained.\n- starting from the same starting policy but maintained 2 set of weights separately for generation and verification.\n- starting from the same policy, but does 2 separate RL training update for generation and verification where the loss is masked for one another.\n\nFor another thing, there're quite a few components deviated from the standard PPO/GRPO practice in the reward design, please see above. \n\n2. Limitation\n\nThe propose framework aims at providing more unit test cases to serve as signal in the multi-turn code generation in the competitive programming framework. Wrong code solutions that leads to wrong answer for hidden test cases could benefit from the proposed framework if a verifier produces new test cases. However, another big set of wrong code solutions, which are functionally correct but do not have the desired time/space complexity, could fall out of the catch of the framework: think of a brute force solution where it produces expected output given input. To correct such solution, the verifier needs to have the ability either to produce signal for stats in runtime, or to produce large test cases to hopefully trigger timeout (which might exceed the context length for example). Discussion on whether the framework can properly address such chanllenge or even discussion in a dedicated limitation section is appreciated.  \n\n3. Writing and clarity\n\n- It seems that the \"tool feedback\" refers to solely python execution environment in this paper, which would be good if this could be confirmed.\n- The short-term memeory is only briefly mentioned. I thought it means in the multi turns, it only kept recent history (like most recent 3 turns), but L699 says \"Critical information such as successful patterns, error types, and effective test structures are preserved in complete formats.\" Further explanation or example would be appreciated.\n- The reward design part necessite more clarity. For example, I'm confused since Verification reward is \"For each verification turn k (even), we reward the proportion of generated tests that succeed when executed on a golden code\", which therefore, doesn't depend on the generated code but only on the golden code. Which should make it unhackable, but the paper later says in L222 \"To mitigate adversarial reward gaming (e.g., generating trivial code that hacks the verification reward) ...\". I do not understand how this could happen.\n- Eq (5) seems not quite useful as the manuscript never present the token level reward r_t. The whole reward design makes it look like there're token-level return and turn-level return. If I understand correctly, in the end the return is the same for all tokens in the same turn? Is there case where the return is different within one turn for different token? My understanding for the current moment is that the whole design for return is actually at the turn level, this return is \"densified\" by broadcasting to each token, and the advantage for each token is  this return substract a value baseline.\n- L214 says \"replaces GAE-based advantages with a turn-aware return.\" I do not think the author mean it or otherwise the authors use the term \"advantage\" and \"return\" interchangeably (which I found confusing), since there're still the advantage estimation in Eq(8), where the V I presume is coming from a value model?"}, "questions": {"value": "Please see above for major questions. Also minor question:\n- The number on CodeContests is on validation set or test set? I didn't find such information."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7UViXbZqh4", "forum": "q56ZI1Co43", "replyto": "q56ZI1Co43", "signatures": ["ICLR.cc/2026/Conference/Submission15991/Reviewer_6pm9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15991/Reviewer_6pm9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557639722, "cdate": 1761557639722, "tmdate": 1762926201135, "mdate": 1762926201135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ReVeal, a multi-turn reinforcement learning framework for code generation that explicitly optimizes self-verification alongside code generation. The key innovation is treating verification as an optimization target rather than relying solely on outcome rewards. ReVeal structures long-horizon reasoning as iterative generation-verification loops, where the model generates code, constructs test cases, executes them via external tools (Python interpreter), and uses the feedback to refine solutions across multiple turns. The authors propose Turn-Aware Policy Optimization (TAPO), which assigns credit at both token and turn granularity using joint verifiable rewards (outcome, generation, and verification). Experiments on LiveCodeBench and CodeContests show that ReVeal enables test-time scaling beyond the training horizon and achieves higher Pass@k than baseline methods, suggesting expanded reasoning boundaries."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a genuinely novel perspective on multi-turn code generation by explicitly optimizing verification as a co-equal objective with generation. While prior work has explored critic models or execution feedback, ReVeal's approach of jointly training generation and verification within a single model through structured turn-level rewards is innovative. The TAPO algorithm, though building on PPO, introduces a sensible credit assignment mechanism tailored to the generation-verification paradigm.\n\nThe paper is generally well-structured with clear motivation. Figure 3 effectively illustrates the generation-verification loop and TAPO's reward structure. The case study provides intuitive understanding of how the iterative refinement process works in practice. The distinction between training (with golden solution filtering) and inference (fully autonomous) is clearly articulated."}, "weaknesses": {"value": "The ablation analysis is concerningly limited for a paper making multiple methodological contributions. Table 1 only compares \"outcome reward\" versus \"TAPO with joint rewards\" as a monolithic change, without isolating the contribution of individual components. Critical missing ablations include: (1) What happens with only generation rewards or only verification rewards? (2) How does the specific turn-level return formulation in the equation compare to simpler alternatives? (3) What is the effect of different reward hyperparameters (abs/imp coefficients, the factor of 5 in passrate)? (4) Why is the ablation only shown at turn 8 when main results extend to turn 25? These gaps make it difficult to assess which design choices truly matter.\n\nThe paper claims to \"widen the verification-generation asymmetry\" (Figure 2) but provides only empirical observation without theoretical grounding. Why does TAPO's specific formulation in Equations 6-8 prevent reward hacking? The intuitive explanation that \"generation turns are rewarded solely based on code quality\" is not rigorously proven. What are TAPO's convergence properties compared to standard PPO? The paper would benefit from formal analysis showing that TAPO's credit assignment scheme provably incentivizes the desired co-evolution behavior rather than reward exploitation.\n\nDespite claiming the method is \"applicable to any reasoning task with verifiable rewards\" and \"verification asymmetry,\" experiments are confined entirely to code generation on two similar benchmarks (LiveCodeBench, CodeContests). No evidence supports the broader applicability claim. Even within code generation, evaluation is limited to competitive programming problems—what about software engineering tasks like debugging, refactoring, or API usage? The paper tests only two base models from the same model family (Qwen). Testing on diverse model architectures and problem types would strengthen generalization claims significantly."}, "questions": {"value": "Can you provide complete ablations isolating: (a) generation reward only, (b) verification reward only, (c) outcome + generation, (d) outcome + verification? This would clarify each component's contribution. Additionally, what happens with different reward hyperparameters (the factor of 5, abs/imp coefficients)?\n\nWhat percentage of model-generated tests are filtered during training because they fail on the golden solution? How does this evolve over training? Since filtered tests provide no learning signal, how does the model learn to avoid generating bad tests?\n\nCan you provide theoretical or empirical analysis of TAPO's convergence properties? How does the combination of token-level and turn-level returns (Equation 8) affect optimization stability compared to standard GAE? Have you observed any instabilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qusXUBjWGD", "forum": "q56ZI1Co43", "replyto": "q56ZI1Co43", "signatures": ["ICLR.cc/2026/Conference/Submission15991/Reviewer_beFR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15991/Reviewer_beFR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830487024, "cdate": 1761830487024, "tmdate": 1762926200654, "mdate": 1762926200654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel reinforcement learning framework named ReVeal, which aims to enhance the long-horizon reasoning and self-correction capabilities of Large Language Models (LLMs) in code generation tasks. The core contribution of this work lies in decomposing complex programming competition tasks into an iterative, multi-turn process of \"code generation\" and \"test case generation.\" For this process, the authors designed a joint reward mechanism that includes Turn-Aware Policy Optimization (TAPO) to collaboratively optimize the model's code generation and self-verification abilities, thereby enabling deeper test-time expansion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "• Utilizing reinforcement learning to enhance a model's ability to solve complex problems, self-reflect, and correct code is a crucial research direction.\n\n• The paper is well-structured with a clear and logical flow."}, "weaknesses": {"value": "• The experimental dataset is somewhat limited: The main experiments are based on programming competition-style problems. While this effectively tests the model's algorithmic capabilities, it raises questions about the method's generalizability to broader, more realistic real-world development scenarios.\n\n• Some experimental setups are not sufficiently clear or systematic: The experimental comparison section has issues that could affect the reliability of the conclusions, such as inconsistent evaluation turns and a lack of fairness in baseline comparisons. This makes it difficult for readers to accurately determine the source of the performance improvements."}, "questions": {"value": "To further enhance the paper's persuasiveness and rigor, I suggest the authors consider the following points for revision and supplementation:\n1. Recommendation to Expand Datasets to Verify Generalization\nTo more comprehensively validate the effectiveness and generalizability of the ReVeal framework, I strongly recommend that the authors supplement their experiments on more diverse datasets. For example: Foundational code generation datasets like HumanEval+ and MBPP+. Datasets that are closer to real-world development scenarios, such as CodeUJB or SWE-bench. Experiments on these datasets would demonstrate that ReVeal is not only applicable to algorithmic problems but can also generalize to complex, real-world software engineering scenarios, which would significantly enhance the value of this work.\n2. Recommendation to Systematize Experimental Setup for Fair Comparison\nRegarding the systematicity and fairness of the experimental setup, I have two main concerns:\nUnify evaluation metrics for direct comparison: In the main results table (e.g., Table 1), the evaluation turns should be unified for all multi-turn methods being compared (e.g., reporting Pass@1 at turns 1, 5, and 8 for all methods). This would allow readers to more intuitively see the performance differences between methods at equivalent inference costs.\nConfigure a fair reasoning mechanism for key baselines: Baseline models like DAPO-Qwen2.5-32B and Single-turn RL should also employ a multi-turn feedback mechanism for test-time generation, similar to ReVeal. This would ensure a fair evaluation and help clearly distinguish whether the performance gains come from the RL training itself or from the reflection mechanism.\n3. Report and Align Training Costs\nThe paper lacks a discussion of training costs. I recommend supplementing the paper with data on training overhead, such as the GPU hours for both ReVeal and Single-turn RL. This would not only help readers assess the method's cost-effectiveness but also allow for a fairer measurement of the ReVeal framework's true value and efficiency when compared under a similar training budget."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QrzOJIDj8k", "forum": "q56ZI1Co43", "replyto": "q56ZI1Co43", "signatures": ["ICLR.cc/2026/Conference/Submission15991/Reviewer_jPh8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15991/Reviewer_jPh8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991568025, "cdate": 1761991568025, "tmdate": 1762926199739, "mdate": 1762926199739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Current Reinforcement Learning with Verifiable Rewards (RLVR) only optimizes \"generation capability\" relatively well, while \"verification capability\" is barely explicitly optimized—leading to a significant gap between generation and verification—this paper proposes ReVeal, a multi-turn reinforcement learning (RL) framework for code agents. Within a single trajectory, ReVeal alternates between \"generation → verification → tool feedback\" and, for the first time, places \"self-verification\" and \"generation\" under the same optimization target. In this process, the model not only generates code but also automatically produces executable test cases and leverages feedback from external tools for self-correction. To ensure the balanced improvement of generation and verification capabilities, ReVeal introduces Turn-Aware Policy Optimization (TAPO), which assigns explicit reward signals to both generation and verification actions at each turn. This enables fine-grained credit assignment, thereby simultaneously enhancing these two capabilities during the training phase and narrowing the performance gap between generation and verification. Unlike traditional sparse reward methods that rely solely on the final pass rate, ReVeal incorporates tool-based feedback from interpreters/code judges at each turn. This allows the model to continuously optimize code quality during the inference phase, even when performing a large number of inference turns. Experimental results demonstrate that ReVeal achieves excellent performance on LiveCodeBench, validating the effectiveness of optimizing verification capability on par with generation capability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe ReVeal framework innovatively introduces explicit verification turns during the training process and uses the gold solution to pre-verify the generated test cases. This method enables the model to explicitly learn self-verification capabilities during training, thereby enhancing the reliability of self-verification.\n2.\tReVeal introduces a Turn-level return mechanism: once the generation is correct, the test generator of the previous round can also receive rewards. This design effectively ties the generation and verification processes together, preventing undesirable behaviors such as \"writing useless tests\" and other reward gaming phenomena, thus promoting the co-optimization of the generation and verification processes.\n3.\tExperimental results show that on LiveCodeBench V6, as the number of inference turns increases, the model's code generation capability and performance continue to improve. This verifies the phenomenon that even with short training turns, the model can still perform continuous multi-turn reasoning during inference. The model indeed learns effective self-verification strategies during training, enabling it to continuously improve its performance in the inference phase."}, "weaknesses": {"value": "1.\tThe paper motivates ReVeal as a way to make self-verification reliable “in realistic environments where public tests are unavailable”. During training, however, the model-generated tests are filtered against a golden solution to guarantee high-quality feedback. At evaluation time (e.g., LiveCodeBench) the final correctness is judged by the benchmark’s own test suites, i.e., still under a setting with reliable canonical tests. Thus, the experiments mainly demonstrate that optimizing verification helps on code benchmarks that already provide trustworthy execution oracles, rather than showing robustness in the fully autonomous, low-test-coverage setting highlighted in the introduction. This is not fatal — code is exactly the domain where such oracles exist — but the paper’s claim about “realistic environments without pre-existing tests” would be stronger with a no-filter / noisy-test ablation.\n2.\tCurrent experiments are only conducted on large 32B models, and do not involve the performance of small-scale models such as 7B or 14B, nor sparse models. Considering that small-scale models may face greater challenges in generating test cases, the lack of verification on small-scale/sparse models limits the generalization ability of the paper's conclusions regarding model scale. It is suggested that the authors expand the experimental scope in future research to cover models of different scales, so as to evaluate the applicability of the ReVeal framework on various models.\n3.\tThe paper provides a relatively brief description of the reward mechanism, and does not elaborate on the specific calculation methods and weight settings of format rewards, final pass rewards, turn-level rewards, etc. This may require researchers to adjust these hyperparameters by themselves when reproducing the experiments. It is suggested that the authors provide more detailed reward mechanism design and hyperparameter settings or released the open-source implementation of the ReVeal framework in future research to improve the transparency and reproducibility of the study."}, "questions": {"value": "1.\tThe turn-level return (Eq. 6 in the manuscript) assigns each generation reward to both the generation turn and the preceding verification turn with weight 1.0. This tightly couples the verification reward to the next-turn generation quality, which can increase variance when the next turn underperforms (e.g., due to context truncation or large code edits). Have the authors tried attenuated backflow or exponential decay across later turns to test whether verification still resists reward hacking under weaker coupling? Even a small ablation would clarify whether the current choice of weight 1 is essential or merely convenient.\n2.\tSince the policy is shared between generation and verification, it is hard to disentangle whether the gain comes from (i) better code generation under multi-turn tool feedback, or (ii) explicit optimization of the verification subtask. A simple ablation where the verification head (or the verification part of the loss) is frozen/removed, while keeping the rest of the setup identical, would clarify the contribution of “training verification as a first-class task”. Without such an ablation, the current evidence could still be explained by “multi-turn generation with tool context”.\n3.\tA key claim is that ReVeal, by explicitly optimizing verification, can extrapolate beyond the training horizon (3 turns → 25 turns at inference). This is compelling, but currently only one training horizon is reported. Could the authors report results with a longer training horizon (e.g., 5 turns) and a shorter one (e.g., 1–2 turns) to show (i) whether the benefit saturates, and (ii) whether the ability to extrapolate is truly due to the proposed TAPO design rather than a coincidental choice of 3 turns? This is especially relevant since context is truncated to the last few turns during inference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zNEIYSxQyU", "forum": "q56ZI1Co43", "replyto": "q56ZI1Co43", "signatures": ["ICLR.cc/2026/Conference/Submission15991/Reviewer_utHn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15991/Reviewer_utHn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991959488, "cdate": 1761991959488, "tmdate": 1762926198579, "mdate": 1762926198579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}