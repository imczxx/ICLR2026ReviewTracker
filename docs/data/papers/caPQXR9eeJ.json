{"id": "caPQXR9eeJ", "number": 13782, "cdate": 1758222431656, "mdate": 1759897413077, "content": {"title": "SWE-Refactor: A Repository-Aware Benchmark for Evaluating LLMs on Real-World Code Refactoring", "abstract": "Recent advances in Large Language Models (LLMs) have garnered significant attention for their applications in software engineering tasks. Among these tasks, code refactoring has its own unique challenges. Unlike code generation, refactoring requires precise changes that preserve program behavior while improving structure, making automated evaluation difficult. Existing refactoring benchmarks suffer from three key limitations: (1) they often focus on atomic refactoring types while missing more complex ones; (2) they contain noisy data with entangled, unrelated code changes, making it difficult to study LLM’s true refactoring capability accurately; and (3) they lack code repository and structural information to support realistic evaluations. To address these issues, we propose SWE-Refactor, a new benchmark for LLM-based code refactoring. SWE-Refactor contains 1,099 real-world, pure refactorings collected from 18 real-world Java projects. Each refactoring instance is verified through compilation, test execution, and automated refactoring detection tools to ensure correctness. Unlike prior benchmarks, SWERefactor covers both atomic and compound refactoring types (single and multiple code changes). It includes rich repository-level data (e.g., method callers and callees, class hierarchies), as well as configuration details like test coverage and build settings. We evaluate nine widely used LLMs on SWE-Refactor, including GPT-4o-mini, DeepSeek-V3, and CodeLLaMa. DeepSeek-V3 achieves the best performance with 457 successful refactorings (41.58%), followed by GPT-4o-mini with 438 (39.85%). DeepSeek-V3 performs particularly well on Extract Method, completing 301 cases, while GPT-4o-mini demonstrates stronger performance on more complex refactoring types, such as Move Method and Extract and Move Method. Furthermore, we find that adding retrieval context via few-shot examples and using a multi-agent workflow significantly improve performance, with the multi-agent approach achieving the highest success rate. We release SWE-Refactor and all evaluation results to support future research on LLM-based code refactoring.", "tldr": "SWE-Refactor is a new benchmark addressing key limitations of existing refactoring datasets, enabling robust evaluation of LLMs on atomic and compound refactorings with automated correctness checks.", "keywords": ["Code Refactoring", "Large Language Models", "Software Engineering", "Java Projects"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/53e3a96ffb9dc181b6657af48cd418f310cd54ae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SWE-Refactor, a benchmark for evaluating LLMs on code refactoring using 1099 \"pure\" instances from 18 real-world Java projects. The contributions are ensuring data purity by filtering out non-refactoring changes and rigorously verifying each instance for correctness using the projects' full test suites. The benchmark provides repository-level context and covers both atomic and compound refactoring types. An evaluation of 9 LLMs finds that a multi-agent workflow performs best, but with a modest top success rate of 52.7%."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Originality: The paper addresses a clear gap in existing work. Current refactoring benchmarks are often noisy, lack repository-level context, and are heavily biased toward Python. SWE-Refactor’s focus on pure, verifiable, context-rich refactorings in Java is an original contribution.\n- Quality: Verifying each refactoring via compilation and the full test suite is a significant improvement over prior benchmarks, ensuring models are evaluated on the refactoring task itself.\n- Clarity: The paper is well-written, and the problem, solution, and construction pipeline are all clearly articulated."}, "weaknesses": {"value": "- Limited Scope: The benchmark's primary weakness is its focus on six well-defined, syntactic refactoring types (e.g., Extract Method) that are already reliably automated by modern IDEs. It misses the opportunity to evaluate models on more complex, semantic refactors that lack tool support.\n- Outdated Model Evaluation: For a paper targeting a 2026 conference, the model selection is not representative of the current state of the art. While the evaluation includes 2024 models like GPT-4o-mini and DeepSeek-V3, the rapid advances throughout 2025 have introduced significantly more powerful models (e.g. Claude 4 series, or GPT reasoning models).\n- Lack of Difficulty: Related to the point above, the benchmark's difficulty appears mismatched with current state-of-the-art agentic frameworks. While the paper's top score is 52.7% with a multi-agent workflow, other recent work has shown much higher performance on similar tasks. For instance, the MANTRA multi-agent framework, which is designed for this type of method-level refactoring, reports an 82.8% success rate on a similar \"pure refactoring\" dataset [1]. And the public leaderboard for the popular aider coding tool shows a recent version of Claude 3.5 Sonnet achieving 92.1% accuracy on its refactoring benchmark [2]. This suggests the benchmark may already be largely solved by SOTA agents, limiting its long-term utility for a 2026 conference.\n- Contradictory Scalability Claims: The paper claims its \"fully automated... pipeline\" is an advantage. However, this pipeline produced only 1099 examples. The authors themselves concede this scale is \"still limited\". This small size fails to convincingly demonstrate the claimed scalability.\n\n---\n[1] Xu, Y., Lin, F., Yang, J., Chen, T. H., & Tsantalis, N. (2025). MANTRA: Enhancing Automated Method-Level Refactoring with Contextual RAG and Multi-Agent LLM Collaboration. arXiv preprint arXiv:2503.14340.\n\n[2] Aider. (2025). Refactoring Leaderboard. https://aider.chat/docs/leaderboards/refactor.html"}, "questions": {"value": "1. Could the authors provide results from more recent SOTA models (e.g., GPT-4o, Claude 4.x Sonnet, Gemini 2.5 Pro) and agentic scaffolding (e.g., OpenAI Codex, Claude Code)? This is crucial to demonstrate that the benchmark is not already \"solved\" and remains a useful challenge.\n2. What was the main bottleneck in the \"automated pipeline\" that limited the dataset to 1099 instances? Specifically, what was the filtering rate? (i.e., how many candidate refactorings were discarded by PurityChecker for being \"impure\"?) This would clarify the trade-off being made between purity and scale.\n3. Given that the included refactoring types are largely automated by IDEs, can the authors elaborate on the practical value of having LLMs solve these specific tasks, as opposed to focusing on more complex, semantic refactors that lack any tool support?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mtMbAI8RCL", "forum": "caPQXR9eeJ", "replyto": "caPQXR9eeJ", "signatures": ["ICLR.cc/2026/Conference/Submission13782/Reviewer_5ytC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13782/Reviewer_5ytC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760927908556, "cdate": 1760927908556, "tmdate": 1762924314113, "mdate": 1762924314113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors propose a benchmark for code refactoring. Given that this task is heavily reliant on memory (i.e., structure and class hierarchies in a code repository), as well as inter-component dependencies; it captures a whole different nuance in coding agents (as opposed to bug-fixing as in SWE-Bench et al.). The authors present a methodical approach for data collection, directly addressing the limitations of existing code refactoring benchmarks; while designing a two-agent, iterative feedback loop to incorporate repository awareness. However, the evaluation remains flawed (discussed more in Weaknesses), which fails to capture the true complexity of refactoring quality and behavioral preservation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Code refactoring is a complex SE task that demands systemic understanding of class hierarchies and inter-file dependencies. Through this work, the authors correctly shift coding agent evaluation away towards context-dependent reasoning, which is essential for agent development.\n\n2. The data collection is methodical and significantly improves over the existing refactoring benchmarks, which typically suffer from ambiguous or noisy data."}, "weaknesses": {"value": "1. The Reviewer Agent appears to rely on: static analysis, which cannot verify functional correctness; and pre-existing test suite, which inherently assumes completeness of the test suite. The other metric, CodeBLEU, is in itself brittle.\n \n2. The multi-agent improvements are notable, but the discussion does not analyze why they succeed (e.g., self-critique vs. feedback loops) or whether such workflows generalize across domains. A breakdown of iteration counts or failure recoveries would add clarity.\n\n3. While the methodical data collection is a strength, the benchmark itself is limited to Java; and would greatly benefit from being designed for multiple programming languages."}, "questions": {"value": "1. Beyond eliminating noisy refactoring commits, how did the authors ensure that the original code's test suite, used for validation, is stable and functionally complete?\n\n2. Besides the final correctness and CodeBLEU score, did the authors quantify the efficiency of the iterative process? Specifically, what are the metrics for the token consumption per task and the average number of iterations required to reach a correct solution?\n\n3. What are the technical barriers and necessary tool replacements for adapting SWE-Refactor to a second language?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JUzMwHdLJK", "forum": "caPQXR9eeJ", "replyto": "caPQXR9eeJ", "signatures": ["ICLR.cc/2026/Conference/Submission13782/Reviewer_aF72"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13782/Reviewer_aF72"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963866595, "cdate": 1761963866595, "tmdate": 1762924313613, "mdate": 1762924313613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SWE-Refactor, a new benchmark for evaluating LLMs on real-world Java code refactoring tasks. The benchmark contains 1,099 pure refactorings from 18 Java projects, covering both atomic and compound refactoring types. This paper propose fully automated pipeline for benchmark construction and evaluate 9 popular LLMs, finding that DeepSeek-V3 achieves best performance with 41.58% success rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is well-written and addresses important limitations such as supporting compound refactorings, ensuring pure refactorings without noise, and providing an automated construction pipeline.\n- This method has built comprehensive evaluation metrics and conducted extensive experiments, which provide the community with valuable insights."}, "weaknesses": {"value": "- This benchmark focuses only on Java limits generalizability. While authors justify this choice, it's significant limitation for comprehensive LLM evaluation.\n- This benchmark contains only 1,099 samples across 6 refactoring types, some categories have very few examples, which may raise some biases."}, "questions": {"value": "- How do you ensure RefactoringMiner's 99% precision  translates to correct ground truth, given potential tool errors?\n- Could you provide quantitative comparison of data quality between SWE-Refactor and RefactorBench on overlapping refactoring types?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AB2xKxtmga", "forum": "caPQXR9eeJ", "replyto": "caPQXR9eeJ", "signatures": ["ICLR.cc/2026/Conference/Submission13782/Reviewer_GqSu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13782/Reviewer_GqSu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966663245, "cdate": 1761966663245, "tmdate": 1762924313196, "mdate": 1762924313196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}