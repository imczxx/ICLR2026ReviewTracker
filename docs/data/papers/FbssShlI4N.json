{"id": "FbssShlI4N", "number": 14390, "cdate": 1758234290819, "mdate": 1759897373212, "content": {"title": "FALCON: Few-step Accurate Likelihoods for Continuous Flows", "abstract": "Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann Generators tackle this problem by pairing a generative model, capable of exact likelihood computation, with importance sampling to obtain consistent samples under the target distribution. Current Boltzmann Generators primarily use continuous normalizing flows (CNFs) trained with flow matching for efficient training of powerful models. However, likelihood calculation for these models is extremely costly, requiring thousands of function evaluations per sample, severely limiting their adoption. In this work, we propose Few-step Accurate Likelihoods for Continuous Flows (FALCON), a method which allows for few-step sampling with a likelihood accurate enough for importance sampling applications by introducing a hybrid training objective that encourages invertibility. We show FALCON outperforms state-of-the-art normalizing flow models for molecular Boltzmann sampling and is \\emph{two orders of magnitude faster} than the equivalently performing CNF model.", "tldr": "Few-step Flow Matching with Accurate Likelihoods for Scalable Boltzmann Generators", "keywords": ["Generative Models", "Flow Matching", "Boltzmann Generators", "AI for Science"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/748fcc26e80becee192393ef64fad47d5bec1585.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes to train a mean-flow model with an additional soft constraint on the flow invertibility by optimising the $\\mathcal{L}_{inv}$ (Equation 8). Once the optimum of this inv loss is achieved, the log-prob of a generated sample can be calculated by change-of-variable. The proposed model, FALCON, could generate samples with few network calls while having access to model density, which can be applied as a Boltzmann generator.\n\nThe method is clearly written, and the empirical results show a clear gap between other baselines on either ESS or sample quality related metrics when scaling to complex tasks. However, the main concern lies in the low ESS of FALCON on ALDP, which is significantly lower than one of the baseline ECNF++.\n\nOverall, the reviewer recommends a weak accept."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is clearly written\n\n2. The proposed method is simple and effective\n\n3. The experiments show the scalability of the proposed method, which outperforms other baselines"}, "weaknesses": {"value": "The main weakness of the paper lies in the experimental result on ALDP, where the ESS of FALCON is much lower than that of ECNF++. It would be great to elaborate more on why this happens. On the other hand, to showcase the effectiveness of proposed method, one could use the same network architecture as ECNF++ and then train FALCON. Such setting would be fairer to illustrate the effectiveness of the invertibility loss.\n\nOn the other hand, it is not clear how invertible the trained network is. In particular, the author could elaborate more on the invertibility of $X(x, s, t)$ by using a 2-dimensinonal heatmap. Intuitively speaking, for farer s and t, the invertibility is more difficult to maintain, and therefore increasing the number of NFE could improve ESS."}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g9EEzvKm2U", "forum": "FbssShlI4N", "replyto": "FbssShlI4N", "signatures": ["ICLR.cc/2026/Conference/Submission14390/Reviewer_tZEt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14390/Reviewer_tZEt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760906692872, "cdate": 1760906692872, "tmdate": 1762924805101, "mdate": 1762924805101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FALCON, a framework designed to overcome a long-standing bottleneck of flow matching and related continuous-time generative models: although training is efficient, evaluating accurate likelihoods remains computationally expensive due to the need to integrate the divergence term along the ODE trajectory.\n\nFALCON proposes a theoretically grounded and computationally efficient approach that enables few-step yet accurate log-likelihood estimation by optimizing a surrogate network to approximate the evolution of the log-density under flow dynamics. The method provides an effective approximation that avoids explicit computation of the ODE while maintaining high accuracy in the estimated likelihoods. Theoretical analyses support its consistency and error bounds, and extensive experiments demonstrate that FALCON achieves comparable or better accuracy than full integration methods at a fraction of the cost.\n\nOverall, the paper offers a mathematically elegant and practically useful solution to a central problem in flow-based models."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper addresses a core unresolved issue in the flow-matching and continuous normalizing flow (CNF) literature — accurate and efficient likelihood computation — using a very novel and conceptually clean idea. The formulation of few-step likelihood estimation through continuous optimization is both original and impactful.\n\n2. The theoretical development is rigorous, with solid mathematical grounding that explains why the surrogate optimization scheme yields unbiased or low-bias likelihood estimates.\n\n3. Empirically, the experiments are thorough and convincing, covering a range of flow architectures and datasets. The results consistently demonstrate that FALCON significantly reduces computational overhead without sacrificing estimation accuracy.\n\nIn summary, this contribution is potentially highly influential for both the theoretical and applied machine-learning communities. It removes a practical barrier that has limited the deployment of flow-based models in real-world density estimation. FALCON may become a standard component for efficient probabilistic inference with continuous flows."}, "weaknesses": {"value": "Overall, the work is strong and technically solid, but a few aspects could benefit from further clarification or discussion:\n\n1. Since the method still depends implicitly on the invertibility and numerical conditioning of the local Jacobian, it would be important to analyze the behavior when the Jacobian determinant approaches zero, i.e., when the approximate mean velocity field u_theta becomes locally near-singular. Could such regions lead to unstable or biased likelihood estimates? Most practical architectures cannot guarantee strict invertibility.\n\n2. Theoretical analysis likely assumes certain smoothness properties of the vector field to ensure the validity of the continuous approximation. These assumptions should be clearly stated and their necessity discussed, especially for large-scale neural networks that are not strictly smooth."}, "questions": {"value": "1. If the Jacobian determinant becomes close to zero in some regions (i.e., the mapping is nearly non-invertible), how does this affect the numerical stability and bias of the estimated likelihood? Can the authors provide practical guidance or regularization strategies to mitigate this issue?\n\n2. What specific smoothness or Lipschitz continuity assumptions are required on u_theta for the theoretical guarantees to hold? Would the results still apply to models with ReLU-based, piecewise-linear velocity fields that are not globally C^1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NW2RpOU6gq", "forum": "FbssShlI4N", "replyto": "FbssShlI4N", "signatures": ["ICLR.cc/2026/Conference/Submission14390/Reviewer_vNfw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14390/Reviewer_vNfw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758273469, "cdate": 1761758273469, "tmdate": 1762924804602, "mdate": 1762924804602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes FALCON (Few-step Accurate Likelihoods for Continuous Flows), which does what the title says. The addition of an invertability loss supports the training of a few-step flow map from which accurate likelihoods can be computed efficiently.  The results demonstrate this advances the state-of-the-art for Boltzmann Generators."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The results (in particular Figure 2) are impressive and advance the field.  Inference time is orders of magnitude faster than traditional CNFs.\n\nScalable Boltzmann generators can enable new direction in molecular modeling - this is an important advance.\n\nThe paper is well written and the approach is well motivated, both intuitively and mathematically."}, "weaknesses": {"value": "Figure 4 is hard to interpret. What are the x's? Does FALCON really have a constant W2 regardless of the number of samples generated? I don't think this figure is clearly illustrating the data as intended.\n\nAlthough the ability to use a larger model is attributed as a strength of the approach, for comparison purposes it would be nice to train a model of similar size to comparative approaches to factor the effect of model size on performance.\n\nL_1 is used without introduction (defined in appendix, but not main paper)."}, "questions": {"value": "What do you mean by combining L and L_{avg}? This is unclear.\n\nHow could your approach be adapted to a transferable model (not single molecule system)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JmkJcUAXJA", "forum": "FbssShlI4N", "replyto": "FbssShlI4N", "signatures": ["ICLR.cc/2026/Conference/Submission14390/Reviewer_cmBL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14390/Reviewer_cmBL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972806842, "cdate": 1761972806842, "tmdate": 1762924804177, "mdate": 1762924804177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors tackle the problem of accurate likelihood estimation for the family of Flow Map models, an extension of standard CNFs. While efficient in terms of the number of steps needed for accurate sampling, Flow Maps cannot model the required change of variables needed for accurate density estimation.\n\nThe authors tackle this problem by introducing an additional regularization term that enforces the flow map to be approximately invertible (Eq. 8). If strong enough, this regularization term makes it possible to calculate the change in density using the log-determinant of the flow map’s Jacobian, similar to what is used in time-discrete normalizing flows. The authors formally show that the exactly invertible architecture is the minimizer of the proposed loss.\n\nIn their experimental evaluation, the authors show how a Flow Map trained using their proposed regularization technique can be used within the context of equilibrium sampling of peptide systems using importance sampling, similar to how standard CNFs and discrete-time NFs are used in Boltzmann generators. The experimental section focuses on showing improved scalability over CNFs and improved sample quality over discrete-time NFs. Furthermore, the paper studies the efficiency of the method (comparing training against discrete-time NFs and inference against CNFs) and studies the inference–accuracy trade-off. Throughout all experiments, the approach performs better than or on par with current state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Overall, the paper is well executed. The background and preliminaries sections are well written and contain sufficient depth; the problem setting is clear, and the proposed solution is well described. Furthermore, the proposed solution itself seems intuitive (although novelty might be limited, as noted below). Finally, the experimental section is extensive and provides clear evidence in favor of the proposed approach.\n\nAs such, I am generally in favor of accepting the paper for publication based on the content alone. However, as highlighted below, I have a few concerns regarding the novelty of the paper that need to be answered before I can vote for acceptance with absolute confidence."}, "weaknesses": {"value": "My primary objection to voting for acceptance of the paper at this time is the similarity of the proposed method to the one presented in Rehmann et al., 2025. This paper has a very similar objective: combining the benefits of CNFs and discrete-time NFs by enforcing approximate invertibility. Additionally, both papers share a very similar set of experiments (Rehmann et al. is, however, slightly more extensive). While the method presented here has, in theory, sufficient novelty to warrant publication, further clarification about the similarities and a comparison across methods are needed.\n\nWithout this clarification included in the paper, I do not feel confident in accepting the paper for publication.\n\nIn addition, I have included a short list of comments/questions below that I would appreciate the authors’ response to or see addressed in the paper:\n- Fig. 1: This is very nit-picky, but the debiased target should completely overlap with the true density.\n- Fig. 2: This should also include a discrete-time NF.\n- Tab. 3: A key contribution of the paper is that, with the Flow Map–based approach instead of discrete-time NFs, FALCON should scale better. Table 3, however, shows that when considering large systems (e.g., Hexa-Alanine), the difference between the methods shrinks quite significantly, with SBG-IS and SBG-SMC even outperforming FALCON in terms of Wasserstein distance over energy distributions. This should be discussed in the paper.\n- Tab. 4: It’s difficult to establish a clear trend in these results. ECNF is worse for smaller systems but has better training + inference time for Alanine Tetrapeptide. Conversely, the largest system is again considerably slower. SBG, on the other hand, is slower than FALCON for all system sizes except the largest. This should be discussed."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "07w6CClQvi", "forum": "FbssShlI4N", "replyto": "FbssShlI4N", "signatures": ["ICLR.cc/2026/Conference/Submission14390/Reviewer_ExRz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14390/Reviewer_ExRz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762286815716, "cdate": 1762286815716, "tmdate": 1762924803800, "mdate": 1762924803800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}