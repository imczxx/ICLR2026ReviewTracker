{"id": "ObF4WIMkY6", "number": 23319, "cdate": 1758342071016, "mdate": 1759896821259, "content": {"title": "EEPO: Exploration-Enhanced Policy Optimization via Sample-Then-Forget", "abstract": "Balancing exploration and exploitation remains a central challenge in reinforcement learning with verifiable rewards (RLVR) for large language models (LLMs). Current RLVR methods often overemphasize exploitation, leading to entropy collapse, reduced exploratory capacity, and ultimately limited performance gains. Although techniques that add randomness increase policy stochasticity, they frequently fail to escape dominant behavioral modes. The resulting sample-and-reward dynamics amplify these modes, eroding exploration and leading to entropy collapse. We introduce Exploration-Enhanced Policy Optimization (EEPO), a novel framework that promotes exploration through two-stage rollouts with adaptive unlearning. In the first stage, the model generates half of the trajectories; it then undergoes a lightweight, temporary unlearning step to suppress these sampled responses, forcing the second stage to explore different regions of the output space. This sample-then-forget mechanism actively steers the policy away from dominant modes and encourages mode-seeking exploration. Across five reasoning benchmarks, EEPO consistently outperforms baselines, achieving average gains of 24.3% on Qwen2.5-3B, 33.0% on Llama3.2-3B-Instruct, and 10.4% on Qwen3-8B-Base.", "tldr": "We propose EEPO, which enhances exploration in RLVR by temporarily suppressing sampled trajectories during rollouts, achieving 10-33% improvements across mathematical reasoning benchmarks.", "keywords": ["large language models", "reasoning models", "reinforcement learning", "RLVR", "exploration", "unlearning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/46a694bd11e8eb133818677b634649b5a0bd93d2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of insufficient exploration and resulting entropy collapse in Reinforcement Learning with Verifiable Rewards (RLVR) used for training large language model (LLM) reasoning capabilities. Current methods often over-exploit dominant trajectories, and while adding randomness increases stochasticity, it fails to effectively shift the policy away from these modes. To counter this, the authors propose Exploration-Enhanced Policy Optimization (EEPO), a framework that modifies the rollout phase with a novel \"sample-then-forget\" mechanism. EEPO performs rollouts in two stages: after sampling the first half of trajectories, it applies a temporary, lightweight unlearning step to the rollout model to suppress the just-sampled responses, using a complementary loss function gated by an entropy threshold. This forces the second stage of sampling to explore different, less probable regions of the output space, actively steering away from dominant modes. The standard GRPO objective is then used to update the main policy model using the combined trajectories from both stages, decoupling the exploration enhancement from the policy optimization itself. Experiments across five reasoning benchmarks and three different LLMs show that EEPO consistently outperforms GRPO and other exploration-focused baselines, achieving significant average accuracy gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper investigates the critical problem of insufficient exploration and entropy collapse in Reinforcement Learning with Verifiable Rewards (RLVR), a central challenge that limits the performance gains and generalization of LLMs trained for reasoning tasks.\n2. The proposed Exploration-Enhanced Policy Optimization (EEPO) method is conceptually intuitive, employing a straightforward \"sample-then-forget\" mechanism within a two-stage rollout process to actively suppress dominant modes and encourage exploration.\n3. The effectiveness and robustness of EEPO are demonstrated through comprehensive experiments conducted across five different mathematical reasoning benchmarks and three distinct LLM architectures and sizes."}, "weaknesses": {"value": "1. The comparison primarily focuses on standard GRPO and variations based on randomness or rollout count, neglecting direct benchmarks against more advanced, contemporary RLVR algorithms like the full DAPO framework.\n2. Experimental validation is confined to LLMs with fewer than 10 billion parameters (specifically 3B and 8B models), leaving the scalability and effectiveness of EEPO on larger foundation models untested.\n3. While EEPO aims to improve performance via enhanced exploration, the provided training curves show its mean reward is comparable to or even slightly lower than standard GRPO (Fig. 6), and the paper lacks a detailed explanation for why the proposed method results in slightly faster wall-clock training times despite incorporating an additional unlearning step."}, "questions": {"value": "Please see paper weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8z61x7Zwwm", "forum": "ObF4WIMkY6", "replyto": "ObF4WIMkY6", "signatures": ["ICLR.cc/2026/Conference/Submission23319/Reviewer_X4NZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23319/Reviewer_X4NZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760965170252, "cdate": 1760965170252, "tmdate": 1762942602128, "mdate": 1762942602128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces EEPO, a framework that addresses the exploration-exploitation dilemma in RLVR. EEPO enhances the rollout process through a sample-then-forget mechanism, which temporarily suppresses recently sampled trajectories during generation. This deliberate exclusion discourages the policy from repeatedly collapsing into dominant, high-probability modes and instead promotes the discovery of alternative, underexplored reasoning pathways. By converting passive stochasticity into purposeful, diversity-driven exploration, EEPO steers the policy toward more comprehensive coverage of the output distribution.\n\nExtensive experiments across three model families and five mathematical reasoning benchmarks show that EEPO outperforms existing methods in final performance while preserving comparable training efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(1) Well-written (2) Detailed experiment (3) The problems related to training efficiency that have been solved are distinctive and seem valuable to the industrial sector"}, "weaknesses": {"value": "N/A"}, "questions": {"value": "I do not know this specialized research field very well. I will adjust my score and optimize my review document based on the evaluations of other expert reviewers and my performance during the rebuttal period."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "PL6y6CZkAv", "forum": "ObF4WIMkY6", "replyto": "ObF4WIMkY6", "signatures": ["ICLR.cc/2026/Conference/Submission23319/Reviewer_WrTx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23319/Reviewer_WrTx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761479561723, "cdate": 1761479561723, "tmdate": 1762942601889, "mdate": 1762942601889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a novel method to maintain exploration while learning in LLMs with RLVR. Current approaches overemphasize exploitation, causing premature entropy policy collapse. The current method avoids this by adaptive unlearning of the dominant modes and focusing on secondary modes. Improving performance compared to baselines is shown."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem solution is simple and elegant. This is complemented by simple and useful intuitions that help us to understand the proposed algorithm."}, "weaknesses": {"value": "Nice intuitions are presented, like in Fig. 2, but no theory is provided to back up those intuitions. This means that improvement can strongly depend on parameters and on a case-by-case basis. \n\nAblations studies are missing, so it is unclear what aspect of the introduced algorithm is critical: is it enough to have an entropy-conditioned activation of unlearning just based on Eq. 8, assuming a constant $p_{clip}$?\n\nIs it possible that in some problems no secondary mode exists? Then how would the method fare in discovering the most promising actions in the vast space of tokens and reasoning trajectories?\n\nWhile performance increases compared to other methods, the performance is very close to GRPO with updated parameters, so it seems that the gains are relatively small. \n\nAs far as I can see, no comparison with other methods of RL exploration is introduced (epsilon-greedy with optimal epsilon, entropy regularization, and such). Does the method outperform the obvious modifications of the algorithms with those additional exploration tricks?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9Ze1iuONVQ", "forum": "ObF4WIMkY6", "replyto": "ObF4WIMkY6", "signatures": ["ICLR.cc/2026/Conference/Submission23319/Reviewer_zNJJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23319/Reviewer_zNJJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557009188, "cdate": 1761557009188, "tmdate": 1762942601529, "mdate": 1762942601529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposes Exploration-Enhanced Policy Optimization (EEPO), a novel framework for reinforcement learning with verifiable rewards (RLVR) that addresses policy entropy collapse."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The \"sample-then-forget\" idea is a clever and novel approach to the exploration-exploitation problem. Instead of just adding indiscriminate noise (like increasing temperature), it actively and strategically steers the policy away from dominant modes it is starting to overfit on.\n\n2. The method demonstrates consistent and significant performance gains over strong GRPO baselines across five challenging mathematical reasoning benchmarks and three different language models. The average relative improvements are substantial (e.g., +33.0% on Llama3.2-3B-Instruct)."}, "weaknesses": {"value": "1. The method introduces at least two key new hyperparameters: the entropy threshold $\\alpha$ for activating unlearning and the unlearning rate $\\eta$. The paper uses fixed values ($\\alpha=0.3$, $\\eta=3\\times10^{-3}$) without providing an ablation study or discussion on how these values were chosen or how sensitive the model's performance is to them. This could be a point of fragility.\n\n2. The unlearning step modifies the rollout policy ($\\pi_{\\theta^{\\prime}}$) in the middle of generating a single batch of trajectories. It is unclear how the importance sampling (IS) ratio for the standard GRPO objective (Eq. 2) remains valid when trajectories in the same batch ($O$) are drawn from two different policies (pre-unlearning and post-unlearning). This potential violation of the IS assumption needs a more rigorous justification."}, "questions": {"value": "Please address the concerns in weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AeKPjZk2kV", "forum": "ObF4WIMkY6", "replyto": "ObF4WIMkY6", "signatures": ["ICLR.cc/2026/Conference/Submission23319/Reviewer_SHnD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23319/Reviewer_SHnD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886601039, "cdate": 1761886601039, "tmdate": 1762942601088, "mdate": 1762942601088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}