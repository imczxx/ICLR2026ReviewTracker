{"id": "E68cnZQ9dV", "number": 20603, "cdate": 1758308153001, "mdate": 1763581871834, "content": {"title": "Benchmarks for Reinforcement Learning with Biased Offline Data and Imperfect Simulators", "abstract": "In many reinforcement learning (RL) applications one cannot easily let the agent act in the world; this is true for autonomous vehicles, healthcare applications, and even some recommender systems, to name a few examples. Offline RL provides a way to train agents without real-world exploration, but is often faced with biases due to data distribution shifts, limited coverage, and incomplete representation of the environment. To address these issues, practical applications have tried to combine simulators with grounded offline data, using so-called hybrid methods. However, constructing a reliable simulator is in itself often challenging due to intricate system complexities as well as missing or incomplete information. In this work, we outline four principal challenges for combining offline data with imperfect simulators in RL: simulator modeling error, partial observability, state and action discrepancies, and hidden confounding. To help drive the RL community to pursue these problems, we construct ``Benchmarks for Mechanistic Offline Reinforcement Learning'' (B4MRL), which provide dataset-simulator benchmarks for the aforementioned challenges. Our results show that current algorithms fail to synergize these sources, often performing worse than using one source alone, especially when faced with hidden confounding.", "tldr": "", "keywords": ["Reinforcement learning", "Reinforcement learning benchmarks", "offline reinforcement learning", "sim2real gap"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f341bc15287a772b890ef743b6579b8d2a0a71c2.pdf", "supplementary_material": "/attachment/9b23d9be82061c830850b321fa20198012c281b5.zip"}, "replies": [{"content": {"summary": {"value": "This work presents B4MRL, a set of offline datasets and code to add errors to MuJoCo and Highway simulators. The aim is to provide a benchmark for evaluating challenges in hybrid simulator-augmented offline RL, including modeling error, partial observability, state/action discrepancies, and hidden confounding."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem addressed by the paper is a relevant practical problem and the paper includes some interesting analysis."}, "weaknesses": {"value": "**W1.** This work releases some datasets and code for modifying two simulators and presents results with some prior methods. I am not sure this is enough for a paper at this venue. Other reviewers may have a different opinion, but, since the aim of the paper is to aid future algorithm development, I think a paper of this kind should additionally include:\n\n- (A) Specification of a small collection of experiments that serves as a standard testbed for developing future new methods (ie. similar to how D4RL specifies the -random, -mixed, -medium, etc.). Otherwise, there are so many degrees of freedom that future papers will all pick different settings, making fair evaluation of methods extremely difficult.\n\n- (B) A more complete codebase benchmarking current methods on each of these experiments (e.g. potentially based on Unifloral or CORL). At the moment the baseline implementations are drawn from seven different codebases, which likely introduces confounding variables due to differences in implementation details beyond the core algorithms, which makes comparisons and conclusions difficult. I think for a paper of this kind, rigorous evaluation of prior methods should be part of the contribution.\n\n**W2.** Since the main contribution of this paper is the code and datasets for the benchmark, rather than algorithm insights or results, the authors should release the code in a way that can be reviewed easily. It is easy to release anonymised repos with https://anonymous.4open.science/. The benchmark would also benefit from web-hosted docs.\n\n**W3.** The paper's own novel baseline, HyMOPO, is noted to be unsuitable for the Walker2D and Hopper environments due to observation clipping issues, limiting its generality."}, "questions": {"value": "**Q1.** Do you have an intuition as to why hybrid-RL algorithms scored worse than algorithms without the offline dataset? Do you think they could perform better if tuned properly, or do you think there is a fundamental problem?\n\n**Q2.** Since baselines were drawn from different repositories, is it possible that the poor performance of some hybrid methods is due to suboptimal tuning for this specific hybrid task rather than a fundamental algorithmic flaw?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MfhZ9Xf1C4", "forum": "E68cnZQ9dV", "replyto": "E68cnZQ9dV", "signatures": ["ICLR.cc/2026/Conference/Submission20603/Reviewer_5Jp9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20603/Reviewer_5Jp9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761777375148, "cdate": 1761777375148, "tmdate": 1762934008330, "mdate": 1762934008330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents B4MRL (Benchmarks for Mechanistic Offline Reinforcement Learning), a comprehensive benchmark suite designed to evaluate offline-to-online RL algorithms when combining offline datasets with imperfect simulators in hybrid RL settings. The paper identifies four principal challenges in hybrid RL: (1) simulator modeling error (sim2real gap), (2) partial observability and state discrepancy, (3) action discrepancy, and (4) hidden confounding (offline2real bias). Unlike existing benchmarks (D4RL, VD4RL, ODRL, CARL, etc.), B4MRL uniquely addresses all four challenges simultaneously, providing a systematic framework for evaluation. Through extensive empirical evaluation on MuJoCo and Highway environments using online (TD3, SAC), offline (TD3-BC, IQL, MOPO), and hybrid (H2O, PAR-BC, HyMOPO) algorithms, the paper demonstrates a critical finding: current hybrid RL methods frequently fail to leverage both data sources synergistically, often performing worse than using either source alone, particularly when hidden confounding is present."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Comprehensive and well-motivated problem characterization.** The paper articulates a compelling and timely problem: while offline RL and simulator-based RL are well-studied, hybrid methods that combine both remain poorly understood and underspecified. The four challenges (modeling error, partial observability, action discrepancy, confounding) are systematically presented with concrete real-world examples (healthcare, autonomous driving, recommender systems), making the motivation clear and accessible. The distinction between partial observability and hidden confounding (Section 2.4) is particularly valuable and often overlooked.\n\n**Rigorous and modular benchmark design.** B4MRL provides a principled, composable benchmark architecture where challenges can be independently controlled and combined. The design choices are justified through ablation studies (Figure 4 demonstrates how to select which variables to hide), and the implementation details (Section B, Appendix) are thorough and reproducible. The use of parametric modifications to MuJoCo environments (gravity, friction, action noise) provides clean, interpretable ways to introduce discrepancies.\n\n**Diverse algorithm coverage.** Evaluating eight different RL algorithms (online, offline, and hybrid) across multiple environments and challenges provides broad empirical coverage.​"}, "weaknesses": {"value": "**Hidden confounding implementation is somewhat simplistic.** The confounding benchmarks (Section 3, Challenge 4) introduce confounding by either adding noise or removing variables from observations, with the assumption that the data-generating agent saw the missing variables. This is a stylized form of confounding that may not capture the full complexity of real-world confounding scenarios (e.g., time-dependent confounding, continuous latent confounders). More sophisticated confounding mechanisms could strengthen the benchmarks.\n\n**Narrow scope of environments and tasks.** All experiments use MuJoCo continuous control and one highway driving environment. The scalability and applicability to image-based observations, discrete action spaces, or more complex domains remain unclear. D4RL benchmarks are well-studied but represent a narrow slice of RL problems."}, "questions": {"value": "**Q1:** Why is three seeds used instead of five or more? Was this a computational constraint? Can results be re-run with additional seeds for higher statistical confidence?\n\n**Q2:** How sensitive are confounding results to the specific variables chosen to hide? Figure 4 shows variable importance varies across algorithms. Is there a principled way to select impactful confounders?\n\n**Q3:** Can you provide theoretical characterization of when hybrid methods have fundamental limitations? Is there a theoretical explanation for why some combinations of challenges make hybrid methods perform worse than individual methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1BpX7GbkXH", "forum": "E68cnZQ9dV", "replyto": "E68cnZQ9dV", "signatures": ["ICLR.cc/2026/Conference/Submission20603/Reviewer_mnbH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20603/Reviewer_mnbH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919354350, "cdate": 1761919354350, "tmdate": 1762934007821, "mdate": 1762934007821, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a new benchmark for offline RL with biased offline data and imperfect simulators. Unlike standard benchmarks in offline RL (D4RL, etc.), they focus on realistic challenges such as modeling errors, causal confusion, and partial observability. Across four high-level challenges, they propose diverse variants of MuJoCo tasks (halfcheetah, hopper, and walker2d) and the Highway task. They benchmark several representative offline and online RL algorithms on this benchmark, showing that there is still room for improvement in addressing these challenges."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The problem posed in this paper is sensible. I believe the community can benefit from offline RL benchmarks that incorporate more realistic challenges encountered in the real world.\n* The paper is well-organized and easy to understand."}, "weaknesses": {"value": "* The main weakness of this benchmark is its relevance. The authors motivate this benchmark from diverse real-world challenges in offline RL, such as dynamics discrepancy, causal confusion, and partial observability. However, the quality of the environments provided in this benchmark is limited in realizing these challenges. Specifically, the environments are limited to simple 2-D MuJoCo tasks (halfcheetah, hopper, and walker2d) and the (highly simplified) Highway environment. I'm unsure how impactful and useful these tasks are for today's offline RL research. From the motivation in the Introduction, I expected much more realistic benchmarks, such as datasets collected from actual human demonstrators or at least more realistic scripted or non-Markovian policies on more relevant tasks (e.g., complex and realistic robotic manipulation, long-horizon navigation, computer games, etc.).\n* Moreover, while the individual challenges listed in Table 1 are sensible, they are implemented in a highly contrived manner. For example, the authors simply change the gravity or friction parameter to simulate modeling errors, and add Gaussian noise to simulate state/action discrepancies or causal confusion. In the real world, these challenges are often much more subtle -- such errors or noises are typically temporally correlated, non-Markovian, and biased. I'm not sure how representative and realistic the challenges implemented in this benchmark are. I also believe these implementations are too simplistic to be impactful enough as a standalone benchmark. I'd be fine with such simplifications in methodology papers, but as a benchmark paper, I think the bar should be higher than that of typical experiments in such papers."}, "questions": {"value": "* Why is the benchmark called \"mechanistic\" offline RL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NJCcaZCeCT", "forum": "E68cnZQ9dV", "replyto": "E68cnZQ9dV", "signatures": ["ICLR.cc/2026/Conference/Submission20603/Reviewer_4CTZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20603/Reviewer_4CTZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065176179, "cdate": 1762065176179, "tmdate": 1762934007342, "mdate": 1762934007342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank the reviewers for their time, effort, and valuable feedback. We are pleased to report that we have conducted the requested experiments, including frame-stacking and hybrid RL degradation analysis. These new results have not only addressed specific concerns but have further deepened the paper's insights, validating B4MRL as a rigorous diagnostic tool.\n\nWe have uploaded a **revised PDF** containing the following updates:\n1. **New Section 4.3 (Page 10)**: Detailed analysis of failure mechanisms, including the new \"Frame-stacking\" (figure 5, page 9, note that the previous figure 5 is now figure 6) and \"Hybrid-RL degradation\" (table 3) experiments.\n2. **Appendix B**: The Core Evaluation Protocol to ensure standardization.\n3. **Clarity**: We have decluttered Figure 5b (which is now Figure 6b in the revised paper), and added small modifications as suggested by reviewers.\n\n### **Summary of new experimental findings:**\n**1. Frame-stacking analysis:**\n\nTo determine if the confounding in B4MRL is merely a solvable POMDP, we evaluated IQL with frame stacking (aggregating 4 frames of observations together) on two variants of the confounding challenge (see Figure 5, Page 9):\n* **Hidden dimensions**: History recovers ~95% of expert performance. This confirms that when confounding is due to missing physical variables (e.g., velocity), it can behave as a solvable POMDP.\n* **Mechanistic noise**: History recovers only ~46% of performance.\n* **Conclusion**: This finding implies that the mechanistic noise in B4MRL acts as a structural barrier that was not resolved, distinguishing this confounding error from standard partial observability.\n\n**2. Mechanism diagnosis for Hybrid-RL degradation:**\n\nTo understand why hybrid algorithms fail on the confounding challenge (and whether it is a fundamental issue), we analyzed the Value Estimation Error (ratio of predicted Q value and the total return G) of HyMOPO (see table below which is also table 3 on page 10 in the revised version).\n\n| Algorithm | Challenge Type | Real Return ($G$) | Predicted $Q$ | Ratio ($Q/G$) |\n| :--- | :--- | :---: | :---: | :---: |\n| **HyMOPO** | Modeling Error (Challenge 1) | 11,500 | 882 | **0.077** |\n| **HyMOPO** | Low Confounding ($\\sigma=0.01$) | 9,664 | 752 | **0.078** |\n| **SAC** | Hidden Dims (Challenge 2) | 2,254 | 178 | **0.079** |\n| **HyMOPO** | **High Confounding ($\\sigma=0.05$)** | **1,538** | **395** | **0.269** |\n\n**Result:** In \"healthy\" runs, the agent maintains a calibrated ratio of ~0.08 (which makes sense considering the discount factor). In high-confounding settings, the agent overestimates returns by a ratio of 0.269, over 3 times the calibrated ratio.\n\n**Conclusion:** This isolates the failure mode as signal dominance: the agent optimizes the simulator's \"clean\" (but biased) physics while rejecting the offline data's \"noisy\" (but true) signal. This level of diagnostic precision validates the utility of B4MRL's controlled design.\n\n**3. Standardization**\n\nTo prevent \"cherry-picking\" in future work, we have added Appendix B: The B4MRL Core Evaluation Protocol, which formally defines the exact matrix of environments and noise levels required for a valid benchmark run.\n\nWe believe these revisions directly address the reviewers' concerns regarding the benchmark's utility and rigor. We are happy to answer any further questions."}}, "id": "BUy22MrhyX", "forum": "E68cnZQ9dV", "replyto": "E68cnZQ9dV", "signatures": ["ICLR.cc/2026/Conference/Submission20603/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20603/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20603/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763582440045, "cdate": 1763582440045, "tmdate": 1763582440045, "mdate": 1763582440045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces B4MRL, a new benchmark suite for evaluating reinforcement learning algorithms that combine offline data and simulators. The benchmarks address four real-world challenges: modeling error, state and action discrepancies, partial observability, and hidden confounding. Experiments show that current hybrid RL methods often fail, especially when offline data contains hidden confounders. The results highlight the need for more robust algorithms that can reliably integrate both data sources. The paper proposes the benchmark to test these algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Originality: Proposes (to my knowledge) the first RL benchmark that systematically combines all four sources of sim2real and offline2real error.\n  \n- Quality: Has educational value in the taxonomy of challenges. Implements them in simulated benchmark settings.\n- Quality: Provides careful experimental evaluation with many baselines across offline, online, and hybrid RL methods\n\n- Clarity: Clearly motivated and systematic presentation\n\n- Significance: Addresses an important issue in the RL community, that is important for real-world applications."}, "weaknesses": {"value": "- W1: partial observability calls for methods for POMDPs. The methods used, such as TC3-BC, SAC etc are all using TD learning and rely on the Markov property. Methods that use Monte-Carlo returns (such as PPO) or methods that have recurrent architectures (policy and value functions) would be the natural algorithms to be tested.\n\n- W2: The hidden confounder problem is present in modern sim2real pipelines with teach to student distillation and privileged information used for the teacher. The students are always recurrent networks, to perform latent state estimation. I think you benchmark is great, but I also believe the problems have been solved already in practice.\n\n- W3: Limited diversity of tasks/environments: While MuJoCo and Highway are used, the benchmarks are centered on classic continuous control tasks and may not generalize to more complex domains such as vision-based control."}, "questions": {"value": "- Q1: I am mostly concerned about the non-Markovianity. I suspect that the results will change quite drastically, if methods are used that are designed for partial observability. Would be very interesting to understand if the confounding is really such a strong problem then. Now, confounding is somewhat convolved with partial observability. Can you provide evidence that the observed phenomena persist?\n\n- Q2: what happens if you use recurrent architectures? (or as a first approximation provide a history of 4 observations?\n\n- Q3: Fig 5: what do you mean with \"moving from simple modeling error to high-impact hidden confounding\"? Is the drop when setting 1+4 vs the setting 1? \n\n\n\nComments:\n- line 204: \"and P (r = 1|z = 1, a = a0) = 1/6\" should that not be z=0?\n- line 294/295: \"...Gaussian noise into the action implemented by the agent to the simulator’s present state...\". What means action into simulator's state?\n- line 377: $o'_{\\text{sim}}$ sim should prob. be a subscript. \n- Fig 5b: I think the labels are confusing and redundant. The x-labels already contain the challenge. (Maybe use the descriptive names instead of numbers). Reduce the number of markers to the actually different runs (so one line should have only one marker). Also, a clearer description of sigma, h and g would be good."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H55hz605MA", "forum": "E68cnZQ9dV", "replyto": "E68cnZQ9dV", "signatures": ["ICLR.cc/2026/Conference/Submission20603/Reviewer_afxy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20603/Reviewer_afxy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762336940280, "cdate": 1762336940280, "tmdate": 1762934006923, "mdate": 1762934006923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}