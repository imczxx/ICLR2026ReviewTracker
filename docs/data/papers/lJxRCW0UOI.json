{"id": "lJxRCW0UOI", "number": 11999, "cdate": 1758205137197, "mdate": 1759897540363, "content": {"title": "Large Language Models outperform state-of-the-art methods on multiclass sentiment polarity detection", "abstract": "Sentiment polarity detection remains a significant problem with applications such as opinion tracking and social network analysis. In this study, we evaluate whether contemporary open-weight large language models (LLMs) can rival or surpass specialized approaches on multiclass polarity detection while accounting for inference cost. We perform a systematic zero-shot evaluation of 31 open-weight LLMs on two canonical five-class benchmarks, assessing accuracy, Macro-Average Mean Absolute Error, and instances-per-second measures to quantify cost-performance trade-offs, also analyzing the best models according to the Pareto criterion. We found that several LLMs, without fine-tuning or elaborate prompting, outperform previous state-of-the-art results on a large dataset (SemEval) and approach a similar to state-of-the-art performance on a smaller benchmark (SST-5). Our Pareto frontier analysis highlights models that combine high accuracy with low inference costs, offering practical deployment choices for fine-grained sentiment polarity detection.", "tldr": "Open-Weight LLMs outperform SOTA methods on multiclass sentiment polarity detection with comparatively low inference cost.", "keywords": ["sentiment polarity detection", "multiclass sentiment polarity", "open-weight llm", "pareto frontier"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/113a634d9c6c5874463f71b90f8be1893b59020a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper systematically benchmarks 31 open-weight LLMs on five-class sentiment classification (SemEval-2017 Task 4C and SST-5).\nUsing a zero-shot setup and measuring accuracy, macro-average mean absolute error, and instances-per-second, it finds that several open models (e.g., Gemma-3 27B, Mistral-small3.2 24B) exceed previous SOTA on SemEval without fine-tuning. The paper also provides a Pareto frontier to illustrate trade-offs between accuracy and inference speed."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Evaluates a large, diverse set of open-weight LLMs across families and sizes, offering a valuable comparative resource for the community.\n2. Goes beyond accuracy by including Macro-Average MAE and instances-per-second, capturing both quality and efficiency trade-offs."}, "weaknesses": {"value": "1. The study mainly reuses existing datasets and evaluation setups, focusing on performance comparison rather than introducing any new modeling idea, benchmark design, or evaluation framework.\n\n2. The introduction section references only a small portion of prior work, offering limited connection to the broader sentiment analysis and LLM evaluation literature.\n\n3. The experiments rely on a single fixed prompt for each dataset, without examining how small changes in wording might affect model behavior, which is a key factor in large language model reliability.\n\n4. The paper does not address potential overlap between the test data and model pretraining corpora, which is particularly relevant because both evaluated datasets are long-established and widely circulated.\n\n5. The empirical analysis remains narrow: only two datasets are tested, and there is little exploration of error patterns or qualitative reasoning behind the models’ successes and failures."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vUUQaOw9ZH", "forum": "lJxRCW0UOI", "replyto": "lJxRCW0UOI", "signatures": ["ICLR.cc/2026/Conference/Submission11999/Reviewer_HQCw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11999/Reviewer_HQCw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761533345406, "cdate": 1761533345406, "tmdate": 1762922990743, "mdate": 1762922990743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluate the performance of 31 open-source large language models (open-weight LLMs) on the task of multiclass sentiment polarity detection, whilst examining the trade-off between performance and inference cost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Tables and graphs provide a more straightforward presentation. The appendix contains a comprehensive list of models, prompts, failure sample rates, and detailed results."}, "weaknesses": {"value": "1.Unclear motivation and innovation\nThe paper does not propose novel algorithms or provide clear motivation demonstrating the necessity of this work. It reads more like a report expressing a single thesis: that untuned large models achieve superior sentiment classification performance. However, this falls short of the requirements for an academic paper.\n2.Writing and structural issues\nThe paper's structure is rather loose, and its language is more akin to that of a report or technical white paper;"}, "questions": {"value": "1.Sentiment classification is not a particularly challenging task. What is the point of evaluating large language models on this task?\n2.The paper claims that the large model's performance surpasses state-of-the-art models, yet this is not explicitly demonstrated within the paper, such as through tables or images."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JO0ZBXgV8b", "forum": "lJxRCW0UOI", "replyto": "lJxRCW0UOI", "signatures": ["ICLR.cc/2026/Conference/Submission11999/Reviewer_Xx3W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11999/Reviewer_Xx3W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966457738, "cdate": 1761966457738, "tmdate": 1762922990326, "mdate": 1762922990326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper evaluates the performance and efficiency of 31 open-weight LLMs on two sentiment analysis tasks, providing comprehensive results. This is the paper’s only contribution."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- A large number (31) of open-weight LLMs are evaluated on a multi-label sentiment analysis dataset, with both accuracy and efficiency metrics reported."}, "weaknesses": {"value": "- The paper’s contribution is minimal. It only provides a joint evaluation of performance and efficiency of LLMs on sentiment analysis tasks. The so-called Pareto frontier analysis, while practical and informative, is not a novel evaluation approach.\n\n- Although the reported results may be useful to the community, they are more appropriate for an initial technical report or blog post. In contrast, conferences such as ICLR emphasize novelty in research questions or methodologies—introducing new ideas, not merely new results.\n\n- The paper’s content is rather superficial. Of the 8 pages of main text, less than half a page is used to present the benchmark label distribution (Figure 1), and two full pages are devoted to displaying largely redundant evaluation results (Figures 2 & 3). Moreover, about half a page remains blank. Given the level of detail, the same content could likely be presented in 4–5 pages through concise writing.\n\n- The reference list is short, containing only 13 works, with just one citation in the introduction. This suggests a limited understanding of prior research in the field. Also, the presentation is quite immature, with many informal, repetitive, and unprofessional expressions throughout the paper.\n\n- To make the work suitable for a conference submission, the authors could consider extending beyond simple evaluation and explore potential research contributions, such as:\n   - If focusing on the accuracy–efficiency trade-off, propose methods to optimize this trade-off—e.g., achieving higher accuracy with smaller or more efficient LLMs.\n   - If focusing on evaluation methodology, investigate the limitations of existing evaluation practices—are they efficient, and do they truly reflect model capabilities?\n   - Alternatively, broaden the research scope beyond sentiment polarity detection to other, more general NLP tasks.\n   - ..."}, "questions": {"value": "no questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EPvIozsyBp", "forum": "lJxRCW0UOI", "replyto": "lJxRCW0UOI", "signatures": ["ICLR.cc/2026/Conference/Submission11999/Reviewer_kCuQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11999/Reviewer_kCuQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762458124219, "cdate": 1762458124219, "tmdate": 1762922989882, "mdate": 1762922989882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}