{"id": "mf35JXqWHS", "number": 14266, "cdate": 1758231507020, "mdate": 1763735436361, "content": {"title": "Noise Tolerance of Distributionally Robust Learning", "abstract": "Given the importance of building robust machine learning models, considerable efforts have recently been put into developing training strategies that achieve robustness to outliers and adversarial attacks. Yet, a major aspect that remains an open problem is systematic robustness to global forms of noise such as those that come from measurements and quantization. Hence, we propose in this work an approach to train regression models from data with additive forms of noise, leveraging the Wasserstein distance as a loss function. Importantly, our approach is agnostic to the model structure, unlike the increasingly popular Wasserstein Distributionally Robust Learning paradigm (WDRL) which, we show, does not achieve improved robustness when the regression function is not convex or Lipschitz. We provide a theoretical analysis of the scaling of the regression functions in terms of the variance of the noise, for both formulations and show consistency of the proposed loss function. Lastly, we conclude with numerical experiments on physical PDE Benchmarks and electric grid data, demonstrating competitive performance with an order of magnitude reduction in computational cost.", "tldr": "We propose a computationally efficient learning approach featuring robustness to global forms of noise.", "keywords": ["Distributional Robustness", "Wasserstein Distance", "Deep Learning", "Operator Learning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c891f7b0e36ff617e8d78396135ec79120c6e3bc.pdf", "supplementary_material": "/attachment/b0b747f0f7e2aecc14433a39a7b774c62d2e2939.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces WBM, a new regression loss to improve robustness to additive and global noise, addressing the limitations of Wasserstein Distributionally Robust Learning when regression functions are not Lipschitz or convex case. Theoretical results demonstrate that WBM loss aligns better with noise variance than WDRL. Numerical evaluation on PDE operator learning and electric grid forecasting further valid the robustness of proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe motivation of this paper is good. As WDRL lacks of robustness to global additive noise.\n\n2.\tThe paper provides rigorous proofs Wasserstein Batch Matching and noise scaling case.\n\n3.\tExperiments over diverse tasks validate the effectiveness of proposed WBM."}, "weaknesses": {"value": "1.\tSome assumptions of theoretical results are not reasonable. For example, Proposition 4.1 requires bandlimited and continuously differentiable $f$ and co-monotonic of $g$, which may not hold for modern DNNs. Moreover, Corollary 5.2 relies on strong convexity and smoothness with second to even fifth order, that is not reasonable.\n\n2.\tNo error bars or confidence intervals. The number of runs only has 13 samples, which is small for modern deep learning.\n\n3.\tThe computational gains do not show practical analysis. Although a complexity gains achieved, the real-time runtime such as GPU time evaluation is missed.\n\n4.\tThere is limited comparison with other regression baselines. For instance, Noise2Noise.\n\n5.\tAppendix B used partial derivatives of Lagrange multipliers $\\alpha$, $\\beta$ with respect to $\\sigma$ without proving differentiability rigorously, only a sketch via implicit function is provided."}, "questions": {"value": "1.\tHow does WBM perform in non-convex neural networks beyond CNN operators?\n\n2.\tCan WBM expand to multiplicative or correlated noises?\n\n3.\tWhat will happen if the true regression function is discontinuous? \n\n4.\tHow does WBM performance depend on batch size? Is there any optimal range balancing between robustness and bias?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RuhUb5Hanx", "forum": "mf35JXqWHS", "replyto": "mf35JXqWHS", "signatures": ["ICLR.cc/2026/Conference/Submission14266/Reviewer_SL38"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14266/Reviewer_SL38"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760707862536, "cdate": 1760707862536, "tmdate": 1762924717417, "mdate": 1762924717417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses robustness to additive noise in regression, proposing a more distributional measure as an alternative to sample-wise Wasserstein robustness. The authors argue that standard Wasserstein robustness fails when regression functions are neither convex nor Lipschitz, and demonstrate that their \"batch matching\" method performs better with high-variance noise while being computationally cheaper."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-- The focus on global noise in WBM is timely and distinct from adversarial/outlier robustness, while the authors provide a clear drawback to standard adversarial robustness measures such as sample-wise Wasserstein\n\n-- The theory seems genuinely novel (at least, it is to me) and interesting\n\n-- The scheme is model-agnostic and plugs into SGD, and the loss is differentiable via envelope theorem\n\n-- The experiments on PDE learning and time series demonstrate practical utility for the scheme\n\nOverall, I find the paper enlightening on an important limitation of standard sample-wise Wasserstein robustness, with an interesting practical fix as well as supporting analysis."}, "weaknesses": {"value": "My main concern is that the paper is fuzzy on implementation details, which makes me a bit concerned for the reproducibility of the method, e.g.\n\n-- The WBM scheme's performance must heavily depend on batch size (larger batches = more flexible matching?), but this is barely discussed\n\n-- There is a 10 fold run improvement mentioned but no timing comparisons provided"}, "questions": {"value": "-- Is there any connection between your \"Wasserstein Batch Matching\" approach and the Central Limit Theorem? If so, I think it would be interesting to make this connection explicit.\n\n-- Why not compare to simpler robust losses like Huber loss or quantile regression?\n\n-- Strong convexity is assumed for SGD stationary distribution analysis (Cor. 5.2), then justified via RKHS regularization. This diverges from the deep-net setting used in experiments. Could you provide some discussion here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3nWR1kNI31", "forum": "mf35JXqWHS", "replyto": "mf35JXqWHS", "signatures": ["ICLR.cc/2026/Conference/Submission14266/Reviewer_P1nf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14266/Reviewer_P1nf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760982662464, "cdate": 1760982662464, "tmdate": 1762924716995, "mdate": 1762924716995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of robustness to global additive noise such as measurement and quantization noise, which is often encountered in practical regression and physical modeling tasks. The authors introduce a new training framework named Wasserstein Batch Matching (WBM). It replaces instance-wise matching between predictions and responses with batch-level Wasserstein alignment between their empirical distributions.  \n\nTheoretically, the paper proves (i) **consistency** of WBM estimators (Proposition 4.1), and (ii) **favorable noise scaling properties** (Proposition 5.1), showing that WBM loss grows sublinearly with noise variance σ² compared to MSE and Wasserstein DRO (WDRL). Empirical results on **PDE operator learning (wave & Navier–Stokes)** and **electric grid forecasting (TSMixer)** confirm that WBM achieves significantly better robustness under Gaussian and especially Cauchy heavy-tailed noise, with lower computational cost than divergence-based DROs (CVaR-DRO, Chi²-DRO)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Addresses a **real and underexplored problem** (robustness to global additive and heavy-tailed noise).\n+ Provides **rigorous theoretical analysis**, including a new _noise scaling law_ comparison with MSE and WDRL.\n+ Demonstrates **consistent empirical gains** on PDE and time-series tasks, matching theoretical expectations.\n+ Clear writing and strong logical flow from motivation → theory → experiments.\n+ Computation-friendly and easy to implement (batch-level matching without extra hyperparameters)."}, "weaknesses": {"value": "+ The theoretical contribution is relatively limited from the optimal transport perspective. The paper mainly adapts existing Wasserstein formulations to a batch-level matching context rather than developing new theoretical results or convergence guarantees.\n+ Given the moderate theoretical depth, stronger empirical support is needed to demonstrate the method’s effectiveness. However, the experiments are narrow in scope, focusing only on two case studies (PDE operator learning and electric load forecasting).\n+ The lack of broader evaluations across different domains such as tabular, visual, or structured regression reduces the generality of the claims. More systematic ablation studies on batch size, noise variance, and sample size would strengthen the experimental evidence.\n+ Claims about the model-agnostic property and computational efficiency of WBM are qualitative and are not supported by formal complexity analysis or quantitative runtime evaluation."}, "questions": {"value": "1. The paper shows clear improvements on PDE operator learning and electric load forecasting. How effective is the proposed WBM method more generally? Could it achieve comparable gains on other types of regression or prediction tasks beyond these two domains?\n2. How sensitive is the method to design choices such as batch size, noise level, and the Sinkhorn regularization parameter?\n3.  The paper claims that WBM is model-agnostic and could, in principle, generalize beyond regression tasks. However, no experimental or theoretical evidence is provided to support this claim. Could the authors clarify whether WBM has been tested or formally analyzed for classification or structured prediction settings ？\n4. The appendix mentions that WBM achieves about a 10-fold computational gain mainly because it does not require hyperparameter tuning. Could the authors provide quantitative evidence or runtime measurements to support this statement, or clarify whether the comparison includes hyperparameter search time for other methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6ZaV0Kqj50", "forum": "mf35JXqWHS", "replyto": "mf35JXqWHS", "signatures": ["ICLR.cc/2026/Conference/Submission14266/Reviewer_uyo8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14266/Reviewer_uyo8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761446999157, "cdate": 1761446999157, "tmdate": 1762924716323, "mdate": 1762924716323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies robustness to global, additive label noise and argues that standard Wasserstein Distributionally Robust Learning does not improve robustness when the regression function is neither Lipschitz nor convex. It proposes Wasserstein Batch Matching: in each step, match the empirical distributions of predictions and responses within a batch via the 2-Wasserstein distance and train by minimizing that batchwise Wasserstein loss. The authors prove a consistency result for WBM in the noiseless case and analyze noise scaling of the loss and its effect on SGD iterates, showing milder sensitivity for WBM than MSE and linear-in-σ deterioration for WDRL under its assumptions. Experiments on operator learning and electric load forecasting find WBM competitive or better than MSE and divergence-based DRO baselines, especially under heavy-tailed noise, with lower computational burden than WDRL."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Clear formulation & intuition: Batchwise distribution matching gives a simple, architecture-agnostic robust objective.\n- Robustness demonstrated on wave/Navier–Stokes operator learning and ETDataset forecasting with TSMixer; WBM outperforms MSE and beats CVaR/Chi-Sq in cost-adjusted comparisons.\n- WBM involves a tractable per-step LP in the response dimension and avoids WDRL’s minimax complexity."}, "weaknesses": {"value": "- The text says WBM solves an LP costing O(s) with s=dim(Y) and compares to WDRL’s O(s^3) when convex–concave; however, in practice the batch assignment (e.g., Hungarian/OT) can scale with batch size. Please specify the exact solver (e.g., Sinkhorn, network simplex) and report end-to-end wall-clock vs. batch size and s"}, "questions": {"value": "- Could you check the format of the paper? margin and fontsize seem do not mach with the iclr template?\n- Missing reference on global regularization using WDR https://arxiv.org/abs/2203.00553"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "s4vFxFVmVT", "forum": "mf35JXqWHS", "replyto": "mf35JXqWHS", "signatures": ["ICLR.cc/2026/Conference/Submission14266/Reviewer_gAn2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14266/Reviewer_gAn2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762296284401, "cdate": 1762296284401, "tmdate": 1762924715832, "mdate": 1762924715832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}