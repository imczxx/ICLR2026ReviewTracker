{"id": "KjxS4AgFol", "number": 12202, "cdate": 1758206292865, "mdate": 1759897525538, "content": {"title": "Multi-Synaptic Cooperation: A Bio-Inspired Framework for Robust and Scalable  Continual Learning", "abstract": "Continual learning aims to acquire new knowledge incrementally while retaining prior information, with catastrophic forgetting (CF) being a central challenge. Existing methods can mitigate CF to some extent but are constrained by limited capacity, which often requires dynamic expansion for long task sequences and makes performance sensitive to task order. Inspired by the richness and plasticity of synaptic connections in biological nervous systems, we propose the Multi-Synaptic Cooperative Network (MSCN), a generalized framework that models cooperative interactions among multiple synapses through multi-synaptic connections modulated by local synaptic activity. This design enhances model representational capacity and enables task-adaptive plasticity by means of multi-synaptic cooperation, providing a new avenue for expanding model capacity while improving robustness to task order. During learning, our MSCN dynamically activates task-relevant synapses while suppressing irrelevant ones, enabling targeted retrieval and minimizing interference. Extensive experiments across four benchmark datasets, involving both spiking and non-spiking neural networks, demonstrate that our method consistently outperforms state-of-the-art continual learning methods with significantly improved robustness to task-order variation. Furthermore, our analysis reveals an optimal trade-off between synaptic richness and learning efficiency, where excessive connectivity can impair circuit performance. These findings highlight the importance of the multi-synaptic cooperation mechanism for achieving efficient continual learning and provide new insights into biologically inspired, robust, and scalable continual learning.", "tldr": "We propose the Multi-Synaptic Cooperative Network (MSCN) for continual learning. MSCN achieves higher accuracy and superior robustness compared to state-of-the-art baselines.", "keywords": ["Continual Learning", "Catastrophic Forgetting", "Brain-inspired Computing", "Multi-Synaptic Cooperation"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a27ac01672f8a4f1673b4a1ebd1f4ba629c623c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the Multi-Synaptic Cooperative Network (MSCN), a biologically inspired continual learning framework that models cooperative interactions among multiple synapses.The authors draw inspiration from multi-synaptic connectivity in biological neurons and local synaptic plasticity modulated by eligibility traces. Each neural connection is modeled as having multiple parallel synapses, whose plasticity is modulated locally based on activity. This enables adaptive task-dependent plasticity, allowing the model to selectively activate or inhibit synapses during new task learning while minimizing interference with prior knowledge. Extensive experiments across both spiking and non-spiking architectures on four benchmark datasets demonstrate that MSCN consistently outperforms state-of-the-art methods in both accuracy and robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well-written and easy to follow.\n\n2. The proposed Multi-Synaptic Cooperative Network (MSCN) provides an innovative, biologically-grounded strategy for expanding network capacity in continual learning.\n\n3. The proposed method applies to both spiking and non-spiking neural networks, and achieves 0 BWT."}, "weaknesses": {"value": "1. The experiments are mainly performed with CIFAR-100 and Tiny-ImageNet of relatively small image scales. Does the proposed method apply to larger-scale images, such as 224*224 images of ImageNet (subsets)?\n\n2. The proposed method seems to provide very marginal performance gains under more clear distribution shifts (i.e., 5-Dataset).\n\n3. As an architecture-based method, similar to other baselines, the proposed method may apply to only task-incremental setting.\n\n4. What’s the total parameter cost of the proposed method, compared to other baselines?"}, "questions": {"value": "My major concerns lie in the applicability of the proposed method. Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2uvUnbEuFU", "forum": "KjxS4AgFol", "replyto": "KjxS4AgFol", "signatures": ["ICLR.cc/2026/Conference/Submission12202/Reviewer_M3Db"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12202/Reviewer_M3Db"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760883891777, "cdate": 1760883891777, "tmdate": 1762923149476, "mdate": 1762923149476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper mainly focuses on the catastrophic forgetting in continual learning and presents a biologically inspired framework termed the Multi-Synaptic Cooperative Network (MSCN). Drawing inspiration from the organization of biological neural systems, MSCN enhances the representational capacity of a fixed network architecture by associating multiple plastic synapses with each neuronal connection, thereby enabling flexible information encoding without structural expansion. In addition, the authors propose a bio-inspired plasticity modulation mechanism that employs local eligibility traces to adaptively regulate synaptic updates. During learning, this mechanism selectively strengthens, weakens, or maintains specific synapses based on their relevance to the current task. This strategy helps prevent interference between tasks and reduces forgetting. Extensive experiments on multiple benchmark datasets demonstrate the superior performance and robustness of MSCN compared to existing approaches"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.Sound methodology: The proposed MSCN framework is well-motivated by biological inspiration and conceptually well-grounded. The idea of equipping each neuronal connection with multiple synapses to enhance representational capacity within a fixed architecture is both intuitively appealing and theoretically sound.\n2.Comprehensive experiments: The experimental evaluation is thorough and well-executed. The paper benchmarks MSCN on multiple datasets using both spiking neural networks (SNNs) and artificial neural networks (ANNs), demonstrating its broad applicability across architectures. Moreover, the analysis under various experimental settings—such as different task order scenarios—provides a comprehensive understanding of the method’s robustness and generalization ability."}, "weaknesses": {"value": "1. Lack of clarity: Although the overall framework is well-structured, the definition and notation of the multiple-synapse mechanism are somewhat ambiguous. In particular, the variables such as s, j, and f in Equations (2) and (3) are not clearly explained.\n2. Limited performance improvement and trade-offs: The performance gains of MSCN are relatively modest, particularly on the 5-Datasets benchmark. Moreover, as shown in Table 3, although MSCN achieves shorter training times, this advantage comes at the cost of reduced performance, suggesting a trade-off between efficiency and accuracy that warrants further discussion.\n3. Scalability and applicability to large-scale settings: The current experiments are limited to datasets with relatively small numbers of classes and models of modest size (e.g., MLPs, ResNet-18). The absence of evaluations on large-scale datasets such as ImageNet-1K or on modern architectures like Vision Transformers (ViT) makes it difficult to assess MSCN’s scalability and practicality in more complex, real-world scenarios. In particular, the computational and memory overhead, as well as the training stability of multi-synaptic connectivity in large models, remain unexplored."}, "questions": {"value": "1.In Figure 3 (a–c), the current grayscale visualization makes it somewhat difficult to distinguish key patterns. Using color plots or more distinctive contrast could improve readability and help highlight the differences between conditions more clearly.\n2. Moreover, in Figure 3, EWC shows large accuracy fluctuations on the first task, likely due to task difficulty variations, whereas MSCN maintains notably stable performance. It would be helpful to clarify how MSCN achieves such robustness to task order and difficulty.\n3.Most baseline comparisons are made against architecture-based methods, yet the manuscript does not clarify the network size or total parameter count. Since each connection in MSCN contains K synapses, it is unclear how this differs in practice from simply increasing the network width by a factor of K. When K = 3, for instance, does the improvement primarily stem from the increased parameter count rather than the proposed mechanism itself?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XpyOJqKcry", "forum": "KjxS4AgFol", "replyto": "KjxS4AgFol", "signatures": ["ICLR.cc/2026/Conference/Submission12202/Reviewer_dFvv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12202/Reviewer_dFvv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734604980, "cdate": 1761734604980, "tmdate": 1762923149003, "mdate": 1762923149003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Multi-Synaptic Cooperative Network (MSCN), a biologically inspired framework for continual learning. The core idea is to model multiple synaptic connections between neuron pairs, modulated by local synaptic activity via eligibility traces, to enhance representational capacity and robustness within a fixed network architecture."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of integrating multi-synaptic cooperation with localized plasticity modulation is biologically inspired and potentially novel.\n2. The focus on improving robustness to task-order variation and scalability without dynamic network expansion targets significant practical limitations of existing architecture-based methods."}, "weaknesses": {"value": "1. The abstract and introduction present the work as \"the first continual learning framework that explicitly leverages the multi-synaptic collaboration mechanism.\" This claim is overstated and needs significant qualification. The concept of having multiple, plastic pathways or \"synapses\" between units is not new. \n2. The authors must clearly differentiate MSCN from these related approaches. Is the novelty primarily in the specific biological inspiration(the explicit modeling of multiple contacts per axon-dendrite pair) and its integration with eligibility traces? This needs to be precisely stated and contrasted with the prior art.\n3. The statement that \"the brain achieves continual learning without suffering from dynamic structural growth\" is a simplification. While the overall neuron count is relatively stable in adulthood, synaptic formation and pruning (structural plasticity) are fundamental to learning.\n4. While capacity (CAP) is analyzed, the direct computational cost (FLOPs, memory footprint, training/inference time) of maintaining and updating P synapses per connection is not thoroughly discussed. For a method aiming for scalability, this is an important practical consideration. Table 3 mentions training time but under a fixed parameter budget; a discussion of the inherent cost of the multi-synaptic structure is warranted."}, "questions": {"value": "1. How does the proposed multi-synaptic connectivity fundamentally differ from, for example, maintaining a per-parameter importance score (as in regularization-based methods) or learning a supermask over a fixed, over-parameterized network? What specific capabilities does the explicit modeling of multiple, independent synapsesenable that these other approaches do not?\n2. The eligibility trace mechanism is a key component. In the context of ANNs trained with backpropagation, what is the precise advantage of this bio-inspired, activity-dependent modulation over a standard gradient-based update? The ablation shows it helps, but the theoretical or empirical reasonwhy this specific form of modulation is particularly effective for task-order robustness remains unclear.\n3. The results show performance saturates with increasing P. Was there any exploration into making P adaptive or variable across layers/connections? The biological system it aims to emulate is highly heterogeneous, not uniform.\n4. Have you considered comparing against a strong, non-biologically-inspired baseline that also uses a fixed but over-parameterized network (e.g., a simple, wider network) to see if the gains are specifically due to the structured, multi-synapticnature of the over-parameterization, rather than just increased capacity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RdoZDVpJVl", "forum": "KjxS4AgFol", "replyto": "KjxS4AgFol", "signatures": ["ICLR.cc/2026/Conference/Submission12202/Reviewer_CnT3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12202/Reviewer_CnT3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916904591, "cdate": 1761916904591, "tmdate": 1762923148609, "mdate": 1762923148609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a continuous learning framework for multi-synaptic cooperative networks (MSCN), inspired by the structural and plasticity mechanisms of multi-synaptic connections in biological neural systems. By introducing multi-synaptic connections within a fixed network architecture and integrating plasticity regulation based on local synaptic activity, MSCN aims to enhance the model's representational capacity, resistance to forgetting, and robustness to task sequence variations in continuous learning tasks. Experiments conducted across multiple benchmark datasets—including PMNIST, CIFAR-100, TinyImageNet, and 5-Datasets—covering both spiking neural networks (SNNs) and artificial neural networks (ANNs) demonstrate that MSCN outperforms existing state-of-the-art methods in terms of accuracy and forgetting suppression"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces and models the multi-synaptic coordination mechanism in continuous learning for the first time. Unlike traditional network expansion or pruning methods, MSCN enhances the overall model's capacity by strengthening the representational power of individual connections.\n    \n2. The paper has a clear structure, with the Methods section describing the implementation of both SNN and ANN.\n    \n3. This work provides new insights for bio-inspired artificial intelligence models."}, "weaknesses": {"value": "1. Table 1 compares MSCN with various baseline methods such as EWC, GPM, PackNet, SupSup, etc. However, the paper does not explicitly state whether all baseline methods were run under the identical “multi-head, task label known” setting.Regularization-based and replay-based methods are typically employed more frequently in “class-incremental learning” settings. Comparing their performance in this context to architecture-based approaches (PackNet, SupSup, WSN, MSCN) that achieve BWT=0 in “task-incremental learning” settings is profoundly unfair and exaggerates the latter's advantages.\n\n2. The core modulation function f_mod (Equation 7) in this paper is directly referenced from the work of Zhang et al. (2023). While its integration with a multi-synaptic architecture constitutes a contribution, the paper fails to clearly delineate in its presentation which elements are cited and which represent original innovations."}, "questions": {"value": "1. Can the author clearly specify the specific experimental settings for all comparison methods?\n\n2. The paper demonstrates MSCN's robustness to task orderings. How does the overlap degree of synaptic masks m_j vary under different task sequences? Furthermore, how does the qualification trace f_mod mechanism dynamically preserve or allocate synapses across different task orders?\n\n3. All current experiments are conducted under task-specific incremental learning settings. Could the authors conduct experiments in more realistic and challenging incremental learning scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yhTpEhlfU8", "forum": "KjxS4AgFol", "replyto": "KjxS4AgFol", "signatures": ["ICLR.cc/2026/Conference/Submission12202/Reviewer_jVm8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12202/Reviewer_jVm8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980086072, "cdate": 1761980086072, "tmdate": 1762923148221, "mdate": 1762923148221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}