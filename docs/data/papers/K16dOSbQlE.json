{"id": "K16dOSbQlE", "number": 16678, "cdate": 1758267568042, "mdate": 1759897225475, "content": {"title": "Sample-efficient diffusion-based control of complex nonlinear systems", "abstract": "Complex nonlinear system control faces challenges in achieving sample-efficient, reliable performance. While diffusion-based methods have demonstrated advantages over classical and reinforcement learning approaches in long-term control performance, they are limited by sample efficiency. This paper presents SEDC (Sample-Efficient Diffusion-based Control), a novel diffusion-based control framework addressing three core challenges: high-dimensional state-action spaces, nonlinear system dynamics, and the gap between non-optimal training data and near-optimal control solutions. Our approach introduces a novel control paradigm by architecturally decoupling state-action learning and decomposing dynamics, while a guided self-finetuning process iteratively refines the control policy. These coordinated innovations allow SEDC to achieve 39.5\\%-47.3\\% better control accuracy than baselines while using only 10\\% of the training samples, as validated across multiple complex nonlinear dynamic systems. Our approach represents a significant advancement in sample-efficient control of complex nonlinear systems. The implementation of the code can be found \\href{https://anonymous.4open.science/r/DIFOCON-C019}{here}.", "tldr": "We purpose a new diffusion-based controller that masters complex nonlinear systems with 90% less training data than state-of-the-art methods.", "keywords": ["Complex System", "Data-driven Control", "Generative Model", "Diffusion Model", "AI for Science"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/18274201e3302c22d13f59bec25e4a350677efb3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SEDC (Sample-Efficient Diffusion-based Control) to solve the sample inefficiency of diffusion-based control for nonlinear systems with limited, non-optimal data. SEDC combines three innovations: (1) Decoupled State Diffusion (DSD), which diffuses only on state trajectories (reducing dimensionality) and uses a separate inverse dynamics model to infer actions; (2) Dual-Mode Decomposition (DMD), a dual-UNet denoiser imposing an inductive bias by decomposing dynamics into linear and nonlinear components; and (3) Guided Self-finetuning (GSF), which iteratively refines the policy by augmenting training data with cost-guided trajectories. The insight is that this decoupled and decomposed architecture simplifies learning, while GSF bridges the data-optimality gap. Experiments on systems like Burgers and Kuramoto show SEDC achieves 39.5%-47.3% higher accuracy, matching SOTA performance with only 10% of the training data, with further validation on high-dimensional PDE tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is exceptionally well-written and organized. It clearly articulates three core challenges (dimensionality, nonlinearity, data optimality) and systematically presents three corresponding solutions (DSD, DMD, GSF), making the overall narrative easy to follow.\n- The work addresses a significant and practical bottleneck in data-driven control: the high sample cost of diffusion models, especially when trained on limited, suboptimal offline data. The problem analysis is both insightful and highly relevant.\n- The proposed methodological innovations, particularly DSD and DMD, are well-motivated and provide strong inductive biases. Decoupling state diffusion (DSD) to reduce complexity and decomposing dynamics (DMD) to handle nonlinearity are both elegant and effective architectural contributions.\n- The framework is supported by exceptionally thorough and convincing experimental validation. Beyond demonstrating high sample efficiency in the main results, the paper includes extensive validation on high-dimensional PDE tasks, and noisy observations, which strongly supports the robustness and scalability of the claims."}, "weaknesses": {"value": "- The Guided Self-Finetuning (GSF) component appears to rely on access to an interactive ground-truth simulator to generate new state trajectories for finetuning. This assumption seems to contradict the initial motivation of learning from a purely fixed, offline dataset, blurring the line between offline and online learning.\n- The inverse dynamics model in DSD learns a deterministic \"average\" policy (the conditional expectation) for a given state transition. This approach may be insufficient or suboptimal for complex systems with true multi-modality, where multiple, distinct control actions (e.g., with very different costs) could produce the same transition.\n- While effective and validated in ablations, the conceptual novelty of the GSF component is less significant than the architectural innovations of DSD and DMD. It largely follows an established paradigm of iterative self-training, making it more of a procedural refinement than a foundational contribution."}, "questions": {"value": "1.  Does the GSF module require an interactive simulator for finetuning, or can it operate in a purely offline setting?\n2.  How does DSD's \"average\" policy handle control multi-modality, where averaging distinct valid actions (e.g., high-cost vs. low-cost) could result in a suboptimal or invalid action?\n3.  Why is the 2nd-order DMD approximation so robust for higher-order nonlinearities, and what are its anticipated failure modes as system complexity increases?\n4.  Regarding the DSD trade-off: in which system classes (e.g., chaotic, high action dimension) might learning the inverse dynamics model become as sample-inefficient as the original joint diffusion problem?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wvqtCXVN4D", "forum": "K16dOSbQlE", "replyto": "K16dOSbQlE", "signatures": ["ICLR.cc/2026/Conference/Submission16678/Reviewer_wVxQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16678/Reviewer_wVxQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761206019981, "cdate": 1761206019981, "tmdate": 1762926735714, "mdate": 1762926735714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a diffusion-based method to control nonlinear systems, SEDC (Sample-Efficient Diffusion-based Control).\nSEDC is developed to address: (1) high-dimensional state-action spaces, (2) nonlinear system dynamics, and (3) how to go from non-optimal training data to near-optimal control.\nTheir contributions are structured in three components: \n- (i) Decoupled State Diffusion (DSD) structuring the diffusion process. \n    - (i-1) They train their diffusion process to generate trajectories of states. \n    - (i-2) In parallel, they train an inverse dynamics model to get the control inputs.\n    - (i-3) They use in-painting to impose both the initial and the final states. \n    - (i-4) They add a cost optimization term directly in the denoising diffusion estimate. \n- (ii) Dual-Mode Decomposition (DMD), their network architecture is composed of three networks (one linear and two UNets), imposing a structure intended to explicitly separate the linear and non-linear parts of the system. \n- (iii) Guided Self-Finetuning (GSF), to go beyond non-optimal training data, they rollout the trained policy by interacting with the environment, and enrich the training dataset with these new trajectories.\n\nThey demonstrate the effectiveness of their method with experiments over challenging control tasks (1-D Burgers, Kuramoto and the inverted pendulum). They show that their method requires fewer training samples than pre-existing methods (hence the name SEDC). The experiment section includes an ablation study of each of their components (DSD, DMD and GSF). In the appendix, they provide an expanded analysis of their method, in particular, they evaluate SEDC over harder tasks (e.g. the Jellyfish locomotion) to demonstrate its scalability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The presentation of the method in Sec 4 is mostly clear, with each component well presented.\n- The authors conducted many experiments on SEDC, with a good ablation study and a well-organized appendix section.\n- While I'm not familiar with specific network architectures to address non-linearities of complex systems, their Dual-Mode Decomposition seems to be a good contribution, according to their ablation study.\n- From their experiments, SEDC seems better than DiffPhyCon, which is designed to address the same problems."}, "weaknesses": {"value": "To me, the paper is not well-placed in the literature, affecting not only the comparison with prior works, but more worryingly, it concerns how the problem is defined, and so how SEDC's components are presented.\n\n- (Major-1) The goal of this paper is to learn a policy to control dynamic systems. The field of policy learning is divided in subfields depending on the type of data used. Behavior Cloning (BC) operates on a dataset of trajectories without associated rewards/costs. Offline Reinforcement Learning (RL) works with a fixed dataset of state-action-reward trajectories. On the opposite, online RL assumes access to the environment.\nIn this paper, the authors divide the data-driven approaches in three separate types: BC, RL and diffusion-based. But, to my understanding, BC, Offline-RL and Online-RL are fields, assuming different access to rewards and to the environment. While diffusion-based policies are methods that belong to these fields.\nSEDC is introduced as a data-driven method, first assuming a fixed dataset, and reporting performances depending on the size of this dataset. But its third component, GSF, consists in collecting new trajectories by interacting with the environment (page 12, line 622). So, it seems to me, SEDC is an offline-to-online method.\n\n- (Major-2) The problem is not well defined. Methods are first evaluated based on their ability to reach a target state $y_f$, but then the authors add an energy cost $J$ minimization term. It becomes unclear what the objective is. Fig 2, SEDC is compared to methods that do not have access to $J$ (as most are BC methods), except for DiffPhyCon. The idea of presenting the Pareto front of these two objectives is relevant to aggregate the results, but it hides the lack of a clear objective. The idea of plotting the Pareto front should have been credited to DiffPhyCon. \n\n- (Major-3) Most SEDC's components are design choices presented as new ideas.\n    - 3-1) The idea of using diffusion for control is not new, and while the wording may differ, all diffusion-based methods mentioned seek to train a policy to control a system. The application to physical systems (e.g. Burgers dynamics, i.e nonlinear wave propagation and turbulent fluid flow) is interesting, but the paper contribution is not to be the first to formulate a control problem as a denoising diffusion process.\n    - 3-2) DSD, the idea of diffusing only over the state, is a pre-existing popular design choice. DecisionDiffuser [1] does the same, it diffuses a state-trajectory and learns an inverse dynamics model. So the main contribution highlighted in the abstract \"Our approach introduces a novel control paradigm by architecturally decoupling state-action learning and decomposing dynamics\" was already present in the work mentioned by the authors as the reference for diffusion-based control frameworks...\n    - 3-3) While in-painting the target position was not present in the cited works, authors should mention that the first work on diffusion-based policies, by Janner et al [2], already used in-painting (for the initial position). This is also related to remarks (Major-4) and (Minor-5).\n    - 3-5) GSF, the idea of rolling out the policy by interacting with the environment, is the simplest version of offline-to-online.\n    \n- (Major-4) Diffusion-based control is described as learning to diffuse the whole trajectory at once. While it was the case in the first diffusion-based control works [2], diffusion policies are now mostly used to generate chunks of trajectories [3], predicting the states or actions over a mid-term horizon. Authors should at least mention this, and maybe consider how their method can work with chunks. Diffusing over the whole trajectory is only possible when T is small (10 or 15 for Burgers and Kuramoto) or when the state space is very small (dimension 2 for the inverted pendulum). In particular, it raises the question of how to adapt the target in-painting and how to define the objective.\n\n- (Minor-5) The target inpainting is misleadingly presented as a \"hard constraint\", but as $\\hat y_T$ differs from $y_T$, it is not a hard constraint, it is actually the target loss of the control system.\n\n- (Major-6) Overall, some important papers are missing in the introduction and in the related work section. For diffusion policies, \"Diffusion policy\" [3] is not mentioned, while being the most cited work in the field. Behavior PPO is oddly the only work cited to represent \"RL\". First, it should be made clear that \"RL\" here is actually offline-RL. Second, even when restraining RL to offline-RL, BPPO is not the only relevant approach. In particular, using diffusion/flow matching for offline-RL and offline-to-online is a trendy topic [4,5], and I believe SEDC belongs to these subfields.\n\n- (Minor-7) BPPO is introduced as \"Batch PPO\", instead of \"Behavior PPO\" in the introduction, line 041.\n\n- (Concern-8) I'm not familiar with Burgers and Kuramoto dynamics, so it is hard for me to judge the number of trajectories used (20000). But using 90000 trajectories (+ resampling offline-to-online \"GSF\") for the inverted pendulum, or even 10% of it, 9000 trajectories, does not convince me of the sample efficiency.\n\n- (Minor-9) The background paragraph on diffusion mixes general diffusion mathematical formulations with applications to control.\n\nOverall, I think this paper should be framed as an application of diffusion control to physical systems, not as developing a new diffusion paradigm (\"our approach introduces a novel control paradigm\"). \nUpon my understanding, this paper does offline-to-online RL, and therefore it should position itself with respect to that field. In particular, in their experiments, authors compared SEDC to methods requiring less information and so belonging to different fields (mainly BC). Most of the contributions presented as innovations are design choices, and should rather be presented as such, focusing on their applications to new complex systems.\n\n[1] Ajay, A., Du, Y., Gupta, A., Tenenbaum, J., Jaakkola, T., & Agrawal, P. (2022). Is conditional generative modeling all you need for decision-making?\n\n[2] Janner, M., Du, Y., Tenenbaum, J. B., & Levine, S. (2022). Planning with diffusion for flexible behavior synthesis\n\n[3] Chi, C., Xu, Z., Feng, S., Cousineau, E., Du, Y., Burchfiel, B., ... & Song, S. (2025). Diffusion policy: Visuomotor policy learning via action diffusion\n\n[4] Hansen-Estruch, P., Kostrikov, I., Janner, M., Kuba, J. G., & Levine, S. (2023). Idql: Implicit q-learning as an actor-critic method with diffusion policies.\n\n[5] Park, S., Li, Q., & Levine, S. (2025). Flow q-learning."}, "questions": {"value": "- Why are the experiments with the 2D PDE Jellyfish locomotion only presented in the appendix, while being a more challenging system than the ones presented in the main paper ?\n- I did not find anywhere in the paper the number of trajectories recollected with GSF (neither in the main paper nor in the appendix). While Fig 5 presents results after 2 rounds, it is not clear how many trajectories are collected per round. Is it only one trajectory per round, as described in the algorithm Appendix-A ?\n- Could the authors explain how the guidance done using the gradient of the cost, Eq-3, relates to classifier-based guidance diffusion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Small comments that I am now sure where to write.\n\nThe link at the end of the abstract leads to a page saying \"Thank you for reviewing our NIPS 2025 manuscript\". \nAnd the name in the URL refers to a paper rejected last year at ICLR 2025, \"DIFOCON\" \"A Diffusion-based Generative Approach for Model-free Finite-time Control of Complex Systems\".\nWhile resubmitting a paper is not a problem, here the authors indirectly partially de-anonymize their submission.\nAlso, the Google Drive said to contain the pre-collected data seems empty.\n\nNote on the use of LLMs: This review was done and written without the assistance of any LLMs."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OJJi1LBfLS", "forum": "K16dOSbQlE", "replyto": "K16dOSbQlE", "signatures": ["ICLR.cc/2026/Conference/Submission16678/Reviewer_W3tF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16678/Reviewer_W3tF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733547135, "cdate": 1761733547135, "tmdate": 1762926734673, "mdate": 1762926734673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Sample-Efficient Diffusion-Based Control of Complex Nonlinear Systems, a framework for sample efficient control of nonlinear systems. The paper introduces three seperate techniques towards this end: 1) decoupling state-based planning with diffusion from an inverse dynamics model 2) a technique for decomposing the trajectory generator into linear and nonlinear components and 3) a technique for iterative refinement by rolling out the current policy. The paper applies the framework to several nonlinear systems, including high-dimensional PDE control."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "$\\textbf{Improvement Over Baselines}$: On the baselines considered, the proposed approach achieves substantially lower target tracking losses with a much smaller dataset than baselines.\n\n$\\textbf{Novelty}$: The idea of decomposing the state dynamics into linear and nonlinear components within the diffusion backbone in quite intriguing."}, "weaknesses": {"value": "$\\textbf{Limited Experimental Evaluation}$: While the application of the approach to PDEs is refreshing, a major weakness of the paper is that it only compares to baselines on 'toy' robotics problems such as the inverted pendulum. Without comparing to baselines on standard benchmark environments (e.g. hopper, humanoid) it's unclear if the improvements from the approach hold in more realistic settings. In short, empirical evaluation is substantially below the bar for acceptance at ICLR. \n\n$\\textbf{Related Work and Novelty}$: A quick google search found several important related works which were not adressed in the paper: \n\n\n\"Learning Coordinated Bimanual Manipulation Policies using State Diffusion and Inverse Dynamics Models\" (Chen et al, 2025)\n\n\"Latent Diffusion Planning for Imitation Learning\" (Xie et al, 2025)\n\nThese works also explore decoupling state diffusion from actions, but demonstrate this core idea in substantially more scaled up settings. Given these prior works, there's minimal contributions that I believe the ICLR community will value.\n\n$\\textbf{Three different ideas, one paper:}$ The three contributions noted above could each have an entire paper written about them. Moreover, these ideas have little to do with one another, and thus a) none of the ideas are discussed with enough detail in the paper and b) it's difficult to understand what's the key thing I should take away from reading the paper. \n\n$\\textbf{Fit for ICLR}$: I rather enjoy the notion of decomposing linear and linear and non-linear dynamics, but I do not believe ICLR is the best venue for this paper. I believe a venue such as L4DC will have more appreciation for these ideas. For the ICLR community to take note of these ideas, it really comes down to whether improvements on standard benchmarks can be shown."}, "questions": {"value": "Questions: \n\n- Can you show that the approach yields benifits on standard benchmarks? \n\n- Can you compare to the prior works I noted?\n\n- Why do all these techniques need to be combined together to achieve strong results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "72vg9u4A16", "forum": "K16dOSbQlE", "replyto": "K16dOSbQlE", "signatures": ["ICLR.cc/2026/Conference/Submission16678/Reviewer_3SSR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16678/Reviewer_3SSR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936987541, "cdate": 1761936987541, "tmdate": 1762926734298, "mdate": 1762926734298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers using diffusion-based methods to control complex nonlinear systems with high sample efficiency and reliable performance. It presents sample-efficient diffusion-based control (SEDC), which is claimed to address three core challenges: high-dimensional state-action spaces, nonlinear system dynamics, and the gap between non-optimal training data and near-optimal control solutions."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The structure of the paper is clear. \n\n1. Empirically, the proposed method works better than the baselines."}, "weaknesses": {"value": "1. The paper's technical writing can be improved. The use of the notations is not fully clear to me. Some examples:\n    - Line 53: the definitions of $y^P$ and $u^M$ are not clear. \n    - The $P$ in Line 53 and the $P$ in Line 133 seem different. \n    - Section 3.1 introduces that the dataset $D = \\\\{\\mathbf{u}^{(i)}, \\mathbf{y}^{(i)}\\\\}_{i=1}^P$, but Line 190 says that $\\mathbf{x}, k, \\mathbf{y}_0^*, \\mathbf{y}_f$ are sampled from the dataset. Seems that the dataset does not contain all of them. Also, the definition of $\\mathbf{x}$ is unclear. \n    - Line 155, $\\mathbf{x}^0$ is used to denote the clean trajectory, but in Line 206, the superscript $0$ is used for the final denoised output from the diffusion model, which is different from the clean trajectory from the dataset. \n    - Line 258: $B$ is not defined. \n\n1. Many claims in the paper are not supported by evidence or citations. When talking about the drawbacks of existing works, the paper usually only cites the existing work and states the drawback, without mentioning which paper has discussed this drawback. Some examples:\n    - Line 53: \"This joint distribution implicitly encodes system dynamics of state transitions under external control inputs, which often leads to physically inconsistent trajectories when training samples are insufficient.\"\n    - Line 60: \"learning effective control policies from limited data remains particularly challenging for complex systems with strong nonlinearity, such as fluid dynamics and power grids.\"\n    - Line 103: \"Supervised learning and reinforcement learning offer adaptive approaches but can also struggle with long-horizon credit assignment and compounding errors.\"\n    - Line 201: \"Jointly modeling the state-action distribution is highly sample-intensive and risks generating physically inconsistent trajectories.\"\n    - Line 249: \"the limitations of single-network approaches that struggle to model both simultaneously\"\n    - Line 255: \"modeling a dominant linear part and a subtle nonlinear correction is a more stable and sample-efficient task than forcing a monolithic network to learn the entire complex function from scratch\"\n\n1. The report of the experimental results lacks error bars in the figures and standard deviation in tables, which are essential for observing the statistical significance. \n\n1. The paper claims to address \"high-dimensional state-action spaces\", but the proposed experiments are low-dimensional with simple dynamics (I cannot see why the Burgers dynamics have 128 states). \n\n1. The ablation study of the effectiveness of DMD can be improved. The current experiment compares using only the linear intermediate output of the denoising network and the original nonlinear output, but this cannot support the claim that \"single-network approaches struggle to model both simultaneously.\" To support this claim, experiments should be designed to compare the current method with using one network to learn $\\mathbf{O}_1 + \\mathbf{O}_2$. \n\n1. The RL baseline chosen by the authors is not based on diffusion policies, which makes it vague that whether the performance improvement is from the proposed framework or the use of the diffusion model. \n\n1. The proposed GSF method seems standard in online fine-tuning for BC policies."}, "questions": {"value": "1. This paper seems highly related to offline reinforcement learning. What are the differences? Why are none of the offline RL works discussed?\n\n1. Why does the paper propose to generate the whole trajectory instead of a feedback control policy?\n\n1. Section 4.1, the paper claims to address the risks of generating physically inconsistent trajectories. How can the proposed DSD method do this, given that the inverse dynamics model is learned, and no physical constraints are considered when generating the state-only trajectories? \n\n1. How many seeds are used in the experiments? \n\n1. Given the Burgers dynamics in Appendix B.2, why does this dynamics have 128 states?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LUHG763AME", "forum": "K16dOSbQlE", "replyto": "K16dOSbQlE", "signatures": ["ICLR.cc/2026/Conference/Submission16678/Reviewer_GV3U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16678/Reviewer_GV3U"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940062216, "cdate": 1761940062216, "tmdate": 1762926733672, "mdate": 1762926733672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}