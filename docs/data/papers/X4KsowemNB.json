{"id": "X4KsowemNB", "number": 5237, "cdate": 1757873611019, "mdate": 1759897986294, "content": {"title": "SF-Mamba: Rethinking State Space Model for Vision", "abstract": "The realm of Mamba for vision has been advanced in recent years to strike for the alternatives of Vision Transformers (ViTs) that suffer from the quadratic complexity.\nWhile the recurrent scanning mechanism of Mamba offers computational efficiency, it inherently limits non-causal interactions between image patches.\nPrior works have attempted to address this limitation through various multi-scan strategies; however, these approaches suffer from inefficiencies due to suboptimal scan designs and frequent data rearrangement. Moreover, Mamba exhibits relatively slow computational speed under short token lengths, commonly used in visual tasks.\nIn pursuit of a truly efficient vision encoder, we rethink the scan operation for vision and the computational efficiency of Mamba.\nTo this end, we propose SF-Mamba, a novel visual Mamba with two key proposals: auxiliary patch swapping for encoding bidirectional information flow under an unidirectional scan and batch folding with periodic state reset for advanced GPU parallelism.\nExtensive experiments on image classification, object detection, and instance and semantic segmentation consistently demonstrate that our proposed SF-Mamba significantly outperforms state-of-the-art baselines while improving throughput across different model sizes.\nWe will release the source code after publication.", "tldr": "A high-performance, high-speed vision encoder based on improved information flow and GPU parallelism", "keywords": ["Mamba", "State space model", "vision encoder"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b8a86c3e4400b7241a0eab79c941f03e59e34bbf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SF-Mamba, a vision model designed to improve the efficiency and performance of State Space Models (SSMs) for visual tasks. It proposes two main contributions: Auxiliary Patch Swapping to enable bidirectional information flow in a unidirectional scan, and Batch Folding with Periodic State Reset to improve GPU parallelism for short sequences common in vision."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors correctly identify that visual Mamba models are often slow not just due to scan strategies but because of suboptimal GPU utilization on the short sequences common in vision tasks. The \"Batch Folding\" technique is a clever, hardware-aware solution that provides significant speedups."}, "weaknesses": {"value": "**Limited Novelty**\n\nOverlap with Adventurer [1]: The core idea of swapping auxiliary tokens to enable bidirectional information flow is conceptually almost identical to the flip operation between consecutive blocks proposed in Adventurer (CVPR 2025). Adventurer also uses this technique to address the causality constraint in unidirectional visual models. The absence of any discussion, citation, or empirical comparison to this highly relevant prior work is a major oversight and significantly weakens the paper's claim to novelty.\n\nBatch Folding as an Engineering Optimization: While the Batch Folding technique is shown to be effective, it functions more as a low-level implementation optimization or an \"engineering trick\" rather than a novel academic contribution. Its impact is valuable for performance but its conceptual depth is limited for a top-tier conference paper.\n\n\n**Questionable Motivation**\n\nThe paper's motivation rests on the claim that multi-directional scans are inherently slow, but this is not convincingly proven and contradicts previous findings.\n\nContradiction with VMamba: The authors claim inefficiency in multi-directional scans, but this conclusion is challenged by the original VMamba paper, which demonstrated that a multi-directional scan could achieve comparable throughput to a single-directional one. This discrepancy is not addressed.\n\n\nUnfair Ablation Study (Table 3): The ablation study in Table 3, which aims to show the slowness of multi-scan methods, is methodologically flawed. The comparisons do not keep the parameters and MACs constant across different scan methods. For example, a \"parallel bi-scan\" would naturally have a higher computational cost unless channel dimensions are halved to ensure a fair comparison, which does not appear to be the case here. This invalidates the conclusion that the proposed uni-scan with swapping is inherently faster than a properly configured multi-scan architecture.\n\n\n**Writing and Paper Structure**\n\nAn extensive amount of space in the main paper is used to describe the MambaVision macro-architecture. This detailed background could be moved to the appendix. This would free up valuable space to provide a more thorough analysis of the results on downstream tasks like COCO and ADE20K, which currently feel rushed and lack sufficient detail.\n\n\n**Marginal Performance on Downstream Tasks**\n\nWhile SF-Mamba excels on ImageNet classification, its advantages diminish significantly on more complex downstream tasks. The performance improvements over the MambaVision baseline are minimal for semantic segmentation on ADE20K.\nMore critically, on the MS COCO object detection and instance segmentation tasks, some SF-Mamba configurations underperform the MambaVision baseline they are supposed to improve upon.\n\n\n[1]. Causal Image Modeling for Efficient Visual Understanding. CVPR2025."}, "questions": {"value": "Please check weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RJtL3ByBf5", "forum": "X4KsowemNB", "replyto": "X4KsowemNB", "signatures": ["ICLR.cc/2026/Conference/Submission5237/Reviewer_ypap"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5237/Reviewer_ypap"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760521595953, "cdate": 1760521595953, "tmdate": 1762917965116, "mdate": 1762917965116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents SF-Mamba, an improved visual Mamba architecture for vision tasks. SF-Mamba introduces several innovations: auxiliary patch swapping with an extra token, which enables bidirectional information flow during a unidirectional scan, and batch folding with periodic state reset, which enhances GPU parallelism. Extensive experiments demonstrate that SF-Mamba significantly outperforms existing models in terms of accuracy and throughput across image classification and segmentation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is good. Previous methods addressed the problem from the perspective of multiple scans, whereas this paper innovatively addresses the Mamba architecture's issue of sequential reasoning by focusing on a single-scan approach for causal information swapping.\n\n2. By restructuring the tensor dimensions of batch data, the paper improves parallelism from a GPU computation perspective, which benefits the acceleration of large-scale training.\n\n3. The experiments are comprehensive, covering CNN architectures, transformer architectures, and hybrid architectures. Detailed ablation studies and supplementary materials provide a thorough exploration of the proposed method."}, "weaknesses": {"value": "1. The performance improvement of the model is not significant. The accuracy improvement is not obvious compared to the baseline Mamvbavision. Moreover, the gain in speed is not so significant.\n\n2. The comparison models do not include Fast R-CNN or Faster R-CNN for the object detection experiment, and the comparison in the validation set is insufficient.\n\n3. The method of exchanging token positions to achieve contextual structure interaction may not be the optimal approach. The current ablation studies do not directly prove that the performance improvement is caused by the position swapping.\n\n4. What about other hyperparameters designed in the results in Figure 4? Does it have an optimal design for performance and speed?"}, "questions": {"value": "Please see the weakness. I would raise the ratings if the rebuttal addressed the questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KgspInhZGp", "forum": "X4KsowemNB", "replyto": "X4KsowemNB", "signatures": ["ICLR.cc/2026/Conference/Submission5237/Reviewer_G5Nk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5237/Reviewer_G5Nk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706855746, "cdate": 1761706855746, "tmdate": 1762917964870, "mdate": 1762917964870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SF-Mamba, a novel visual encoder addressing limitations of existing Vision Transformers and visual Mamba models. ViTs suffer from quadratic complexity, while visual Mamba faces non-causal information flow constraints and inefficiency with short tokens. SF-Mamba introduces two core innovations: auxiliary patch swapping for bidirectional information flow under unidirectional scanning, and batch folding with periodic state reset to enhance GPU parallelism. Extensive experiments on image classification, object detection, and segmentation demonstrate superior accuracy-throughput trade-offs compared to SOTA baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper targets critical pain points of visual Mamba (causality constraint, short-sequence inefficiency) with lightweight, non-intrusive solutions.\n2. Comprehensive validation across three fundamental vision tasks (classification, detection, segmentation) with consistent performance gains.\n3. Practical optimizations (e.g., adaptive $\\(B_1\\)$, Triton kernel for swapping) enhance real-world applicability, with code release planned."}, "weaknesses": {"value": "1. The macro-architecture heavily relies on MambaVision’s hybrid (Mamba+Attention) design, lacking significant innovations in overall network structure.\n2. Ablation studies on auxiliary token initialization and discard timing are limited; deeper analysis of their impact on different tasks is needed.\n3. No discussion on generalization to ultra-high-resolution images or low-resource devices (e.g., edge GPUs), restricting scope insights."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "623UxrM3Ux", "forum": "X4KsowemNB", "replyto": "X4KsowemNB", "signatures": ["ICLR.cc/2026/Conference/Submission5237/Reviewer_V3Nn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5237/Reviewer_V3Nn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807882736, "cdate": 1761807882736, "tmdate": 1762917964610, "mdate": 1762917964610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript describes a modification to vision state space models. To improve the processing speed of mamba-based vision models, the authors proposed to 1) auxilliary patch swapping for bidirectional information flow, and 2) batch folding with periodic state reset. Experiments shows positive results on these components."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The manuscript is well-presented and easy to follow.\n+ It is good to see analysis on the inference speed of the mamba based model.\n+ The reset trick is interesting."}, "weaknesses": {"value": "+ The proposed method is only testifed on MambaVision. It should be applied to more mamba-based models to support the claim of \"Rethinking State Space Model for Vision\".\n+ Swapping last token is not equivant to bi-directional scan, and the author failed to prove the superiority of swapping last token. As in table 3, if the attention is removed, swapping last token worse than parallel bi-scan and even series bi-scan. Since attention itself is a undirectional operation, this seems that switching to swapping last token is not working but attention works."}, "questions": {"value": "+ In the manuscript of MambaVision, they utilized the same hardware, but they reported a throughput of  3670 img/s for MambaVision-B. However, in this manuscript, the speed is downgraded to 2974 img/s. Why? If jittering exists, please report mean and std over multiple test runs.\n+  Why Mamba kernel requires 32 parallel threads? If current mamba kernel is not suitable for short sequence of image classification, why do we need parallel scan?  Why do we need mamba? The performance drops severly in Tab.3 if attention are removed.\n+ In Table 3, why the inference troughput drop when attention is removed? If that so, the inference scheme may not be appropriate. Please report performance under large batch size, say, 2048 as in SHViT and EfficientViT.\n+  Can the proposed method be applied to other mamba-based model, say, Vim, VMamba and their variants?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XQo0O4l2VT", "forum": "X4KsowemNB", "replyto": "X4KsowemNB", "signatures": ["ICLR.cc/2026/Conference/Submission5237/Reviewer_MDas"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5237/Reviewer_MDas"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895167126, "cdate": 1761895167126, "tmdate": 1762917964351, "mdate": 1762917964351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}