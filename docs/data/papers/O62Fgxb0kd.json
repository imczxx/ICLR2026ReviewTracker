{"id": "O62Fgxb0kd", "number": 17746, "cdate": 1758280093293, "mdate": 1763687826907, "content": {"title": "Density-Aware Translation of Spurious Correlations in Zero-Shot VLMs", "abstract": "Vision-Language models (VLMs), such as CLIP, achieve powerful zero-shot classification. However, their predictions remain highly sensitive to spurious correlations, where common background or contextual cues dominate predictions over semantic content. Earlier solutions typically rely on fine-tuning, but this undermines the advantages of pre-trained models. Others depend on prompt engineering, which is prone to hallucination issues. In addition, most approaches are limited to a single modality, increasing the risk of misalignment between text and images. In this work, we propose Density-Aware Translation (DAT) that refines image-text similarity scores using a local geometric density term derived from group reference sets. Our approach is motivated by the phenomenon that CLIP embeddings exhibit a modality gap and lie on an anisotropic shell in the feature space: common patterns cluster near the mean, while rare patterns are pushed outward. This geometry creates uneven alignment, where spurious correlations are amplified while semantically meaningful but rare cues are marginalised. To address this, we employ a relative measure that rescales similarities based on embedding density, suppressing overconfident scores in diffuse regions while preserving dense, semantically consistent matches. Experimental results on benchmark datasets demonstrate consistent improvements in worst-group and average accuracy, highlighting density-aware translation as a simple and effective calibration mechanism for reliable zero-shot classification using multimodal models.", "tldr": "", "keywords": ["spurious correlation", "vision-language models", "density"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d45ede673b730da19ad56f3017aa50645fc5cc14.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of spurious correlations in zero-shot Vision-Language Models (VLMs) like CLIP, where models over-rely on frequent but semantically irrelevant cues rather than meaningful content. The authors propose Density-Aware Translation (DAT), a method that refines image-text similarity scores by incorporating a local geometric density term derived from group reference sets. DAT rescales similarities to suppress overconfident scores in sparse regions while preserving dense, semantically consistent matches. The approach operates in a zero-shot regime without fine-tuning or prompt engineering. Theoretical analysis shows that DAT corrects biases in cosine similarity under anisotropic embeddings, aligning scores with Bayes-optimal decisions. Experiments on benchmarks demonstrate consistent improvements in worst-group and average accuracy across multiple VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1)\tDAT consistently improves worst-group and average accuracy across multiple benchmarks and model architectures.\n2)\tFormal proofs show DAT reinstates anisotropy-sensitive terms ignored by cosine similarity, aligning with Bayes-optimal decisions.\n3)\tDAT operates in a zero-shot setting, requires no model parameters, and is computationally efficient compared to baselines like TIE."}, "weaknesses": {"value": "1)\tLimited discussion on the sensitivity of DAT to the quality and diversity of reference sets, especially in datasets with high class or attribute complexity (e.g., FMoW).\n2)\tThe theoretical analysis relies on the Kent distribution and log-SLOF fidelity assumption, which may not fully capture real-world embedding geometries.\n3)\tNo comparison to methods that explicitly model uncertainty or use generative approaches for debiasing, which could provide additional context for DAT’s advantages."}, "questions": {"value": "1)\tCould the authors provide more intuition or visualization for how SLOF captures density in high-dimensional embedding spaces, especially for non-experts?\n2)\tHow does DAT scale to datasets with a large number of spurious attributes or fine-grained classes, and are there computational trade-offs as group diversity increases?\n3)\tBeyond accuracy, has DAT been evaluated on other fairness metrics (e.g., Equality of Opportunity, Demographic Parity)? This is important for assessing its utility in sensitive applications.\n4)\tThe experiments use fixed prompt templates. How would the relative advantage of DAT change if more powerful or domain-specific prompt engineering (e.g., using LLMs) were employed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jfKVUHO8Yd", "forum": "O62Fgxb0kd", "replyto": "O62Fgxb0kd", "signatures": ["ICLR.cc/2026/Conference/Submission17746/Reviewer_3JSL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17746/Reviewer_3JSL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717128191, "cdate": 1761717128191, "tmdate": 1762927588892, "mdate": 1762927588892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The objective of this paper is to address spurious correlations in zero-shot VLMs. This work builds on existing research showing that spurious-correlated embeddings exhibit an anisotropic ellipsoidal structure, with such samples located near the mean of the geometry. To mitigate spurious correlations, the authors propose density-aware translation (DAT), which adjusts the scores of samples to break the spurious relationship. To further improve performance, they introduce an aggregation method over predictions. The visualization in Figure 4 demonstrates the effectiveness of DAT. The study is evaluated on widely accepted benchmark datasets and outperforms existing methods in improving robustness to spurious correlations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I find this paper quite interesting for the following reasons:\n\n1. It leverages previous research showing that spurious-correlated embeddings are located near the mean of an ellipsoidal geometry. The proposed DAT method addresses spurious correlations from a geometric perspective, which I find particularly interesting and novel.\n\n2. It provides a theoretical result showing that DAT yields a Bayes-aligned decision boundary, offering a theoretical justification for its effectiveness.\n\n3. The paper is well written. The background information is clearly presented, and the overall flow of the paper is smooth and easy to follow.\n\n4. The experimental results are solid. The authors evaluate their method across different scenarios within multi-class classification settings. I agree that FMOW is a very challenging dataset, and an improvement of nearly 7% on WG is convincing evidence of the strength of their approach."}, "weaknesses": {"value": "1. The aggregation step lacks justification. In Equation (4), both components could serve as prediction logits. I don’t quite understand the difference between using the average group-specific score (second term) and the max score (first term). Could you clarify why the chosen combination is better?\n\n2. A straightforward baseline would be to use SLOF to detect outliers, i.e., samples that break the spurious correlation. Since CLIP embeddings are often dominated by spurious features (e.g., background), if we set a threshold to flag outliers and then flip the zero-shot prediction, would that work? And compared with the translation results, how close does this simple baseline get?\n\n3. What is the numerical range of  $D_{y,a}(a)$, and how does it vary across groups? Would dividing by this value cause disproportionately large shifts for certain groups? Could you provide a bar plot per dataset to illustrate the distribution across groups?"}, "questions": {"value": "1. Could DAT be used to diagnose spurious concepts or discover new spurious features, similar to the first-step procedure in [1]?\n\n2. In Section 3.2, the authors only evaluate one prompt. Can the evaluation generalize to different prompts and different datasets?\n\n[1] Wu, Shirley, et al. \"Discover and cure: Concept-aware mitigation of spurious correlation.\" International Conference on Machine Learning. PMLR, 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M3jEDT8gaR", "forum": "O62Fgxb0kd", "replyto": "O62Fgxb0kd", "signatures": ["ICLR.cc/2026/Conference/Submission17746/Reviewer_7J4W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17746/Reviewer_7J4W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841214228, "cdate": 1761841214228, "tmdate": 1762927588334, "mdate": 1762927588334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of spurious correlations in vision-language models (VLMs). It proposes Density-Aware Translation (DAT), a method that rescales CLIP similarity scores based on data density, leveraging group reference sets to capture the local geometry of the data."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The idea of addressing the spurious correlation problem in VLMs from a density-based perspective is novel."}, "weaknesses": {"value": "The debiasing approach proposed in this paper (DAT) uses training or validation data as a reference set to capture the geometry of the data. This design contradicts the “zero-shot” claim made by the authors, as it requires access to target images. And the hyperparameters also seem to require target data for tuning. In contrast, competing methods such as ROBOSHOT, TIE, and Perception CLIP do not rely on any training or validation images, making them genuinely zero-shot debiasing approaches. Therefore, the comparison between DAT and these baselines is not fair. \n\nMoreover, the paper introduces a large number of notations, but they are not organized clearly. Many complex equations are left unlabeled, making them difficult for readers to reference. Some equations also lack clarity—for example, it is unclear what the variable w represents in the definition of TMD."}, "questions": {"value": "In Equation (2), embeddings from sparse regions have larger SLOF values, resulting in lower similarity scores. This mechanism appears to encourage similarities among dense regions while penalizing those from sparse regions. According to the abstract, this design aims to “preserve dense, semantically consistent matches.” However, as illustrated in Figure 1(a), frequent but spuriously correlated samples tend to cluster near the mean, whereas rare yet semantically meaningful samples lie in sparser regions. This seems paradoxical. Could the authors please clarify this point?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MdsPZ8mad1", "forum": "O62Fgxb0kd", "replyto": "O62Fgxb0kd", "signatures": ["ICLR.cc/2026/Conference/Submission17746/Reviewer_65tF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17746/Reviewer_65tF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892831701, "cdate": 1761892831701, "tmdate": 1762927587703, "mdate": 1762927587703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free method, Density-Aware Translation (DAT), for mitigating spurious correlations in zero-shot VLMs. The authors propose to rescale image–text cosine similarities using a local density term estimated from small, balanced group reference sets, motivated by the observation that CLIP embeddings are anisotropic and lie on an ellipsoidal shell where frequent cues cluster near the mean while rarer, semantically meaningful cues lie in sparser regions. The authors model group distributions with the Fisher–Bingham distribution and show DAT restores anisotropy-sensitive log-likelihood terms, aligning decisions with Bayes-optimal scoring. Experiments on standard spurious-correlation benchmarks show gains in worst-group and average accuracy across multiple VLMs in a zero-shot setting."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method, DAT, enhances robustness without retraining or gradient updates, making it efficient and broadly applicable to pre-trained vision-language models.\n\nThe authors provides a principled correction for embedding anisotropy via density-aware scoring derived from the Kent distribution, linking geometric intuition with Bayesian optimality."}, "weaknesses": {"value": "The proposed method depends on small, balanced reference sets per sensitive group, which undercuts the core appeal of zero-shot classification. Can the proposed method be generalized to the scenarios with no reference set or partially labelled reference sets? \n\nOn several backbones/datasets, the method does not outperform TIE or other competitive baselines, despite requiring more label information and extra computation. Consequently, it is unclear the specific advantages of the proposed method over existing baselines.\n\nThe theoretical correction relies on Kent/anisotropy assumptions and local-density estimates that may be brittle or noisy. \n\nMaintaining balanced references and computing densities could scale poorly as groups or classes grow."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I do not identify any remarkable ethics concerns."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uQXySJaAjP", "forum": "O62Fgxb0kd", "replyto": "O62Fgxb0kd", "signatures": ["ICLR.cc/2026/Conference/Submission17746/Reviewer_Z3Kr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17746/Reviewer_Z3Kr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146496837, "cdate": 1762146496837, "tmdate": 1762927586893, "mdate": 1762927586893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful suggestions. Based on the comments, we have made the following changes to the paper:\n\n**Newly added:**\n\nAppendix B.7: Added evaluation of robust text prompts for CelebA. Across both CelebA and Waterbirds (already evaluated in Appendix B.6), we find that using robustified prompts has a minimal effect on DAT, but can modestly improve DAT*, as spurious prompts directly influence its zero-shot attribute inference.\n\nAppendix B.8: Added a discussion on the effect of different spurious text prompt templates.\n\nAppendix C: Added a focused discussion on the combined effects of spurious prompt template structure and spurious object term specificity.\n\nFigures 5 and 6 have been added to Appendix B.3 to illustrate how SLOF enhances group separation.\n\n**Clarified in our responses:**\n\nProvided full definitions and descriptions for all variables in the Tangent–Space Mahalanobis Distance (TMD) formulation (lines 188–192, 199–200).\n\nFigure 3 demonstrates that DAT remains stable across a wide range of hyperparameters, including the scaling factor, number of reference samples, and number of neighbors, without requiring dataset-specific tuning.\n\nEfficiency analysis in Appendix B.5 confirms that DAT is lightweight and fast. \n\nResults on FMoW also confirm that DAT scales robustly to datasets with many classes and subgroups.\n\nConducted Wilcoxon signed-rank tests, showing that DAT significantly outperforms recent baselines on both average and worst-group accuracy."}}, "id": "bGysEwzCRH", "forum": "O62Fgxb0kd", "replyto": "O62Fgxb0kd", "signatures": ["ICLR.cc/2026/Conference/Submission17746/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17746/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission17746/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763687565829, "cdate": 1763687565829, "tmdate": 1763718050721, "mdate": 1763718050721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}