{"id": "AWSVzzhbr7", "number": 15624, "cdate": 1758253307821, "mdate": 1759897294354, "content": {"title": "LD-EnSF: Synergizing Latent Dynamics with Ensemble Score Filters for Fast Data Assimilation with Sparse Observations", "abstract": "Data assimilation techniques are crucial for accurately tracking complex dynamical systems by integrating observational data with numerical forecasts. Recently, score-based data assimilation methods emerged as powerful tools for high-dimensional and nonlinear data assimilation. However, these methods still incur substantial computational costs due to the need for expensive forward simulations. In this work, we propose LD-EnSF, a novel score-based data assimilation method that fully eliminates the need for full-space simulations by evolving dynamics directly in a compact latent space. Our method incorporates improved Latent Dynamics Networks (LDNets) to learn accurate surrogate dynamics and introduces a history-aware LSTM encoder to effectively process sparse and irregular observations. By operating entirely in the latent space, LD-EnSF achieves speedups orders of magnitude over existing methods while maintaining high accuracy and robustness. We demonstrate the effectiveness of LD-EnSF on several challenging high-dimensional benchmarks with highly sparse (in both space and time) and noisy observations.", "tldr": "We develop LD-EnSF, a score-based data assimilation method that learns latent dynamics and operates fully in latent space, removing the need for expensive forward simulations.", "keywords": ["Data Assimilation", "Latent Dynamics", "Physical Models"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eab5172d73436e9dd7af6119b476e463d21c0601.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "## Summary\nThis paper introduces LD-EnSF, a data-driven framework for data assimilation (DA) that aims to solve two major challenges: the high computational cost of numerical simulations and the difficulty of applying score-based methods to sparse observations. The core idea is to (1) learn a fast surrogate model that forwards the system dynamics in a low-dimensional latent space, and (2) use a LSTM encoder to map sparse observations into this latent space, where EnSF is then performed. The numerical experiments on three challenging high-dimensional benchmarks show that LD-EnSF achieves notable speedups while maintaining high accuracy even with sparse observations."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "## Strengths\n- The authors provide a comprehensive numerical studies on three challenging, high-dimensional benchmarks and present solid comparisons of LD-EnSF against strong baselines. The results show the advantages of proposed methods in computational efficiency and assimilation accuracies."}, "weaknesses": {"value": "## Weakness\n- The core idea of performing EnSF in a latent space was already achieved by Latent-EnSF [Si and Peng, 2025]. This paper's primary contributions are (1) replacing the VAE encoder from Latent-EnSF with an LSTM encoder and (2) replacing the full-space dynamics with an LDNet surrogate. While this is a successful *engineering* effort that yields speedups and imporved performances, the novlties in machine learning for DA is minimal. The paper does not provide sufficient theoretical justification for why this specific combination is fundamentally superior, beyond empirical performance.\n- The paper claims the latent representation addresses sparsity. However, the actual mechanism enabling the system to work is the \"*history-aware* LSTM encoder\". By processing a sequence of sparse observations $(y_{1:t})$, the LSTM is implicitly learning a time-delay embedding. It is a well-established concept (e.g., via Takens' embedding theorem [Takens et al. 1981; Noakes et al., 1991]). The paper does not discuss this mechanism entirely. And, It lacks a large and highly relevant recent advances of such works for DA, for example Gottwald et al., 2021; Tarumi et al, 2025; Yang et al, 2025.\n- Several key statements and claims regarding the method's motivation and mechanism are vague and require further clarification and justification, see Questions."}, "questions": {"value": "## Questions\n\n- In Line 47, what is meant by *score becomes ill-posed*? This phrasing is imprecise. Could you clarify? The term *ill-posed* refers to problems (typically inverse problems) that fail to meet one or more of the following criteria: a solution exists, the solution is unique, and the solution depends continuously on the data. There is no such definition for a function itself, e.g., score function. Do you mean the estimation of the score from sparse data is high-variance, or simply that the likelihood gradient is zero in unobserved dimensions? \n- The authors claim the latent representation itself addresses observation sparsity, e.g., Line 48-50. But no detailed justification. Could authors elabroate on: (1) How can the latent projection itself solve the sparsity problem, given that the input observation (y_t) is indeed sparse? A latent projection does not create information for that is not already present. (2) Should this capability not be attributed almost entirely to the \" LSTM Encoder\"? By processing a sequence of sparse observations, this encoder is implicitly learning a time-delay embedding. This concept has been used in data assimilation, yet this is not discussed and cited.\n- In Line 50, the authors state that \"*latent representations enable more informative gradients*\". Could you please explain what \"more informative gradients\" means in this context? How exactly does working in the latent space make the gradients better for handling sparse observations compared to working in the original space? \n- It is a valuable strength to handle the irregular grid observation data which is commen in DA. Could the author include a numerical case study demonstrating the method‚Äôs performance on irregular grids?\n- In Line 210, the author state the random projection $B$ is trianbale. However, in Random Fourier Features/positional encodings, $B$ is typically fixed [Tancik et al., 2020]; optimizing it can change the random feature distribution across update steps and making the training unstable. This is an unconventional choice and its benefits are not inituitive. Could authors provide justify the reason why set $B$ as trainable and provide an ablation study: fixed versus trainable $B$, including training loss curves and DA accuracy.\n\n\n## References\n- Takens, Floris. \"Detecting strange attractors in turbulence.\" Lecture Notes in Mathematics, Berlin Springer Verlag 898 (1981): 366.\n- Noakes, Lyle. \"The Takens embedding theorem.\" International Journal of Bifurcation and Chaos 1.04 (1991): 867-872.\n- Gottwald, Georg A., and Sebastian Reich. \"Combining machine learning and data assimilation to forecast dynamical systems from noisy partial observations.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 31.10 (2021).\n- Tancik, Matthew, et al. \"Fourier features let networks learn high frequency functions in low dimensional domains.\" Advances in neural information processing systems 33 (2020): 7537-7547.\n- Si, Phillip, and Peng Chen. \"Latent-EnSF: A Latent Ensemble Score Filter for High-Dimensional Data Assimilation with Sparse Observation Data.\" The Thirteenth International Conference on Learning Representations (2025).\n- Tarumi, Yuta, Keisuke Fukuda, and Shin-ichi Maeda. \"Deep Bayesian Filter for Bayes-Faithful Data Assimilation.\" Forty-second International Conference on Machine Learning (2025).\n- Yang, Yiming, et al. \"Tensor-Var: Efficient Four-Dimensional Variational Data Assimilation.\" Forty-second International Conference on Machine Learning (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Ef7NwBaorZ", "forum": "AWSVzzhbr7", "replyto": "AWSVzzhbr7", "signatures": ["ICLR.cc/2026/Conference/Submission15624/Reviewer_YcmD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15624/Reviewer_YcmD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761522578021, "cdate": 1761522578021, "tmdate": 1762925889785, "mdate": 1762925889785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LD-EnSF (Latent Dynamics Ensemble Score Filter), a novel data assimilation method designed for high-dimensional dynamical systems with extremely sparse and irregular observations. The core idea is to avoid expensive full-dimensional simulations during assimilation by operating entirely in a learned latent space. The authors train an improved Latent Dynamics Network (LDNet) to serve as a surrogate model of the system‚Äôs dynamics, and a history-aware LSTM encoder to map past sparse observations into the latent space. Assimilation is then performed via the Ensemble Score Filter (EnSF) in the latent space, jointly updating the latent state and system parameters. The method yields orders-of-magnitude speedups (up to $10^5$‚Äì$10^6\\times$ faster in the experiments) compared to traditional approaches, while maintaining high accuracy and robustness. Extensive experiments on three challenging scenarios (Kolmogorov flow, tsunami propagation, and an atmospheric model) demonstrate that LD-EnSF consistently achieves the lowest estimation errors among strong baselines (LETKF, EnSF, Latent-EnSF), and remains stable even under extreme sparsity where other methods break down. The contributions include: (1) enhancing LDNet with a novel initialization, two-stage training, and a ResNet+Fourier architecture for accurate low-dimensional dynamics; (2) an LSTM-based observation encoder for irregularly-sampled, time-sequential observations; (3) the integration of these components into a fast latent-space score-based filtering framework, delivering real-time capable performance without sacrificing accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Significant Practical Advance: The paper addresses a critical bottleneck in data assimilation ‚Äì the computational cost under high-dimensional, sparse observation settings. By eliminating full-state simulations during filtering, LD-EnSF achieves massive speedups (e.g. 200,000√ó in one case), enabling applications (real-time forecasting, larger ensembles) that were previously infeasible. This practical improvement is highly valuable for the community.\n\nRobust Accuracy under Extreme Conditions: Empirical results show state-of-the-art accuracy and robustness. LD-EnSF outperforms LETKF, EnSF, and the prior Latent-EnSF by a clear margin in all tested scenarios. Notably, in an extremely sparse observation scenario (0.1% spatial and 0.2% temporal coverage with 10% noise), LD-EnSF attains about 5% RMSE, whereas LETKF and EnSF either diverge or fail to converge. The method also successfully estimates system parameters (e.g. Reynolds number, forcing amplitude) alongside the state, which is a strong plus.\n\nWell-Motivated Method Design: The integration of learned latent dynamics with score-based filtering is novel and well-justified. The authors identify the shortcomings of VAE-based latent filters (oscillatory latent trajectories, need to propagate in full space) and introduce targeted improvements. In particular, the improved LDNet architecture (with shifted initial latent state handling, two-stage training, and Fourier-featured ResNet decoder) yields a remarkably compact and smooth latent representation that outperforms both the original LDNet and a VAE+LSTM baseline in surrogate modeling accuracy. Similarly, the LSTM-based observation encoder is a sensible choice to capture temporal context and irregular spatial sampling, surpassing the static VAE encoder used in Latent-EnSF. Each design choice is supported by discussion or ablation (e.g. Table 1, Fig. 3 for LDNet vs VAE).\n\nThorough Experiments: The evaluation is comprehensive. The paper tests three diverse and complex systems, includes strong baseline comparisons (including a ‚ÄúLatent-EnSF-dyn‚Äù variant using a VAE-dynamics baseline), and reports detailed metrics. The authors also explore scenarios with varying observation noise (Appendix E.5) and demonstrate the method‚Äôs insensitivity to ensemble size (even a single-sample ‚Äúensemble‚Äù recovers reasonable estimates, akin to MAP). Such thoroughness increases confidence in the results.\n\nReproducibility: The inclusion of references to code for hyperparameter search (Weights & Biases) and discussion of computational setup (CPU/GPU times in Table 2) is appreciated."}, "weaknesses": {"value": "Dependence on Offline Training and Generalization Limits: \nA potential concern is the heavy reliance on training a surrogate model (LDNet) on simulation data before deployment. Acquiring a comprehensive training dataset covering all relevant system behaviors and parameter ranges can be costly, and the method‚Äôs performance may degrade if the true system behavior deviates from the training distribution. The paper‚Äôs approach is essentially as good as its learned model ‚Äì for scenarios with significant model uncertainty or evolving dynamics, one might need to frequently retrain or fine-tune the surrogate. This limitation is hinted at in the conclusion (need to retrain adaptively for long-term complex systems), but it remains a practical caveat: the method inherits the generalization limitations of data-driven surrogates.\n\nIncremental Novelty and Comparison with Variational Surrogate Approaches: \nWhile combining latent dynamics learning with score-based filtering is a creative and well-engineered contribution, the methodological novelty of LD-EnSF is largely incremental rather than foundational. The core components‚ÄîEnKF/EnSF-style ensemble filtering, neural surrogate modeling (LDNet), and LSTM-based latent encoders‚Äîare all built upon established techniques. The innovation primarily lies in the system-level integration and practical realization of these elements, rather than in introducing fundamentally new theoretical insights or learning paradigms. Consequently, LD-EnSF may be viewed as an effective and elegant evolution of Latent-EnSF, replacing the VAE and full-state propagation with a better learned surrogate while maintaining the same overall Bayesian filtering framework.\n\nMoreover, when comparing LD-EnSF against variational methods such as 3D-Var or 4D-Var, it should be noted that those methods can also leverage surrogate models like LDNet to reduce computational cost, provided the surrogate is differentiable for adjoint-based optimization. Thus, the runtime advantage of LD-EnSF does not arise solely from operating in latent space, but also depends on whether comparable surrogates are incorporated into the variational baseline. Explicitly discussing this relationship would clarify that LD-EnSF‚Äôs main strength lies in its practical integration of latent surrogates with score-based filtering‚Äîan important step forward, but not a fundamentally new formulation of data assimilation.\n\nRelation to recent latent-space Bayesian filters:\nThe paper would benefit from discussing its relationship to other recent approaches that perform Bayesian filtering directly in learned latent spaces, such as the Deep Bayesian Filter (DBF, ICML 2025).\nWhile both LD-EnSF and DBF share the goal of combining learned latent dynamics with probabilistic filtering, they differ in philosophy: DBF integrates inference and dynamics in an end-to-end generative framework, whereas LD-EnSF decouples the surrogate dynamics learning (LDNet) from score-based filtering.\nPositioning LD-EnSF more explicitly within this broader family of latent-space Bayesian filters‚Äîhighlighting differences in training objectives, inference mechanisms, and scalability‚Äîwould clarify its contribution and increase its relevance to ongoing developments in this research area.\n\nComputational Complexity and Implementation Practicality: \nWhile the paper convincingly demonstrates that LD-EnSF achieves major runtime savings by performing data assimilation in a low-dimensional latent space, its computational efficiency still involves important trade-offs. Each assimilation cycle requires iterative reverse-time SDE integration for $N_e$ ensemble members over $T_{diff}$ diffusion steps, leading to a total cost of $O(N_e T_{diff} d_s)$. This iteration structure is conceptually analogous to the optimization loops in 3D-Var, with the distinction that LD-EnSF's iterations are fully parallelizable across ensemble members but sequential in diffusion time. Moreover, the ensemble size $N_e$ generally needs to grow with the latent dimension to maintain statistical accuracy, as analyzed in recent works (e.g., Oko et al., 2023). Discussing these trade-offs explicitly would help clarify when LD-EnSF offers practical computational advantages over variational baselines, especially since 3D-Var can also benefit from surrogate models such as LDNet if differentiable.\n\nIn terms of implementation, the overall pipeline remains quite complex: it integrates multiple neural components (latent dynamics network, reconstruction network, and LSTM-based observation encoder) with a non-trivial score-based assimilation algorithm. Implementing and tuning such a system‚Äîincluding hyperparameter searches, ensuring stable LDNet training, and discretizing the SDE solver‚Äîrequires substantial expertise. While the methodology is sound, this complexity may pose barriers to adoption compared to simpler ensemble or variational filters. Providing open-source code or detailed pseudocode would greatly enhance reproducibility and accessibility for the community.\n\nLimitation and Clarity of Latent-State Initialization: \nThe paper initializes all latent trajectories with a fixed zero state ($s_{-1} = 0 $) and relies on the parameter input $u_t$ to encode differences among trajectories. While this design simplifies training and stabilizes the latent dynamics, it implicitly assumes that all variation in the initial conditions can be captured through the parameter space. In practice, however, many physical systems exhibit diverse and high-dimensional initial states that cannot be fully represented by a few parameters. As a result, this assumption may limit LD-EnSF‚Äôs generalization to systems with unseen or highly variable initial conditions.\n\nIn addition, the paper‚Äôs explanation of this mechanism is somewhat unclear. The statement that the initialization ‚Äúflexibly accommodates varying initial conditions‚Äù is misleading, since all trajectories start from the same latent point. Figure 1 also omits how the initial latent state is introduced or how it connects to the LSTM encoder, which may confuse readers about how the model handles initial-state diversity. Clarifying this design choice‚Äîpossibly with an explicit schematic or additional discussion‚Äîand considering a learned initial encoder (e.g., mapping initial full states to latent $s_{-1}$) would both improve clarity and broaden the applicability of LD-EnSF to more general dynamical regimes.\n\nEvaluation Scope: \nA minor weakness is that the experimental comparison could be broadened or analyzed further. The authors did not compare against particle filtering or variational methods (e.g., 4D-Var). It‚Äôs understandable given those struggle in such high-dimensional sparse settings, but discussing their expected performance or including them in a smaller-scale experiment would strengthen the positioning. Additionally, an ablation study on each novel component of LD-EnSF (for instance, using a static VAE-based encoder instead of LSTM, or using the ground truth dynamics vs. LDNet during assimilation) would isolate the contributions of each part. While the paper does compare LDNet vs VAE offline, it doesn‚Äôt explicitly show the effect of the LSTM encoder in the online phase versus a baseline. Such experiments (perhaps in an appendix) would provide deeper insight into how much each innovation (smooth latent dynamics, history encoder) contributes to the final performance.\n\nMinor Presentation Issues: \nThere are a few presentation details that could be improved. For example, the text‚Äôs statement of speedups (e.g. ‚Äú$2\\times10^3$ times speedup‚Äù) seems slightly inconsistent with Table 2 values ‚Äì clarifying these calculations would avoid confusion. Also, the results discussion could more explicitly highlight why LD-EnSF outperforms baselines (e.g., pointing out in text that EnSF fails due to vanishing likelihood gradients in unobserved dimensions, which LD-EnSF overcomes by using informative latent gradients). The figures are generally clear; still, adding a bit more explanation in the captions or main text for Figure 4 (e.g., explaining the behavior of ‚ÄúLatent-EnSF-dyn‚Äù curves, or noting that LETKF diverged in the hardest case) would be helpful. These are minor issues and easily addressable."}, "questions": {"value": "Generality to Out-of-Distribution Dynamics: How well would LD-EnSF handle scenarios where the true dynamics deviate from the training data? For instance, if the system experiences an unforeseen regime or a parameter outside the trained range, would the assimilation degrade gracefully, or could it diverge? Did you observe any cases where the learned LDNet struggled when the truth lay outside its training distribution?\n\nAblation on Observation Encoder: Have you evaluated the impact of the LSTM-based observation encoder versus a simpler or non-temporal encoder? For example, how would Latent-EnSF perform if augmented with your LDNet but still using the original VAE observation encoding at each time step (ignoring history)? This would isolate the benefit of the history-aware LSTM. Similarly, what happens if observations are on a fixed grid ‚Äì does the LSTM still offer advantages over a time-independent encoder?\n\nParameter Estimation Performance: The method jointly assimilates state and parameters. Can you provide more insight into how accurately the uncertain parameters (Re, initial bump location, forcing amplitude/spread) were estimated in your experiments? It would be useful to know, for example, the final parameter RMSE or if the filter consistently converges to near-true parameter values. This would demonstrate the effectiveness of treating $(s_t, u_t)$ together in the state vector.\n\nEnsemble Size and Filter Stability: You mentioned that increasing ensemble size beyond a point had minimal impact on LD-EnSF‚Äôs accuracy. Could you elaborate on this? Is LD-EnSF less sensitive to ensemble size because the score-based update effectively approximates the Bayesian posterior even with few samples? Any intuition on why, say, even 1 or 5 ensemble members can yield good results would be interesting ‚Äì it‚Äôs an intriguing contrast to standard EnKF which typically benefits from larger ensembles.\n\nComputational Overheads: Table 2 shows dramatic speedups in the online phase. Could you comment on the offline costs (training LDNet and LSTM) relative to those gains? For a fair real-world assessment, one might consider how many assimilation cycles are needed to amortize the training cost. Do you envision scenarios (like repeatedly assimilating in the same system) where the upfront training is justified by many future uses of the model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "This paper does not raise any specific ethical concerns.\nThe study involves numerical simulations and synthetic datasets for evaluating data assimilation methods.\nNo human subjects, personal data, or sensitive information are used.\nThe work focuses on methodological and computational contributions rather than societal or safety-critical applications."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5OLKb8s6Cs", "forum": "AWSVzzhbr7", "replyto": "AWSVzzhbr7", "signatures": ["ICLR.cc/2026/Conference/Submission15624/Reviewer_TvkW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15624/Reviewer_TvkW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700048213, "cdate": 1761700048213, "tmdate": 1762925889240, "mdate": 1762925889240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LD-EnSF, a score-based Bayesian filtering method that performs all assimilation steps in a low-dimensional latent space learned by an improved Latent Dynamics Network (LDNet) and coupled to a history-aware LSTM observation encoder. The method extends EnSF (ensemble score filtering) to handle severe spatiotemporal sparsity without resorting to costly full-state simulations by: (i) learning smooth latent dynamics that can be time-stepped (Euler) with parameter inputs, plus a stronger reconstruction network; (ii) mapping sparse, possibly irregular observations $ùë¶_{1:ùë°}$  to latent pairs $(ùë†^ùë°, ùë¢^ùë°)$ with an LSTM; and (iii) running EnSF entirely in latent space then decoding to physical space only when needed. Overall this presents a principled way of approaching the problem. Experiments on Kolmogorov flow, tsunami (shallow water), and a forced hyperviscous rotating atmosphere show lower assimilation RMSE than EnSF, Latent-EnSF (VAE), and LETKF, with very large runtime speedups because full-space propagation is replaced by latent dynamics"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Decoder-free assimilation loop: All filtering happens in latent space (state + params), avoiding per-step decoding and cutting both compute and error accumulation.\n\nEfficiency at scale: Small latent dimension + ensemble updates ‚Üí orders-of-magnitude cheaper than full-state DA; design is hardware-friendly and parallelizable.\n\nRobustness features: Reverse-SDE damping and simple latent noise modeling make the update stable under severe sparsity/noise.\n\nClear training recipe: Two-stage LDNet training with well-specified schedules/hparams improves reproducibility and stability.\n\nExtensive Evaluation : Demonstrated on Kolmogorov flow, tsunami (shallow water), and a rotating atmosphere covering increasing complexity and different observation settings.\n\nThorough ablations: Sensitivity to latent rank, ensemble size, observation density/cadence, noise level, and architecture choices; includes OOD initial-condition tests.\n\n\nReproducibility: Detailed setups (observation models, training schedules, metrics (see Appendix) ) and systematic reporting make results believable and repeatable."}, "weaknesses": {"value": "Overall the paper presents a clear contribution with thorough experimentation. Following are my major concerns : \n\nRelated Work Missing : \n\nThe paper under-cites several very relevant 2024‚Äì2025 works in  that would strengthen positioning:\n\nNeural Operators for DA and Semilinear PDEs : Fourier Neural Operator and SFNO have presented great result in PDE/wether modeling but no discussion have been provided in regard to them. Additionally Semilinear Neural Operator (ICLR 2024) that proposes a recursive neural-operator framework that explicitly addresses prediction and data assimilation for semilinear PDEs have also not been cited ; \nNeural Koopman priors and Koopman-based DA : Frion et al., ‚ÄúNeural Koopman Prior for Data Assimilation‚Äù formulates DA with a neural Koopman prior (now a TSP 2024 article), and KODA (arXiv 2024) integrates forecasting with an online data-assimilation loop using Koopman-guided components. Both connect directly to low-dimensional latent linearizable dynamics for DA, much like LD-EnSF‚Äôs latent evolution;  Modern 4D-Var and deep 4D-Var variants : The paper cites classical 3D/4D-Var (e.g., Rabier & Liu 2003) but misses recent learned or hybrid 4D-Var systems that tackle cost and sparsity with neural parameterizations‚Äîe.g., 4DVarNet (end-to-end DA backboned on variational objectives), En4DVarNet for uncertainty, 4DVarFormer (attention-based 4D-Var surrogate with rapid multivariate analyses), and operational-scale hybrids like FuXi-En4DVar. These are important baselines or at least conceptual references for the atmosphere case and the efficiency narrative.\n\nAction item : by adding a paragraph in Related Work discussing (SNO/NO-DA, ClimODE, Koopman-DA, deep/hybrid 4D-Var), explaining what LD-EnSF gains with a training-free score component and learned latent dynamics, and why that‚Äôs preferable under extreme sparsity.\n\nTechnical Weaknesses :\n \nLatent observation model = identity. The filter assumes $ùêª_{latent}(ùúÖ_ùë°)=ùúÖ_ùë°$, so the LSTM‚Äôs outputs $(ùë†_ùë°,ùë¢_ùë°)$\t‚Äã\n are taken as direct noisy measurements of the true latent pair $(ùë†_ùë°,ùë¢_ùë°)$. Any encoder bias/miscalibration directly contaminates the update; there‚Äôs no learned/structured latent observation operator to absorb mismatch.\n\nDecoder-free assimilation hides reconstruction bias. Assimilation runs entirely in latent space (a strength), but it also means reconstruction errors don‚Äôt get corrected during the loop. If the decoder has bias, final physical-space fields can drift even when latent RMSE falls. (The paper itself emphasizes that decoding is only needed at the end, which is fast, but leaves this feedback gap.)\n\nNoise handling is crude in latent space. The latent observation noise ùõæ^_{ùë°} is estimated post-hoc and then treated as uniform across latent dimensions. That‚Äôs convenient but brittle when different latent directions have very different uncertainty."}, "questions": {"value": "Fourier Neural Operators have shown great success recently in weather and PDE modelling. Why have no discussion been provided in that regards?\n\nLatent noise modeling: In Eq. (8) the identity $H_{latent}$ and a scalar ùõæ^_{ùë°}  are assumed via empirical estimation. How sensitive is EnSF‚Äôs update to misspecifying ùõæ^_{ùë°} across latent dimensions? Could you learn a diagonal or low-rank covariance in latent space cost-effectively? \n\nAre proposals for $u_{t}$ purely resampling from the previous posterior, or is there diffusion/jitter? What happens when u varies slowly or remains static? \n\nPhysical time vs latent  Œît: Since Œît is tuned, how does the method behave if the observation sampling rate changes (e.g., sparser/faster streams) without re-training? \n\nWhen LDNet struggles (very long horizons, regime changes), could LD-EnSF seamlessly ‚Äúfallback‚Äù to occasional full-model nudging (a la hybrid LD-EnSF/4D-Var) while keeping most steps latent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9P3TnX2kTN", "forum": "AWSVzzhbr7", "replyto": "AWSVzzhbr7", "signatures": ["ICLR.cc/2026/Conference/Submission15624/Reviewer_cjhs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15624/Reviewer_cjhs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864206698, "cdate": 1761864206698, "tmdate": 1762925888741, "mdate": 1762925888741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LD-EnSF, a score-based data assimilation framework that combines Latent Dynamics Networks (LDNets) and an LSTM-based observation encoder to perform Bayesian filtering directly in a low-dimensional latent space.\n\nTraditional data assimilation methods such as EnKF or EnSF are computationally expensive because they operate in the full physical space and require repeated forward model simulations. LD-EnSF negates this by learning surrogate dynamics in latent space and performing all filtering steps there, using an ensemble score filter (EnSF) to update the posterior distribution. The LSTM encoder processes sparse and irregular observations and aligns them with the latent states and parameters.\n\nEmpirical validation is conducted on three different systems (Kolmogorov flow, tsunami modeling, and atmospheric dynamics) and under severe spatial and temporal sparsity. The method demonstrates strong improvements in accuracy and speed, with orders of magnitude reductions in runtime while maintaining or improving assimilation accuracy."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Clear Motivation and Relevance\n- The paper tackles a key limitation of recent score-based filters, their high computational cost and poor performance with sparse observations. \n\nSolid Technical Design\n- The integration of latent surrogate dynamics (LDNet) and score-based Bayesian filtering (EnSF) is smart.\n- The introduction of a history-aware LSTM observation encoder effectively extends the latent assimilation framework to handle irregular and sparse data.\n\nComprehensive Experiments\n- The authors test across multiple physical systems of increasing complexity.\n- Results include both structured and unstructured observation setups, multiple levels and differing types of noise, and sensitivity analyses.\n\nExtensive Ablation and Robustness Studies\n- Appendices systematically evaluate noise robustness, out-of-distribution generalization, latent dimension sensitivity, ensemble size, and architectural design choices."}, "weaknesses": {"value": "Limited Theoretical Novelty\n- The proposed method primarily combines existing techniques (LDNet, EnSF, LSTM encoding). While the combination is well-executed and impactful the theoretical advancement is modest. The novelty lies more in the integration and empirical rigor.\n\nBenchmark Coverage and Positioning\n- Comparisons are limited to EnSF, Latent-EnSF, and LETKF. While these are strong and relevant baselines, the paper could benefit from a clearer discussion of recent efficient variational and diffusion-based data assimilation approaches, such as Tensor-Var: Efficient Four-Dimensional Variational Data Assimilation (Yang et al., 2025) and DiffDA: A Diffusion Model for Weather-Scale Data Assimilation (Huang et al., 2024). Although implementing these methods in the current setup may not be straightforward, a more explicit positioning in the Related Work section would help situate LD-EnSF within the broader landscape."}, "questions": {"value": "1. Theoretical Guarantees:\nCan the authors comment on whether any convergence guarantees exist for the latent-space assimilation process, particularly as a function of the surrogate model error?\n\n2. Dynamic Re-training:\nHow feasible is LD-EnSF in systems where governing dynamics change over time? Could partial retraining or online fine-tuning mitigate the offline cost?\n\n3. Uncertainty Quantification:\nHow reliable are uncertainty estimates when mapped back to physical space?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OgbA194Zw9", "forum": "AWSVzzhbr7", "replyto": "AWSVzzhbr7", "signatures": ["ICLR.cc/2026/Conference/Submission15624/Reviewer_ogtz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15624/Reviewer_ogtz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993524446, "cdate": 1761993524446, "tmdate": 1762925888280, "mdate": 1762925888280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}