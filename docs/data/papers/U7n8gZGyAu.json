{"id": "U7n8gZGyAu", "number": 16622, "cdate": 1758266856429, "mdate": 1759897228971, "content": {"title": "JoyAgents-R1: Accelerating Multi-Agent Evolution Dynamics with Variance-Reduction Group Relative Policy Optimization", "abstract": "Large Language Model (LLM)-based multi-agent systems represent a promising paradigm with broad applicability, exemplified by general-purpose Artificial Intelligence (AI) assistants capable of performing multiple tasks. Nevertheless, joint optimization across functionally distinct agents remains challenging due to divergent working modes and reward functions. To address this issue, we introduce JoyAgents-R1, a framework that accelerates multi-agent evolution with a novel Variance-Reduction Group Relative Policy Optimization (VR-GRPO), integrating efficient sampling and update strategies. Specifically, VR-GRPO performs Monte Carlo sampling based on an initial reasoning trajectory to avoid the exponential explosion of the joint action space while maintaining policy diversity. Then, the method selects the top-$K$ sampling groups with maximal reward fluctuations based on the marginal benefit principle, thereby enabling cost-effective parameter updates. To further complement evolution, an adaptive memory evolution mechanism that repurposes GRPO rewards as cost-free supervisory signals is designed to eliminate repetitive reasoning and accelerate convergence. Experiments on multi-task AI assistant datasets across both general and e-commerce scenarios demonstrate that JoyAgents-R1, built upon smaller 3B/7B open-source models, achieves performance comparable to that of larger LLMs, such as DeepSeek-R1, and surpasses DeepSeek-V3 by an average of 6\\%.", "tldr": "", "keywords": ["Multi-Agent Systems", "Joint Evolution Dynamics", "Group Relative Policy Optimization"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e63094342c6061532903391d02e766979f2330e3.pdf", "supplementary_material": "/attachment/40871549c4ef3d392e7afa822c94023d01374667.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces JoyAgents-R1, a framework that accelerates multi-agent evolution with a novel Variance-Reduction Group Relative Policy Optimization (VR-GRPO), integrating efficient sampling and update strategies. Specifically, VR-GRPO performs Monte Carlo sampling based on an initial reasoning trajectory to avoid the exponential explosion of the joint action space while maintaining policy diversity. Then, the method selects the top-K sampling groups with maximal reward fluctuations based on the marginal benefit principle, thereby enabling cost-effective parameter updates. To further complement evolution, an adaptive memory evolution mechanism that repurposes GRPO rewards as cost-free supervisory signals is designed to eliminate repetitive reasoning and accelerate convergence."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1, The paper claims that it is the first work to adapt GRPO for functionally distinct multi-agents, enabling synergistic enhancement of their decision-making and memory capabilities. \n2, The paper proposes Variance-Reduction Group Relative Policy Optimization (VR-GRPO), tailored for multi-agent systems."}, "weaknesses": {"value": "I think the main weakness is in the experiment part. Please see details in the questions."}, "questions": {"value": "1, In A.4 MORE ANALYSIS FOR ABLATION STUDY, for title \"Ablation on updating top-K models\" you mean nodes here?\n2, How much nodes in total? I see the table 4 compares top1, 2, 5, all. But if the total nodes are like 20 or even more than a hundreds, can you maybe show like top 4,6,8,10? And also I am curious about how general top-5 is, like if using different model in different datasets, would top-5 always be the best or you would actually need to carefully tune the parameter k?\n3, For the design of the main table, is there any related articles I could follow or refer to? Like how previous works did the comparison between their proposed method and baselines? And why you choose these as your baselines? Or add a simple explanation of your selected baselines.\n4, For the closed-source models used in the paper, could you show maybe chatgpt 5 and Claude4.0-sonnet?\n5, In table 1, I could tell that for JoyAgents-R1 larger Master leads to a better Avg performance. But I am wondering what would the performance be if use like 3B Master + 7B Sub agents? And also could you scale up the master to 14B to test the scalability of your method? For example would JoyAgents-R1(14B Master + 3B Sub agents) directly outperform the JoyAgents-R1(7B Master + 3B Sub agents)? This concern is due to some specific tasks, it seems like smaller Master performs better than larger Master.\n6, For all the experiments and evaluation of proposed method, Qwen2.5 is the base model family. I think it would be necessary to show the results from at least one more model family such as Llama3, Llama2 or any other open source model families.\n7, Did you run your experiments across different random seeds?\n8, Could you also provide the comparison results for JoyAgents-R1(7B Master + 3B Sub agents), JoyAgents-SFT (3B+3B) and JoyAgents-SFT-no (3B+3B) in table 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L8qYHF4g5W", "forum": "U7n8gZGyAu", "replyto": "U7n8gZGyAu", "signatures": ["ICLR.cc/2026/Conference/Submission16622/Reviewer_erZT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16622/Reviewer_erZT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760773635944, "cdate": 1760773635944, "tmdate": 1762926691444, "mdate": 1762926691444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces JoyAgents-R1, a new framework for Multi-Agent Reinforcement Learning (MARL) designed to address cooperative inefficiency and training instability in heterogeneous agent systems. The approach integrates a hierarchical architecture comprising a master agent and specialized sub-agents (e.g., Q&A, Function-call, Math). The core of the method is a variance-reduction GRPO tailored for MARL, which includes node-wise monte carlo sampling to manage trajectory explosion and a marginal benefit-driven selection strategy for efficient parameter updates. Furthermore, JoyAgents-R1 incorporates an adaptive memory evolution mechanism that leverages GRPO rewards as a \"cost-free supervisory signal\" to improve convergence and eliminate repetitive reasoning. The overall goal is to achieve performance comparable to larger LLMs using smaller, open-source models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly identifies and proposes solutions for key challenges in applying traditional policy optimization methods like GRPO to heterogeneous MARL: **low sampling efficiency** and **slow convergence**. This shows a strong grasp of the practical difficulties in LLM-based MARL.\n2. Novel variance-reduction GRPO adaptation. In this method, node-wise monte carlo sampling greatly improves sampling efficiency, while marginal benefit-diven model update addresses training stability and efficiency.\n3. Cost-effective memory evolution using the $R_A+R_F$ rewards as a cost-free supervisory signal for memory evolution is highly appealing. This \"free lunch\" approach avoids the need for a separate memory training loss or a dedicated, potentially large, supervisory model, which is excellent for cost-effectiveness and accelerating convergence. The dynamic memory updating algorithm provides a concrete mechanism for memory curation based on reward thresholds."}, "weaknesses": {"value": "1. Clarity the marginal benefit principle. The connection between reward variance and the marginal benefit principle requires stronger theoretical justification. While high variance intuitively indicates high potential for improvement (or deterioration), the paper does not **formally prove** or give empirical experiments that there is a phenomenon of excessive variance in MARL, and if the method maximizes $Var(R_i )$ for parameter updates the marginal joint benefit for the overall multi-agent system.\n2. Maybe potential bias in reward definition? the reward $R_E$ is excluded from the memory update reward but included in the policy optimization reward ($R=R_A+R_F-R_E$). This inconsistency can lead to a divergence between the memory's preference (optimized for accuracy/format) and the agent's policy preference (optimized for accuracy/format/efficiency).\n3. How to effectively allocate rewards to each agent, is this a CTDE framework? Agents are heterogeneous, why can they be jointly optimized in this way?\n4. Limited heterogeneity and scalability demonstration: The current architecture features a Master and four specific sub-agents (QA, EFC, GFC, Math). While heterogeneous in function, the underlying LLM backbone is the same. The claim of handling heterogeneous agents would be stronger if the framework were tested with agents based on fundamentally different architectures or a much larger number of sub-agents to truly test the limits of the VR-GRPO's scalability.\n5. Algorithm 1 uses a reward thresholding mechanism based on an approximate normal distribution (2.5% and 97.5% percentiles) for memory updates. A brief justification for the normality assumption or a comparison with a simpler fixed threshold approach (which would be an interesting ablation) is missing. The time decay and reward difference terms in line 15 also lack a clear justification for setting α=β=1.\n6. Perhaps it is necessary to add a line of performance for untrained JoyAgents."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CxTvaRW9cz", "forum": "U7n8gZGyAu", "replyto": "U7n8gZGyAu", "signatures": ["ICLR.cc/2026/Conference/Submission16622/Reviewer_7gUC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16622/Reviewer_7gUC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578235674, "cdate": 1761578235674, "tmdate": 1762926690465, "mdate": 1762926690465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on variance control in the training process of the Group Relative Policy Optimization (GRPO) algorithm within LLM-based multi-agent systems. The overall training framework is similar to the decentralized training approach in Multi-Agent Reinforcement Learning (MARL), where each agent optimizes its policy assuming the policies of other agents remain fixed, solely using GRPO to optimize its own policy. Through this approach, the proposed algorithm intuitively achieves higher exploration efficiency compared to directly exploring the joint action space, potentially reducing the exponential complexity of action space exploration to a linear level. Furthermore, the authors introduce a refinement in the policy optimization phase, which is the main step for their variance control method: by sorting the variance of the grouped rewards during each agent's policy learning, they determine which agent policies to optimize in the current iteration. The experiments are conducted using Qwen2.5 as the base model, tested on multiple SFT and RL tasks. The tested baselines include mainstream open-source and non-open-source models/algorithms. The comparative results show that the average performance is comparable to that of DeepSeek-R1."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is easy to follow\n- The proposed method shows as a plugin-in which can be implemented compatibly with existing methods."}, "weaknesses": {"value": "- The key components of the algorithm lack strong experimental verification, with only a few experiments provided to support them. This includes: 1) whether the proposed variance reduction method impacts the convergence of the algorithm; 2)the Monte Carlo sampling is similar to a coordinate descent method, and its limitations and scope of applicability are not discussed.\n- The designed experiments fail to highlight the advantage of the proposed algorithm in terms of variance control and exploration efficiency improvement."}, "questions": {"value": "- What exactly do M1-M6 in Table 3 represent? I suggest providing a brief explanation in the caption, as the current layout is not clear.\n- Why are there no learning curves provided?\n- JoyAgents-R1 (7B Master + 3B Sub agents) uses a larger model but exhibits poorer performance. Why is this the case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xCn0KI5fJl", "forum": "U7n8gZGyAu", "replyto": "U7n8gZGyAu", "signatures": ["ICLR.cc/2026/Conference/Submission16622/Reviewer_2MVW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16622/Reviewer_2MVW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891868786, "cdate": 1761891868786, "tmdate": 1762926689985, "mdate": 1762926689985, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces JoyAgents-R1, a novel framework for accelerating multi-agent reinforcement learning through Variance-Reduction Group Relative Policy Optimization (VR-GRPO). The method addresses the computational challenges of joint optimization across heterogeneous agents by employing Monte Carlo sampling along initial trajectories to avoid exponential action space explosion, selecting top-K agents with maximal reward variance for efficient updates, and incorporating an adaptive memory evolution mechanism that leverages GRPO rewards as supervisory signals. Experimental results on multi-task AI assistant benchmarks demonstrate that the framework, built on smaller 3B/7B models, achieves performance comparable to much larger models like DeepSeek-R1."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents the application of GRPO to functionally distinct multi-agents, introducing VR-GRPO with efficient Monte Carlo sampling and marginal benefit-driven updates that effectively manage the exponential growth of joint action spaces.\n2. The framework cleverly repurposes GRPO rewards as \"free lunch\" supervisory signals for memory updates, enabling synchronous optimization of decision-making and memory modules without requiring additional training overhead.\n3. Despite using significantly smaller models (3B/7B parameters), JoyAgents-R1 achieves competitive results against much larger systems, demonstrating excellent parameter efficiency and practical applicability for resource-constrained deployments."}, "weaknesses": {"value": "1. The computation and utilization of similarity metrics in Algorithm 1 lack clear specification, making it difficult to understand how similarity scores influence memory updates and agent decision-making processes.\n2. The experimental datasets primarily consist of isolated tasks, and the paper does not adequately explain how the 100 collaborative task instances were constructed, raising questions about the framework's evaluation on truly interdependent multi-agent scenarios.\n3. The observation that 3B Master + 3B Sub agents outperforms 7B Master + 3B Sub agents on EFC and GFC tasks suggests potential scalability issues, indicating the framework may be more dependent on sub-agent improvements rather than master agent capacity.\n4. Figure 4 shows that removing memory initially improves performance before declining, which contradicts the expected monotonic degradation and raises questions about the true contribution of the memory mechanism to overall system performance."}, "questions": {"value": "1. How does the framework handle more complex real-world scenarios beyond the current agent types (e.g., file search, application control), and what modifications would be necessary to extend the architecture?\n2. What is the computational overhead of the VR-GRPO sampling strategy compared to standard GRPO, particularly as the number of agents and trajectory length increase?\n3. Could the authors provide more analysis on why the memory-free variant shows initial performance improvements, and how this relates to the GRPO training dynamics?\n4. Given the hierarchical architecture, how does error propagation from the master agent affect sub-agent performance, and are there mechanisms to mitigate cascading failures?\n5. What are the convergence guarantees of VR-GRPO compared to standard GRPO, particularly when only updating a subset of agents based on variance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UVCamDbMit", "forum": "U7n8gZGyAu", "replyto": "U7n8gZGyAu", "signatures": ["ICLR.cc/2026/Conference/Submission16622/Reviewer_PoeU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16622/Reviewer_PoeU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016299300, "cdate": 1762016299300, "tmdate": 1762926689392, "mdate": 1762926689392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Summary:\nThe paper proposes JoyAgents-R1, a multi-agent reinforcement learning framework based on Variance-Reduction Group Relative Policy Optimization (VR-GRPO). It claims to improve multi-agent coordination and efficiency through (1) Monte Carlo trajectory sampling, (2) variance-based top-K updates, and (3) memory evolution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The system architecture (Master/Sub-agents) and training pipeline (SFT → RL fine-tuning) are clearly described.\nExperiments is solid, including multi-domain evaluations (general reasoning, e-commerce, function calling)."}, "weaknesses": {"value": "1. Lack of Novelty\nThe proposed VR-GRPO method is not substantively new.\nThe combination of Monte Carlo sampling + selective update based on variance has already been explored in several existing agentic RL and multi-agent PPO frameworks.\nSpecifically, the paper’s “trajectory-based Monte Carlo sampling” is nearly identical to prior works on *Monte-Carlo Tree Search in Multi-Agent RL* and *Monte-Carlo Sampling for Agentic LLM RL*.\nIn addition, the “variance reduction + top-K update” heuristic is a mild optimization tweak, not a new learning principle.\n\n2. Incomplete Comparison\nJoyAgents uses multi-agent specialization, while baselines are single-model. The paper only compares against LLM models (DeepSeek-V3, DeepSeek-R1, GPT-4o), rather than other agentic RL methods. \nIt completely omits comparisons with recent multi-agent RL frameworks, such as MLPO mentioned in related work."}, "questions": {"value": "Why EFC for most of the llm model is extremely low, even for gpt-4o-mini and Claude3.5-sonnet?\nWhat is “Cooperation” task mentioned in Table 1? The paper only said “The test set … 100 cases for the collaborative task”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pHQdB1bxHa", "forum": "U7n8gZGyAu", "replyto": "U7n8gZGyAu", "signatures": ["ICLR.cc/2026/Conference/Submission16622/Reviewer_VQxJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16622/Reviewer_VQxJ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762158384175, "cdate": 1762158384175, "tmdate": 1762926688843, "mdate": 1762926688843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}