{"id": "GxEyklHpWB", "number": 2719, "cdate": 1757221694216, "mdate": 1759898131616, "content": {"title": "SCOPE: Selective Cross-modal Orchestration of Visual Perception Experts", "abstract": "Vision-language models (VLMs) benefit from multiple vision encoders, but naively stacking them yields diminishing returns while multiplying inference costs. We propose SCOPE, a Mixture-of-Encoders (MoEnc) framework that dynamically selects one specialized encoder per image-text pair via instance-level routing, unlike token-level routing in traditional MoE. SCOPE maintains a shared encoder and a pool of routed encoders. A lightweight router uses cross-attention between text prompts and shared visual features to select the optimal encoder from the routed encoders. To train this router, we introduce dual entropy regularization with auxiliary losses to balance dataset-level load distribution with instance-level routing confidence. Remarkably, SCOPE with one shared plus one routed encoder outperforms models using all four extra encoders simultaneously, while reducing compute by 24-49\\%. This demonstrates that intelligent encoder selection beats brute-force aggregation, challenging the prevailing paradigm in multi-encoder VLMs.", "tldr": "", "keywords": ["VLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2af1166487d2481aea58432b2f5b39a913076d1d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This authors presents SCOPE, an Mixture-of-Encoders-based framework designed to enhance VLMs by dynamically selecting the most suitable off-the-shelf visual encoder for a given image-text pair. SCOPE utilizes a lightweight router, guided by cross-attention between the text prompt and shared visual features, to achieve instance-level routing. The authors demonstrate that selective orchestration significantly reduces computational cost (up to 49%) while outperforming models that naively aggregate multiple encoders. This work challenges the prevailing paradigm of encoder aggregation and offers a highly efficient and valuable alternative."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  **Efficient Architecture:** The core idea of using instance-level routing to select from a pool of off-the-shelf visual encoders tackles the critical problem of high inference costs in multi-encoder VLMs. The approach effectively decouples the benefits of specialized encoders from their cumulative computational burden.\n\n2.  **Principled Router Training:** The introduction of the Dual Entropy Regularization and Auxiliary Losses provides a well-motivated mechanism to train the non-differentiable routing process. This structure addresses the inherent conflict between requiring load balancing(for robust training) and demanding high-confidence selection (for performance).\n\n3.  **Strong Empirical Efficiency Claim:** The paper provides compelling evidence that intelligent selection beats brute-force aggregation. The result that SCOPE, using only one shared plus one routed encoder, outperforms models that simultaneously use all four extra encoders in OCR-related and chart understanding scenes."}, "weaknesses": {"value": "**1.  Necessity of Strict Load Balancing:** \nThe strict imposition of load balancing losses ($\\mathcal{L}_{be}, \\mathcal{L}_{ba}$) requires further discussion and justification. Unlike traditional MoE where experts are randomly initialized FFN blocks prone to \"dying\" without balancing, SCOPE utilizes powerful, off-the-shelf vision encoders whose utility is naturally non-uniform across tasks. Forcing the router to use demonstrably weaker experts to maintain balance might introduce noise and counteract the primary goal of performance maximization, especially if one encoder is overwhelmingly strong. An investigation into loss annealing (gradually reducing the balancing weight) or an adaptive expert selection/pruning mechanism (similar to the concept in [1]) would allow the model to learn the intrinsic value of each expert and achieve more self-adaptive selection.\n\n**2.  Scope of Experimental Evaluation:** \nThe empirical evaluation is heavily focused on OCR-related and chart understanding benchmarks (DocVQA, InfoVQA, TextVQA, ChartQA). The paper lacks evaluation on general-purpose visual reasoning and perception benchmarks (e.g., MME, MMStar, MMBench). This limitation makes it difficult to assess the framework's versatility and its ability to generalize across a broader spectrum of visual understanding tasks, which is my major confusion point.\n\n**3.  Baselines and Router Input:**\n* The performance comparison against the original Qwen-2.5-VL is missing. The authors use the Qwen-2.5-VL vision encoder and Qwen-2.5-7B LLM, but the reported baseline performance (e.g., ChartQA 68.3) appears significantly lower than the established performance of the original Qwen-2.5-VL model (87.3). This discrepancy makes it challenging to accurately gauge the actual gain brought by SCOPE over a strong, optimized monolithic VLM baseline.\n* Additionally, the choice to use the Qwen-2.5-VL vision encoder's output as the router's visual input is a key design choice. The authors should investigate and report on the effect of using alternative visual features to drive the routing decision.\n\n**4.  Expert Pool Exploration:** \nThe paper only explores a constrained set of visual experts. There is no discussion or experimentation with incorporating outputs from other prominent or specialized vision encoders. For instance, the authors could explore experts providing direct dense visual outputs (like depth estimation used in [1]) or integrating other powerful foundational models (like SAM or EVA-02 as mentioned in [2]).\n\n**5.  Challenges in Real-World Scenarios:** \nThe paper does not discuss or provide solutions for the challenges of **multi-image input** or **multi-turn conversation** scenarios. The current instance-level routing design seems inherently limited to single-turn, single-image inputs. Clarification on how SCOPE would be adapted or whether it is intended to handle more complex, sequential VLM interactions is necessary."}, "questions": {"value": "Please see the above comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qmIjssvS66", "forum": "GxEyklHpWB", "replyto": "GxEyklHpWB", "signatures": ["ICLR.cc/2026/Conference/Submission2719/Reviewer_XqF8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2719/Reviewer_XqF8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760886890961, "cdate": 1760886890961, "tmdate": 1762916343582, "mdate": 1762916343582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Scope, a VLM framework coupled with a mixture of vision encoders. Scope relies on a learnable router that, conditioned on image and text embeddings from Qwen models, dynamically selects a vision encoder per instance as additional image features for vision-language reasoning. To avoid mode collapse during model training, the proposed method incorporates both batch-level and instance-level entropy and auxiliary losses to encourage the model to explore all options of the included vision experts while maintaining high confidence in the selected expert. Scope was extensively ablated with varying auxiliary loss weights and the number of activated experts on standard VQA-related benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written, and I appreciate the use of matched colours in Figure 2, making it easy to navigate and understand the design pipeline. The proposed method offers new insights into designing VLM systems in conjunction with encoder designs."}, "weaknesses": {"value": "The overall paper has a really good flow on introducing the architecture design and training losses. However, I found the experiments are very insufficient and reading like a novel with a rushed ending. This might be due to some many other design choices in the model implementation, which I will explain below. \n\n- Scope has poorly chosen vision experts. For other notable related VLM works in this domain like Prismer and Eagle; both models chose vision experts spanning across multiple perception domains and tasks, like 3D understanding: depth/surface normal experts, OCR detection models, object segmentations, etc, to cover a wide range of diversity of expert domains. However, the chosen experts here are all general vision models aimed for general-purpose vision backbones (with large-scale self-supervised training and language-image training). In particular, I feel like the inclusion of the ViT + ConvNext variants of the same DINOv3 model seems to be very redundant. I hope the author could explain why Scope chose the vision experts in such a way, and why not consider domain-specific experts like depth estimation, object detection experts heavily explored in the prior work?\n- Related to the previous point, it makes the model nearly incomparable to other recent state-of-the-art VLMs, and it is hard to make sense of the notion that more experts lead to worse results. It's also difficult to interpret the chosen experts, e.g. whether the OCR expert really helps on OCR-related tasks; and whether depth experts really help on visual spatial reasoning type of tasks. \n- I found it is very difficult to understand Table 2: I thought baseline-1 should be equivalent to Scope-CA as they both use 1 expert? Or is it possible to report: \n    - The same single expert across all questions, to confirm the router design and selection is really helpful. \n    - Additional experts (top-k) from the router, to make sure the router ranking really makes sense. \n- Some other prior designs on the encoder ensembling techniques are missing: e.g. 1. the expert resampler (perceiver-like) design to merge all experts’ embeddings into a fixed length of tokens proposed from Prismer. 2. Channel-concat, sequence-concat design space proposed by Eagle, all seem to be very important. The first one will significantly reduce the training complexity, without having the proposed auxiliary losses, as we always consider all experts (and with the bounded inference compute). 2. We will know the performance upper bound by having no compression on the expert knowledge."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sJTOwq8jIn", "forum": "GxEyklHpWB", "replyto": "GxEyklHpWB", "signatures": ["ICLR.cc/2026/Conference/Submission2719/Reviewer_7jGP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2719/Reviewer_7jGP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761507761851, "cdate": 1761507761851, "tmdate": 1762916343406, "mdate": 1762916343406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SCOPE, a novel approach for improving the efficiency and performance of VLMs through dynamic selection of vision encoders. By maintaining a pool of specialized vision encoders and a shared encoder, SCOPE uses a lightweight routing mechanism to dynamically select the most appropriate encoder for each image-text pair. This results in improved computational efficiency and performance, achieving significant reductions in inference costs (24-49%) compared to models using all encoders simultaneously. The proposed routing mechanism is trained with dual entropy regularization and auxiliary losses, aiming to balance load distribution and routing confidence."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe dynamic selection of encoders based on input image-text pairs is an innovative approach that helps optimize the use of vision encoders, enhancing both computational efficiency and model performance. This allows the system to select the most relevant expert encoder without incurring the cost of always using all encoders.\n\n\n2.\tThe paper demonstrates that SCOPE outperforms static multi-encoder models on several benchmark tasks, such as VQA and document understanding, even when using fewer encoders. This is a solid demonstration of the proposed model's effectiveness."}, "weaknesses": {"value": "1.\tWhile the dynamic encoder selection reduces computational overhead, SCOPE still requires all encoders to be loaded into memory, especially in batch inference settings. In these cases, if all encoders must be preloaded to ensure fast dynamic selection, the memory consumption could become prohibitive. This significantly diminishes the model’s utility in memory-constrained environments, such as edge devices or real-time applications.\n\n2.\tAlthough SCOPE outperforms models that use all four encoders, there are existing methods (eg ToVE: Efficient Vision-Language Learning via Knowledge Transfer from Vision Experts)that achieve similar or better results by integrating the strengths of different visual experts during training. These approaches do not require dynamic selection at inference time, which reduces the overhead of loading multiple encoders.\n\n3.\tThe top-1 routing strategy limits the ability of the model to leverage the diversity of visual features that multiple encoders might provide. In scenarios where a more complex fusion of features from multiple encoders is beneficial, the current approach might not capture all relevant information. Top-k routing or alternative mechanisms could enhance the model's ability to integrate diverse visual information, especially in more complex tasks."}, "questions": {"value": "As the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XxjTnFzrNJ", "forum": "GxEyklHpWB", "replyto": "GxEyklHpWB", "signatures": ["ICLR.cc/2026/Conference/Submission2719/Reviewer_Wicr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2719/Reviewer_Wicr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2719/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762054795974, "cdate": 1762054795974, "tmdate": 1762916342977, "mdate": 1762916342977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}