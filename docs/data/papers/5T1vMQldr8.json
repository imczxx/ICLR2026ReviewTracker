{"id": "5T1vMQldr8", "number": 16068, "cdate": 1758259419628, "mdate": 1759897264150, "content": {"title": "Subgoal-Guided Reward Shaping: Improving Preference-Based Offline Reinforcement Learning via Conditional VAEs", "abstract": "Offline preference-based reinforcement learning (PbRL) learns complex behaviors from human feedback without environment interaction, but suffers from reward model extrapolation errors when encountering out-of-distribution region during policy optimization. These errors arise from distributional shifts between preference-labeled training trajectories and unlabeled inference data, leading to reward misestimation and suboptimal policies. We introduce SPOT (Subgoal-based Preference Optimization Through Attention Weight), which mitigates extrapolation errors by leveraging attention-derived subgoals from preference data. SPOT regularizes the policy toward subgoals observed in preferred trajectories. This approach constrains learning within the training distribution, reducing reward model extrapolation errors. Through comprehensive experiments, we demonstrate that our subgoal-guided approach achieves superior performance compared to existing methods while reducing extrapolation errors. Our approach preserves fine-grained credit assignment information while enhancing query efficiency, suggesting promising directions for reliable and practical offline preference-based learning.", "tldr": "", "keywords": ["Preference-based reinforcement learning", "Reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/69c823a11ea00351c267edaa3a2d1084e4ffe528.pdf", "supplementary_material": "/attachment/06712bc6f6b579e216332476992897bb8fca4310.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes an attention-weight-based subgoal extraction method to mitigate the extrapolation error problem in offline preference-based reinforcement learning. The experiments are sufficient, and the framework is clear."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The authors cleverly introduce the idea of solving extrapolation error in offline RL into reward model training, which is novel.\n\n2) The authors validate the effectiveness of the proposed method through extensive experiments on multiple tasks."}, "weaknesses": {"value": "1) The algorithm essentially constrains the reward to focus more on the in-distribution data, which may limit the method's generalization capability to some extent.\n2) Training the CVAE introduces additional computational cost. Using cosine similarity as part of the reward lacks theoretical motivation and analysis."}, "questions": {"value": "1) The true ground truth ($g_t$) is obtained via Eq. (5) - is it ultimately randomly sampled from a set? Providing pseudocode for the algorithm is recommended to improve clarity.\n\n2) When training the CVAE, why is the similarity term added rather than subtracted? Shouldn't the regularization be reduced for samples with high similarity?\n\n3) How stable is the CVAE? Do the generated subgoal states truly hold significant meaning in actual trajectories?\n\n4) For manipulation tasks, success rate is the primary concern. How does the algorithm's success rate compare to PT on these tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ofRHCQ3LOj", "forum": "5T1vMQldr8", "replyto": "5T1vMQldr8", "signatures": ["ICLR.cc/2026/Conference/Submission16068/Reviewer_1Ppp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16068/Reviewer_1Ppp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636657056, "cdate": 1761636657056, "tmdate": 1762926257354, "mdate": 1762926257354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address a core challenge in Offline PbRL: extrapolation errors in the reward model. The authors propose Subgoal-based Preference Optimization Through Attention Weight, which utilizes an attention-based preference model to extract subgoals and identify critical states within trajectories. Learning is then conducted via reward shaping."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem is well-defined. Extrapolation error in offline PbRL is a real and critical issue.\n- The method is novel and intuitive. Transforming attention weights into subgoals is an intuitive approach that makes additional use of attention information.\n- SPOT's approach of filtering subgoals based on confidence is interesting."}, "weaknesses": {"value": "1. SPOT heavily relies on attention weights. However, with limited feedback (especially noisy feedback), the learning process of this attention-based preference model can become very unstable, failing to provide fine-grained importance signals.\n2. The experimental setup primarily follows that of Preference Transformer, but two key points from the paper remain unverified. First, the importance of subgoals could be better validated in tasks with original sparse rewards (e.g., AntMaze or Adroit). Second, the paper mentions that reward extrapolation becomes more difficult with noisy preferences, yet no experiments were conducted in noisy environments. Instead, 100% accurate synthetic feedback was used. For example, real human feedback, such as in Uni-RLHF[1], contains noise.\n3. The definition of subgoals depends on hyperparameters and heuristic rules. Despite supporting ablation studies, selecting appropriate hyperparameters for different environments is difficult. Additionally, how was the K=10% value chosen?\n4. The visualizations show key subgoals for the Hopper task. However, goal-guidance may not provide significant gains in locomotion tasks. It would be beneficial to see visualizations of key states for manipulation and goal-oriented tasks.\n\n[1] Uni-rlhf: Universal platform and benchmark suite for reinforcement learning with diverse human feedback. ICLR2024."}, "questions": {"value": "- In Table 1, the scores for lift-ph and can-ph are relatively low. Given that algorithms like diffusion policy[2] can achieve 100% success rate in reward-free (e.g., imitation learning) settings, why is the performance on these tasks low?\n\n[2] Diffusion policy: Visuomotor policy learning via action diffusion. IJRR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n6HQkZxZZU", "forum": "5T1vMQldr8", "replyto": "5T1vMQldr8", "signatures": ["ICLR.cc/2026/Conference/Submission16068/Reviewer_gRT9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16068/Reviewer_gRT9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716192636, "cdate": 1761716192636, "tmdate": 1762926256792, "mdate": 1762926256792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel method named SPOT, which addresses reward model extrapolation errors in offline PbRL by using subgoals extracted from high-attention weight points on preferred trajectories to improves reward model reliability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper presents a simple approach for offline PbRL that can improve performance to some extent."}, "weaknesses": {"value": "It also requires training a CVAE in addition to the preference model, which introduces extra computational cost."}, "questions": {"value": "I have several questions as following:\n\n1.To be honest, novelty of this work is limited. Involving a CVAE after the PT to compute subgoals does not seem very novel, and it also introduces significant extra computation.\n\n2.The experimental results are not sufficiently solid. PT includes AntMaze experiments, and the paper does not compare against more recent baselines such as DTR[1], SEER[2], CPL [3] and more. Including more baselines and tasks would make the results more convincing.\n\n3.It would be helpful to report the increase in training time compared to PT and IPL. I guess the overhead to be substantial.\n\n[1] In-Dataset Trajectory Return Regularization for Offline Preference-based Reinforcement Learning. AAAI 2025.\n\n[2] Efficient preference-based reinforcement learning via aligned experience estimation. 2024.\n\n[3] Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning. 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wG53QMru56", "forum": "5T1vMQldr8", "replyto": "5T1vMQldr8", "signatures": ["ICLR.cc/2026/Conference/Submission16068/Reviewer_5WQr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16068/Reviewer_5WQr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824321487, "cdate": 1761824321487, "tmdate": 1762926256343, "mdate": 1762926256343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}