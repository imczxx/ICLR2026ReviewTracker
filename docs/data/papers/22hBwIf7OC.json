{"id": "22hBwIf7OC", "number": 12857, "cdate": 1758210990362, "mdate": 1763657870081, "content": {"title": "Plug-and-Play Compositionality for Boosting Continual Learning with Foundation Models", "abstract": "Vision learners often struggle with catastrophic forgetting due to their reliance on class recognition by comparison, rather than understanding classes as compositions of representative concepts. \nThis limitation is prevalent even in state-of-the-art continual learners with foundation models and worsens when current tasks contain few classes. \nInspired by the recent success of concept-level understanding in mitigating forgetting, we design a universal framework CompSLOT to guide concept learning across diverse continual learners. \nLeveraging the progress of object-centric learning in parsing semantically meaningful slots from images, we tackle the challenge of learning slot extraction from ImageNet-pretrained vision transformers by analyzing meaningful concept properties. \nWe further introduce a primitive selection and aggregation mechanism to harness concept-level image understanding. \nAdditionally, we propose a method-agnostic self-supervision approach to distill sample-wise concept-based similarity information into the classifier, reducing reliance on incorrect or partial concepts for classification. \nExperiments show CompSLOT significantly enhances various continual learners and provides a universal concept-level module for the community.", "tldr": "We introduce CompSLOT, a universal concept learning method to continual learning with foundation models system to establish a concept-level understanding of class prediction for alternative continual learners.", "keywords": ["Continual learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c81c9005feeb45437fe440da38b1266ea77b8919.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new continual learning algorithm for foundation models called CompSLOT. CompSLOT utilizes an existing object-centric plug in for concept extraction, and incorporates concept learning through decomposition and selection mechanism. Experiment results show that CompSLOT boosts several continual learning methods' performance on standard metrics and benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method bridges object-centric learning, concept learning and continual learning, which is a novel intersection.  \n2. Extensive experiments have been conducted to confirm that CompSLOT improves continual learning performance on standard evaluation metrics and benchmarks.\n3. The proposed method can be easily combined with other continual learning methods.\n4. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. **The models for experiments are not specified.** The authors do not mention of a specific architecture in the main paper. Does all experiments in Table 1 conducted with the same model architecture? Do CLG-CBM and CompSLOT use the same pretrained model? Without these details, it is difficult to confirm the effectiveness and the versatility of the proposed method.\n2. **Lack of concept analysis.** The authors claim that CompSLOT learns human-interpretable concepts for continual learning. However, no experiments are presented to analyze these concepts or evaluate interpretability of the learned representations. \n3. **Hyperparameter sensitivity.** CompSLOT requires hyperparameter tuning ($\\alpha, \\beta, \\tau_t, \\tau_p, \\tau_a$), which is data-dependent. The sensitivity to these hyperparameters is not discussed in the paper."}, "questions": {"value": "1. Does Slot Attention suitable for any ViT, or other vision models?\n2. I was wondering if CompSLOT benefits the naive continual learners: finetuning? Or does CompSLOT need to be combined with other continual learning algorithms? It will be an informative baseline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0xQRNNERIu", "forum": "22hBwIf7OC", "replyto": "22hBwIf7OC", "signatures": ["ICLR.cc/2026/Conference/Submission12857/Reviewer_rGQP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12857/Reviewer_rGQP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12857/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761458001997, "cdate": 1761458001997, "tmdate": 1762923650372, "mdate": 1762923650372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed CompSLOT (Compositional Slot plug-in) is a novel method designed to address the challenges of continual learning on compositional benchmarks. It enhances vision models by extracting disentangled, class-relevant concepts directly from images using Slot Attention mechanism. The core of CompSLOT lies in its robust concept learning phase, which uses the primitive selection and aggregation mechanism to identify essential class-relevant concepts. Experimental results consistently demonstrate that CompSLOT significantly boosts the performance of various state-of-the-art continual learners across challenging compositional benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- CompSLOT can significantly mitigate catastrophic forgetting in continual learning.\n\n- CompSLOT features a highly flexible and method-agnostic plug-and-play design.\n\n- Extensive experiments on challenging compositional datasets robustly validate CompSLOT's superior performance in continual learning."}, "weaknesses": {"value": "- It is unclear how a fair comparison was achieved, as the compared methods were not used in the benchmark paper. It is also not understood how these Class-Incremental Learning methods were applied in this setting.\n\n- Essentially, CompSLOT uses an external model as a teacher to reduce catastrophic forgetting.\n\n- It is unclear how this plugin works with baseline methods. For instance, the plugin generates a set of logits. However, the contribution of some methods, such as CPrompt, is to constrain the logits at different stages during incremental learning. Would the proposed method conflict with these baselines?\n\n- The method is too complex to be reproduced easily."}, "questions": {"value": "see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "03X0xSDxOG", "forum": "22hBwIf7OC", "replyto": "22hBwIf7OC", "signatures": ["ICLR.cc/2026/Conference/Submission12857/Reviewer_hVcV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12857/Reviewer_hVcV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12857/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838273028, "cdate": 1761838273028, "tmdate": 1762923650141, "mdate": 1762923650141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed CompSLOT (Compositional Slot plug-in) is a novel method designed to address the challenges of continual learning on compositional benchmarks. It enhances vision models by extracting disentangled, class-relevant concepts directly from images using Slot Attention mechanism. The core of CompSLOT lies in its robust concept learning phase, which uses the primitive selection and aggregation mechanism to identify essential class-relevant concepts. Experimental results consistently demonstrate that CompSLOT significantly boosts the performance of various state-of-the-art continual learners across challenging compositional benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- CompSLOT can significantly mitigate catastrophic forgetting in continual learning.\n\n- CompSLOT features a highly flexible and method-agnostic plug-and-play design.\n\n- Extensive experiments on challenging compositional datasets robustly validate CompSLOT's superior performance in continual learning."}, "weaknesses": {"value": "- It is unclear how a fair comparison was achieved, as the compared methods were not used in the benchmark paper. It is also not understood how these Class-Incremental Learning methods were applied in this setting.\n\n- Essentially, CompSLOT uses an external model as a teacher to reduce catastrophic forgetting.\n\n- It is unclear how this plugin works with baseline methods. For instance, the plugin generates a set of logits. However, the contribution of some methods, such as CPrompt, is to constrain the logits at different stages during incremental learning. Would the proposed method conflict with these baselines?\n\n- The method is too complex to be reproduced easily."}, "questions": {"value": "see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "03X0xSDxOG", "forum": "22hBwIf7OC", "replyto": "22hBwIf7OC", "signatures": ["ICLR.cc/2026/Conference/Submission12857/Reviewer_hVcV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12857/Reviewer_hVcV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12857/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838273028, "cdate": 1761838273028, "tmdate": 1763564945087, "mdate": 1763564945087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses the problem of catastrophic forgetting in continual learning (CL) models, particularly those using Foundation Models (FMs), which struggle because they rely on simple class comparisons rather than understanding complex objects as compositions of basic concepts.\n\nThe proposed solution is CompSLOT, a universal, plug-and-play module that injects concept-level compositionality into any CL method with an FM backbone.\n\n1. It uses a self-supervised Slot Attention mechanism to break down images into low-dimensional representations called slots (concepts). \n2. It introduces a primitive selection mechanism to identify and aggregate the most class-relevant concepts from the slots. A primitive loss ensures these primitives are consistent across different examples of the same class.\n3.  The core plug-in component is a primitive-logit alignment loss. This loss distills the concept-level similarities between images directly into the modelâ€™s final predictions. This regularization guides the model to make decisions based on meaningful, shared, and distinct concepts, rather than simple feature comparisons."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Slot attention module is highly stable and shows almost no forgetting across sequential tasks.\n\nCompSLOT is method-agnostic and computationally lightweight as it builds upon the existing FM backbone.\n\nCompSLOT significantly boosts the accuracy of diverse CL baselines.\n\nIt enhances compositional generalization abilities.\n\nBenchmarking is broad, and even includes fine-grained classification such as CUB 200. \n\nThe work is clearly written, images are readable. Only Figure 1 is too detailed for teaser image and should be simplified to improve the clarity.\n\nExperiments are convincing. \n\nIdea is novel, straightforward and easy to follow."}, "weaknesses": {"value": "The work should be better contextualized in terms of concept-based continual learning, including discussion with work of [1] and follow-up works. \n\nFigure 1 can be improved as it is right now too complex and does not convey the message about novelty well. \n\nContribution description is vague and unclear. It looks like in the second bullet the CompSlot designed something, not the authors. Looks like the artifact from LLM text improvement. The language there should be simpler, and maybe following organisation made:\n\n- first dot about introduction of compslot and its key components.\n\n- second dot about novel training components as losses\n\n- last one about extensive experiments. \n\n[1] Rymarczyk, Dawid, et al. \"Icicle: Interpretable class incremental continual learning.\" Proceedings of the IEEE/CVF international conference on computer vision. 2023."}, "questions": {"value": "I would like to ask authors for better contextualization of the work and improved clarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "G4qAFGetTV", "forum": "22hBwIf7OC", "replyto": "22hBwIf7OC", "signatures": ["ICLR.cc/2026/Conference/Submission12857/Reviewer_gbFN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12857/Reviewer_gbFN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12857/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862916162, "cdate": 1761862916162, "tmdate": 1762923649766, "mdate": 1762923649766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}