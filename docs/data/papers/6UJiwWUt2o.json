{"id": "6UJiwWUt2o", "number": 15920, "cdate": 1758257129259, "mdate": 1759897273073, "content": {"title": "InvBench: Can LLMs Accelerate Program Verification with Invariant Synthesis?", "abstract": "Program verification relies on loop invariants, yet automatically discovering strong invariants remains a long-standing challenge. We introduce a principled framework for evaluating LLMs on invariant synthesis. Our approach uses a verifier-based decision procedure with a formal soundness guarantee and assesses not only correctness but also the speedup that invariants provide in verification. We evaluate 7 state-of-the-art LLMs, and existing LLM-based verifiers against the traditional solver UAutomizer. While LLM-based verifiers represent a promising direction, they do not yet offer a significant advantage over UAutomizer. Model capability also proves critical, as shown by sharp differences in speedups across models, and our benchmark remains an open challenge for current LLMs. Finally, we show that supervised fine-tuning and Best-of-N sampling can improve performance: fine-tuning on 3589 instances raises the percentage of speedup cases for Qwen3- Coder-480B from 8% to 29.2%, and Best-of-N sampling with N=16 improves Claude-sonnet-4 from 8.8% to 22.1%.", "tldr": "", "keywords": ["Large language model", "Program Invariant"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dc76e63630739d3da47aca3ee3cd796f856eb226.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents InvBench, a benchmark and evaluation framework for assessing large language models (LLMs) on the task of loop invariant synthesis, a central challenge in program verification. Unlike prior work, which assessed LLM-generated invariants based only on correctness or similarity to ground truth (e.g., invariants generated by Daikon), InvBench introduces a verifier-based decision procedure with a formal soundness guarantee, ensuring that correctness judgments are logically valid. The benchmark evaluates both invariant correctness and verification speedup across seven state-of-the-art LLMs and several LLM-based verifiers, compared to the non-LLM solver UAutomizer.\nKey results show that current LLMs struggle to produce strong invariants that meaningfully accelerate verification, though fine-tuning and Best-of-N sampling yield slight improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Provides a sound and general evaluation framework for LLM-generated invariants.\n\nCombines formal reasoning with empirical benchmarking.\n\nIncludes a large dataset derived from SV-COMP benchmarks."}, "weaknesses": {"value": "Limited novelty.\n\nEvaluation leads only to already established conclusions.\n\nDataset construction requires more detailed explanation.\n\nDetails:\n\nThe paper makes no significant novel contribution. It provides an evaluation of LLM-generated invariants in accelerating program verification and reaches the expected conclusion that LLM-generated invariants are not yet useful for verification. However, this evaluation does not yield actionable insights into how to improve LLM-based invariant synthesis.\nThe conclusions are largely unsurprising, reiterating well-established findings namely, that generating strong invariants is more difficult than generating correct invariants, and that model capability is a key factor in performance (lines 311–315). Line 444 also provides the takeaway that “more training is needed,” which is too vague and uninformative, especially since the experiments show that fine-tuning did not produce meaningful improvements.\n\nThe paper also claims a dataset of training instances as a central contribution, but the dataset construction process is insufficiently detailed. Lines 257–267 do not clearly explain how the dataset is created. \nGiven that dataset construction is a central contribution, this process should be described in more detail."}, "questions": {"value": "How are the (899–226) seed programs expanded into 3,589 instances through LLM mutation, parameter variation, or other augmentation techniques?\n\nIf a seed program is supplied to the LLM (GPT-4o), what exactly does the LLM do—does it mutate the program to create new ones? Line 265 states that each synthetic program is analyzed by UAutomizer and only those whose assertions are proved are retained. Which assertions? The following line says that invariants extracted from UAutomizer logs are part of the fine-tuning dataset; are these the same assertions being proved by UAutomizer? How are the (899–226) seed programs expanded into 3,589 problem instances?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vAWIWqHofR", "forum": "6UJiwWUt2o", "replyto": "6UJiwWUt2o", "signatures": ["ICLR.cc/2026/Conference/Submission15920/Reviewer_2Qxp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15920/Reviewer_2Qxp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761780061673, "cdate": 1761780061673, "tmdate": 1762926138617, "mdate": 1762926138617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces InvBench, a benchmark to measure the capabilites of LLMs for generating invariants that accelerate the program verification process. \n\nThe authors claim that prior benchmarks using LLMs (or LLM-based systems) for invariant synthesis (a) use customized datasets and (b) omit comparisons with state-of-the-art symbolic baselines. For the generated invariants to be useful, they need to be weak enough to be provable and strong enough to help prove the target assertion. To measure these, the InvBench benchmark measures the correctness of the generated invariants and the speedup that they provide for verification as compared to the UAtomizer symbolic solver. An evaluation of 7 LLMs and prior LLM-based systems shows that, for the most part, given the same timeout (with the time for LLM-based systems including inference time), UAtomizer outperforms LLM-based methods. Finally, the paper shows that fine-tuning and best-of-N sampling can improve the performance of LLMs on InvBench."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper provides a unified evaluation of multiple LLMs and LLM-based systems on invariant synthesis, which, even though I have reservations about the setup, provides a useful picture of where the field stands today."}, "weaknesses": {"value": "- The claim that prior works omit comparisons with state-of-the-art symbolic baselines is false. In particular, the authors claim in multiple places in the paper, in the Introduction and Results sections, that Loopy [1] [2] (for some reason, they cite the newer version in the introduction but compare against the older version in Section 5) does not compare against UAtomizer. But both the papers do compare against UAtomizer, and even report that UAtomizer outperforms Loopy at invariant synthesis. This raises serious concerns about the veracity of the claims made in the paper.\n- The above point also means that the core insight of the paper (that UAtomizer is still better than LLMs) has been reported before, calling into question the novelty of the work.\n\n- It is unclear why InvBench is different/more useful than the other \"customized datasets\" that the paper mentions. The benchmarks used in [2] and [3] both draw from the benchmark used by Code2Inv [4], SV-COMP problems and some other sources. InvBench also draws its problems from SV-COMP, and thus seems quite similar to prior benchmarks.\n\n- The paper makes two design choices for evaluation, whose justification is unclear to me. First, it measures correctness of generated invariants and speedup due to generated invariants on benchmarks that a symbolic solver (i.e., UAtomizer) can already solve. In contrast, prior works (e.g., [2]) measure whether adding the generated invariants as annotations allow a symbolic solver to verify the program. Being able to verify programs that a solver could not have solved without the generated invariants seems closer to the goal of invariant synthesis than speeding up the verification of programs that a symbolic solver can already verify.\n\n- Second, it includes the LLM inference time in the timeout measurement when comparing LLMs and LLM-based systems to UAtomizer. I understand that this is to compare the overall verification time between LLM (or LLM-based system) + UAtomizer and UAtomizer. However, given the variability of inference times (because they involve network calls to model providers), it is unclear whether the results are due to capability of LLMs or just the slowness of APIs. Running experiments with a larger timeout (which would make the inference time a smaller portion of the total time) seems like a more useful experiment.  I believe that is quite relevant because the UAtomizer solving time can scale very quickly for small but complex programs while the LLM inference time would remain roughly the same. Thus, if total time is what we are interested in, then it is important understand how it scales for complex programs.\n\n[1]: Kamath, Adharsh, et al. \"Finding inductive loop invariants using large language models.\" arXiv preprint arXiv:2311.07948 (2023).\n\n[2]: A. Kamath et al., \"Leveraging LLMs for Program Verification,\" 2024 Formal Methods in Computer-Aided Design (FMCAD).\n\n[3]: Guangyuan Wu, Weining Cao, Yuan Yao, Hengfeng Wei, Taolue Chen, and Xiaoxing Ma. 2024. LLM Meets Bounded Model Checking: Neuro-symbolic Loop Invariant Inference. In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering (ASE '24).\n\n[4]: Xujie Si, Aaditya Naik, Hanjun Dai, Mayur Naik, and Le Song. 2020. Code2Inv: A Deep Learning Framework for Program Verification. In Computer Aided Verification: 32nd International Conference, CAV 2020, Los Angeles, CA, USA, July 21–24, 2020, Proceedings, Part II."}, "questions": {"value": "1. Could you clarify the point about previous approaches not comparing with symbolic tools, and, in particular, Loopy not comparing with UAtomizer?\n\n2. Given that the Loopy paper already shows that UAtomizer outperforms LLMs on invariant synthesis, what is/are the new insight(s) provided by your experiments?\n\n3. How is InvBench different and/or more useful than the benchmarks used in Loopy and LaM4Inv?\n\n4. Could you explain the rationale for the two design choices I have mentioned above?\n\n5. For the second one, is it possible to run an experiment with a larger timeout? I believe that would be quite relevant for the reasons I stated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "owlB0jdGTg", "forum": "6UJiwWUt2o", "replyto": "6UJiwWUt2o", "signatures": ["ICLR.cc/2026/Conference/Submission15920/Reviewer_ZjxM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15920/Reviewer_ZjxM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884443520, "cdate": 1761884443520, "tmdate": 1762926138089, "mdate": 1762926138089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a method for evaluating alleged loop invariants, in particular loop invariants generated by LLMs. Rather than comparing to a known ground-truth or \"gold standard\" invariant synthesis tool, the method proposed works as follows. Consider a property $p*$ (e.g. some assertion) and the loop while($C$){do $B$; assert $p^*$}.\n- Suppose that the LLM proposes the invariant $I$ holds at the start of each loop. That is, while($C$){do assert $I$; $B$}.\n- Then, use an external tool to prove two claims: First, while($C$){do assert $I$; $B$}, and second while($C$){do assume $I$; $B$; assert $p^*$}.\n\nIf a batch of invariants is generated, the method is applied individually to each. Only invariants proved by the external tool are retained.\n\nThe authors use this method to generate a dataset, InvBench, as follows. Starting from SV-COMP (899 instances), use UAutomizer to obtain 3589 program-invariant pairs. The authors then test several LLM-based invariant generators on the benchmark. Fine-grained analysis depending on the instance difficulty (measured by UAutomizer running time) is presented. Runtime and success rate are the main metrics, i.e., how long does it take for a server-hosted LLM to generate a correct invariant, including latency introduced by the server. Additional experiments show that fine-tuning on a train split of the dataset improves performance on a test split."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Loop invariant generation is an important problem, and a promising domain for LLM-based improvements upon classical tools.\n- The method underlying InvBench data generation is simple and easy to understand. I emphasize that simplicity is a strength, not a weakness, of scientific work.\n- The method is proven to be sound.\n- In recent years, fine-tuning has largerly been abandoned in favor of in-context (or zero-shot) methods in many domains. Thus, showcasing a domain in which LLMs showcase poor zero-shot performance which is dramatically improved by fine-tuning, is significant.\n- It's tricky to write an \"interdisciplinary\" paper such as this, at the intersection of ML and formal verification. Although I consider myself an outsider to the latter, I was still mostly able to follow the approach and meaning of experiments. The figure and concrete examples helped."}, "weaknesses": {"value": "- If my summary above is correct, it seems that the finetuning approach is essentially Imitation Learning on UAutomizer. As such, comparing the neural methods to UAutomizer itself seems strange. Concretely, consider scaling up this approach (e.g. as a way to get new SOTA invariant generators). Is the idea that given enough data, eventually a fine-tuned LLM would be able to replace UAutomizer (either it discovers more invariants, or does so faster)?\n- In-context learning (k-shot prompting) is not examined. I don't expect it to do much better than zero-shot, but considering its ubiquity it should be tested.\n- In general, the significance of this new dataset (and approach) would be strengthened if a better experimental analysis was carried out. Right now, the analysis mainly examines whether existing LLMs are able to achieve speed-up on UAutomizer, and the answer is generally \"no\". It would be interesting to conduct an error analysis to understand _why_ these tools fail. Is there an inherent limitation to LLM-based invariant synthesis, or is it a matter of generalizability (e.g. LLMs do very well of instances of a certain type, and poorly on others)? Considering that papers showing that LLMs _can't_ do a certain task are exceedingly rare, I think changing the narrative (and supporting this experimentally) to \"LLMs fail on invariant synthesis, and here is a concrete hill to climb on (a new dataset)\" would be compelling."}, "questions": {"value": "Statements in my summary and Weaknesses can be taken as questions, and I welcome comments by the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SuMxdlHMQS", "forum": "6UJiwWUt2o", "replyto": "6UJiwWUt2o", "signatures": ["ICLR.cc/2026/Conference/Submission15920/Reviewer_GZ5Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15920/Reviewer_GZ5Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003230833, "cdate": 1762003230833, "tmdate": 1762926137721, "mdate": 1762926137721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Strong loop invariants are important for efficient verification. This paper presents InvBench, a simple formal framework and benchmark for evaluating the ability of LLMs to synthesise loop invariants. Prior work on this by Pei et al (2023) has several shortcomings, including no consideration of the strength of generated invariants, and a notion of \"correctness\" based on (syntactic?) comparison of generated invariants to those generated by another (unsound by design) tool. And just naively comparing generated invariants (presumably syntactically) fails to detect invariants that are semantically equivalent.\n\nThe formal framework part of InvBench improves on this using a two-step interaction with an external verifier V (assumed to be sound) to evaluate a candidate invariant q. Given target property p* and program P, the two steps are (if my understanding is correct):\n\n- V(P, ∅, q) to check if q is actually an invariant (i.e. holds for all executions)\n- V(P, {q}, p*) to check if the target property holds when q is treated as an assumption (i.e. holds for all  executions where q is satisfied).\n\nThe first step checks q for correctness and the second step then allows q to be tested for speedup in the verification of p*.\n\nThe authors also highlight another strand of related work (building on Pei et al) which embeds LLMs into verification frameworks. This presumably is closer to the work presented here, but this prior work suffers from a lack of standard benchmarks for evaluation, and lack of comparison against a traditional verifier baseline (e.g. Ultimate Automizer =  UAutomizer). A second contribution of the paper is thus an invariant synthesis benchmark which is used to comparatively evaluate some of the prior work. Overall the LLM-based approaches perform slightly worse than UAutomizer but the paper also shows that fine-tuning and Best-of-N sampling improve things a bit."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It's nice to have the formal framework which uses soundness rather than relying on \"equivalence\" (syntactic or otherwise) to a system which isn't sound. The framework itself is simple, parameterised by an external verifier acting as an oracle. Proof of soundness of the decision procedure seems trivial (not necessarily a weakness though).\n- Makes a convincing case that a standard benchmark for invariant synthesis would be useful and reports on using such a benchmark to evaluate a bunch of existing LLM-based approaches.\n- Provides some evidence that current LLMs find the invariant synthesis problem hard."}, "weaknesses": {"value": "- It's nice that the framework itself is parameterised on V. However the heavy reliance on UAutomizer as the baseline solver for the experimental results (e.g. determining the split of the dataset and speedup) might limit the generality of the results.\n\n- The benchmark itself seems to be a specific (randomly chosen) subset of problems from SV-COMP, after partitioning into easy/hard according to UAutomizer. The benchmark itself is described as a contribution, but the benchmark itself (or sufficient metadata to reconstruct it) wasn't submitted as Supplementary Material."}, "questions": {"value": "### Questions\n\n- Are there reasons other traditional verifiers beyond UAutomizer haven't been considered? If an advantage of your framework is that V is \"pluggable\", perhaps could that be demonstrated by trying out other (sound) verifiers.\n\n- Table 2 and elsewhere: for the point estimates given for correctness/speedup, was there a reason not to report standard deviations or some other indication of uncertainty? And do the figures reported represent single runs or averages over multiple runs of a given model?\n\n- Do you plan to release your benchmark problem set (and perhaps other derivatives of SV-COMP based on verifiers other than UAutomizer)?\n\n### Minor suggestions\n\nMinor textual suggestions that didn't impacted my score but could be improved during the revision/discussion\nphase:\n\n- l.80 (and elsewhere): Is \"customized\" the right word here? \"Customized\" implies some modification of an existing artifact, whereas perhaps you mean a unique dataset specific to that solution? If so, a better choice of word might be \"custom\" (or \"bespoke\", or \"idiosyncratic\").\n- l.82: \"underscoring\": Not sure this is what you mean. (The fact that earlier work omits such comparisons doesn't highlight the need for them -- that's what you're doing!) Maybe you can just omit this clause.\n- l.103: \"the only exception\" -> \"the exception\"\n- l.151: Is it correct to say \"Loopy...*builds*\" an LLM-based pipeline, rather than \"Loopy...*is*\"?\n- l.160: Citation parentheses need fixing for (Hoare, 1969).\n- l.211: d /neq F might be fractionally clearer than d \\in {T, U}.\n- l.266: \"UAutomizer 's\" has a spurious space. You might be able to fix that with \\xspace (if you're using a\n  macro).\n- l.427: LoRA needs a citation.\n- l.444: \"more training efforts are required\" -> \"more training effort is required\"\n\n- The summary of contributions (l.118 onwards) is a bit redundant with the preceding 2 paragraphs. I would think about combining -- i.e. taking the earlier two paragraphs, streamlining slightly, and then making those the summary of contributions 2 and 3. That will make for a smoother read. The contributions could also forward-ref the relevant  sections, so they also work as a summary of the paper.\n\n- Second paragraph of Related Work, especially the second half of the paragraph, is rather cursory. From Chakraborty to Liu there isn't really any commentary, so it's hard to understand the relevance. Maybe just a short sentence to introduce them all as different learning-based methods for improving LLM synthesis would help? And Liu feels a bit out of place; it's just a dataset. Taking a bit more time over this paragraph would be good."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rfz7GS9NWW", "forum": "6UJiwWUt2o", "replyto": "6UJiwWUt2o", "signatures": ["ICLR.cc/2026/Conference/Submission15920/Reviewer_x7S4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15920/Reviewer_x7S4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762106439864, "cdate": 1762106439864, "tmdate": 1762926137286, "mdate": 1762926137286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}