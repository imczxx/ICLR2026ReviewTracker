{"id": "B3Muyi2zgo", "number": 13858, "cdate": 1758223866325, "mdate": 1759897408082, "content": {"title": "NRGPT: An Energy-based Alternative for GPT", "abstract": "Generative Pre-trained Transformer (GPT) architectures are the most popular design for language modeling. Energy-based modeling is a different paradigm that views inference as a dynamical process operating on an energy landscape. We propose a minimal modification of the GPT setting to unify it with the EBM framework. The inference step of our model, which we call eNeRgy-GPT (NRGPT), is conceptualized as an exploration of the tokens on the energy landscape. We prove, and verify empirically, that under certain circumstances this exploration becomes gradient descent, although they don’t necessarily lead to the best performing models. We demonstrate that our model performs well for simple language (Shakespeare dataset), algebraic ListOPS tasks, and richer settings such as OpenWebText\nlanguage modeling. We also observe that our models may be more resistant to overfitting, doing so only during very long training.", "tldr": "we introduce an energy-based GPT model, where inference is gradient descent on the energy; we evaluate it on three datasets Listops, Shakespeare and OWT", "keywords": ["energy-based model", "GPT", "LLM", "small language models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/63c82558f71ea2aa745e0d39aab0543fe8e08c6b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces an interesting and novel approach to generative modelling that integrates energy-based methods with standard GPT like methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Work is novel and interesting.\nModifies standard GPT methods with energy-based methods.\nThe work complements previous studies suggesting that transformers perform GD during inference. Unlike past work, in the proposed model inference is explicitly a gradient-based dynamics, while still maintaining an architecture very similar to GPT.\n\nExtensive experiments show results comparable to standard GPT implementations but requiring fewer parameters."}, "weaknesses": {"value": "Has a drawback similar to diffusion models in that inference is an iterative process and thus takes time and incurs a heavy computational burden. As the paper mentions, a T-step gradient descent can be mapped to a T-layer GPT transformer architecture, but the computational burden remains if the required number of steps T is large."}, "questions": {"value": "The proposed method has similarities to diffusion models. Can it be used to enhance diffusion models (e,g, to generative images instead of applying it to LLMs, using a GAN discriminator to provide a score)?\n\nCan proposition 2.1 be strengthened to provide a weaker constraint on the required inference rate matrix? (i.e. are there other inference rate matrices where the energy decreases?) There is some discussion of this, but I am mainly wondering if there are proofs where the inference rate is something that is more useful for the problem at hand.\n\nNot necessarily a weakness, but the issue of asymptotic stability raises the question of short-term stability. Does this affect the operation and training of the network? How long does one have to wait to reach quasi-stability where things have settled down enough for reliable inference?\n\nWhat is it about the EBM approach that results in needing fewer parameters than equivalent GPT models (table 2)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i6tEYXDczA", "forum": "B3Muyi2zgo", "replyto": "B3Muyi2zgo", "signatures": ["ICLR.cc/2026/Conference/Submission13858/Reviewer_d1Ma"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13858/Reviewer_d1Ma"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752995666, "cdate": 1761752995666, "tmdate": 1762924377930, "mdate": 1762924377930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an energy-based framework for next-token prediction by making key edits to the Transformer architecture to achieve a recurrent block. This model is learned via a differentiable sampling process where parameters are learned during sampling. The energy function mimics the Transformer via a parallel block with an attention-based component and a feed-forward based component, as in parallel transformer designs. Tokens are transformed into the next via exploring the energy landscape. The block is repeatedly applied to mimic layer depth in a recurrent fashion. Results on simple tasks like domain-specific language modeling and list ops show performance comparable to a layer-recurrent Transformer."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This work extends prior work on energy based modeling and the Transformer archtecture to the new and popular setting of next token prediction, advancing from prior work largely focusing on masked infilling based modeling. \n2. The experiments testing NR-GPT on ListOPs and Shakespeare data show competitive results with a recurrent version of a transformer model, indicating that this different learning paradigm has promise and can perform competitively with a modficiation on a dominant paradigm."}, "weaknesses": {"value": "1. It is unclear how much novelty is presented in this work versus the immediate prior work of Energy Transformer. For example, the parallel block structure for EBM was proposed in the prior work, but not attributed directly in this work. Additionally, while proper attribution was given for the attention energy, this architecture and energy formulation are also directly from the prior work, constituting a major part of the current paper's claimed contributions. \n2. The motivation for this avenue of exploring EBMs and Transformers is lacking in the current work - while I believe that studying new avenues for language modeling are important, it is not clear from this work what precise benefit EBMs may give over current language modeling, even if they are \"not there yet.\" \n3. Key details on how sampling actually occurs in this model are not present. How does the arcitecture embed tokens into the network? And more importantly, how are samples actually decoded into tokens?"}, "questions": {"value": "1. How should the original: and energy: expressions be interpreted in equations 13 and 15?\n2. While it is clearly not the point of this work to outperform current language models given the exploratory and new nature of this work, I am still curious to know what are the run-time, memory, FLOP/compute use differences between this model and normal GPT? \n\nNotes:\n1. Cutoff on line 323, sentence not finished, “For”. \n2. Cutoff on line 353, “To reach”, sentence not finished\n3. Citations for Qwen, Llama, and RMSNorm are missing on line 245."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1EcLgxMXQw", "forum": "B3Muyi2zgo", "replyto": "B3Muyi2zgo", "signatures": ["ICLR.cc/2026/Conference/Submission13858/Reviewer_JUki"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13858/Reviewer_JUki"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762113226848, "cdate": 1762113226848, "tmdate": 1762924377607, "mdate": 1762924377607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reformulates the GPT as an EBM. Instead of stacking multiple transformer layers, it uses a single recurrent module that performs iterative energy descent steps. Each forward pass updates the token representations by following the gradient of a total energy function that combines attention and feedforward energies, turning inference into motion along an energy landscape.\n\nThe authors derive energy formulations whose gradients reproduce the transformer’s attention and feedforward operations. They introduce a learnable inference rate matrix η(t), prove that energy monotonically decreases under LayerNorm or RMSNorm, and show asymptotic stability of the update dynamics.\n\nExperiments demonstrate that NRGPT achieves performance comparable to standard and recurrent GPTs on ListOps, Shakespeare, and OpenWebText benchmarks, while requiring fewer parameters and showing greater resistance to overfitting.\n\nOverall, the work provides a minimal and theoretically grounded link between GPT and EBM frameworks, showing that transformer inference can be interpreted as an explicit gradient-based energy minimization process without sacrificing model quality."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "This paper stands out for its originality in redefining the GPT architecture through an energy-based modeling perspective. Instead of proposing a new model class, it reinterprets existing transformer mechanics as iterative energy minimization, offering a fresh theoretical lens rather than a purely architectural innovation.\n\nIts technical quality is solid: the authors construct explicit energy functions for both attention and feedforward components, establish convergence and stability guarantees, and validate the model on multiple benchmarks. The experiments are carefully designed and demonstrate that the energy-based formulation can match GPT performance while using fewer parameters.\n\nThe clarity of presentation is strong, with concise mathematical derivations and intuitive explanations that make the energy interpretation easy to follow.\n\nIn terms of significance, the work bridges a conceptual gap between transformers and energy-based models, opening a path toward more theoretically grounded, interpretable, and potentially more efficient generative models."}, "weaknesses": {"value": "First, the experiments are limited to small and medium-scale models (≤90M parameters) and simple datasets like ListOps and Shakespeare, making it unclear whether NRGPT remains stable and efficient at larger LLM scales. Scaling experiments on multi-billion parameter models and broader benchmarks such as MMLU or GSM8K would make the conclusions more general.\n\nSecond, the theoretical guarantees rely on a learnable inference rate matrix η(t) and specific normalization (LayerNorm/RMSNorm), but the paper lacks ablations on their robustness and training feasibility. Systematic studies of different η parameterizations and normalization variants would clarify when the energy monotonicity and stability conditions hold in practice.\n\nThird, the paper does not analyze runtime efficiency or expressivity trade-offs. Because the model enforces weight sharing, it may have higher iteration costs or reduced fitting capacity, which should be quantified under equal FLOPs or wall-clock settings.\n\nFinally, the paper should discuss its relationship to *Energy-Based Transformers are Scalable Learners and Thinkers* (arXiv:2507.02092). While that work focuses on scalable, multimodal reasoning with “System 2” inference, NRGPT formalizes energy descent within standard GPT. A clear comparison of their goals, formulations, and implications would strengthen the paper’s positioning and highlight its distinct contributions."}, "questions": {"value": "1. Is the proposed energy formulation essentially unique, or could multiple energy functions induce the same transformer dynamics? Any intuition or simple example would help clarify interpretability.\n\n2. Is one-step energy descent meant to be exactly equivalent to a transformer layer, or a principled approximation? If approximate, what governs the approximation quality (e.g., β, normalization, η structure)?\n\n3. Beyond fixed iteration counts, have the authors explored adaptive criteria (energy change, gradient norm, or confidence signals) to decide when to stop the recurrent updates?\n\n4. Do energy trajectories correlate with uncertainty, generation quality, or failure cases? Could energy serve as a runtime diagnostic or control signal (e.g., confidence gating, early stopping)?\n\n5. Since the formulation emphasizes iterative refinement, can NRGPT naturally support test-time “extra thinking steps” (longer inference) without tuning? If so, are there tasks where this yields measurable gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nD60DdNUvM", "forum": "B3Muyi2zgo", "replyto": "B3Muyi2zgo", "signatures": ["ICLR.cc/2026/Conference/Submission13858/Reviewer_cL58"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13858/Reviewer_cL58"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149031377, "cdate": 1762149031377, "tmdate": 1762924377103, "mdate": 1762924377103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents eNeRgy-GPT (NRGPT) a framework for autoregressive language modeling that converts a Generative Pre-trained Transformer (GPT) into an energy-based model. NRGPT inference is conceptualized as an exploration on an energy landscape, where each layer becomes a gradient descent step on an energy function comprising attention and feedforward components. The authors establish theoretical conditions under which this process guarantees energy descent.\nOn three datasets, nested math operations on lists of integers (ListOps), character-level modeling (Shakespeare), and standard language modeling (OpenWebText) NRGPT achieves comparable performance with GPT baselines across different parameter sizes. Notably, the model exhibits greater resistance to overfitting compared to standard transformers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors unify the well-known GPT architecture with Energy-based models, providing explicit energy functions for both attention and feedforward components. This theoretical contribution is novel and advances recent work exploring these connections.\n    \n- The paper is well-written and easy to follow.\n    \n- While the experiments on ListOps, Shakespeare, and Open Web Text cover a narrow range of tasks, they are nonetheless informative for drawing meaningful comparisons between models."}, "weaknesses": {"value": "- While the theoretical connection between energy-based models and transformers is interesting, the paper does not clearly state why this direction is practically valuable or how it could lead to meaningful advances beyond conceptual unification.\n- While NRGPT achieves performance comparable to a recurrent GPT, this comparison has limited practical relevance. The meaningful benchmark remains the standard GPT architecture.\n- NRGPT requires weight sharing across layers (recurrent steps), which posits a constraint in terms of architectural flexibility."}, "questions": {"value": "- Why not comparing NRGPT with recurrent GPT and GPT at approximately similar number of parameters?\n- When comparing models in Open Web Text (Section 3.4), are you using the same tokenizer across all models?\n- How do GPT/Rec-GPT differ from NRGPT in terms of computational cost. How much does the need of the inference rate matrix affect FLOPs?\n\n\nComments:\n\nl114: This is also the premise Energy-Based Models (EBM)\n\nl121: the final point match real datapoints\n\nl322: TO evaluate the\n\nl323 ends abruptly"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "US7Jar9DAl", "forum": "B3Muyi2zgo", "replyto": "B3Muyi2zgo", "signatures": ["ICLR.cc/2026/Conference/Submission13858/Reviewer_xLun"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13858/Reviewer_xLun"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762220342639, "cdate": 1762220342639, "tmdate": 1762924376716, "mdate": 1762924376716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}