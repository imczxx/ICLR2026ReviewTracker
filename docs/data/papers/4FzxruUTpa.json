{"id": "4FzxruUTpa", "number": 23927, "cdate": 1758350452521, "mdate": 1759896790041, "content": {"title": "Two step diffusion:  fast sampling and reliable prediction of 3D Keller-Segel chemotaxis systems in fluid flows", "abstract": "In this work, we study fast and reliable generative transport for the 3D Keller-Segel in different fluid flows, where the goal is to map initial particles $x_0$ to terminal states $x_1$ for a range of physical parameters $\\sigma$. While the quadratic Wasserstein distance $W_2$ serves as a better metric for differences between distributions, optimizing $W_2$ directly is unstable and computationally expensive in high dimensions. We propose a two-stage pipeline that retains one-step efficiency while reinstating an explicit $W_2$ objective where it is tractable. In Stage I, a MeanFlow-style regressor trained via a MeanFlow identity yields a deterministic, 1-NFE global transport that moves particles close to their terminal states without simulating forward diffusions. In Stage II, we freeze this initializer and train a near-identity corrector (Deep Particle, DP) that directly minimizes a mini-batch $W_2$ objective using warm-started EMD (Earth Mover's Distance)/OT (Optimal Transport) couplings computed on the MeanFlow outputs. Crucially, after the one-step transport (from Stage I) has concentrated mass on the correct support, the induced geometry stabilizes high-dimensional $W_2$ optimization. We validate our construction on the 3D Keller Segel setting with different flows and find that our method consistently reduces the empirical $W_2$ relative to one-step flows. Moreover, the two-stage refinement reduces MeanFlow's generalization error across $\\sigma$, demonstrating improved robustness to parameter shift, which is evidenced by consistently lower $W_2$ over the $\\sigma$ sweep. At the same time, the approach preserves the properties of 1-NFE and deterministic sampling, and yields clear qualitative gains in anisotropy and mass placement, as supported by our simulations and $W_2$-vs-$\\sigma$ curves.", "tldr": "", "keywords": ["3D Keller–Segel chemotaxis systems", "Two-Step Diffusion", "Optimal Transport (OT)", "Wasserstein-2 distance (W2)", "GPU-friendly mini-batch OT training"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b4d0bbe05f88329184370f262acd621841fb98c1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a 2-stage method to solve the Keller-Segel transport problem. In the first stage, solutions are generated by a MeanFlow generative model. In the second stage a corrector network is trained by optimizing a mini-batch Wasserstein objective between the generated samples and a reference distribution. The resulting method improves the generalization error across different physical parameters as demonstrated in two experiments in 2D and 3D."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The method is sound and the corrector in the second stage achieves improved performance consistently. \n- Combination of 1 NFE generative models with a final corrector step seems promising."}, "weaknesses": {"value": "- 3D Keller-Segel chemotaxis is a very specific application. I think this framework could generalize to different problems (e.g. simulation-based inference, cell dynamics, etc.). A wider focus on the experiment/application side would make this paper more approachable for the community. It is also difficult for me to assess the experiment setup and scientific importance, since I am not familiar with Keller-Segel Chemotaxis.  \n\n- The paper contains only two main experiments. The improvements from the corrector in the second stage are evaluated and the generalization to different physical parameters $\\sigma$. I would expect additional baselines to show what are the advantages of the method. Since the main goal of the paper is to map the initial particles at time 0 to particles at time 1, would it not be possible to use any framework for optimal transport as benchmarks, for example [1, 2, 3]?\n\n- The paper relies on a 1-step generative model in stage 1. If I understand correctly, paired data $(x_1, x_2, \\sigma)$ is used for training, which are generated via an interacting particle approximation. While 1-step generation is attractive, I am wondering what are the actual benefits compared to a multi-step generation with higher quality; It would be interesting to compare timings from 1 to N steps in stage 1 and consider the compute/time and quality tradeoff. In general, I think comparing the different timings: generation of data, training of models in stage 1/2, inference time is necessary to better motivate the methodology. \n\n- Sometimes the notation is confusing, e.g. z_0 = x_1 in line 182. \\citep and \\citet are not used consistently. Algorithm 1, Fig. 3 and Fig. 4 take up a lot of space that could be filled with additional experiments; line 246 is confusing, \"MeanFlow is restricted to mappings from a Gaussian base to the target\", I thought $x_0$ was sampled from $\\pi_1(\\sigma)$, line 299, which I assumed was not Gaussian. Overall, I think the presentation could be improved and polished.\n\n[1] https://proceedings.mlr.press/v238/tong24a.html\n\n[2] https://arxiv.org/abs/2310.10649\n\n[3] https://arxiv.org/abs/2309.16948"}, "questions": {"value": "See questions mentioned in weaknesses:\n\n- Can you use different methods for OT and compare your method with them?\n- What is the compute/time vs. quality trafeoff when you consider generative models with multiple steps in stage 1?\n- Can you give timings for dataset generation/running the PDE solver compared to inference with your method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "A8YBcie0A2", "forum": "4FzxruUTpa", "replyto": "4FzxruUTpa", "signatures": ["ICLR.cc/2026/Conference/Submission23927/Reviewer_g1Pi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23927/Reviewer_g1Pi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842583706, "cdate": 1761842583706, "tmdate": 1762942861283, "mdate": 1762942861283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a 2-stage algorithm for learning a transport map to solve 3D Keller-Segel problem across different fluid flows. Stage 1 trains a MeanFlow algorithm as one-step transport (1-NFE) that moves prior samples close to the target states. Stage 2 fits a deep particle corrector by directly minimizing a mini-batch $W_2$ objective with warm-started OT couplings, yielding local, geometry-aware refinements."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* **Motivation**: Authors propose an interesting problem of optimizing $W_2$ distance between end-point target distribution and MeanFlow predictions in higher dimensions where MeanFlow fails due to not directly targeting $W_2$ distance\n* **Clarity**: Authors provide technical background on MeanFlow making the paper accessible to wider audience\n* **Results Interpretability**: Results are supported by visualizations making it easy for the reader to understand objectives"}, "weaknesses": {"value": "* **Comparison to baselines**: Work lacks comparison to other one-step model baselines. The method is centered around MeanFlow, however it seems that the key novelty is adding deep particle corrector to minimize $W_2$, which is not tied to mean flow as a first stage algorithm. It would be interesting to see what outperformance the method achieves with various one-step models (e.g. consistency models, flow maps via self-distillation [1] or IMM [2]) as stage 1 algorithms. \n* **Limited Novelty**: Authors cite [3] in line 229 which if I understand correctly also uses one-step generative models and introduces DP method as a $W_2$ corrector, testing empirically on the similar set of Keller-Segel problems. Given the strong similarities between the two works, they should be further compared in the main text, highlighting key differences and ideally comparing them empirically.\n* **Limited Range of Real-world Experiments**: The method is applied across limited range of practical examples. I would encourage authors to discuss in the main text or appendix whether the proposed method could be extended to tackle other high-dimensional problems."}, "questions": {"value": "* In equations (12) and (14), I believe there is a typo? They should read as $x_0 = x_1 - \\int_0^1{v(z_t,t)dt}$ (equation 12) and $x_0 = x_1 - u(x_1, 0, 1)$ (equation 13). I believe the same applies to the expression in line 194.\n* Do you model higher dimensions ($d>3$)? The work mentions learning in high-dimension setting as one of the core limitation of MeanFlow, and one of the key advantages of DP to correct for $W_2$. It would be useful to see how the method generalizes in higher dimensions and if there are trade-offs between using proposed method and only MeanFlow in terms of accuracy and computational performance. I would suggest authors test ideally in real-world or at least a synthetic setting, and compare their results to MeanFlow.\n\n**Minor comments**\n* Figure 1 should perhaps be moved closer to page 1 where it is mentioned \n\n**References**:\n\n[1] Boffi, Nicholas M., Michael S. Albergo, and Eric Vanden-Eijnden. \"How to build a consistency model: Learning flow maps via self-distillation.\" arXiv preprint arXiv:2505.18825 (2025).\n\n[2] Zhou, Linqi, Stefano Ermon, and Jiaming Song. \"Inductive moment matching.\" arXiv preprint arXiv:2503.07565 (2025).\n\n[3] Zhang, Tan, et al. \"A Bidirectional DeepParticle Method for Efficiently Solving Low-dimensional Transport Map Problems.\" arXiv preprint arXiv:2504.11851 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7WMpeTEXNK", "forum": "4FzxruUTpa", "replyto": "4FzxruUTpa", "signatures": ["ICLR.cc/2026/Conference/Submission23927/Reviewer_AiFm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23927/Reviewer_AiFm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149218518, "cdate": 1762149218518, "tmdate": 1762942861087, "mdate": 1762942861087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a two-stage, fast-then-precise transport scheme: a MeanFlow initializer deterministically moves particles near the target support, then a near-identity Deep-Particle corrector is trained with mini-batch W₂ using warm-started optimal-transport couplings—preserving one-step sampling while restoring geometry-aware accuracy. After Stage I concentrates mass, the OT plan becomes nearly permutation-like, making W₂ optimization numerically stable and GPU-efficient. The second stage consistently reduces W₂—including under out-of-distribution conditions in stiff Keller–Segel settings—and improves anisotropy and mass placement; a KPP front-speed check suggests these gains translate to downstream physics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "On 3D Keller–Segel with flow, DP consistently lowers W₂ across σ and shines in the singular-perturbation/OOD regime (e.g., σ=160: 0.0403→0.0082, σ=200: 0.1970→0.0214). The W₂-vs-σ curve (Figure 2, p. 7) flattens after DP, and projections (Figures 3–4) show better anisotropy and mass placement. A KPP front-speed experiment reveals faster estimator convergence when warm-started by MF→DP, accompanied by a significant drop in W₂ (0.2548→0.02933), indicating that the refinement moves distributions closer in a manner that matters for downstream physics. (Appendix, p. 12.)"}, "weaknesses": {"value": "The W₂ refinement still scales quadratically in batch size and depends on solving (mini-batch) OT subproblems. Even after MeanFlow brings supports closer, Stage II must build an O(N²) cost matrix and repeatedly update couplings; the authors explicitly note W₂ is expensive and only “well-suited…in low dimensions,” so scalability in large-N or higher-d regimes remains the bottleneck."}, "questions": {"value": "What are the time/memory costs vs batch size N for Stage II (cost-matrix O(N²), coupling refresh cadence S_γ), and how does training wall-clock compare to a stronger single-step baseline trained longer?\n\nWhich OT solver/hyper-params are used (EMD vs Regularized Sinkhorn/interior-point), how often are couplings re-solved during training, and how sensitive are results to entropic regularization or early stopping of the OT subproblem?\n\nWhen MeanFlow’s initializer is not close (e.g., multimodal supports with large separation), does W₂ training destabilize? Any diagnostics to detect when Stage II should be skipped or down-weighted?\n\nBeyond σ, how does performance vary with particle count, noise in endpoints, and kernel regularization δ in the KS simulator?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "98zYVZFlaF", "forum": "4FzxruUTpa", "replyto": "4FzxruUTpa", "signatures": ["ICLR.cc/2026/Conference/Submission23927/Reviewer_pm4m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23927/Reviewer_pm4m"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762233952526, "cdate": 1762233952526, "tmdate": 1762942860895, "mdate": 1762942860895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}