{"id": "V7dKmfdtGh", "number": 5075, "cdate": 1757843620459, "mdate": 1762962920411, "content": {"title": "Omnicam: Unified Multimodal Video Generation via camera Control", "abstract": "Camera control, which achieves diverse visual effects by changing camera position and pose, has attracted widespread attention. However, existing methods face challenges such as complex interaction and limited control capabilities. To address these issues, we present OmniCam, a unified multimodal camera control framework. Leveraging large language models and video diffusion models, OmniCam generates spatio-temporally consistent videos. It supports various combinations of input modalities:  the user can provide text or video with expected trajectory as camera path guidance, and image or video as content reference, enabling precise control over camera motion. To facilitate the training of OmniCam, we introduce the OmniTr dataset, which contains a large collection of high-quality long-sequence trajectories, videos, and corresponding descriptions. Experimental results demonstrate that our model achieves state-of-the-art performance in high-quality camera-controlled video generation across various metrics. DemoPage can be found in \\url{https://flexcam.github.io/}.", "tldr": "", "keywords": ["Multimodal learning", "Camera-trajectory controllable video generation", "Large language model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/874b153696be9dbffc1b368e7a2c248806959293.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents OmniCam, a unified multimodal camera control framework that enables diverse and precise visual effects by changing camera position and pose. Unlike previous methods that struggle with limited interaction modes and poor scene generalization, OmniCam integrates large language models and video diffusion models to generate spatially and temporally consistent videos. It supports flexible input combinations, allowing users to provide text or video as camera path guidance and image or video as content reference. To train the model, the authors introduce OmniTr, a large-scale dataset with over 10K scenes containing long video trajectories, videos, and detailed descriptions. Experiments show that OmniCam achieves state-of-the-art results in multimodal camera-controlled video generation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths\n1. It introduces a multimodal approach that combines text, image, and video, which expands interaction possibilities and lowers user effort.\n2. The proposed OmniTr dataset provides a valuable large-scale resource for training and evaluation.\n5. The paper is clear in motivation and problem definition."}, "weaknesses": {"value": "Weaknesses\n1. The framework is complex and may be hard to reproduce or apply in real scenarios.\n2. The model depends heavily on the large OmniTr dataset, which may affect generalization.\n3. The ablation study is simple and does not clearly explain how each module contributes."}, "questions": {"value": "Questions\n1. Can the authors provide more details or guidelines to help reproduce the full framework?\n2. In the ablation study, could the authors further explain how each module interacts and contributes to the final performance, possibly with visual or qualitative analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "whmv73fwa6", "forum": "V7dKmfdtGh", "replyto": "V7dKmfdtGh", "signatures": ["ICLR.cc/2026/Conference/Submission5075/Reviewer_zgAa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5075/Reviewer_zgAa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876704712, "cdate": 1761876704712, "tmdate": 1762917857620, "mdate": 1762917857620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Thanks"}}, "id": "j9adeilfRH", "forum": "V7dKmfdtGh", "replyto": "V7dKmfdtGh", "signatures": ["ICLR.cc/2026/Conference/Submission5075/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5075/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762962918995, "cdate": 1762962918995, "tmdate": 1762962918995, "mdate": 1762962918995, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OmniCam, a framework for multimodal camera-controlled video generation. The system aims to unify text-, image-, and video-based inputs for guiding camera motion. It generates videos where users can specify both content (via image/video reference) and trajectory (via text or reference video).\n\nThe core pipeline consists of:\n- Discrete Motion Representation (DMR): A symbolic encoding of camera movement (start time, end time, speed, direction, rotation).\n- Trajectory Generation: Using a LoRA-tuned LLM to convert text (or a fine-tuned VLM for video) into DMR tokens.\n- Trajectory Planning: Mapping DMR into 3D camera poses (ϕ, θ, r).\n- Rendering + Diffusion: Projecting a reconstructed point cloud from the content reference to new camera viewpoints and filling missing regions via a video diffusion model (DiT).\n- Dataset — OmniTr: A synthetic multimodal dataset with 10K+ videos, text descriptions, and camera trajectories, built largely from CO3D and ViewCrafter outputs.\n\nThe authors report strong results in rotation and translation accuracy, FID, and PSNR against models like CameraCtrl, MotionCtrl, and LucidDreamer, and demonstrate text- and video-based camera control on synthetic scenes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Unified pipeline: Handles combinations of (image | video) × (text | video) trajectory inputs — something prior works handled separately.\n- New dataset (OmniTr): Synthetic but diverse; integrates text, video, and trajectory annotations for controllable camera motion.\n- Strong empirical performance: Quantitative improvements over CameraCtrl and MotionCtrl in FID, PSNR, and trajectory accuracy."}, "weaknesses": {"value": "- Paper writing is very unclear. The second paragraph in introduction makes no sense. Also in line 283, given one image, how is it possible to retrieve a camera pose using Dust3R?\n- I am more confused about rendering on point cloud. So are you rendering on point map constructed by the first image. If so, how do you handle the back of the object?\n- The paper titles as unified multimodal video diffusion via camera control and it is interesting to see this paper simplifies complicated 4D problem into first construct a dynamic 3D and then enable trajectory rendering. However, in the main text, I cannot find good examples of dynamic objects and camera control. While I find some examples in the supplementary materials (website), the camera motion is very small which is not convincing to me currently."}, "questions": {"value": "- Does OmniCam generalize to real camera footage without retraining, or only synthetic datasets?\n- This is more like editing task. Can we see original camera pose trajectory and you can edit some camera poses and show the results? I believe the text to discrete camera pose part, but I want to see more visualizations for the camera pose rendering from video diffusion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lO0kTebvA1", "forum": "V7dKmfdtGh", "replyto": "V7dKmfdtGh", "signatures": ["ICLR.cc/2026/Conference/Submission5075/Reviewer_sBFA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5075/Reviewer_sBFA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880299724, "cdate": 1761880299724, "tmdate": 1762917857367, "mdate": 1762917857367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "OmniCam proposes a unified framework for fine-grained camera-controlled video generation across modalities. Its core is a discrete motion representation (start/end time, speed, direction, rotation) that translates either text or a reference video into a per-frame camera trajectory, while content comes from a single image or a video. A spherical trajectory planner turns the representation into extrinsics; point-cloud rendering provides geometry, and a DiT-based diffusion module inpaints unknown regions conditioned on the renders and CLIP features. To better match human perception, the system evaluates and adapts motion speed by bucketing scenes by scale. The paper also introduces OmniTr, a dataset with about 1K trajectories, 10K text descriptions, and 30K videos annotated at the sub-instruction level, and proposes trajectory-following metrics alongside standard video quality measures."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- **Simple and unified motion representation**\nThe discrete motion representation serves as a simple bridge that enables translation from multimodal conditions into control signals that guide the motion trajectory. \n\n- **Text-to-trajectory at frame-level granularity**\nTurning text prompts into multi-segment moves (with speed/angle/rotation) is practically valuable and provides a more user-friendly way to lower interaction costs.\n\n- **Complete end-to-end path**\nThe system map text/video trajectories + image/video content all the way to output video, providing a more user-friendly and straightforward way for camera-controlled video generation."}, "weaknesses": {"value": "- **Unclear writing, lack of technical details**\nLots of details are missing. For example, how does the condition (point cloud rendering, images, and videos) incorporate into the DiT model? What are the training specifics regarding section 4.1.1?\n\n- **Dataset issues**\nOmniTr Dataset appears to consist of synthetically generated videos (e.g., using ViewCrafter) and LLM-authored text, with some manual correction. Will this approach introduce any upper bounds or performance limitations due to the distribution gap? \n\n- **Reliance on point-cloud reconstruction**\nHeavy reliance on point-cloud reconstruction means failure cases can propagate to the diffusion stage. The ablation on sparse point clouds is useful, but wild real-world sequences with motion blur or rapid exposure changes are not convincingly stress-tested."}, "questions": {"value": "See \"Weaknesses\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BFsDfUJ3pO", "forum": "V7dKmfdtGh", "replyto": "V7dKmfdtGh", "signatures": ["ICLR.cc/2026/Conference/Submission5075/Reviewer_Moje"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5075/Reviewer_Moje"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927055087, "cdate": 1761927055087, "tmdate": 1762917856909, "mdate": 1762917856909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents OmniCam, a unified framework for camera-controlled video generation that accepts multimodal inputs for both content (an image or video) and trajectory (text instructions or a reference video). The core technical approach is a multi-stage pipeline that decouples trajectory planning from content generation: first, an LLM parses the trajectory reference into a \"Discrete Motion Representation\" of commands; next, a planning algorithm converts this into a continuous 6DoF camera pose sequence. To synthesize the video, the system uses a \"reconstruct-render-inpaint\" process: it creates a 3D point cloud from the content, renders it from the new poses to create a draft with holes, and then uses a video diffusion model as a sophisticated in-painter to fill in these unknown regions. Finally, the paper introduces the OmniTr dataset, a large-scale synthetic dataset for training, and employs a reinforcement learning step to fine-tune the system, achieving state-of-the-art performance in trajectory accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's primary strength is its truly \"unified\" approach. The ability to mix and match input modalities for both content (Image/Video) and trajectory (Text/Video/Direct Params) is a step forward in usability and flexibility, addressing a clear need for creative and predictable control in video generation.\n- The OmniTr dataset is a major contribution. A key bottleneck in camera control research has been the lack of large-scale, diverse, and well-annotated data. By generating 10,000 descriptions and 30,000 videos, the authors have provided a valuable resource. Furthermore, the introduction of new metrics (Mstarttime, Mspeed, Mdirection, etc.) to evaluate the trajectory parsing step is a great addition, allowing for fine-grained analysis of the system's \"understanding\" of the prompt."}, "weaknesses": {"value": "- The paper is less of a technical contribution, but more of a complex, multi-stage system effort of at least four large, separately-trained components: an LLM, a SLAM/pose-extraction module, a monocular 3D reconstruction model, and a video diffusion model. This makes the system extremely complex to train, tune, and replicate. The \"end-to-end\" RL optimization is an attempt to couple two of these, but it's a post-hoc fix.\n- The dataset creation process seems unclear to me. Is the OmniTr dataset consisted of real-world videos?"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nKgDAXJ4U5", "forum": "V7dKmfdtGh", "replyto": "V7dKmfdtGh", "signatures": ["ICLR.cc/2026/Conference/Submission5075/Reviewer_8Wjo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5075/Reviewer_8Wjo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983312934, "cdate": 1761983312934, "tmdate": 1762917856597, "mdate": 1762917856597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}