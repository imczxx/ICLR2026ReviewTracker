{"id": "8SnAGYf2wM", "number": 21084, "cdate": 1758313588555, "mdate": 1759896942930, "content": {"title": "Learning from Historical Activations in Graph Neural Networks", "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable success in various domains such as social networks, molecular chemistry, and more. A crucial component of GNNs is the pooling procedure, in which the node features calculated by the model are combined to form an informative final descriptor to be used for the downstream task. However, previous graph pooling schemes rely on the last GNN layer features as an input to the pooling or classifier layers, potentially under-utilizing important activations of previous layers produced during the forward pass of the model, which we regard as \\emph{historical graph activations}. This gap is particularly pronounced in cases where a node’s representation can shift significantly over the course of many graph neural layers, and worsen by graph-specific challenges such as over-smoothing in deep architectures. To bridge this gap, we introduce HistoGraph, a novel two‑stage attention‑based final aggregation layer that first applies a unified layer-wise attention over intermediate activations, followed by node-wise attention. By modeling the evolution of node representations across layers, our HistoGraph leverages both the activation history of nodes and the graph structure to refine features used for final prediction. Empirical results on multiple graph classification benchmarks demonstrate that HistoGraph offers strong performance that consistently improves traditional techniques, with particularly strong robustness in deep GNNs.", "tldr": "We propose a novel mechanism for learning from intermediate activations of GNNs called HistoGraph. We discuss its properties and demonstrate its effectiveness by evaluating it on a variety of graph benchmarks.", "keywords": ["Deep Learning; Graph Neural Networks; Graph Pooling"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d260844be7b9035fd1c0d9fb34a80a02b6c11ea8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "HISTOGRAPH is a graph pooling method that leverages historical activations from all GNN layers rather than just the final layer. The method uses a two-stage attention mechanism: (1) layer-wise attention that aggregates node representations across all layers using the final layer as a query, and (2) node-wise self-attention that models spatial interactions between nodes. This approach aims to mitigate over-smoothing in deep GNNs and capture multi-scale graph features."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The method can be used both for end-to-end training and as a post-processing step on frozen pretrained GNNs, making it practical for different scenarios.\n* Extensive experiments across graph classification, node classification, and link prediction tasks demonstrate the method's generalizability.\n* Strong improvement in benchmark results for the proteins dataset (15%)."}, "weaknesses": {"value": "* While the overall idea is novel, the individual components (layer-wise attention, node-wise attention) are standard techniques. The main contribution is their combination for this specific purpose.\n* Storing all intermediate activations (N×L×D) could be memory-intensive for very large graphs or deep networks.\n* Tested datasets are not of large nature."}, "questions": {"value": "* The method assumes all GNN layers produce embeddings of the same dimension Din. How would you handle architectures where layer dimensions vary, which is common in some GNN designs?\n* Beyond the over-smoothing mitigation proof, can you provide any theoretical guarantees about the expressiveness of HISTOGRAPH compared to standard pooling methods?\n* How critical are the sinusoidal positional encodings for layer positions? Have you tried other encoding schemes or learning the positional embeddings?\n* How sensitive is the method to hyperparameters, particularly the hidden dimension D and the number of attention heads?\n* Have you tested HISTOGRAPH on industrial-scale graphs (millions of nodes)? What are the practical limitations you've encountered?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gedlR5dpDI", "forum": "8SnAGYf2wM", "replyto": "8SnAGYf2wM", "signatures": ["ICLR.cc/2026/Conference/Submission21084/Reviewer_rAFQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21084/Reviewer_rAFQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761375377522, "cdate": 1761375377522, "tmdate": 1763000002935, "mdate": 1763000002935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel and effective GNN architecture, HISTOGRAPH, which mitigates the over-smoothing problem by explicitly modeling and leveraging historical node representations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear; the authors propose leveraging historical representations to mitigate over-smoothing, which is reasonable and well-justified.\n2. The experiments are comprehensive, thoroughly validating the effectiveness of their method across various tasks."}, "weaknesses": {"value": "1. Lacks comparison with some more recent baselines [1].\n2. No experimental comparisons were conducted on larger graphs, such as those in the OGB [2] suite. How does the time efficiency compare to the baseline when the graph size increases?\n3. How are the historical representations specifically utilized? What are the theoretical advantages of the gating mechanism?\n4. Lacks a theoretical analysis of the method's effectiveness.\n\n[1] Wang Y, Liu S, Zheng T, et al. Unveiling global interactive patterns across graphs: Towards interpretable graph neural networks[C]//Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2024: 3277-3288.\n\n[2] Hu W, Fey M, Zitnik M, et al. Open graph benchmark: Datasets for machine learning on graphs[J]. Advances in neural information processing systems, 2020, 33: 22118-22133."}, "questions": {"value": "See the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JvxRNcD45q", "forum": "8SnAGYf2wM", "replyto": "8SnAGYf2wM", "signatures": ["ICLR.cc/2026/Conference/Submission21084/Reviewer_z6Gq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21084/Reviewer_z6Gq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761467366108, "cdate": 1761467366108, "tmdate": 1762941089820, "mdate": 1762941089820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an architectural modification for graph neural networks (GNNs) consisting in (1) an intra-node attention mechanism that aggregates the representations for each node at different layers (i.e., different message passing iterations), and (2) a global inter-node self attention layer (aggregating information across all nodes). \nThe motivation from this architecture comes from the fact that current methods ignore the history of graph activations across layers, even though this may include some useful information.\nThe experimental evaluation compares the proposed method on standard node and graph classification benchmarks, both when training from scratch and when applying the method to a pre-trained model, and compares against a vast number of competitors. The proposed method shows high performance and an ablation studies explores different attention modifications."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- It is clear that the authors have spent a lot of effort in the experimental section as they compare against a large number of baselines and consider a large number of datasets.\n- The proposed method can be easily included into existing architectures (at the cost of some training for the new parameters)."}, "weaknesses": {"value": "- Global self-attention is quadratic in the number of nodes, which makes the method impractical for large graphs.\n- Caching in memory the activations at all layers for all nodes can become prohibitively expensive. Together with the above, this makes the proposed method very impractical for large graphs.\n- Section 4 is not very convincing as the arguments are too general. Regarding oversmoothing, Proposition 1 is obvious, and in practice different nodes might perform better with different alphas (which however is not allowed). Furthermore applying global self-attention is actually promoting smoothing. Regarding the trajectory filter, the argument can be made for any attention mechanism, and also it is hard argue whether models actually learn to use this information this way."}, "questions": {"value": "- The computational complexity analysis is a bit confusing. First it is mentioned that the proposed method improves over a naive \"joint node-layer attention\" which has complexity O(LN^2D) by instead having a method which is O(NLD + N^2D) but then it is (correctly) mentioned that the complexity is dominated by N^2D, which means that there is no advantage. So it seems that the paragraph from line 216 to 220 does not really have much sense. Could you please clarify the point of this paragraph?\n- The same happens in the \"Frozen Backbone Efficiency\" paragraph: as the complexity is dominated by N^2D there is no advantage (I do not doubt that in practice it can have a difference in runtime, but in terms of computational complexity there is no difference). Could you clarify what is the advantage that is mentioned in the paper?\n- What are the hyperparameters for the baselines and how where they selected? These details should be included in the paper\n- It is mentioned that the method can overcome oversmoothing, but global self-attention actually goes against this. Could you elaborate on why global self-attention would not lead to oversmoothing?\n- On PROTEINS the proposed method reaches 97% (almost perfect), while all other methods stop at 80%. This difference is quite striking and should be analyzed. Could the authors comment on this? \n- The runtime analysis shows training time, but I think inference is actually much more important as training only happens once. Could you include some number for inference times? Some plots showing how runtime and memory scale as a function of N and L would be great.\n\nMinor comments:\n- There are some imprecisions in the text, specially in the math formalism:\n\t- lines 166 and 170 define X in different ways\n\t- in line 183 for computing X^tilde the sizes do not match (I understand there is an implicit broadcasting, but it should be formalized better)\n\t- in Section 4 the notation for node vectors is not introduced\n- It would be interesting to see an analysis of the attention maps to understand which layers are receiving the most attention"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Et7MbBIckO", "forum": "8SnAGYf2wM", "replyto": "8SnAGYf2wM", "signatures": ["ICLR.cc/2026/Conference/Submission21084/Reviewer_iaN5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21084/Reviewer_iaN5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761592575032, "cdate": 1761592575032, "tmdate": 1762941068858, "mdate": 1762941068858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HISTOGRAPH, a two-stage attention-based pooling framework for Graph Neural Networks (GNNs) that leverages intermediate activations (“historical graph activations”) from all layers rather than only the final layer. The approach first applies layer-wise attention to capture the evolution of node embeddings across depths, followed by node-wise attention to model spatial dependencies. The method can be integrated end-to-end with a backbone GNN or applied as a lightweight post-processing head on frozen models. Experimental results on TU and OGB benchmarks, as well as node classification tasks, demonstrate improved performance and robustness to over-smoothing in deep architectures."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel perspective: The paper introduces a clear and well-motivated idea of learning from the historical trajectory of node activations, addressing the common limitation of relying solely on the last GNN layer.\n\n2. Comprehensive experiments: Evaluations across multiple datasets (TU, OGB, node classification, and link prediction) with both GIN and GCN backbones show consistent improvements.\n\n3. Well-written and well-positioned: The paper situates HISTOGRAPH clearly within prior works on pooling, residual connections, and over-smoothing mitigation"}, "weaknesses": {"value": "1. Limited interpretability of learned attention weights: While attention is used layer-wise and node-wise, the paper could benefit from deeper analysis of what the model learns—e.g., visualization of layer weights across datasets.\n2. The attention mechanism itself is widely adopted and not novel. However, the paper should further clarify why the proposed method achieves such notable performance gains. A deeper analytical discussion and illustrative case studies would substantially strengthen the contribution."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OWmxRBYXIa", "forum": "8SnAGYf2wM", "replyto": "8SnAGYf2wM", "signatures": ["ICLR.cc/2026/Conference/Submission21084/Reviewer_Pj7q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21084/Reviewer_Pj7q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762275760118, "cdate": 1762275760118, "tmdate": 1762941059922, "mdate": 1762941059922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}