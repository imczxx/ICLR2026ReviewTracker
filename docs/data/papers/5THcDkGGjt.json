{"id": "5THcDkGGjt", "number": 300, "cdate": 1756734409905, "mdate": 1759898268482, "content": {"title": "Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation", "abstract": "Camera-centric understanding and generation are two cornerstones of spatial intelligence, yet they are typically studied in isolation. We present Puffin, a unified camera-centric multimodal model that extends spatial awareness along the camera dimension. Puffin integrates language regression and diffusion-based generation to interpret and create scenes from arbitrary viewpoints. To bridge the modality gap between cameras and vision-language, we introduce a novel paradigm that treats camera as language, enabling thinking with camera. This guides the model to align spatially grounded visual cues with photographic terminology while reasoning across geometric context. Puffin is trained on Puffin-4M, a large-scale dataset of 4 million vision-language-camera triplets. We incorporate both global camera parameters and pixel-wise camera maps, yielding flexible and reliable spatial generation. Experiments demonstrate Puffin’s superior performance over specialized models for camera-centric generation and understanding. With instruction tuning, Puffin generalizes to diverse cross-view tasks such as spatial imagination, world exploration, and photography guidance. We will release the code, models, dataset pipeline, and benchmark to advance multimodal spatial intelligence research.", "tldr": "We make the first attempt to unify camera-centric understanding and generation in a cohesive multimodal framework.", "keywords": ["Unified Multimodal Model", "Spatial Intelligence", "Controllable Generation", "Camera Calibration"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e731816d9c5db647a44429defc66ec99245369a4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Puffin, a unified multimodal framework that treats the camera as a first-class modality to both (1) understand camera geometry from images and (2) generate images under precise camera control. Key ideas include: “thinking with camera” (reasoning in photographic terms such as Dutch angle / tilt-up) to bridge numeric parameters and language; a geometry-aligned vision encoder; and a continuous camera latent via pixel-wise camera maps to condition a diffusion generator. A new Puffin-4M dataset (4M image–text–camera triplets) and evaluation sets (Puffin-Und, Puffin-Gen) are introduced. Puffin reports strong results on camera understanding across several datasets and large margins over LMM baselines and PreciseCam for camera-controllable generation; ablations suggest the “thinking” and camera-map latent help."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Precise, flexible camera control via discrete tokens plus a dense per-pixel camera map.\n\n\n2. Unified “think-with-camera” design that improves both understanding (pose/FoV) and controllable generation, and the concept itself is novel.\n\n\n3. Scales with large curated data and cleanly extends to new tasks and parameters."}, "weaknesses": {"value": "1. Missing details on the construction of Puffin-Und and Puffin-Gen.\n\n2. Training at fixed 512 and use of central crop + resize for non-square inputs degrades understanding on datasets like LaMAR; this is acknowledged but might affect claims of generality."}, "questions": {"value": "1. Line 207, table A1 in the appendix should not be directly referred to in the main content.\n\n2. How sensitive are results to the exact photographic term thresholds (Table A1)? Any continuous-to-discrete ablation or learned bins?\n\n\n3. How does Puffin handle fisheye or smartphone ultrawide distortion at test time? Could the camera-map latent be extended with distortion fields?\n\n\n4. If you remove LLM-generated “thinking” (or replace with noisy/short versions), how quickly do understanding/generation scores degrade? Any attempts at self-consistency or rationale-free training?\n\n\n5. The abbreviation “FoV” was never introduced."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yS3L5gRpXJ", "forum": "5THcDkGGjt", "replyto": "5THcDkGGjt", "signatures": ["ICLR.cc/2026/Conference/Submission300/Reviewer_D4GA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission300/Reviewer_D4GA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800019462, "cdate": 1761800019462, "tmdate": 1762915488263, "mdate": 1762915488263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Puffin, a unified multimodal framework that treats the camera as a first-class modality to both (1) understand camera geometry from images and (2) generate images under precise camera control. Key ideas include: “thinking with camera” (reasoning in photographic terms such as Dutch angle / tilt-up) to bridge numeric parameters and language; a geometry-aligned vision encoder; and a continuous camera latent via pixel-wise camera maps to condition a diffusion generator. A new Puffin-4M dataset (4M image–text–camera triplets) and evaluation sets (Puffin-Und, Puffin-Gen) are introduced. Puffin reports strong results on camera understanding across several datasets and large margins over LMM baselines and PreciseCam for camera-controllable generation; ablations suggest the “thinking” and camera-map latent help."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Precise, flexible camera control via discrete tokens plus a dense per-pixel camera map.\n\n\n2. Unified “think-with-camera” design that improves both understanding (pose/FoV) and controllable generation, and the concept itself is novel.\n\n\n3. Scales with large curated data and cleanly extends to new tasks and parameters."}, "weaknesses": {"value": "1. Missing details on the construction of Puffin-Und and Puffin-Gen. [UPDATE: Upon further check, I found the dataset construction details in appA3. However, this appendix section is never referred to in the main content. In fact, none of the appendix is mentioned in the main content. Authors should fix this writing issue in the updated manuscript. Thanks!]\n\n2. Training at fixed 512 and use of central crop + resize for non-square inputs degrades understanding on datasets like LaMAR; this is acknowledged but might affect claims of generality."}, "questions": {"value": "1. Line 207, table A1 in the appendix should not be directly referred to in the main content.\n\n2. How sensitive are results to the exact photographic term thresholds (Table A1)? Any continuous-to-discrete ablation or learned bins?\n\n\n3. How does Puffin handle fisheye or smartphone ultrawide distortion at test time? Could the camera-map latent be extended with distortion fields?\n\n\n4. If you remove LLM-generated “thinking” (or replace with noisy/short versions), how quickly do understanding/generation scores degrade? Any attempts at self-consistency or rationale-free training?\n\n\n5. The abbreviation “FoV” was never introduced."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yS3L5gRpXJ", "forum": "5THcDkGGjt", "replyto": "5THcDkGGjt", "signatures": ["ICLR.cc/2026/Conference/Submission300/Reviewer_D4GA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission300/Reviewer_D4GA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800019462, "cdate": 1761800019462, "tmdate": 1763675464095, "mdate": 1763675464095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on an interesting task -- unified camera-centric understanding and generation. There are two major contributions: they collect a 4M high-quality dataset with multiple labels for both mm understanding and generation, while they also train a unified VLM called Puffin for the target task. The proposed model achieved state-of-the-art performance on the camera-centric tasks and enables quite a few interesting applications. Overall, this paper positions “thinking with camera” as a step toward foundation‑level models that natively incorporate 3D geometry in both understanding and generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is well-structured and easy to follow\n- The framing of a single model and interface for both camera‑centric understanding and generation, with camera tokens as the key abstraction, is interesting (though it follows the popular way of designing unified models)\n- Puffin‑4M appears carefully constructed to supervise geometry/camera attributes across many scenes.\n- Superior performance compared with sotam odels, for both tasks."}, "weaknesses": {"value": "- Missing specialized strong baselines: some comparisons to **strongest specialized 3D models** (e.g., recent camera calibration/pose methods) seem limited in the main text; more head‑to‑head numbers would strengthen claims.\n\n- L408-416: for generation comparisons, it is also useful to show standard generation evaluation metrics, such as FID -- perfect camera control makes no sense if the overall visual quality is poor. \n- Better to show the error bar for the compared models in the major experiments. \n- Details of thinking with camera is missing."}, "questions": {"value": "- In A.3.3, the caption of the Puffin is generated by Qwen2.5VL will this explain why Qwen-image (reuse qwen2.5vl as encoder) performs better than GPT4o and Nano-banana? How to mitigate the affect of this issue? \n- Table A.3, bottom right cell, should be 0.2 rather than 0.05?\n- For thinking with camera, will RL, such as GRPO, help improve the reasoning capability? \n- Following the previous comment, it seems the thinking with camera training data is generated by Qwen2.5VL, how to make sure Qwen is able to provide some high-quality data since it is not trained on this task? Is it possible to measure their quality? \n- Are **camera tokens** robust to out‑of‑distribution intrinsics/extrinsics?\n- How does performance scale with data set size and quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JlgFELYDb5", "forum": "5THcDkGGjt", "replyto": "5THcDkGGjt", "signatures": ["ICLR.cc/2026/Conference/Submission300/Reviewer_2Lvj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission300/Reviewer_2Lvj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949991460, "cdate": 1761949991460, "tmdate": 1762915488143, "mdate": 1762915488143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents Puffin, a multimodal model that unified camera-centric generation and understanding. Puffin treats camera parameters such as pitch, yaw and FoC as discrete tokens, similar to language tokens, thereby enabling thinking with cameras. Puffin-4M is introduced which consists of vision-language-camera triplets, constructed by collecting panoramic images followed by perspective crops on different camera angles, and synthetic captions generated by a VLM. The camera understanding takes as input : text and camera discrete tokens, and image tokens from a geometry-aligned vision encoder, while the camera generation module additionally has a learnable connector module and camera maps as additional conditioning. After multi-stage training on Puffin-4M, experiments demonstrate Puffin’s superior performance over specialized models for camera-centric generation and understanding. Futher, with instruction tuning Puffin can be extended to cross-view tasks such as world exploration and spatial imagination."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper is easy to read and well-motivated; in that it is the first attempt to unify camera generation and understanding.\n2. Puffin outperforms existing baselines and methods across multiple benchmarks.\n3. The finding that representing and learning camera parameters as discrete tokens is an impactul finding; making them almost analogous with how text tokens are used in today's generative vision systems.\n4. The paper is technically dense; all design choices and the reasonings behind them are well documented.\n5. I believe the Puffin-4M dataset will be a great contribution to the community."}, "weaknesses": {"value": "I have a few minor weakeness/comments : \n\n1. As shown in Table A1, the parameter-to-term mapping is not exhaustive; for example how is a small tilt-up with a clockwise\ndutch angle handled?\n2. Since, the camera parameter tokenizer is similar to the text tokenizer, is there any ablation that show the effect of different kinds of text encoder? For example, is there a performance delta in using encoder / encoder-decoder / decoder-only models?"}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iKns4pSmfm", "forum": "5THcDkGGjt", "replyto": "5THcDkGGjt", "signatures": ["ICLR.cc/2026/Conference/Submission300/Reviewer_j8cQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission300/Reviewer_j8cQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979040040, "cdate": 1761979040040, "tmdate": 1762915488021, "mdate": 1762915488021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Puffin, a unified multimodal model for camera-centric understanding and generation. Specifically, Puffin formulates camera understanding as an AR text token generation task, and camera-centric generation as a learnable query + conditional diffusion generation task in a meta-query way. Both tasks are unified in an LLM with multiple additional components (a connector and diffusion model for generation, a visual encoder for understanding).  The authors further build a large-scale dataset, Puffin-4M, containing 4M vision-language-camera triplets, to facilitate this new paradigm. Extensive experiments on benchmarks demonstrate Puffin’s strong performance across both understanding and generation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Unified camera-centric understanding and generation framework: It's a novel and meaningful idea to unify camera understanding and controllable generation in one framework.\n2. Thinking with Camera: Interpreting camera parameters as text description bridges geometry and language, and further enables reasoning with the camera in understanding and generation tasks.\n3. The newly collected 4M high-quality camera-centric vision-language dataset should be very helpful to the community.\n4. The proposed model can outperform previous works in both understanding and generation tasks, covering multiple benchmarks. Also, the ablation comprehensively covers different components of the model."}, "weaknesses": {"value": "1. Parameter comparison over previous works:  A clear analysis of model scale and computational cost is missing. Since Puffin integrates multiple large-scale model components (LLM, diffusion model, vision encoder), comparing the total parameter count and FLOPs with prior understanding and generation baselines (e.g., GeoCalib, PreciseCam, etc) is needed.\n2. Data vs. model contribution: It's not quite clear whether Puffin’s performance gains mainly come from the model architecture or the large new dataset (Puffin-4M). Ablation and fair comparison under the same training dataset would be useful.\n3. Multi-round conversation capability: While the paper discusses instruction tuning and cross-view reasoning, it is not clear whether the proposed model can generalize well on multi-turn interleaved understanding and generation. For example, first do generation, then understanding in the second round."}, "questions": {"value": "Please see weaknesses. \n\nBesides, I have one more question regarding whether understanding and generation can benefit each other. In this paper, the author(s) claim that unifying understanding and generation can help each other (to a significant degree). However, it seems that in other general unified image understanding and generation works, the mutual benefit is not quite clear or not very significant. Can you explain why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KtoonWa3r4", "forum": "5THcDkGGjt", "replyto": "5THcDkGGjt", "signatures": ["ICLR.cc/2026/Conference/Submission300/Reviewer_pAbj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission300/Reviewer_pAbj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762064739540, "cdate": 1762064739540, "tmdate": 1762915487843, "mdate": 1762915487843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}