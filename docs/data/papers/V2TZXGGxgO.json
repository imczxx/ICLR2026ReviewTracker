{"id": "V2TZXGGxgO", "number": 13924, "cdate": 1758225267873, "mdate": 1759897403074, "content": {"title": "Latent Personality Alignment: Improving harmlessness without mentioning harms", "abstract": "Current safety training methods for large language models rely on extensive datasets of harmful prompts paired with refusal responses, requiring many thousands of examples to achieve robustness against adversarial attacks. However, these approaches suffer from poor generalization to novel attack vectors and require substantial computational resources. We propose Latent Personality Alignment (LPA), a data-efficient alternative that trains models to embody beneficial personality traits rather than memorizing specific refusal patterns. Using fewer than 100 abstract personality statements, LPA guides models toward positive traits through latent adversarial training. Our approach achieves comparable safety performance to methods trained on hundreds of thousands of harmful examples while maintaining superior utility on benign tasks. These results suggest that personality-based alignment offers a more principled and scalable approach to harmlessness training than current methods.", "tldr": "", "keywords": ["AI Safety", "Personality Traits", "Latent Adversarial Training", "Adversarial Training", "Safety Finetuning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4c8d65ff3506955bfce75c3bcb85d190abf1b90e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an efficient way to train helpful and harmless language models that does not require examples of harmful behavior. First, the paper uses a personality alignment technique. Instead of training on positive and negative demonstrations, affirmations or negations of personality, trait, descriptions. Second, the paper uses latent adversarial training to deepen that alignment. This paper presents competitive results with prior methods, using much less data and no examples of harmful behavior. Overall, I think it's pretty clever and these experiments worked better than I would've predicted."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "S1: One thing I like about this type of method is that it does not require examples of harmful behavior in order to train a model to avoid exhibiting them. This is more efficient, it doesn't require precise knowledge of how harmful behavior looks, and there are some instances in which having examples of harmful behavior can be bad in and of itself. This does not work on diffusion models, but I wonder if it would be possible to make diffusion models more resistant to outputting NSFW content without using any NSFW content. This would probably be challenging, and it is not within the scope of this paper, but the idea came to mind.\n\nS2: Overall, I am impressed by the efficiency and effectiveness of the method. I am pretty convinced that it can be useful in practice. I could imagine this directly helping to shape the SOTA for training helpful, harmless systems. I would not have expected the type of training examples in table 2 to make the model generalize so effectively. Inside of the success of this method lies some kind of lesson about how training influences model personalities.\n\nS3: Table 3 is compelling.\n\nS4: I'm glad the paper used OR Bench to confirm minimal over-refusal"}, "weaknesses": {"value": "W1: Ideally the NaN experiments could be rerun. Gradient clipping could probably fix stability issues.\n\nW2: The paper is framed as contributing LPA as an improvement over LAT and CAT which it in many ways is. But I think it would be equally reasonable to highlight how prompt alignment can be an improvement over refusal training in general."}, "questions": {"value": "Q1: Did you use the same implementation/code as Sheshadri et al? This seems like what happened, but I'm not sure. There are a lot of technical details to doing LAT like the layer selection, and which latent positions get perturbed that are not explicitly detailed in this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DxrzsuQQV0", "forum": "V2TZXGGxgO", "replyto": "V2TZXGGxgO", "signatures": ["ICLR.cc/2026/Conference/Submission13924/Reviewer_jgmu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13924/Reviewer_jgmu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761539559662, "cdate": 1761539559662, "tmdate": 1762924431335, "mdate": 1762924431335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Latent Personality Alignment (LPA), a data-efficient alternative that trains models to embody beneficial personality traits rather than memorizing specific refusal patterns. The method demonstrates advantages over two latent training baselines on several safety benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The method is straightforward and easy to implement.\n- The method is better than the latent adversarial training baselines."}, "weaknesses": {"value": "- Moderate performance. According to Table 4, LAT is much better than the proposed LPA. Although the authors claim that LAT needs much more data, its performance with less data is not clear. \n- More baselines are needed. There are various safety training approaches (instead of latent training), which also don't require many training data and can achieve good performances. I am not sure why the authors exclude these methods during evaluation.\n- Limited novelty. It seems that only data content is changed compared to LAT."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tAPfeiRbbQ", "forum": "V2TZXGGxgO", "replyto": "V2TZXGGxgO", "signatures": ["ICLR.cc/2026/Conference/Submission13924/Reviewer_zVm3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13924/Reviewer_zVm3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980182749, "cdate": 1761980182749, "tmdate": 1762924430890, "mdate": 1762924430890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Latent Personality Alignment (LPA). This method is based on fewer than 100 abstract personality statements to perform latent adversarial training (LAT). The LPA model can be used to identify whether a safety-related prompt should be reject or not. The method is quite simple and has a good performance from the experimental results."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This safety field is indeed important.\n2. The method seems very simple yet effective. This paper analyzes over-reject situations."}, "weaknesses": {"value": "1. **Insufficient methodological clarity.** The method description is inadequately detailed. The paper dedicates less than half a page to describing \"Our work: LPA training,\" with most of that space devoted to a single example. Given that the paper falls well short of the nine-page limit, it is unclear why the authors did not utilize the available space to provide a more comprehensive and clearer explanation of the methodology. To be honest, I read many times and still cannot undertand very well how exactly this method works and what is the input/output of the proposed method.\n\n2. **Limited novelty and contribution.** From my understanding, this paper applies latent adversarial training (LAT)—a technique proposed a decade ago—without substantial modifications, merely substituting the dataset with personality trait data. The technical contribution appears insufficient for a venue of this caliber.\n\n3. **Inadequate experimental evaluation.** \n   - The choice of LLAMA-2/3-base as the baseline is outdated; more recent models should be considered.\n   - The paper lacks comparison with other standard baselines such as supervised fine-tuning (SFT) or reinforcement learning (RL) techniques, which are common approaches in alignment research.\n\n4. **Lack of justification and analysis.** The paper reads as a mere description of an algorithm followed by experiments, without adequate explanation of the underlying mechanisms. Specifically:\n   - Why and how do personality traits differ fundamentally from other refusal training data?\n   - What is the theoretical basis for expecting this approach to work?\n   - Could the personality traits simply be functioning as a form of the well-known chain-of-thought (CoT) prompting? This hypothesis deserves investigation and discussion."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MrOGLt52Ck", "forum": "V2TZXGGxgO", "replyto": "V2TZXGGxgO", "signatures": ["ICLR.cc/2026/Conference/Submission13924/Reviewer_VRHc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13924/Reviewer_VRHc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980914603, "cdate": 1761980914603, "tmdate": 1762924430500, "mdate": 1762924430500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **Latent Personality Alignment (LPA)**: an adaptation of the **LAT** framework that replaces explicit safety supervision with a very small set (<100) of high-level “personality statements,” trained via agree/disagree signals under latent adversarial training. The authors claim this allows refusal behavior to emerge without direct exposure to harmful exemplars, reporting improvements on attack-success metrics with limited utility degradation, mainly on Llama-2/3 models after extensive model selection."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The problem is important: Reducing reliance on hazardous data while aligning safety behavior is a timely and consequential goal for the community.\n- Motivation is interesting: Casting alignment through abstract personality traits is an appealing conceptual angle."}, "weaknesses": {"value": "1. **Marginal methodological contribution.** In practice, the method largely swaps the supervision signal within LAT (from harmful QA to personality statements). Its effectiveness remains unsubstantiated (see below). Moreover, the claim of “no exposure to harmful data” is weakened by the inclusion of explicitly negative traits (e.g., *malice* / “evil assistant”), which act as proxy safety labels rather than removing dependence on the harm distribution.\n2. **Unfair baseline evaluation.** The reported L2-LAT baseline is effectively broken (e.g., *clean* score ≈0.13 vs. 0.78 baseline; MT-Bench 0.632 → 0.189), rendering comparisons uninformative. Given the authors trained this baseline themselves, these numbers likely reflect **training pipeline issues**, not inherent weaknesses of LAT.\n3. **Missing and biased baselines.** The study covers few models (only Llama; no Qwen, etc.) and employs **inconsistent system prompts** across methods (orig/simple/alpha), while LPA benefits from a more favorable prompt. This masks the true contribution. \n4. **Misleading efficiency claims.** While each run uses <100 statements and 5–10 minutes, the paper relies on a large grid search (≈17 trait subsets × 30–50 models ≈ 70 GPU-hours). \n5. **Ablation is uninformative.** The LPA-flip experiment merely demonstrates that if you train the model to be evil, it becomes more evil—an obvious conclusion that offers almost no valuable insight"}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GJi6YQP6cs", "forum": "V2TZXGGxgO", "replyto": "V2TZXGGxgO", "signatures": ["ICLR.cc/2026/Conference/Submission13924/Reviewer_ZQRm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13924/Reviewer_ZQRm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993825518, "cdate": 1761993825518, "tmdate": 1762924429897, "mdate": 1762924429897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}