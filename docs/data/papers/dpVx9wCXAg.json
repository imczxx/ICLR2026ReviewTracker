{"id": "dpVx9wCXAg", "number": 304, "cdate": 1756734766514, "mdate": 1763092662713, "content": {"title": "Beyond Textual CoT: Interleaved Text-Image Chains with Deep Confidence Reasoning for Image Editing", "abstract": "Image editing with natural language has gained significant popularity, yet existing methods struggle with intricate object intersections and fine-grained spatial relationships due to the lack of an explicit reasoning process. While Chain-of-Thought (CoT) has been explored to enhance reasoning, purely textual CoT or CoT augmented with coordinate information is fundamentally limited in its ability to represent intricate visual layouts and lacks the necessary visual cues to guide the generation of fine-grained, pixel-level details. To address these challenges, we propose $\\textbf{Mu}$ltimodal $\\textbf{R}$easoning $\\textbf{E}$dit ($\\textbf{MURE}$), a novel framework that $\\textit{shifts the visual editing process from purely text-based reasoning to a series of interleaved textual and visual rationales}$. Our framework performs image editing using a natively multimodal, interleaved text-image CoT. This approach generates a step-by-step chain of reasoning where a textual description is followed by a corresponding visual cue, such as a positional mask that defined intended edited regions or a representation of new content. Furthermore, to mitigate the hallucination phenomenon of large language models, we introduce $\\textbf{M}$ulti$\\textbf{m}$odal $\\textbf{D}$eep $\\textbf{C}$onfidence ($\\textbf{MMDC}$) reasoning paradigm. This paradigm explores a tree of visual reasoning paths at each step. By pruning low-quality branches using a deep confidence score from a reward model, it ensures the model consistently follows a high-quality trajectory towards the final edited result. The proposed method decomposes complex editing tasks into interdependent sub-tasks, achieving greater precision at each stage and yielding high-fidelity edited results. We define the formulation for interleaved text-image chains and release the first CoT-Edit-14K dataset, comprising 14K high-quality editing examples. Extensive experiments show that our method yields significant improvements across three image editing benchmarks, establishing a more effective reasoning framework for visual editing.", "tldr": "", "keywords": ["Image Editing", "Multi-modal Chain-of-Thought", "Deep Confidence Reasoning."], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/23b612e27ffb6008a9c6015bd39ce05b66f42478.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an interleaved CoT formation with confidence-based pruning, along with a new dataset CoT-Edit-14K to fine-tune an existing MLLM to enhance its image editing capability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed CoT-Edit-14K can be useful to fine-tune existing MLLMs for interleaved multi-modal CoT for the image editing task.\n\n2. The proposed interleaved CoT and MMDC achieves state-of-the-art performance across MagicBrush, Emu, and SmartEdit benchmarks."}, "weaknesses": {"value": "1. Lack of failure case analysis. For example, in Fig. 14, the first row doesn't show a **backwards** baseball cap; 4th row, the style of the helicopter doesn't match the original image (ICEdit's result seems better).\n\n2. Lack of training/inference efficiency information. The proposed MMDC requires generating multiple candidates (5 as reported in the paper) and feeding them to VLM to generate reward scores, which may bring a drastic slowdown to the generation process.\n\n3. Unclear reliability of the reward model based on Qwen-2.5 VL-7B. In the sample shown in Fig. 17, the rightmost mask generation adheres to the prompt best (as the editing prompt is to remove the green plant); however, the reward model selects the inferior leftmost path. Also, it is unclear if the proposed inference-time reward-based pruning is better than reward fine-tuning methods such as DPO and GRPO."}, "questions": {"value": "1. According to the Appendix, the training was conducted with a batch size of 1, which is uncommon. Is it the effective batch size or batch size per GPU? Other unclear points: Is it full-parameter fine-tuning or LoRA? How many and what kind of GPUs were used?\n\n2. What is the inference overhead of introducing MMDC? In Figure 4, the line's slope appears to decrease, suggesting a diminishing improvement in scores as the token count increases.\n\n3. The authors used a 0-100 scoring scheme for the reward model, which, according to the qualitative samples shown in Fig. 17, is not reliable enough. How about using a coarser scheme, such as 1-5, or even pairwise comparison?\n\n4. Instead of introducing inference-time MMDC, which introduces extra inference overheads, how about directly fine-tuning the interleaved CoT with DPO / GRPO?\n\n\n[1] Wang, Yibin, et al. \"Unified reward model for multimodal understanding and generation.\" arXiv preprint arXiv:2503.05236 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VD7j9h6Trc", "forum": "dpVx9wCXAg", "replyto": "dpVx9wCXAg", "signatures": ["ICLR.cc/2026/Conference/Submission304/Reviewer_dwQc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission304/Reviewer_dwQc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932919944, "cdate": 1761932919944, "tmdate": 1762915489297, "mdate": 1762915489297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "4ERsmWGyWa", "forum": "dpVx9wCXAg", "replyto": "dpVx9wCXAg", "signatures": ["ICLR.cc/2026/Conference/Submission304/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission304/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763092661882, "cdate": 1763092661882, "tmdate": 1763092661882, "mdate": 1763092661882, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MURE, a unified editing model that replaces purely textual chain-of-thought with an interleaved text–image reasoning chain. Each step alternates between textual reasoning and visual generations (e.g., a mask for the edit region or an image of new content). The authors also add an inference-time selection method, Multimodal Deep Confidence (MMDC), which samples multiple candidates at each visual step and greedily keeps the branch with the highest score from a reward VLM (Qwen2.5-VL). Training uses cross-entropy for text tokens and a rectified-flow MSE for image latents. The authors collected a new dataset, CoT‑Edit‑14K, to support interleaved editing chains."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- [S1] The interleaved text–image paradigm is an intuitive way to ground “where/what to edit,” with explicit masks/new‑object images that are easy to inspect and debug; the pipeline diagram and walkthrough make the idea clear.\n- [S2] Consistent benchmark results with sensible ablations: improvements on MagicBrush/Emu and SmartEdit, plus ablations that separate interleaved CoT and MMDC contributions; search‑width increases yield reasonable gains.\n- [S3] CoT‑Edit‑14K could be a useful resource for step‑wise editing with interleaved chains across 10 edit types; construction details and distributions are provided."}, "weaknesses": {"value": "- [W1] Evaluation is insufficient for the paper’s core claims. There is no human study and no targeted measures of (i) mask correctness (e.g., IoU/precision/recall) and (ii) physical consistency (e.g., shadows/reflections/occlusions). Given the stated motivation, these are significant omissions.\n- [W2] Technical novelty is moderate relative to visual‑reasoning work that already makes the generation/editing process explicit via reasoning steps. GoT [1] formulates textual reasoning (with semantic–spatial guidance) for both generation and editing, and GoT‑R1 [2] extends it with RL; ImageGen‑CoT [3] and T2I‑R1 [4] add textual CoT and scale‑up/RL for text‑to‑image; MM‑R1 [5] introduces cross‑modal CoT for personalized synthesis. MURE’s distinct aspect is producing editing‑specific visual artifacts inside the chain plus a greedy step scorer which is useful engineering, but not an algorithmic novelty.\n- [W3] Related‑work positioning should be clearer. The paper should explicitly contrast what MURE can do (and cannot do) relative to GoT/GoT‑R1, ImageGen‑CoT/T2I‑R1, and MM‑R1, as well as earlier multimodal/visual‑only reasoning (Multimodal‑CoT [6]; CCoT [7]; Visual Planning [8]).\n- [W4] Mixed metrics are not discussed. Emu shows a slight CLIP‑Out drop versus Bagel, and SmartEdit “reasoning” has a small LPIPS regression; these deserve analysis of trade‑offs (Tables 1–2, p.7).\n- [W5] Failure cases are not demonstrated. Showing failure cases can help analyze what is missing and motivate future research.\n\n\n[1] Fang et al., “GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing,” arXiv, 2025.  \n[2] Duan et al., “GoT‑R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning,” arXiv, 2025.  \n[3] Liao et al., “ImageGen‑CoT: Enhancing Text‑to‑Image In‑Context Learning with Chain‑of‑Thought Reasoning,” arXiv, 2025.  \n[4] Jiang et al., “T2I‑R1: Reinforcing Image Generation with Collaborative Semantic‑level and Token‑level CoT,” arXiv, 2025.  \n[5] Liang et al., “MM‑R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation,” arXiv, 2025.  \n[6] Zhang et al., “Multimodal Chain‑of‑Thought Reasoning in Language Models,” arXiv, 2023.  \n[7] Mitra et al., “Compositional Chain‑of‑Thought Prompting for Large Multimodal Models,” CVPR, 2024.  \n[8] Xu et al., “Visual Planning: Let’s Think Only with Images,” arXiv, 2025."}, "questions": {"value": "1) What is the latency/compute overhead of MMDC as search width increases? Any sensitivity to the reward prompt/model, and have you tried non‑greedy (e.g., beam/global) selection?  \n2) For edit types that currently omit visual steps, does forcing masks or object images help or hurt? An ablation (mask‑only / object‑only / both) would clarify."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "High‑fidelity edits raise misuse risks (misinformation/impersonation). The paper notes general concerns but does not discuss watermarking/provenance or release safeguards."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jRjsZbq3zN", "forum": "dpVx9wCXAg", "replyto": "dpVx9wCXAg", "signatures": ["ICLR.cc/2026/Conference/Submission304/Reviewer_HVFG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission304/Reviewer_HVFG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762041939305, "cdate": 1762041939305, "tmdate": 1762915489069, "mdate": 1762915489069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MURE, a novel framework for image editing that shifts from purely text-based Chain-of-Thought (CoT) reasoning to interleaved text-image CoT sequences. The key innovation is decomposing complex editing tasks into a series of sub-tasks, where each textual reasoning step is paired with corresponding visual outputs such as segmentation masks or content representations. The paper also introduces a CoT-Edit dataset with significant improvements on three benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of interleaved CoT for image editing, along with the MMDC reasoning paradigm, is intuitive and interesting.\n- The proposed approach is effectively demonstrated through experiments.\n- The presentation is clear and easy to follow."}, "weaknesses": {"value": "- The latency of the approach could be a significant concern, since the model is required to generate intermediate images. In other words, the number of output tokens per sample (in Figure 4) would be much higher than in non-CoT or textual CoT frameworks. I don’t think the approach or paradigm is scalable, especially when we extend it to longer search trajectories.\n- The proposed approach is limited to object-oriented image editing, as it heavily relies on extracting objects in the approach and the dataset construction process. It could not be applied to other types of editing, e.g., changing the background, or changing other visual features, such as color or shape.\n- From Table 5 which compares textual CoT and interleaved CoT, the improvement in the interleaved CoT is quite marginal."}, "questions": {"value": "- Is it possible to extend the framework to more types of editing, such as changing the background?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vrPDkXfHQb", "forum": "dpVx9wCXAg", "replyto": "dpVx9wCXAg", "signatures": ["ICLR.cc/2026/Conference/Submission304/Reviewer_kLpd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission304/Reviewer_kLpd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762069227883, "cdate": 1762069227883, "tmdate": 1762915488764, "mdate": 1762915488764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors proposed a new approach that enahnces the purely textual reasoning to interleaved text–image reasoning chains that alternate between textual reasoning steps and visual cues (e.g., masks, synthesized intermediate content)\n\nAdditionally, it presents Multimodal Deep Confidence (MMDC) — a reward-model–driven pruning mechanism that evaluates multiple visual reasoning paths, selects high-confidence branches, and mitigates hallucinations.\n\nThe authors also construct CoT-Edit-14K, a dataset with 14 K high-quality interleaved text–image CoT examples, covering 10 editing subtasks (e.g., object replacement, removal, color change)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The high-quality dataset (CoT-Edit-14K) would be a contribution to the image editing community.\n\n2. The evaluation and solid performance are comprehensive. Demonstrates consistent gains across CoT-Edit-14K, MagicBrush and Emu benchmarks on multiple metrics (CLIP, DINO, PSNR, SSIM, LPIPS). Ablations isolate contributions of interleaved CoT and MMDC convincingly."}, "weaknesses": {"value": "1. The MURE framework works very well for object swapping or adding-- it did text reasoning, mask predition and new object generation. However, there might be some concerns when conducting other editing types, especialy for eidting exisiting objects. For example, changing the object location, size or shape. Such method might not be able to keep the identity consistent when generation the new object."}, "questions": {"value": "It would be great for the reviewer to understand the strength or weakness when the authors report the performance of MURE in different editing types."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CIvEeVBJf3", "forum": "dpVx9wCXAg", "replyto": "dpVx9wCXAg", "signatures": ["ICLR.cc/2026/Conference/Submission304/Reviewer_8LDb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission304/Reviewer_8LDb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762151893058, "cdate": 1762151893058, "tmdate": 1762915488591, "mdate": 1762915488591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}