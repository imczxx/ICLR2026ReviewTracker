{"id": "eCwzgXYYvK", "number": 7447, "cdate": 1758022542777, "mdate": 1759897852432, "content": {"title": "PGI: Pareto-Guided Gradient Interference for Generalized Category Discovery", "abstract": "Generalized Category Discovery (GCD) is a task that concentrates on identifying both base and novel categories in an unlabeled dataset while preserving knowledge from a labeled dataset. A key challenge in this setting lies in balancing the supervised learning of base categories and the unsupervised discovery of novel ones. Through empirical analysis, we observe that conventional multi-objective optimization approaches suffer from significant gradient interference between the classification objective and the representation objective, which hinders effective joint training. Therefore, we propose a simple yet effective framework, named $\\textbf{P}$areto-guided $\\textbf{G}$radient $\\textbf{I}$nterference ($\\textbf{PGI}$), to alleviate this issue. The PGI employs a Pareto-annealing optimization approach to explore the Pareto front that balances representation objective and classification objective. Additionally, a regularization term is introduced which can leverage multi-view consistency to enhance clustering structure in the feature space, facilitating better separation of novel classes. Extensive experiments across fine-grained benchmarks demonstrate the superiority of our approach in discovering novel categories while maintaining accuracy on base classes.", "tldr": "", "keywords": ["Generalized Discovery Category", "Computer Vision", "Image Classification"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d8baa39df5a489ce845333ae1f3b58b4cbedeef4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the task of Generalized Category Discovery (GCD). The issue lies in the gradient interference between the classification objective and the representation objective. To solve this problem, this work proposes a simple yet effective framework, named Pareto-guided Gradient Interference (PGI). Besides, the paper further proposes a regularization term using multi-view consistency in SwAV to ensure better representations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well-motivated and easy to follow.\n2. The proposed method PGI, serves as a plug-and-play component to enhance existing GCD methods consistently.\n3. A thorough theoretical analysis has been presented in the appendix to validate the validity of PGI."}, "weaknesses": {"value": "1. The novelty of this paper is somewhat limited. In the multi-view consistency regularization, the paper directly adopts the swap prediction loss in SwAV. The method seems to just combine existing algorithms in other domains with GCD methods.\n2. There are some typos and flaws in the writing and presentation. For example, Appendix A.3 in line 269, line 281 should be correctly referenced. Some typos include L_sup and L_u in Line 151, as well as the method name $\\mu$GCD and ProtoGCD in Table 2. Each method in Table 2 should also be cited. The overall writing should be refined.\n3. The effect of the hyper-parameters in Eq. 3 should be explored, including $\\alpha_{min}, \\alpha_{max}$ and T.\n4. Why, and to what extent, does gradient intervention affect the performance of GCD? Further in-depth experiments should be conducted."}, "questions": {"value": "Please see weakness. The motivation and influence of the gradient interference on the final performance should be further presented."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wqJBsmUB6i", "forum": "eCwzgXYYvK", "replyto": "eCwzgXYYvK", "signatures": ["ICLR.cc/2026/Conference/Submission7447/Reviewer_t7ab"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7447/Reviewer_t7ab"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623892630, "cdate": 1761623892630, "tmdate": 1762919561419, "mdate": 1762919561419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a Pareto annealing optimization method that adaptively fuses the gradient directions of the classification and representation objectives to achieve Pareto-optimal updates. In addition, a SwAV loss is introduced to enhance the model's sensitivity to fine-grained features. Experimental results demonstrate that the proposed method outperforms existing approaches in most cases."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe motivation of the paper is clear, and the content is easy to follow.\n2.\tThe idea of fusing gradient directions from different objectives is interesting.\n3.\tThe experimental results show that the proposed plug-and-play technique is effective across different methods."}, "weaknesses": {"value": "1.\tThe ablation study is insufficient, as it is conducted on only one dataset and one method.\n2.\tThe generalization experiments involving different benchmark methods are limited, and more validation is needed.\n3.\tThe performance of MOS and GET is significantly lower than the results reported in their original papers. The reason for this discrepancy remains unclear.\n4.\tThe authors are suggested to further justify the presence of gradient inference with additional experiments, which would help support the rationale behind the motivation."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ROYNcrNmFU", "forum": "eCwzgXYYvK", "replyto": "eCwzgXYYvK", "signatures": ["ICLR.cc/2026/Conference/Submission7447/Reviewer_P9on"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7447/Reviewer_P9on"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956352472, "cdate": 1761956352472, "tmdate": 1762919560585, "mdate": 1762919560585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets generalized category discovery (GCD) and claims that one underexplored reason for sub-optimal performance on fine-grained datasets is gradient interference between (i) a classification objective and (ii) a representation objective. To alleviate this, the authors propose PGI (Pareto-Guided Gradient Interference), which\n\n1. combines the two gradients as a convex combination with a cosine-style annealed preference, and\n2. adds a SwAV-style multi-view consistency regularization to “refine” the representation side.\n\nThey show consistent but small improvements on CUB, Stanford Cars, FGVC-Aircraft, and Herbarium19, and they report that the average gradient angle between the two objectives is slightly reduced when using PGI."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. **Problem framing is reasonable.** The paper clearly separates GCD training signals into a classification part and a representation part, and uses gradient-angle statistics to show these two can conflict, especially on fine-grained datasets. This is an intuitive diagnostic for GCD. \n2. **Method is simple and pluggable.** The proposed PGI step is just a convex combination of two gradients with a smooth schedule; it can be inserted into existing GCD pipelines with minimal code changes. \n3. **Results are consistent across several fine-grained benchmarks.** Even though the gains are modest, they appear on CUB, Cars, Aircraft, and Herbarium19, which suggests the idea is not overfitted to one dataset."}, "weaknesses": {"value": "1. **“Pareto” is overstated.** The actual update is a hand-designed convex combination with a cosine schedule. This is not what the  PCGrad[1] / CAGrad[2] / MGDA[3] literature calls “exploring the Pareto front”: there is no per-step solving of a Multi-Objective Optimization subproblem, no projection, no dominance test, no preference elicitation, no actual front visualization. It is simply a *hand-scheduled loss-weighting in gradient space*. The paper cites MGDA and says “PGI is capable of achieving Pareto-optimal solutions” but does not show that any iterate is on or near the front. That is an **over-interpretation of existing theory**. In Sec. 4.1 the coefficient is even fixed ($ \\alpha = 0.35$) without specifying $\\alpha_{min}$ and $\\alpha_{max}$ . This effectively collapses the annealing scheme to a fixed weight, so the claimed “Pareto annealing” is not actually evaluated. Please clarify the actual values of $\\alpha_{min}$ and $\\alpha_{max}$, or report results with the intended schedule.\n\n2. **Motivation is under-validated.** The paper claims gradient interference is a key reason for suboptimal GCD, but the measured angle reductions are small in Table 4 and there is no comparison to standard anti-conflict baselines (PCGrad[1], CAGrad[2], GradNorm[4]), so we cannot tell if PGI is the best way to address it. \n\n3. **Two levels of weighting are used but not justified.**\nThe paper first defines a gradient-space combination with $\\alpha$ (Eq. 2), then defines a loss-space combination with $\\lambda_{\\text{cls}}$ (Eq. 13):\n  * gradient step: $\\widetilde{G} = \\alpha \\nabla L_s + (1 - \\alpha)\\nabla L_u$\n  * loss itself: $L_{\\text{overall}} = \\lambda_{\\text{cls}} L_s + L_u$\n\nThis immediately raises the question: which quantity actually controls the trade-off --- $\\alpha$ or $\\lambda_{\\text{cls}}$ --- and why are both needed? If one simply backpropagates $L_{\\text{overall}}$, the resulting gradient is already a fixed convex combination of the two components. Adding an additional convex combination on top (PGI) appears redundant unless the authors explain why loss-level weighting alone cannot produce the observed angle changes. This omission makes the method look more like extra manual tuning than a principled optimizer.\n\n4. **SwAV is weakly tied to the main narrative.** The paper sells PGI as “reducing gradient interference,” but when SwAV is added, it is not shown to reduce gradient angles or resolve conflicts; it functions more like a stability bonus than a component derived from the proposed interference analysis.\n\n5. **Gains are within typical run-to-run noise.** Many improvements are ≈0.5–1.0% and some (e.g., Herbarium19) are 0.2%. Without multi-seed results and fully unified implementations of baselines, it is hard to attribute the gains purely to PGI. \n\n6. **Theory is mostly a restatement.** Theorems essentially say “a convex combination of two conflicting gradients can still be a descent direction,” which is standard in MTL, but they do not explain why the specific cosine schedule is necessary or optimal. \n\n7. **No hyperparameter analysis.** The paper does not study sensitivity to $\\alpha_{\\min}, \\alpha_{\\max}, T$ or to $\\lambda_{\\text{cls}}$. Since the method’s effect is precisely to rebalance two conflicting objectives, the lack of such analysis makes it unclear whether the reported gains are robust or just the result of a good setting on these datasets.\n\n[1] Gradient Surgery for Multi-Task Learning\n\n[2] Conflict-Averse Gradient Descent for Multi-Task Learning\n\n[3] Multiple-gradient descent algorithm (mgda) for multiobjective optimization\n\n[4] GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks"}, "questions": {"value": "1. **Did all reported experiments actually use the cosine schedule for $\\alpha$, or was $\\alpha$ fixed to 0.35 as in Sec. 4.1?** Please clarify the exact setting used for Tables 2–6. \n2. **Why not compare to PCGrad[1] / CAGrad[2] / GradNorm[3]?** These are the closest baselines for “gradient interference,” and without them it is difficult to judge the contribution. \n3. **Can you report gradient-angle statistics *after* adding SwAV?** Right now SwAV is justified as “further reducing interference,” but no evidence is shown. \n4. **How sensitive is PGI to $\\alpha_{\\min}, \\alpha_{\\max}, T, \\lambda_{\\text{cls}}$?** If the method works only in a narrow range, then the “plug-and-play” claim is weaker.\n5. **Please report mean ± std over ≥3 runs for all main results.** This is important because many gains are within 0.5–1.0%.\n\n[1] Gradient Surgery for Multi-Task Learning\n\n[2] Conflict-Averse Gradient Descent for Multi-Task Learning\n\n[3] GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yqhe8e6Zdd", "forum": "eCwzgXYYvK", "replyto": "eCwzgXYYvK", "signatures": ["ICLR.cc/2026/Conference/Submission7447/Reviewer_Fzy6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7447/Reviewer_Fzy6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985611032, "cdate": 1761985611032, "tmdate": 1762919559602, "mdate": 1762919559602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a learning method for fine-grained generalized category discovery (GCD) that introduces two main components: 1) a Pareto-annealing optimization strategy designed to balance the objectives of representation learning and classification by mitigating gradient interference, and  2) a multi-view consistency regularization based on the SwAV loss to enhance feature alignment. The method is evaluated on four fine-grained benchmarks, with comparisons to multiple existing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of employing a Pareto-annealing optimization scheme to reduce gradient interference between learning objectives is interesting and potentially relevant to the GCD setting.\n\n- The experimental results on fine-grained GCD benchmarks show consistent, albeit modest, improvements over baseline methods."}, "weaknesses": {"value": "- Unclear motivation and formulation. The rationale for decomposing the GCD objective into separate representation learning and classification components, and for treating their gradient interference as the main issue, is not well justified. There are multiple valid ways to structure the GCD loss, and it is not evident that gradient interference is a major source of overfitting. Moreover, the claim that this phenomenon “explains overfitting” (line 053) is not sufficiently supported. In practice, gradients from these objectives may provide complementary rather than conflicting information.\n\n- Limited conceptual contribution. The proposed method combines two existing ideas—Pareto-annealing optimization and SwAV-based regularization—without a clear conceptual or theoretical connection between them. The integration appears somewhat ad hoc, and the contribution is incremental relative to prior literature. Additionally, the focus on fine-grained GCD narrows the method’s general applicability.\n\n- Weak motivation for the annealing strategy. The derivation and intuition behind the annealing formulation in Equation (3) are insufficiently explained. Theoretical analysis of the proposed Pareto Gradient Interference (PGI) framework appears generic and lacks direct linkage to the specific challenges of GCD.\n\n- Insufficient experimental validation. 1) The performance gains reported in Tables 2 and 3 are marginal relative to the baselines. 2) The ablation results (Table 5) show that the contribution of the SwAV loss is inconsistent, raising questions about its necessity. 3) The method is only evaluated on fine-grained datasets, without results on coarse-grained GCD benchmarks, which limits the generality of the claims."}, "questions": {"value": "- Can the authors clarify why gradient interference is particularly problematic in GCD, and how it theoretically or empirically leads to overfitting?\n\n- What is the underlying motivation or derivation behind the annealing function in Equation (3)?\n\n- Could the authors provide additional results on coarse-grained GCD datasets to demonstrate broader applicability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TafMG2WYRE", "forum": "eCwzgXYYvK", "replyto": "eCwzgXYYvK", "signatures": ["ICLR.cc/2026/Conference/Submission7447/Reviewer_xLLx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7447/Reviewer_xLLx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762090400723, "cdate": 1762090400723, "tmdate": 1762919559044, "mdate": 1762919559044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}