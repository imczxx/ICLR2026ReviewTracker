{"id": "sJxBWDc8SM", "number": 24729, "cdate": 1758359765373, "mdate": 1759896752202, "content": {"title": "Revisiting Associative Recall in Modern Recurrent Models", "abstract": "Modern recurrent deep learning models -- such as state-space models (SSMs) -- have emerged as a promising computationally efficient alternative to Transformers for sequence modeling. However, how their practical differences in learnability and optimization impact core capabilities remains underexplored. In this paper, we thoroughly compare SSM and Transformer learning dynamics on two fundamental benchmarks highly correlated with language modeling performance: associative recall and copying. We find that, while Transformers are robust to optimization hyperparameters, the performance of modern recurrent models suffers from critical instabilities: success is confined to an extremely narrow window of learning rates, outside of which accuracy drastically drops. This issue can confound performance evaluations and expressivity conclusions, revealing a fundamental mismatch in the loss landscape of modern recurrent models compared to Transformers. We demonstrate that this brittle optimization has a direct impact on scaling, causing SSMs to favor width over depth. Indeed, we also find that, while the 1-layer Transformer's performance on recall does not exceed random guessing, well-tuned Mamba and other SSMs can learn to recall with one layer, yet with dynamics that do not resemble the formation of induction heads. Taken together, our findings suggest that a crucial differentiator between these architectures lies not just in their expressivity but in their fundamental learnability properties, pointing to optimization stability as a key challenge for the future of SSMs.", "tldr": "", "keywords": ["SSMs", "Attention", "In-Context Learning", "Language Modeling", "Mamba"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f293510ff8dbd73726dddae46a5a1894566098de.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work provides a study that focuses on the Associative Recall (AR) task, which requires the model to dynamically infer the correct value associated with a given key from a sequence provided as context. The authors conduct a number of different analyses on models such as transformers and state space models, showing that different hyper-parameter choices can affect different models in very different manners. For example, learning rate and model width have surprisingly different effects. Overall the results of this work provides evidence that such a task is very sensitive to to hyper-parameter choices and therefore more careful consideration needs to be placed onto using it as a standardized benchmark for language models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The authors produce a comprehensive empirical study on the associative recall task and reveal some potentially interesting features that could be useful to consider when designing models to work on this task."}, "weaknesses": {"value": "My main concern is the lack of direct explanation that accompanies the results that are provided by the authors; while it is appreciated that there are findings that are more interesting and not well explored in literature (example the number of layers required by some linear recurrent models to learn associative recall tasks), there's ultimately no real explanation for why this is being observed. As a result, the results can appear to be ablating on a number of different variables while lacking a main finding that summarizes the effort as a whole.\n\nFurthermore, some of the results the authors provide such as on the copying task or on the induction heads task with 1-layer are not significantly novel; perhaps it is a positioning issue that requires re-writing but in its current form it does appear to be simply ablating on prior work without a large amount of novel findings."}, "questions": {"value": "See above; it would be appreciated if the authors could provide a more comprehensive theory as to what is being learned by the model and why the different observations occur. Given the synthetic nature of the task, I believe that the authors should focus more effort on this front."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fxKQ08nFRW", "forum": "sJxBWDc8SM", "replyto": "sJxBWDc8SM", "signatures": ["ICLR.cc/2026/Conference/Submission24729/Reviewer_3xjD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24729/Reviewer_3xjD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24729/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760451129726, "cdate": 1760451129726, "tmdate": 1762943177878, "mdate": 1762943177878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper conducts an empirical study on SSMs and Transformers in terms of their ability to solve associative recall and copying. For both tasks, the authors find that Transformers are more robust whereas SSMs are more sensitive to the choice of hyperparameters. Focused on the associative recall task (MQAR), the authors empirically show that Transformer benefits from scaling depth whereas SSMs benefit from scaling width. Consequently, on single-layer models, Transformer fails to solve MQAR whereas SSMs succeed, with Mamba being more efficient than Hyena. The author provide ablations on the architectural components in Mamba and Transformers, showing that removing convolution or gating in Mamba still retain its performance on MQAR."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Understanding the optimization difference between SSMs and Transformers is an important and timely research direction.\n\n2. The authors conduct extensive experiments, with a fine-grained grid search over learning rate hyperparameters."}, "weaknesses": {"value": "1. Although the paper presents empirical evidence on the optimization difference between SSMs and Transformers, it lacks theoretical analysis or discussions to explain such observations. Some theoretical grounding can also be found in [1] [2] (which give concrete constructions on 2-layer Transformers and 1-layer Mamba solving associative recall, respectively).\n\n2. Some claims are not well supported, such as the optimization difference in SSM and transformers (see Question 1) and the benefit of width in SSM (see Question 2).\n\nReferences:\n\n[1] Bietti et al. \"Birth of a transformer: A memory viewpoint.\" NeurIPS 2023.\n\n[2] Huang et al. \"Understanding Input Selectivity in Mamba: Impact on Approximation Power, Memorization, and Associative Recall Capacity.\" ICML 2025."}, "questions": {"value": "1. Transformers interleave attention mixer layer and (pointwise) MLP layer, whereas Mamba is a stack of Mamba block containing a (short) convolution layer, a SSM mixer layer, and a gating branch. Can the optimization difference, specifically the learning rate sensitivity of Mamba, arise from its additional degrees of freedom (e.g., the choice of convolution kernel size, the state matrix, and gating branch)? To strengthen the claims, can the authors fix similar backbone (say using Mamba block), and study the learning rate sensitivity of a softmax attention layer versus the original Mamba S6 layer?\n\n2. In Sec 4.1, the authors claim that recurrent models always benefit from width (line 312). However, from Fig.3 (sequence length 256, KV pairs 16), we see that Mamba with large width (512, 2048) works less well than smaller width (64-256). Moreover, when datasets contain noise, large-capacity models may overfit, resulting in the bias-variance tradeoff. Can the authors clarify or/and suitably modify their claim?\n\n3. Sec 7. The role of convolution in 1-layer Mamba for solving MQAR. The authors find that removing the convolution in 1-layer Mamba does not affect its ability to solve MQAR. I find this result surprising, contradicting the empirical findings in [3] where they showed that convolution kernel in Mamba with length $\\geq 2$ is necessary for 1-layer Mamba to solve MQAR (Fig 6). Can the authors explain possible solution mechanisms and/or provide evidence? Can the authors examine how robust is the Mamba without convolution for solving MQAR when increasing the sequence length, key-value pairs, etc?\n\nReferences\n\n[3] Arora et al. \"Mechanistic evaluation of Transformers and state space models.\" arXiv:2505.15105 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fEeNIhiqlW", "forum": "sJxBWDc8SM", "replyto": "sJxBWDc8SM", "signatures": ["ICLR.cc/2026/Conference/Submission24729/Reviewer_XvGh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24729/Reviewer_XvGh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24729/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942299769, "cdate": 1761942299769, "tmdate": 1762943177677, "mdate": 1762943177677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper empirically compares state-space models (SSMs) and Transformers on associative recall (MQAR) and copying. It finds SSMs succeed only within a narrow learning-rate window, whereas Transformers are robust across a wide range; this optimization brittleness can confound expressivity comparisons. It further shows opposite scaling preferences—SSMs benefit from width, Transformers from depth—and reports *1-layer* training dynamics (e.g., loss “bumps” reminiscent of induction circuits) that differ across families."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "### Originality\n\n* Re-frames SSM vs. Transformer comparisons through learnability: finds critical LR-sensitivity for modern SSMs on MQAR/copying, contrasting with robust Transformers.  \n\n### Quality\n\n* Executes large LR sweeps and reports results (3 seeds) that expose narrow “goldilocks” regions for Mamba/Hyena, and wide basins for attention.  \n* Provides scaling experiments showing width helps SSMs while depth helps Transformers; includes ablations (conv on Q/K/V; gating) that isolate drivers.   \n\n### Clarity\n\n* Figures and captions explicitly note seed counts and LR-sweep setup.\n\n### Significance\n\n* Highlights **optimization stability** as the key differentiator; argues scaling along each architecture’s “preferred axis” (width vs. depth) is crucial for fair comparisons."}, "weaknesses": {"value": "1. **Most results center on 1–2-layer models on synthetic tasks**. It’s unclear if the LR brittleness persists for deeper stacks (e.g., 12–24 layers) or on standard LM tasks. Adding at least one real-world benchmark (e.g., small LM perplexity) would be better. \n\n2. Many plots average over 3 seeds; for **stability** claims, 3 is thin. \n\n3. Modern recurrent models encompass many SSM (and non-SSM) variants; so the paper should name explicit implementations used in each figure (e.g., Mamba, Hyena, RWKV, H3) in-line, and clarify the Based attention baseline."}, "questions": {"value": "1. Can you replicate Fig. 3 on a 12-layer Mamba-style stack and report the LR-window width vs. depth? This would test whether brittleness is primarily a shallow-model artifact. \n\n2. For Fig. 1–4/7, please re-run with ≥5 seeds and plot CIs; this will calibrate how “narrow” the SSM LR window is statistically."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8nyEK5nRMa", "forum": "sJxBWDc8SM", "replyto": "sJxBWDc8SM", "signatures": ["ICLR.cc/2026/Conference/Submission24729/Reviewer_1bHM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24729/Reviewer_1bHM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24729/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951026903, "cdate": 1761951026903, "tmdate": 1762943177494, "mdate": 1762943177494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes the differences between the transformer model architecture and the state space model (SSM), one type of recurrent model architecture, in associated recall and copy tasks. The paper finds that compared to the transformer model, the SSM suffers from severe optimization instability, and training with different learning rates is crucial for the performance evaluation of the SSM. The paper also reveals the distinct scaling behaviors and induction head formation mechanisms of the two architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper demonstrates the differences between SSM and transformer architectures from the perspective of model optimization and stability, providing a new dimension for understanding and analyzing SSM models.\n2. The experimental analysis is very interesting, especially in exploring in-context recall-intensive tasks that are a key focus of these linear architectures, which may bring important insights to the development of the RNN community.\n3. The paper is well-written, clearly structured, and easy to understand."}, "weaknesses": {"value": "1. The paper doesn't adequately explain why even a single-layer Mamba without conv1d can still perform well on MQAR. It would be better to analyze the formation mechanism of induction heads in single-layer recurrent models.\n2. The paper presents the various differences between the transformer and SSM in a fragmented manner, seemingly lacking a unified analytical explanation. Is there a connection between the optimization instability, width/depth scaling behavior, and induction heads phenomenon mentioned in the paper?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5uAMKbUOZL", "forum": "sJxBWDc8SM", "replyto": "sJxBWDc8SM", "signatures": ["ICLR.cc/2026/Conference/Submission24729/Reviewer_p7X8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24729/Reviewer_p7X8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24729/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992653541, "cdate": 1761992653541, "tmdate": 1762943177291, "mdate": 1762943177291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}