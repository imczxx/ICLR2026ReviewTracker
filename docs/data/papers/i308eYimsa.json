{"id": "i308eYimsa", "number": 19813, "cdate": 1758299636520, "mdate": 1759897017740, "content": {"title": "Active Model Selection for Large Language Models", "abstract": "We introduce LLM SELECTOR, the first framework for active model selection of Large Language Models (LLMs). Unlike prior evaluation and benchmarking approaches that rely on fully annotated datasets, LLM SELECTOR efficiently identifies the best LLM with limited annotations. In particular, for any given task, LLM SELECTOR adaptively selects a small set of queries to annotate that are most informative about the best model for the task. To further reduce annotation cost, we leverage a judge-based oracle annotation model. Through extensive experiments on 6 benchmarks with 151 LLMs, we show that LLM SELECTOR reduces annotation costs by up to 59.62% when selecting the best and near-best LLM for the task.", "tldr": "", "keywords": ["LLM evaluation", "active model selection", "information gathering", "LLM-as-a-Judge"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab68bb183c270a0b8f9983af8d11a3b024d4334a.pdf", "supplementary_material": "/attachment/bb5940703a6ea23124a2899b0be0e8e59e056fd7.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a LLM selection approach, named LLM Selector, that aims to identify the best LLM for a set of queries out of a pool of LLM candidates. LLM Selector identifies this best LLM through annotations on a subset of queries and comparison of LLM candidates over a baseline LLM (instead of pairwise) on this subset."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The objective is clearly presented.  \n- The experiment is comprehensive with 6 benchmarks and 151 LLM candidates. The proposed method claims to have superior results than baselines."}, "weaknesses": {"value": "1. The paper makes several implicit and unjustified assumptions.  \n2. Figure 1 is unclear.  \n3. Lines 075–081 claim the paper compares all LLMs against a baseline instead of using pairwise comparisons. This assumes that for two models a and b, WR_Q(a, f) > WR_Q(b, f) implies a > b, which is not justified.  \n4. Line 158 mentions the variable F but does not define it.  \n5. It is unclear how the equation in lines 250–254 is derived from Equation 2.  \n6. The intuition behind the use of weak judges is unclear. If weak judges are proxies to evaluate the usefulness of a given query q on each candidate’s response—so that the expensive oracle judge doesn’t need to be called—then the paper should explain and justify how well the weak judges approximate the oracle. This is especially important since the weak judges are k-gram language models, which are likely to differ significantly from the oracle LLM judge (e.g., GPT-4).  \n7. It’s unclear what the weak judges output. If each judge ranks LLMs and identifies which is better, then the meaning of “We use r_k to represent the noisy annotation made by weak judge k” in line 249 is unclear.  \n8. Algorithm 1 has many errors: what is j in line 282? What is z in line 284? These are undefined."}, "questions": {"value": "1. Line 050 says \"... which examples should be annotated in order to ...\". Does “examples” here mean “queries”? Does “annotated” mean “responded”?  \n2. Why does LLM Selector perform worse on AlpacaEval and Bingo than other baselines when b is small?  \n3. Does the paper include the ground truth query subset, which could be obtained via brute force by evaluating all C(|Q|, b) query subsets across all LLM candidates to identify the subset that selects the correct LLM? The correct LLM could be determined by evaluating all queries in Q and selecting the best-performing model.\n4. Are there existing work / papers that can serve as baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YRBLCvfskx", "forum": "i308eYimsa", "replyto": "i308eYimsa", "signatures": ["ICLR.cc/2026/Conference/Submission19813/Reviewer_LzUR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19813/Reviewer_LzUR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761366773465, "cdate": 1761366773465, "tmdate": 1762931669196, "mdate": 1762931669196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of picking the best LLMs for a given test set within a limited annotation budget. It introduces a framework named LLM selector, which formulates the model selection as a mutual information maximization problem. The approach sequentially selects the most informative prompts to annotate using an information-gain criterion. The paper performs experiments on 6 benchmarks with 151 LLMs, and the results demonstrate the annotation cost can reduce up to 59.6% while maintain competitive winning model identification accuracy."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles a well motivated problem given the rapid proliferation of LLMs, especially given the computational cost required to evaluating LLM is also increasing rapidly. The paper does a good job on formulate the problem and present it in a clear fashion. A few strengths for the paper:\n\n1. The framework proposed is principled. The formulation uses mutual information and sequential information maximization, which provides a reasonably well theoretical ground to the problem.\n\n2. The method proposed in the paper is easily applicable in the real world, due to: 1/ the design is model agnostic, thus it doesn't require access to the model internals (e.g., log likelihood for each tokens). It works well for API based models; 2/ It uses a modified version of the pairwise preference judgement (reducing the complexity from O(m^2) to O(m) by using a baseline model), which significantly reduced the cost to annotate even a single data point. \n\n3. The experiments show promising results. The proposed method show significantly performance gain (i.e., less examples required) on most of the benchmarks, and the method delivers consistent performance comparing to other baselines."}, "weaknesses": {"value": "1. The experiments setup are not comprehensive, and it missing important ablation studies. I appreciate the authors perform experiments on many datasets and LLMs, however, I think a few important design choices are not justified and the impact on the method is not studied: \n    1/ The design of the weak judge is very simple (n-gram based), but it works surprisingly well. How will the choose of weak judge impact the performance of the proposed method? What is the rationale behind this choice? How will this work on different type of tasks (e.g., math tasks)? This will help understand the limitation of the method and shed light on choosing the weak judge for real world application.\n    2/ The two parameter model assumption is very simple (but it works), which contradicts the premise that different queries will have varying difficulty levels thus varies the model performance. Ablation is required to understand the impact of this assumption, e.g., how sensitive the model performance is to different initialization, justification/analysis for this choice etc. \n\n2. Lack of analysis for the existing experiments. For example, Bradley-Terry is the only adaptive baseline and it performs well on a few datasets, why does it work well in that case but overall unstable? How does the proposed method mitigate that issue? On Bingo and MediQA, the method seems to perform comparably well with confidence and uncertainty based methods. Why did these two baseline works well and how does the proposed method compare with them? \n\nOverall I think the paper makes some simple assumption on the modeling side but show very competitive results. This proves the effectiveness of the method. However, it works a bit \"surprisingly\" well, I think we certainly need to do more analysis to unfold the magic there to help the community understands why and how does the method works."}, "questions": {"value": "1. I think our method heavily relies on the quality of the weak judge. The parameter selection logic for ε_loss and ε_draw purely depends on the weak judge, as well as the sample selection. Do we have any study on how the weak judge can impact the final model performance? Any justification why we use a n-gram based model as the judge?\n\n2. How do you justify the two-parameter model  when it assumes uniform behavior across queries of varying difficulty? Have you considered query-dependent parameters or hierarchical models?\n\n3. I noticed on some datasets the Bradley-Terry performs comparably with LLM SELECTOR. Do we have insight why in these cases Bradley-Terry works well? And under what condition our proposed method will work well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ur3KD07t0X", "forum": "i308eYimsa", "replyto": "i308eYimsa", "signatures": ["ICLR.cc/2026/Conference/Submission19813/Reviewer_wYka"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19813/Reviewer_wYka"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953100291, "cdate": 1761953100291, "tmdate": 1762931668639, "mdate": 1762931668639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LLM SELECTOR, an active query selection framework to identify the best LLM for a target task using few annotated comparisons. It adaptively picks queries that maximize a mutual-information style criterion with respect to a chosen baseline model, and it reduces human labeling by relying on a judge-based oracle. Experiments cover various benchmarks and models, reporting reductions in annotation cost in identifying the best model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper focuses on a practical and important goal of fast best-model identification under limited labels.\n\n2. The components (baseline anchoring, entropy/MI scoring, weak judges) are easy to implement and scale, and the authors evaluate broadly across tasks and model families."}, "weaknesses": {"value": "1. **Weak-judge design is fragile:** The “weak judge” uses k-gram likelihood signals to approximate informativeness. Perplexity/likelihood and simple k-gram statistics often correlate poorly with human judgments and can exhibit spurious length/overlap effects, which may bias query selection.\n\n2. **Baseline-anchored selection can misidentify the global best:** Because all comparisons are made against a single baseline (a star graph), the model with the highest win-rate versus the baseline need not be the Condorcet or Borda winner overall, especially under intransitive or near-tied preferences. This is a known pitfall in dueling-bandit settings and active ranking when the comparison graph lacks diversity [1]. \n\n3. **Missing guarantees for the greedy objective:** The method uses an entropy/MI-style greedy policy. Greedy near-optimality holds under adaptive submodularity for certain objectives, but the paper does not show that its objective satisfies these conditions, nor does it provide sample-complexity bounds for identification.\n\n[1] Jamieson, Kevin G., and Robert Nowak. \"Active ranking using pairwise comparisons.\" Advances in neural information processing systems 24 (2011)."}, "questions": {"value": "The active learning part of the method is query selection, and model choice is produced after annotation. Could the current title \"active model selection\" be a bit of misleading?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JKkagNDp1h", "forum": "i308eYimsa", "replyto": "i308eYimsa", "signatures": ["ICLR.cc/2026/Conference/Submission19813/Reviewer_NpL6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19813/Reviewer_NpL6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19813/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988616939, "cdate": 1761988616939, "tmdate": 1762931667888, "mdate": 1762931667888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}