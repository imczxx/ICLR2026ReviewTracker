{"id": "mEw4lhAn0F", "number": 23059, "cdate": 1758338987966, "mdate": 1763732487959, "content": {"title": "OmniMouse: Scaling properties of multi-modal, multi-task Brain Models on 150B Neural Tokens", "abstract": "Scaling data and artificial neural networks has transformed AI, driving breakthroughs in language and vision. \nWhether similar principles apply to modeling brain activity remains unclear. Here we leveraged a dataset of 3.3 million neurons from the visual cortex of 78 mice across 323 sessions, totaling more than 150 billion neural tokens recorded during natural movies, images and parametric stimuli, and behavior.  We train multi-modal, multi-task transformer models (1M–300M parameters) that support three regimes flexibly at test time: neural prediction (predicting neuronal responses from sensory input and behavior), behavioral decoding (predicting behavior from neural activity), neural forecasting (predicting future activity from current neural dynamics), or any combination of the three. \nWe find that performance scales reliably with more data, but gains from increasing model size saturate -- suggesting that current brain models are limited by data rather than compute. This inverts the standard AI scaling story: in language and computer vision, massive datasets make parameter scaling the primary driver of progress, whereas in brain modeling -- even in the mouse visual cortex, a relatively simple and low-resolution system -- models remain data-limited despite vast recordings. These findings highlight the need for richer stimuli, tasks, and larger-scale recordings to build brain foundation models. The observation of systematic scaling raises the possibility of phase transitions in neural modeling, where larger and richer datasets might unlock qualitatively new capabilities, paralleling the emergent properties seen in large language models.", "tldr": "Using an unprecedented dataset of over three million single-neuron recordings, we demonstrate a clear power-law relationship between transformer model scale and performance on neural encoding and decoding tasks.", "keywords": ["Scaling laws", "Multimodal Transformers", "Foundation models", "Visual cortex", "Neural encoding models", "Neural decoding", "Behavioral prediction", "Calcium imaging"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc4f888f72a81ed9e56f9ff8a34895cec61cdb5e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a model architecture and training methodology and uses it to analyze data and model scaling trends for neuroscience data. It discovers positive trend in data scaling, and a limited scaling on the model side (suggesting the need for more data). The overall message is an important contribution. My main concerns are with one of the baseline used in evaluation, and some concerns about phrasing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Analysis of both data scaling and model scaling trends is very valuable for the community. One of the first papers to do that.\n- Evaluation strategy is well designed, and a good number of tasks are included."}, "weaknesses": {"value": "I like the overall message of this paper, but I think the problems mentioned below need to be addressed before I can recommend acceptance.\n\n**Main remaks**\n- I would prefer the reasoning for upsampling the data was put in the main text (I see it is in the appendix right now.)\n- (Opinionated) Just using number of neurons to compare dataset sizes doesn't seem like a good idea. This is reference to the line: \"We used a dataset of over 3 million single-unit neuronal recordings – an order of magnitude larger than...\". After all, I could have 1 second long recordings of 10 million neurons, and that dataset would not be considered big. To me, it seems a metric like \"neuron-hours\" would be better. i.e. including both the number of neurons and the recording durations to indicate data size.\n- Section 3, Data utilization: \"A key novelty is our ability to sample...\". Can you explain how this is novel in context to existing work? As far as I know, POYO and POYO+ have used this kind of arbitrary continuous sampling, and it has been a part of the open-source [`torch_brain`](https://github.com/neuro-galaxy/torch_brain) package for quite some time.\n- My biggest issue: When comparing the performance of behavior decoding, why not compare with something like POYO (individual behaviors), or POYO+ (multi-behavior)? These models (and many other recent methods) have proven to be much better than the CEBRA baseline. Comparing with something that is not a leading method for behavior decoding diminishes the results. Including stronger baselines would help convince the reader that the suggested expensive large-scale semi-supervised training does indeed lead to better performance, better than simple supervised learning using strong methods. I understand the paper is more focused on scaling, but it is important having a better reference of where purely supervised methods are in comparison.\n\n**Nits**\n- Fig 1. Missing y-axis labels\n- Some places, such as paragraph 2 of Introduction should have em-dashes `---` instead of en-dashes\n- Line 071 - comma after task - \"single modality, task, or dataset\"\n- Typo: Fig 3 caption - \"tookens\"\n- Line 249 - \"shared linear\" -> \"shared linear layer\""}, "questions": {"value": "**Questions**\n- It is unclear whether the dataset used for pretraining is public or private.\n- Can you please expand on \"restart training from intermediate checkpoints every 20k steps\"? (From section 4, Training paragraph) Which intermediate checkpoints do you restart from? Why restart? Why not just continue with a step change in LR?\n- In section 5, Forecasting: Why \"40 frames of behavior\" were used to condition the forecasting? Why not 30?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "D9K6L7lWhS", "forum": "mEw4lhAn0F", "replyto": "mEw4lhAn0F", "signatures": ["ICLR.cc/2026/Conference/Submission23059/Reviewer_8RvB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23059/Reviewer_8RvB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781299201, "cdate": 1761781299201, "tmdate": 1762942496245, "mdate": 1762942496245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Factual Errors in Literature Review and Unfair Baseline Comparisons"}, "comment": {"value": "As someone who has worked on similar neural foundation models, I need to flag multiple factual errors in the literature review and methodologically problematic baseline comparisons that misrepresent both prior work and this paper's actual contributions. \n\n## Issue 1: Overstated Novelty Regarding Multi-Modal Models\n\n**Lines 60-61:** *\"But single-neuron resolution, multi-modal foundation models are still missing.\"*\n\nThis claim is inaccurate. **Neural Encoding and Decoding at Scale (NEDS; Zhang et al., 2025)** already presents a transformer trained at single-neuron resolution across data from 70 animals that performs:\n- **Encoding**: behavior → neural activity  \n- **Decoding**: neural activity → behavior  \n- **Multi-task masking**: alternating between neural, behavioral, within-modality, and cross-modality masking\n\nNEDS uses Poisson loss for neural responses and MSE for continuous behavior decoding—**the same multi-modal loss structure employed here**. Stating such models are \"still missing\" overlooks directly comparable prior work.\n\nThe meaningful distinction is that **OmniMouse adds video** as a third modality (neural + behavior + video), which NEDS does not include, but the framing should acknowledge the existence of multi-modal single-neuron models rather than claiming to pioneer this space entirely.\n\n## Issue 2: Citation Misattribution\n\n**Lines 133-134:** *\"Jiang et al. (2025) questioned their applicability, analyzing the NDT-based model of Zhang et al. (2025), which predicted ∼30,000 spiking neurons across 74 sessions.\"*\n\nThis citation is incorrect. Jiang et al. (2025) analyzed **Zhang et al. (2024)** (*\"Towards a Universal Translator for Neural Dynamics\"*), not Zhang et al. (2025) (NEDS). This conflates two distinct models and misattributes the analysis done by Jiang et al. (2025).\n\n## Issue 3: Overstated Novelty of Temporal Sampling Strategy\n\n**Lines 187-192:** *\"A key novelty is our ability to sample arbitrary 2-second windows from any point in the experiment, including inter-trial intervals and blank screens.\"*\n\nWhile this is a useful capability, characterizing it as a \"key novelty\" is incorrect. **POYO** (Azabou et al., 2023) introduced tokenization that removes \"the need for time-window binning,\" and allows for sampling arbitrary windows throughout experimental sessions, including inter-trial periods.\n\n## Issue 4: Unacknowledged Prior Architectural Patterns\n\n**Lines 295-297:** \"Finally, the outputs of the decoder cross-attention block for each modality are routed to modality-specific linear readouts, projecting from dM back to the original dimensionality.\"\n\n**POYO+** (Azabou et al., 2025) already implements this design pattern: *\"All tokens regardless of the task are processed through the same cross-attention layer, and a custom router groups together tokens from the same task and feeds them to a task-specific linear decoder.\"* Presenting it without acknowledging prior use in POYO+ misrepresents the novelty of the design.\n\n## Issue 5: Inadequately Disclosed Training Data Disparity\n\n**Table 2** compares OmniMouse-300M (trained on **323 sessions**) against baselines trained on only **8 sessions** (lines 354-357: *\"We train all state-of-the-art baselines on...eight mice...to reduce computational cost\"*).\n\nThis represents a **40× difference in training data** that is not clearly indicated in Table 2's caption or prominently discussed when interpreting results. A reader examining Table 2 cannot readily determine how much of the performance gap is attributable to the model versus the substantially larger training set.\n\n## Issue 6: Questionable Baseline Configuration for Different Recording Modality\n\n**Appendix C.2:** The paper uses IBL (Zhang et al., 2024) as a baseline for forecasting, stating: *\"We used the default hyperparameters from 'ndt1_stitching_prompting' and 'ssl_session_trainer' configs from https://github.com/colehurwitz/IBL_MtM_model\"*\n\nWhile the authors did train the IBL on their calcium imaging data, they used hyperparameters designed for **Neuropixels electrophysiology data** without any adaptation for **two-photon calcium imaging**. A fair comparison would require hyperparameter tuning the IBL model on calcium imaging data or at least acknowledging that the baseline may be disadvantaged by the hyperparameter mismatch.\n\nOverall, **there is little discussion of baseline hyperparameter tuning** in this work which raises questions about the SOTA claims.\n\n## Issue 7 - Mischaracterizing NDT3\n\n**Lines 136-138** - The paper states that Ye et al. (2025) (NDT3) was trained on EEG data but it was trained on **intracortical recordings**.\n\n---\n\n## Summary\n\nThis paper makes genuine contributions: the systematic scaling study across 150B neural tokens, the empirical finding that performance is currently data-limited rather than compute-limited, and strong results across diverse prediction tasks are all valuable advances. However, these contributions need to be situated accurately within existing work."}}, "id": "gSpnhECfIA", "forum": "mEw4lhAn0F", "replyto": "mEw4lhAn0F", "signatures": ["~Cole_Lincoln_Hurwitz1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Cole_Lincoln_Hurwitz1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23059/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762999315010, "cdate": 1762999315010, "tmdate": 1762999315010, "mdate": 1762999315010, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies whether the scaling laws in AI also apply to modeling neural activity. Using a dataset with 3.3 million neurons from the mouse visual cortex (78 mice, 323 sessions, and over 150 billion neural tokens), the authors trained multi-modal, multi-task transformer models ranging from 1M to 300M parameters. These models can perform neural prediction, behavioral decoding, and neural forecasting. Empirically, the authors found that performance scales with data quantity but saturates with increasing model size, indicating that current modeling efforts in neuroscience are data-limited."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper makes a timely contribution by systematically studying scaling laws in neurofoundation models using an unprecedentedly large dataset. The work is novel in introducing the first large-scale, multi-modal, multi-task transformer that unifies neural encoding, decoding, and forecasting with naturalistic video inputs. The paper is also well written and organized."}, "weaknesses": {"value": "1. The paper is impressive in scale, but several aspects could be improved. The contribution is empirical rather than methodological. The proposed transformer largely builds on existing model designs (e.g., POYO+ and prior multi-modal fusion techniques), with limited architectural innovation beyond scaling and integration. Clarifying which components are novel vs. adapted from prior work would help better explain the contribution.\n\n2. The paper would benefit from more qualitative analyses or visualizations to show what the model has learned.\n\n3. Although the inclusion of naturalistic video inputs is interesting, the current analyses do not disentangle how much information comes from visual vs. behaviors (e.g., pupil location, size, running speed)."}, "questions": {"value": "1. The single-trial correlation metrics in Fig. 5 are quite low. Could the authors clarify the reason for this? For example, is it due to the use of naturalistic video stimuli rather than repeated trials? Also, why was correlation chosen as the primary evaluation metric, given that it only captures linear relationships and does not account for nonlinear dependencies or variance structure in the data?\n\n2. The paper claims that scaling in neuroscience is limited by data rather than model size, unlike in AI. In Fig. 5, it seems that as data increases, model size should also grow; otherwise, performance may saturate. This implies that model scaling is still necessary when handling larger datasets. In AI, there is already abundant data, which may make increasing model size particularly helpful. I am curious if the authors have considered this factor."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dDOmuwMuZq", "forum": "mEw4lhAn0F", "replyto": "mEw4lhAn0F", "signatures": ["ICLR.cc/2026/Conference/Submission23059/Reviewer_Cuf1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23059/Reviewer_Cuf1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792518356, "cdate": 1761792518356, "tmdate": 1762942495917, "mdate": 1762942495917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents OmniMouse, a multi-modal, multi-task transformer model trained to predict activity in the mouse visual cortex. The model is trained on a new, massive-scale dataset of 3.3 million neurons from 323 recording sessions, totaling over 150 billion neural tokens.\n\nThe primary contribution is a systematic study of scaling laws, training models from 1M to 300M parameters. The authors find that performance gains saturate with increasing model size but scale reliably and consistently with increasing dataset size. This leads to the central conclusion that current models of the visual cortex are data-limited, not parameter-limited, a finding that inverts the standard scaling narrative in mainstream AI. The largest model achieves new state-of-the-art performance on several predictive tasks, including neural forecasting, stimulus-driven prediction, and behavioral decoding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Scaling study for brain foundation model**: To my knowledge, this is the first work to systematically apply the scaling laws methodology to a single-neuron, multi-modal brain foundation model of this magnitude. The primary finding is that the model performance is data-limited rather than parameter-limited, which provides a clear directive for future progress, emphasizing the need for larger and more diverse datasets.\n2. **Dataset and model contribution:** The assembly of a 150B+ token dataset from 3.3M neurons is a major contribution in itself. Furthermore, OmniMouse-300M outperforms strong, specialized baselines (e.g., IBL, CEBRA) across the Sensorium 2023 competition even with its core weights frozen (training only neuron/animal embeddings).\n3. **Flexible and generalizable prediction:** The model's multi-task design, based on flexible masking of modalities, is effective. The paper shows that this design allows the model to learn representations that generalize to contextual variations not seen during training."}, "weaknesses": {"value": "1. **Datasets scale misalignment in baseline comparison:**  The OmniMouse-300M model was trained on the full 323-session dataset, whereas all baselines were trained only on the smallest 8-mice data collection to reduce computational cost. It is unclear how much of OmniMouse's SOTA performance is due to its superior architecture versus simply having access to ~40x more data than the baselines it's compared against.\n2. **Generalization bottleneck on per-neuron identity embeddings:** The use of per-neuron (and per-session/animal) identity embeddings limits the generalization of the model. As shown in Table 1, these neuronal parameters account for the vast majority of the model's total parameters (e.g., 779M $p_N$ vs. 348M $p_M$ for the \"300M\" model). The model cannot perform zero-shot prediction on a new, unseen neuron or animal; it must be fine-tuned. A true foundation model should learn a general representation of a neuron rather than memorizing 3.3 million specific instances."}, "questions": {"value": "1. To provide a fair architectural comparison, what is the performance of a smaller OmniMouse model (e.g., OmniMouse-80M) when trained only on the 8-mouse dataset? How does that model compare to the baselines (IBL, CEBRA, etc.) trained on the same 8-mouse dataset? This would isolate the architectural contribution from the data-scaling contribution.\n2. Have you explored neuron-agnostic tokenization strategies? For example, could a neuron be represented by an embedding derived from its anatomical coordinates, its functional tuning properties (e.g., a pre-computed receptive field), or its relative position to other neurons, rather than a unique learned ID?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "niBgkP6q4G", "forum": "mEw4lhAn0F", "replyto": "mEw4lhAn0F", "signatures": ["ICLR.cc/2026/Conference/Submission23059/Reviewer_Lnv7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23059/Reviewer_Lnv7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821241966, "cdate": 1761821241966, "tmdate": 1762942495643, "mdate": 1762942495643, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a scaling analysis using a large neural model. The model is trained on multi modal data from head-fixed mice in a VR task consisting of visual stimulus, locomotion speed and pupil features recording. The neural data is calcium imaging confined to the visual cortex, for an overall tally of 3M neurons on 78 subjects. The model is based previous work POYO, with an additional hierarchical vision encoder to input the visual stimulus data. The authors present learning curves as a function of model size (measured in parameters) and show that performance saturates with model size, but does not saturate with input dataset size. The model performance is evaluated on a 2023 decoding benchmark with scores beating the baselines in all categories."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The scale analysis is a welcome contribution to the field.\n- The discussion correctly assesses limitations of the current approach, namely the narrow brain region and behavior protocol.\n- Despite the narrow brain region and behaviour type studied, the authors manage to show that their model is still data limited, which makes the claim that dataset size is a limiting factor even more compelling."}, "weaknesses": {"value": "- The performance is compared to baselines that may not be state of the art models. For example in behaviour decoding tasks, to justify the claim of SOTA we may be more interested in others large transformer based ANNs results than in the contrastive learning CEBRA method\n- No clear statement about sharing of code, models and private data for others to address the point above or even reproduce the proposed results, while the authors have made heavy use of public resources.\n\n\nMinor comments:\n- figure 1bc: label y-axis missing (loss)\n- Code / model / data availability statement"}, "questions": {"value": "- What reasonable steps could be taken to reproduce those results from a third-party ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pqpzUajSo6", "forum": "mEw4lhAn0F", "replyto": "mEw4lhAn0F", "signatures": ["ICLR.cc/2026/Conference/Submission23059/Reviewer_SAAW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23059/Reviewer_SAAW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934541568, "cdate": 1761934541568, "tmdate": 1762942495357, "mdate": 1762942495357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}