{"id": "6PgOtwaEF8", "number": 16500, "cdate": 1758265215157, "mdate": 1759897237079, "content": {"title": "FedSUM Family: Efficient Federated Learning Methods under Arbitrary Client Participation", "abstract": "Federated Learning (FL) methods are often designed for specific client participation patterns, limiting their applicability in practical deployments. We introduce the FedSUM family of algorithms, which supports arbitrary client participation without additional assumptions on data heterogeneity. Our framework models participation variability with two delay metrics, the maximum delay $\\tau_{\\max}$ and the average delay $\\tau_{\\text{avg}}$. The FedSUM family comprises three variants: FedSUM-B (basic version), FedSUM (standard version), and FedSUM-CR (communication-reduced version). We provide unified convergence guarantees demonstrating the effectiveness of our approach across diverse participation patterns, thereby broadening the applicability of FL in real-world scenarios.", "tldr": "", "keywords": ["Federated Learning", "Distributed Learning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e45be5f30f725a9b3c34194b5a2e2a2badf8e479.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the FedSUM family designed to handle arbitrary client participation patterns without additional assumptions on data heterogeneity. The authors introduce two delay metrics, $\\tau_{max}$ and $\\tau_{avg}$, to quantify client availability variability and incorporate them into a unified convergence analysis. The theoretical results show that FedSUM achieves convergence rates comparable to or better than existing methods under diverse participation scenarios. Empirical experiments on MNIST, SVHN, and CIFAR-10 show that FedSUM variants outperform several baselines in both convergence speed and stability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is very theoretical, and it provides a unified treatment of arbitrary client participation, bridging several prior assumptions into one analytical framework. \n\n2. The three FedSUM variants balance communication cost and performance trade-offs well. \n\n3. The most related baselines are almost covered in this paper. Compared with the baselines, FedSUM shows better performance under different client participation patterns."}, "weaknesses": {"value": "W1. While the paper claims that FedSUM-CR achieves reduced communication cost, it lacks quantitative analysis or empirical validation of this claim. The paper would be much stronger if it included explicit communication-per-round cost comparisons or more specific theoretical communication complexity analysis.  \n\nW2. A very related work should be included and compared in this paper. See Q1 below. \n\nMinor: \n\nLine 117: \"objective *funtions*\" -> \"objective *functions*\""}, "questions": {"value": "Q1. What is the connection between FedSUM and FOCUS [1] algorithms (these two may be equivalent to some degree)? \n\nQ2. For algorithms with the correction step, why is the bounded gradient heterogeneity still needed? (Scaffold or FOCUS [1] do not need that)\n\nQ3. For uniform sampling, what is $\\tau_{max}$?\n\n[1] Ying, B., Li, Z., & Yang, H. (2025). Exact and Linear Convergence for Federated Learning under Arbitrary Client Participation is Attainable. arXiv preprint arXiv:2503.20117.\n\nSee weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1GSXBoFQ3q", "forum": "6PgOtwaEF8", "replyto": "6PgOtwaEF8", "signatures": ["ICLR.cc/2026/Conference/Submission16500/Reviewer_Z1o5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16500/Reviewer_Z1o5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761527057150, "cdate": 1761527057150, "tmdate": 1762926593839, "mdate": 1762926593839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem of arbitrary and unpredictable client participation in Federated Learning (FL). The core contribution is the FedSUM family of algorithms, including FedSUM-B, FedSUM, and FedSUM-CR. They leverage a Stochastic Uplink-Merge update framework to maintain meaningful global progress even when clients participate infrequently or irregularly. The paper provides unified convergence guarantees and shows that FedSUM variants either match or outperform prior methods under diverse participation patterns by introducing two participation variability metrics: maximum delay ($\\tau_{max}$) and average delay ($\\tau_{avg}$)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The arbitrary participation setting, characterized maximum delay ($\\tau_{max}$) and average delay ($\\tau_{avg}$), captures a wide range of realistic client behaviors, which significantly broadens applicability beyond common uniform sampling frameworks.\n\n2. The convergence guarantees are presented in a unified manner for all three FedSUM variants, recovering known results in canonical special cases."}, "weaknesses": {"value": "1. Datasets are small to medium scale (MNIST, SVHN, CIFAR-10).\n\n2. Client participation patterns in experiment does not perfectly align with the four participation examples."}, "questions": {"value": "1. What are the maximum delay ($\\tau_{max}$) and average delay ($\\tau_{avg}$) in practice or in experiment?\n\n2. I am curious about the performance of sparse participation regimes. For example, in cross-device federated learning setting, there are so many clients and very small fraction of clients can participate in each round. This means $\\tau_{max}$ and $\\tau_{avg}$ would be very large. I suppose FedAvg works well in this case but not sure about the proposed methods.\n\n3. I wonder if Case 4: Reshuffled Cyclic Participation corresponds to client-reshuffling-based FL (Malinovsky, Grigory, et al. \"Federated learning with regularized client participation.\" arXiv preprint arXiv:2302.03662 (2023).)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KwzIoqbdMw", "forum": "6PgOtwaEF8", "replyto": "6PgOtwaEF8", "signatures": ["ICLR.cc/2026/Conference/Submission16500/Reviewer_ew4X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16500/Reviewer_ew4X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789627369, "cdate": 1761789627369, "tmdate": 1762926593429, "mdate": 1762926593429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FedSUM, a family of three federated-learning algorithms (i.e., FedSUM-B, FedSUM, and FedSUM-CR), that are robust to arbitrary client participation patterns, including both controlled and uncontrolled, stochastic and deterministic, and homogeneous and heterogeneous schemes. The key idea is to capture participation variability through two delay metrics: maximum delay $\\tau_{max}$ and average delay $\\tau_{avg}$. These metrics enable a unified convergence analysis for the FedSUM algorithms without requiring restrictive assumptions on data heterogeneity.\nThe algorithms employ a Stochastic Uplink-Merge technique to address data heterogeneity among clients. Theoretical convergence bounds are provided under standard smoothness and bounded variance assumptions. Experiments on MNIST, SVHN, and CIFAR-10 under various participation patterns validate the effectiveness of FedSUM compared to the baseline algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The use of maximum delay ($\\tau_{max}$) and average delay ($\\tau_{avg}$) provides an elegant and intuitive way to quantify participation variability and enables a unified convergence analysis for the FedSUM algorithms.\n2. The FedSUM family subsumes many client participation scenarios (random, cyclic, reshuffled, etc.) under a single analytical umbrella, offering broad applicability.\n3. The convergence rates recover the known rates in special cases and are derived under standard assumptions (smoothness, bounded variance), avoiding unrealistic restrictions on data heterogeneity."}, "weaknesses": {"value": "1. While the delay metrics are central to the theoretical framework, empirical results do not explicitly demonstrate how varying $\\tau_{max}$ or $\\tau_{avg}$ impacts performance or convergence speed. An ablation study varying delay levels across synthetic participation schemes would be beneficial.\n2. While FedSUM-CR emphasizes communication savings and acknowledges additional memory overhead, neither the communication cost nor the memory overhead is quantitatively analyzed.\n3. The evaluation is limited to vision tasks (MNIST, SVHN, CIFAR-10) using a single model architecture (CNN). Incorporating non-vision tasks (e.g., NLP) and additional model types (e.g., ResNet) would provide stronger evidence of the framework's generalizability.\n4. The figures contain too many lines in a single plot, making them difficult to distinguish."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eACSXgMI7f", "forum": "6PgOtwaEF8", "replyto": "6PgOtwaEF8", "signatures": ["ICLR.cc/2026/Conference/Submission16500/Reviewer_7duF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16500/Reviewer_7duF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955484181, "cdate": 1761955484181, "tmdate": 1762926593071, "mdate": 1762926593071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the FedSUM family of federated learning (FL) algorithms, addressing the challenge of arbitrary client participation, a common issue in real-world FL systems where clients may drop out or join unpredictably. A family of algorithm, FedSUM, is proposed followed by convergence analysis. Numerical simulations are provided to illustrate the performance of proposed algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. By unifying several participation schemes into a single theoretical lens, the FedSUM family provides a flexible and general approach.\n\n2. It (partially) addresses the problem of unpredictable participation from a theoretical perspective, bridging the gap between practical and theoretical studies of FL."}, "weaknesses": {"value": "1. Evaluations are limited to standard vision datasets (MNIST, SVHN, CIFAR-10). No experiments on larger or non-IID real-world datasets (e.g., cross-device or cross-silo FL). Adding more realistic experiments would significantly improve the soundness of the paper.\n\n2. The unified framework seems fail to capture time-dependent client participation. For example, shown in (Ribero et al., 2022, Sun et al, 2025), the participation pattern is Markovian, which do not fit in the proposed framework. Moreover, the bias issue discussed in (Ribero et al., 2022, Sun et al, 2025) is worthy to be analyzed and compared under the proposed framework.\n\n\n[1]. Ribero, MÃ³nica, Haris Vikalo, and Gustavo De Veciana. \"Federated learning under intermittent client availability and time-varying communication constraints.\" IEEE Journal of Selected Topics in Signal Processing 17.1 (2022): 98-111.\n\n[2]. Sun, Zhenyu, et al. \"Debiasing Federated Learning with Correlated Client Participation.\" The Thirteenth International Conference on Learning Representations, 2025."}, "questions": {"value": "1. Could authors add more experiments under more practical settings?\n\n2. Could authors discuss whether the proposed framework can fit time-related participation? And how to address bias issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T7bZEz6YI6", "forum": "6PgOtwaEF8", "replyto": "6PgOtwaEF8", "signatures": ["ICLR.cc/2026/Conference/Submission16500/Reviewer_7geH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16500/Reviewer_7geH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762216572117, "cdate": 1762216572117, "tmdate": 1762926592666, "mdate": 1762926592666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}