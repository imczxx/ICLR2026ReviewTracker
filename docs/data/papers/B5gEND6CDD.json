{"id": "B5gEND6CDD", "number": 4602, "cdate": 1757725058934, "mdate": 1759898023808, "content": {"title": "Learning Diverse Textual Contexts for Robust Personalization of Text-to-Image Diffusion Models", "abstract": "Text-to-image (T2I) personalization aims to adapt pre-trained T2I models based on user-provided example images for customized image generation. In existing personalization approaches, the models are typically trained with a small number of personal concept images captured in limited contexts. This often weakens robustness, resulting in poor alignment between the text prompts and the generated images. Existing approaches tackled this by collecting images, thereby introducing diversity in the personal concept’s context. Despite its effectiveness, this is often impractical, considering the high cost of image collection. To circumvent this limitation, we instead diversify the contexts of the personal concept in \\emph{text space}. Based on the fact that the T2I personalization method represents personal concepts as text tokens \\textit{e.g., ``[v]''}, this diversification can be easily achieved by composing the tokens with various contextual words \\textit{e.g., ``[v] at Eiffel Tower''}, offering an efficient alternative to costly manual image collection. During personalization, we leverage these text prompts for training to learn diversified contexts. However, utilizing diversified text prompts for personalization is not straightforward, as T2I personalization typically requires paired images as learning targets. To achieve learning without requiring images, we propose to learn them within \\emph{text space}. Specifically, we leverage masked language modeling (MLM), which operates entirely within \\emph{text space}. By leveraging MLM during personalization, diversified contexts are learned without involving any images.\nWe demonstrate the effectiveness of the proposed approach with extensive experimental results to show that diverse context learning with MLM yields notable improvements in prompt fidelity and state-of-the-art results on widely used public benchmarks. Furthermore, we present an analytical study showing how our approach influences representations in text space through cosine distance analysis of text embeddings, and how these effects propagate to image space via cross-attention maps analysis, providing evidence of its effectiveness.", "tldr": "We diversify the contexts within text space and learn the contexts within text spce for robust T2I personaliation.", "keywords": ["Personalized Text-to-Image Generation", "Text-to-Image Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/abe9f2a9b121972e63b6d349b8f823445b847729.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on the personalization task in the field of image generation. The authors proposed a method to enrich the training sample diversity by adopting MLM and a tuning-based personalization method. The experiment show the method can achieve competitive performance for this task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The presentation of figures is great and easy to understand.\n- The math notations in this paper are self-contained and well-defined.\n- The paper writing is easy to follow."}, "weaknesses": {"value": "- I have to say that tuning-based personalization (like, DreamBooth, Custom Diffusion) is outdated. Learning-based personalization is the mainstream in the current image generation community.\n- The key part of the proposed method is adopting MLM for Learning Diverse Contexts. However, the reason why MLM works is not straightforward and requires further explanation.\n- The authors kept highlighting that the output token is the linear combination of the input tokens in MLM. But how is this related to personalization?\n- The proposed method requires training a lightweight transformer network to bridge MLM and the CLIP text space. Why train another module for personalization fine-tuning? It's just so weird.\n- The experiment in Fig. 4 is unreasonable. Why could the textual diversity affect the attention association?\n- The compared methods are also outdated."}, "questions": {"value": "Please see the section of weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nbnisux7yj", "forum": "B5gEND6CDD", "replyto": "B5gEND6CDD", "signatures": ["ICLR.cc/2026/Conference/Submission4602/Reviewer_TZWq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4602/Reviewer_TZWq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571166903, "cdate": 1761571166903, "tmdate": 1762917464321, "mdate": 1762917464321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles overfitting in personalized text-to-image generation caused by limited training images. It propose a context diversification strategy in the text space: by applying random masking and semantic replacement to expand the text prompts, the model is encouraged to learn consistent semantic attributes of the target concept under diverse contexts, thereby improving generalization. The method improves generation quality and diversity without extra images and is validated via CLIPScore, attention maps, and embedding similarity analysis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "•\tThe paper introduces a novel perspective by enhancing concept binding through diversified textual semantic contexts，represents a clear and valuable conceptual shift.\n\n•\tThe proposed method is lightweight and practical, requiring no additional images or modifications to the architecture of the diffusion model.\n\n•\tThe experimental analysis is interpretable, using semantic alignment and attention-based diagnostics to demonstrate why the method works."}, "weaknesses": {"value": "•\tThe approach can be viewed as a textual data augmentation strategy. The novelty may appear less substantial given the limited architectural or algorithmic innovation.\n\n•\tThe core hypothesis—that semantic diversity in text can serve as an effective regularization signal similar to image diversity—lacks rigorous validation. It would be more valuable if more interpretable evidence could be provided to directly establish the link between text diversity and improved generalization ability.\n\n•\tComparisons with more recent and stronger personalization baselines (e.g., Custom Diffusion, ELITE, SVDiff) are missing, making it difficult to fully assess the competitiveness of the method.\n\n•\tThe experiments are mainly limited to common, coarse object categories. It is still unclear whether this method can be extended to concepts with stronger identity constraints (for example, faces or fine-grained categories)."}, "questions": {"value": "•\tCan the author provide more evidence to support the regularization effect on text semantic diversity? \n\n•\tHow do the results compare with recently stronger baseline methods?\n\n•\t The rest can be seen in the Weaknesses ."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VNlZrnwIUb", "forum": "B5gEND6CDD", "replyto": "B5gEND6CDD", "signatures": ["ICLR.cc/2026/Conference/Submission4602/Reviewer_YPra"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4602/Reviewer_YPra"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829853966, "cdate": 1761829853966, "tmdate": 1762917464078, "mdate": 1762917464078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel and cost-effective method to inject contextual diversity by leveraging the text space compared to how prior works that diversify cost via using an increased number of images. The key technical contribution is utilizing a Masked Language Modeling (MLM) objective on a large set of automatically generated, diverse text prompts (e.g., \"object at Eiffel Tower\") during personalization."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The most interesting part is how the authors attempt tackling one of the most practical issue faced (i.e. lack of diverse context in training images- usually there are only 3-5 reference images available for a concept). Since MLM operates entirely in the text space, it does not require paired images for these diverse contexts, overcoming the main limitation of having to collect/curate more data per concept.\n2. The proposed method is sufficiently novel and cost-effective. Prior works have focussed quite a bit on different ways of customizing diffusion models, but this paper attempts to solve a common fundamental limitation of ensuring that the learned custom embedding does not overfit the provided reference images.\n3. The qualitative results in Figure 6 demonstrate that the proposed method outperforms baselines significantly in terms of editability and context generation (e.g. purple dress for the customized cat in the second row)\n4. The performance gains seen via ImageReward in Tables 4-6 further strengthen the claims of the proposed method"}, "weaknesses": {"value": "1. The quality/diversity of the training is dependent on the predefined templates/words used for creating the prompt set. It is unclear whether the different contexts generated during inference are limited by the ones generated by the LLM. A discussion on the sensitivity to the source or quality of the diverse prompts would strengthen this point.\n2. In a couple examples, the baselines seem to have better preservation of the custom concept features. For example, in the first row in figure 6, the red color present near the hair and hands of the toy are missing in the proposed method's results- while those details are missing for baselines too, would appreciate any intuition from the authors on why the reconstruction may not be perfect."}, "questions": {"value": "The main question I have is why it was necessary to add a separate transformer and whether adding a linear prediction head on top of the unmodified, frozen CLIP encoder, or finetuning the existing higher layers of the CLIP encoder with the MLM objective would have helped as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mFu6F8aCBI", "forum": "B5gEND6CDD", "replyto": "B5gEND6CDD", "signatures": ["ICLR.cc/2026/Conference/Submission4602/Reviewer_SYkC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4602/Reviewer_SYkC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939234854, "cdate": 1761939234854, "tmdate": 1762917463733, "mdate": 1762917463733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles robustness issues in few-shot text-to-image (T2I) personalization caused by limited contextual diversity of the user-provided examples. Instead of collecting more images, it proposes to diversify the textual contexts of the personalized token (e.g., “[v] at Eiffel Tower”, “[v] in Pop Art style”) and to learn these contexts entirely in text space via a masked language modeling (MLM) objective. Concretely, during DreamBooth+LoRA fine-tuning, an auxiliary MLM loss is computed on a large, curated prompt set where some tokens are masked and predicted by a lightweight transformer head plugged on top of the CLIP text encoder. The personal concept token is contextualized through self-attention as a linear combination of context and concept tokens, encouraging the concept embedding to carry richer semantics without requiring paired images."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1 Clear, practical idea: address contextual overfitting by diversifying prompts in text space and enforcing MLM consistency, avoiding costly image collection. The approach is orthogonal to, and easily composable with, common personalization pipelines (DreamBooth+LoRA).\n\n2 Solid empirical gains: consistent SOTA across CLIP (text alignment), DINO (subject fidelity), ImageReward (human preference proxy), and BLIP-VQA on two benchmarks. Qualitative examples show better prompt adherence while preserving identity.\n\n3 Insightful analyses: textual-space cosine distances and image-space cross-attention diagnostics convincingly link the MLM objective to reduced semantic collapse and improved grounding.\n\n4 Implementation details and ablations: reports sensitivity to λ, masking probability, and prompt set size; excludes benchmark prompts from MLM training to avoid leakage; single-GPU feasibility."}, "weaknesses": {"value": "1 Dependence on curated prompt sets and LLMs: The large, hand/LLM-crafted prompt bank (tens of thousands) is central to performance. The construction process, coverage, and potential bias are only partially formalized; generalization to other domains/languages remains unclear.\n\n2 Extra component and training stage: Requires pretraining a separate MLM head (albeit lightweight) on COCO+constructed prompts. The integration details (which layers are updated, interaction with CLIP tokenization nuances) could be more rigorously specified. It also assumes MLM is beneficial while keeping CLIP frozen beyond LoRA, which may not always hold.\n\n3 Metric reliance: CLIP, DINO, ImageReward, BLIP-VQA have known limitations. There is no human user study or identity retrieval evaluation to triangulate improvements, and no failure analysis by prompt category (e.g., heavily compositional, relational, or negations)."}, "questions": {"value": "1 Multiple personalized tokens and continual personalization: Can the approach handle multiple personal concepts jointly, or sequential personalization without interference?\n\n2 Data leakage and safety: How do you ensure the MLM prompt set does not inadvertently include near-duplicates of benchmark prompts? Any safeguards for harmful or biased contexts when using LLMs to scaffold prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DC3hPuWLVk", "forum": "B5gEND6CDD", "replyto": "B5gEND6CDD", "signatures": ["ICLR.cc/2026/Conference/Submission4602/Reviewer_tM5B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4602/Reviewer_tM5B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970193591, "cdate": 1761970193591, "tmdate": 1762917463516, "mdate": 1762917463516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}