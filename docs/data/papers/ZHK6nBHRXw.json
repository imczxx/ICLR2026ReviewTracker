{"id": "ZHK6nBHRXw", "number": 1982, "cdate": 1756974243921, "mdate": 1759898175444, "content": {"title": "Knowledge Externalization: Reversible Unlearning and Modular Retrieval in Multimodal Large Language Models", "abstract": "Multimodal Large Language Models (MLLMs) achieve remarkable cross-modal understanding by training on vast web-scale datasets, but inadvertently internalize sensitive personal and proprietary information. Existing machine unlearning methods address this by irreversibly altering model parameters to permanently erase knowledge. This destructive paradigm conflicts with modern privacy regulations that mandate auditable, reversible, and user-controllable data management. To address these challenges, we propose Knowledge Externalization, a novel framework for reversible and modular knowledge management in MLLMs.  We first propose Dual-Stream Memory Tuning, a method that transfers targeted knowledge from a model's internal parameters into external memory tokens. To mitigate gradient interference when externalizing multiple concepts, we further introduce Soft Orthogonal Weighting, a technique that preserves the independence of each token. Our resulting framework demonstrates three key capabilities: (i) It achieves effective forgetting of target concepts within the base model, while enabling high-fidelity knowledge restoration using the corresponding memory token. (ii) It supports continuous knowledge editing, allowing the information stored within an external token to be dynamically updated post-externalization. (iii) It displays a remarkable emergent ability for compositionality, where multiple memory tokens (including edited ones) can be freely combined to simultaneously recover knowledge corresponding to each concept. Our source code will be released in the near future.", "tldr": "", "keywords": ["Machine unlearning", "Multimodal Large Language Model"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/39ffd9be74ee2947136af36798dc375b6227be71.pdf", "supplementary_material": "/attachment/a9a084db98d92eeefdf92336ad07568cbd57e6b7.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Knowledge Externalization, a new paradigm for reversible and modular unlearning in MLLMs. The framework transfers target knowledge from model weights into external memory tokens, allowing later recovery or editing. DSM runs two opposing optimization streams simultaneously: gradient ascent on a forgetting loss that pushes the base model away from the target concept’s representation, and gradient descent on a recovery loss that encodes the same concept into a dedicated memory token."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- DSM’s simultaneous dual-stream optimization offers a theoretically principled mechanism for balancing loss and restoration. The idea of maintaining a zero-sum dynamic between base parameters and memory tokens is original and well-motivated.\n- The Soft Orthogonal Weighting formulation shows mathematical maturity, with bounded gradient interference and analytic stability claims. This is more rigorous than prior hard orthogonalization masks, and conceptually connects unlearning to continual learning theory."}, "weaknesses": {"value": "- Outdated baselines: it seems like that paper only contain SFR and AT two baselines, and I don't think your ablated algorithm can be considered as a baseline (i.e. DSM). Plus, the citations of those works are missing. Many of other baselines are available such as NPO, KL Divergence, IDK, DPO, SKU.etc (as mentioned in works like [1], [2])\n- The framework assumes clearly separable concepts (e.g., “Donald Trump,” “Elon Musk”), yet it is unclear how it would externalize diffuse or relational knowledge such as “US presidents” or “political debate events.” The independence guaranteed by SOW may not hold without a principled way to segment overlapping semantic clusters.\n- Experiments are limited to ≤ 20 concepts on the tested benchmark. While SOW theoretically scales as O(e^{2λ}), empirical analysis (Fig. 5) shows degradation in GEN and REC beyond 8 concepts. For real-world unlearning workloads involving thousands of entities, token explosion and gradient-history management may become bottlenecks.\n- Most benchmarks involve celebrity or object recognition; the method’s behavior (or the side effects) on model's general utility such as factual, linguistic, or reasoning knowledge remains unexplored (Benchmarks like MIA-Bench, MMMU, MathVISTA.etc)\n- The proposed DSM framework appears conceptually similar to existing Gradient Difference (Grad-Diff) algorithms, which also alternate between gradient ascent on the forget set and gradient descent on the retain set. It is unclear what the core technical novelty of this work is beyond that established paradigm. The authors should clarify how DSM fundamentally differs from prior gradient-difference, based unlearning methods mentioned in [1], [2], and [3]. \n\n\n\n\nReference:\n- [1] Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench\n- [2] CLEAR: Character Unlearning in Textual and Visual Modalities\n- [3] Large Language Model Unlearning"}, "questions": {"value": "- MMUBench does not seem to be an open-source benchmark (as it claims in its paper), how did you run your experiments with this? If the authors relied on internal or proprietary access, this severely limits reproducibility and undermines the generalizability of the reported results. Given the existence of several comparable public MLLM unlearning benchmarks, it would be more convincing to validate the proposed method on open datasets.\n- Table 5 shows asymmetric recovery under reversed concatenation. Is there an analytical explanation or is it an emergent heuristic phenomenon?\n- What mechanisms ensure that erased knowledge is not reconstructable without the token?\n\nI am willing to adjust my score if the authors provide convincing explanations to my concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "z7p6yRvzlh", "forum": "ZHK6nBHRXw", "replyto": "ZHK6nBHRXw", "signatures": ["ICLR.cc/2026/Conference/Submission1982/Reviewer_2Myd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1982/Reviewer_2Myd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760493716304, "cdate": 1760493716304, "tmdate": 1762915983453, "mdate": 1762915983453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the Multimodal Large Language Models (MLLMs) unlearning, especially proposing the reversible data management, termed knowledge externalization, to realize the modular knowledge management for MLLMs. Specifically, the framework consists of two major components introduced in knowledge externalization, the first is dual-stream memory tuning, and the second is gradient interference, which show three capabilities like effective forgetting, continuous knowledge editing, and emergent ability for compositionality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces reversible knowledge management for MLLMs, which is novel and practical research setting.\n2. This paper is easy to follow with intuitive illustrations and well-formalized equations.\n3. The experiments cover different scale models and comprehensive metrics to support the empirical claims."}, "weaknesses": {"value": "1. The considered research scenario is good and practical, but it could be better to explicitly discuss more about the high-level research question, it seems that currently, there are three aspects that can be discussed singly in each specific research area, so what is the current work's biggest takeaway and difference from those separate research works on MLLMs?\n2. The dual-stream memory tuning essentially combined two core mechanisms conventionally in unlearning, while the soft orthogonal weighting also stems from the common factorization techniques in the knowledge editing area. The question is what is the unique technical contribution that is closely related to the new research setting or MLLM? It is important to better position this work in the related area, and highlight the core contributions instead of a simple application.\n3. For the three major claims about the capabilities of knowledge externalization, the empirical justification is not comprehensive and convincing enough to demonstrate the effectiveness.\nWhile I appreciate the idea and the presentation, the paper's claim needs carefully revised or more either theoretical or empirical justficiation to support. Please consider the question part for potential suggestions."}, "questions": {"value": "For the first two points in weakness, please refer to the comments directly for discussion, and for the third part, I have a few questions for potential suggestions about revision:\n1. Please consider involving more work about knowledge editing on MLLMs for discussion and empirical comparison if applicable.\n2. The current empirical performance is good in those tables shown in the paper, while could the authors further discuss and elaborate how the three capabilites are specifically justified (e.g., with what kind of strong baseline, in what kind of aspect does the method suppress the previous methods or not, is there any failure modes), especially for the continual editing and composition part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jpOqfWlsSj", "forum": "ZHK6nBHRXw", "replyto": "ZHK6nBHRXw", "signatures": ["ICLR.cc/2026/Conference/Submission1982/Reviewer_q6J9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1982/Reviewer_q6J9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850367086, "cdate": 1761850367086, "tmdate": 1762915982855, "mdate": 1762915982855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Knowledge Externalization, a novel framework for managing knowledge in Multimodal Large Language Models (MLLMs) that addresses the limitations of current, irreversible machine unlearning methods. Motivated by modern privacy regulations that mandate reversible, auditable, and user-controllable data management , the framework works by transferring targeted knowledge from the MLLM's internal parameters into external memory tokens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This is a major advancement over traditional destructive unlearning, offering a mechanism for auditable and reversible data management, which is critical for meeting modern privacy regulations (e.g., the right to be forgotten and the right to have data restored).\n\n2. Strong Modularity and Compositionality: The externalized knowledge is managed as independent, self-contained memory tokens. This allows for dynamic composition of knowledge during inference (e.g., retrieving information about two distinct entities simultaneously), offering immense flexibility.\n\n3. Minimal Utility Degradation: By transferring the knowledge instead of simply erasing it, the method minimizes catastrophic forgetting—the loss of the base model’s general capabilities or non-target knowledge—a common issue in traditional unlearning"}, "weaknesses": {"value": "1. Scalability Analysis of Knowledge Externalization: As the number of unique knowledge entities requiring externalization grows, the quantity of external memory tokens will explode. Managing, storing, and efficiently indexing this large, potentially sparse memory store introduces significant storage and retrieval overhead in practical deployment. The authors should provide an analysis of Forgetting Effectiveness, General Utility Preservation, and Inference Latency as a function of the increasing scale of externalized knowledge\n\n2. Robustness of Unlearning: The authors should explore the robustness of unlearning against attacks targeting forgotten knowledge."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GcV3ZCcIh9", "forum": "ZHK6nBHRXw", "replyto": "ZHK6nBHRXw", "signatures": ["ICLR.cc/2026/Conference/Submission1982/Reviewer_UCga"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1982/Reviewer_UCga"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903351083, "cdate": 1761903351083, "tmdate": 1762915982560, "mdate": 1762915982560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “Knowledge Externalization: Reversible Unlearning and Modular Retrieval in Multi-Modal Large Language Models” proposes a new framework to make unlearning in multimodal models reversible rather than destructive. The authors introduce two key methods: Dual-Stream Memory Tuning (DSM), which simultaneously erases target knowledge from model parameters and transfers it to external memory tokens, and Soft Orthogonal Weighting (SOW), which minimizes interference between multiple externalized concepts by adjusting gradient similarity. Extensive experiments on several MLLMs (LLaVA-7B/13B and InternVL-2B) show that the method effectively removes target information, preserves unrelated knowledge, and restores forgotten knowledge using external tokens. The framework also demonstrates emergent compositionality (combining multiple memory tokens) and dynamic editability (updating specific memories without retraining)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces Knowledge Externalization, a novel and well-structured framework for reversible and modular unlearning in Multimodal Large Language Models (MLLMs).\n\n2. The proposed Dual-Stream Memory Tuning (DSM) and Soft Orthogonal Weighting (SOW) methods are technically sound and effectively validated across multiple MLLMs (e.g., LLaVA-7B, LLaVA-13B, and InternVL-2B). The experimental design is comprehensive, including ablation studies, multi-concept externalization, and emergent compositionality analysis.\n\n3. Importantly, the authors explore unlearning, a crucial research topic, through the novel perspective of knowledge externalization, which provides an intriguing direction for controllable and reversible knowledge management."}, "weaknesses": {"value": "1. Despite its novelty, several conceptual issues remain unaddressed. Personal or proprietary knowledge embedded in multimodal models is unlikely to have been collected with explicit consent. In such cases, the reversibility of personal data could contradict privacy protection principles, since once removed, sensitive information should not be restorable. Although the authors cite ISO/IEC 27701 to justify reversibility, it is unclear which specific literature explicitly supports that this standard “emphasizes reversibility\".\n\n2. While the framework is claimed to generalize to MLLMs, it remains unclear why it has only been tested on multimodal settings, not text-only LLMs. A justification for this limitation is necessary.\n\n3. The design choice of using prefix embeddings for the external memory restricts retrieval, as information can only be recovered when the exact prefix tokens are provided. When numerous entities are externalized, this dependency could become inefficient and difficult to manage.\n\n4. Experiments are limited to a small number of entities and show reduced performance as the number of externalized concepts increases (as in Figure 5). This raises concerns about scalability and stability. The baseline comparison is also insufficient: given that reversible adaptation could potentially be achieved more simply with LoRA modules, the authors should discuss why LoRA-based reversible unlearning was not considered as a baseline. A LoRA-based approach could be more practical and powerful."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZGyEpownkM", "forum": "ZHK6nBHRXw", "replyto": "ZHK6nBHRXw", "signatures": ["ICLR.cc/2026/Conference/Submission1982/Reviewer_j8Uo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1982/Reviewer_j8Uo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981293001, "cdate": 1761981293001, "tmdate": 1762915982298, "mdate": 1762915982298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}