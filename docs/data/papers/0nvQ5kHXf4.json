{"id": "0nvQ5kHXf4", "number": 12296, "cdate": 1758206892672, "mdate": 1759897519444, "content": {"title": "Efficient Resource-Constrained Training of Vision Transformers via Subspace Optimization", "abstract": "In today’s world, where AI plays a major role in everyday life, energy consumption and data privacy have become critical concerns. On-device learning offers a promising solution by enabling models to train directly on edge devices, thereby reducing energy usage and minimizing the risk of data leakage. However, the increasing size of modern neural networks poses a serious challenge for on-device training. Although prior work has mainly focused on compact convolutional architectures, we explore a different direction by applying subspace-based training to transformer models. Based on the idea that a model’s essential information resides in a fixed subspace, we introduce Weight-Activation Subspace Iteration (WASI), a method designed to overcome the memory bottleneck of backpropagation and improve inference efficiency in transformer-based models by constraining training to this subspace. Our results show that, with accuracy comparable to vanilla training, WASI reduces memory usage by up to $62\\times$ and computational cost (FLOPs) by up to $2\\times$. Moreover, when tested on a Raspberry Pi 5, WASI delivers approximately $1.5\\times$ faster training and inference than vanilla training.", "tldr": "We propose a novel method that enables training vision transformer models within a low-rank subspace to optimize computational resources, making on-device learning practically feasible.", "keywords": ["Deep Learning", "Computer Vision", "Compression", "Low rank"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4b9e85ff6c70392ca75c60d1ffd316e7c7a00bf7.pdf", "supplementary_material": "/attachment/25fcdf1e228da49cc5077ebb8e41ebe93248610e.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Weight–Activation Subspace Iteration (WASI), a method that performs model training entirely within a low-rank subspace of both weights and activation. Experimental results demonstrate that WASI significantly reduces memory usage and roughly halves the computational cost (FLOPs), making on-device training feasible even on constrained hardware such as the Raspberry Pi."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clearly written and easy to follow.\n\n- The proposed method can be applied to various architectures, including ViT, Swin Transformer, and TinyLlama.\n\n- Experiments convincingly show that WASI drastically reduces memory consumption and FLOPs while maintaining comparable accuracy to full fine-tuning."}, "weaknesses": {"value": "- The core ideas, low-rank approximation of parameters and activations via subspace iteration, have been explored in prior work. Thus, the novelty is somewhat limited, and the main contribution is largely engineering integration rather than conceptual innovation.\n\n- The ablation study indicates that the method can be sensitive to certain hyperparameters and datasets, which may affect stability.\n\n- Experiments are conducted on relatively small datasets, which could lead to higher variance and limit generalization.\n\n- Minor: The figures could be improved, for example, by labeling the ε thresholds more clearly to make the trends easier to interpret."}, "questions": {"value": "- Could the authors evaluate the proposed method on a larger-scale dataset, such as training from scratch on ImageNet-1K? Even if direct on-device testing is infeasible in this scenario, a simulated experiment on full GPUs would provide stronger evidence of the method’s scalability and the effectiveness of the low-rank approximation at large scale.\n- Is there a principled way to choose the ε  other than grid sweeping? For example, can it be adaptively determined or estimated from training dynamics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R9YgopYx3f", "forum": "0nvQ5kHXf4", "replyto": "0nvQ5kHXf4", "signatures": ["ICLR.cc/2026/Conference/Submission12296/Reviewer_cFHJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12296/Reviewer_cFHJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761785299454, "cdate": 1761785299454, "tmdate": 1762923227193, "mdate": 1762923227193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes WASI, a weight–activation subspace iteration framework that trains and runs transformers entirely in a learned low-rank subspace. WASI couples (i) Weight Subspace Iteration (WSI)—initial SVD with explained-variance threshold ε and iterative subspace updates—with (ii) a redesigned Activation Subspace Iteration (ASI) (dynamic-programming rank selection, 3D/4D activations)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Unlike LoRA-style PEFT that re-inflates at inference, WASI keeps both weights and activations in low rank throughout forward/backward, with explicit FLOPs/memory formulas and subspace-space equations, yielding predictable savings end-to-end. \n+ The paper motivates stable layer ranks during fine-tuning  and shows singular-value/rank stability, enabling subspace iteration instead of per-step SVD; empirically WSI dominates repeated SVD in FLOPs/accuracy.\n+ The on-device results seems interesting."}, "weaknesses": {"value": "- The edge device used in this paper is only Rasperry pi 5. More edge devices like Jeston Nano should be included and evaluated.\n\n- The final accuracy should be highlighted and reported with Tables. Now it is hard to find it in the draft.\n\n- Main comparisons focus on MLP/linear blocks for fairness with baselines; attention-layer coverage is deferred to appendix. Non-IID, partial-participation, and straggler/client-heterogeneity studies (key for on-device contexts) are not central, leaving external validity to real deployments somewhat open."}, "questions": {"value": "- Can you provide a micro-benchmark of one WASI iteration (basis update, orthogonalization, matmuls) on Pi-5 vs. GPU to attribute the speedups precisely?\n\n- Can you adopt more concrete edge devices for expriments?\n\n- How sensitive are results to ε and the number of subspace-iteration steps? Any auto-tuning strategy that targets a latency or memory cap directly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Frfn0myxfd", "forum": "0nvQ5kHXf4", "replyto": "0nvQ5kHXf4", "signatures": ["ICLR.cc/2026/Conference/Submission12296/Reviewer_qswV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12296/Reviewer_qswV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995444264, "cdate": 1761995444264, "tmdate": 1762923226770, "mdate": 1762923226770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes WASI, a weight–activation subspace iteration framework that trains and runs transformers entirely in a learned low-rank subspace. WASI couples (i) Weight Subspace Iteration (WSI)—initial SVD with explained-variance threshold ε and iterative subspace updates—with (ii) a redesigned Activation Subspace Iteration (ASI) (dynamic-programming rank selection, 3D/4D activations)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Unlike LoRA-style PEFT that re-inflates at inference, WASI keeps both weights and activations in low rank throughout forward/backward, with explicit FLOPs/memory formulas and subspace-space equations, yielding predictable savings end-to-end. \n+ The paper motivates stable layer ranks during fine-tuning  and shows singular-value/rank stability, enabling subspace iteration instead of per-step SVD; empirically WSI dominates repeated SVD in FLOPs/accuracy.\n+ The on-device results seems interesting."}, "weaknesses": {"value": "- The edge device used in this paper is only Rasperry pi 5. More edge devices like Jeston Nano should be included and evaluated.\n\n- The final accuracy should be highlighted and reported with Tables. Now it is hard to find it in the draft.\n\n- Main comparisons focus on MLP/linear blocks for fairness with baselines; attention-layer coverage is deferred to appendix. Non-IID, partial-participation, and straggler/client-heterogeneity studies (key for on-device contexts) are not central, leaving external validity to real deployments somewhat open."}, "questions": {"value": "- Can you provide a micro-benchmark of one WASI iteration (basis update, orthogonalization, matmuls) on Pi-5 vs. GPU to attribute the speedups precisely?\n\n- Can you adopt more concrete edge devices for expriments?\n\n- How sensitive are results to ε and the number of subspace-iteration steps? Any auto-tuning strategy that targets a latency or memory cap directly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Frfn0myxfd", "forum": "0nvQ5kHXf4", "replyto": "0nvQ5kHXf4", "signatures": ["ICLR.cc/2026/Conference/Submission12296/Reviewer_qswV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12296/Reviewer_qswV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995444264, "cdate": 1761995444264, "tmdate": 1763616595806, "mdate": 1763616595806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "WASI is a novel method that applies subspace-based training to Transformer models, primarily aiming for highly efficient on-device fine-tuning by restricting parameter updates to a low-rank subspace that captures the model's essential information. The method overcomes the memory bottleneck of backpropagation and decreases inference latency in transformer models important for edge devices. The results show that WASI maintains accuracy comparable to vanilla training while reducing memory usage by up to 62× and computational cost (FLOPs) by up to 2×. On a Raspberry Pi 5, WASI achieves roughly 1.5× faster training and inference compared to vanilla training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "WASI solves an important problem, which is the memory bottleneck of backpropagation. Previous parameter-efficient methods often ignored the activation memory footprint, which scales linearly with batch size and context length. This comes from the fact that WASI iteratively maintains a subspace that efficiently handles both the weights and the intermediate activations.\n\nThe method has been shown to reduce memory usage by up to 62 times compared to vanilla training. This is crucial for enabling on-device fine-tuning of large models with severely limited RAM.\n\nBy operating within a low-rank subspace, the overall number of floating-point operations (FLOPs) required for both forward and backward passes is significantly reduced. Research reports up to a 2x reduction in computational cost, leading to demonstrably faster training and inference speedups on constrained hardware (e.g., 1.5x faster on devices like the Raspberry Pi).\n\nUnlike methods that aggressively prune models, WASI aims to preserve the model's learning capacity by training within a space believed to contain the essential information. The key strength is maintaining task accuracy comparable to vanilla training, which means the huge efficiency gains do not come at a major performance cost.\n\nI find the writing is clear and the paper is well structured. \n\nThe experiment on Raspberry PI is good. \n\nThe choice of baseline models, such as VIT, SwinT, on a couple of datasets is satisfactory."}, "weaknesses": {"value": "The method is novel yet incremental. The fundamental idea of restricting model updates to a low-dimensional subspace is established. We see this in earlier methods like Stochastic Weight Averaging (SWA) and subsequent research focused on finding paths or subspaces of high-accuracy models.\n\nThe major gains in efficiency are obtained when compared to vanilla and ASI methods. The gains compared to SVD-LLM are minimal, i.e, only for memory consumption during training. \n\nNo power consumption results on the device.  \n\nThe results are shown to work for seed 233. It is mentioned that the variance between the results for different seeds is not high, but there are no results showing that."}, "questions": {"value": "What was the variance between the results with diffence seeds? What is average of results if you try 5 different seeds."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wtFQ5GJiq1", "forum": "0nvQ5kHXf4", "replyto": "0nvQ5kHXf4", "signatures": ["ICLR.cc/2026/Conference/Submission12296/Reviewer_ZTZ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12296/Reviewer_ZTZ2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762165054685, "cdate": 1762165054685, "tmdate": 1762923226323, "mdate": 1762923226323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}