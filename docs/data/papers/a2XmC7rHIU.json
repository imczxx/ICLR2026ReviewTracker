{"id": "a2XmC7rHIU", "number": 4513, "cdate": 1757693456325, "mdate": 1763547001496, "content": {"title": "The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs", "abstract": "In recent months, large language models (LLMs) have made significant progress in mathematical proof generation, but further advancement is hindered by the lack of a large-scale, high-quality dataset of human-evaluated proofs. While expensive to create, such a dataset is essential for driving improvements in training and addressing key open questions in the field of automated proof generation. Specifically, it remains unknown (1) how large the gap is between natural language and formal proof generation, (2) how final-answer accuracy relates to full proof correctness, and (3) how best-of-n selection strategies can affect proof quality. In this work, we present the Open Proof Corpus (OPC), a dataset comprising over 5,000 human-evaluated proofs produced by state-of-the-art LLMs. The OPC was specifically designed for broad applicability and downstream usage in proof generation research and is the first large dataset of LLM-generated solutions to problems from prestigious mathematics competitions such as the USAMO and IMO. Using the OPC, we address the open questions outlined above and provide new insights into LLMs' strengths and limitations in mathematical reasoning. Finally, to showcase the utility of the OPC, we finetune an 8B-parameter model on the dataset, obtaining a model that matches Gemini-2.5-Pro, and performs close to the best model, GPT-5, on evaluating proof correctness.", "tldr": "We conduct the largest human-based evaluation of frontier Large Reasoning Models on challenging mathematical proofs and answer multiple open questions in the field.", "keywords": ["ai", "artificial intelligence", "reasoning", "llm", "math", "benchmark", "dataset", "proof", "gpt", "machine learning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/43bb55635f2b3592d20146d49da62773ccd5716c.pdf", "supplementary_material": "/attachment/cda3c03765354bdeedf93ebf8d4546d2e5915a0f.zip"}, "replies": [{"content": {"summary": {"value": "LLMs excel at math final-answer tasks (e.g., AIME) but struggle with rigorous proof generation—critical for research/theorem proving. Existing proof benchmarks are small, outdated, or closed, leaving 3 key questions unaddressed: (1) natural vs. formal proof gap, (2) final-answer vs. proof correctness link, (3) best-of-n strategy impact. Several findings are elaborated in the paper:\n- Natural language proofs outperform formal ones (GEMINI-2.5-PRO solves 4x more PutnamBench problems than top formal model).\n- Final-answer accuracy is not equal to proof correctness (O3 loses ~30% accuracy when proofs are required, vs. 8% for GEMINI-2.5-PRO).\n- Best-of-n pairwise ranking boosts accuracy significantly.\n- LLMs match humans in proof judging.\n- LLMs rarely admit uncertainty and struggle to judge their own work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Rigorous proof generation/evaluation pipeline.\n- First large, open, human-validated LLM-proof dataset (OPC).\n- High-Quality Data: Expert judges (IMO background) ensure reliable labels; diverse, competition-sourced problems.\n- Actionable Insights: Quantifies critical gaps (natural vs. formal proofs) and validates best-of-n strategies.\n- Openness: OPC and code are open-sourced; transparent methodology for reproducibility. I think this will be a good resource in theorem proving area."}, "weaknesses": {"value": "- Narrow Problem Scope: Most problems are high school-level (IMO/USAMO); few undergraduate/research-level tasks.\n- Outdated Provers: GROK-4/GPT-5 are only used as judges, not prover, which misses latest LLM proof capabilities.\n- Lack of analysis of why formal theorem provers lag behind natural language counterparts. This is an interesting comparison, it would be great if there can be some deeper analysis."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QspVhn6pHH", "forum": "a2XmC7rHIU", "replyto": "a2XmC7rHIU", "signatures": ["ICLR.cc/2026/Conference/Submission4513/Reviewer_Ybod"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4513/Reviewer_Ybod"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816678990, "cdate": 1761816678990, "tmdate": 1762917417203, "mdate": 1762917417203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Rebuttal (Main)"}, "comment": {"value": "We thank the reviewers for their feedback and positive scores. In particular, we are happy to hear they found our work fills a major gap (J7sP,kWHN), our methodology rigorous (C4Eu, J7sP, Ybod), our findings interesting and significant (J7sP, LpED,kWHN, Ybod), and our finetuned model a welcome addition (C4Eu, LpED). We have identified two common questions among the reviewers, and will address all other points in the review-specific replies.\n\n**Q1. Why did the authors not include more advanced mathematical problems? (kWHN, Ybod, J7sP)**  \nExtending our pipeline to handle more advanced mathematics would require considerably more time and resources, making it impractical to construct a dataset comparable in scale to the current OPC. Therefore, our focus on high-school and undergraduate problems is a deliberate choice to increase the dataset size. Many of our conclusions depend on having a large number of problems for statistical validity. We expect qualitatively similar patterns for more advanced problems, though quantifying this is an important future direction.\n\nConcretely, creating an advanced-level dataset would lead to a smaller dataset due to two main difficulties. Accurately grading advanced problems would require recruiting mathematics PhD students, who are less available for large-scale annotation efforts. Further, advanced problems need to be carefully designed to ensure correctness: directly extracting theorems from research papers can lead to incomplete or ill-posed problems, as these theorems typically rely on substantial prior context. Even large, well-funded efforts such as FrontierMath have produced only a few hundred problems, opting for final-answer evaluation instead of proof-based grading due to the high cost of manual annotation.\n\n**Q2. Why do formal theorem provers lag behind their natural language counterparts? (Ybod,J7sP)**  \nTwo main factors explain the performance gap. First, formal theorem proving is inherently much harder than natural language proving. Every reasoning step must be rigorously specified, and even minor syntax errors can lead to invalid proofs. For instance, recent 2025 IMO problems formalized in Lean can exceed 4,000 lines of code, whereas their natural language counterparts are far shorter. Second, the quantity of available formal proof data is orders of magnitude smaller than that for informal text, limiting the ability to train large-scale formal systems, as recently argued in the AlphaProof paper [1]. These factors together explain the observed performance disparity.\n\nProviding a deeper analysis of this comparison is hard. Formal and informal proofs are often structurally different, preventing an accurate one-to-one comparison. Our problem set is also too small to provide statistically valid results across problem categories. To the best of our knowledge, there is limited systematic error analysis for formal provers. However, there are some basic observations, such as a difficulty with combinatorics (since these problems are hard to formalize) [2], and research-level problems (since these do not appear in the training data) [3].\n\n[1] https://www.nature.com/articles/s41586-025-09833-y  \n[2] https://arxiv.org/abs/2505.03171  \n[3] https://arxiv.org/abs/2511.02872"}}, "id": "gg930WLKsL", "forum": "a2XmC7rHIU", "replyto": "a2XmC7rHIU", "signatures": ["ICLR.cc/2026/Conference/Submission4513/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4513/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4513/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763547171309, "cdate": 1763547171309, "tmdate": 1763547171309, "mdate": 1763547171309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work the authors curated a new dataset called the open proof corpus, which contains 5,062 LLM-generated proofs of 1,010 distinct problems from math contests. These proofs are all incorporated with manual reviews from human experts. They have also done a lot of researches around this dataset, especially on the proof judging capabilities of different LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This work answers a lot of interesting questions in this area.  \n\n* This paper provides a comprehensive study on the proof evaluation capability of LLM, fulfilled an important blank in the research of AI4Math."}, "weaknesses": {"value": "* The evaluation of proof judgement may be heavily dependent on the judger’s prompt or criteria, so neither the judging accuracy nor the alignment with human graders are accurate enough.  \n\n* This work did not cover problems from advanced math or research-level math, where proof problems weigh more importance than math competitions. This limits the contribution of OPC.  \n\n* The creation of this OPC heavily depends on manual annotation from experts, which limits the scalability of this work."}, "questions": {"value": "* How are the results in Table 2 (Section 5.2) evaluated actually? I did not find any further descriptions of the settings of these experiments around this chapter. If the performance of LLM judges is evaluated by comparing with human labels, then are they directly comparable with human baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OVk8C68Lz0", "forum": "a2XmC7rHIU", "replyto": "a2XmC7rHIU", "signatures": ["ICLR.cc/2026/Conference/Submission4513/Reviewer_kWHN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4513/Reviewer_kWHN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866721838, "cdate": 1761866721838, "tmdate": 1762917416719, "mdate": 1762917416719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents large-scale dataset of high-difficulty math problems which includes both correct/incorrect proofs and profound annotations which are represented via human-expert judgements. Based on the dataset, some exciting insights are proposed."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. A fairly large dataset of math problems is proposed, with its core value lying in expert annotations. Furthermore, both correct and incorrect proofs are included. It is quite unusual for modern math benchmarks and datasets to contain such annotations, making this contribution valuable to the community.\n2. Some exciting empirical insights are provided. While I do not feel that the exploration of performance differences between natural and formal proof generation is particularly valuable, since the performance degradation in formal setups is largely expected, the examples illustrating the discrepancy between final-answer correctness and proof correctness are important.\n3. The performance of the fine-tuned, moderately sized LLM judge is impressive."}, "weaknesses": {"value": "It might be somewhat subjective, but the presentation quality is low, even considering the number of figures included in the manuscript. While it is understandable that the authors tend to include more content rather than placing all figures in the Appendix, the text in each section is highly granular. I think this weakness could be addressed by rethinking the overall structure. The main focus should be on the dataset itself, which is valuable, while the incremental contributions in the form of interesting observations should either be explored more deeply or moved to the Appendix."}, "questions": {"value": "1. Can you clarify the guidelines used by judges for borderline proofs? How are omissions or shortcuts treated when deciding correctness?\n2. Could you provide more insight into common errors when models give correct answers but incorrect proofs? Are these mostly algebraic mistakes, logical gaps, or misapplied theorems?\n3. Does this discrepancy vary systematically by problem type or difficulty?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VfngolkN9b", "forum": "a2XmC7rHIU", "replyto": "a2XmC7rHIU", "signatures": ["ICLR.cc/2026/Conference/Submission4513/Reviewer_LpED"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4513/Reviewer_LpED"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762096921884, "cdate": 1762096921884, "tmdate": 1762917416190, "mdate": 1762917416190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Open Proof Corpus (OPC), a dataset of 5,062 LLM-generated mathematical proofs across 1,010 problems from prestigious competitions (IMO, USAMO, Putnam), each evaluated by human judges. Using the OPC, the authors demonstrate: (1) informal proof generation outperforms formal by 4x on PutnamBench, (2) significant gaps exist between final-answer accuracy and proof correctness (especially for o3, dropping from 87.6% to 59.5%), and (3) ranking-based best-of-n strategies achieve 47% accuracy versus 26% pass@1. They also fine-tune an 8B model that achieves 88.1% accuracy in judging proofs, approaching GPT-5's performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses critical need: First large-scale dataset of human-evaluated LLM proofs, filling a major gap since existing benchmarks focus only on final answers (e.g., AIME, HMMT).\n- Methodology: Well-designed grading pipeline using 13 former IMO participants, clear guidelines, double-grading (90.4% agreement), and clever use of LLM-generated issue summaries to aid grading efficiency.\n- Significant empirical findings: The 4x gap between informal/formal proof generation and the divergence between answer accuracy and proof correctness are important insights. The ranking-based best-of-n showing 21% absolute improvement is practically valuable.\n- High-quality dataset design: Thoughtful splits (MathArena, PutnamBench, best-of-n, generic) enable targeted analyses while preventing test set contamination."}, "weaknesses": {"value": "- Binary evaluation loses information: The \"5+/7 points counts as correct\" threshold is arbitrary and discards nuanced quality differences that partial credit scoring would capture.\n- Unfair formal/informal comparison: Comparing specialized formal proof models against general-purpose LLMs isn't apples-to-apples. The brief mention of Seed-Prover's 50% formal accuracy suggests the gap may be overstated.\n- Missing statistical analysis: No confidence intervals, significance tests, or error bars despite sufficient sample sizes.\n- Insufficient contamination analysis: Section C.2's comparison of \"Standard\" vs \"Non-standard\" competitions is suggestive but not conclusive. The performance differences could be explained by difficulty alone."}, "questions": {"value": "- Why binary labels? Could you release the raw judge feedback to enable partial credit analysis? This would be valuable for understanding proof quality gradients.\n- Model failure modes: The observation that only 114/1700 incorrect proofs acknowledged uncertainty is striking. Could you analyze whether this varies by problem difficulty or type?\n- Formal proof training: Have you considered fine-tuning informal models on formal proof data to better understand the performance gap?\nCompetition selection rationale: Was there systematic criteria for choosing these specific competitions over others (e.g., Putnam over Mathcounts)?\n- Extending beyond competitions: Have you considered including undergraduate textbook problems or research-level lemmas? What would be needed to extend OPC to these domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tkE5ZoRYzP", "forum": "a2XmC7rHIU", "replyto": "a2XmC7rHIU", "signatures": ["ICLR.cc/2026/Conference/Submission4513/Reviewer_J7sP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4513/Reviewer_J7sP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762147752640, "cdate": 1762147752640, "tmdate": 1762917414675, "mdate": 1762917414675, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a set of approximately 1000 contest math problems, drawn from existing competitions, whose LLM proof is humanly rated (binary), to be used both as an eval set and training set. They use the dataset to assess how correct the proof is compared to the (correct) final answer. The dataset, as a training dataset, is validated by fine-tuning an 8B R1-Qwen model, which is claimed to match GPT5."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem assessment pipeline is rigorous. \n\nThe single fine-tuned model on OPC is a welcome addition that supports OPC. \n\nThe dataset size is sufficiently large to allow finetuning."}, "weaknesses": {"value": "- misleading statement: my biggest issue is that apparently not the full dataset was shared, as only USAMO and BMOSL seem to appear in the zipped supplementary (and also for these, not the full dataset? seems quite small), contrary to the claim: \"We have included our dataset in the supplementary material, along with detailed descriptions of our methodology and experimental setup to ensure full reproducibility.\"\n\n- ambiguous claim: \"The OPC was specifically designed for broad applicability and downstream usage in proof generation research and is the first to include a substantial number of correct, LLM-generated solutions to problems from prestigious mathematics competitions such as the USAMO and IMO.\"\nAs it reads, it is unclear if the authors claim to be the first ever to create such a dataset, or the first to create such a dataset in the more narrow domain of contest math problems (only the latter is correct). Since this is from the **appendix**, I would urge the authors to rewrite.\n\n- missed important prior literature: Probably the first paper on \"pure\" autograding was https://arxiv.org/abs/2406.10268, and there is (rather similar) follow-up work by these authors https://arxiv.org/html/2502.13337v1  (it would be good to include this in the related work section). But much earlier work exists implicitly in ML even if not marked as autograding, e.g. in 2024 https://arxiv.org/abs/2402.11111 a more detailed methodology for \"LMs as evaluators\" was derived (see also more papers on prior such literature), or **2021** in the well-known paper https://arxiv.org/abs/2110.14168, which used what they called \"verifiers\" All this points to an existing body of work on proof judging that is missing from this paper.\n\n- Wrong claim: \"Data contamination poses only a minor risk for proof judging, since generated proofs cannot be present in the training data.\"\nI am unsure on what information this claim rests - who is to say that the main LLM companies don't exactly do this? They have their LLMs generate outputs on contest-level problems to ensure that their LLMs can potently act as judges, which can be of use for subsequent pipelines that the companies might use, or in case the public wants to use LLMs as judges, and companies are interested in having their LLMs perform well on publicly known problems. This seems entirely plausible to me, so I believe this statement should be retracted.\n\n- in terms of the results, with some exceptions, the paper seems to reinforce known folklore beliefs about how models performance on mathematics.\n\n- Almost no details are given about fine-tuning on R1 Qwen3-8B. In particular, rivalling the performance on GPT-5 is a dubious claim (presumably a heldout subset of OPC was used for this on which R1 Qwen was not train? details are missing)"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wIfB4bDJRP", "forum": "a2XmC7rHIU", "replyto": "a2XmC7rHIU", "signatures": ["ICLR.cc/2026/Conference/Submission4513/Reviewer_C4Eu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4513/Reviewer_C4Eu"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762777722948, "cdate": 1762777722948, "tmdate": 1762917414070, "mdate": 1762917414070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}