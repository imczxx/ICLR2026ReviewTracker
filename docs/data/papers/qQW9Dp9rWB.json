{"id": "qQW9Dp9rWB", "number": 6754, "cdate": 1757994587023, "mdate": 1759897896250, "content": {"title": "SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models", "abstract": "Large Multimodal Models (LMMs) have achieved remarkable progress across various capabilities;\nhowever, complex video reasoning in the scientific domain remains a significant and challenging frontier.\nCurrent video benchmarks predominantly target general scenarios where perception/recognition is heavily\nrelied on, while with relatively simple reasoning tasks, leading to saturation and thus failing to effectively\nevaluate advanced multimodal cognitive skills. To address this critical gap, we introduce SciVideoBench,\na rigorous benchmark specifically designed to assess advanced video reasoning in scientific contexts.\nSciVideoBench consists of 1,000 carefully crafted multiple-choice questions derived from cutting-edge\nscientific experimental videos spanning over 25 specialized academic subjects and verified by a semiautomatic\nsystem. Each question demands sophisticated domain-specific knowledge, precise spatiotemporal\nperception, and intricate logical reasoning, effectively challenging models’ higher-order cognitive abilities.\nOur evaluation highlights significant performance deficits in state-of-the-art proprietary and open-source\nLMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating substantial room for advancement in video\nreasoning capabilities. Detailed analyses of critical factors such as reasoning complexity and visual grounding\nprovide valuable insights and clear direction for future developments in LMMs, driving the evolution of truly\ncapable multimodal AI co-scientists. We hope SciVideoBench could fit the interests of the community\nand help to push the boundary of cutting-edge AI for border science.", "tldr": "", "keywords": ["Video Reasoning", "Large Multimodal Model", "AI4Science"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b91ad565b4f2d4d8c0aa5f871494aced9d6dd1fc.pdf", "supplementary_material": "/attachment/5ec75f76b17eb95d5fcc9996776af8263e07dd53.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces SciVideoBench, a benchmark targeting research-level scientific video reasoning. It comprises 1,000 multiple-choice questions drawn from research-grade experimental videos (sourced from JoVE) spanning physics, chemistry, biology, and medicine, with questions verified via a human-in-the-loop, multi-agent pipeline that enforces visual grounding with timestamps. The benchmark emphasizes conceptual, hypothetical, and quantitative reasoning beyond recognition. Reported results show: (i) very low human graduate-student accuracy (17.4%), (ii) “vision-blind” text-only baselines performing near chance (e.g., GPT-4o ~15.8%), (iii) a large gap between proprietary and open-source LMMs (e.g., Gemini-2.5-Pro 64.3% vs. best open 38.8%), and (iv) sizeable gains from chain-of-thought prompting for some models. The dataset redistributes annotations/metadata/timestamps (not full videos) under JoVE’s copyright constraints."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality. Focuses on scientific video reasoning with research-grade lab videos (tri-modal alignment: video, audio narration, and peer-reviewed manuscripts), addressing a gap in existing general video QA benchmarks.  \nQuality. The human-in-the-loop multi-agent annotation pipeline aims to ensure answerability and visual grounding (timestamps + visual comparer + expert verification).  \nClarity. Scope and contribution are clearly stated; the paper documents evaluation settings and analyses (blind baselines, proprietary vs. open-source, CoT effects).  \nSignificance. Results reveal substantial headroom (e.g., human grads at 17.4%; big proprietary–open gaps), underscoring the difficulty and potential impact for LMM research on scientific reasoning."}, "weaknesses": {"value": "Scale & format. The benchmark is limited to 1k MCQs. At this difficulty level, multiple-choice can obscure partial competence and invite guessability. Consider expanding the scale and adding open-ended rationales or structured reasoning targets (e.g., numeric derivations, unit-checked answers) to better capture scientific reasoning depth.  \nDomain coverage constraints. Dependence on JoVE (English) narrows modality and topical breadth, which the authors also note as a limitation. Broader sources (field studies, simulations, non-English material) would reduce domain bias and improve generality.  ￼\nLLM-in-the-loop artifacts. LLMs are used in annotation (generation, rationales, phrasing). While expert verification is present, quantify and stress-test for annotation artifacts (e.g., distractor style regularities) and measure inter-annotator agreement to ensure reliability beyond model-authored templates.  \nText-only solvability & audio confounds. The blind baseline helps, but add stronger text-only and audio-only ablations per question type, plus adversarial filtering to ensure solutions truly require video evidence rather than transcripts or narration alone. (Appendix mentions “Impact of Audio” — consider promoting those analyses.)  \nHuman evaluation design. The 17.4% graduate-student accuracy suggests extreme difficulty. Provide more detail: subject expertise matching, time limits, interface, allowed replay, and report per-domain human scores and inter-rater stats to calibrate realism vs. trickiness."}, "questions": {"value": "Answerability audits. What fraction of items remain solvable from transcripts alone or audio alone? Can you release per-item tags indicating which modality is necessary/sufficient? \nAnnotation reliability. Please report inter-annotator agreement, human edit rates on LLM-proposed items, and an artifact probe (e.g., train a small text-only classifier on distractor style to test leakage).  \nHuman study details. How were graduate students recruited and matched to subjects? What were the instructions/time budgets, and could they scrub video timelines? Any per-discipline human results?  \nAudio impact. The appendix lists “Impact of Audio.” Can you quantify modality contributions (video vs. audio vs. both) per reasoning category in the main text?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BTFI74cJlO", "forum": "qQW9Dp9rWB", "replyto": "qQW9Dp9rWB", "signatures": ["ICLR.cc/2026/Conference/Submission6754/Reviewer_1k1D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6754/Reviewer_1k1D"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6754/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707291459, "cdate": 1761707291459, "tmdate": 1762919036555, "mdate": 1762919036555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SCIVideoBench, a benchmark for evaluating scientific video reasoning in large multimodal models. It focuses on research-level experimental videos sourced from JoVE and constructs 1,000 multiple-choice questions across four disciplines. The benchmark aims to assess domain-specific, spatiotemporal, and logical reasoning abilities of LMMs, and reports results over 35 models, highlighting substantial performance gaps between proprietary and open-source systems. The work fills a relatively underexplored space between general video QA and specialized scientific reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The benchmark addresses an underrepresented domain: research-level scientific experiments, that goes beyond existing video QA datasets focused on everyday scenes or lectures.\n2. The dataset construction pipeline (multi-agent annotation + human verification) is well-documented and demonstrates substantial engineering and curation effort.\n3. The evaluation over a wide range of open and proprietary models provides a clear empirical baseline for future LMM research in scientific contexts."}, "weaknesses": {"value": "1. The human baseline design is insufficiently described—participants’ domain alignment, access to supporting materials, and the rationale for the closed-book setup remain unclear, raising questions about the interpretability of the 17% human accuracy.\n2. Although the text-only baseline provides aggregate evidence for visual dependence, the paper lacks per-item verification (e.g., labeling or statistics on which questions are visually indispensable vs. text-sufficient)."}, "questions": {"value": "1. Human baseline: What were the participants’ backgrounds and how closely did they match the disciplines of the videos? Were they allowed to use papers, subtitles, or frame-by-frame playback? What motivated the closed-book design?\n2. Answerability: Is there a visual-evidence audit (timestamps, annotated frames, or criteria) verifying that each question truly requires watching the video?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XzGjJngxEp", "forum": "qQW9Dp9rWB", "replyto": "qQW9Dp9rWB", "signatures": ["ICLR.cc/2026/Conference/Submission6754/Reviewer_FZjT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6754/Reviewer_FZjT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6754/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981848050, "cdate": 1761981848050, "tmdate": 1762919036043, "mdate": 1762919036043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SciVideoBench, a new benchmark designed to evaluate advanced video reasoning capabilities of Large Multimodal Models (LMMs) in scientific contexts. The benchmark consists of 1,000 multiple-choice (more than 4 choices) questions derived from 241 research-level experimental videos from the Journal of Visualized Experiments (JoVE), spanning four major disciplines (Physics, Chemistry, Biology, Medicine) and over 25 specialized subjects. Questions are categorized into three types: conceptual, hypothetical, and quantitative reasoning. The authors employ a semi-automatic annotation pipeline combining multi-agent LLM systems with human expert verification. Evaluation of 35 models (5 proprietary, 30 open-source) reveals significant performance gaps, with the best model (Gemini 2.5 Pro) achieving only 64.3% accuracy, and best open-source model (InternVL-3-78B) only achieves 38.0, indicating substantial room for advancement in scientific video understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The benchmark addresses a critical gap in evaluating LMMs on research-level scientific reasoning, moving beyond elementary/college-level content that current models are beginning to saturate.\n2. The semi-automatic annotation pipeline with human-in-the-loop verification, multiple specialized agents (QA Generator, Evaluator, Visual Comparer, Refiner), and expert validation ensures high-quality questions that require genuine multimodal reasoning.\n3. The paper evaluates 35 models across multiple dimensions (proprietary vs. open-source, different sizes, with/without chain-of-thought prompting) and provides detailed analysis of failure modes. And the benchmarks shows significant gap between close-source and open-source models' performance, highlighting a important direction for the open-source community to push."}, "weaknesses": {"value": "1. The paper lacks one doable ablation that whether the performance of models scale as the number of frames sampled scaled up. This is also essential for a video understanding benchmark.\n2. The low human performance makes it difficult to assess whether questions are genuinely research-level or simply poorly calibrated"}, "questions": {"value": "1. Given that Gemini 2.5 Pro was used extensively in annotation, were any steps taken to validate that the benchmark doesn't inadvertently favor Gemini models? What would performance look like if a different model had been used for annotation? Is the high performance of Gemini 2.5 Pro on the benchmark because of this issue?\n2. What metrics were used to validate annotation quality beyond human review? Was inter-annotator agreement measured? How many questions were rejected during the validation process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fq8NFVKI1j", "forum": "qQW9Dp9rWB", "replyto": "qQW9Dp9rWB", "signatures": ["ICLR.cc/2026/Conference/Submission6754/Reviewer_aWZV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6754/Reviewer_aWZV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6754/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983415453, "cdate": 1761983415453, "tmdate": 1762919034900, "mdate": 1762919034900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **SciVideoBench**, a benchmark designed to evaluate advanced video reasoning capabilities of Large Multimodal Models (LMMs) in scientific contexts. The benchmark consists of 1,000 multiple-choice questions derived from 241 research-grade experimental videos sourced from the Journal of Visualized Experiments (**JoVE**), spanning over 25 scientific subjects across four major disciplines. Each question is categorized as conceptual, hypothetical, or quantitative, requiring sophisticated domain knowledge, precise spatiotemporal perception, and complex logical reasoning. The authors evaluate 35 models including proprietary systems (Gemini 2.5 Pro, GPT-4o) and open-source alternatives, revealing significant performance gaps with the best models achieving only modest accuracy compared to human experts. The work aims to address limitations in existing video benchmarks that have become saturated and fail to adequately challenge advanced multimodal reasoning capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel and challenging benchmark addressing a critical gap**\n- Targets research-level scientific reasoning rather than college-level knowledge, filling a significant void in current evaluation frameworks (Sec. 1, Table 1)\n- Demonstrates clear performance gaps even for state-of-the-art models like Gemini 2.5 Pro (≈27% accuracy vs. 64% human performance), indicating substantial room for improvement (Table 2)\n2. **Comprehensive experimental design and evaluation methodology**\n- Evaluates 35 diverse models across proprietary and open-source categories with consistent experimental protocols (Sec. 4)\n- Implements multi-stage human-AI collaborative annotation pipeline with quality control mechanisms (Sec. 3.2, Figure 2)\n- Provides detailed analysis across question types (conceptual, hypothetical, quantitative) revealing differential model capabilities (Sec. 4.3, Figure 5)\n3. **High-quality dataset construction with rigorous validation**\n- Covers broad scientific scope with 25+ subjects across Biology, Chemistry, Physics, and Engineering ensuring domain diversity (Figure 3)\n- Provides clear categorization framework distinguishing conceptual understanding, hypothetical reasoning, and quantitative analysis (Sec. 3.2)\n4. **Practical impact and reproducibility considerations**\n- Addresses real-world scientific video understanding needs relevant to AI for Science applications (Sec. 2.2)\n- Provides detailed error analysis revealing specific failure modes like ignoring visual evidence and missing domain knowledge (Sec. 4.4)"}, "weaknesses": {"value": "1. **Dataset construction and annotation concerns**\n- Human annotation process details are underspecified, particularly regarding inter-annotator agreement and quality control metrics (Sec. 3.2)\n- No reported analysis of potential annotation bias or consistency across different scientific domains (Sec. 3.2)\n- Limited discussion of how question difficulty was calibrated or validated beyond expert review (Sec. 3.2)\n2. **Limited theoretical analysis of reasoning requirements**\n- Lacks formal characterization of what constitutes “research-level” reasoning versus college-level knowledge, relying primarily on intuitive distinctions (Sec. 1)\n- No systematic analysis of cognitive complexity or reasoning depth required for different question types beyond basic categorization (Sec. 3.2)\n3. **Methodological and statistical rigor issues**\n- No reported confidence intervals or statistical significance testing for performance comparisons across models (Table 2)\n- Missing analysis of question difficulty distribution and potential ceiling/floor effects in evaluation (Sec. 4)\n- Limited discussion of potential dataset contamination or overlap with model training data (Sec. 4)"}, "questions": {"value": "How do you formally define and validate “research-level” reasoning? Can you provide cognitive complexity metrics or expert assessments that distinguish your questions from existing college-level benchmarks? What specific reasoning skills (causal inference, counterfactual reasoning, multi-step deduction) are required?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ak6xRWNRp6", "forum": "qQW9Dp9rWB", "replyto": "qQW9Dp9rWB", "signatures": ["ICLR.cc/2026/Conference/Submission6754/Reviewer_wpTT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6754/Reviewer_wpTT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6754/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990297855, "cdate": 1761990297855, "tmdate": 1762919034274, "mdate": 1762919034274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}