{"id": "oOSEF3TVb3", "number": 17518, "cdate": 1758277083880, "mdate": 1759897169791, "content": {"title": "Neuro-Symbolic AI for Analytical Solutions of Differential Equations", "abstract": "Analytical solutions to differential equations offer exact insight but are rarely available because discovering them requires expert intuition or exhaustive search in large combinatorial spaces. We introduce SIGS, a neuro-symbolic framework that automates this process. SIGS uses a formal grammar to generate only syntactically and physically valid building blocks, embeds these expressions into a continuous latent space, and then searches this space to assemble, score, and refine candidate closed-form solutions by minimizing a physics-based residual. This design unifies symbolic reasoning with numerical optimization; the grammar constrains candidate solution blocks to be proper by construction, while the latent search makes exploration tractable and data-free. Across a range of differential equations SIGS recovers exact solutions when they exist and finds highly accurate approximations otherwise, outperforming tree-based symbolic methods, traditional solvers, and neural PDE baselines in accuracy and wall-clock efficiency. These results are a step forward integrating symbolic structure with modern ML to discover interpretable, closed-form solutions at scale.", "tldr": "", "keywords": ["Neural-Symbolic AI", "Compositional Learning", "Formal Grammars", "Differential Equations", "Analytical Solutions"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c0b306642fe0c5be60e3fa4606440b9c5e6a9a93.pdf", "supplementary_material": "/attachment/a1bde92f6a56b0bdf8670e43605facd33a4462e0.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes SIGS (Symbolic Iterative Grammar Solver), a neuro-symbolic framework for discovering analytical (closed-form) solutions of PDEs.\n\nThe key idea is to construct a grammar-based atom library and use a Topology-Regularized Grammar VAE (TGVAE) to learn a smooth latent space of symbolic expressions. SIGS then performs a two-stage search: \n\n- Stage 1: cluster latent codes and interpolate within clusters to identify promising candidate structures;\n\n- Stage 2: refine constants through gradient-based optimization. Experiments across elliptic, parabolic, and hyperbolic PDEs show that SIGS can recover exact or near-exact symbolic solutions and outperform recent symbolic discovery baselines (HD-TLGP, SSDE) in accuracy and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel neuro-symbolic design: Integrating grammar-level atoms, latent interpolation, and geometric/topological constraints is a creative and elegant framework.\n2. Strong empirical performance: On selected PDE benchmarks, SIGS accurately reconstructs analytical expressions, sometimes achieving machine precision.\n3. Well-motivated two-stage pipeline: The coarse-to-fine (structure search → constant refinement) approach is conceptually clean and provides interpretability."}, "weaknesses": {"value": "1. Dependence on handcrafted Ansatz and grammar: The system assumes access to an appropriate symbolic library. When the correct building blocks are missing, SIGS likely fails; this is not tested quantitatively.\n2. Limited fairness and transparency in baselines: It is unclear whether competing methods (e.g., HD-TLGP, SSDE, FEniCS) had access to the same symbols, constants, or data budgets. Table 2 comparisons could be biased.\n3. Scalability and robustness not demonstrated: The method is validated on PDEs with known, smooth analytic forms. Its behavior on nonlinear, discontinuous, or noisy settings is untested.\n4. Motivational gap vs. numerical solvers: Since SIGS can be slower and less accurate than classical solvers (e.g., FEniCS), the paper should articulate clearer motivation for discovering symbolic forms instead of direct numerical solutions."}, "questions": {"value": "1. Ansatz sensitivity: How does SIGS perform when the provided Ansatz or atom library omits key operators (e.g., removing tanh for Burgers)? Please quantify degradation.\n2. Residual evaluation: Have you computed the residuals of the recovered analytical expressions of Poisson equation in Table 3? Please compare residuals between SIGS and FEniCE under different spatial resolutions. And how about other boundary conditions, such as Neumann and periodic?\n3. FEniCS resolution: What spatial resolution was used for FEniCS in Table 2? How does the FEniCS error and runtime change with mesh refinement, and how do these compare to SIGS?\n4. Motivation for symbolic discovery: Given that FEniCS achieves higher accuracy faster, what is the practical motivation for SIGS? Is interpretability the main benefit, or does SIGS generalize across PDE families?\n5. Failure modes: Can you show examples where SIGS fails or outputs incorrect forms, and analyze why?\n\nTypo: Inconsistent of poisson equation form between 1175 line and 1313 line."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4vsXXs7kDL", "forum": "oOSEF3TVb3", "replyto": "oOSEF3TVb3", "signatures": ["ICLR.cc/2026/Conference/Submission17518/Reviewer_3xsx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17518/Reviewer_3xsx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881286564, "cdate": 1761881286564, "tmdate": 1762927398039, "mdate": 1762927398039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The aim of this paper is to automatically find analytical solutions/closed-form expressions of PDE without relying on observational data, and give a scalable solution to the combinatorial explosion and illegal expression problems of traditional symbolic tree search.\nMethods: SIGS based on formal grammar and topological regularization grammar VAE (TGVAE) are proposed: The limited feasible expression is constructed with the hierarchy of \"Ansatz+atoms\", and the discrete expression is embedded into the smooth latent space for structure search and parameter refinement.\nExperiments show that the proposed method is superior to the strong baseline in accuracy and efficiency on multiple classes of PDE, and has the advantages of data independence, interpretability, and scalability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes A method to obtain the analytical solution of PDE (although there is more than one such work, for example, Closed-form Solutions: A New Perspective on Solving Differential Equations). Unlike previous methods, the generated expressions are \"mathematically valid and physically meaningful\".\n\n\n2. SIGS seems to give good results"}, "weaknesses": {"value": "1. I think the biggest innovation of this paper is that it is possible to obtain an analytical solution of a PDE, but unfortunately, this idea has been previously worked on by many people, such as Closed-form Solutions: A New Perspective on Solving Differential Equations. And this article doesn't even cite that article!\n\n2, it is mentioned that the generated expressions are \"mathematically valid and physically meaningful\", which is not new in the field of symbolic regression and has also been well solved.\n\n3. It is felt that this method is a combination of existing methods and is not too innovative."}, "questions": {"value": "Q1: Please analyze how your way of making expressions \"mathematically valid and physically meaningful\" differs from previous symbolic regression methods, and what are your innovations and advantages?\n\nQ2: As you mentioned in your article, many of the expressions obtained by previous GP and RL-based methods are invalid, but previous symbolic regression methods impose constraints and handle them, and work well. So I don't think your assumption is meaningful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8pUKuz9H2Q", "forum": "oOSEF3TVb3", "replyto": "oOSEF3TVb3", "signatures": ["ICLR.cc/2026/Conference/Submission17518/Reviewer_9NH9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17518/Reviewer_9NH9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896693496, "cdate": 1761896693496, "tmdate": 1762927397542, "mdate": 1762927397542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SIGS (Symbolic Iterative Grammar Solver), which is a neuro-symbolic framework that discovers analytical solutions to differential equations by combining symbolic grammars with deep latent-space optimization. The approach constructs candidate functional forms (Ansatze) using a formal grammar of atoms (e.g., polynomials, trigonometric functions, exponentials), which are then embedded into a continuous manifold using a Topological Grammar Variational Autoencoder (TGVAE). This enables smooth optimization of symbolic expressions through gradient-based search while maintaining symbolic interpretability. The method is validated on several canonical PDE families (Burgers, Poisson, Schrodinger, and wave equations). SIGS achieves exact or near-exact recovery of ground-truth analytical solutions, outperforming symbolic regression and hybrid neural PDE solvers. It also shows robust performance under noise and demonstrates interpretability advantages over black-box neural solvers."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This is an innovative and well-executed paper that pushes the boundary of symbolic and neural hybrid modeling. The central idea of representing symbolic solution structures through a grammar-based latent manifold is both original and conceptually elegant. The method bridges symbolic regression and deep learning by introducing a structured, differentiable search space, allowing neural optimization to operate over interpretable symbolic forms. The theoretical framing is rigorous, with clearly defined grammar rules, topology-preserving constraints in the TGVAE, and a solid justification for why the latent manifold preserves functional equivalence among expressions. The methodological novelty lies in using the grammar-VAE coupling to enable continuous symbolic optimization, which is a meaningful advance over existing neuro-symbolic PDE solvers. Empirically, the results are comprehensive and convincing. SIGS is benchmarked across multiple PDE categories, with both analytical recovery and quantitative accuracy metrics. The figures and tables are clear, and the visual comparison between recovered and ground-truth solutions is compelling. Ablation studies and timing analyses add credibility to the claims. The writing and presentation are polished and well-organized; complex ideas are explained with appropriate examples, and the motivation and related work sections are thorough. Overall, the paper is a strong contribution that demonstrates how neuro-symbolic methods can recover physically meaningful, interpretable solutions to PDEs."}, "weaknesses": {"value": "While the contribution is conceptually strong, several aspects could be improved for clarity and generalization. First, the scalability of SIGS to higher-dimensional or more chaotic systems remains unproven. The method depends on handcrafted grammars and pre-specified Ansatz templates, requiring substantial domain expertise to design. This semi-manual setup could limit its practical use for complex, real-world systems where the appropriate functional vocabulary is unknown. Second, although the authors claim computational efficiency, the runtime and scaling analysis is only qualitatively discussed. A more detailed comparison of times, gradient steps, and scaling with grammar size would better substantiate the efficiency claim. Additionally, the method is evaluated mostly against symbolic or PINN-type baselines, not against neural operator architectures such as FNO or DeepONet, which are now standard references in PDE learning. Including such comparisons would help contextualize the performance advantage. Finally, the technical presentation, while mathematically correct, is dense in sections describing the TGVAE regularization and grammar construction. These could be made more intuitive with small worked examples illustrating how grammar generation and latent search interact. Despite these issues, the weaknesses are primarily about scope and exposition, not correctness."}, "questions": {"value": "1) How sensitive is SIGS to the choice of grammar primitives or Ansatz structure? Could an incorrect or incomplete grammar prevent the discovery of correct solutions?\n\n2) Have you tested the method on approximate analytical solutions or PDEs with no closed form, where SIGS might produce interpretable approximations?\n\n3) Can the framework scale to parameterized PDE families or higher-dimensional systems, and if so, how does the latent search complexity grow with grammar size?\n\n4) How would SIGS compare with Neural Operators (FNO, DeepONet) in accuracy and efficiency for continuous solution families?\n\n5) Could the symbolic latent space be combined with physical constraints or PINN losses to enable hybrid symbolic–numeric discovery?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IuJC26B9nX", "forum": "oOSEF3TVb3", "replyto": "oOSEF3TVb3", "signatures": ["ICLR.cc/2026/Conference/Submission17518/Reviewer_qGHk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17518/Reviewer_qGHk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938431282, "cdate": 1761938431282, "tmdate": 1762927396952, "mdate": 1762927396952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a formal grammar-based approach for discovering analytic solutions of PDEs. The authors use a pre-trained variational autoencoder trained on expressions that fit a certain ansatz. The latent space of the autoencoder is then used to search for solutions fitting the PDE.\n\nWhile a potentially interesting methodology for symbolic regression, I am not convinced by the chosen application and experiments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors propose a conceptually interesting approach to incorporating inductive bias, in terms of the chosen ansatz, into the grammar-based search using the latent space of an autoencoder."}, "weaknesses": {"value": "First, it is not at all clear what the real-world application is for this kind of analytic solution discovery. Outside of linear PDEs, most PDEs do not admit analytic solutions except in very simple geometries/boundary conditions.\n\nRelatedly, most of the PDEs studied in this work are linear PDEs with very well-understood general solutions. There is no need for symbolic discovery for these linear PDEs. The only nonlinear PDE considered here is Burgers equation, and the authors had to include the relevant basis function (tanh) to handle that equation, which makes the solution trivial.\n\nRegarding the claim and experiments with approximate solutions, it is not clear what advantage this approach has over simply looking at the numerical solution. For a fast approximate solution, you could have achieved the same result by just choosing to expand the solution in a particular basis set and fitting the linear combination of basis elements."}, "questions": {"value": "1. What applications do you see this kind of method being used for?\n2. What advantage can you demonstrate for this approach over classical analytic methods?\n3. Given the simplicity of the discovered solutions (especially for the one nonlinear PDE), it is not clear what role the main contribution of the paper is playing. Why not naively restrict to the form of the given ansatz during a direct search rather than using a latent space?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z6tkWM9OZQ", "forum": "oOSEF3TVb3", "replyto": "oOSEF3TVb3", "signatures": ["ICLR.cc/2026/Conference/Submission17518/Reviewer_1nRi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17518/Reviewer_1nRi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023792965, "cdate": 1762023792965, "tmdate": 1762927396443, "mdate": 1762927396443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}