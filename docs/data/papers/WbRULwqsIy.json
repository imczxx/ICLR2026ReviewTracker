{"id": "WbRULwqsIy", "number": 14612, "cdate": 1758239980184, "mdate": 1763684554655, "content": {"title": "Understanding In-Context Learning on Structured Manifolds: Bridging Attention to Kernel Methods", "abstract": "While in-context learning (ICL) has achieved remarkable success in natural language and vision domains, its theoretical understanding—particularly in the context of structured geometric data—remains unexplored. This paper initiates a theoretical study of ICL for regression of H\\\"older functions on manifolds. We establish a novel connection between the attention mechanism and classical kernel methods, demonstrating that transformers effectively perform kernel-based prediction at a new query through its interaction with the prompt. This connection is validated by numerical experiments, revealing that the learned query–prompt scores for H\\\"older functions are highly correlated with the Gaussian kernel. Building on this insight, we derive generalization error bounds in terms of the prompt length and the number of training tasks. When a sufficient number of training tasks are observed, transformers give rise to the minimax regression rate of H\\\"older functions on manifolds, which scales exponentially with the intrinsic dimension of the  manifold, rather than the ambient space dimension. Our result also characterizes how the generalization error scales with the number of training tasks, shedding light on the complexity of transformers as in-context kernel algorithm learners. Our findings provide foundational insights into the role of geometry in ICL and novels tools to study ICL of nonlinear models.", "tldr": "", "keywords": ["In-Context Learning", "Transformer Approximation Theory", "Kernel Regression on Manifold"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1460d0fedc832b76954a68218e3e9327657e09ce.pdf", "supplementary_material": "/attachment/33e01c0dddbf6329fb61c078d0b89208fd92ab37.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the theoretical properties of in-context learning when applied to data residing on structured manifolds with a low intrinsic dimension. The authors begin by presenting an expressiveness result, demonstrating that a Transformer can utilize $O(n)$ hidden tokens to approximate a kernel smoothing operation. Building on this constructive proof, the paper derives a generalization bound for a function class of a comparable size to the Transformer construction. This bound is intended to clarify the role of the manifold's intrinsic dimension $d$ in the generalization of ICL. Finally, the paper includes synthetic experiments to validate these theoretical findings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper frames an interesting and relevant problem: the theoretical analysis of in-context learning on high-dimensional data that possesses a low-dimensional intrinsic manifold structure. This setup is well-motivated and timely given current data regimes.\n\n2. The theoretical claims regarding expressiveness and generalization are supported by corresponding synthetic experiments, which adds to the paper's completeness.\n\n3. The paper is generally clearly written and well-organized."}, "weaknesses": {"value": "1. The paper's discussion of novelty could be strengthened. The connection between attention mechanisms and kernel smoothing has been explored in prior work (e.g., [1, 2]). It would be beneficial for the authors to more clearly articulate the novelty of their specific formulation in light of these existing results.\n\n2. The central claim regarding the generalization bound's (Eq. 15) dependence on the intrinsic dimension $d$ and ambient dimension $D$ requires further clarification.\n\n   - Firstly, the claim that the bound scales exponentially with $d$ is difficult to verify, as the constants $C_1$ and $C_2$ are also noted to be dependent on $d$. To make this claim more concrete, it would be helpful to explicitly characterize the dependency of $C_1$ and $C_2$ on $d$.\n\n   - Secondly, the bound's polynomial dependence on the ambient dimension ($D^3$) seems to obscure the intended message about the intrinsic dimension $d$. A key motivation for manifold-based analysis is often to obtain bounds that depend primarily on $d$ and are less sensitive to $D$. As written, the significance of the $D^3$ term on the bound's practical implications is unclear.\n\n3. The theoretical setup, which relies on $l = O(n)$ hidden tokens for the expressiveness result, appears to differ from the standard ICL paradigm, where new outputs are typically generated without a set of internal \"hidden tokens\". The authors could provide a clearer justification for this modeling choice and discuss its implications for the relevance of the derived bounds to standard Transformer-based ICL.\n\n[1] Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel. https://arxiv.org/abs/1908.11775.\n\n[2] Implicit Kernel Attention. https://arxiv.org/abs/2006.06147."}, "questions": {"value": "1. Could the authors provide a more explicit characterization of the generalization bound (Eq. 15)? Specifically, how do the constants $C_1$ and $C_2$ depend on the intrinsic dimension $d$, and how should the $D^3$ term be interpreted in the context of a manifold-aware analysis?\n\n2. What are the theoretical implications of the $l = O(n)$ hidden token requirement? Does the analysis hold for a more standard setting, such as $l = 0$? A clearer justification for the role of these hidden tokens and their necessity for the expressiveness result would be valuable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H7c8pE9tAS", "forum": "WbRULwqsIy", "replyto": "WbRULwqsIy", "signatures": ["ICLR.cc/2026/Conference/Submission14612/Reviewer_iBpf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14612/Reviewer_iBpf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659933335, "cdate": 1761659933335, "tmdate": 1762924994330, "mdate": 1762924994330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a bridge between Transformer attention mechanisms and classical kernel methods in in-context learning (ICL) over manifold-structured data. The authors prove that a Transformer can exactly implement Gaussian kernel regression and establish an approximation error bound for α-Hölder functions on compact Riemannian manifolds, which demonstrates dependence on intrinsic (manifold) rather than ambient dimension. They further validate their claims through simulations, observing high correlations between attention weights and Gaussian kernels."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper aims to establish connection between attention scores and kernel estimators (Nadaraya-Watson). This yields a new perspective on Transformers as in-context kernel learners, which conceptually advances and generalizes beyond prior linear-model analyses.\n\n+ The construction of Lemma 1 remarking that a Transformer network that exactly realizes kernel regression with zero approximation error is nontrivial. The explicit architectural specification further concretizes its theoretical claim.\n\n+ Theorem 1’s bound  separates the task-level and prompt-level effects, showing minimax-optimal scaling with intrinsic dimension d."}, "weaknesses": {"value": "- The theoretical results rely on exact kernel implementation via attention and perfect manifold sampling. These assumptions may obscure how robust the results remain under approximate or noisy conditions typical in practice.\n\n- Some key proof components, particularly how the softmax attention with masking produces the Gaussian weights in Eqs. (12) - (14) are deferred to appendices. The main text should outline at least the constructive steps more transparently for better readability.\n\n- While the simulation results illustrate correlation trends between attention scores and Gaussian kernels, the experiments are synthetic and low-dimensional (S² sphere). There is no ablation on real-world datasets or on deviations when the model is not perfectly aligned with the Gaussian kernel hypothesis."}, "questions": {"value": "1. On Remark 1 (Universality), the lemma asserts that the same parameterization works for all f and {x_i}. Could the authors clarify how positional encodings and attention masks encode x_i-dependent interactions without weight adaptation? Does this require scaling of input normalization with h or b?\n\n2. The generalization rate depends on the kernel bandwidth h. How is it chosen or adapted across prompts/tasks? Is it optimized implicitly by the Transformer parameters, or assumed known? The statistical rate in Proposition 1 seems sensitive to this.\n\n3. The analysis is limited to α-Hölder smooth functions, but why is it required? Could the framework be extended to broader classes such as Besov or Sobolev spaces, or to non-compact manifolds? Can you discuss how would that alter the covering number and rate derivations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ubg7hOXyvb", "forum": "WbRULwqsIy", "replyto": "WbRULwqsIy", "signatures": ["ICLR.cc/2026/Conference/Submission14612/Reviewer_WAPE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14612/Reviewer_WAPE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761694462878, "cdate": 1761694462878, "tmdate": 1762924993875, "mdate": 1762924993875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper theoretically studies in-context learning on structured data represented as manifolds pertaining to certain assumptions. They start with an explicit construction of a transformer that can perform Gaussian kernel regression without any error. They also give a theoretical framework for generalizing this result by analyzing transformer-based ICL to Reimannian manifolds, and give bounds on the error which are tighter than existing ones. Subsequently, they verify the theoretical claims with experiments that show that the bounds are indeed as one would expect."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- This is an important direction which theoretically studies theoretical representational strengths of transformer-based ICL, and relates it to the role of geometry of the data.\n- The approximation bounds provided are tight.\n- This will have several scopes for future work, where learning techniques can be studies using similar methods to relate to the structure of the data.\n- The proofs are very nicely structured despite being long, which makes them easy to understand."}, "weaknesses": {"value": "- Since the main purpose of the work is to provide a theoretical analysis, more emphasis should be given to the results and the proof techniques in the main part of the paper. Especially a proof/technique overview along with the first part of Section C would make more sense.\n- Section 3- Please define the input tokens more precisely, for e.g. we are given with (n+1) tokens where the first n tokens contain (x_i, y_i) and the last token contains x_{n+1}, we expect the final output to be present in a certain token etc. Also is \\Gamma the batch size? I thought later it is used as the size of the training data.\n- The constructed transformer has 5 layers, it would be helpful to give an intuitive understanding of what those layers perform.\n- The paper assumes some background on manifolds, but it would be better to explain the concepts more elaborately, especially in Sec B.1."}, "questions": {"value": "- Line 790: What is P_f?\n- What are hidden tokens in Def 3? And how are they represented in the input?\n- Will the construction change if softmax was used as an activation in all the layers instead of ReLU? It would be nice to know what went wrong."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l7w5IrYTKi", "forum": "WbRULwqsIy", "replyto": "WbRULwqsIy", "signatures": ["ICLR.cc/2026/Conference/Submission14612/Reviewer_Sc1b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14612/Reviewer_Sc1b"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916806519, "cdate": 1761916806519, "tmdate": 1762924993369, "mdate": 1762924993369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes in-context learning (ICL) for regression of Hölder functions on a compact d-dimensional Riemannian manifold. It establishes a connection between the attention mechanism and kernel methods, showcasing that attention interactions can be seen as regression kernels. The authors then construct a transformer for kernel regression. Building on this construction, the authors derive a generalization bound for transformer learnability and statistical difficulty. \n\nI would like to not that I am not an expert on this topic (especially for proof details), and will defer to other reviewers for more in-depth judgement, and put a low confidence on my review as an indication of this."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is well-written and generally easy to follow with novel contributions.\n- Lemma 1, where the authors explicitly construct a 5-block, multi-head transformer that exactly equals the Gaussian Nadaraya–Watson estimator on the prompt (no approximation error), is easy to follow and clearly makes the connection between the styles of methods and regression. \n-The task-/prompt-level generalization analysis is interesting and very relevant for the field. The decomposition isolates (a) learning the algorithm across tasks and (b) per-prompt nonparametric difficulty on manifolds."}, "weaknesses": {"value": "- The assumptions for the generalization error bound in Section 5 are quite idealised and strong. For example, the assumption that $p_x$ is uniform is unrealistic, but understandable for the work. A discussion of where this assumption might hold and where this framework may break down due to these assumptions would be helpful to add to the Appendix. \n- Although the paper's main contributions are the theoretical bounds and connection to kernel methods, the empirical experiments are fairly weak to show the practicality of the kernel-attention connection. As such, the paper is mainly a conceptual contribution.\n- The network class of $O(Dn)$ heads and length $2n+1$ is computationally heavy for any realistic $n$ and $D$ with a larger dataset and prompt. No efficiency argument or parameter reduction is provided, and no experiments report compute or runtime scaling. Therefore, the construction serves more as a conceptual bridge than as a scalable algorithm. As such, this makes the connections to typical LLM configurations weaker, and difficult to understand when practically useful. It may be helpful to provide a compute/memory profile for the constructive transformer and show that trained practical architectures (far fewer heads) empirically approach the same behavior. Larger context lengths in Table 1 would also help to clarify this.\n- Additionally, the constructed architecture is a synthetic transformer for kernel simulation, not an explanation of why actual transformers implement kernel regression. Without ablation experiments showing convergence toward this construction under realistic constraints (fewer heads, smaller width, learned parameters), the claim of “bridging attention and kernels” remains largely conceptual."}, "questions": {"value": "- I understand that this paper differs from Kim et al (2024) in the network size and exponential rate, but why is a comparison to their baseline not provided then, if it is the closest paper in the field for the empirical results?\n- Why do the p-values have very large standard deviations in Table 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gXCxEM2fN9", "forum": "WbRULwqsIy", "replyto": "WbRULwqsIy", "signatures": ["ICLR.cc/2026/Conference/Submission14612/Reviewer_pVF9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14612/Reviewer_pVF9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762831951409, "cdate": 1762831951409, "tmdate": 1762924992786, "mdate": 1762924992786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}