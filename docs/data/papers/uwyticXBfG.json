{"id": "uwyticXBfG", "number": 16662, "cdate": 1758267390538, "mdate": 1759897226297, "content": {"title": "ChessArena: A Chess Testbed for Evaluating Strategic Reasoning Capabilities of Large Language Models", "abstract": "Recent large language models (LLMs) have shown strong reasoning capabilities. However, a critical question remains: do these models possess genuine reasoning skills—particularly complex strategic reasoning—or are they primarily excelling at sophisticated pattern recognition within their training data? To address this question, this paper presents a chess testbed, ChessArena, to evaluate the strategic reasoning capabilities of LLMs. Chess requires complex strategic reasoning capabilities including long-term planning, strict rule comprehension, and multi-turn conversation memorization. Specifically, ChessArena is a competitive framework where LLMs play against each other, under four different play modes. The testbed is equipped with a ranking algorithm and a leaderboard. The testbed can also evaluate fine-grained capabilities including basic understanding, move selection, and puzzle solving. Over 13 LLMs with different modes are evaluated in ChessArena, playing over 800 games. The results reveal significant shortcomings in current LLMs: no model can beat Maia-1100 (a chess engine at human amateur level), while some even failed to defeat a random player that selects moves arbitrarily. We also present a strong baseline to the testbed: our fine-tuned Qwen3-8B substantially improved performance, approaching much larger state-of-the-art reasoning models.", "tldr": "Evaluating strategic reasoning capabilities of large language models through a chess testbed.", "keywords": ["Large Language Models", "Benchmark", "Chess", "Strategic Reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b52ddc43f8c9f5dc628671c5152a23fc7e38c028.pdf", "supplementary_material": "/attachment/36bb41d18a2ef42e377ddeaa76725bd212e0c976.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces ChessArena, a competitive platform where large language models play chess against each other to evaluate their strategic reasoning capabilities. The framework supports four play modes (Bullet, Blitz, Standard, Blindfold), uses a Glicko rating system with optimized competition sampling, and includes three fine-grained evaluation tasks (basic understanding, move selection, puzzle solving). The authors evaluate 13+ LLMs through 800+ games and find that no model defeats Maia-1100, a chess engine at human amateur level. They also present Qwen3-8B-Chess, trained via supervised fine-tuning and GRPO reinforcement learning, showing improved performance on chess tasks and some transfer to logical reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper demonstrates careful experimental design with systematic evaluation across multiple state-of-the-art models and substantial computational investment in running hundreds of games. The competition sampling algorithm is mathematically grounded with clear derivations in the appendix. The fine-grained evaluation tasks successfully isolate different aspects of chess ability, and the diagnostic value of distinguishing format errors from illegal moves from suboptimal legal moves is useful for understanding model failures. The commitment to open-source the platform, competition data, and trained model would benefit the community. The finding that thinking models generally outperform non-thinking models is interesting, and the observation that providing legal moves can paradoxically reduce analytical depth (Appendix G.3) merits further investigation."}, "weaknesses": {"value": "The fundamental issue is the mismatch between method choice and task nature. Chess is a sequential decision problem where strategic value emerges from multi-move combinations—classic tactics like \"sacrifice for positional advantage\" require evaluating positions 3-5 moves ahead. Yet the paper uses single-step RL, sampling board states independently and scoring individual moves via Stockfish. This design cannot learn sequence-level strategy: a move appearing unfavorable now (e.g., sacrificing material) may lead to checkmate later, but receives negative reward under single-step evaluation. The paper provides no explanation for not using PPO (supporting full trajectory learning) or MCTS (the gold standard in chess AI), nor discusses why self-play was not explored despite the competition framework already supporting model-vs-model games.\nThe GRPO choice lacks justification. The paper cites its success on mathematical reasoning, but math verification is single-step (answer correctness), whereas chess evaluation is inherently sequential (outcome of move sequences). Reducing chess to \"selecting from candidates\" obscures genuine strategic reasoning—generating and evaluating complex move sequences. Experimental data confirms this: models perform significantly better with legal move lists provided (MAR +41.1% vs -1.2%), indicating they learn ranking rather than reasoning.\nThe reward design further exposes methodological flaws. Using binary top-3 indicators discretizes Stockfish's continuous evaluation (0-100% win rate) into {0,1}, losing nuanced quality distinctions. Moves with +50% and +48% win rates receive identical rewards, eliminating incentive for fine-grained judgment. The 56,000 training samples are called \"virtually unlimited,\" yet chess has ~10^43 legal positions—this scale is negligible in the search space.\nThe Blindfold mode claims to test multi-turn reasoning, but this is a false premise in perfect information games. The current FEN string fully encodes all decision-relevant information; move history is theoretically redundant. Providing \"the last 10 moves to prevent fivefold repetition\" actually reveals models cannot accurately parse FEN representations—this tests format parsing, not strategic memory. Case studies (Table 15) confirm: thinking models fail at board reconstruction, non-thinking models use shortcuts based on recent moves, neither demonstrates genuine state reasoning.\nGeneralization experiments lack basic controls. Table 16 shows ZebraLogic improves 17.6%, but six of eight benchmarks decline, with code tasks systematically degrading. The paper assumes chess training transfers strategic reasoning, but fails to exclude alternative explanations: the format reward in RL training (ε_f=0.1) itself incentivizes structured outputs; the \"Step 1-6\" framework and \"Verification\" steps appearing after training (Table 17) may purely result from format constraints rather than chess knowledge. Essential ablation experiments—RL training on equivalent-scale math or logic problems as controls—are completely absent."}, "questions": {"value": "The adoption of single-step RL over sequential decision methods requires explicit justification. Standard chess AI (AlphaZero, Leela Chess) relies on MCTS to evaluate future move trees, while algorithms like PPO naturally support learning from complete game trajectories. Your 56,000 board states are extracted from Lichess games, which inherently provide full (state, action, next_state, ..., outcome) sequences. Why reduce these to independent single-step samples? If the technical reason is that GRPO implementation is simpler, can you provide experimental evidence that single-step methods are sufficiently effective for chess? If the conceptual reason is that single-step evaluation captures chess strategy, this contradicts 50 years of domain research (from minimax to MCTS) and requires strong argumentation.\nThe absence of self-play is puzzling. You have implemented a complete competition framework supporting any two models playing against each other—precisely the infrastructure needed for self-play. The standard pipeline is: SFT initialization → self-play RL → iterative improvement. The paper stops at SFT, training only on static Lichess positions. Can you clarify whether self-play was considered? If the issue was weak base models (Qwen3-8B's >70% illegal move rate), did you attempt bootstrapping with high-quality model (e.g., GPT-4.1) game data before transitioning to self-play? The current approach relies on Stockfish-labeled static data, limiting exploration space—the model can never discover novel strategies Stockfish hasn't evaluated.\nCan the reward function be continuous? Stockfish outputs precise win-rate evaluations (e.g., +0.5 for slight advantage, +3.0 for decisive advantage), but the paper binarizes this to top-3 indicators. This means the 3rd and 4th ranked moves (potentially differing by 0.1% win rate) receive completely different rewards (1 vs 0), while the 1st and 3rd ranked moves (potentially differing by 5%) receive identical rewards. Sparse rewards are known to reduce RL training efficiency—why not use reward = normalize(stockfish_score)? If concerned about continuous reward scaling, were rank-based continuous rewards attempted, such as reward = 1 - rank/num_legal_moves?\nGeneralization experiments need critical controls. The current design compares chess RL vs no RL, but cannot distinguish chess-specific effects from general RL effects. Suggested ablations: (1) GRPO training on 56k math reasoning problems, test on ZebraLogic; (2) GRPO training on 56k code problems, same test; (3) training on mixed-domain samples. If (1)(2) also show similar ZebraLogic improvements, chess's specificity is questionable. Additionally, can the case study in Table 17 be expanded? Currently only one success case exists—we need to know: (a) qualitative differences between success/failure cases before/after training on ZebraLogic; (b) whether improvements systematically manifest as better constraint tracking and logical chain completeness, or are random; (c) whether counterexamples exist where outputs become more structured but answers remain incorrect.\nHow do you explain the performance degradation from SFT-Stage2 (6/8 benchmarks decline)? This model becomes the RL starting point, meaning final improvements may partially be \"repairing SFT damage\" rather than \"exceeding baseline.\" Can you provide a direct Base → Base+RL comparison, skipping SFT-Stage2? Or explain why chess domain data (distilled from GPT-4.1, etc.) so severely harms code generation ability (CruxEval 73.25→68.0)? This is important for understanding the cost of domain adaptation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3qdrfJ6eZK", "forum": "uwyticXBfG", "replyto": "uwyticXBfG", "signatures": ["ICLR.cc/2026/Conference/Submission16662/Reviewer_UYks"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16662/Reviewer_UYks"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761506947974, "cdate": 1761506947974, "tmdate": 1762926721512, "mdate": 1762926721512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ChessArena, a benchmark framework for evaluating the strategic reasoning capabilities of large language models (LLMs) using the game of chess. It proposes several play modes (Bullet, Blitz, Standard, and Blindfold) and fine-grained subtasks (basic rule understanding, move selection, and puzzle solving). The authors run extensive comparisons across over a dozen LLMs and also fine-tune Qwen3-8B to improve chess-related reasoning. Results suggest that current LLMs perform poorly in chess compared to even simple rule-based engines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper provides a comprehensive benchmark, including several models and scoring metrics.\n2.The experiments cover a broad range of LLMs under consistent conditions, providing useful insight for current model weaknesses in symbolic game reasoning."}, "weaknesses": {"value": "1.The core idea that testing LLM through chess is not a novel concept, as there are already multiple earlier works like AlphaZero.\n2.The author does not provide a clear and comprehensive analysis of why the model failed in chess.\n3.Only fine-tuning on Qwen3-8B and showing the not-obvious gains.\n4.Statements like “ChessArena reveals the reasoning limits of LLMs” are overreaching given that the benchmark measures a very narrow skill."}, "questions": {"value": "1.Could the author comprehensively state the difference between your method and the current other game arena and general reasoning benchmark?\n2.What qualitative reasoning patterns emerge from “thinking-enabled” models (O3, Gemini-Pro)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vxzylUnHG4", "forum": "uwyticXBfG", "replyto": "uwyticXBfG", "signatures": ["ICLR.cc/2026/Conference/Submission16662/Reviewer_4csM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16662/Reviewer_4csM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762070428604, "cdate": 1762070428604, "tmdate": 1762926720796, "mdate": 1762926720796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ChessArena is a unified testbed for measuring LLMs’ strategic reasoning through full chess games, with a built-in Glicko ranking system and four play modes (Bullet, Blitz, Standard, Blindfold) to probe speeded decision-making, long-horizon memory, and rule following. The platform also includes fine-grained diagnostics (basic board understanding, single-move quality vs. engine, and multi-step puzzles). In a large evaluation of 13+ LLMs over 800+ games, no model beats the amateur-level engine Maia-1100, and some even fail to outperform a random legal-move baseline, revealing persistent weaknesses in legality/format compliance, tactical selection, and long-range consistency."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Four complementary play modes (Bullet/Blitz/Standard/Blindfold) isolate different capabilities (speeded decision-making, reasoning allowance, and memory-only board reconstruction in Blindfold), which is an original capability-oriented decomposition.\n2. Clear, interoperable interfaces (FEN for state; UCI/SAN for moves), scalable competition pipeline, and engine tooling (Stockfish for analysis; Maia-1100 and a Random baseline as anchors) make the setup robust and reproducible."}, "weaknesses": {"value": "1. The arena evaluates only chess, so it is unclear how results transfer to broader strategic or multi-modal settings.\n2. The paper contrasts models with human/engine anchors, but the human participant N, expertise, and variance are not clearly reported in the visible sections, limiting interpretation of the “gap.”\n3. Post-training is validated on only a very small set of base models (Qwen3-8B only), making it hard to support a claim of general effectiveness. The observed gains may stem from family-specific characteristics or scale effects, rather than representing a stable benefit of the training paradigm itself."}, "questions": {"value": "1. In Bullet/Blitz/Standard/Blindfold, how much of the gap is due to token budget or output-format differences? Please add a format-invariant comparison (e.g., fixed JSON “move” field with CoT stored separately) and report legality/format error rates as nuisance variables.\n2. Will you release pairing logs, instance IDs, and trace-to-metric scripts (legality, top-move, rating computation), plus a minimal Docker? A short leaderboard policy (submission format, anti-overfit checks) would encourage adoption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "quCeLyZAeT", "forum": "uwyticXBfG", "replyto": "uwyticXBfG", "signatures": ["ICLR.cc/2026/Conference/Submission16662/Reviewer_TdZ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16662/Reviewer_TdZ7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762089154677, "cdate": 1762089154677, "tmdate": 1762926720012, "mdate": 1762926720012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}