{"id": "iSEN8irLUD", "number": 24546, "cdate": 1758357851825, "mdate": 1759896761058, "content": {"title": "Dual Distillation of Trajectory and Guidance Knowledge for Faster Inference in Conditional Masked Diffusion Language Models", "abstract": "Masked diffusion language models (MDLMs) have emerged as a promising generative framework for natural language, owing to parallel non-autoregressive generation capabilities with iterative unmasking/denoising. However, typical MDLMs require a very large number of neural network function evaluations for effective inference, making them computationally expensive in many real-world NLP applications that rely on conditional sequence-to-sequence generation. In this work, we propose a two-stage distillation method for conditional MDLMs that distills knowledge of (i) classifier-free guidance as well as (ii) unmasking trajectory from the existing teacher MDLM into a student MDLM. This allows the student MDLM, during inference, to (i) reduce two forward passes, required by a classifier-free guided (teacher) MDLM, to a single pass, and (ii) drastically reduce the number of unmasking steps. In this way, by dual distillation of guidance and trajectory knowledge, our MDLM achieves speedups of up to 16$\\times$ while virtually retaining the quality of generation.", "tldr": "We propose a two-stage distillation approach for conditional masked diffusion language models (MDLMs) applied to seq-to-seq NLP tasks that tackles sampling inefficiencies in computing guided and multi-step denoising outputs from MDLMs.", "keywords": ["Diffusion Language Models", "Knowledge Distillation", "sequence-to-sequence NLP", "non-autoregressive generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/50f88496ca691cfcad6109a8c42c5f5e613d573e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a two-stage distillation method to address the high inference cost of conditional MDLMs. Evaluated on multiple tasks, the approach achieves up to $16\\times$ speedup ($2\\times$ from guidance distillation and up to $8\\times$ from trajectory distillation) while maintaining or improving output quality. Qualitative examples indicate that the distilled model performs markedly better in low-step regimes, producing more coherent outputs with fewer denoising steps. The authors further suggest that the framework can be extended to multimodal MDLMs and scaled to larger architectures, potentially broadening its applicability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow, with clear explanations of the two-stage distillation methodology and comprehensive algorithmic descriptions.\n\n2. Under the authors' experimental settings, the two-stage approach achieves substantial speedup across multiple standard benchmarks while maintaining generation quality. The qualitative analysis with generated examples further supports this claim."}, "weaknesses": {"value": "1. Limited Model Scale and Task Scope. Experiments are conducted only on a 113M parameter model. Scaling to larger MDLMs such as LLaDA [1] or Dream [2] remains unverified, despite the authors' claim that the framework can be extended to larger architectures. Furthermore, evaluation is restricted to relatively simple sequence-to-sequence tasks. Generalization to other conditional generation tasks (e.g., mathematical and code generation) is unclear. If the authors could provide supplementary results demonstrating the framework's acceleration potential on larger models and on challenging benchmarks (e.g., GSM8K [3], MATH [4], HumanEval [5]), the scalability claims would be significantly more convincing.\n\n2. The claims regarding CFG acceleration are questionable. As shown in Table 1 of SMDM [6], the model can still achieve good results without using CFG. In the authors' claimed $16\\times$ speedup, $2\\times$ comes from stage-one guidance distillation. However, for the vanilla fine-tuned teacher model, using CFG may not be necessary. If the authors could provide evaluation results without CFG, it would further confirm the necessity of using CFG and better demonstrate the utility of guidance distillation.\n\n\n\n[1] Nie et al., Large Language Diffusion Models.\n\n[2] Ye et al., Dream 7B: Diffusion Large Language Models.\n\n[3] Cobbe et al., Training Verifiers to Solve Math Word Problems.\n\n[4] Hendrycks et al., Measuring Mathematical Problem Solving With the MATH Dataset.\n\n[5] Chen et al., Evaluating Large Language Models Trained on Code.\n\n[6] Nie et al., Scaling up Masked Diffusion Models on Text."}, "questions": {"value": "1. The paper states that for guidance distillation, $\\gamma_{\\min} = 1$ and $\\gamma_{\\max} = 3$ are chosen, and results are reported for $\\gamma = 1.4$ and $\\gamma = 2.0$. What is the rationale behind these specific choices? Additionally, in Table 2, each task uses different initial step sizes and total numbers of rounds. How were these values determined, and why is a unified set of hyperparameters not used across all tasks?\n\n2. Could the authors provide a comparison with the concurrent work D2F [1]? Such a comparison would help contextualize the contributions of the proposed approach.\n\n[1] Wang et al., Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FFgzm94H9J", "forum": "iSEN8irLUD", "replyto": "iSEN8irLUD", "signatures": ["ICLR.cc/2026/Conference/Submission24546/Reviewer_Ag1V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24546/Reviewer_Ag1V"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760517612043, "cdate": 1760517612043, "tmdate": 1762943118898, "mdate": 1762943118898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-stage knowledge distillation framework for accelerating inference in conditional MDLMs on sequence-to-sequence NLP tasks. The first stage CFG into a single forward pass, eliminating the computational overhead of computing both conditional and unconditional distributions. The second stage progressively distills multi-step denoising into fewer steps through self-distillation. The approach achieves up to 16× speedup (2× from guidance distillation, 8× from trajectory distillation) while maintaining generation quality across three tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses genuine computational bottlenecks in MDLMs—both the dual forward passes required by CFG and the multi-step generation process. These are real inefficiencies affecting practical deployment. And efficiency of the advantage of MDLM over AR.\n2. The two-stage distillation approach is straightforward and well-explained. The architectural modification (adding a guidance scale embedding) for encoding guidance preferences is simple and practical."}, "weaknesses": {"value": "1. While aims to improve the sampling efficiency of MDLM, this work only presents experiments with three specific conditional text generation, unlike many existing work that address the unconditional generation and more challenging tasks.\n2. Limited technical novelty: The paper essentially applies existing techniques (guidance distillation from Meng et al. 2023, progressive distillation from Salimans & Ho 2022 and the discrete progressive distillation SDTT from Deschenaux & Gulcehre 2025) to conditional MDLMs. While the adaptation is competent, the core methodological contribution is incremental. The authors acknowledge this is an extension of prior image diffusion work but position it as the \"first\" for guided conditional MDLMs on seq-to-seq tasks, which is a narrow novelty claim.\n3. There are many missing baselines in the direction of KV cache and faster sampler for MDLM"}, "questions": {"value": "$q_0$ is missing in the expectation in eq(3)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7YIePrAh4n", "forum": "iSEN8irLUD", "replyto": "iSEN8irLUD", "signatures": ["ICLR.cc/2026/Conference/Submission24546/Reviewer_y9eZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24546/Reviewer_y9eZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760904315749, "cdate": 1760904315749, "tmdate": 1762943118708, "mdate": 1762943118708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the high computational cost of conditional masked diffusion language models (MDLMs) with distillation techniques. MDLMs have the advantage of parallel (non-autoregressive) generation but require running many denoising (unmasking) steps during inference, and the classifier-free guidance (CFG) procedure even doubles the number of network evaluations. The paper proposes a two-stage distillation algorithm for conditional MDLMs. In the first stage, a student MDLM, which takes the guidance strength $\\gamma$ as an extra input, is trained to emulate the output of a CFG-guided teacher. In the second stage, the guidance-distilled student model from Stage 1 is then used as the initial teacher for a series of progressive distillation steps, where each step trains a new student to distill two teacher unmasking steps into a single step. Empirical results demonstrate that the final student model achieves accelerated inference speed while maintaining comparable performance on NLP seq-to-seq generation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper identifies two sources of inefficiency in conditional MDLMs (iterative unmasking and CFG) and adapts existing diffusion distillation techniques to address these issues.\n2. Empirical results and ablation studies on NLP seq-to-seq generation tasks demonstrate that the distilled models achieve 16x speedup at inference time without significant degradation of generation quality.\n3. The paper is generally well-written and easy to understand."}, "weaknesses": {"value": "1. The novelty of the proposed method is limited, as the proposed dual distillation framework is a direct adaptation of existing distillation method in the literature. This contribution does not meet the acceptance bar in my opinion.\n2. Although the proposed method achieves a good speedup at inference time, it involves a computationally very expensive distillation procedure (especially in stage two where several rounds of progressive distillation are performed), which limits the scalability of the proposed method for larger models and larger datasets.\n3. Limitations are not discussed in the paper."}, "questions": {"value": "1. How is the diversity of the generated sequences from the final student model compared to those from the initial teacher model?\n2. How is the inference speed of the final student model compared to standard auto-regressive LLMs of similar size?\n3. Given the significant training costs introduced by the proposed method, could the authors quantify the total training cost and elaborate on this trade-off?\n4. In the second stage, the proposed method adapts progressive distillation, which requires several rounds of distillation to produce a multi-step student model. However, recent diffusion distillation approaches (e.g., [1,2,3, 4]) can distill diffusion models into one-step student models in one round. Could those approaches be adapted to the conditional MDLM distillation setting to reduce training costs?\n5. Please add a paragraph to discuss the limitations of the proposed method in the paper.\n6. Is Equation (2) based on the assumption that $\\alpha_t=1-t$? This assumption cannot be found in Sec 2.1.\n\n[1] S Xie, et al. \"EM distillation for one-step diffusion models.\" NeurIPS 2024.\n\n[2] W Luo, et al. \"Diff-Instruct: A universal approach for transferring knowledge from pre-trained diffusion models.\" NeurIPS 2023.\n\n[3] M Zhang, et al. \"Towards Training One-Step Diffusion Models Without Distillation\". arXiv.\n\n[4] H Zheng, et al. \"Ultra-fast language generation via discrete diffusion divergence instruct.\" arXiv."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UcElIp2pwS", "forum": "iSEN8irLUD", "replyto": "iSEN8irLUD", "signatures": ["ICLR.cc/2026/Conference/Submission24546/Reviewer_G1S3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24546/Reviewer_G1S3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761207103495, "cdate": 1761207103495, "tmdate": 1762943118527, "mdate": 1762943118527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a two-stage distillation framework for conditional MDLMs to improve inference efficiency in sequence-to-sequence NLP tasks. The method distills (i) classifier-free guidance and (ii) unmasking trajectory knowledge from a teacher MDLM into a student model. The distilled student can generate text with a single forward pass per step and fewer unmasking steps, achieving up to 16× speedup while maintaining comparable generation quality"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and clearly motivated; both the overall idea and the two-stage distillation algorithm are clearly presented and easy to follow.\n\n2. Experiments on sequence-to-sequence tasks convincingly demonstrate the effectiveness of the proposed method in improving inference efficiency while maintaining generation quality."}, "weaknesses": {"value": "1. The novelty of this work appears limited. The two proposed algorithms mainly follow the frameworks of previous studies [1, 2], and the paper does not clearly explain how its approach differs conceptually or technically from them.\n\n2. The experimental scope is narrow. The paper focuses on relatively simple seq-to-seq tasks, where the reported inference acceleration is not particularly impressive. It would be more convincing to include evaluations on challenging tasks such as mathematical reasoning or code generation, which are central to current research on large language models and diffusion language models [3, 4].\n\n[1] Meng et al. On Distillation of Guided Diffusion Models.\n\n[2] Deschenaux et al. Beyond Autoregression: Fast LLMs via Self-Distillation Through Time.\n\n[3] Ye at al. Dream 7B: Diffusion Large Language Models.\n\n[4] Xie et al. Dream-coder 7b: An open diffusion language model for code."}, "questions": {"value": "How does the proposed method compare with training-free acceleration approaches [5, 6] that are widely adopted in the community?\n\n[5] Wu et al. Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding. \n\n[6] Ben-Hamu et al. Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lgMq49pF45", "forum": "iSEN8irLUD", "replyto": "iSEN8irLUD", "signatures": ["ICLR.cc/2026/Conference/Submission24546/Reviewer_ai6w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24546/Reviewer_ai6w"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762077073025, "cdate": 1762077073025, "tmdate": 1762943118347, "mdate": 1762943118347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}