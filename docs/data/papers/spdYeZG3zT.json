{"id": "spdYeZG3zT", "number": 21377, "cdate": 1758316861275, "mdate": 1759896925248, "content": {"title": "Dissecting Attention and MLP Roles: A Study of Domain Specialization in Large Language Models", "abstract": "Large language models (LLMs) perform well across diverse domains such as programming, medicine, and law, yet it remains unclear how domain information is represented and distributed within their internal mechanisms. A key open question is the $\\textit{division of labor}$ between the Transformer's core components: self-attention and MLP layers. We address this question through a mechanistic study that dissects their roles by integrating three complementary analyses: representation separability via probes, parameter change under adaptation, and causal effects from activation swaps. We propose a clear division of labor: attention layers route domain identity, while MLP layers implement domain-specific computation. Causal interventions strongly support our claims. For instance, swapping attention activations at specific mid-depth layers (e.g., for Python $\\leftrightarrow$ C++) reliably shifts the next-token distribution, whereas layers with low domain separability have a negligible effect. In contrast, while finetuning, MLP layers exhibit relatively larger weight changes, consistent with domain-specific knowledge being stored there. This pattern holds consistently across four models and six domains. In a supplementary experiment, we demonstrate that selecting a few components highlighted by our study can accelerate domain adaptation, indicating the potential for more focused fine-tuning.", "tldr": "In LLMs, attention layers act as domain \"routers\" while MLP layers store and perform the domain-specific \"computation\" in high-level domain abstraction", "keywords": ["Mechanistic Interpretability", "Large Language Models", "Domain control", "layer analysis", "activation patching"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/32167941cbf8e41a8603b5d5572fc08d437f203c.pdf", "supplementary_material": "/attachment/dc11799592c864aac2b1ff00ef428e4f7216ecc1.zip"}, "replies": [{"content": {"summary": {"value": "The paper investigates how transformers internalize domain specialization, arguing that self-attention primarily routes domain identity while MLP blocks implement the domain-specific computation. The authors investigate this by studying layerwise separability probes, where fine-tuning result in change under LoRA, and causal activation swapping that measures both disruption and directional shift toward donor domains. Empirically, they show that a few attention layers do most of the steering, while the MLPs take on most of the updates during fine-tuning. Inspired by these findings, they also use the derived layer map to fine-tune only the top-k layers, achieving performance that matches or even exceeds full-model tuning."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides:\n\n- Unified methodological frame: A cohesive design that combines representational probes, adaptation/parameter-change analysis, and causal activation swaps, yielding mutually reinforcing evidence rather than relying on a single metric or viewpoint.\n\n- Comprehensive investigation with valuable insights: Broad, cross-domain and cross-model analysis that clarifies the division of labor between attention (routing/steering) and MLPs (domain-specific computation), and identifies where interventions are actually effective.\n\n- Efficient PEFT derived from findings: Uses the learned layer-importance map to fine-tune only a small set of top-k layers, delivering performance comparable to, or better than, full-model tuning while training far fewer parameters."}, "weaknesses": {"value": "- Code-centric and limited causal evidence: Most activation-swap tests are confined to Python and C++ and rely on hand-crafted token sets. Whether the causal “attention routes, MLP computes” pattern holds in non-code domains remains unproven.\n\n- Short-horizon intervention metric: Effects are measured only via next-token changes rather than multi-step rollouts or task-level metrics, so impacts on reasoning and sequence-level coherence are unclear.\n\n- Limited baselines across experiments: Several analyses would benefit from stronger comparison baselines, for probing , activation swaps, and targeted PEFT.\n\n- Limited PEFT/design space: Adaptation analysis centers on LoRA; there’s little comparison to alternatives adapters or varied training regimes that might change where updates accumulate.\n\n- Small-model scope: Experiments are limited to 1B–4B models (likely due to resource constraints), so it’s unclear whether the findings generalize to larger LLMs."}, "questions": {"value": "- In the activation-swap experiments, could you add baselines such as swapping with random noise, shuffled tokens, or mismatched layers?\n\n- Can you report the impact of interventions on task-level performance (e.g., GSM8K accuracy), not just next-token metrics?\n\n- Did you normalize donor/recipient activations (e.g., re-centering or rescaling) to rule out scale or LayerNorm artifacts during swaps?\n\n- Early layers show high separability but low causal leverage (“hydra” effect). Can you quantify which peaks are true “steering handles” versus hydra-like signals?\n\n- For targeted PEFT, how does “top-k by separability” compare to baselines like top-k by LoRA delta-norm or random k layers?\n\n- Would your main conclusions change if you tuned hyperparameters per setting instead of fixing them globally?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mrl7WoSySw", "forum": "spdYeZG3zT", "replyto": "spdYeZG3zT", "signatures": ["ICLR.cc/2026/Conference/Submission21377/Reviewer_WDxQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21377/Reviewer_WDxQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761595464800, "cdate": 1761595464800, "tmdate": 1762941732850, "mdate": 1762941732850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to investigate the functional division of Transformer components in representing domain knowledge by combining three analytical perspectives — representation analysis, adaptational analysis, and causal analysis. The authors propose that attention layers primarily handle **domain routing**, while MLP layers perform **domain-specific computation**, supporting this claim through three types of evidence: Fisher/MMD separability analysis, LoRA-based fine-tuning experiments, and activation-swapping causal interventions.\n\nWhile the topic is potentially meaningful, the paper exhibits noticeable limitations in **novelty**, **logical rigor**, and **empirical sufficiency**. The proposed “unified cross-layer interpretability framework” lacks methodological innovation. **As a mechanistic interpretability paper, the reasoning chain is overall discontinuous, the experimental scale is limited, and the causal claims are not statistically supported.**"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Originality:**  \n  The paper demonstrates conceptual originality by reframing the question of *where domain knowledge resides within Transformer architectures* into a multi-perspective interpretability problem. Although each analytical method (Fisher/MMD, LoRA, activation swapping) has been used before, their deliberate triangulation to infer functional specialization between Attention and MLP layers represents a novel synthesis that broadens the scope of existing mechanistic interpretability work.\n\n**Quality:**  \n  The empirical analyses are carefully executed and technically sound within the chosen experimental setting. The authors systematically design experiments across multiple domains and models, maintaining consistent evaluation pipelines. The causal intervention setup, while limited, reflects a thoughtful attempt to move beyond correlational analyses toward a more explanatory approach.\n\n**Clarity:**  \n  The manuscript is clearly organized and accessible. Each methodological component is described with sufficient detail, and the figures effectively support the narrative. The transitions between the three analytical perspectives are smooth, and the overall exposition allows readers to easily follow the authors’ reasoning and hypotheses.\n\n**Significance:**  \n  The work engages with a fundamental and timely question — the internal structure of knowledge within large language models — which is central to advancing the field of mechanistic interpretability. By attempting to connect representational separability, adaptation behavior, and causal impact, the paper provides a unifying conceptual lens that may inspire future research aiming to formalize “functional modularity” in Transformers.\n\nOverall, the paper’s value lies less in proposing new metrics and more in articulating an integrative analytical framework that could guide future interpretability research. With broader validation and stronger statistical grounding, the study has the potential to make a meaningful conceptual contribution to understanding the internal organization of large language models."}, "weaknesses": {"value": "1. **Limited Substantive Contribution in Innovation and Methodological Integration.**\n\n   The core claim of this work lies in integrating three existing analytical tools to infer the internal functional division within Transformers. However:\n\n   - The employed methods — **Fisher/MMD separability**, **LoRA parameter shift**, and **activation swapping** — are all well-established in prior literature. The paper introduces no new metric, aggregation scheme, or unified modeling framework.  \n   - Although the authors attempt to discuss the complementarity of the three analyses in Section 4, this integration remains **qualitative**, lacking quantitative validation or a formal consistency test across the methods.  \n   - The so-called “orthogonal evidence integration” is thus descriptive rather than methodological. The paper does not demonstrate why such integration is necessary or whether it offers explanatory power beyond juxtaposing known analyses.\n\n2. **Logical Gap Between Metrics and the term \"Knowledge\"**\n\n   The reasoning from statistical measures to “knowledge representation” is insufficiently supported:\n\n   - The authors treat high **Fisher/MMD separability** as evidence of domain identity representation but never establish equivalence with domain knowledge storage (e.g., *line 459*, *line 479*). This leap from separability to semantics is conceptually weak.  \n   - The **LoRA-based adaptation** analysis indeed identifies layer-level parameter shifts but does not independently verify that these shifts correspond to “knowledge locations.” Since LoRA itself is premised on partial-parameter adaptation, using it to prove that “MLP layers store domain knowledge” risks **circular reasoning**.  \n   - The **causal swapping experiments** show distributional perturbations but lack statistical significance testing (confidence intervals, control groups, or permutation tests). Therefore, they do not substantiate the strong causal claim that *“attention causes domain-directed behavior”* (*line 361*).  \n\n\n3. **Limited Experimental Scale and Generalizability**\n\n   Experiments are limited to small-scale models (1B–4B) such as **LLaMA-3.2-3B** and **Gemma-3-1B/4B**, not mainstream large models (≥30B).  \n   - Small models differ significantly in architectural depth, routing granularity, and parameter dynamics; thus, the observed “layer division” may not generalize.  \n   - Although the authors claim cross-model consistency, no scaling trends or statistical validations are provided.  \n   - The dataset covers only six relatively simple domains (programming, medicine, finance, mathematics, science, C++/Python) and omits multimodal, multilingual, or reasoning-heavy settings."}, "questions": {"value": "> I encourage the authors to thoroughly address the weaknesses and questions raised in this review. If the authors can provide detailed explanations and in-depth clarifications during the rebuttal, and if the revised version demonstrates substantial progress in both clarity and improvement, I will be willing to reassess the manuscript and **adjust my overall rating** accordingly, based on the quality and depth of the revision.\n\n---\n\n1. **Theoretical Ambiguity**  \n   - How exactly do the authors define “domain knowledge”? Is it intended to capture *semantic understanding* (e.g., conceptual meaning), *procedural competence* (e.g., problem-solving strategies), or *factual recall* (e.g., specific information storage)?  \n   - Without a formal operational definition, it is difficult to determine whether the observed layer-specific effects actually correspond to “knowledge representation” or merely reflect *domain-related stylistic or distributional differences* in the data.  \n   - Could the authors clarify whether their framework treats “domain knowledge” as a property of the learned representation, the behavior of the model, or both? Explicitly distinguishing these levels would make the theoretical scope more coherent.\n\n2. **Reproducibility**  \n   - The paper omits key implementation details that are necessary for replication. Could the authors specify:  \n     (a) the kernel function and hyperparameters used in Fisher and MMD separability analyses;  \n     (b) the number of samples per domain and per layer;  \n     (c) the criteria for selecting layers to probe;  \n     (d) the LoRA configuration (rank, scaling, optimizer settings, etc.); and  \n     (e) the random seeds or initialization strategies used?  \n   - Additionally, is there any plan to release the code or experiment scripts to support verification of these findings? Given that minor hyperparameter differences can significantly affect separability metrics, transparency here is essential.\n\n3. **Statistical Significance and Robustness**  \n   - The figures (e.g., separability plots and fine-tuning deltas) present mean trends but omit confidence intervals, standard deviations, or hypothesis tests (e.g., t-tests or ANOVA). How can readers evaluate whether the observed Attention–MLP divergence is statistically meaningful rather than random variation?  \n   - Were results consistent across multiple random seeds or model checkpoints? Without reporting variance or repeated trials, the conclusions about “significant differences” between components lack quantitative support.  \n   - Would the authors consider including effect sizes or bootstrap confidence intervals to improve interpretability and robustness?\n\n4. **Causal Interpretation**  \n   - The authors frequently use strong causal terminology — such as *“cause”* (line 152), *“reveal”* (line 361), and *“directs”* (line 388) — even though the experiments appear to rely on *activation perturbation* rather than *formal causal modeling*.  \n   - Can the authors clarify how these causal claims are justified? For example, were interventions designed with causal counterfactual control, or are they simply correlational probes observing outcome shifts?  \n   - If the method is indeed correlational, could the authors rephrase or qualify these claims to avoid overinterpretation? Alternatively, a discussion comparing their results to formal causal frameworks (e.g., SCMs or do-calculus) would strengthen the argument.\n\n5. **Literature Contextualization and Incremental Contribution**  \n   - The paper does not clearly situate its contribution relative to influential mechanistic interpretability works such as *Transformer Circuits*, *Induction Head Hypothesis*.\n   - Specifically, the conclusion *“attention layers serve as domain routers”* (line 483) reiterates an idea the authors themselves acknowledge in the Introduction (line 52: “attention mechanisms are understood as routers, moving and aligning information throughout the context”).  \n   - In what sense does this paper extend or refine that established hypothesis? Does it provide new *quantitative evidence* for this claim, or merely replicate it in a new context (e.g., across abstract domains like programming and medicine)?  \n   - More broadly, how does the proposed “triangulated evidence framework” advance the field beyond confirming previously suspected functional roles? The authors might clarify whether the integration across representational, adaptational, and causal perspectives leads to *novel mechanistic insight* or simply consolidates prior intuitions.\n\n6. **Scope and Generalization**  \n   - The experiments are conducted primarily on small models (1B–4B scale). How can the authors justify the generalization of their claims to large-scale LLMs (30B+), where emergent modularity and representation compression differ significantly?  \n   - Would the authors expect the same “attention-as-router, MLP-as-computation” division to persist or evolve in higher-capacity models? Empirical or theoretical discussion on this scaling behavior would substantially strengthen the paper’s external validity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A8CtnlUW8k", "forum": "spdYeZG3zT", "replyto": "spdYeZG3zT", "signatures": ["ICLR.cc/2026/Conference/Submission21377/Reviewer_ZbLm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21377/Reviewer_ZbLm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844724565, "cdate": 1761844724565, "tmdate": 1762941732541, "mdate": 1762941732541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the division of labor between self-attention and MLP layers in LLMs with respect to domain specialization. \nSpecifically, authors conduct three types of analysis:\n(1) Probing to identify which layers contain linearly separable information about domain identity. Two metrics, i.e., Fisher ratio and Maximum Mean Discrepancy with RBF-kernel are used to measure whether the domain information is seperable.\n(2) LoRA fintuning to find where parameters undergo adaptation. Higher Frobenius norm of the changed parameter of a layer means the layer is essential for domain adaptation.\n(3) Causal Intervention that swapping domain-specific activations to test whether it influence domain-directed generation.\nExperiments conducted on six domains and two families of LLMs,i.e., Llama and Gemma.\nResults show that attention layers act as domain routers, while MLP layers serve as domain-specific computational units."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The experiment result of causal intervention is interesting.\n2. The three types of analysis are well-organized, with clearly and progressively motivations and carefully designed metrics.\n3. The paper is written fluently and easy to follow.\n4. Discussions and limitations are included."}, "weaknesses": {"value": "1. The paper only contains analysis, therefore the technical contribution and novelty are limited.\n2. Some typos need to be corrected, e.g. Line 285 LoRA-based, caption of Table 1 should be below the table.\n3. I doubt the max value in Table 1 is not that persuasive, since it may compares between different layers. Authors may consider report the max value of the difference between attention and MLP of a single layer. Moreover, only the max value of Fisher show large discrepancy.\n4. Why conduct LoRA setting instead of full-finetuning?\n5. Table 2 results might need further explanation: (1) why in some domains like python, finetuning on both attention and MLP drastically decreases the performance? (2) Does the finetuning dataset overlaps with the testing data? If so, is that the reason of the improvement in medical domain?\n6. Authors did not provide the performance after causal swapping, e.g. accuracy and Pass@K used in the paper. If the performance decreases, the finding is not meaningful since even though we can swap C++ to Python but the generated program may be wrong.\n7. Authors should conduct experiments of more model architectures, like models without gate_proj, MoE models, etc.\n8. The parameter size of the models is relatively small, authors should at least try some 7B models.\n9. My biggest concern is whether the topic is meaningful. With the strong zero-shot ability of SOTA LLMs, we do not even bother to consider domain adaptation problems. This significantly reduces the application value of the paper."}, "questions": {"value": "1. Have authors consider using different domain-specific token number in swapping?\n2. Fig 1, deeper layer MLP has higher Fisher and MMD score, authors may give analysis on the phenomenon.\n3. Does the higher weight change in MLP is caused by natural architecture order of attention and MLP (attention calculation is always ahead of MLP calculation)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WXNxdtLspc", "forum": "spdYeZG3zT", "replyto": "spdYeZG3zT", "signatures": ["ICLR.cc/2026/Conference/Submission21377/Reviewer_pthK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21377/Reviewer_pthK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920506751, "cdate": 1761920506751, "tmdate": 1762941732270, "mdate": 1762941732270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper seeks to answer the question of how do transformer language models process information from different domains. Namely where is the domain knowledge stored and how is the domain identified. The authors tackle the question using three distinct approaches. Firstly, they gauge the separability of the representation at each layer using Maximum Mean Discrepancy and the Fisher score. Secondly, they fine-tune models on specific domains and measure the normalized update magnitude for separate layers and thirdly they swap activations between domains and measure the effect it has on the predicted distribution. The latter is done for manually chosen prompts that differ only in a few tokens. In the experimental section of the paper the authors present their findings and discuss the results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and has quite a thorough supplementary."}, "weaknesses": {"value": "The main weakness of the paper is in its experimental results. Unfortunately, looking at the results I do not think that they produce the same conclusions as the paper.\n\nLet's assume that hidden feature separability is somehow a good metric for identifying domain specific computation or domain identification performed by the model (it certainly is a measure of how easy the latter is). The results of section 3.1 show that features from both the MLP and the attention layers are equally separable. Moreover they are separable at various levels with no discernible pattern with respect to layers. A point directly agains the conclusions of the authors that the attention exhibits higher separability would be that when it comes to the average metrics the MLPs are more separable in 6/6 cases (in Table 1). From the above I would conclude that it is unclear whether the MLPs or the attention contain more separable representations wrt to the domain. Finally, looking at Figure 1 we observe that very often the representation of the 1st layer is the most separable. As a result it is unclear whether we can identify which point in the network computes domain specific features using this metric since they seem to be there since layer 1.\n\nMoving on to section 3.2. The experiments show that the MLPs have been \"moved\" more during finetuning compared to the weights of the attention layers. This result can be interpreted as most of the new knowledge being stored in the MLPs, however, the point of the paper is to connect this to the domain. Namely, figure 2 does not tell us whether domain specific knowledge is stored in MLPs but rather that any new knowledge is. Perhaps the authors could finetune jointly on all domains and compare the deltas then.\n\nFinally, when it comes to the results of Table 2, I think we cannot draw the conclusion that it is better to fine-tune the important layers. There is absolutely no pattern to these results. Often full finetuning results in 0 accuracy which clearly is either overfitting or some other collapse. Some times like in Finance or Python all models perform worse than the pre-trained model. Some times tuning the MLPs is better and others it is the attention layers."}, "questions": {"value": "I have laid out my questions in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iKSii9kjez", "forum": "spdYeZG3zT", "replyto": "spdYeZG3zT", "signatures": ["ICLR.cc/2026/Conference/Submission21377/Reviewer_R4WC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21377/Reviewer_R4WC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762132509617, "cdate": 1762132509617, "tmdate": 1762941731931, "mdate": 1762941731931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}