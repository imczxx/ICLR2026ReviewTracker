{"id": "isWKA4rdXx", "number": 19096, "cdate": 1758293553292, "mdate": 1759897059780, "content": {"title": "Multi-Task Sequence Models Generalize in Offline Multi-Agent Reinforcement Learning", "abstract": "Recent sequence model architectures have demonstrated great promise in offline multi-agent reinforcement learning (MARL). However, even for this expressive model class, generalising to tasks unseen in the training data remains a core challenge. A sensible response to this challenge is to simply scale the amount of offline data available for training. Yet, in this work, we find that task diversity has a stronger influence on generalisation than sheer dataset size. To obtain our findings, we study offline MARL sequence models trained on single-task datasets, clearly demonstrating their limited ability to zero-shot transfer to held-out test tasks.\nLeveraging this insight, we train and test multi-task versions of offline sequence modeling architectures. We identify three key design choices for successful offline multi-task training: (i) task-balanced mini-batches, (ii) treating value estimation as classification and (iii) agent masking to handle variable team sizes. Using multi-task datasets from three challenging cooperative environments (Connector, RWARE, and LBF), we investigate generalisation to unseen tasks and the scaling behaviour of our multi-task offline algorithms.\nWe show that our multi-task sequence models generalise better across all environments compared to single-task models, and achieve a mean improvement of 219% on held-out test tasks. Moreover, our offline MARL sequence models consistently outperform behaviour cloning (a surprisingly strong baseline). Our results clearly show that scaling task diversity by increasing the number of tasks used during training leads to improved generalisation gains over simply scaling the dataset size at a fixed level of task diversity.", "tldr": "In offline MARL, increasing task diversity is far more important for generalisation than scaling dataset size. Multi-task models trained with our proposed design choices achieve substantially better zero-shot transfer than single-task models.", "keywords": ["multi-agent reinforcement learning", "reinforcement learning", "offline reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d16a87d96c1e7ccc45067adadb09972a340938f3.pdf", "supplementary_material": "/attachment/6adcecb133c5769ddb1a19cb2985cf9d665a04af.zip"}, "replies": [{"content": {"summary": {"value": "The paper investigates generalization in offline multi-agent reinforcement learning (MARL) through multi-task training. The authors propose modifications to existing sequence models (introducing BC-Sable, CQL-Sable, and adapting Oryx) to handle multiple tasks with varying agent numbers. Key contributions include: (1) a multi-task offline MARL benchmark across three environments (LBF, RWARE, Connector), (2) demonstrating that multi-task training significantly improves zero-shot transfer to unseen tasks (219% average improvement), and (3) showing that dataset diversity matters more than dataset size for generalization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Well-structured and clearly written paper addressing the under-explored regime of offline MARL in multi-task settings.\n-Strong empirical results with substantial performance gains (up to 442% on RWARE) demonstrating the effectiveness of multi-task training for zero-shot generalization.\n- Comprehensive experimental setup with three different algorithms tested across three environments, showing consistent improvements from multi-task training.\n- Important finding on dataset diversity vs. size: Clear experimental evidence (Section 3.4) that increasing task diversity improves generalization more than simply scaling dataset size.\n- Thorough ablation studies (Section 3.5) validating each design choice, particularly showing 37% performance drop without task-balanced batching.\n- Contradicts prior pessimistic findings by demonstrating that offline RL methods (particularly MT Oryx) can outperform behavior cloning, contrary to Mediratta et al. (2024).\n- Multiple baselines and fair comparisons including two newly contributed baselines (BC-Sable and CQL-Sable) for this work.\nCode and dataset availability with promised public release upon publication."}, "weaknesses": {"value": "- Limited algorithmic novelty: The actual differences between BC-Sable, CQL-Sable and Oryx-based models are unclear. The paper primarily applies known techniques (task-balanced batching, HL-Gauss, agent masking) rather than introducing fundamentally new methods.\n- Lack of theoretical justification: No theoretical analysis, proofs, or theorems explaining why intra-task transfer aids representation learning for generalization. The empirical results lack theoretical grounding.\n- Insufficient task and environment context: The main text lacks adequate explanation of what agents do in each environment and why these tasks are challenging/beneficial for multi-task learning.\n- Section 3.4 appears more exploratory than contributory: The findings about dataset/model scaling largely iterate on well-known ideas from function approximation and supervised learning. As noted, Mediratta et al. (2024) has similar results, raising questions about scientific contribution.\n\nInconsistent experimental design:\n- Different numbers of training tasks across benchmarks (5 for LBF, 10 for Connector, 15 for RWARE) without clear justification\nRWARE specifically chosen for scaling experiments without explanation\n- Only best checkpoint results shown rather than average performance\n\n- Many undefined abbreviations throughout the main text (Dec-POMDP, SABLE, RWARE, LBF) that reduce readability.\n- Missing comparisons to other recent multi-task MARL methods mentioned in related work."}, "questions": {"value": "- Line 048, Figure 1: What type of normalization is used for test performance? This is crucial for interpreting the 442% improvement claims.\n- Line 120, Figure 2: Which specific task is shown in the visualization, and from which dataset?\n- Line 130: What is a \"Dec-POMDP\"? This acronym needs definition when first introduced.\n- Line 249: Why show only the best checkpoint? Is this common practice? Wouldn't average performance across checkpoints provide more robust evaluation of the approach?\n- Line 259: Why use different numbers of training tasks for different benchmarks? Is this due to environment complexity, data availability, or other factors?\n- Line 329: When \"MT Oryx performs the best\" aggregated across tasks, what specific properties allow it to leverage offline RL that BC struggles with? What makes this finding different from Mediratta et al. (2024)?\n- Line 350, Experiment (b): Why was RWARE specifically chosen for model scaling experiments? Do other environments show similar trends? Also, why only vary embedding dimensions rather than exploring different scaling strategies for encoder vs. decoder?- \n- General: How do your methods compare to other multi-task MARL approaches like MaskMA or HiSSD mentioned in related work? MADT are Decision Transformers and sequence models, like in your case.\n- Reproducibility: What are the computational requirements (training time, memory) for the multi-task models compared to single-task variants?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "78CYZ9cTL3", "forum": "isWKA4rdXx", "replyto": "isWKA4rdXx", "signatures": ["ICLR.cc/2026/Conference/Submission19096/Reviewer_7Eoh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19096/Reviewer_7Eoh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761688485125, "cdate": 1761688485125, "tmdate": 1762931120244, "mdate": 1762931120244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies offline multi-task reinforcement leaning with sequence models. They demonstrate that scaling task diversity and the number of tasks leads to greater generalisation beyond scaling single task datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper is very well written and provides sound insights and interesting discussions of results, making it an enjoyable read. It proposes logical claims backed up with evidence across settings. It provides details on the generalisation gap as well as additional experiments on model/data scaling. The paper uses current strong sequence models for baselines."}, "weaknesses": {"value": "Whilst there is novelty in application for offline MARL with sequence models, it seems the primary takeaway from this paper is that increasing number of tasks and diversity of tasks improves generalization. This is not a particularly novel insight and has been the motivation of multi-task and meta-reinforcement learning since its inception. This paper could benefit from additional baselines, particularly those from different architectures such as those using decentralised actors to increase the rigor of its contribution. The scaling experiment whilst interesting, could benefit from seeing how data requirement scale with model parameters."}, "questions": {"value": "Given its dramatic effect, would you consider task-balanced batching to be the primary enabling component for multi-task MARL?\nHow would you expect these results to transfer to other dominant MARL architectures? Do you hypothesize that the generalization gap between MT Oryx and MT BC-Sable would widen or narrow in a decentralised and CTDE algorithms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tyCkfHQT72", "forum": "isWKA4rdXx", "replyto": "isWKA4rdXx", "signatures": ["ICLR.cc/2026/Conference/Submission19096/Reviewer_CUDK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19096/Reviewer_CUDK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865640846, "cdate": 1761865640846, "tmdate": 1762931119366, "mdate": 1762931119366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper validates an conclusion: In the offline MARL domain, the current best-performing class of models, Multi-Task sequence models, exhibit favorable scaling properties in terms of zero-shot generalization capability with respect to task diversity. Specifically, increasing task diversity during training can significantly enhance the model's zero-shot generalization ability on unseen tasks. However, when task diversity is held constant, merely increasing the size of the dataset does not improve generalization capability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to understand, with a clear logical structure and few typos.\n- Most of the conclusions drawn in the paper are supported by corresponding experiments.\n- The conclusion proven in the paper, that \"increasing task diversity can significantly improve the model's zero-shot generalization capability\", is somewhat inspiring for future work in this field."}, "weaknesses": {"value": "- Inappropriate metric: I think the metric used is not suitable. Since it is about success rate, the increase could simply be measured by $yourSR - baselineSR$. Using $\\frac{yourSR - baselineSR}{baselineSR}$ is counterintuitive and may exaggerate the conclusion obtaining from the results.\n- Are the size of dataset of multi-task and single-task the same? That is, are the numbers of episodes the same? Because the paper keeps emphasizing that increasing task diversity is more effective than increasing the size of the dataset, it is crucial to clarify whether the size of the dataset has been increased to the same level as in the multi-task scenario. Note that I am aware that Section 3.4 of the paper proves that simply increasing the size of the dataset does not help improve generalization capability. However, in Figure 1, multi-task and single-task still need datasets of the same size to rigorously prove your claim.\n- Inappropriate Evaluation protocol: The evaluation is based on the best checkpoint achieved during training, which is inappropriate. If an algorithm has high variance, its best checkpoint may perform well. The paper should provide the asymptotic performance curve of the evaluation SR as training progresses.\n- \"We observe that performance on the training tasks remains high across all environments, even as the number of tasks increases. This indicates that the model can successfully learn across multiple tasks simultaneously.\" This statement is not correct. In fact, this statement only holds in Connector. The authors later also mentioned the performance drop in RWARE and LBF. Therefore, using this statement as the first sentence of this paragraph is very unrigorous.\n- The paper draws several conclusions inconsistent with prior work but does not explain why the opposite conclusions were reached. For example, (1) \"The experimental conclusion of this paper is that offline has better generalization capability than BC, while Mediratta et al. (2024) concluded that BC has better generalization capability than offline.\" Why did the opposite experimental results occur? The paper does not provide an analysis. I believe it is very important to provide a reasonable explanation for \"reaching conclusions opposite to prior work,\" at least with some insight-level analysis or explanation. (2) \"Notably, this result contrasts with the single-task setting reported by Formanek et al. (2025), where the optimal embedding dimension was just 64, underscoring the unique potential of multi-task data for enabling scale.\" Similarly, this conclusion opposite to prior work also needs to be analyzed.\n- Why not conduct experimental comparisons on SMAC (or SMAX, i.e., SMAC in JAX)? On the one hand, SMAC is a very commonly used (as far as I know, the most commonly used) benchmark in the MARL field. On the other hand, there have been previous works on task-level generalization on SMAC, such as DT2GS [1], ODIS [2], and UPDeT [3].\n\n[1] Tian et al. Decompose a Task into Generalizable Subtasks in Multi-Agent Reinforcement Learning. NeurIPS 2023\n\n[2] Zhang et al. Discovering Generalizable Multi-agent Coordination Skills from Multi-task Offline Data. ICLR 2023\n\n[3] Hu et al. UPDeT: Universal Multi-agent Reinforcement Learning via Policy Decoupling with Transformers. ICLR 2021"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1OTET3XmIA", "forum": "isWKA4rdXx", "replyto": "isWKA4rdXx", "signatures": ["ICLR.cc/2026/Conference/Submission19096/Reviewer_mjub"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19096/Reviewer_mjub"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901165272, "cdate": 1761901165272, "tmdate": 1762931118715, "mdate": 1762931118715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates generalization in offline multi-agent reinforcement learning (MARL) using sequence models. The paper demonstrates that task diversity is more important than dataset size for achieving zero-shot transfer to unseen tasks. The paper introduces three multi-task sequence models (MT Oryx, MT CQL-Sable, and MT BC-Sable) and identify three key design choices for successful multi-task training: (1) task-balanced mini-batches, (2) value estimation as classification (HL-Gauss), and (3) agent masking/shuffling for variable team sizes. Experiments across three cooperative environments (LBF, RWARE, Connector) show that multi-task models achieve 219% average improvement on held-out test tasks compared to single-task models, and that offline MARL methods can outperform behavior cloning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### Clarity\nThe paper is well written and easy to understand. The empirical findings are clearly presented and can provides actionable insights for practitioners, especially the insights that task diversity matters more than dataset size.\n\n### Significance\nThe paper introduces three practical design contributions: task-balanced batching, HL-Gauss, agent masking. These identified design choices are simple, well-motivated, and effectively address multi-task MARL challenges. The ablation studies (Figure 6) validate their importance.\n\nThe observation that model capacity scaling improves generalization on difficult tasks (Figure 5b) is valuable and suggests promising directions for future work."}, "weaknesses": {"value": "My major concerns are that the paper has limited novelty, and the empirical evaluation appears to be insufficient. \n\n1. Limited novelty in the proposed models, and design choices: \n\n**Incremental sequence models**: The paper's main algorithmic contributions are MT CQL-Sable and MT BC-Sable, which are essentially Sable (Mahjoub et al., 2025) with CQL and BC losses respectively. MT Oryx is Oryx (Formanek et al., 2025) adapted for multi-task settings. The core architectures (Sable's Retentive Network backbone, Oryx's ICQ formulation) are unchanged. The paper essentially shows these existing methods can be extended to multi-task settings with relatively minor modifications. Hence the claim that “We present two novel MARL sequence models (BC-Sable and CQL-Sable)” may not hold and the contributions appear very incremental.  \n\n**Limited technical depth in design choices**: The three design choices (task-balanced batching, HL-Gauss, agent masking) are sensible engineering decisions but not fundamental algorithmic innovations: Task-balanced batching is standard practice in multi-task learning (acknowledged via Cui et al., 2019 citation); HL-Gauss was already proposed by Farebrother et al. (2024) for handling varying reward scales; Agent masking/shuffling is a straightforward solution to variable team sizes\n\n\n2. the empirical evaluation is insufficient and has limited insights.  \n\n**Insufficient analysis of generalization**: The paper demonstrates that multi-task training improves generalization, which has already been reported by many publications, e.g., [A generalist agent](https://arxiv.org/abs/2205.06175) and [Multi-Game Decision Transformer]( https://papers.neurips.cc/paper_files/paper/2022/file/b2cac94f82928a85055987d9fd44753f-Paper-Conference.pdf). The paper only provides limited insight into why or how. What shared structure are the models learning? Are certain task features more transferable? A representation analysis (e.g., visualization of learned features, attention patterns) would strengthen the work. Further, the discussion of task selection and the diversity measurement is limited: How were train/test splits designed to ensure meaningful distributional shift? What constitutes \"diverse\" tasks? Is it just varying parameters, or do tasks differ in structure? Would random task splits yield similar results, or is careful curation necessary?\n\n**Missing multi-task MARL baselines**: the paper has no comparison with other multi-task MARL methods like MaskMA (which explicitly addresses multi-task MARL with varying agent/action spaces and shows strong zero-shot transfer on SMAC), ODIS (which tackles multi-task offline MARL via skill discovery) or HiSSD (which works on similar problem but uses hierarchical approach), though these are discussed in related work. How do the proposed methods compare to these specialized multi-task approaches?"}, "questions": {"value": "1. Figure 5b only shows results on RWARE with one algorithm. Do similar scaling trends hold for other environments and algorithms? The claim about scaling benefits needs broader support.\n\n2. In Figure 6a the HL-Gauss ablation shows marginal benefits for MT CQL-Sable, which is a bit contradictory to the claim that it's essential for multi-task training. Can authors explain why HL-Gauss is not effective for MT CQL-Sable? \n\n3. Some claims are overclaimed (e.g., \"clearly show\" in abstract when results are mixed). “a challenging multi-task ofﬂine MARL benchmark” but not any multi-task offline MARL methods have been benchmarked; “two novel MARL sequence models (BC-Sable and CQL-Sable)” clearly these are only incremental modifications of Sable. \n\n4. Results may not be stable with only 3 seeds. Why not more seeds? \n\n5. the paper only reports the normalized test performance. What about the computational cost: No discussion of training time, computational requirements, or efficiency. How practical are these methods for large-scale applications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UOQXMV8LfG", "forum": "isWKA4rdXx", "replyto": "isWKA4rdXx", "signatures": ["ICLR.cc/2026/Conference/Submission19096/Reviewer_FYMH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19096/Reviewer_FYMH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956491568, "cdate": 1761956491568, "tmdate": 1762931118260, "mdate": 1762931118260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}