{"id": "WSpDZVEGNi", "number": 8190, "cdate": 1758073005536, "mdate": 1763756675100, "content": {"title": "Measuring Physical-World Privacy Awareness of Large Language Models: An Evaluation Benchmark", "abstract": "The deployment of Large Language Models (LLMs) in embodied agents creates an urgent need to measure their privacy awareness in the physical world. Existing evaluation methods, however, are confined to natural lanague based scenarios. To bridge this gap, we introduce EAPrivacy, a comprehensive evaluation benchmark designed to quantify the physical-world privacy awareness of LLM-powered agents. EAPrivacy utilizes procedurally generated scenarios across four tiers to test an agent's ability to handle sensitive objects, adapt to changing environments, balance task execution with privacy constraints, and resolve conflicts with social norms. Our measurements reveal a critical deficit in current models. The top-performing model, Gemini 2.5 Pro, achieved only 59\\% accuracy in scenarios involving changing physical environments. Furthermore, when a task was accompanied by a privacy request, models prioritized completion over the constraint in up to 86\\% of cases. In high-stakes situations pitting privacy against critical social norms, leading models like GPT-4o and Claude-3.5-haiku disregarded the social norm over 15\\% of the time. These findings, demonstrated by our benchmark, underscore a fundamental misalignment in LLMs regarding physically grounded privacy and establish the need for more robust, physically-aware alignment.", "tldr": "", "keywords": ["LLM", "Privacy"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b7341250e4d0af4bc3ad771d391ec4987cf7ac2.pdf", "supplementary_material": "/attachment/8489e481cfa92f39e016b35a5517cccc4e78862a.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents EAPrivacy which contains four tiers to evaluate the privacy awareness of current LLMs in physical world scenarios. The four tiers cover sensitive object identification from messy environments, contextual appropriateness of actions when environments change, balancing an explicit task with an inferred privacy constraint, and ethic dilemmas when social norms and personal privacy collide. The data are formatted in structured PDDL. This paper conducted experiments on current SOTA LLMs to find insights."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Each tier is clearly defined with tier-specific metrics\n- The evaluation covers 10+ current SOTA LLMs and reports representative results with failure pattern analysis."}, "weaknesses": {"value": "- There lacks inter-annotator agreement analysis, and the annotation procedure is not well described.\n- The negative effect of thinking is not well discussed. How do you control the thinking tokens and prompts across families? Could this finding be due to the over-long reasoning traces over context limit? How to ensure a fair across various LLMs?\n- The paper uses PDDL and textual descriptors to cover 'multimodal' cues. It is unclear whether PDDL representation is a good option for LLMs to understand the environments in these four tiers, and the paper misses justification and verification.\n- Candidates and rubrics are provided to the models, leading to information leakage. For example, negative examples contain strong sentiment markers will be avoided. Moreover, there is also positional bias given multiple choices, where some LLMs prefer earlier options."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0Yg2v5DpRE", "forum": "WSpDZVEGNi", "replyto": "WSpDZVEGNi", "signatures": ["ICLR.cc/2026/Conference/Submission8190/Reviewer_6CNf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8190/Reviewer_6CNf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761736050091, "cdate": 1761736050091, "tmdate": 1762920147390, "mdate": 1762920147390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for their careful evaluations and constructive feedback. All reviewers consistently acknowledge the novelty and contribution of building privacy evaluation into physical settings, the principled four-tier framework, the extensive evaluation across 16 SOTA models, and the interesting insights found (e.g., the critical shortfall in balancing privacy with task completion and the counterintuitive effect of \"thinking\"). We appreciate the thoughtful critiques regarding spatial grounding, methodological clarity for \"thinking,\" etc., which will be clarified and addressed in the specific responses to each reviewer."}}, "id": "DjJtSNkJyf", "forum": "WSpDZVEGNi", "replyto": "WSpDZVEGNi", "signatures": ["ICLR.cc/2026/Conference/Submission8190/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8190/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8190/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763756253732, "cdate": 1763756253732, "tmdate": 1763756857719, "mdate": 1763756857719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EAPrivacy, a benchmark for evaluating large language models’ (LLMs) privacy awareness in physical-world settings. The benchmark includes four progressively complex tiers—(1) Sensitive Object Identification, (2) Privacy in Shifting Environments, (3) Inferential Privacy under Task Conflicts, and (4) Social Norms vs. Personal Privacy—totaling over 400 procedurally generated scenarios. The authors evaluate multiple state-of-the-art models (GPT-5, Gemini 2.5, Claude 3.5, Qwen, Llama) and find that although LLMs perform reasonably well on explicit social-norm dilemmas, they perform poorly in nuanced contextual reasoning, failing to balance task completion with privacy protection."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper extends privacy evaluation beyond text-only settings into physical contexts, an underexplored but crucial domain as LLMs move into embodied and agentic use cases.\n2. The benchmark is comprehensive, assessing models from four levels. The authors also did a careful job in testing 16 models. I believe the empirical study is thorough and leads to meaningful insights."}, "weaknesses": {"value": "1. The four-tier design and contextual-integrity framing bear strong resemblance to prior work [1] and the data construction approach is similar to [2]. I would still think this paper has novel contributions because the study is in physical settings. However, approach wise, it needs to compare with [1] and [2] to highlight which parts are similar and which parts need new innovation due to the specialty of physical settings.\n2. The results show that while the selection accuracy is high, the model cannot act in a way that nicely caliberate helpfulness and privacy awareness. This is very similar to the finding of [2]. I would suggest separating multi-choice probing and behavioral analysis at the forefront.\n3. While the error cases are interesting, especially in Tier 4, i doubt they have a perfect solution even for human and the study is somewhat normative. The benchmark’s practical implications for real-world deployment are uncertain. he paper could better connect benchmark outcomes to concrete usecase and scenarios for embodied AI.\n\n\n[1] Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory, Mireshghallah et al., ICLR 2024\n[2] Privacylens: Evaluating privacy norm awareness of language models in action, Shao et al., NeurIPS 2024"}, "questions": {"value": "1. The paper mentions that in Tier 4, binary selection ground truth labels come from majority vote among five raters. What's the inter annotator agreement?\n2. How might these findings translate to real robotic systems versus simulated PDDL environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vNxf3tIXOX", "forum": "WSpDZVEGNi", "replyto": "WSpDZVEGNi", "signatures": ["ICLR.cc/2026/Conference/Submission8190/Reviewer_2uXL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8190/Reviewer_2uXL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844574114, "cdate": 1761844574114, "tmdate": 1762920147096, "mdate": 1762920147096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EAPrivacy, a novel evaluation benchmark designed to measure the physical-world privacy awareness of Large Language Models (LLMs) used as the cognitive core for embodied agents (like robots). The authors argue that existing privacy benchmarks are limited to text-based scenarios and fail to capture the challenges of physical interaction. EAPrivacy addresses this gap using over 400 procedurally generated scenarios across four tiers of increasing complexity: (1) identifying sensitive objects, (2) adapting to shifting social contexts, (3) inferring privacy constraints that conflict with tasks, and (4) navigating ethical dilemmas where privacy conflicts with critical social norms. The paper's key finding is that current state-of-the-art models, including Gemini 2.5 Pro, GPT-4o, and Claude-3.5-haiku, exhibit a \"critical deficit\" in this area. For instance, models prioritized completing a task over a clear privacy constraint in up to 86% of cases, highlighting a fundamental misalignment that needs to be addressed for the safe deployment of embodied AI."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelty of the Problem: \n\nThe paper tackles a highly novel and critical problem. While LLM privacy is studied, the research is overwhelmingly focused on digital and textual data. This paper is one of the first to \"bridge this gap\" by formally defining and evaluating physically-grounded privacy for embodied agents. This is an urgent and forward-looking research direction as models are increasingly integrated with robotics.\n\n2. Novelty and Rigor of the Benchmark: \n\nThe key idea, the EAPrivacy benchmark itself, is a significant contribution. The four-tiered structure (Identification, Context, Inference, Dilemma) is logical, comprehensive, and escalates in difficulty in a way that effectively probes different facets of privacy awareness. Using structured PDDL formats and simulated multimodal cues (as shown in Appendix K) is a much more robust evaluation method for embodied agents than simple text prompts.\n\n3. Extensive and Solid Experiments: \n\nThe evaluation is thorough. The authors tested a wide range of 16 SOTA models, providing a comprehensive snapshot of the current landscape. The benchmark's scale (400+ scenarios, 60+ scenes) and the use of varying complexity (e.g., changing the number of distractor items in Tier 1) make the results reliable.\n\n4. Impactful Results and Qualitative Analysis: \n\nThe paper's findings are \"good\" in that they are clear, significant, and impactful. Discovering a \"critical deficit\" and a \"fundamental misalignment\" in top-tier models is a major finding for the AI safety and robotics communities. The detailed, qualitative case studies of why models fail (e.g., \"Asymmetric Social Conservatism\" in Tier 2, \"Literal Interpretation over Social Nuance\" in Tier 3) are a major strength, providing actionable insights beyond just quantitative scores.\n\n5. Clear and Surprising Findings: \n\nThe paper uncovers specific, counter-intuitive phenomena. The finding that models prioritize task completion over inferred privacy 86% of the time (Tier 3) is a stark, memorable statistic. Furthermore, the discovery of the \"negative effect of 'thinking'\" (Section 4.6), where enabling reasoning steps degraded performance, is a fascinating and important finding that challenges common assumptions about chain-of-thought prompting."}, "weaknesses": {"value": "1. Limitation of Simulation: \n\nThe paper's claims about the \"physical-world\" are based on a simulated environment. The models receive structured PDDL inputs and pre-parsed textual cues (e.g., \"Visual: 5 people at table...\"). This is a long way from the noisy, high-dimensional, and ambiguous data from real-world cameras and microphones. This \"sim-to-real\" gap is a major, albeit acknowledged, limitation.\n\n2. Limited Human Annotation: \n\nThe \"ground truth\" for subjective Tiers 2 (Appropriateness) and 4 (Ethical Dilemmas) is based on ratings from only \"five PhD-level raters\". This is a very small sample size for tasks that are inherently subjective and culturally sensitive. This small, expert-only pool may not capture the full range of human social norms.\n\n3. Inherent Cultural Bias: \n\nThe authors explicitly state that the benchmark is \"grounded in US-based legal and social norms\" (Section 3.4). This is a significant limitation for a benchmark on social and ethical norms, which vary dramatically across cultures. The paper's findings on \"appropriateness\" and \"social norms\" are, therefore, culturally specific and may not generalize globally.\n\n4. Lack of a Constructive Solution: \n\nThe paper is purely diagnostic—it excels at identifying and measuring a problem (\"critical deficit\"). However, it does not propose a constructive solution. It stops short of proposing a new alignment technique, a model architecture, or even releasing the benchmark as a fine-tuning dataset to help solve the identified misalignment.\n\n5. Ambiguity in \"Thinking\" Methodology: \n\nThe paper's interesting finding on the \"negative effect of 'thinking'\" (Section 4.6) is weakened by a lack of detail. The texts are vague about how this \"thinking\" mode was enabled or disabled (e.g., specific prompts, API parameters, chain-of-thought vs. zero-shot). This ambiguity makes the finding harder to reproduce and interpret."}, "questions": {"value": "1. Is it possible to validate the human-rated tiers (2 and 4) with a much larger and more culturally diverse group of annotators. This would strengthen the \"ground truth\" and allow for a valuable analysis of how privacy norms differ across populations.\n\n2. Is it possible to propose and test a baseline solution? For example, after creating the EAPrivacy-Train dataset, we could fine-tune a model (e.g., Llama-3.3-70B) on it and show how much its performance improves on the benchmark, providing a starting point for future research."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VP840Yyp3m", "forum": "WSpDZVEGNi", "replyto": "WSpDZVEGNi", "signatures": ["ICLR.cc/2026/Conference/Submission8190/Reviewer_5nsH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8190/Reviewer_5nsH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997713439, "cdate": 1761997713439, "tmdate": 1762920146846, "mdate": 1762920146846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents EAPrivacy, a benchmark designed to evaluate the physical-world privacy awareness of LLM-powered agents. It covers four tiers: 1) identification of sensitive objects; 2) inferring contextual appropriateness; 3) taking actions in accordance with privacy norms; 4) properly handling ethical dilemmas between privacy and societal benefits. The results reveal a prevalent lack of privacy awareness in the physical world. The analysis also reveals a counterintuitive trend where more thinking reduces the performance, and provides possible explanations and implies directions for future improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The investigation of the privacy awareness in physical world is a novel contribution to research in this area.\n- The four tiers are built on a principled and comprehensive framework that captures capabilities and challenges across multiple critical levels.\n- The evaluation reveals critical gaps in current models, demonstrating the value of the benchmark in guiding and continuously assessing models aimed at addressing this important issue."}, "weaknesses": {"value": "- In Tier 1, the example doesn't convincingly demonstrate the relevance of spatial location to the task. The scenario explicitly enumerates several items, and the prompt is simply to “list all sensitive objects,” which appears to have a direct textual mapping. As a result, it is unclear how much specialized spatial reasoning is actually required here, or to what extent the task meaningfully differs from standard text-level understanding.\n- With respect to spatial reasoning and spatial relationships, the paper does not provide sufficient analysis of why the models fail. It is unclear whether the errors stem from an inability to correctly interpret the physical spatial relationships between objects, from missing domain knowledge about privacy, or from an inability to perform the reasoning needed to connect privacy knowledge to the spatial layout. The paper reports results, but the analysis does not make it clear at which stage the failure occurs, particularly once reasoning is enabled.\n- There are also concerns regarding the benchmark’s ground truth. The evaluation centers on social norms and includes dilemma cases where competing needs may conflict. The authors rely on five “PhD-level” annotators to produce the labels, but it is unclear whether PhD-level expertise is actually relevant for this task. The task appears to rely more on shared societal norms than on specialized academic knowledge, so it is not obvious that these five annotators are an appropriate proxy for general social consensus.\n- Furthermore, the paper notes that even these annotators disagree with one another. This raises questions about the validity of the final ground truth, how interpersonal differences are handled, and how such disagreement should be interpreted if the benchmark is proposed as a potential alignment target for LLMs. The paper does not sufficiently analyze these issues. If this benchmark is to be used for broader model evaluation, a clearer understanding of its downstream impact is important"}, "questions": {"value": "- For Tier 1, how is spatial reasoning is involved? For example does it require spatial reasoning to know if certain object is visible or not visible, or would it fall back to a simple text-level identification of mentioning of sensitive objects?\n- How much of the failure can be attributed to a general failure in spatial reasoning, or a more specific failure in lacking privacy and spatial reasoning capabilities?\n- Does the high variance in human ratings (Figure 3) imply that a norm (i.e., shared consensus) might not exist in the selected scenarios? How would this affect the validity of the ground truth for evaluating the LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fo1odvsryd", "forum": "WSpDZVEGNi", "replyto": "WSpDZVEGNi", "signatures": ["ICLR.cc/2026/Conference/Submission8190/Reviewer_YGZ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8190/Reviewer_YGZ4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762047987060, "cdate": 1762047987060, "tmdate": 1762920146439, "mdate": 1762920146439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}