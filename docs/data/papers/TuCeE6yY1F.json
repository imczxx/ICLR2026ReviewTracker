{"id": "TuCeE6yY1F", "number": 10365, "cdate": 1758168302298, "mdate": 1763098443931, "content": {"title": "Head-wise Adaptive Rotary Positional Encoding for Fine-Grained Image Generation", "abstract": "Transformers rely on explicit positional encoding to model structure in data. While Rotary Position Embedding (RoPE) excels in 1D domains, its application to image generation reveals significant limitations such as fine-grained spatial relation modeling, color cues, and object counting. This paper identifies key limitations of standard multi-dimensional RoPE—rigid frequency allocation, axis-wise independence, and uniform head treatment—in capturing the complex structural biases required for fine-grained image generation.  We propose HARoPE, a head-wise adaptive extension that inserts a learnable linear transformation parameterized via singular value decomposition (SVD) before the rotary mapping. This lightweight modification enables dynamic frequency reallocation, semantic alignment of rotary planes, and head-specific positional receptive fields while rigorously preserving RoPE's relative-position property. Extensive experiments on class-conditional ImageNet and text-to-image generation (Flux and MMDiT) demonstrate that HARoPE consistently improves performance over strong RoPE baselines and other extensions. The method serves as an effective drop-in replacement, offering a principled and adaptable solution for enhancing positional awareness in transformer-based image generative models.", "tldr": "", "keywords": ["Position embedding; image generation; generative model"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/4dd07e1f2b6da421a53e2960f795a182c4cf2fa9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims at improving the rotary embedding in transformer achitectures. The method introduces a lightweight, head-specific learnable linear transformation that is applied to queries and keys before the standard rotary mapping. This transformation is parameterized via Singular Value Decomposition SVD. The authors conduct experiments on image understanding using ViT, image generation using DiT, and text-to-image generation wo validate the effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper writing is clear, and the method is simple and easy to follow.\n- The experiments covers multiple aspects, i.e., image understanding, image generation.\n- The ablations is detailed and clear."}, "weaknesses": {"value": "- The method seems engineered and lacks novelty.\n- The paper describes the method as lightweight. However, it introduces $N_{\\text{heads}} \\times d \\times d$ parameters per layer. This is a non-trivial increase compared to the parameter-free RoPE. The TFLOPS comparison in Table 3 is against the entire model, which obscures the relative cost of the new positional encoding. A clearer analysis of the parameter and FLOP overhead of the attention block itself would be more transparent.\n- The experiments are not sufficient. ViT-B is a small backbone in transformers and the performance gain can be potentially vanishing on large backbones like ViT-XL/DiT-XL."}, "questions": {"value": "Could you provide a more direct quantification of the parameter and FLOPs overhead? Specifically, what is the percentage increase in parameters and computation for an attention block when switching from ROPE to HAROPE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AKMTTwpyqI", "forum": "TuCeE6yY1F", "replyto": "TuCeE6yY1F", "signatures": ["ICLR.cc/2026/Conference/Submission10365/Reviewer_LZZx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10365/Reviewer_LZZx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921333243, "cdate": 1761921333243, "tmdate": 1762921689212, "mdate": 1762921689212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "hMOwyzmIdr", "forum": "TuCeE6yY1F", "replyto": "TuCeE6yY1F", "signatures": ["ICLR.cc/2026/Conference/Submission10365/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10365/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763098443100, "cdate": 1763098443100, "tmdate": 1763098443100, "mdate": 1763098443100, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a positional encoding scheme (HARoPE) that enhances the standard rotary positional encoding by introducing a learnable, head-wise linear transformation (via SVD) before the rotary mapping in the attention modules. They suggest that this design addresses limitations of rigid frequency allocation, axis-independence, and uniform head treatment in fine-grained image generation, resulting in improved structural bias modeling for image generation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper conducts thorough ablation studies to evaluate the contribution of each component of the proposed method, including multi-head specialization, orthogonal transformations, and the SVD-based design.\n- The authors present the background and related work clearly and coherently, making the technical context easy to follow and well-motivated."}, "weaknesses": {"value": "- The claim regarding \"misalignment with learned semantic subspaces\" may be overstated. Since the model is trained end-to-end with these relative positional embeddings, it likely adapts its semantic subspaces accordingly.\n- The claimed compositional improvements in text-to-image models are supported only by qualitative examples. Quantitative evaluation on established benchmarks such as T2I-CompBench would strengthen the paper’s evidence.\n- The comparisons in Table 4 may not be entirely fair, as the models compared have differing numbers of trainable parameters—performance gains could stem partly from increased parameter count.\n- The paper’s novelty is somewhat limited. The proposed SVD-based head-wise transformation essentially adds another linear mapping on top of the existing QKV projections. Given that attention already learns distinct projections for each head, the idea of head-specific transformations is not entirely new."}, "questions": {"value": "- You mentioned MMDiT and FLUX as two different baselines. Could you clarify which specific model you are referring to as “MMDiT”? Since FLUX itself is an MMDiT-based architecture, it’s unclear what separate MMDiT model you are comparing against."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H0cbYvc0vN", "forum": "TuCeE6yY1F", "replyto": "TuCeE6yY1F", "signatures": ["ICLR.cc/2026/Conference/Submission10365/Reviewer_aRKm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10365/Reviewer_aRKm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958122725, "cdate": 1761958122725, "tmdate": 1762921688771, "mdate": 1762921688771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a head-wise adaptive rotary positional encoding (HARoPE) for image generation, which enables dynamic frequency reallocation, semantic alignment of rotary planes, and head-specific positional receptive fields. The proposed HARoPE generates better performance than RoPE."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work is well-organized.\n- The motivation of the proposed HARoPE is clear and convincing. The proposed approach is simple and useful. \n- The experimental results demonstrate its effectiveness."}, "weaknesses": {"value": "This work claims that the proposed HARoPE enables dynamic frequency reallocation, semantic alignment of rotary planes, and head-specific positional receptive fields. However, the paper lacks detailed analysis or empirical evidence demonstrating that the introduced linear transformation layer indeed resolves these issues after training, which makes the argument for the method's effectiveness less convincing.\n\nMoreover, this additional linear transformation introduces extra parameters during fine-tuning and can also be interpreted as a feature transformation applied to the query and key."}, "questions": {"value": "- Does the network initialize and update $U_h$, $V_h$, and $\\sum_h$ during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IeDydNMGsX", "forum": "TuCeE6yY1F", "replyto": "TuCeE6yY1F", "signatures": ["ICLR.cc/2026/Conference/Submission10365/Reviewer_fYQT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10365/Reviewer_fYQT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995594925, "cdate": 1761995594925, "tmdate": 1762921688268, "mdate": 1762921688268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}