{"id": "ra7CSHcVCv", "number": 17607, "cdate": 1758278252301, "mdate": 1759897165120, "content": {"title": "Supporting High-Stakes Decision Making Through Interactive Preference Elicitation in the Latent Space", "abstract": "High-stakes, infrequent consumer decisions, such as housing selection, challenge conventional recommender systems due to sparse interaction signals, heterogeneous multi-criteria objectives, and high-dimensional feature spaces. \nThis work presents an interactive preference elicitation framework that couples preferential Bayesian optimization (PBO) with two complementary components: (i) large language models (LLMs) that interpret natural language input to produce personalized probabilistic priors over feature utility weights to mitigate cold start, and (ii) an autoencoder (AE)-based latent representation that reduces effective dimensionality for sample-efficient exploration. The framework learns a latent utility function from user pairwise comparisons observed and integrated in real-time.\nWe evaluate the developed method on rental real estate datasets from two major European cities. The results show that executing PBO in an AE latent space improves final pairwise ranking accuracy by 12%. For LLM-based preference prior generation, we find that direct, LLM-driven weight specification is outperformed by a static prior, while probabilistic weight priors that use LLMs only to rank feature importance achieve 25% better pairwise accuracy on average than a direct approach.", "tldr": "", "keywords": ["Bayesian optimization", "preference elicitation", "autoencoder", "LLM"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/106c076324bdf08e46fa72adf560df985bdd8322.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose an interactive preference elicitation (PE) framework based on preferential Bayesian optimization (PBO).\n\nIt learns a latent utility function for the user from pairwise comparisons queried to that user.\n\nTo address the cold start issue and obtain informative probabilistic priors of feature weights for the utility function, they use an LLM-guided user interview, instead of a predefined static weight vector.\n\nThey also use an autoencoder (AE) to obtain a latent representation of lower dimension that can be used improve the sample efficiency of exploration.\n\nThey evaluate their method on rental market datasets from two European cities (Madrid, Spain and Munich, Germany)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper is well-motivated and addresses a real-world problem. As the authors state in the conclusion, it has immediate applications for online real estate platforms, where it could reduce user fatigue by minimizing the number of property comparisons needed to identify suitable options.\n\nThe presentation of the paper is mostly clean and easy to follow. Notation is properly introduced, and the order in which concepts are introduced is good.\n\nThe experimental results are detailed, including a high number of runs, graphs with confidence intervals, and a table with numerical metrics."}, "weaknesses": {"value": "Experiments are limited to rental markets in two cities: Madrid, Spain and Munich, Germany.\n\nIt would be nice to see experiments beyond the rental market setting, to get a better assessment of how well the method performs in other domains.\n\nThe LLM that the authors used, Gemini 2.5 Flash-Lite, is closed-source.\n\nThe Munich dataset that the authors used is not publicly available due to licensing issues.\n\nMinor corrections:\n\nPage 3: Replace r << d with r \\ll d.\n\nPage 6: Replace \"as activation function\" with \"as the activation function\".\n\nSuggestions:\n\nPage 7: Since there's space, include the formula for NDCG@k."}, "questions": {"value": "Page 4: \"we do not explicitly denote data normalization\" What does this mean?\n\nHave you experimented with other LLMs, besides Gemini 2.5 Flash-Lite? Why not experiment with an open-source LLM? It would be good to run the experiments with other LLMs, to see how much the choice impacts performance.\n\nHow did you select the personas?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fthA1OftbF", "forum": "ra7CSHcVCv", "replyto": "ra7CSHcVCv", "signatures": ["ICLR.cc/2026/Conference/Submission17607/Reviewer_tdZs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17607/Reviewer_tdZs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761778811851, "cdate": 1761778811851, "tmdate": 1762927468696, "mdate": 1762927468696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets high-stakes, sparse-interaction decisions such as housing selection, where conventional recommender systems fail due to limited feedback and high-dimensional features. To address this, it proposes an interactive preference elicitation framework that combines Preferential Bayesian Optimization (PBO) with two key components: LLM-based probabilistic priors, which interpret natural-language interviews to initialize user utility weights and mitigate cold start; and an Autoencoder-based latent representation, which reduces dimensionality for efficient exploration. The system learns a latent utility model from pairwise user comparisons using a Gaussian-process surrogate and qEUBO acquisition for adaptive querying. Evaluations on real-estate datasets from Madrid and Munich show that latent-space PBO improves ranking accuracy and that LLM-guided priors significantly enhance sample efficiency and cold-start performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles an important and realistic problem of learning preferences in high-stakes, sparse-feedback decision settings. It proposes a coherent framework that integrates LLM-based prior elicitation, Autoencoder latent-space learning, and Preferential Bayesian Optimization into a unified probabilistic approach. The method is well-founded, combining Gaussian Process modeling with qEUBO-based active querying for efficient preference learning. Experiments on real-estate datasets show clear improvements in ranking accuracy and cold-start performance, demonstrating both methodological novelty and strong practical relevance."}, "weaknesses": {"value": "## Weaknesses\nA central modeling limitation lies in the treatment of noise within the preference likelihood. The paper explicitly assumes that both the user’s preference inconsistency and the autoencoder (AE) reconstruction error can be modeled as *jointly Gaussian and additive*. In Appendix A.1 (lines 593 page 11), the authors state that the decoder output can be written as $ \\hat{x} = h_\\theta(g_\\theta(x)) = x + \\epsilon $ with $ \\epsilon \\sim \\mathcal{N}(0, \\Sigma_\\epsilon) $, and that the user’s utility varies locally linearly, $ u(\\hat{x}) \\approx u(x) + \\nabla u(x)^\\top \\epsilon $. Combining these yields a single Gaussian error term $ \\eta $ with variance $ \\sigma^2 = \\sigma_{\\text{pref}}^2 + \\sigma_{\\text{recon}}^2 $ in the Probit likelihood, expressed as  \n$ P(z \\succ z') = \\Phi\\!\\left(\\frac{u(x) - u(x')}{\\sigma}\\right) $,  \nwhere $ \\sigma $ captures both preference noise and AE uncertainty (page 5).  \nWhile elegant, this simplification conflates heterogeneous uncertainty sources—human inconsistency, model reconstruction bias, and latent-space distortions—into one scalar variance term. It presumes homoscedastic, isotropic noise and local linearity of the utility function, which are rarely valid in complex, nonlinear real-estate feature spaces. In practice, AE errors vary significantly across regions of the feature manifold, and user responses exhibit contextual, multimodal variability. This global Gaussian assumption therefore risks *underestimating epistemic uncertainty*, leading to overconfident posterior estimates under the Laplace approximation and potentially premature exploitation in qEUBO selection.  \n\nBeyond these statistical concerns, the model’s abstraction of user feedback neglects potential heteroscedastic and structured noise patterns. Real users exhibit region-dependent reliability—for example, being consistent on low-price listings but noisy on high-price ones—yet the framework treats all feedback as equally noisy."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4qit3tanej", "forum": "ra7CSHcVCv", "replyto": "ra7CSHcVCv", "signatures": ["ICLR.cc/2026/Conference/Submission17607/Reviewer_EyRm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17607/Reviewer_EyRm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828830902, "cdate": 1761828830902, "tmdate": 1762927468176, "mdate": 1762927468176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an interactive preference elicitation framework for complex, infrequent decisions like housing purchase choice. It combines LLMs (to generate personalized priors from natural language) and an autoencoder (to reduce feature dimensionality) within a PBO that learns user utility from pairwise comparisons in real time, evaluated on rental datasets from two European cities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written with clear structures. The problem itself is well motivated."}, "weaknesses": {"value": "The intuition for the selection of each structure for each purpose could be strengthened. Please refer to the questions."}, "questions": {"value": "- In line 64-67, what is the purpose/intuition of still using LLM for the continuous feature space? Even after reading Section 3.2.1, my question about this does not go away.\n- In line 82, you may want to elaborate/define the meaning of ``interactive preference elicitation'' which has appeared in the abstract & title but nowhere else before line 82 in the intro.\n- What is the purpose of the warm-start, and any more empirical results to show the importance of this?\n- In the experiment, what is the intuition for the vanilla (statistical) having an opposite trend (increase first and then decrease over the number of queries) with other benchmarks? \n\nI might raise my rating after seeing the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TYbDA2CAAo", "forum": "ra7CSHcVCv", "replyto": "ra7CSHcVCv", "signatures": ["ICLR.cc/2026/Conference/Submission17607/Reviewer_9kWw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17607/Reviewer_9kWw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981748979, "cdate": 1761981748979, "tmdate": 1762927467837, "mdate": 1762927467837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}