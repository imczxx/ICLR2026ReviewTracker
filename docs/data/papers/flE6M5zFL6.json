{"id": "flE6M5zFL6", "number": 13007, "cdate": 1758212647280, "mdate": 1763670172084, "content": {"title": "DiVE-k: DIFFERENTIAL VISUAL REASONING FOR FINE-GRAINED IMAGE RECOGNITION", "abstract": "Large Vision Language Models (LVLMs) possess extensive text knowledge but struggles to utilize this knowledge for fine-grained image recognition, often failing to differentiate between visually similar categories. Existing fine-tuning methods using Reinforcement Learning (RL) with exact-match reward signals are often brittle, encourage memorization of training categories, and fail to elicit differential reasoning needed for generalization to unseen classes. To address this, we propose $\\textbf{DiVE-k}$, $\\textbf{Di}$fferential $\\textbf{V}$isual r$\\textbf{E}$asoning using top-$\\textbf{k}$ generations, framework that leverages model's own top-k predictions as a training signal. \nFor each training image, DiVE-k creates a multiple-choice question from the model's top-k outputs and uses RL to train the model to select the correct answer. This approach requires the model to perform fine-grained differential reasoning among plausible options and provides a simple, verifiable reward signal that mitigates memorization and improves generalization. \nExperiments on five standard fine-grained datasets show that our method significantly outperforms existing approaches. \nIn the standard base-to-novel generalization setting, DiVE-k surpasses the QWEN2.5-VL-7B and ViRFT by 10.04% and 6.16% on the Harmonic Mean metric, respectively. Further experiments show similar gains in mixed-domain and few-shot scenarios.", "tldr": "", "keywords": ["zero-shot image classification", "visual reasoning", "vision-language model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/85f321fc59f2181c09f7467193639e114fe09e5f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces the DiVE-k framework, which significantly enhances the base model's fine-grained image recognition capabilities by leveraging its top-k prediction results to construct Multiple-Choice Question (MCQ) data, and subsequently fine-tuning the model using the GRPO algorithm for Reinforcement Learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Using the model's own top-k predictions as training data is interesting and insightful, which serves as a form of hard-negative mining against the model's confusions.\n2. DiVE-k achieves significant performance improvements over baseline models across multiple datasets and tasks.\n3. The overall writing and presentation of the paper is good."}, "weaknesses": {"value": "1. Limited comparison with related work: The paper primarily contrasts its results with ViRFT. To comprehensively validate the efficacy of the proposed method, it should be compared against additional approaches mentioned in the \"Related Work\" section.\n2. Lack of diverse backbone models: The current experiments are exclusively conducted on Qwen2.5-VL. It is crucial to demonstrate the generalizability of DiVE-k by performing experiments on different LVLMs, such as confirming performance improvements on models like InternVL."}, "questions": {"value": "What would the performance be if the obtained data were used directly for Supervised Fine-Tuning (SFT)? This would help in understanding the advantage of using RL."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BhRj59qgFC", "forum": "flE6M5zFL6", "replyto": "flE6M5zFL6", "signatures": ["ICLR.cc/2026/Conference/Submission13007/Reviewer_CNJv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13007/Reviewer_CNJv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760950813163, "cdate": 1760950813163, "tmdate": 1762923752167, "mdate": 1762923752167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DiVE-k, a framework to improve fine-grained visual recognition (FGVR) in Large Vision Language Models (LVLMs). The core idea is to address the model's inability to differentiate between visually similar categories. The method first uses an offline step where the base model generates $K$ rollouts to create a top-k set of candidate answers for each image. This set is then used to formulate a Multiple-Choice Question (MCQ). In the second step, the model is trained using Reinforcement Learning (RL) with a simple, verifiable reward (i.e., selecting the correct option letter) to perform differential reasoning among these pre-defined options. The experiments show strong performance gains over baselines like ViRFT and the base QWEN2.5-VL-7B model."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The primary strength of this paper is its strong empirical performance. The proposed DiVE-k method achieves significant improvements in base-to-novel generalization, mixed-domain, and few-shot settings, as shown in Tables 1, 2, and 3. The qualitative examples in Figures 4 and 9 are also compelling, illustrating that the model learns to perform more detailed differential reasoning when forced to choose from a set of plausible, similar options, which is a key challenge in FGVR."}, "weaknesses": {"value": "Despite the strong results, I have major concerns about the methodological choices and novelty of this work.\n* Limited Novelty: The proposed method's novelty is marginal. At its core, it is a two-step process: 1) an offline data curation step that converts an open-ended generation task into a multiple-choice classification task, and 2) a standard RL training step (GRPO) on this new task. The \"differential reasoning\" appears to be a direct consequence of this prompt reformatting (from open-ended to MCQ), rather than from a new algorithmic insight.\n* Questionable Offline Top-k Generation: The decision to use a static, offline set of top-k options generated by the reference model ($\\pi_{ref}$) is a significant weakness. This means the policy ($\\pi_\\theta$) is trained on a fixed set of problems that were defined by an older version of itself. This introduces a distribution mismatch and severely limits the model's learning. The model is not learning to generate better candidates itself; it's only learning to rank a fixed set of candidates provided to it.Lack of a \n* Principled RL Formulation: A more sound and principled approach would be a dynamic, multi-step RL process. For instance, a multi-turn RL agent could first generate its own top-k candidates in a \"generation\" phase and then, in a \"reasoning\" phase, select the best one. The entire sequence would then receive a reward. The current method, by decoupling candidate generation (offline) from candidate selection (RL), feels like an ad-hoc pipeline rather than an end-to-end reasoning framework."}, "questions": {"value": "* The core of the method is the offline top-k generation. Why was this choice made over a dynamic, multi-turn RL formulation where the policy first generates its own options and then selects from them within a single episode?\n* Could the authors please provide a comparison against a baseline where the options are generated dynamically by the current policy ($\\pi_\\theta$) at each training step, rather than fixed offline by $\\pi_{ref}$?\n* Please provide a comparison against a more complete \"multi-turn RL\" baseline, as described in Weakness #3. This seems like a more correct and challenging setup for this problem.\n*Given that the main change is reformatting the problem as an MCQ, how much of the gain is simply from this prompt engineering versus the RL training itself?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no concerns"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JhVtvIoE3g", "forum": "flE6M5zFL6", "replyto": "flE6M5zFL6", "signatures": ["ICLR.cc/2026/Conference/Submission13007/Reviewer_G36b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13007/Reviewer_G36b"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761542093505, "cdate": 1761542093505, "tmdate": 1762923751759, "mdate": 1762923751759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DiVE-K formulated finegrained classification task into a self-generated MCQ leveraging the base model reasoning ability. This forces the VLM to perform reasoning using a differentiable rewards, leading to better generalization. The paper is well written and the framework is empirically validated, marking it as a strong contribution. Only concern is, there is no/lack of evidences to show the failure scenarios of the proposed solution."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Formualation**: DiVE-K formulated the task using base model's top-k predictions as a source hard-negative examples to construct the MCQ, and leverages model's reasoning ability with a differentiable reward system, making it a highly effective training method.\n\n- **Differentiable Reasoning**- The MCQ format inherently encourages the model to focus on attribute level discriminative reasoning, which beneficial for semantically similar concepts.\n\n- **Reward**: Simple reward based on MCQ index selection overcomes the existing string matching proposal in previous RL based system.\n\n- **Experiments**: The proposed method is empirically evaluated and ablated under different settings. The performance of the DiVE-k is significantly improved with the proposed mechanisms making it as SOTA of the task."}, "weaknesses": {"value": "- **Senstivity to roll outs and MCQ size**:  The performance of DiVE-k is heavily relies on K (number of rollouts) and m (Size of the final MCQ). While it is stated some processing to keep consistency, but there is no ablation on how variations in K and m affect the quality of the negative set and final performance.\n\n- **Failure cases**: There's systematic analysis/discussions  when this differentiable  reasoning analysis could fail, because this reasoning in next step relies on base model capacity of identifying and including the ground truth in the rollouts. Therefore, it naturally raises question, what is the bottleneck of the proposed solution: initial option mining or differential reasoning chain. For example, in Fig 5, performance on Pets dataset drops when increasing the  top-k generations. Why did that happen?\n\n- **Applicability**: As mentioned earlier, the performance of the base model could influence the final performance. Authors could consider testing the algorithms with different models to demonstrate the proposed solution as model-agnostic. \n\n- **Evaluation**: Figure 8: Provides a prompt intended to evaluate the fine-grained image classification results. Is this used for only property models or all the experiments?  If it is used for open-sourced model as well, what is the reason?. Given the prediction and groundtruth, performance score can be easily evaluated. Beyond accuracy, there's no other evaluation metric considered in the analysis."}, "questions": {"value": "1.  Could authors provide computational cost as it involves multiple step pipeline during training and inference?\n2. Could authors include zero-shot performance of the CLIP model on same classes (which can be obtained from original paper)? It will inform how models trained with differ in performance on same dataset?\n3. Why objective function is moved to supplementary?\n4. Does Table 1 has any results with supervised finetuning?If not, could authors include it as well.\n5. I’m unsure if this is a typo or an actual output. In Figure 3, all the reasoning steps of DiVE-k compare “X3” and “X6”, but the final prediction is B (which I assume is the second prediction from the top-k). However, the reasoning step itself contains a statement that it cannot be “X5”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uVRCcIjg0G", "forum": "flE6M5zFL6", "replyto": "flE6M5zFL6", "signatures": ["ICLR.cc/2026/Conference/Submission13007/Reviewer_TDsH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13007/Reviewer_TDsH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797501795, "cdate": 1761797501795, "tmdate": 1762923751392, "mdate": 1762923751392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DiVE-k framwork which uses the top-k generation of base model as a training signal for fine-grained image classification. For each training image, DiVE-k creates a multiple-choice question from the model's top-k outputs and uses RL to train the model to select the correct answer. Experiments on standard base-to-novel generalization and mixed-domain zero-shot base-to-novel generalization demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It transfers the open-world classification task into closed-world classification task, which is a promising way to settle the problem of brittle exact string-match reward.\n2. The evaluation metrics is reasonable for fine-grained classification task. Previous work typically uses string matching to evaluate the accuracy, while this paper uses the LLM to determine whether the prediction and ground truth belong to the same fine-grained category or not."}, "weaknesses": {"value": "1. A direct way to construct the hypotheses set is to select the most similar top-k categories by CLIP text features, and the advantage of the proposed offline option mining lacks experimental support.\n2. Since the framework uses a two-step pipeline with chain-of-thought, it incurs additional computational cost due to the requirement of two forward passes.\n3. The final accuracy heavily depends on the recall of the first inference step, which is not presented in the experimental results."}, "questions": {"value": "1. What is the performance if the ground truth label is already included in the options, i.e., the typical closed-world multiple-choice setting of evaluating LVLM's fine-grained classification performance?\n2. What is the performance if the model is trained to obtain options and do differential reasoning in one step?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3drtg9GGmh", "forum": "flE6M5zFL6", "replyto": "flE6M5zFL6", "signatures": ["ICLR.cc/2026/Conference/Submission13007/Reviewer_Gbyt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13007/Reviewer_Gbyt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883776237, "cdate": 1761883776237, "tmdate": 1762923750914, "mdate": 1762923750914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}