{"id": "bhR00j6Mku", "number": 14080, "cdate": 1758228144764, "mdate": 1759897391573, "content": {"title": "On The Fragility of Benchmark Contamination Detection in Reasoning Models", "abstract": "Leaderboards for large reasoning models (LRMs) have turned evaluation into a competition, incentivizing developers to optimize directly on benchmark suites. A shortcut to achieving higher rankings is to incorporate evaluation benchmarks into the training data, thereby yielding inflated performance, known as benchmark contamination. Despite that numerous contamination detection approaches have been proposed, surprisingly, our studies find that evading contamination detections for LRMs is alarmingly easy. We focus on the two scenarios where contamination may occur in practice: (I) when the base model evolves into LRM via supervised fine-tuning (SFT) and reinforcement learning (RL), we find that contamination during SFT can be originally identified by contamination detection methods. Yet, even a brief Group Relative Policy Optimization (GRPO) training can markedly \\textbf{conceal contamination signals} that most detection methods rely on. Further empirical experiments and theoretical analysis indicate that Proximal Policy Optimization (PPO) style importance sampling and clipping objectives are the root cause of this detection concealment, indicating that \\textbf{a broad class of RL methods} may inherently exhibit similar concealment capability; (II) when SFT contamination with CoT is applied to advanced LRMs as the final stage, most contamination detection methods \\textbf{perform near random guesses}. Without exposure to non-members, contaminated LRMs would still have more confidence when responding to those unseen samples that share similar distributions to the training set, and thus, evade existing memorization-based detection methods. Together, our findings reveal the unique vulnerability of LRMs evaluations: Model developers could easily contaminate LRMs to achieve inflated leaderboards performance while leaving minimal traces of contamination, thereby strongly undermining the fairness of evaluation and threatening the integrity of public leaderboards. This underscores the urgent need for advanced contamination detection methods and trustworthy evaluation protocols tailored to LRMs.", "tldr": "", "keywords": ["Benchmark Contamination", "Large Reasoning Model", "Benchmark Contamination Detection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5bc5561370377afed20219616c2805d9ab76aca2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies benchmark contamination for LRMs in two practical scenarios: 1) SFT contamination on a base model is initially detectable, but a small amount of RL greatly conceals contamination signals across many existing detection methods. 2) When advanced LRMs are SFT-contaminated with CoT on benchmark samples, pass@1 improves significantly while the contamination detection becomes near random. The paper provides theoretical reasoning for this phenomenon by showing the gap in the log-likelihoods of benchmark members' and non-members' contracts after RL training. Such contraction is attributed to the common importance-sampling + clipping trick in RL."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper looks into benchmark contamination, which is a timely and important issue in modern LLM evaluation. Showing that light RL can significantly mask contamination signals is surprising and consequential.\n\n2. The paper clearly splits the study into two realistic scenarios, including 1) base-model SFT contamination followed by RL, and 2) CoT contamination of advanced LRMs, making the threat model concrete and realistic. It also clarifies where current detectors fail and why mitigation is nontrivial.\n\n3. Attributing the contamination concealment to the importance weighting + clipping is interesting. This claim is supported by theory and empirical evidence in the paper.\n\n4. The paper includes extensive experimental evaluation across a wide range of representative detectors and reasoning benchmarks."}, "weaknesses": {"value": "1. Table 1 suggests RL brings little or no gain on clean data. Is the RL correctly applied/tuned in the experiments? Why does RL not further help with reasoning even when there is no contamination?\n\n2. Some prior works have shown that contamination detection can be evaded in LLMs. A clearer positioning relative to these closely related papers could be added to the main text (currently largely in the appendix/references)."}, "questions": {"value": "1. The main results are on math tasks with long CoT. Can the post-LRM “near-random detection” result replicate on coding, QA, and other tasks where solutions are relatively less CoT-heavy?\n\n2. Do larger/smaller models strengthen or weaken contraction? Is there any non-monotonicity regarding model size (e.g., medium models most vulnerable)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QEmMBE1rHE", "forum": "bhR00j6Mku", "replyto": "bhR00j6Mku", "signatures": ["ICLR.cc/2026/Conference/Submission14080/Reviewer_F3s6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14080/Reviewer_F3s6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868507180, "cdate": 1761868507180, "tmdate": 1762924558808, "mdate": 1762924558808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the fair evaluation of Large Reasoning Models on public benchmarks, showing that contamination that can be detected from SFT can be effectively concealed using GRPO, pushing many contamination detection methods to near-random performance, while the contaminated LRMs retain improved performance over their uncontaminated counterparts."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Benchmark contamination is a critical and timely problem to address, and the interplay between GRPO and contamination detection is interesting to investigate.\n- Experiments are well-designed: There is a clear concealment effect of GRPO on almost all detection methods, and current contamination detection approaches are clearly inadequate for ensuring the integrity of public leaderboards."}, "weaknesses": {"value": "The main weakness comes from the underlying motivation for this work. Primarily, it seems the authors are highlighting that contamination detection methods are ineffective, yet it is unclear whether these methods are being used in practice. Recent work has highlighted pitfalls in data contamination approaches [1,2] and prohibiting training on test tasks [3]. A further discussion on how this work fits into prior work on fair evaluation and data contamination would strengthen the work.\n\nIn addition, do the authors believe that tailoring contamination detection methods will be the way forward for ensuring faithful evaluation when developers are incentivized to game benchmarks?\n\nMinor\n- Line 156 \"Olypaid\"\n\n[1] Liu, Ken Ziyu, et al. \"Language models may verbatim complete text they were not explicitly trained on.\" arXiv preprint arXiv:2503.17514 (2025).\n\n[2] Fu, Yujuan, et al. \"Does data contamination detection work (well) for llms? a survey and evaluation on detection assumptions.\" arXiv preprint arXiv:2410.18966 (2024).\n\n[3] Dominguez-Olmedo, Ricardo, Florian E. Dorner, and Moritz Hardt. \"Training on the test task confounds evaluation and emergence.\" arXiv preprint arXiv:2407.07890 (2024)."}, "questions": {"value": "- Could the authors clarify their claims in the Discussion and Conclusion? Specifically the claim \"This fundamentally challenges the assumption that all the detection approaches rely on, which is that benchmark contamination is more about memorizing the benchmark samples.\"\n- Could the authors provide additional details about why GRPO does not improve concealment on the Verbatim/Neighbor attacks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VeWi1O8U9O", "forum": "bhR00j6Mku", "replyto": "bhR00j6Mku", "signatures": ["ICLR.cc/2026/Conference/Submission14080/Reviewer_hTiN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14080/Reviewer_hTiN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939269317, "cdate": 1761939269317, "tmdate": 1762924558088, "mdate": 1762924558088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates contamination detection methods for benchmarks in base models when they are being conditioned as LRMs via SFT and RL, and in advanced Large Reasoning Models via SFT with CoT contaminated data. They find that GRPO conceals contamination, and minimal detectable traces are left. 10 different detection methods and 6 mathematical reasoning benchmarks are used. A theoretical analysis is provided to show that PPO-style clipping and importance sampling are the root cause of the concealment. The paper makes the key claim that memorization based detection are not optimal for LRMs, and that these can evade it easily."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- It is highlighted that almost all the contamination detection methods (tested) consistently perform near random guesses in all the benchmarks after performing SFT on the benchmark samples with Co. This is a very significant insight.\n- The paper is thorough in exploring its claim within the mathematical reasoning domain, using 6 common benchmarks in the field. And in using a wide range of detection methods (10).\n- It is comprehensive in both providing results to support its claim and a thorough theoretical analysis of the premise"}, "weaknesses": {"value": "- The authors mention benchmark performance after SFT but do not mention results after SFT and GRPO.\n- While enough benchmarks from the mathematical reasoning domain are used, no other domain is surveyed to make the points the paper raises applicable globally."}, "questions": {"value": "- Do you have some figures on the length of outputs across the paper? Have you found any tested detection methods sensitive to output length?\n- How do you generate your CoT RL data for each benchmark?\n- In the first proposed direction in your conclusion (lines 477-478), how would the release of intermediate training checkpoints help with the issues the paper raises?\n- Do you generate your own contaminated CoT and SFT data? If so, can you share it and how it was generated?\n- Embedding based methods are mentioned but not used, why?\n\nSuggestions:\n- In line 72,can you clarify what you mean by “in the later stage”.\n- In line 81, what kind of contamination is this? Is it verbatim contamination, paraphrased, etc..\n- In the theoretical analysis it would be good to explicitly define E in formula 1 (and subsequent mentions).\n- Given that this paper could aid malicious actors, can you provide more concrete recommendations for evaluators in the conclusion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DfUchtcReY", "forum": "bhR00j6Mku", "replyto": "bhR00j6Mku", "signatures": ["ICLR.cc/2026/Conference/Submission14080/Reviewer_enZz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14080/Reviewer_enZz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975428285, "cdate": 1761975428285, "tmdate": 1762924557560, "mdate": 1762924557560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}