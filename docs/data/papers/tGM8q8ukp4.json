{"id": "tGM8q8ukp4", "number": 10774, "cdate": 1758181637643, "mdate": 1763722629643, "content": {"title": "RouteJudge: Benchmarking LLM-as-a-Judge with Routing Strategies", "abstract": "Large language models (LLMs) and large reasoning models (LRMs) are increasingly adopted as automated judges for pairwise evaluation of model outputs. However, their deployment faces three unresolved challenges: inconsistent reliability, high latency and token costs, and the lack of principled routing strategies. We introduce RouteJudge, the first unified framework for benchmarking and routing automated judges under accuracy–latency–cost trade-offs. Our contributions are threefold. (1) We construct six difficulty-aware datasets spanning reasoning (Math, Logic, Code) and non-reasoning (Knowledge, Roleplay, Writing) tasks, with human-verified gold standards. (2) We present the first benchmark of LRM-as-a-Judge, analyzing how intermediate thinking traces interact with final verdicts and uncovering systematic mismatches such as “good thinking but wrong verdict.” (3) We develop and evaluate both offline and online routing strategies that adaptively assign judges per instance, achieving strong accuracy–efficiency trade-offs. Experiments on 19 models show that LRMs improve reasoning accuracy at higher cost, while difficulty-aware online routing narrows this gap substantially. By unifying benchmarking and routing, RouteJudge establishes the first comprehensive framework for scalable and interpretable evaluation, positioning automated judges as a practical alternative to human experts.", "tldr": "We introduce RouteJudge, a unified framework that addresses these issues through adaptive routing across a pool of judges.", "keywords": ["Route; Judge; Large Reasoning Models; Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/89ca656555f50afe6db20d0f2b619fcbf0f55d60.pdf", "supplementary_material": "/attachment/ebd1ab08e7e14dec913ef3002864b86aeca568ca.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces RouteJudge, a framework to benchmark and route automated judges (LLMs and “Large Reasoning Models,” LRMs) under accuracy–latency–cost trade-offs. The authors (1) build six difficulty-aware pairwise datasets spanning reasoning (Math/Logic/Code) and non-reasoning (Knowledge/Roleplay/Writing) with human-verified gold labels, (2) provide what they claim is the first benchmark of LRM-as-a-Judge including an analysis of mismatches between thinking traces and final verdicts, and (3) design offline and online routing strategies that adapt judge selection to budgets and difficulty, reporting gains over fixed single-judge baselines on 19 models."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Timely and practical: Evaluating and routing judges under budget constraints reflects real deployment needs. The multi-objective utility and explicit abstention option are appropriate.\n- Breadth of models and domains: 19 models, covering both LLMs and LRMs across reasoning/non-reasoning; domain-specific breakdowns reveal heterogeneity.\n- Difficulty-aware design: The easy/medium/diff (and “-Think”) splits support nuanced routing and robustness analysis; cross-bucket “unresolved” rates are informative.\n- Thinking-vs-verdict diagnostics: Error-type distributions and confusion heatmaps capture “good thinking but wrong verdict” vs “flawed thinking but right verdict” patterns—useful for future calibration work."}, "weaknesses": {"value": "- Ground-truth agreement and reliability under-reported. The corpus relies on 7-rater majority voting, but there’s no explicit reporting of inter-annotator agreement, tie rates, or label uncertainty per item—critical for judging “upper bounds” and router training targets.\n- Metrics and statistical rigor. Accuracy is the main metric; there’s no reporting of confidence intervals, statistical significance, or bootstrap tests across models/routers. This makes small improvements hard to interpret. (Figures show trends; quantification is missing.)\n- Router supervision and leakage risk. Online routers are trained on “inexpensive features” and an oracle derived from offline utilities. It’s not fully specified whether any instance-level signals could leak answer keys (e.g., gold label correlations, domain heuristics that inadvertently encode difficulty labels created with the same dataset). Clearer train/val/test segregation for router fitting is needed.\n- Limited human-expert baseline depth. An “Expert (Human & GPT-4o)” row appears, but the setup is not described enough (selection criteria, consistency, time/cost) to anchor the scale as a target for routing. \n- Cost/latency measurement protocol. Token-cost and latency normalization across closed vs open models, local vs remote inference, and different context lengths isn’t deeply documented; variability across runs (cold vs warm) is not quantified."}, "questions": {"value": "- What are Krippendorff’s α / Fleiss’ κ per domain? What fraction of items are ties or near-ties (e.g., 4–3 splits), and how do routers behave on those?\n- Bias controls: Beyond randomizing A/B order, did you quantify position bias and verbosity bias per judge? Any adversarial flips?\n- Provide CIs and paired tests (e.g., stratified bootstrap over items) for accuracy/time/cost, especially when claiming that online routers close/surpass offline heuristics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vlJNHwzxEa", "forum": "tGM8q8ukp4", "replyto": "tGM8q8ukp4", "signatures": ["ICLR.cc/2026/Conference/Submission10774/Reviewer_tuWm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10774/Reviewer_tuWm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848528252, "cdate": 1761848528252, "tmdate": 1762921984540, "mdate": 1762921984540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RouteJudge, a benchmark for evaluating LLM-as-a-Judge routing strategies. The authors propose and evaluate various offline and online routing strategies from accuracy, latency, and cost perspectives. The authors also benchmark large reasoning models (LRMs)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is motivated and well-written.\n2. The evaluation is comprehensive, spanning 19 models.\n3. The paper makes prescriptive suggestions backed with evidence regarding when certain judges are better than others, and when certain routing strategies may be better than others."}, "weaknesses": {"value": "1. The main paper is missing some technical details. For example, regarding the RouteJudge dataset, it's not clear (1) which datasets it was constructed from, (2) how the instances were assigned a difficulty, and (3) which models the candidate responses come from.\n2. This is not the first work to benchmark LRMs in the LLM-as-a-Judge context. For example, JudgeBench [1] evaluates several LRMs and draws similar conclusions (e.g., LRMs tend to outperform LLMs on reasoning-heavy tasks).\n3. In real-world settings, automatic evaluations often span multiple dimensions (e.g., helpfulness, factuality, safety). Accounting for this in the routing would offer more practical benefit.\n\n[1] https://arxiv.org/abs/2410.12784"}, "questions": {"value": "1. Could you clarify this \"thinking\" variant of the RouteJudge dataset? Initially, I thought it aligned with the reasoning/non-reasoning split, but that doesn't appear to be the case in Table 1. Do the candidate responses themselves contain the thinking? If so, does the judge also get to see the thinking?\n2. In L.250-251, you say \"All models are evaluated under standardized decoding settings (temperature = 0, maximum output length = 1024) to ensure comparability across runs.\" Is this the model that generated the responses? Or is it the judge models? Is this also true for LRMs, which typically degrade at temp=0 and require far more output tokens?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tLhWbeUPN5", "forum": "tGM8q8ukp4", "replyto": "tGM8q8ukp4", "signatures": ["ICLR.cc/2026/Conference/Submission10774/Reviewer_cqw2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10774/Reviewer_cqw2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952883477, "cdate": 1761952883477, "tmdate": 1762921984073, "mdate": 1762921984073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"RouteJudge,\" a unified framework designed to benchmark and optimize the use of \"LLM-as-a-Judge\" by introducing routing strategies. \n\nThe authors propose the following:\n* A new difficulty-aware, human-verified dataset spanning six domains (Math, Logic, Code, Knowledge, Roleplay, Writing).\n* An analysis of the \"thinking traces\" of the LLMs\n* design and evaluate different routing strategies"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper provides an analysis of how a judge's explicit reasoning aligns—and misaligns—with its final decision. The confusion matrix in Figure 6c, which identifies \"good thinking + wrong verdict\" (JR/TW) and \"flawed thinking + correct verdict\" (TR/JW)* is helpful for understanding the behavior of the LLM judges.\n* The author does a comprehensive evaluation of the LLM judges with a benchmark over multiple categories."}, "weaknesses": {"value": "1. The paper identifies the fascinating \"good thinking but wrong verdict\" mismatch (Fig 6c) but fails to analyze it in depth. Why does this happen? Is the \"thinking\" trace a post-hoc rationalization that is decoupled from the model's actual decision-making process? Is it a failure in the final step of synthesizing complex reasoning into a single A/B/C verdict? The paper presents the \"what\" (it happens 10.4% of the time in Knowledge) but never explores the \"why.\"\n2. The \"Online Routing II\" (Training-based) strategy is presented as a \"lightweight router,\" but Table 3 notes it uses Llama-3.1-8B and Qwen2.5-7B as backbones. These are not lightweight models. Is the router cost/inference latency considered when calculating the latency and cost of the pipeline? If so, what is the breakdown. The authors fail to present these information in a clear way."}, "questions": {"value": "1. Could you please provide a table that explicitly details the inference latency and token cost of the router model itself (e.g., the Llama-3.1-8B model) for a single routing decision? How does this overhead factor into the \"Average\" Time/Cost results in Table 3?\n2. You identify a mismatch between thinking quality and verdict correctness. What is your hypothesis for why this occurs? Does this finding suggest that LRM \"thinking\" traces are not faithful representations of the model's internal decision process?\n3. What is the main difference and novelty between the benchmark you created vs JudgeBench? How is it different in terms of the dataset composition, data curation method and what makes your benchmark unique or novel?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2fchixbKSI", "forum": "tGM8q8ukp4", "replyto": "tGM8q8ukp4", "signatures": ["ICLR.cc/2026/Conference/Submission10774/Reviewer_LpeS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10774/Reviewer_LpeS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762041330726, "cdate": 1762041330726, "tmdate": 1762921983656, "mdate": 1762921983656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is trying to do two things: 1) benchmark LRMs-as-judge on both the intermediate thinking and output levels and comparing to various non-reasoning LLMs, and 2) present a routing framework to assign judges to get the best accuracy, latency, cost tradeoffs. The main contribution comes from the routing framework, which consists of one offline and two online settings, the latter of which uses LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The need for better LLM judge evaluations is always necessary, and this paper does a nice job collecting a diverse dataset and evaluating a wide set of models on it. The routing frameworks seem novel and address an important solution to the increasing cost of inference with LRMs. The additional analysis on reasoning steps and difficulty-aware results provide more input into the performance that one would hope for when using LRMs."}, "weaknesses": {"value": "The paper suffers from unclear motivation linking its two main contributions (benchmarking and routing), overstated novelty claims given existing work, and omissions of experimental details leaving the reader confused.\n\nThough the focus is on both benchmarking LRMs-as-judges and presenting a routing framework, the benchmark component lacks novelty, as existing work already evaluates on reasoning models like o3-mini and DeepSeek-R1. This paper would benefit from refocusing on the routing contribution. However, in its current state, that discussion (Section 3.3) is too short, so the routing isn’t as well motivated as it should be and is confusing to the reader. E.g., I am not sure what models are being chosen to route to, what in practice the routing looks like, what overhead the routing itself has, etc.\n\nFor the reasoning contribution, given the paper focuses on evaluating LRMs, many reasoning-specific aspects are underexplored. Section 5.5's treatment of intermediate traces is too brief given its stated importance as a contribution. An in-depth treatment should warrant more experiments, e.g., ablation studies on scaling reasoning effort via API parameters or budget forcing or examining how standard LLMs change when prompted to give more reasoning and how this compares to LRMs. Additionally, the paper lacks discussion distinguishing LRMs-as-judge from regular LLMs with chain-of-thought. The appendix shows that non-reasoning models (e.g., GPT-4o) often provide reasoning before answering, meaning all LLMs are effectively \"reasoning\" but some are just explicitly trained in the LRM style. This needs clarification, and further experiments would boost the contribution.\n\nMoreover, there are experimental aspects that need fixing. The main body lacks essential information about the evaluation data. There is no description of what \"Coding, Math, Knowledge, etc.\" categories represent, and no references to the Appendix D details that describe the actual evaluation datasets. Readers cannot understand what the benchmark is truly evaluating without consulting the appendix, which should be referenced and summarized in the main text, but even then, there exists missing data like what models generated the traces being judged. Additionally, QwQ-32B is described as a reasoning model (line 249) but is listed under non-reasoning LLMs in the results.\nMinor fixes:\n•\t091: ‘Tie’ should not be italics since it isn’t in the definition of the label set\n•\tTable 1: I would suggest visually differentiating the reasoning from the non-reasoning\n•\tFigure 4: The reasoning vs. non-reasoning models should be visually highlighted in different ways so it’s easy to view how they change. Additionally, given the Pareto-optimal models are mentioned it would be nice to highlight them in some way as well.\n•\t1021: “Knowledge-llm Task Judge” in the header of the box -> “Knowledge-lrm Task Judge”"}, "questions": {"value": "•\tWhat is 'Avg. Thinking Length' measured in for Table 1? (177 seems too small for tokens; is it for human-labeled rationale?) In general, Table 1 should have a more concrete caption explaining these things.\n•\tWith what model were the traces generated? (Only Claude 3.5 and GPT-4o mentioned at line 392). Datasets are referenced in Appendix D but it may be important to see which models were used to inspect if there was any bias.\n•\tWhat is the difference between 'rationale' and 'thinking' in line 141? (reasoning could benefit from formalization similar to Section 2)\n•\tWas error analysis in Section 5.5 done by LLMs? If so, has a human verified it?\n•\tHow are difficulty buckets for instances determined?\n•\tIs answer choice reversal performed to mitigate positional bias (prompting the judge with both A then B and B then A)?\n•\tWhat is the set of models that the routers choose from, is it all LLMs/LRMs? Has there been any condensing to a smaller set done here?\n•\tWhat models are used within the online routers? (Referenced in Table 3 but should be discussed earlier. Also in Table 3, what about Router 1?)\n•\tHow would new models be added into the routing framework?\n•\tHow might training LRMs as routers instead of LLMs affect performance?\n•\tIs 1024 tokens sufficient for LRMs? Seems limiting and unrealistic for many LRMs.\n•\tWhy is Gemini-2.5-Flash using more tokens than GPT-5, Kimi-K2, and o3-mini in Figure 4? (Expected to be more similar/less). Similarly, how is the token cost so high if maximum output length is 1024?\n•\tAre all models truly evaluated at temperature 0? This doesn’t seem right. DeepSeek R1 recommends 0.6-0.7; o3-mini doesn't have temperature options. If this is true, repeated trials would likely be needed to account for the diversity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C6JV4dzQw9", "forum": "tGM8q8ukp4", "replyto": "tGM8q8ukp4", "signatures": ["ICLR.cc/2026/Conference/Submission10774/Reviewer_47oV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10774/Reviewer_47oV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10774/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763039684219, "cdate": 1763039684219, "tmdate": 1763039684219, "mdate": 1763039684219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}