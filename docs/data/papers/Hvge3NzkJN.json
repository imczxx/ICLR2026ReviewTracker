{"id": "Hvge3NzkJN", "number": 11286, "cdate": 1758195259226, "mdate": 1763709774698, "content": {"title": "Diffusion Models as Dataset Distillation Priors", "abstract": "Dataset distillation aims to synthesize compact yet informative datasets from large ones. A significant challenge in this field is achieving a trifecta of diversity, generalization, and representativeness in a single distilled dataset. Although recent generative dataset distillation methods adopt powerful diffusion models as their foundation models, the inherent representativeness prior in diffusion models is overlooked. Consequently, these approaches often necessitate the integration of external constraints to enhance data quality. To address this, we propose Diffusion As Priors (DAP), which formalizes representativeness by quantifying the similarity between synthetic and real data in feature space using a Mercer kernel. We then introduce this prior as guidance to steer the reverse diffusion process, enhancing the representativeness of distilled samples without any retraining. Extensive experiments on large-scale datasets, such as ImageNet-1K and its subsets, demonstrate that DAP outperforms state-of-the-art methods in generating high-fidelity datasets while achieving superior cross-architecture generalization. Our work not only establishes a theoretical connection between diffusion priors and the objectives of dataset distillation but also provides a practical, training-free framework for improving the quality of the distilled dataset.", "tldr": "", "keywords": ["diffusion mdoels", "dataset distillation", "diffusion priors", "kernel method"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/489d64e0d3b40914816bb3ea14b32d114660a4f2.pdf", "supplementary_material": "/attachment/1835cb7a7bb52c0ea79c7978b355a9123877bb25.zip"}, "replies": [{"content": {"summary": {"value": "This paper interprets diffusion models as possessing three key characteristics: diversity, generalization, and representativeness. Building on this interpretation, the authors propose a training-free framework aimed at enhancing the representativeness of diffusion models. The improvement is achieved by incorporating a distance metric based on the Mercer kernel into the backward process of diffusion models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed DAP method substantially improves the quality of distilled datasets and leads to higher test accuracy.\n2. The paper provides a thorough theoretical analysis that clearly establishes the connections between diffusion priors and the dataset distillation (DD) task.."}, "weaknesses": {"value": "1. Sections 3.1 and 3.2 mainly restate that diffusion models exhibit diversity and generalization, which are well-known properties. This content would be more appropriate as background material in the introduction rather than the method section, as it does not present any novel contributions.\n2. The introduction of the kernel function in Section 3.3.1 lacks clarity. Its purpose and motivation are not well explained, and the connection between representativeness and the kernel function is missing. Although its application becomes evident in Section 3.3.2, the earlier introduction in 3.3.1 is confusing when read sequentially.\n3. In Algorithm 1, the procedure describes how to generate a single sample using representative guidance. However, it is unclear how multiple synthetic images per class (i.e., IPC images) are generated. Additionally, the explanation of how these generated images collectively maintain the three claimed characteristics (diversity, generalization, and representativeness) remains insufficient, given the inherent randomness in individual generations."}, "questions": {"value": "Why didn’t the authors provide an analysis of the computational cost? Since the proposed framework is claimed to be training-free, it should inherently offer computational advantages over training-based methods. However, without quantitative comparisons (e.g., runtime, memory usage, or efficiency metrics)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BcTxgkL4LM", "forum": "Hvge3NzkJN", "replyto": "Hvge3NzkJN", "signatures": ["ICLR.cc/2026/Conference/Submission11286/Reviewer_URQY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11286/Reviewer_URQY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761420155585, "cdate": 1761420155585, "tmdate": 1762922438527, "mdate": 1762922438527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank all reviewers for their constructive feedback, especially for recognizing the strengths and contributions of our work.\nWe have polished the paper, added the experiment results, and made clearer clarifications in the revised version. The updated version now includes:\n\n- We have included clearer explanations of diffusion priors and improved presentation of Section 3.\n\n- We have expanded intuition and motivation for the kernel-based representativeness prior.\n\n- We have discussed how DAP can achieve the three properties collectively.\n\n- We have included clarifications on sampling multiple IPC samples, feature computation, and implementation details.\n\n- We have compared and analyzed DAP with the recent diffusion-based DD methods.\n\n- We have improved our writing, figures, and organization.\n\nAll modifications in the main text and appendix are highlighted in **blue** for ease of reference.\n\nWe greatly appreciate the reviewers’ insights, which helped us significantly improve the clarity and impact of this work. Please let us know if any further clarification is needed."}}, "id": "LXnjTpdhs0", "forum": "Hvge3NzkJN", "replyto": "Hvge3NzkJN", "signatures": ["ICLR.cc/2026/Conference/Submission11286/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11286/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11286/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763703791341, "cdate": 1763703791341, "tmdate": 1763703791341, "mdate": 1763703791341, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel framework that employs diffusion models as powerful generative priors for dataset distillation, the task of compressing a large dataset into a compact synthetic subset while preserving downstream task performance. Unlike traditional approaches that directly optimize synthetic samples in pixel or feature space, this method leverages the diffusion process to model the underlying data manifold, enabling the generation of representative and diverse samples without costly bi-level optimization. Furthermore, the framework enhances representativeness by encouraging the synthetic data to align with real samples in the feature space via a Mercer kernel-based similarity measure. Experimental results on the ImageNet benchmark demonstrate that the proposed approach consistently outperforms state-of-the-art baselines in classification accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a comprehensive literature review and demonstrates promising experimental results, highlighting the effectiveness and practical potential of the proposed approach."}, "weaknesses": {"value": "Although the proposed method is straightforward and effective, several key implementation details are missing, making the reported performance difficult to reproduce. For instance, the paper does not clearly explain how the training samples are selected to pair with synthetic samples when computing the kernel distance."}, "questions": {"value": "1. **Clarification of $x^{\\text{train|c}}_t$ selection.**\nIn Algorithm 1, how are the samples $x^{\\text{train|c}}_t$ obtained or selected before the DAP sampling process?\n\n\n2. **Diversity Comparison with Baselines.**\nThe proposed method (Diffusion as Prior, DAP) demonstrates substantially greater diversity compared to baseline methods such as MGD³ and IGD, as illustrated in Figure 1. Interestingly, while MGD³ explicitly incorporates mechanisms to enhance diversity, the proposed method does not directly encourage the generation of diverse samples. Could the authors elaborate on the underlying reasons for this observed improvement in diversity?\n\n3. **Quantitative and Qualitative Comparison.**\nCould the authors provide a more detailed comparison between DAP and the baseline methods in terms of diversity, representativeness, and classification performance under different IPC (images per class) settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Diy97a1OvV", "forum": "Hvge3NzkJN", "replyto": "Hvge3NzkJN", "signatures": ["ICLR.cc/2026/Conference/Submission11286/Reviewer_1VVS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11286/Reviewer_1VVS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730511505, "cdate": 1761730511505, "tmdate": 1762922438177, "mdate": 1762922438177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Diffusion As Priors (DAP), a novel framework for dataset distillation that leverages inherent priors in pre-trained diffusion models. The authors establish a theoretical connection between diffusion model objectives and dataset distillation requirements, identifying three key priors: diversity, generalization, and representativeness. The main contribution is formalizing representativeness using Mercer kernel-induced distances and incorporating this as guidance during the reverse diffusion process without requiring model retraining. Extensive experiments on ImageNet-1K and its subsets demonstrate that DAP achieves state-of-the-art performance while maintaining cross-architecture generalization.\n\nThe key novelty is decomposing the conditional score function as: $\\nabla_x \\log p(x|R) = \\nabla_x \\log p(x) + \\nabla_x \\log p(R|x)$, where the first term captures diversity/generalization priors from the pre-trained diffusion model, and the second term introduces representativeness through energy-based guidance using kernel-induced distance measurements."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides a principled framework connecting diffusion model objectives to dataset distillation requirements through clear mathematical formulations and proofs.\n\n- Extensive experiments across multiple datasets (ImageNet-1K, ImageNette, ImageWoof, ImageIDC), architectures (ConvNet, ResNet, MobileNet, EfficientNet, Swin), and protocols (hard-label, soft-label) demonstrate broad applicability.\n\n - Unlike methods requiring fine-tuning or external training, DAP leverages pre-trained diffusion models directly, making it practical and computationally efficient (no additional training cost)."}, "weaknesses": {"value": "- While the Mercer kernel framework is mathematically sound, the paper does not convincingly argue why minimizing kernel-induced distance in feature space is the optimal objective for representativeness in DD. Alternative formulations (e.g., maximum mean discrepancy, optimal transport) could be equally valid.\n\n- Table 10 reveals significant computational costs during sampling, with speed increasing from 15-36 seconds per iteration depending on data size. This overhead could be prohibitive for large-scale applications. The paper acknowledges this but doesn't propose solutions.\n\n- The method requires access to the full training dataset during sampling for representativeness guidance, which somewhat limits the practical benefits of distillation\n\n- The paper primarily uses linear kernels with brief exploration of RBF (Table 8). Other kernel choices  and their theoretical implications are not discussed."}, "questions": {"value": "- Could you provide more justification for why kernel-induced distance is the right metric for representativeness? Have you considered alternative metrics like Maximum Mean Discrepancy (MMD) or Optimal Transport distances? How would these compare theoretically and empirically?\n\n- Are there scenarios or dataset characteristics where DAP underperforms? For instance, does it struggle with fine-grained classification, imbalanced data, or out-of-distribution classes?\n\n- The linear kernel is chosen \"due to its tractability\" but Table 8 shows RBF performs comparably. Could you discuss the theoretical implications of different kernel choices more deeply? Does kernel selection depend on dataset characteristics?\n\n- When the method does not have access to full training samples during sampling but instead a handful of subset of them, how will the proposed method perform?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YHdMVc4Lpy", "forum": "Hvge3NzkJN", "replyto": "Hvge3NzkJN", "signatures": ["ICLR.cc/2026/Conference/Submission11286/Reviewer_wTWR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11286/Reviewer_wTWR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761732361653, "cdate": 1761732361653, "tmdate": 1762922437659, "mdate": 1762922437659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Inspired by the diffusion classifiers, it posits that the feature extraction capability inherent in a well-trained diffusion model itself constitutes a representativeness prior highly relevant to DD. It hypothesizes that high representativeness corresponds to high similarity between synthetic and original data in the representation space. To formalize this, it employs the Mercer kernel, a specific type of kernel function, to quantify the similarity within feature spaces. The Mercer kernel provides mathematical guarantees of convexity and tractability in optimization, ensuring that the representativeness prior is computationally feasible. Empirically, it defines the representativeness score function as an energy function based on Mercer kernel, which allows to inject the unused representativeness prior into the distilled data through guided sampling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "It proposes Diffusion As Priors (DAP) and applies it to datasets of varying scales, including large-scale ImageNet-1K and its small subsets.\nBoth quantitative and qualitative results show that DAP significantly enhances the quality of distilled datasets. It validates the theoretical connections between diffusion priors and DD task, while achieving competitive performance compared to other methods.\n\nIt prove the priors in the well-trained DMs meet the diversity and generalization requirements of DD. It derives the overlooked representativeness prior from DMs and formalize it into a kernel-induced distance, which guides the sampling dynamic and improves the quality of distilled datasets. It further shows that by introducing the desired priors, the distilled datasets have the same generalization and transferability as the original ones.\n\nTo investigate whether DAP enforces diversity and representativeness priors in the distilled datasets, it visualizes the data distribution using t-SNE alongside both the training and test sets. It reveals that the synthetic data aligns well with the training set while generalizing to the test set, demonstrating that the DAP can accurately match the underlying data manifold. Moreover, the embeddings show intra-class diversity and inter-class separability, indicating that the distilled datasets capture meaningful variability without sacrificing discriminability.\n\nIt conducts ablation studies to investigate the influence of feature layer selection in representativeness guidance. The cases consistently reveal that the final output layers are suboptimal for representativeness guidance, as they prioritize distribution alignment over representativeness."}, "weaknesses": {"value": "Diffusion models are widely adopted in dataset distillation to extract features and obtain information. It is similar to use diffusion as priors in this paper. It is better to discuss related works and highlight the differences.\n\nIt is better to compare with more baslines such as [R1,R2,R3], which also adopts diffusion for dataset distillation. \n\n\n[R1] CaO2: Rectifying Inconsistencies in Diffusion-Based Dataset Distillation\n\n[R2] Taming Diffusion for Dataset Distillation with High Representativeness\n\n[R3] Dataset Distillation via Vision-Language Category Prototype"}, "questions": {"value": "see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v5MzWU0EAk", "forum": "Hvge3NzkJN", "replyto": "Hvge3NzkJN", "signatures": ["ICLR.cc/2026/Conference/Submission11286/Reviewer_2soE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11286/Reviewer_2soE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761853458412, "cdate": 1761853458412, "tmdate": 1762922436913, "mdate": 1762922436913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}