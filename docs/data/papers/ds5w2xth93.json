{"id": "ds5w2xth93", "number": 418, "cdate": 1756738777736, "mdate": 1763654291912, "content": {"title": "Representation Alignment for Diffusion Transformers without External Components", "abstract": "Recent studies have demonstrated that learning a meaningful internal represen-\ntation can accelerate generative training. However, existing approaches necessi-\ntate to either introduce an off-the-shelf external representation task or rely on a\nlarge-scale, pre-trained external representation encoder to provide representation\nguidance during the training process. In this study, we posit that the unique dis-\ncriminative process inherent to diffusion transformers enables them to offer such\nguidance without requiring external representation components. We propose Self-\nRepresentation Alignment (SRA), a simple yet effective method that obtains rep-\nresentation guidance using the internal representations of learned diffusion trans-\nformer. SRA aligns the latent representation of the diffusion transformer in the\nearlier layer conditioned on higher noise to that in the later layer conditioned on\nlower noise to progressively enhance the overall representation learning during\nonly the training process. Experimental results indicate that applying SRA to\nDiTs and SiTs yields consistent performance improvements, and largely outper-\nforms approaches relying on auxiliary representation task. Our approach achieves\nperformance comparable to methods that are dependent on an external pre-trained\nrepresentation encoder, which demonstrates the feasibility of acceleration with\nrepresentation alignment in diffusion transformers themselves.", "tldr": "Self-Representation Alignment for Diffusion Transformers", "keywords": ["Diffusion Transformers", "Self-Representation Alignment"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/be5401370f35f9d63a4b748e06ddfe0320cb12eb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Self-Representation Alignment (SRA), which improves representation learning quality in diffusion/flow transformers (DiTs/SiTs) without relying on pre-trained encoders or auxiliary representation learning tasks. SRA achieves accelerated training speed and improved generation performance in practice. The key insight is that diffusion/flow transformers learn a hierarchy of representations which progress from coarse features (in earlier layers and with higher noise levels) to fine features (in later layers and with lower noise levels). SRA leverages this internal structure in a student-teacher framework, where the teacher is an EMA version of the student network. The diffusion/flow training objective is then augmented with an alignment loss that encourages the projection of the student's coarse representations to match the teacher's more refined representations. Large-scale experiments are performed to evaluate SRA's empirical performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The observation of the hierarchy of the internal representations in diffusion/flow transformers is interesting, which is a natural motivation for the proposed method.\n2. The key idea of using the EMA model's representation in later layer and with lower noise level for self-guidance is novel. This has the advantage of eliminating the need of pretrained encoder or auxiliary representation learning tasks for representation alignment.\n3. Thorough empirical evaluation and ablation studies are performed to demonstrate the proposed method's competitive empirical performance.\n4. The paper is easy to understand with a clear description of the preliminaries and the proposed method, as well as a well-organized section for experiment result."}, "weaknesses": {"value": "1. The primary weakness is a lack of a principled foundation, as many core design choices feel arbitrary and are determined through empirical trial-and-error rather than guided by a clear theoretical or analytical framework. Below are some examples:\n\n    (a) The projection head is a bit counterintuitive but is shown to be crucial during training with no clear justification. Intuitively, it feels more natural to perform alignment without projection or with projection for both teacher and student outputs.\n\n    (b) It is unclear why a fixed EMA decay of a particular value works well and whether this is still the optimal value for new tasks/architectures.\n\n    (c) The choices of the layers and time intervals for self-guidance feel ad-hoc.\n\n    (d) It is suspicious that there is only a little performance variance for different values of the alignment loss's weight $\\lambda$, given that it controls the trade-off between the diffusion loss and alignment loss.\n\n2. The proposed method requires tuning many hyperparameters and its performance seems highly sensitive to these specific hyperparameter choices, for which the paper offers no clear scientific explanation or general rule. As a result, it is unclear whether these hyperparameter will generalize to new architectures/tasks/settings.\n\n3. Although the authors propose several hypotheses to explain why the chosen hyperparameters make sense, it still does not provide sufficient scientific intuition for why the proposed alignment is useful as the hypotheses are unverified.\n\n4. Another major weakness is that there is no discussion of the limitations of the proposed method in the paper.\n\n5. There are some minor typos in the paper:\n\n    (a) \"Dose\" -> \"Does\" in Line 227,\n\n    (b) \"Tabel 2\" and \"Tabel 3\" -> \"Table 2\" and \"Table 3\" in Line 373,\n\n    (c) \"Defualt\" -> \"Default\" in Line 918,\n\n    (d) \"useage\" -> \"usage\" in Line 1068."}, "questions": {"value": "1. Could the authors comment on the generalizability of the chosen hyperparameters to other settings/tasks and provide some scientific explanations and/or general rules for selecting these hyperparameters?\n2. Could the authors discuss the limitations of the proposed method in the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VL8YCRFU7a", "forum": "ds5w2xth93", "replyto": "ds5w2xth93", "signatures": ["ICLR.cc/2026/Conference/Submission418/Reviewer_MZU4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission418/Reviewer_MZU4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761037092934, "cdate": 1761037092934, "tmdate": 1762915516074, "mdate": 1762915516074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to accelerate the training of diffusion transformers by learning meaningful internal representations while training the model. In particular, this paper proposes SRA, which align the diffusion model with a better representation without external guidance. The key idea is to introduce a teacher network (EMA), and then use different noise and layers of the teacher network for alignment to self-distill a better representation. Experiments demonstrate that SRA accelerates generative training without an external model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is overall easy to follow, and the method is simple yet effective.\n\n2. Using self-distillation from the teacher network to boost the training of diffusion models is an intriguing idea.\n\n2. SRA shows faster training with respect to time compared to vanilla Diffusion Transformers, showing the effectiveness of the method."}, "weaknesses": {"value": "1. The authors claim that later blocks with small noise have fine-grained features compared to the first blocks with large noise. However, it does not indicate that later blocks with small noise have better representations to be aligned. For instance, an extremely larger layer even has more detailed features, but it is not beneficial (e.g., 4 -> 12 performs worse than naive SiT in Table 1). Therefore, I think it needs additional / more analysis to justify the effectiveness of the proposed method.\n\n2. The choice of block layers is quite sensitive. In particular, it sometimes performs worse than vanilla SiT. It (1) makes the practitioners to difficult to use this technique to improve the training speed of Diffusion Transformers, and (2) makes it difficult to extend the suggested framework to different domains."}, "questions": {"value": "Please answer the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LpMmejQVgW", "forum": "ds5w2xth93", "replyto": "ds5w2xth93", "signatures": ["ICLR.cc/2026/Conference/Submission418/Reviewer_1H2k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission418/Reviewer_1H2k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551295859, "cdate": 1761551295859, "tmdate": 1762915515882, "mdate": 1762915515882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Self-Representation Alignment (SRA), leveraging internal representation guidance during diffusion transformer training. The core idea is to align latent representations from earlier layers (conditioned on higher noise) with those from later layers (conditioned on lower noise), without relying on any external models. To achieve this, the authors employ an EMA teacher of the same network and apply a lightweight projection head with a patch-wise distance loss. Experimental results show that SRA attains performance comparable to methods using external alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation and flow of ideas in this paper are clear. It starts by analyzing the internal representations of diffusion transformers and then proposes a way to effectively use them to speed up training and improve performance. The paper also shows self-alignment can serve as a good alternative in the absence of external encoders.\n\n- The paper is well-written and easy to follow. The figures are clear and helpful, and the authors include an analysis of training cost, which is one of the considerations with having another network forward pass. \n\n- Experiments across different resolutions (e.g., 512×512) and models (DiTs and SiTs) show that the method scales well and generalizes effectively."}, "weaknesses": {"value": "- The EMA teacher is one of the key components, but the paper gives little discussion about it. From Table 5, the results are quite sensitive to how $\\alpha$ is updated - some settings (e.g., 0.0 and 0.996 → 1.0) even show worse FID than the baseline. A deeper analysis beyond saying it is “commonly used” in other works would make this part more convincing.\n\n- The paper does not include experiments or discussions demonstrating SRA’s effectiveness in cases where strong external encoders are unavailable - even though this is the main motivation. It would be helpful to include or discuss results in another setting (e.g., video generation) to make the motivation more convincing.\n\n- The authors claim that “the teacher’s constantly improving capacity allows better representation guidance as training proceeds,” but there is no evidence supporting this. Showing how the teacher’s representation quality (e.g., via linear probing) improves over time would support the claim. Otherwise, the statement should be toned down."}, "questions": {"value": "- Figure 6 shows that SRA keeps improving while external alignment methods saturate early. It gives a good signal, but it’s still unclear whether SRA eventually converges or continues improving over longer training. Could the authors provide longer training curves or convergence analysis?\n\n- I’m also curious whether SRA can naturally extend to text-to-image generation. Would the same self-alignment strategy work there, or are there additional challenges?\n\n- (Minor) Typo in line 373 - “Tabel” → “Table”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y2v5D8MB2F", "forum": "ds5w2xth93", "replyto": "ds5w2xth93", "signatures": ["ICLR.cc/2026/Conference/Submission418/Reviewer_NzJf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission418/Reviewer_NzJf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834637393, "cdate": 1761834637393, "tmdate": 1762915515730, "mdate": 1762915515730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SRA, a model designed to enhance the generation performance of diffusion transformer models. The key idea of SRA originates from an observation that earlier timesteps and shallow layers of a diffusion transformer capture coarse latent features, while later timesteps and deeper layers perform progressive refinement. Instead of relying on a pre-trained representation encoder (a recent approach for improving diffusion transformer performance) the authors employ an EMA-updated teacher network derived from the training process itself. Through this self-updating training mechanism, the model refines its representations over time. The proposed method is evaluated across various models and achieves performance comparable to those that use large pre-trained encoders."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper presents an intuitive and elegant idea: achieving refinement within the model itself during training, without depending on large-scale pre-trained encoders. This demonstrates the potential for applying the method to settings where such encoders are unavailable.\n\n- The proposed method can be easily integrated into existing diffusion training pipelines, and the authors confirm its applicability to both flow-based and diffusion-based models.\n\n- The linear probing results of the learned latent representations provide yet convincing evidence that the latent quality has been improved."}, "weaknesses": {"value": "- The method requires model-specific hyperparameter tuning. For each new model, appropriate block layers need to be identified, and while the time interval and lambda parameters are less sensitive, they still require some search.\n\n- Since the training process involves an EMA teacher model, the approach is likely to increase GPU memory consumption. It would be helpful if the authors could quantify the additional GPU usage and training time per epoch compared to the baseline."}, "questions": {"value": "- Given sufficient GPU capacity, it seems feasible to combine this method with REPA for joint training. Have the authors explored this possibility?\n\n- There seems to be a noticeable performance difference depending on whether the projection head is used or not. Could the authors elaborate on why the projection head contributes so significantly to performance, and what specific role it plays in the model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SfUdRo6Czn", "forum": "ds5w2xth93", "replyto": "ds5w2xth93", "signatures": ["ICLR.cc/2026/Conference/Submission418/Reviewer_YcnN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission418/Reviewer_YcnN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916777501, "cdate": 1761916777501, "tmdate": 1762915515599, "mdate": 1762915515599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "Dear reviewers and AC,\n\nWe sincerely appreciate your valuable time and effort spent reviewing our manuscript.\n\nWe propose SRA,  improving diffusion transformers training by aligning their own representations in different layers with different noise to achieve representation alignment without external component.\n\n\nAs reviewers highlighted, our method:\n- a novel and elegant idea (Reviewer YcnN, NzJf, 1H2k,  MZU4)\n- simple, effective and easy to follow.  (Reviewer YcnN, NzJf, 1H2k,  MZU4)\n-  comprehensive experiments and analysis (Reviewer YcnN, NzJf,  MZU4)\n\n\nWe appreciate your constructive feedback on our manuscript. In response to your comments, we have meticulously revised and enhanced the manuscript as follows:\n- Text-to-image experiment (Section 3.3 and Table 2)\n- More hyperparameter setting analysis (Appendix F and Appendix G)\n- Additional Linear probing experiment ( Section 3.4 and Figure 7 (d))\n- Limitation and Future Work (Section 5)\n- Correct expression (\"coarse to fine\" -> \"bad to good\")\n- Fixing typos (\"Dose\" -> \"Does\", \"Tabel 2\" and \"Tabel 3\" -> \"Table 2\" and \"Table 3\",  \"Defualt\" -> \"Default\", \"useage\" -> \"usage\")\n\n In the revised version, these updates are temporarily highlighted in blue for your convenience.\n\nWe sincerely believe that these revisions will better enable us to convey the benefits of the proposed SRA to the ICLR community.\n\nThank you very much,\n\nAuthors."}}, "id": "XVHy0YuqLC", "forum": "ds5w2xth93", "replyto": "ds5w2xth93", "signatures": ["ICLR.cc/2026/Conference/Submission418/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission418/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission418/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763654637208, "cdate": 1763654637208, "tmdate": 1763654637208, "mdate": 1763654637208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}