{"id": "yP15n9p1Lr", "number": 14669, "cdate": 1758241286465, "mdate": 1759897356091, "content": {"title": "Patching LLM like Software: A Lightweight Method for improving existing policy in Large Language Models", "abstract": "We propose $\\textit{patching}$ for large language models (LLM) like software versions, a lightweight and modular approach for addressing safety vulnerability. While vendors release improved LLM versions, but major releases are costly, infrequent and difficult to tailor to customer needs, leaving released models with known safety gaps. Unlike full-model fine-tuning or major version updates, our method enables rapid remediation by prepending a compact, learnable prefix to an existing model. This “patch” introduces only $0.003\\%$ additional parameters, yet reliably steers model behavior toward that of a safer reference model. Across three critical domains—toxicity mitigation, bias reduction, and harmfulness refusal—policy patches achieve safety improvements comparable to next-generation safety aligned models while preserving fluency. Our results demonstrate that LLMs can be “patched” much like software, offering vendors and practitioners a practical mechanism for distributing scalable, efficient, and composable safety updates between major model releases.", "tldr": "", "keywords": ["Large language model safety", "Parameter-efficient fine-tuning", "Toxicity mitigation", "Bias reduction", "Direct Preference Optimization (DPO)", "Soft prompts"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f6e4b00959e5f5bb52d10e7fcdd3f973b26482b6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a lightweight safety alignment method through prompt tuning. The authors assumed the scenario that a vendor cannot instantly update their models due to the high computational resource of full fine-tuning and parameter efficient tuning for every failure cases reported from customer. To resolve safety vulnerabilities (toxicity, bias, harmfulness) discovered major model releases, the authors prepend a learnable prefix with only 0.003% additional parameters to the model input. Specifically, they train a policy patch that learns the behavior of an improved model, which customers can then apply to the original model M to create M+. Training proceeds in two stages: SFT mimics M's token distribution, followed by DPO learning preferences between safe and unsafe responses. The method is validated across diverse model families including Llama, Aya, Mistral, and Gemma, demonstrating safety improvements."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's Vendor-Customer scenario is practical and convincing, effectively reflecting real-world industry environments. Full-model fine-tuning (RLHF, DPO) is computationally expensive and inaccessible. Major model releases occur only once or twice a year, making it impossible to immediately address known safety issues. Learning soft prompt can be good approach for handling this scenario.\n\n2. The authors addressed various major safety risk experiments, including toxicity, bias, harmfulness. The proposed method is validated with consistency across diverse model families (Llama-2/3, Aya-23, Mistral, Gemma2, Vicuna). Also, the several ablation studies show the impact of \\beta value, patch length, and initialization methods.\n\n3. Using multiple learned prefix together shows the significance of this approach. By solely combining multiple prompts from toxicity and bias, the proposed method can address both safety risk scenarios even though these prompts are individually trained. \n\n4. As shown in Table 2, the Policy Patch (0.2M parameters, +2.5% inference overhead) achieves ~70% toxicity reduction with 195× fewer parameters compared to LoRA (40M parameters, +24% inference overhead). This presents a practical trade-off from a deployment perspective."}, "weaknesses": {"value": "1. The paper's most significant limitation is its dependency on an improved model M'. The authors assumed that M' (already safety-improved model) must pre-exist. Then, the learnable prefixes are trained on data generated by M'. Thus, the paper fails to actually solve the \"gap between major version updates\" that it claims to address. Current work only addresses the unknown issues  that a company cannot release a updated model. This is not a work addressing the parameter efficiency as it first obtains M' and then train prefix. While Section 5 Limitations mentions \"reliance on an improved reference model,\" it does not sufficiently emphasize that this undermines the method's foundation.\n\n2. The current method consists of well-established techniques from existing works. Method = Prompt Tuning + SFT  + DPO. The core contribution is applying these to safety problems and reinterpreting as a \"patch\" framework. However, there are already many works that addresses the safety issues based on prompt tuning [1]\n\n[1] DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer. ICLR 2024.\n\n3. While I like the experiment in Table1, the current experiment include limited composition with Only Toxicity + Bias combination tested. To further support the authors' claim, conflict potential between patches should be examined. e.g., does the toxicity patch already address or worsen bias? Hence, I suggest the authors to add the experiments with harmfulness included and all three risks combined.\n\nMinor typo in Line 242"}, "questions": {"value": "1. I would like to know the details of LoRA adapter in Table 2. What data did you use for SFT, and do you perform DPO together?\n\n2. In Figure 2, M+ increase PPL compared to M' with 4 value difference. However. in line 302, it is described as it maintains PPL close to the aligned model M'? How can I interpret the result as maintained?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "While this paper addresses ethically sensitive topics (toxicity, bias, harmfulness in LLMs), it does not require an ethics review for the following reasons. First, the paper contributes to make LLMs safer, not create  safety issues. This work focuses on reducing toxicity, bias, and harmful content generation. Moreover, the paper includes \"ETHICS STATEMENT\" section stating that it uses publicly available datasets without personally identifiable or sensitive private data, human subjects involved."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FEQpwbog8F", "forum": "yP15n9p1Lr", "replyto": "yP15n9p1Lr", "signatures": ["ICLR.cc/2026/Conference/Submission14669/Reviewer_2NEg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14669/Reviewer_2NEg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761605897827, "cdate": 1761605897827, "tmdate": 1762925040576, "mdate": 1762925040576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a way to add safety updates to large language models without retraining them. The authors create small “policy patches”—very small learned prefixes that are added to the model’s input. These patches are tiny (only 0.003% of the model’s size) but can strongly improve safety behavior. Experimental results demonstrate the effectiveness of their method."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The practicality and effectiveness of their method: the low cost of implementation of their method enables model developers to fix the safety risks of their models quickly. Considering that most of the model developers have no large computational resources, the method can benefit a broader audience to some extent."}, "weaknesses": {"value": "- The novelty of their method is limited. Their fine-tuning way, like supervised fine-tuning and direct preference optimization, is common. \n- My biggest concern is that, as the saying goes, \"You can't mend a leaking boat with a patch.\" The model only works with the target tokens provided in its training data. This brings up two issues: \n\nFirst, advanced attack methods can easily bypass the security measures, but the authors haven’t fully evaluated this. This could give developers a false sense of security, leading to bigger risks. \n\nSecond, will adding patches iteratively cause previous security patches to be forgotten? There is no related study in this paper."}, "questions": {"value": "See the limitations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "F9QRIkkbAo", "forum": "yP15n9p1Lr", "replyto": "yP15n9p1Lr", "signatures": ["ICLR.cc/2026/Conference/Submission14669/Reviewer_uPub"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14669/Reviewer_uPub"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902283866, "cdate": 1761902283866, "tmdate": 1762925039684, "mdate": 1762925039684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a lightweight “safety patching” approach for large language models, drawing an analogy to software versioning.\nInstead of retraining or redeploying entire models, a small learnable prefix (similar to soft prompt tuning) is prepended to steer a model toward the behavior of a safer reference model M′.\nThe authors train this prefix via a two-stage process, namely Supervised Fine-Tuning followed by Direct Preference Optimization. Evaluations are conducted on multiple LLMs and across three safety dimensions: toxicity, gender bias, and harmfulness refusal.\nThe reported results show that these “policy patches” achieve safety improvements comparable to fully aligned versions, with lower computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed idea is well-motivated and experimentally validated.\n- In general is the paper is well written, introduced methodology is easy to understand\n- Experiments include ablations on prefix length, initialization, and parameter β trade-off, illustrating the safety–fluency Pareto."}, "weaknesses": {"value": "Several aspects weaken methodological rigor:\n- The relationship to prior work on prompt learning, adapter tuning, and steering vectors is not clearly distinguished, leaving it unclear what is fundamentally novel.\n- Notation and terminology are inconsistent (e.g., P used for both parameters and the patch, variable formatting inconsistencies (for instance in line 199 y_w bold, line 222 not bold) ).\n- Experiments appear to lack cross-validation or variance reporting across random seeds, and the appendix omits full training details for baselines (only described for the introduced method).\n\nFurther the presentation suffers from minor editorial and structural issues:\n- For instance, typos (“the the”), overuse of bold text, inconsistent mathematical notation, and incomplete appendix sections (e.g. A.1.1–A.1.3 only show figures without description and discussion).\n- Furthermore, figure and table captions could provide a more concise summary of what is being shown, and the definitions of several reported metrics (e.g., ASR, Toxicity) could be more formally introduced.\n- Clarity on what “next-generation models” (line 54) refers to would also help contextualize comparisons.\n\n\nLastly, technically, the method closely resembles existing prompt- or prefix-tuning approaches, and its distinction from steering-vector or neuron-patching methods (e.g., safety-neuron activation patching, see Chen et al. Towards Understanding Safety Alignment: A Mechanistic Perspective from Safety Neurons. 2024) should be discussed."}, "questions": {"value": "- How does the proposed patching fundamentally differ from activation-editing approaches that modify a small subset of activations or inject pre-computed steering vectors to guide model behavior toward safer outputs?\n- Were experiments repeated with different random seeds or DPO initializations to confirm stability?\n- Could the authors clarify what “next-generation models” refer to?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VAeNuzwKZQ", "forum": "yP15n9p1Lr", "replyto": "yP15n9p1Lr", "signatures": ["ICLR.cc/2026/Conference/Submission14669/Reviewer_GHxp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14669/Reviewer_GHxp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936305543, "cdate": 1761936305543, "tmdate": 1762925038984, "mdate": 1762925038984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies addressing uncovered LLM safety risks with an efficient and minimal approach: that is a sequence of tuned prefix tokens that are prepended to the input (patch). The training of such patches uses both SFT and DPO whose reference/preferred responses are obtained from a fully tuned variant of the model to address the same target risks. Evaluation on toxicity, bias and harmfulness risks shows that the learned patches retain model fluency which matching the safety of the fully tuned model. A comparison to LoRA-based fine-tuning shows that the learned patches are more efficient in terms of the number of additional parameters as well as training and inference time."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation for fast and small safety patches is interesting and of a practical need.\n\n2. The presented approach is shown to be effective across multiple models, and all evaluated 3 risks."}, "weaknesses": {"value": "1. The reliance on a fixed variant of the same model for training data generation is such a strong requirement, and it weakens the practicality of the approach. There could be more practical ways for generating the training data such as relying on advanced prompting techniques or even using a single safer teacher model while showing that it can be used to learn patches for various other unsafe models.\n\n2. In all presented experiments, the paper learns a separate patch for each risk which is also not that practical. In the motivating example provided in the paper, the patch is expected to address reported safety concerns about a particular version of a model, which can cover various risks. The paper needs to show that a patch can be learned to fix a mixture of risks together. In section 4.3, the paper discusses composing multiple patches but still: 1) a separate path needs to be learned for each risk, and 2) the evaluation only considers two risks, i.e., no study of the extent to which the composition approach scales.\n\n3. The paper does not provide any results that the patching approach retains the general capabilities of the model. Beyond the risks studied, the paper only considers the fluency of the patched versions. The paper needs to evaluate other skills such as instruction following, reasoning, conversational abilities, etc.\n\n4. In 4.4.1, the paper compares to LoRA whose rank is set to a fixed value (40M additional parameters). It is not clear how that value was chosen, and more importantly it is not clear how would reducing that value impacts the comparison. It could be the case that a smaller rank would still address the safety risks while being more efficient than the patching approach.\n\n5. For the training data, the paper does not provide enough details on the prompts used whose selection and sourcing is expected to be of high importance. \n\n6. The paper also lacks qualitative analysis that compares the responses before and after the patching."}, "questions": {"value": "Please see weaknesses above especially the missing details about the training prompts and LoRA rank."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DSEfFVyEJl", "forum": "yP15n9p1Lr", "replyto": "yP15n9p1Lr", "signatures": ["ICLR.cc/2026/Conference/Submission14669/Reviewer_W7YT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14669/Reviewer_W7YT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940914404, "cdate": 1761940914404, "tmdate": 1762925038459, "mdate": 1762925038459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}