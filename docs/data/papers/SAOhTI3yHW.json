{"id": "SAOhTI3yHW", "number": 11761, "cdate": 1758203576704, "mdate": 1759897556695, "content": {"title": "RIFT: Group-Relative RL Fine-Tuning for Realistic and Controllable Traffic Simulation", "abstract": "Achieving both realism and controllability in closed-loop traffic simulation remains a key challenge in autonomous driving. Dataset-based methods reproduce realistic trajectories but suffer from \\covariate shift in closed-loop deployment, compounded by simplified dynamics models that further reduce reliability. Conversely, physics-based simulation methods enhance reliable and controllable closed-loop interactions but often lack expert demonstrations, compromising realism. To address these challenges, we introduce a dual-stage AV-centric simulation framework that conducts imitation learning pre-training in a data-driven simulator to capture trajectory-level realism and route-level controllability, followed by reinforcement learning fine-tuning in a physics-based simulator to enhance style-level controllability and mitigate covariate shift. In the fine-tuning stage, we propose RIFT, a novel group-relative RL fine-tuning strategy that evaluates all candidate modalities through group-relative formulation and employs a surrogate objective for stable optimization, enhancing style-level controllability and mitigating covariate shift while preserving the trajectory-level realism and route-level controllability inherited from IL pre-training. Extensive experiments demonstrate that RIFT improves realism and controllability in traffic simulation while simultaneously exposing the limitations of modern AV systems in closed-loop evaluation.", "tldr": "We achieve realistic and controllable traffic simulation by combining IL pre-training in a data-driven simulator for realism with RL fine-tuning in a physics-based simulator for controllability.", "keywords": ["autonomous driving", "traffic simulation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b76a38d5ef5a928f9393638d454e1dfc0f23f79c.pdf", "supplementary_material": "/attachment/b24cdd0ea1b50aef0d18e3c19631133ac0a81cab.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a new method to indentify the key background vehicle and improving the vehicle's trajectory realism by group-relative RL finetuning. The method preverses the trajectory-level realism and multi-modality. Experiments demonstrates RIFT can improve realism and controllability in traffic simulation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The appendix is detailed and the supplementary videos is comprehensive and vivid.\n2. The experiment includes extensive prior works with reasonable realism metrics.\n3. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. The method to indentify the CBV is problematic because it cannot indentify the vehicle in the intersection which has different routes with the ego vehicle but has large collision risk. It is demonstrated in the videos.\n2. The method to evaluate the realism in the simple CARLA environment where all BV behaves convervatively. Consider evaluate and compare in the Waymo Sim Agent benchmark where groud-truth is provided for better evaulation of realism."}, "questions": {"value": "1. Why does the other method except PDM-lite in table 2 not report the BR metrics?\n2. How do you refine the all head? What is the loss for finetuning trajectory generation head?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "40i9SD2MZ3", "forum": "SAOhTI3yHW", "replyto": "SAOhTI3yHW", "signatures": ["ICLR.cc/2026/Conference/Submission11761/Reviewer_Bo1Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11761/Reviewer_Bo1Q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760627228288, "cdate": 1760627228288, "tmdate": 1762922787496, "mdate": 1762922787496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **RIFT**, a closed-loop reinforcement learning fine-tuning framework for driving models. The method pre-trains on NuPlan and fine-tunes using **GRPO** (a PPO variant without KL-regularization, with dual clipping) in the CARLA simulator. The goal is to improve controllability and realism in closed-loop driving.\n\nAlthough the authors provide many experiments, I found the terminology and takeaways unclear (see weaknesses below), which makes it hard to judge the effectiveness of RL-finetuning in this work."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The qualitative results are comprehensive and good for understanding the performance\n- I appreciate authors provide several baselines, but the key baselines, such as CAT-K or Waymo Sim Agent Challenge would be more insightful."}, "weaknesses": {"value": "- This works proposes a different metrics compared to Waymo Sim Agent Challenge, I suggest the authors either take 1-2 baselines from Sim Agent Challenge and evaluate their settings or, simply adapt RIFT for Waymo Sim Agent Challenge. Currently is hard for reviewers to understand what are the strength and limitations of RIFT just by looking at  table numbers.\n- For AV Evaluation, it is hard to draw insights from Table 2 since there are two factors (Sim Agents and different planners). For example, RIFT might be too reactive that gives wrong esimate or the best performance for AV planners."}, "questions": {"value": "- What is the main advantage of using CARLA/Meta Drive as a fine-tuning simulator? What new aspects does it provide? More accurate dynamics? These simulators also provide an approximation of dynamics, and traffic simulation usually focuses on high-level behavior instead of low-level control.\n\n- There are many works that focus on GRPO fine-tuning of traffic models, e.g., [1][2]. However, the main advantage of the RIFT in this work remains unclear. For example, the authors can consider: 1) What new aspect does it unlock for evaluating planning algorithms?\n\n[1] Wang M., Wang J., Ye T., Chen J., Yu K. (2025). *Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving.* CoRL.\n[2] Ahmadi E., Schofield H. (2025). *RLFTSim: Multi-Agent Traffic Simulation via Reinforcement Learning Fine-Tuning.* Technical Report for the Waymo Open Sim Agents Challenge."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sE5AVC2ilb", "forum": "SAOhTI3yHW", "replyto": "SAOhTI3yHW", "signatures": ["ICLR.cc/2026/Conference/Submission11761/Reviewer_WoWw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11761/Reviewer_WoWw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752861314, "cdate": 1761752861314, "tmdate": 1762922787164, "mdate": 1762922787164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents RIFT (Group-Relative Reinforcement Learning), a method for encouraging fair and cooperative driving behaviors among multiple autonomous agents. Instead of optimizing individual rewards, each agent’s performance is compared to the group average, which helps promote teamwork rather than selfish behavior. A fairness regularization term further balances outcomes across agents. The method is implemented in a decentralized actor-critic framework and tested in Nocturne and Waymo Sim, where it leads to smoother, more socially efficient traffic than standard RL approaches."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The idea of using group-relative advantages to encourage cooperation is simple but effective.\n\nResults show clear improvements in fairness, stability, and overall efficiency.\n\nThe framework is decentralized and scalable, making it practical for large-scale driving simulation.\n\nThe experiments are thorough and show emergent, human-like cooperative driving."}, "weaknesses": {"value": "The paper could more clearly analyze how the group-relative term affects individual vs. collective reward trade-offs.\n\nThe related work section should discuss earlier related papers including:\n• A. Kuefler, J. Morton, T. Wheeler, and M. J. Kochenderfer, “Imitating Driver Behavior with Generative Adversarial Networks,” IEEE Intelligent Vehicles Symposium (IV), 2017, pp. 204–211.\n• R. P. Bhattacharyya, B. Wulfe, D. J. Phillips, A. Kuefler, J. Morton, R. Senanayake, and M. J. Kochenderfer, “Modeling Human Driving Behavior through Generative Adversarial Imitation Learning,” Computing Research Repository (CoRR), arXiv:2006.08911, 2020.\n• H. Chen, T. Ji, S. Liu, and K. Driggs-Campbell, “Combining Model-Based Controllers and Generative Adversarial Imitation Learning for Traffic Simulation,” IEEE International Conference on Intelligent Transportation Systems (ITSC), 2022, pp. 1698–1704.\n• K. Brown, K. Driggs-Campbell, and M. J. Kochenderfer, “Modeling and Prediction of Human Driver Behavior: A Survey,” arXiv preprint arXiv:2006.08832, 2020."}, "questions": {"value": "Could the authors elaborate on how the group advantage interacts with overall performance? \n\nHow sensitive is RIFT to the group size or population composition?\n\n\nCould the authors provide a deeper analysis of the failure cases, such as uniformly conservative or overly passive driving behaviors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q4O97KZPwO", "forum": "SAOhTI3yHW", "replyto": "SAOhTI3yHW", "signatures": ["ICLR.cc/2026/Conference/Submission11761/Reviewer_A99Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11761/Reviewer_A99Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011919649, "cdate": 1762011919649, "tmdate": 1762922786488, "mdate": 1762922786488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RIFT, a framework that utilize RL to effectively improve controllability and realism for closed-loop driving simulation. After an IL model (Pluto) is trained to generate realistic, multimodal, route-conditioned trajectories for critical background vehicles (CBVs), the authors introduce Group-Relative RL Fine-Tuning that freezes the trajectory generation head and fine-tunes only the scoring head using a group-relative advantage over all candidate modalities, an equal-weight (mode-preserving) objective, and a dual-clip surrogate for stable optimization without KL anchoring. This design tackles covariate shift while retaining trajectory-level realism, style-level and route-level controllability. Extensive experiment results are provided to show the effectiveness in controllable and realistic scenario generation, closed-loop evaluation on E2E planners, and the effectiveness in the algorithmic design for RIFT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Clarity and Presentation.** The paper is clearly written and the core ideas are easy to follow, with well-motivated design choices and consistent presentation.\n- **Empirical Rigor.** Experiments are extensive, and the ablations are thoughtfully constructed to isolate the contribution of key components."}, "weaknesses": {"value": "- **Baselines for Controllability.** The paper does not compare against closely related controllable traffic generation methods (e.g., CTG, LCTGen). Given the paper’s emphasis on controllability, these baselines are important to position the contribution.\n- **Related Work on Group-Relative RL.** Prior work has already explored group-relative rewards for RL fine-tuning in closed-loop driving (e.g., Gen-Drive [1]). While the present paper targets scenario generation rather than policy learning, acknowledging and contrasting with these methods would sharpen the novelty claims.\n- **Covariate Shift Analysis.** The motivation highlights the open-loop vs. closed-loop covariate shift as a core challenge, yet the paper lacks a dedicated analysis. A quantitative and qualitative comparison of Pluto with SFT vs. Pluto with RIFT—under matched conditions—would strengthen the causal link between RLFT and improved closed-loop robustness.\n- **CBV Identification Assumption.** Critical background vehicles are selected via a distance threshold. Prior studies suggest that distance alone is less robust than TTC-based criteria for identifying safety-critical interactions [2,3]. A justification, sensitivity analysis, or comparison with TTC-based CBV identification would improve credibility.\n\n> [1] Huang et al., *Gen-Drive: Enhancing Diffusion Generative Driving Policies with Reward Modeling and Reinforcement Learning Fine-Tuning*, ICRA 2025.  \n> \n> [2] Chang et al., *Safe-Sim: Safety-Critical Closed-Loop Traffic Simulation with Diffusion-Controllable Adversaries*, ECCV 2024.  \n> \n> [3] Lin et al., *Causal Composition Diffusion Model for Closed-Loop Traffic Generation*, CVPR 2025."}, "questions": {"value": "1. **Equal-Weighted GRPO.** How is equal weighting across modalities operationalized in practice? Does this require on-policy rollouts under a slightly older behavior policy, and if so, how is sampling bias handled or corrected? Please provide algorithmic details and any variance/bias trade-offs.\n2. **Block Rate in Table 2.** For PDM-Lite, should Block Rate be “lower is better”? If so, please clarify the directionality and explain why BR is not reported for other E2E planners lacking privileged information.\n3. **Measuring Style-Level Controllability.** How do you quantify style-level controllability (if at all)? If formal quantification is challenging, could you expand the qualitative analysis (e.g., as in Appendix D, Fig. 6) with more systematic case studies or user studies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RN6XBFT21a", "forum": "SAOhTI3yHW", "replyto": "SAOhTI3yHW", "signatures": ["ICLR.cc/2026/Conference/Submission11761/Reviewer_CQsq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11761/Reviewer_CQsq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762117592212, "cdate": 1762117592212, "tmdate": 1762922786075, "mdate": 1762922786075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}