{"id": "noNpK9Vt8l", "number": 9787, "cdate": 1758140467915, "mdate": 1759897696368, "content": {"title": "Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs", "abstract": "Large language models are often adapted using parameter-efficient techniques such as Low-Rank Adaptation (LoRA), formulated as $y = W_0x + BAx$, where $W_0$ is the pre-trained parameters and $x$ is the input to the adapted layer.\n While multi-adapter extensions often employ multiple LoRAs, prior studies suggest that the inner $A$ matrices are highly similar during training and thus suitable for sharing. We revisit this phenomenon and find that this similarity is largely attributable to the identical initialization rather than shared knowledge, with $B$ playing a more critical role in knowledge encoding and transfer. Motivated by these insights, we propose **ALoRA**, an asymmetric multi-LoRA design with multiple $A$ matrices and a single shared $B$ in multi-task fine-tuning, and **Fed-ALoRA**, which shares $B$ across clients in federated fine-tuning under both homogeneous and heterogeneous settings, through a novel matrix decomposition strategy to accommodate heterogeneous ranks across clients. Experiments on commonsense reasoning, math reasoning, multi-task NLP dataset, and federated NLP dataset demonstrate that our methods achieve more balanced performance across tasks with comparable or superior average accuracy relative to existing multi-LoRA approaches.", "tldr": "", "keywords": ["Multi-Task Fine-Tuning", "Federated Fine-Tuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1505fb739bdbd28e33e76c548e3e95a39ff0eece.pdf", "supplementary_material": "/attachment/fe5def6a479c612d94f8531c5f42a2fc86a1a793.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigated the mixture of expert setup in low-rank adaptation (LoRA) of LLMs, which are particularly useful in multi-task and federated learning. It is observed that similar $A$ matrices are due to the shared initialization rather than knowledge, and multiple $A$ matrices are prone to gradient conflicts leading to lazy learning. Consequently, the authors advocated ALoRA that relies on multiple $A$ matrices yet a single shared $B$. For federated learning, the $B$ matrix is shared by decomposing into two components to account for heterogeneous tasks. Experiments are conducted on four datasets to demonstrate the effectiveness of the design."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The three observations in Section 3 are interesting, which well motivates the design of ALoRA and Fed-ALoRA. \n2. The methodology and design choices are clearly presented. \n3. The experimental results showcase a consistent improvement over HydraLoRA and FedSA-LoRA."}, "weaknesses": {"value": "1. My major concern is the similarity between this paper and [1], which not only has a closely related title, but also explores the same idea of \"multiple A and a single B\". A more detailed comparison with [1] is needed to clarify the distinctions and contributions of this work. \n2. Several closely related baselines that employ MoE for multi-task fine-tuning are missing in both the introduction and experiments, including LoRAMoE [2], MTL-LoRA [3], and CoLA [4]. \n3. In the experimental evaluation, the models are limited to Llama2-7B and Llama3-8B. It would strengthen the paper to include results from additional large language models. \n\n[1] Q. Dong et al., \"MASA: Rethinking the Representational Bottleneck in LoRA with Multi-A Shared Adaptation,\" *arXiv preprint arXiv:2510.06005*, 2025.  \n[2] S. Dou et al., \"LoRAMoE: Alleviating world knowledge forgetting in large language models via MoE-style plugin,\" in *ACL*, 2024.  \n[3] Y. Yang et al,, \"MTL-LoRA: Low-rank adaptation for multi-task learning,\" in *AAAI*, 2025.  \n[4] Y. Zhou et al., \"Cola: Collaborative low-rank adaptation,\" *arXiv preprint arXiv:2505.15471*, 2025."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sAIjPlOFl7", "forum": "noNpK9Vt8l", "replyto": "noNpK9Vt8l", "signatures": ["ICLR.cc/2026/Conference/Submission9787/Reviewer_FJ1x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9787/Reviewer_FJ1x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683249670, "cdate": 1761683249670, "tmdate": 1762921274711, "mdate": 1762921274711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel multi-task fine-tuning and federated learning approach. By rethinking parameter sharing mechanisms, we design the ALoRA and Fed-ALoRA architectures, which outperform existing methods in both performance and communication efficiency. Research indicates that the similarity in A matrices of LoRAs primarily stems from initialization rather than knowledge sharing, while B matrices play a crucial role in knowledge transfer. Building on this, we propose ALoRA and Fed-ALoRA methods that share B matrices for multi-task and federated fine-tuning respectively. Experiments demonstrate that these methods achieve more balanced performance while maintaining or improving accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.This study investigates a critical issue on multi-task fine-tuning. The authors investigate the distribution of LoRA parameters in multiple dimensions to support their claim on the contribution of matrices A and B. Therefore, it makes the proposed method well-motivated.\n2.The authors have conducted extensive experiments to demonstrate the strong performance of ALoRA and FedA-LoRA compared to baseline methods."}, "weaknesses": {"value": "1. The authors claim that Fed-ALoRA reduces communication costs by 50% and 75% in homogeneous and heterogeneous settings respectively, but the experimental part does not provide corresponding evaluation.\n2.The experimental model architecture and scale are limited, lacking exploration of MoE architectures or larger model parameters."}, "questions": {"value": "1. Is it possible for the authors to provide the computational and communication evaluation of Fed-ALoRA？\n2. I can hardly derive any real-world application of the proposed method. Pleas provide some cases."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8rnBp8dH2I", "forum": "noNpK9Vt8l", "replyto": "noNpK9Vt8l", "signatures": ["ICLR.cc/2026/Conference/Submission9787/Reviewer_aLxW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9787/Reviewer_aLxW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795719179, "cdate": 1761795719179, "tmdate": 1762921272372, "mdate": 1762921272372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper revisits the common belief that, in multi-LoRA setups, A matrices are naturally similar and thus good to share across tasks/clients. Through controlled experiments, the authors show this similarity mainly arises from shared initialization, not shared knowledge; instead, B matrices bear most task-specific information and undergo meaningful learning.\nExperiments on multi-task and federated NLP benchmarks show more balanced task performance, competitive or better accuracy, and reduced communication."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Sharing B instead of A is intuitive once the empirical insight is established.\n- Applies to both multi-task and federated settings.\n- Fed-ALoRA substantially reduces communication (up to 75%) while competitive in performance."}, "weaknesses": {"value": "- Theoretical analysis of why B encodes knowledge while A acts as a projector would help.\n- It is unclear which tasks benefit more from A-sharing versus B-sharing.\n- The motivation for sharing B is less compelling, especially since the majority of prior literature has focused on sharing A.\n- More experiments directly comparing sharing A vs. sharing B are needed to demonstrate the advantage of sharing B.\n- Many design choices (e.g., routing strategy, # of A matrices) could benefit from deeper ablation."}, "questions": {"value": "- How sensitive is ALoRA to A/B rank choice-s?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g6mzEbOElA", "forum": "noNpK9Vt8l", "replyto": "noNpK9Vt8l", "signatures": ["ICLR.cc/2026/Conference/Submission9787/Reviewer_i7f4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9787/Reviewer_i7f4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992052017, "cdate": 1761992052017, "tmdate": 1762921271761, "mdate": 1762921271761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper re-examines the common practice of sharing the LoRA `A` matrix in multi-adapter settings. The authors hypothesize that the observed similarity in `A` matrices is an artifact of identical initialization, not shared knowledge, and argue the `B` matrix is more critical for knowledge encoding. Based on this, they propose ALoRA, an asymmetric architecture with multiple task-specific `A` matrices and a single shared `B` matrix. For federated learning, they introduce Fed-ALoRA, which communicates only `B` matrices, and includes a matrix decomposition strategy to handle heterogeneous LoRA ranks. Experiments on multi-task and federated benchmarks are presented to support the claim that these methods achieve more balanced performance with comparable or better average accuracy."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "*   **Provocative Re-evaluation of LoRA Dynamics:** The paper's primary merit is its willingness to question a common assumption in multi-LoRA methods. The initial analysis in Section 3.1, which investigates the differing stability of `A` and `B` matrices under different initializations (Figure 1), poses an interesting and provocative question that challenges the rationale behind `A`-sharing architectures like HydraLoRA.\n\n*   **Practical Design for Heterogeneous Federated Learning:** The proposed Fed-ALoRA method considers a critical real-world problem: enabling collaboration among clients with heterogeneous LoRA ranks. The use of matrix decomposition (Sec 4.2) to standardize the shared parameter space demonstrates thoughtful engineering to address a practical limitation of prior works."}, "weaknesses": {"value": "*   **Fundamental Contradiction Between Motivation and Method:** The paper's core logic is self-contradictory. The authors' own analysis (Sec 3.2, Figure 2) posits that the `A` matrix is largely static (\"fixed feature projector\") while the `B` matrix is dynamic and encodes task-specific knowledge. The most logical conclusion from this finding would be to share the common, static `A` matrix across tasks/clients while allowing the dynamic `B` matrix to be learned independently. Instead, the paper proposes the exact opposite—sharing `B` and keeping `A` separate—without providing a convincing theoretical justification for this inversion. This strongly suggests that the method was not derived from the paper's stated motivation, but rather that the motivation was retrofitted to explain empirical results, which undermines the scientific integrity of the work.\n\n*   **Incremental Contribution and Overstated Novelty:** The core idea of Fed-ALoRA is far from novel and appears to be a derivative of existing work. Architectures that selectively share a single LoRA matrix to reduce communication costs are already established, most notably **FedSA-LoRA** (which shares `A`). The proposed method is essentially a \"symmetric inversion\" of this concept. Furthermore, the technique used to handle heterogeneity—matrix decomposition to align dimensions—is a standard and widely used tool in machine learning for dealing with mismatched parameter spaces. The paper fails to demonstrate a significant conceptual leap, positioning its contribution as incremental at best.\n\n*   **Fundamentally Flawed and Irrelevant Baseline Comparisons:** The experimental design is compromised by unfair and irrelevant comparisons that create a misleading narrative.\n    1.  **Orthogonal Problem Comparison:** The inclusion of **AdaLoRA** as a baseline is highly questionable. AdaLoRA addresses the problem of *intra-task* dynamic rank allocation for parameter efficiency. This paper addresses *inter-task/client* parameter sharing for knowledge transfer and communication efficiency. These are orthogonal research problems. Comparing ALoRA to AdaLoRA does not isolate the benefits of `B`-sharing; it merely shows that a parameter-sharing method can outperform a sophisticated but non-sharing method, which is an expected but uninformative result. This suggests a \"kitchen-sink\" approach to baseline selection rather than principled experimental design.\n    2.  **Apples-to-Oranges Comparison:** The ALoRA vs. HydraLoRA comparison (Table 2) is misleading. ALoRA is an MoE-like architecture with a routing mechanism, whereas HydraLoRA is a simpler parameter-sharing model. Attributing performance differences solely to the choice of shared matrix (`B` vs. `A`) is disingenuous when the architectural complexities are fundamentally different.\n    3.  **Strawman Argument:** The comparison in the heterogeneous setting (Table 5) is built upon a strawman. The authors admit the baseline `FedSA-LoRA*` is their own adaptation of a method not designed for heterogeneity. Its poor performance cannot be reliably interpreted as a failure of `A`-sharing.\n\n*   **Lack of Statistical Rigor Renders Results Unreliable:** The paper reports single-run point estimates for all experiments (Tables 2, 3, 4, 5). This is a critical methodological flaw. Given the high variance inherent in LLM fine-tuning, the small margins of improvement reported are statistically meaningless without mean scores and standard deviations over multiple runs. It is impossible to verify whether the claimed gains are real or simply artifacts of random chance.\n\n*   **Foundational Claims Rest on Extremely Narrow Evidence:** The central claims about the universal roles of matrices `A` and `B` are based on an extremely limited experimental setup (LLaMA2-7B on the Dolly-15K dataset). These findings are presented as general properties of LoRA, but there is no evidence to support their generalizability across recent open source models, different model architectures, model sizes, or a wider variety of task domains.\n\n*   **Ambiguity in the Heterogeneous Federated Learning Method:** The formulation and update procedure for the heterogeneous Fed-ALoRA method are not sufficiently clear.\n    1.  The role of the accumulator $B_i^0$ is confusing. The local training objective is given as $\\mathcal{L}(W_0 + (B_i^0 + B_{i2}B_{i1})M_iA_i)$ (Sec 4.2, Step 2). However, Step 4 states the server broadcasts a global matrix $B_0^t$. It is not specified how client $i$ uses this $B_0^t$ to update its local accumulator $B_i^0$ for the next round. Does $B_i^0$ become $B_0^t$, or is it accumulated (e.g., $B_i^0 \\leftarrow B_i^0 + B_0^t$)? The text only says $B_i^0$ is initialized with $B_{t-1}^0$ (Step 1), which seems to imply it never receives the global update.\n    2.  The paper does not provide a clear justification for the specific decomposition $\\Delta W_i = B_{i2}B_{i1}M_iA_i$. While it enables aggregation, the introduction of three new matrices ($B_{i1}$, $B_{i2}$, $M_i$) increases complexity. The rationale for this particular design over other potential solutions (e.g., projecting heterogeneous $B$ matrices to a common subspace) is not discussed."}, "questions": {"value": "1.  Your own analysis suggests the `A` matrix is static while `B` is dynamic. The intuitive conclusion would be to share the static part (`A`). Your method does the opposite. Could you resolve this apparent **logical contradiction**? Why is it better to share the dynamic, knowledge-encoding part across different clients/tasks?\n\n2.  Could you clarify the rationale for including **AdaLoRA** as a baseline? AdaLoRA solves the orthogonal problem of dynamic rank allocation within a single task, while your work focuses on parameter sharing across tasks. What specific hypothesis is this comparison intended to test, other than showing that parameter sharing can outperform no sharing?\n\n3.  Given prior work like **FedSA-LoRA** (which shares `A`), your proposed Fed-ALoRA can be seen as a \"reverse design.\" Could you clarify the novel scientific contribution of your work beyond this inversion and the application of standard matrix decomposition techniques for handling heterogeneity?\n\n4.  In the heterogeneous setting (Table 5), you compare against `FedSA-LoRA*`, which is your own adaptation. Do you agree that this is not a fair or valid comparison? How can you justify claims of superiority when the baseline's poor performance might simply be an artifact of a suboptimal adaptation?\n\n5.  All reported results lack error bars. Given that the reported performance gains are often marginal, how can you be confident that these results are **statistically significant** and not just noise from a single random seed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qbNauVy3c9", "forum": "noNpK9Vt8l", "replyto": "noNpK9Vt8l", "signatures": ["ICLR.cc/2026/Conference/Submission9787/Reviewer_CEiW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9787/Reviewer_CEiW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997931742, "cdate": 1761997931742, "tmdate": 1762921270556, "mdate": 1762921270556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}