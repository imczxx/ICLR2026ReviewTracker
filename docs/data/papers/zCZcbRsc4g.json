{"id": "zCZcbRsc4g", "number": 15418, "cdate": 1758251151586, "mdate": 1759897308183, "content": {"title": "Avoid Catastrophic Forgetting with Rank-1 Fisher from Diffusion Models", "abstract": "Catastrophic forgetting remains a central obstacle for continual learning in neural models.\nPopular approaches---replay and elastic weight consolidation (EWC)---have limitations: replay requires a strong generator and is prone to distributional drift, while EWC implicitly assumes a shared optimum across tasks and typically uses a diagonal Fisher approximation.\nIn this work, we study the gradient geometry of diffusion models, which can already produce high-quality replay data.\nWe provide theoretical and empirical evidence that, in the low signal-to-noise ratio (SNR) regime, per-sample gradients become strongly collinear, yielding an empirical Fisher that is effectively rank-1 and aligned with the mean gradient.\nLeveraging this structure, we propose a rank-1 variant of EWC that is as cheap as the diagonal approximation yet captures the dominant curvature direction.\nWe pair this penalty with a replay-based approach to encourage parameter sharing across tasks while mitigating drift.\nOn class-incremental image generation datasets (MNIST, FashionMNIST, CIFAR-10, ImageNet-1k), our method consistently improves average FID and reduces forgetting relative to replay-only and diagonal-EWC baselines. In particular, forgetting is nearly eliminated on MNIST and FashionMNIST and is roughly halved on ImageNet-1k.\nThese results suggest that diffusion models admit an approximately rank-1 Fisher.\nWith a better Fisher estimate, EWC becomes a strong complement to replay: replay encourages parameter sharing across tasks, while EWC effectively constrains replay-induced drift.", "tldr": "In diffusion, per-sample gradients become collinear in the low SNR, yielding a rank-1 Fisher. We propose a rank-1 EWC and pair it with replay. On continual learning tasks, it improves FID and nearly eliminates forgetting on MNIST and FashionMNIST.", "keywords": ["continual learning", "diffusion models", "catastrophic forgetting", "image generation", "elastic weight consolidation", "generative replay"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/631c006cb8c46ac98a59e9befe2fab1a5c08918e.pdf", "supplementary_material": "/attachment/5f6880b14505fac3f913df9b40e3d68f2579d118.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the gradient geometry of diffusion models in low-SNR timesteps and argues that per-sample gradients become nearly collinear, making the empirical Fisher effectively rank-1. Building on this, the authors propose a rank-1 EWC penalty (cheap to compute) and pair it with generative distillation for continual image generation. Across MNIST, FMNIST, CIFAR-10, and downsampled ImageNet-1k (32×32), the method improves average FID and notably reduces forgetting versus diagonal-Fisher EWC and distillation alone (e.g., near-zero forgetting on MNIST/FMNIst and roughly halved on ImageNet-1k). Theory (Propositions/Theorem) and empirical analyses (eigenspectra, cosine similarities) support the rank-1 claim."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear theory → practice bridge: formal propositions/theorem (low-SNR → gradient collinearity → rank-1 Fisher) with convincing empirical validation (eigenvalue dominance, Frobenius error vs. diagonal). \n\nSimple & efficient: rank-1 EWC captures dominant curvature at cost comparable to diagonal EWC; easy to add to standard UNet diffusion setups. \n\nConsistent gains: with generative distillation, improves AFID and reduces forgetting on four datasets; forgetting ≈ 0 on MNIST/FMNIst and ~halved on ImageNet-1k. \n\nRobust learning dynamics: smoother curves over long horizons; qualitative samples maintain object sharpness compared to baselines that drift."}, "weaknesses": {"value": "Scope/scale limits: experiments are at 32×32 resolution (even for ImageNet-1k) and on UNet backbones; no results on larger resolutions or Transformer/DiT diffusion models, which the paper itself flags as future work. \n\nHeavy reliance on distillation: EWC alone underperforms; the strongest results require rank-1 EWC + generative distillation, so the standalone benefit of the rank-1 penalty is limited."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4crlX9aklV", "forum": "zCZcbRsc4g", "replyto": "zCZcbRsc4g", "signatures": ["ICLR.cc/2026/Conference/Submission15418/Reviewer_HQm2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15418/Reviewer_HQm2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641900831, "cdate": 1761641900831, "tmdate": 1762925696035, "mdate": 1762925696035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the problem of catastrophic forgetting in the continual learning of generative diffusion models. The authors identify a key limitation of Elastic Weight Consolidation (EWC), a popular regularization method, which typically relies on a diagonal approximation of the Fisher Information Matrix (FIM) that fails to capture important parameter correlations. The paper's main contribution is a theoretical and empirical analysis showing that for diffusion models in the low signal-to-noise ratio (SNR) regime, the empirical FIM is effectively rank-1 and aligned with the mean gradient. Leveraging this insight, the authors propose a \"Rank-1 EWC\" penalty that is computationally efficient yet captures this dominant curvature direction. This regularizer is combined with generative distillation (a form of replay) to create a synergistic approach that encourages a shared parameter space while mitigating drift. Experiments on class-incremental image generation tasks (MNIST, FashionMNIST, CIFAR-10, and ImageNet-1k) show that the proposed method significantly reduces forgetting and improves sample quality (FID) compared to replay-only and diagonal-EWC baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel and Insightful Analysis of Fisher Geometry: The core of the paper is a novel and non-obvious finding about the gradient structure of diffusion models.\n2. Practical and Computationally Efficient Algorithm: The proposed Rank-1 EWC penalty, as formulated in Equation 6, is highly practical.\n3. Strong Empirical Performance and Validation: The method demonstrates impressive empirical results, substantially reducing catastrophic forgetting and improving generation quality (FID) across four different benchmarks."}, "weaknesses": {"value": "1. Heavy Reliance on Generative Distillation: A significant weakness is the method's apparent dependence on the replay component.\n2. Strength of Theoretical Assumptions: The theoretical argument hinges on Assumption 1, which posits that the score network $s_{\\theta}(x_t, t)$ can be approximated as a linear function of its parameters, $s_{\\theta}(x_t, t) \\approx x_t\\theta$, in the low-SNR regime.\n3. Limited Scope of Fisher Matrix Analysis: The detailed empirical analysis of the FIM in Section 3.2 is conducted on a small-scale diffusion model trained on MNIST"}, "questions": {"value": "1. The effectiveness of your Rank-1 EWC is clearly dependent on the generative distillation component. Could you elaborate on the precise role of the penalty?\n2. Your theoretical analysis highlights the emergence of the rank-1 structure in the low-SNR (late timestep) regime. However, the EWC penalty in Equation 6 appears to be based on a mean gradient $\\mu$ that is averaged over all timesteps. How does the inclusion of gradients from high-SNR timesteps, where the Fisher may not be rank-1, affect the validity and performance of your proposed penalty?\n3. The analysis in Figure 3a shows that the mean gradients $\\mu_t(\\theta)$ are highly aligned across timesteps but are not perfectly collinear. This suggests that a single rank-1 approximation for the entire FIM might be discarding some useful information."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dEnmRqWdCQ", "forum": "zCZcbRsc4g", "replyto": "zCZcbRsc4g", "signatures": ["ICLR.cc/2026/Conference/Submission15418/Reviewer_mxSL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15418/Reviewer_mxSL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955237540, "cdate": 1761955237540, "tmdate": 1762925695530, "mdate": 1762925695530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper concentrates on the continual learning of diffusion models with an improved EWC method (i.e., rank-1 EWC). The main contributions are summarized as:\n\na. This paper provides both theoretical and empirical characterizations of Fisher information geometry in diffusion models, showing that low SNR induces a near rank-1 Fisher aligned with the mean gradient.\n\nb. This paper proposes a practical rank-1 EWC penalty that is as cheap as a diagonal penalty but captures more curvature information for diffusion models.\n\nc. This paper provides extensive experiments to prove the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "a. This paper is well written and easy to follow.\n\nb. This paper is technically solid.\n\nc. It is interesting and significant to prove that the empirical Fisher matrix of diffusion models is effectively rank-1."}, "weaknesses": {"value": "I am generally satisfied with the content presented in the paper; however, I still have the following concerns:\n\na. I believe that the most critical premise supporting this paper is Assumption 1. However, its explanation is overly intuitive and lacks rigor. Please use a concrete example and demonstrate, from a mathematical perspective, how this example is connected to Assumption 1. This is the major concern.\n\nb. The Theorem considers the low SNR region (i.e, at later diffusion timesteps). However, SNR will be high at most diffusion timesteps. How does the paper account for this situation?\n\nc. How to determine that a SNR is low?"}, "questions": {"value": "a. In Section 3, it is mentioned that \"This is plausible in practice because a trivial solution for UNet is to directly route the inputs to the output due to the skip connections\". Please theoretically discuss Assumption 1 according to this example in details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PnaUgLUebz", "forum": "zCZcbRsc4g", "replyto": "zCZcbRsc4g", "signatures": ["ICLR.cc/2026/Conference/Submission15418/Reviewer_AxKy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15418/Reviewer_AxKy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966060595, "cdate": 1761966060595, "tmdate": 1762925694638, "mdate": 1762925694638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the gradient geometry of diffusion models and argues that, at low SNR timesteps, per-sample gradients align strongly with their mean, making the empirical Fisher approximately rank-1. Building on this, the authors propose a rank-1 EWC penalty that is as cheap as the diagonal approximation but better aligned with the dominant curvature direction. They pair it with diffusion-based generative distillation/replay. On class-incremental image generation (MNIST, FMNIST, CIFAR-10, downsampled ImageNet-1k), the method improves AFID and reduces forgetting versus diagonal-EWC and replay alone; forgetting is nearly eliminated on MNIST/FM and roughly halved on ImageNet-1k."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation example is well positioned, the story of the paper is clear and easy to follow.\n- Clean connection from low SNR → gradient collinearity → rank-1 Fisher, with both theory and measurement\n- Rank-1 EWC that is drop-in and cost-comparable to diagonal while capturing dominant curvature. \n- Consistent AFID/forgetting improvements; notably near-zero forgetting and significant improvement on different benchmarks."}, "weaknesses": {"value": "- Ablations missing/limited: (i) k-rank (>1) Fisher vs rank-1 vs diagonal; (ii) Sensitivity to λ, μ-estimation schedule, and the SNR/timestep sampling strategy; (iii) replay buffer size; teacher choice (EMA vs last checkpoint) in distillation.\n\n- No wall-clock/memory comparisons vs Diag-EWC and low-rank baselines (e.g., diagonal + small K-FAC block, diagonal + momentum/EMA Fisher smoothing).\n\n- Main long-horizon test uses 32×32 ImageNet-1k; higher-res or domain/task-incremental setups would strengthen claims.\n\n- I suspect that other baselines are not well tuned. Figure 4 shows that the proposed method is consistently better than others in adapting to new task, which means the method can obtain both stability and plasticity overall. I am wondering how the result would look like if λ=0 (no regularization)."}, "questions": {"value": "- Is this the first work about continual learning for diffusion model? If it is not, I expect the author to compare against other related work. The main table result looks like an ablation studies for Fisher approximation approaches.\n\n- Can you conduct ablation studies on rank-k (k=2–8) variants to test diminishing returns beyond rank-1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7TzdEGrcR9", "forum": "zCZcbRsc4g", "replyto": "zCZcbRsc4g", "signatures": ["ICLR.cc/2026/Conference/Submission15418/Reviewer_rvLS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15418/Reviewer_rvLS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762236974738, "cdate": 1762236974738, "tmdate": 1762925693727, "mdate": 1762925693727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}