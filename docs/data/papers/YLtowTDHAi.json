{"id": "YLtowTDHAi", "number": 5027, "cdate": 1757836058644, "mdate": 1759897999667, "content": {"title": "M3E: A Unified Framework for Large-Scale Multimodal Embedding via Multi-Task Mixture-of-Experts", "abstract": "Universal multimodal embeddings are crucial for enabling downstream tasks such as cross-modal retrieval and retrieval-augmented generation. With the powerful semantic understanding capabilities of Large Vision-Language Models (LVLMs), leveraging them for embedding learning has emerged as a new paradigm. Recent research has primarily focused on prompt engineering or synthesizing high-quality training samples to enhance embedding quality. Although significant progress has been made, these methods often overlook the task diversity inherent in general-purpose embedding learning. This leads to two major issues: (1) The presence of too many easy or false negative samples degrades the discriminative power of the learned representations; (2) Diverse training tasks can lead to task conflict and oblivion problems. In this paper, we propose a unified multimodal multi-task embedding framework $\\mathrm{ M^3E}$ that integrates innovations at both the data and model levels. On the data side, we utilize a Hard Negative-Aware Sample Scheduler (HNASS) module to increase the proportion of hard negative samples. In addition, to reduce easy negatives sample in the batch, we ensure the samples in a batch come from the same task dataset. While optimization for different tasks should be decoupled to avoide task conflicts. So on the model side, we design a Task-wise Low-Rank Mixture of Experts (Task-wise MOE) module that allocates task-specific experts to capture specialized representations, while shared experts are used to learn generalizable cross-task knowledge. This effectively mitigates inter-task conflicts and improves the stability of multi-task learning. Extensive experiments demonstrate that our method significantly improves the embedding performance of LVLMs across 36 tasks.  Our code will be released.", "tldr": "", "keywords": ["Multimodal Embedding", "Large Vision-Language Models", "Contrastive Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/89f71ae2527e375b5be4ea0709dc824faef2668a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes M3E, a unified multimodal multi-task embedding framework that leverages Large Vision-Language Models (LVLMs) to learn universal cross-task representations. At the data level, a Hard Negative-Aware Sample Scheduler (HNASS) is designed: a teacher model constructs a similarity graph, followed by graph clustering to raise the ratio of hard negatives and filter easy/false negatives in each batch. At the model level, Task-wise Low-Rank Mixture-of-Experts (Task-wise MOE) splits parameters into task-specific and shared experts, dynamically combined by a router to mitigate task conflicts and forgetting. Joint training on 36 datasets across four task types significantly surpasses prior work, improving average scores by 10.3 and 7.8 points for 2B- and 7B-parameter LVLMs, respectively."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work is well-motivated.  HNASS and Task-wise MOE are designed for tackling easy samples and multi-task challenges respectively.\n2. Compared to some baseline models, the model achieves the state-of-the-art on several different tasks on different LVLM backbones."}, "weaknesses": {"value": "1. HNASS relies on an iteratively updated teacher model, complicating training and raising computational cost. Hyper-parameters p, m, c need manual tuning, transferability across datasets/scales is unclear.\n2. Number and layer allocation of task experts are empirical. Downstream tasks are uniformly cast as ranking; generative or more complex scenarios are not covered. Routing weights and expert specialization lack quantitative analysis, hampering failure diagnosis."}, "questions": {"value": "1. How about the sensitivity of p and m in HNASS across datasets?\n2. How about the impact of expert initialization, routing noise, and regularization on convergence in Task-wise MOE?\n3. How about extra training/inference time and memory caused by HNASS clustering and MOE routing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "60p3JSjzP9", "forum": "YLtowTDHAi", "replyto": "YLtowTDHAi", "signatures": ["ICLR.cc/2026/Conference/Submission5027/Reviewer_bUtV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5027/Reviewer_bUtV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644456295, "cdate": 1761644456295, "tmdate": 1762917829834, "mdate": 1762917829834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces M3E, a multimodal multi-task embedding framework that jointly enhances data sampling and model architecture. On the data side, the Hard Negative-Aware Sample Scheduler (HNASS) builds similarity graphs from embeddings to mine hard negatives via graph clustering. On the model side, the Task-wise Low-Rank Mixture-of-Experts (Task-wise MoE) uses task-specific and shared LoRA experts with a routing mechanism to reduce gradient conflict and task forgetting. Experiments on MMEB-V1 (36 tasks) show consistent improvements over strong LVLM baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Innovative data-model synergy: HNASS complements Task-wise MoE to address both false negatives and multi-task interference.\n2. Well-motivated low-rank MoE design that balances efficiency and specialization.\n3.Strong quantitative results with +10.3 and +7.8 point gains on Qwen2-VL-2B/7B baselines."}, "weaknesses": {"value": "1. Limited analysis of computational overhead (e.g., graph clustering, expert routing latency).\n2. Ablations on dynamic expert allocation show mixed effects (e.g., lower Visual Grounding).\n3. Lack of comparison with alternative negative-sampling or multi-task scheduling methods. (I am not sure whether there are many negative-sampling approaches.)"}, "questions": {"value": "1. How does M3E scale computationally compared to random batching or B3++?\n2. Can the teacher update schedule or similarity thresholds (p,m) be tuned automatically?\n3. What is the runtime/memory cost at inference when multiple experts are active?\n4. Why does dynamic-expert allocation harm Visual Grounding, and could low-layer experts mitigate this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "biIw99FCNU", "forum": "YLtowTDHAi", "replyto": "YLtowTDHAi", "signatures": ["ICLR.cc/2026/Conference/Submission5027/Reviewer_wbgn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5027/Reviewer_wbgn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913993910, "cdate": 1761913993910, "tmdate": 1762917829327, "mdate": 1762917829327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on two problems in the subject of multi-modal embedding representation: 1) hard negative sample mining; and 2) task conflict and oblivion. The problems are well described, but there lacks of novel strategies and new valid metrics to measure the effectiveness on these problems (especially on the task conflict and oblivion)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The research problems are focused. The negative sample mining is an important subject in contrastive learning, while task conflict & oblivion are inspiring.\n- M3E surpasses other baselines and reaches good performance."}, "weaknesses": {"value": "- The overall methodology is based on B3++ and lacks of novelty. M3E shares the same strategy with B3++ which focuses on batch mining with ranking and METIS clustering. The hard negative sample mining problem is not new.\n    - Compared with B3++, the iterative teacher model updating is not clear. How do you refine the teacher model and the representations accordingly to calculate similarities (line 301 - 302)?\n- The task conflict & oblivion problems are not well formed. Although the concept is clearly stated in the introduction, the preliminary experiments (in Figure 2) do not fully represent the phenomenon.\n    - The meaning of Figure 2b x-axis is unclear, is it in an early stage of training or the model has been well trained? It is not clear to confirm whether it is performance fluctuation or real task conflict & oblivion.\n    - What if there are only two tasks? If there are conflicts, the phenomenon would be the same? One obtains higher performance during training, and the other one will degrade.\n    - Although the overall performance is improved, there lacks metrics to measure the levels of task conflict and oblivion. How much do you solve them?\n- Additional experts means additional computational costs. Although the performances are improved, it is not clear if it comes from these additional trainable parameters.\n    - Besides, more shared experts lead to higher performance, but the top-k strategy is worse than the mask-based strategy. Maybe an additional ablation experiment on fully activated experts / scaling the number of top-k experts would help understanding the dynamics?"}, "questions": {"value": "- Figure 1: Archo should be Anchor?\n- MOE → MoE\n- B/C → B/c in line 291\n- Could you provide the whole histograms with thresholds in them rather than presenting the numbers of three categories in Figure 2a?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wQ2SD2gvHX", "forum": "YLtowTDHAi", "replyto": "YLtowTDHAi", "signatures": ["ICLR.cc/2026/Conference/Submission5027/Reviewer_yMWm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5027/Reviewer_yMWm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921999942, "cdate": 1761921999942, "tmdate": 1762917828099, "mdate": 1762917828099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}