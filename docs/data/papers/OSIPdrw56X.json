{"id": "OSIPdrw56X", "number": 2963, "cdate": 1757307611307, "mdate": 1759898116906, "content": {"title": "UniCalli: A Unified Diffusion Framework for Column-Level Generation and Recognition of Chinese Calligraphy", "abstract": "Computational replication of Chinese calligraphy, a cornerstone of cultural heritage, remains challenging. Existing methods split into two flawed camps: some render high-quality isolated characters yet miss page-level aesthetics (ligatures, spacing, scale), while others attempt page/column synthesis but sacrifice calligraphic correctness. We introduce UniCalli, a unified diffusion framework for column-level recognition and generation. Training both tasks in one model is deliberate: recognition constrains the generator to preserve character identity and stroke structure, while generation supplies strong style/layout priors—together fostering concept-level abstractions (radicals, stroke configurations) that improve both tasks under long-tail, limited-label regimes. We curate a dataset of 8,000+ digitized pieces, with ~4,000 densely annotated (script labels, character boxes, transcriptions). UniCalli employs asymmetric noising and a rasterized box map to inject spatial priors, and is trained on a mix of synthetic, labeled, and unlabeled data. The model is robust to rare styles, better disentangles style from script, and attains state-of-the-art generative quality with clear gains in ligature continuity and layout fidelity, alongside stronger recognition. The framework extends to other ancient scripts, demonstrated by successful transfer to Oracle bone inscriptions and Egyptian hieroglyphs. Code and data will be released.", "tldr": "", "keywords": ["calligraphy"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9cd5977999d77925573a90d6df21264851ab1796.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the challenges of data scarcity, long-tailed style distribution, and the quality trade-off between character-level fidelity and page-level coherence in Chinese calligraphy generation and recognition. Firstly, the work constructs a curated, annotated calligraphy dataset, providing a valuable resource for page-level research. Concurrently, the study proposes the UniCalli framework, which synergistically trains generation and recognition as mutually beneficial dual tasks. The framework incorporates a non-autoregressive global planning architecture and introduces a rasterized bounding-box map to enhance the model's spatial reasoning capabilities. Experimental results indicate that these strategies have achieved a performance improvement."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.A primary strength is the construction of a annotated dataset of classical Chinese calligraphy works. This resource is valuable and provides a substantial asset for subsequent research in page-level calligraphy analysis and generation.\n\n2.The introduction of asymmetric noising to integrate generation and recognition into a single, bidirectional framework forms the core contribution of this work. This unified approach is shown to enhance the performance of both tasks, as evidenced by the experimental results."}, "weaknesses": {"value": "1.Unclear writing and definitions: The introduction does not clearly articulate the logical relationship between the two stated challenges: (1) scarce data and a long-tail distribution of styles; (2) the inability of existing methods to generate both high-quality individual characters and coherent full-page layouts. Additionally, the concepts of \"page-level\" and \"column-level\" are used interchangeably without a clear definition of the proposed method's application scope.\n\n2.Incomplete methodological description: Firstly, the workflow of the recognizer is incomplete. According to the methodology section, the recognizer only generates a Condition Image rather than text, leaving it ambiguous how the final text output is actually derived from this image. Secondly, regarding the \"Duplicate RoPE with Modulated Embedding,\" the description does not specify how the learnable modulation embeddings are added to RoPE (e.g., applied to 'Q' or 'K'). This lack of detail raises concerns about whether the operation might disrupt RoPE's relative positional encoding functionality.\n\n3.Insufficient Comparative Baselines: The experimental comparisons for the generation task are restricted to only one specialized calligraphy generation method (FontDiffuser), with the remaining baselines being general-purpose models. Similarly, the recognition task is evaluated solely against general-purpose models. It is recommended to include comparisons with more specialized methods, such as CalliffusionV2 for generation and CalliReader for recognition, as mentioned in the related work.\n\n4.Potential for Data Bias and Lack of Generalization: The recognition accuracy comparison in Table 2 may not be entirely fair. Since the test set is derived from the authors' proprietary dataset, UniCalli's superior performance could be attributed to its prior exposure to a similar data distribution during training, an advantage not shared by the baseline models. Although the authors appropriately acknowledge this potential limitation in line 403, it would be advisable to supplement the evaluation with performance benchmarks on established public calligraphy datasets. Such an addition would help ensure an equitable comparison and more conclusively demonstrate the recognizer’s robustness. \n\n5.Inconsistency Between Generated Results and Descriptions: A qualitative analysis of Table 6 reveals a potential issue: the second generated image appears visually inconsistent with its reference image."}, "questions": {"value": "1.The experimental section only presents results on single-column data. A significant question arises regarding the generalizability of the proposed method: can it be effectively applied to the generation and recognition of multi-column layouts, which are common in actual page-level documents? If not, the claim that the method is suitable for page-level tasks appears overstated and requires qualification.\n\n2.The process by which the recognizer converts the Condition Image into text is unclear. Does the pipeline incorporate an off-the-shelf OCR module? Please clarify the specific implementation details.\n\n3.In the 'Duplicate RoPE with Modulated Embedding', how are the learnable modulation embeddings added to the RoPE? I am confused as to whether doing so will damage the relative positional encoding function of RoPE."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vY4iJVatKe", "forum": "OSIPdrw56X", "replyto": "OSIPdrw56X", "signatures": ["ICLR.cc/2026/Conference/Submission2963/Reviewer_ct53"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2963/Reviewer_ct53"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822447848, "cdate": 1761822447848, "tmdate": 1762916464508, "mdate": 1762916464508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UniCalli, which unifies column-level generation and recognition of Chinese calligraphy in a single diffusion framework. It is evaluated on datasets of Chinese characters, oracle bone scripts, and Egyptian hieroglyphs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis paper analyzes the column-level text recognition and generation tasks and proposes a unified framework.\n2.\tThis paper presents a large-scale column-level calligraphy annotation dataset."}, "weaknesses": {"value": "1.\tIn this paper, \"column-level generation\" refers to the generation of a single column of calligraphy image, which is fundamentally different from the \"page-level generation\" emphasized in the introduction section. Please clarify this distinction.\n2.\tIn Section 3.3, this paper uses conditional dropout addresses the issue of overfitting to long-tail calligraphic styles. Could you provide a more comprehensive explanation of this issue, as well as the motivation of this module?\n3.\tIn the experiments, the paper uses several general models (such as ChatGPT-5, Ernie-4.5, and Doubao). Would it be possible to include additional calligraphy generation method like [1] for comparison?\n4.\tThis paper claims that coupling recognition and generation tasks can improve the model's performance on both tasks, but lacks effective ablation experiments to validate this claim. It is recommended to conduct separate ablation experiments for the generation and recognition modes to demonstrate it.\n5.\tThe ablation experiments lack a clear description of the baseline setup.\n6.\tIt is recommended to provide visualizations to clearly illustrate the impact of RoPE Duplication module?\n\n[1] Dp-font: Chinese calligraphy font generation using diffusion model and physical information neural network, IJCAI, 2024."}, "questions": {"value": "Please refer to Weaknesses 1-6."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0gDEmYkZYI", "forum": "OSIPdrw56X", "replyto": "OSIPdrw56X", "signatures": ["ICLR.cc/2026/Conference/Submission2963/Reviewer_ucuD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2963/Reviewer_ucuD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914492020, "cdate": 1761914492020, "tmdate": 1762916464274, "mdate": 1762916464274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents UniCalli, a diffusion-based model for Chinese calligraphy generation. During training, the model performs both generation and recognition tasks within the same architecture to enhance the learning of key structural features. To better realize column-level ligatures, the authors introduce an additional mask input and leverage shared positional information to improve spatial understanding.During experiments, the authors observe overfitting to long-tail calligraphers and propose random dropout to disentangle style and glyph representations. Overall, the paper is well organized, the results are convincing, and the experiments are sufficient, including evaluations on other ancient scripts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The proposed method is reasonable. By using the timestep ti to control the noise level, the model can indirectly alternate between recognition and generation, achieving joint enhancement of both tasks. This design also facilitates later dropout-based disentanglement.\n2.The experiments are comprehensive, including comparisons with current state-of-the-art multimodal models and other ancient scripts."}, "weaknesses": {"value": "1.Some design choices lack clear justification, such as the use of RoPE and MMDiT. It would be helpful to explain why these particular architectures are necessary for this task.\n2.Although the ablation study is relatively detailed, its progressive setup makes certain conclusions less clear. For example, when dropout is combined with RoPE duplication, performance decreases—would applying dropout alone (without RoPE duplication) lead to better results?\n3.The ablation study does not examine the value or distribution of tc. Since the model jointly trains recognition and generation by mixing content images and masks, the selection of tc may play an important role in model performance."}, "questions": {"value": "1.In Section 3.3, tc is dropped out under different conditions, but it is unclear whether this happens during the recognition task, the generation task, or both.\n2.In Figure 4, some generated examples (e.g., Sun Guoting, Yan Zhenqing, Cliff Inscriptions) contain white patches that do not belong to the calligraphic strokes. What is the cause of these artifacts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ySDBuphQlY", "forum": "OSIPdrw56X", "replyto": "OSIPdrw56X", "signatures": ["ICLR.cc/2026/Conference/Submission2963/Reviewer_QPYC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2963/Reviewer_QPYC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915473446, "cdate": 1761915473446, "tmdate": 1762916464035, "mdate": 1762916464035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UniCalli, a unified diffusion framework for column-level generation and recognition of Chinese calligraphy. The model jointly trains both tasks within a single diffusion transformer, where recognition constrains structural fidelity and generation provides stylistic and spatial priors. An asymmetric noising scheme enables bidirectional task switching, while rasterized box maps encode spatial structure for improved ligature and layout coherence. The authors also present a large-scale annotated dataset of over 8,000 calligraphic works. Experiments show that UniCalli achieves state-of-the-art generative fidelity and competitive recognition accuracy, and generalizes effectively to other ancient scripts, demonstrating its potential for comprehensive calligraphy synthesis and analysis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel framework that unifies calligraphy generation and recognition within a single diffusion transformer. The use of asymmetric noising to switch between generation and recognition is both simple and effective, offering a principled way to couple dual tasks through shared latent representations.\n2. The authors contribute a large-scale, high-quality dataset comprising over 8,000 digitized works from 93 calligraphers, with detailed annotations that fill a long-standing gap in computational calligraphy research.\n3. The framework’s successful extension to other ancient scripts (e.g., Oracle bone inscriptions and Egyptian hieroglyphs) demonstrates its strong adaptability and generalization potential beyond Chinese calligraphy."}, "weaknesses": {"value": "1.\tAlthough the paper claims that the generation and recognition tasks are trained within a unified framework, it remains unclear whether a single set of model weights performs both tasks or if two separate models are trained under a shared architecture. This ambiguity makes it difficult to assess the extent to which the proposed approach is genuinely unified rather than architecturally aligned.\nIf a single set of model weights is indeed shared, the paper does not specify the training schedule or task-balancing strategy—for instance, whether the model is trained sequentially (generation first, then recognition) or jointly with a particular ratio or alternation scheme. Moreover, there is a lack of ablation studies to substantiate the claimed complementarity between the two tasks. It would be valuable to compare a model trained solely on generation with one trained jointly on both generation and recognition tasks to verify that the dual-task setup yields mutual benefits rather than interference. Conversely, if two separate models are trained under a shared architecture, the framework can hardly be regarded as truly “unified.” In this case, the recognition capability appears limited compared with dedicated recognizers, as it produces only standard-font glyph renderings rather than textual outputs. This raises concerns about whether the proposed unification provides substantive advantages beyond a shared architectural backbone.\n\n2.\tBased on the visualizations presented in the paper, it appears that all examples show only a single column of five calligraphy characters. This limited demonstration raises concerns about the model’s claimed page-level generation capability and calls into question whether it can truly produce complete, multi-column calligraphic works.\n\n3.\tThe main text lacks a detailed description of how style conditions are incorporated. For a task that involves generating stylized calligraphy, precise control over style conditioning is crucial. From the appendix, it appears that style is introduced via text prompts rather than style reference images, which is not clearly stated in the main paper. This omission may be perceived as an important detail being underemphasized. Furthermore, this task formulation significantly limits the model’s generalization, as it cannot readily handle unseen style reference images; in essence, the model is constrained to generating fonts for which style labels are pre-defined, reducing its practical applicability in open-ended style transfer scenarios."}, "questions": {"value": "1.\tAs noted in the Weaknesses section, it remains unclear whether a single set of model weights is used for both generation and recognition tasks, or if two separate models are trained under a shared architecture. Additional details about the training strategy and task interaction (e.g., sequential vs. joint training) would be helpful.\n\n2.\tCould the authors provide an example of a text prompt used to specify style conditions? Clarifying this would help understand how style control is implemented in the model. \n\n3.\tFor the recognition task, if the model only generates glyph images rather than textual outputs, how is the character-level recognition accuracy in Table 2 computed? Are any external OCR tools used to convert generated glyphs into text for evaluation? Moreover, the models compared in Table 2 appear to be evaluated without task-specific training on this dataset, whereas the proposed model is specifically trained on it. This discrepancy makes the recognition comparison potentially unfair and may overstate the relative performance of the proposed approach. It would be helpful to compare the recognition performance of the proposed model with a model specifically trained on the same dataset.\n\n4.\tThe experiments on ancient scripts appear limited. For example, in Table 5, only OracleNet is used for comparison, and the recognition performance is worse than OracleNet. It would be informative to include comparisons of generative quality against other models, or to evaluate the effect of fine-tuning different diffusion backbones on generation performance"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MPlBYZvACc", "forum": "OSIPdrw56X", "replyto": "OSIPdrw56X", "signatures": ["ICLR.cc/2026/Conference/Submission2963/Reviewer_g4eJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2963/Reviewer_g4eJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916458438, "cdate": 1761916458438, "tmdate": 1762916463559, "mdate": 1762916463559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}