{"id": "YeEEpPeN1R", "number": 23549, "cdate": 1758345313686, "mdate": 1759896809257, "content": {"title": "Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning", "abstract": "Large Language Models (LLMs) have demonstrated significant improvements in reasoning capabilities through supervised fine-tuning and reinforcement learning. However, when training reasoning models, these approaches are primarily applicable to specific domains such as mathematics and programming, which imposes fundamental constraints on the breadth and scalability of training data. In contrast, continual pretraining (CPT) offers the advantage of not requiring task-specific signals. Nevertheless, how to effectively synthesize training data for reasoning and how such data affect a wide range of domains remain largely unexplored. This study provides a detailed evaluation of Reasoning CPT, a form of CPT that uses synthetic data to generate the hidden thought processes underlying texts, based on the premise that texts are the result of the author's thinking process. Our analysis shows that Reasoning CPT can significantly enhance reasoning ability even when trained on non-STEM corpora that have rarely been used for reasoning tasks. On both MMLU and GPQA, Reasoning CPT achieved substantial improvements over the base model and standard CPT. For instance, on GPQA Diamond, performance improved from 23.7\\% with the base model to 32.8\\% with Reasoning CPT, while on MMLU the benefits became more pronounced as problem difficulty increased, with gains of up to 11.2 points on the hardest questions. Most notably, models trained with hidden thoughts from legal texts outperformed models trained with standard CPT on STEM data, strongly suggesting that reasoning abilities can be enhanced not only from STEM corpora but also from diverse domains, opening a new direction beyond the conventional STEM-centric paradigm of reasoning model training.", "tldr": "We evaluate how reconstructing implicit reasoning processes behind expert texts during continual pretraining enhances LLM reasoning capabilities across STEM and legal domains.", "keywords": ["reasoning", "llm", "synthetic data", "chain-of-thought", "pretraining", "test time computing", "evaluation", "analysis"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/65f41e3936463fb5bc2b24d5b6aa88466f35dbe9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Reasoning Continual Pretraining (**Reasoning CPT**), a method that enhances LLM reasoning by generating and incorporating hidden thoughts—synthetic representations of the implicit reasoning behind texts—into continual pretraining. Unlike SFT or RL approaches requiring task-specific supervision, Reasoning CPT leverages domain-general synthetic data to improve reasoning ability across diverse fields.\n\nExperiments on MMLU and GPQA show consistent gains over standard CPT, with notable improvements on harder problems and strong cross-domain transfer: models trained on legal texts even outperform those trained on STEM data. These results suggest that reasoning skills can emerge from learning implicit cognitive processes, offering a scalable, domain-agnostic alternative to traditional reasoning model training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces the concept of **hidden thought trajectories** into continual pretraining, differing from RL and SFT approaches that rely on explicit reward signals. It proposes a **domain-agnostic mechanism** for generating reasoning data without human annotation or reward supervision, making the method broadly applicable across domains.\n\n2. The study demonstrates that **reasoning ability can transfer from non-STEM texts**, such as legal documents, to STEM tasks. The experiments and analyses are thorough and well-supported, providing **theoretical insight** into how reasoning emerges in large language models."}, "weaknesses": {"value": "1. The concept of **hidden thoughts** is described vaguely, lacking a formal definition and relying on heuristic prompt design rather than a principled formulation.\n2. The **generation model (Gemma2-9B-it)** may limit quality, potentially introducing noise or fictitious reasoning traces that are not analyzed.\n3. Evaluation is restricted to **MMLU and GPQA**, without testing other reasoning styles (e.g., CoT, multi-hop QA, code reasoning), and the paper does not isolate the effect of different hidden-thought components.\n4. The study lacks direct comparison with SFT or post-training baselines, making the use of CPT as the only baseline relatively weak for assessing reasoning improvements."}, "questions": {"value": "1. (Lines 078–083): Why are the proposed “hidden thoughts” considered *hidden* if they are explicitly generated as token sequences? How does this concept differ from existing definitions of latent or hidden rationales?\n2. How do you ensure the quality and reliability of the hidden thoughts generated by **Gemma2-9B-it**, given its limited reasoning capability?\n3. (Section 2.2): Have you compared Reasoning CPT with CPT using existing post-training datasets? How sensitive are the results to the choice of generator and sequence-length truncation?\n4. (Lines 222–227): Since `<start_of_thought>` and `<end_of_thought>` are treated as normal tokens, how is reasoning completeness ensured during inference? Do you require special prompting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ah6kFcueQY", "forum": "YeEEpPeN1R", "replyto": "YeEEpPeN1R", "signatures": ["ICLR.cc/2026/Conference/Submission23549/Reviewer_mtaB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23549/Reviewer_mtaB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637027673, "cdate": 1761637027673, "tmdate": 1762942708053, "mdate": 1762942708053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work explores continual pre-training language models on documents enhanced by another language models. The enhancement is to generate \"hidden thoughts\" for the document, basically asking the LM to explain what essential knowledge is needed to write the document, other ways this document could've been written, validating the answers in the document etc. The authors focus on two subdomains for sourcing more pretraining data, Law (from the Pile) and STEM (from OpenWebMath) and use Gemma2-9B-it to create the hidden thoughts while training Gemma2-9B and Qwen2.5-7B. They find that by creating these hidden thoughts and prepending them to the document, you can continually pretrain these models and have them perform better on MMLU and GPQA over the base model and continual pretraining over just the documents themselves.\n\nThe authors further show that this is not a benefit from more tokens, and that a lot of the benefit comes from being able to solve challenging questions within their evaluation datasets (ones that require significant reasoning), and that a key feature for effective \"hidden thoughts\" comes from the length of the document (at least for the law domain).  The results also suggest some interesting cross-domain transfer, i.e. training on Law data transfers well to GPQA which requires math and science reasoning.  All of this results in a method that can be used to synthetically enhance existing pretraining datasets, encouraging the models to reason more effectively.\n\nOverall, I liked the work! I think this seems pretty easy to implement with nice findings and is well written."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **The results are quite promising.** The hidden thoughts on the Law domain seem to really aid the downstream performance in somewhat surprising ways, i.e., enhancing the performance on GPQA for Qwen2.5-7B\n- **Good reproducability** The process is very straightforward and well documented for other researchers/practitioners to use and experiment on.\n- **Very clear, nice figures & tables, and well written.** The tables and figures present clear results and the paper itself is structured nicely (good flow)."}, "weaknesses": {"value": "- **Weak analysis**, more time could be spent on figuring out where these gains are coming from. For example, \"To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning\" is a paper that shows most of the gain on MMLU comes from questions that are math-heavy (they find it by looking for \"=\" in the generated responses because math questions are sometimes placed into unassuming categories like \"business\").  I would be interested in seeing if some of the cross-domain transfer we see for MMLU and GPQA for that matter are derived from this (math questions in seemingly unrelated math domains).  Additionally, the prompt used to generate hidden thoughts seems based off of what I typically see in \"R1\" style reasoning. I wonder if this format (alternative methods / verify / etc.) is just well suited for math topics and hasn't actually trained the model to \"reason\" on other domains where this wouldn't help.  I like the length experiment, but I would like to see more qualitative analysis and thought put into what these hidden thoughts are really teaching the model and how far we think it can extend. \n- **Narrow scope**, a bit related to my previous remark, law is often tightly coupled with \"logic\" and STEM is often tied with math, the intro kind of makes broad statements about how this could be applied to other domains we typically do not see reasoning working well in, but the domains evaluated and trained on are just that.  (Again MMLU is a bit nuanced because of the findings in that previous paper).  I would like to see evaluations OR training on domains that are less math and logic OR maybe an argument why STEM and Law paired with MMLU and GPQA are fine to generalize from (but I think this will be hard to make convincing, GPQA is also very very math heavy when you look at it).\n\nOne other paper that may be worth mentioning in the related work is \"RATIONALYST: Mining Implicit Rationales for Process Supervision of Reasoning\" by Jiang et al. they also \"mine\" thoughts from pretraining datasets (they do a slightly different training based on that data, but the mining of \"thoughts\" from pretraining data reminded me a lot of this work)."}, "questions": {"value": "See the weakness points, I inlined many questions there. I think it boils down to the generalizability of the method and findings. For example:\n- Are we sure the gains we see in MMLU that are \"cross-domain\" are actually not math questions in disguise \n- What would happen if you selected random pretraining documents to enhance with \"hidden thoughts\" or if you merged law and stem? (the answer to this is kind of telling for general practice, it tells us if we need to enhance subsets of our datasets or if we could just enhance any and every document we come across).\n- What do the hidden thoughts look like for Law / STEM trained models when they are answering a question that is about \"Humanities\" in MMLU? Some analysis / statistics on the surface patterns for these could help argue for generalization (like are \"non-math\" questions in MMLU typically producing shorter hidden thoughts, less helpful hidden thoughts, etc.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s17QarFZCw", "forum": "YeEEpPeN1R", "replyto": "YeEEpPeN1R", "signatures": ["ICLR.cc/2026/Conference/Submission23549/Reviewer_kkHq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23549/Reviewer_kkHq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645384945, "cdate": 1761645384945, "tmdate": 1762942707696, "mdate": 1762942707696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose reasoning continual pre-training (Reasoning CPT), a form of CPT that uses synthetic data to generate the hidden thought processes underlying texts, based on the premise that texts are the result of the author’s thinking process. Analysis shows that Reasoning CPT can significantly enhance reasoning ability even when trained on non-STEM corpora that have rarely been used for reasoning tasks. On both MMLU and GPQA, Reasoning CPT achieved substantial improvements over the base model and standard CPT."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The hidden thoughts are effective for reasoning tasks.\n2. The mechanism behind hidden thoughts are transferable."}, "weaknesses": {"value": "1. The ``hidden thoughts'' is similar to generating chain-of-thought or slow thinking process for a piece of pre-training text.\n2. Insufficient experiments.\n3. The analyses of synthetic data provide neither in-depth explanation for the proposed method nor useful insights for future directions.\n\ndetails can refer to the questions below."}, "questions": {"value": "1. What are the differences among chain-of-thought, slow thinking process in o1 or deepseek-r1, and your proposed hidden thought? I know hidden thought is for pre-training text, but I do not think they have fundamental differences. If they are similar, then hidden thoughts is not novel. If they are not similar, then you should have comparisons among them.\n2. Reasoning CPT works in both Law and STEM domains, and it is transferable, why not combine data in Law and STEM for experiments? I should have further improvement.\n3. This paper focus on continual pre-training, while the data size is small. Moreover, the settings of CPT have some problems such as ``6 epoches'' and \"learning rate of 3e-5\". Does pre-training requires multiple runs of training? If they are the consensus in CPT research area, then I would take back this question.\n4. What is the purpose to conduct the analyses in section 4? I think analyses should give in-depth investigations or explanation to prove the effectiveness of your method, or give insights for future diorections."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S6h3UKcGM8", "forum": "YeEEpPeN1R", "replyto": "YeEEpPeN1R", "signatures": ["ICLR.cc/2026/Conference/Submission23549/Reviewer_pwXB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23549/Reviewer_pwXB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902157311, "cdate": 1761902157311, "tmdate": 1762942707463, "mdate": 1762942707463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Reasoning Continual Pretraining, a method that enhances LLMs’ reasoning abilities by generating \"hidden thoughts\" (implicit expert thinking processes) behind texts to construct synthetic training data, and evaluates it across STEM and Law domains. It uses Gemma2-9B and Qwen2.5-7B as base models, comparing Reasoning CPT with base models and standard CPT on MMLU and GPQA benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The detailed analysis of hidden thought characteristics (e.g., length correlation with original texts, domain variance) offers valuable insights into synthetic data construction for reasoning pretraining.\n\naddresses a critical limitation of existing LLM reasoning training (over-reliance on task-specific signals) by validating non-STEM data’s effectiveness, providing a novel direction for data selection"}, "weaknesses": {"value": "- The abstract and introduction lack logical coherence. For instance, there is an abrupt transition from reasoning models to CPT in the abstract, with insufficient contextual connection to justify this shift.\n\n - No comparisons are made with other related state-of-the-art works. This omission prevents a clear demonstration of the proposed method’s advantages over existing approaches.\n\n- The proposed method bears significant similarities to knowledge distillation directly from stronger reasoning models. Clarifying the essential differences between them is critical, and comparative performance evaluations against such distillation-based methods are indispensable to validate its uniqueness.\n\n- The study does not conduct in-depth exploration of potential biases inherent in synthetic hidden thoughts, for example, domain-specific biases inherited from base models like Gemma2-9B-it. Such unaddressed biases may restrict the method’s practical applicability in fair and unbiased reasoning scenarios.\n\n- The research is limited to only two domains (STEM and Law) without extending to other fields. As a result, it fails to fully corroborate the claimed cross-domain scalability of the proposed method."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FjhHjJIuDI", "forum": "YeEEpPeN1R", "replyto": "YeEEpPeN1R", "signatures": ["ICLR.cc/2026/Conference/Submission23549/Reviewer_nb3E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23549/Reviewer_nb3E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927201181, "cdate": 1761927201181, "tmdate": 1762942707179, "mdate": 1762942707179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}