{"id": "lsLb0WvlqA", "number": 18957, "cdate": 1758292341569, "mdate": 1759897070779, "content": {"title": "Efficient Attention via Pre-Scoring: Prioritizing Informative Keys in Transformers", "abstract": "Recent advances in transformer architectures deeply enhanced long-context language modeling. Among them, HyperAttention achieves competitive efficiency by combining a single-level LSH-based clustering with uniform residual sampling. However, HyperAttention fails to find all significant keys, which in turn raises the overall perplexity. We propose a pre-scoring mechanism that prioritizes significant keys before applying HyperAttention. We introduce three scoring methods: $k$-means and kernel $k$-means clustering, $k$-median clustering, and leverage score-based ranking (inspired by LevAttention) to filter keys effectively. We further replace HyperAttention's original uniform residual sampling, relying exclusively on our pre-scoring mechanism. Experiments on ChatGLM2 (131k token context) reduce perplexity from 12 to 8.3, which outperforms standard HyperAttention. Moreover, when running on the Vision-Transformer (ViT), our method shows that it can guarantee similar accuracy compared with LevAttention, and will surpass LevAttention given specific parameters. Although this method introduces some computational overhead, its combination with HyperAttention remains 20 times faster than FlashAttention, providing a balanced trade-off between speed and modeling accuracy. Our results highlight the effectiveness of integrating pre-scoring into hierarchical attention mechanisms, significantly improving transformer efficiency.", "tldr": "We enhance HyperAttention by using pre-scoring (K-means, K-median, leverage scores) to select important keys before clustering, reducing perplexity from 12 to 8.3 while remaining 20x faster than FlashAttention.", "keywords": ["Transformers", "HyperAttention", "Clustering", "Leverage Scores", "Attention"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ccfb8ab1765f1e6eaf38f018d651e6026a878c94.pdf", "supplementary_material": "/attachment/1c87aba5b1f312d858f740763227e07ff63c0f75.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes an extension of HyperAttention by introducing a pre-selection step using clustering methods (notably k-means) to prioritize informative keys before applying the LSH-based HyperAttention mechanism. The authors provide both theoretical analysis (via a planted-subspace model) and empirical results on long-context LLMs (ChatGLM2/3) and Vision Transformers."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The planted-subspace model provides a useful lens for analyzing why clustering can recover heavy keys, giving the method some analytical backing\n- The idea of preselecting the tokens is intuititive"}, "weaknesses": {"value": "- **Missing prior work:** The paper does not adequately discuss **Routing Transformer[1]**, which also introduced clustering for preselection of tokens. Furthermore, due to conceptual similarities, routing transformer should be one of the compared baselines. Moreover, apart from k-means clustering, **MoSA[2]** recently demonstrated the benefits of expert-choice routing for token preselection, and this should at least be discussed.\n- **Algorithmic ambiguity:** The training procedure for k-means clustering is underspecified‚Äîdoes it employ EMA updates with top-s selection, or is clustering recomputed per step? This is important for reproducibility.\n- **Autoregressivity concern:** The selection procedure relies on a **top-s operator**, which is inherently non-autoregressive (requiring access to future tokens). The implications for causal language modeling are not addressed.\n- **Formatting issues:** Several citations are incorrectly formatted (missing parentheses), which detracts from the paper‚Äôs polish.\n- **Weak gains over LevAttention:** The results do not demonstrate a significant gain over LevAttention baseline.\n- **Convoluted writing:** The writing is often hard to follow, paragraphs seem disconnected, and it is hard to merge them into a cohesive narrative.\n\n[1] - Efficient Content-Based Sparse Attention with Routing Transformers\n[2] - Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing"}, "questions": {"value": "- Why not **pre-select queries as well**, as done in Routing Transformer?\n- Why are **HyperAttention baseline results missing** from Table 2? Without them, it is difficult to measure the incremental gain from pre-scoring.\n- Under what conditions does the proposed method **outperform LevAttention**, given that LevAttention appears faster and in some cases more accurate?\n- How is **k-means training implemented**‚Äîdoes it rely on EMA updates, online clustering, or recomputation per batch?\n- How would the proposed method behave in a **fully autoregressive training regime**, where future tokens are not accessible for top-s selection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "kAJWv5qYag", "forum": "lsLb0WvlqA", "replyto": "lsLb0WvlqA", "signatures": ["ICLR.cc/2026/Conference/Submission18957/Reviewer_m6zB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18957/Reviewer_m6zB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18957/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659728324, "cdate": 1761659728324, "tmdate": 1762931010285, "mdate": 1762931010285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes three methods to score keys before HyperAttention, enabling it to identify important keys: k-means, k-median, and leverage-score ranking. It then feeds the selected keys into HyperAttention, replacing its uniform residual sampling. Experiments on GLM2 show that the approach can be faster than FlashAttention. On ViT, the pre-scoring step captures heavy attention entries as well as, or better than, leverage scores."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Clear and practical idea: The paper provides a straightforward approach to enhance HyperAttention by pre-scoring and then attending. This directly addresses a known issue: HyperAttention‚Äôs hashing is not aware of which keys matter, and LevAttention‚Äôs ‚Äúuniversal set‚Äù can get large. The bridge between them is simple and useful in practice.\n\n+ Mix of theory and experiments: The paper offers proofs under a standard planted-subspace setup (to argue why the pre-scoring should work) and shows results on GLM2/GLM3 and ViT."}, "weaknesses": {"value": "- Reason for PPL improvement: The best perplexity (~8.31) happens when pre-scoring is off (top-k = 0, sample_size = 0) and min_seq_len ‚â• n_query is set. The paper itself says this gain comes from that configuration (forcing the faster block/tiled path), not from pre-scoring. A clean ablation is needed to separate the effects. \n\n- Unclear speedup claims: \n> Compared to the original HyperAttention, these methods can generate a mild acceleration, with performance becoming more remarkable starting at $2^{13}$ with a speedup factor of around 3 to 4 in Figure 1.\n\n&nbsp;&nbsp;&nbsp;&nbsp; The abstract says ‚Äúup to 20√ó faster than FlashAttention‚Äù (when combined with HyperAttention), but the text says around 3‚Äì4√ó at $2^{13}$ for the pre-scored variants, and often the reported gains are relative to HyperAttention. Since HyperAttention is not the paper‚Äôs main contribution, this framing can be misleading. Please clarify the exact conditions for 20√ó vs the 3‚Äì4√ó cases and state whether the 3‚Äì4√ó is typical across settings.\n\n- Narrow baseline: Most comparisons are to HyperAttention and LevAttention. Adding Performer, Reformer, and newer retrieval/streaming methods would make the evaluation more complete and show the accuracy‚Äìspeed trade-offs more clearly.\n\n- Pre-scoring overhead: k-means and k-median add non-trivial compute and can increase memory traffic, which may shrink speed gains, especially in multi-head settings. Please verify this with profiling across many heads, different head dimensions, and batch sizes, and report how much overhead comes from the forward pass vs backward.\n\n- Limited tasks and metrics: Using broader long-context tasks (e.g., QA, retrieval, summarization) and reporting task-level metrics (not just perplexity) would strengthen the paper and validate the method more fully.\n\n### Minor\n- Figure legend and axis text are small and hard to read."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vtTVYKG2uq", "forum": "lsLb0WvlqA", "replyto": "lsLb0WvlqA", "signatures": ["ICLR.cc/2026/Conference/Submission18957/Reviewer_EPNi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18957/Reviewer_EPNi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18957/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761715583548, "cdate": 1761715583548, "tmdate": 1762931009457, "mdate": 1762931009457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an attention acceleration scheme that pre-scores keys before applying HyperAttention. The pre-scoring can be done via k-means or k-median clustering, or via approximate leverage scores. The retained set of ‚Äúinformative‚Äù keys is then fed to HyperAttention, replacing its uniform residual sampling. Empirically, on ChatGLM2 and ChatGLM3 with LongBench prompts, the method lowers perplexity relative to vanilla HyperAttention and, at certain top-k settings, reports a best PPL near 8.3 from a HyperAttention baseline of roughly 12. It also reports layer-level speedups over FlashAttention for sufficiently long sequences and applies a similar key selection idea to ViT, showing accuracy approaching softmax attention when sampling enough keys. Theoretically, the paper analyzes a planted-subspace model and proves that clustering with  ùëò=ùëë+1 separates ‚Äúsignal‚Äù from ‚Äúnoise‚Äù rows comparably to leverage-score selection, giving recovery guarantees of heavy keys under assumptions like row-norm regularity"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Targeting the recall gap of HyperAttention by ranking keys beforehand is a clean, practical idea that directly addresses missed heavy scores. The algorithms are presented with simple wrappers over HyperAttention. \n2.The planted-subspace analysis and Theorems 1‚Äì2 formalize when clustering isolates heavy keys, matching the empirical intuition that important keys align with near-orthogonal directions. \n3. Results span LongBench perplexity on GLM2 and GLM3, speed comparisons vs FlashAttention, and a ViT ‚Äúmonkey-patch,‚Äù giving a multi-angle view of trade-offs. \n4. The paper breaks down where overhead appears, how it scales with  k and  d, and when speedups emerge, which is valuable for deployment decisions."}, "weaknesses": {"value": "1. The strongest PPL ‚âà 8.3 appears tied to the min_seq_len ‚â• n_query configuration and sometimes even top-k set to zero, which partially credits an optimization switch rather than the proposed pre-scoring itself. The paper should isolate gains from pre-scoring vs implementation flags and report both. \n2.Speedups are reported per layer against FlashAttention and discussed asymptotically, but it is unclear how these translate to whole-model throughput and latency under realistic batch sizes and sequence distributions. Consolidated end-to-end metrics are needed. \n3. The paper notes a ‚Äúcorrected coupling‚Äù for GLM3 that changes behavior relative to GLM2. This suggests results are sensitive to integration choices. The exact coupling and ablations should be elevated from appendix to main text with code pointers. \n4. The baseline set focuses on HyperAttention, FlashAttention, and leverage-based selection. Given recent efficient attention methods, the empirical section would be stronger with a few additional modern query-aware or block-sparse baselines, or at least a rationale for exclusions. \n5. Guarantees rely on row-norm regularity and separability that may not hold uniformly across layers or modalities. Although LayerNorm helps, some layers can exhibit skewed norms and mixed subspaces. Sensitivity analyses to violations of these assumptions would strengthen the claims."}, "questions": {"value": "1. How much of the perplexity gain remains when min_seq_len ‚â• n_query is disabled and the exact same HyperAttention kernels and fallbacks are used for all methods, including top-k=0 settings? Please provide a clean ablation table. \n2. Can you report end-to-end speed, throughput, and memory vs FlashAttention and HyperAttention on GLM2 and GLM3 for realistic prompt length distributions and batch sizes, not just per layer? \n3. How stable are results to the choice of  k, number of clusters, and initialization of k-means or k-median? For example, do different random seeds flip the identity of retained keys and the downstream PPL curve? \n4. Could you quantify the additional FLOPs and memory of pre-scoring at inference time and show how they amortize with increasing sequence length, for each variant? \n5. Beyond ViT, have you tried audio or multimodal encoders where key distributions differ strongly from text? Any failure cases that violate the planted-subspace intuition or row-norm regularity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KeJ61LN1N7", "forum": "lsLb0WvlqA", "replyto": "lsLb0WvlqA", "signatures": ["ICLR.cc/2026/Conference/Submission18957/Reviewer_PL5q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18957/Reviewer_PL5q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18957/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908259888, "cdate": 1761908259888, "tmdate": 1762931008988, "mdate": 1762931008988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new method to efficiently approximate the attention mechanism, with the goal of reducing the computational cost of this operation. The paper heavily relies on previous works LevAttention and HyperAttention, trying to combine the best of both methods. In particular, HyperAttention works by grouping keys and queries into bucket, using locality sensitive hashing (LSH), and then compute the attention only for keys and queries that are in the same bucket. On the other hand, LevAttention selects a subset of the most important keys, and compute the attention over these only.\n\nThis paper proposes to select a subset of the keys, using a clustering algorithm such as k-means or k-median, and then to apply the HyperAttention method on the selected keys. The paper also states some theoretical results about the selection process of the keys with the clustering algorith. Finally, some experimental results are provided, replacing the standard self-attention mechanism with the proposed method in existing models such as the GLM language model or vision transformer (ViT) models. Here, the paper show that the method improve the results of LevAttention or HyperAttention."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper provides some theoretical analysis for the proposed method, but it is hard for me to understand what kind of guarantee it actually provides (see next section)."}, "weaknesses": {"value": "Overall, I have many concerns with the paper.\n\nFirst, I found the paper very hard to read. One of the reason is that the authors assume that the reader are very familiar with previous works LevAttention and HyperAttention. For example, many concepts are not introduced in the paper (\"heavy attention scores\" line 59, \"statistical leverage scores\" line 99, \"polynomial based attention\" line 100, \"positional locality\" line 103, \"planted model\" line 135, etc...). Similarly, the different theorems or assumptions are stated without motivation or explanation, making it hard to understand how these relate to the performance of the method. Similarly, the algorithm proposed in the paper is never stated clearly, relying on previous paper instead. These different factors made it very hard for me to understand the method and theoretical claims.\n\nSecond, the paper mostly discusses LevAttention and HyperAttention as previous work to improve the efficiency of the self-attention. These two works are from 2024 and 2023 respectively, while there exists a wide body of earlier literature addressing this problem, and which are not discussed in the paper. Particularly relevant are the Reformer paper (Kitaev et al, 2020) which also proposes to use LSH to group keys and queries and restrict the self-attention between similar keys and queries, or Routing transformer (Roy et al, 2020) which proposes a similar approach based on k-means clustering.\n\nThird, I found the experimetal results to be unconvincing. The paper only compared the proposed approach to LevAttention and HyperAttention, and not earlier works which led to strong results. Second, the performance of the method seems to be quite poor. For example, on the language modeling experiments, the perplexity obtained with the different approximation techniques considered is above 10, while I believe that the perplexity of the original model is around 6. The performance of the original model should actually be included in Figure 3. Similarly, in Figure 4, the reported results for the considered methods are significantly worse than the original models, showing that the method is probably not useful in practice."}, "questions": {"value": "What is the perplexity of the original GLM 2 language model (Fig. 3)?\n\n**Missing references**\n\nAurko Roy, Mohammad Saffar, Ashish Vaswani, David Grangier. 2020. Efficient Content-Based Sparse Attention with Routing Transformers. \n\nNikita Kitaev, ≈Åukasz Kaiser, Anselm Levskaya. 2020. Reformer: The Efficient Transformer\n\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap. 2019. Compressive Transformers for Long-Range Sequence Modelling\n\nZhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, Hongsheng Li. 2018. Efficient Attention: Attention with Linear Complexities\n\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma. 2020. Linformer: Self-Attention with Linear Complexity\n\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Fran√ßois Fleuret. 2020. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\n\nManzil Zaheer et al. 2020. Big Bird: Transformers for Longer Sequences.\n\nIz Beltagy, Matthew E. Peters, Arman Cohan. 2020. Longformer: The Long-Document Transformer\n\nRewon Child, Scott Gray, Alec Radford, Ilya Sutskever. 2019. Generating Long Sequences with Sparse Transformers\n\nYunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh. 2021. Nystr√∂mformer: A Nystr√∂m-Based Algorithm for Approximating Self-Attention"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uqAo23ywre", "forum": "lsLb0WvlqA", "replyto": "lsLb0WvlqA", "signatures": ["ICLR.cc/2026/Conference/Submission18957/Reviewer_1BS1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18957/Reviewer_1BS1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18957/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932854774, "cdate": 1761932854774, "tmdate": 1762931008457, "mdate": 1762931008457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}