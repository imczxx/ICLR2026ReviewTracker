{"id": "S2Q00li155", "number": 19068, "cdate": 1758293259602, "mdate": 1759897062862, "content": {"title": "Pareto-Conditioned Diffusion Models for Offline Multi-Objective Optimization", "abstract": "Multi-objective optimization (MOO) arises in many real-world applications where trade-offs between competing objectives must be carefully balanced. In the offline setting, where only a static dataset is available, the main challenge is generalizing beyond observed data. We introduce Pareto-Conditioned Diffusion (PCD), a novel framework that formulates offline MOO as a conditional sampling problem. By conditioning directly on desired trade-offs, PCD avoids the need for explicit surrogate models. To effectively explore the Pareto front, PCD employs a reweighting strategy that focuses on high-performing samples and a reference-direction mechanism to guide sampling towards novel, promising regions beyond the training data. Experiments on standard offline MOO benchmarks show that PCD achieves highly competitive performance and, importantly, demonstrates greater consistency across diverse tasks than existing offline MOO approaches.", "tldr": "We propose Pareto-Conditioned Diffusion (PCD), a novel framework for Offline Multi-Objective Optimization", "keywords": ["Multi-Objective Optimization", "Conditional Diffusion Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fda15d9749d39b9d1a9c88ad9f6a7afff51bf86e.pdf", "supplementary_material": "/attachment/864568e7ca6251cf71e72890422ffc5d63fee949.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Pareto-Conditioned Diffusion (PCD) for offline multi-objective optimization (MOO). Instead of learning explicit surrogates and optimizing them with MOEAs, PCD trains a conditional diffusion model over designs conditioned on objective vectors. Two key ingredients: (i) a multi-objective reweighting scheme that emphasizes bins with high-quality nondominated solutions; and (ii) a reference-direction conditioning set (NSGA-III-inspired with Riesz s-energy vectors + extrapolation + Gaussian jitter) to generate diverse target trade-offs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The dominance-aware bin reweighting and NSGA-III-style conditioning looks intuitive and principled."}, "weaknesses": {"value": "1. > Unlike prior methods (Xue et al., 2024; Yuan et al., 2025) that rely on complex, multi-stage pipelines of surrogate predictors\nor separate optimization algorithms, PCD offers a more direct, end-to-end framework.\n\nIn my view, ParetoFlow is classifier guidance while this PCD is classifier-free guidance. I do not think the former is more complicated. Although paretoflow requires two stages: predictor training and generative model training, these are established pipelines for that and thus they should be simple instead of complex.\n\n2. The propose conditioning search looks interesting. Have u compared the uniform vectors from Das-Dennis?\n\n3. It looks weird in MORL that no methods can beat against the offline best? so what is the point of all methods?\n\n4. Have u tried flow models instead of diffusion models? \n\n5. I doubt whether we really need diffusion models for some tasks mentioned here. Diffusion models can be very useful it the data has meaningful patterns to learn like image, text, protein. In this context, some designs are pure math numbers. Do they have meaningful patterns?"}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AvSnFmHDYb", "forum": "S2Q00li155", "replyto": "S2Q00li155", "signatures": ["ICLR.cc/2026/Conference/Submission19068/Reviewer_MLZS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19068/Reviewer_MLZS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761407612694, "cdate": 1761407612694, "tmdate": 1762931095664, "mdate": 1762931095664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Pareto-Conditioned Diffusion models (PCD) for offline multi-objective optimization (MOO), which aims to obtain a group of high-performing (i.e., Pareto optimal) final solutions across multiple objectives using only a fixed offline dataset $ \\mathcal{D} = \\\\{ (\\boldsymbol{x}_i, \\boldsymbol{y}_i) \\\\} _{i=1}^N$, without any online iterative evalution. While prior works mainly focus on modeling $p(\\boldsymbol{y}|\\boldsymbol{x})$ and obtaining the inputs that maximize the model’s output, PCD models $p(\\boldsymbol{x}|\\boldsymbol{y})$ instead via conditioned diffusion models, extending the practice of [1] to the MOO scenarios. Specifically, unlike directly parameterizing the distribution, the authors train the models via binning the objective space $\\mathbb{R}^m$, quantified by the dominance number, and assigning weights to each bin, calculated by considering both the number of points inside the bin and performance based on the dominance number. After training, they generate a diverse set of condition $\\boldsymbol{y}$ and sample for final solutions via classifier-free guidance. Experimental results demonstrate the superiority of PCD over a wide range of surrogate-based methods."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is well-written and easy to follow.\n- The idea of extending forward-surrogate-free (i.e., $\\boldsymbol{y} \\to \\boldsymbol{x}$ instead of $\\boldsymbol{x} \\to \\boldsymbol{y}$) methods to offline MOO is novel. It is still unknown that which of the modelings is better, and the discussion in this paper brings new insights into this field.\n- Compared to recent generative-based methods [2-3], PCD does not rely on forward surrogate or classifier, which may introduce compounding error in both the surrogate and the generative model. Instead, PCD employs classifier-free guidance to sample final solution, which is simpler and shows flexible inference control in diffusion model community.\n\nOverall, I think this paper is of high quality. Once my concerns listed below are addressed, I am willing to adjust my ratings."}, "weaknesses": {"value": "- The multi-objective reweighing scheme is somehow heuristic, mainly regarding the choice of dominance number as ranking metric.\n    - From my perspective, the dominance number is just a quantification of the Pareto dominance. However, in MOO, there are still other metrics to further comprehensively examine the goodness of the solutions, e.g.,  the Pareto dominance together with crowding distance from NSGA-II [4] (also used in [3]) and the scalarization mechanism from MOEA/D [5]. I suggest comparing some of them.\n- Typos:\n    - Not correctly using ``\\citep{}``, in line 53, 292, 761.\n    - line 324: “Baselines.” → “Baseline”"}, "questions": {"value": "- Beyond the sampling procedure delivered in Fig. 5,  can PCD recover the dataset distribution by conditioning on different objective space, like the Fig. 4 in [1]? For example, I suggest you examine the modeling performance of PCD by conditioning on different Pareto front layers in the offline dataset (or the test data provided by [6]) and validating if the model can recover the ground-truth points.\n- How does the hypervolume change as the sampling timesteps promote?\n\nI can understand that adding experiments on all task from [6] is quite expensive during rebuttal. I would be pleased that you conduct additional experiments on some representative tasks.\n\n## References\n\n[1] Diffusion models for black box optimization. ICML 2023.\n\n[2] ParetoFlow: Guided Flows in Multi-Objective Optimization. ICLR’25. \n\n[3] Preference-Guided Diffusion for Multi-Objective Offline Optimization. NeurIPS’25. \n\n[4] A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II. IEEE TEvC, 2002. \n\n[5] MOEA/D: A multiobjective evolutionary algorithm based on decomposition. IEEE TEvC, 2007.\n\n[6] Offline multi-objective optimization. ICML 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lKHVLaqZgm", "forum": "S2Q00li155", "replyto": "S2Q00li155", "signatures": ["ICLR.cc/2026/Conference/Submission19068/Reviewer_QMRg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19068/Reviewer_QMRg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635871131, "cdate": 1761635871131, "tmdate": 1762931095220, "mdate": 1762931095220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Pareto‑Conditioned Diffusion (PCD), an end‑to‑end framework for offline multi‑objective optimization (MOO) that removes explicit surrogate predictors. PCD trains a conditional diffusion model to sample designs x conditioned directly on target objective trade‑offs y. Two key components make this work in the offline regime: (1) a multi‑objective reweighting of the static dataset that emphasizes bins with high‑quality, non‑dominated samples (using dominance numbers and binning) (2) a reference‑direction mechanism to generate a diverse set of conditioning points by assigning offline points to uniformly spread directions and extrapolating slightly beyond the dataset. The model is then sampled with classifier‑free guidance. On Off‑MOO‑Bench, PCD reports competitive HV."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Framing offline MOO as conditional sampling is elegant: a single model generates candidate sets conditioned on desired trade‑offs, sidestepping surrogate‑then‑optimizer pipelines.\n2. Method components are well‑motivated:\n- The reweighting combines dominance‑based quality with bin density, to emphasize promising regions without discarding too much signal\n- The reference‑direction procedure provides diverse, high‑quality conditioning targets for sampling\n3. The paper evaluates across different task families and runs targeted ablations that isolate each module’s effect; Table 2 shows the reference‑direction mechanism and reweighting both help on several tasks.\n4. The authors highlight contradictions with prior works or expectations and give plausible hypotheses. This transparency is commendable."}, "weaknesses": {"value": "1. Computation of dominance numbers. Computing the dominance number o(x) for reweighting is quadratic in dataset size if done naively (Eq. 5). The paper would benefit from complexity notes or empirically evaluating the time cost for sampling with the proposed PCD method.\n2. Benchmarks are mostly m≤3 objectives. It’s unclear how the reference‑direction generation and conditioning scale when m grows (e.g., direction coverage, sampling stability, and the number of conditions L required). A small synthetic study with m>3 would help."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZgElk1F2za", "forum": "S2Q00li155", "replyto": "S2Q00li155", "signatures": ["ICLR.cc/2026/Conference/Submission19068/Reviewer_pgCD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19068/Reviewer_pgCD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761647119097, "cdate": 1761647119097, "tmdate": 1762931094787, "mdate": 1762931094787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on offline multi-objective optimization (MOO) and addresses the core challenge of generalizing beyond observed data in static datasets. The motivation stems from existing offline MOO methods, which either rely on error-prone surrogate models or multi-stage pipelines (e.g., ParetoFlow) and struggle to eliminate dependencies on intermediate guidance mechanisms.\n\nThe proposed core method, **Pareto-Conditioned Diffusion (PCD)**, reframes offline MOO as a conditional sampling problem using diffusion models. It integrates two key components: a multi-objective reweighting strategy (prioritizing samples near the Pareto front) and an NSGA-III-inspired reference-direction mechanism (generating diverse conditioning points).\n\nExperiments on five benchmark categories (synthetic, MORL, real-world, scientific design, MONAS) show PCD achieves competitive HV scores and outperforms baselines (e.g., ParetoFlow) in cross-task consistency with a single set of untuned hyperparameters. Ablation studies validate its reweighting and reference-direction components."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. PCD eliminates the need for explicit surrogate models or scalarization schemes (common in existing methods like MOBO or ParetoFlow), simplifying the optimization pipeline and reducing risks of exploiting surrogate inaccuracies.\n2. The multi-objective reweighting strategy (Equation 6) balances dense bins and high-performance samples, while the reference-direction mechanism (inspired by NSGA-III) ensures diverse, high-quality conditioning points. This design is sound and well-motivated.\n3. PCD maintains consistency across tasks with a single set of hyperparameters, and ablation studies confirm the soundness of its mechanism."}, "weaknesses": {"value": "1. PCD underperforms on some challenging tasks, including MORL (high-dimensional) and MONAS (purely categorical). I expect more discussion and guidance for future research. For example, are there targeted modifications (e.g., dimensionality reduction, specialized denoiser architectures) that might address this?\n2. The paper explicitly excludes combinatorial tasks (citing needs for specialized denoising) but provides no technical discussion of how PCD could be extended to this subset of MOO.\n3. The reference-direction mechanism involves extrapolating points along direction vectors and adding Gaussian noise. Please provide more intuition for key choices (e.g., extrapolation distance, noise variance).\n4. The sensitivity analysis (A.2) shows τ must be adjusted for datasets with high quality variance (e.g., ZDT2). How can practitioners **systematically estimate dataset variance** (e.g., via dominance number distribution) and select it without extensive tuning?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Bo3E2FrA9H", "forum": "S2Q00li155", "replyto": "S2Q00li155", "signatures": ["ICLR.cc/2026/Conference/Submission19068/Reviewer_mkio"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19068/Reviewer_mkio"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762165880585, "cdate": 1762165880585, "tmdate": 1762931093792, "mdate": 1762931093792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}