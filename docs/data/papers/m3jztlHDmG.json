{"id": "m3jztlHDmG", "number": 20066, "cdate": 1758302046519, "mdate": 1759897003572, "content": {"title": "Mixture of Cognitive Reasoners: Modular Reasoning with Brain-Like Specialization", "abstract": "Human cognitive behavior arises from the interaction of specialized brain networks dedicated to distinct functions, such as language, logic, and social reasoning. Inspired by this organization, we propose Mixture of Cognitive Reasoners (MiCRo): a modular, transformer-based architecture post-trained with a curriculum that induces functional specialization across experts. Concretely, we partition the layers of a pretrained language model into four expert modules aligned with well-studied cognitive networks in the human brain. MiCRo offers three key advantages over standard language models. (1) The specialized experts are interpretable and causally meaningful---ablating a module causes substantial drops on benchmarks requiring its specialized domain. (2) MiCRo's behavior can be dynamically steered at inference time by routing tokens to particular experts (e.g., favoring social over logical reasoning), enabling fine-grained control over outputs. (3) MiCRo outperforms or matches comparable baselines on both machine-learning reasoning benchmarks (e.g., GSM8K, BBH) and alignment to human behavior (CogBench), while maintaining interpretability. Taken together, cognitively grounded functional specialization yields models that are both more human-like and more human-interpretable.", "tldr": "MiCRo is a brain-inspired modular transformer with interpretable experts that match or exceed baselines and can be steered via expert ablations.", "keywords": ["Mixture of Experts", "Functional Specialization", "Brain-Inspired AI", "Interpretability", "Behavioral Alignment"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/afbe4c6fe9f529f2f2af12549614a12b7b772ab7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Mixture of Cognitive Reasoners (MICRO), a modular transformer architecture inspired by the brain’s functional specialization into distinct cognitive networks (language, logic, social reasoning, world knowledge). The model partitions each transformer block into four “experts” corresponding to these domains, trained through a three-stage curriculum intended to induce brain-like specialization. The authors claim this architecture enhances interpretability, controllability, and alignment with human behavioral benchmarks (COGBENCH), while maintaining competitive reasoning performance on standard NLP tasks (e.g., GSM8K, MATH, MMLU, BBH)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The motivating analogy to cognitive neuroscience is clear and interesting: the authors connect transformer modularity to the brain’s distributed but specialized functional architecture.\n\nThe paper is technically ambitious and proposes a relatively clean experimental pipeline (three-stage training) that is easy to reproduce conceptually.\n\nThe authors provide a comprehensive empirical evaluation, including behavioral alignment metrics, neuroscience “localizers,” and ablation analyses.\n\nThe inclusion of interpretable routing and causal ablation is a meaningful step toward testable hypotheses about functional decomposition in large models."}, "weaknesses": {"value": "1. Conceptual clarity and motivation.\nWhile the analogy to brain modularity is compelling, the paper does not clearly articulate why such modular specialization is desirable in language models. The claimed benefits—interpretability and controllability—are asserted but not demonstrated. The architecture yields mixed performance gains, suggesting that interpretability alone may not justify the added complexity.\n\n2. Neuroscientific grounding.\nThe mapping between the four “expert” modules and the purported brain networks is overly categorical and simplified. The cognitive neuroscience literature remains divided on several of these assumptions. For example:\n- The supposed separation between language and reasoning networks remains debated;\n- The distinction between “logic” and “math” is underdefined, yet the paper treats them as part of a single module.\n- Recent work (e.g., Hope Kean et al.) suggests that logical reasoning may rely on a distinct neural network separate from the multiple-demand system invoked here.\nAs such, the neuroscience framing may be more metaphorical than mechanistic.\n\n3. Data labeling and methodology.\nThe “MICRO_SFT” dataset is central to inducing specialization, but the criteria for domain labeling are unclear. The dataset was pseudo-labeled using O1 and GPT-4o, but there is little evidence that these models’ judgments correspond to meaningful domain boundaries. No human validation or inter-rater reliability is reported. It remains uncertain whether the apparent “specialization” reflects genuine cognitive decomposition or artifacts of the labeling pipeline.\n\n4. Relevance to cognitive neuroscience.\nThe authors seem to suggest (implicitly) that MICRO could bridge AI and brain science, but the connection is speculative. The architecture may be inspired by brain modularity, yet it does not provide new neuroscientific insight—no neural data are modeled, and the alignment tests (using functional localizers) are correlational. While ablation studies show that removing experts affects performance, this is a coarse-grained effect and not obviously interpretable at the cognitive level."}, "questions": {"value": "How do you justify the specific choice of four networks, given the ongoing debates about their boundaries and overlap in the brain?\n\nWhat validation steps were taken to ensure the O1/GPT-4o pseudo-labels correspond to human-like task domains?\n\nHow do you distinguish interpretability (as in mechanistic insight) from mere architectural labeling?\n\nWhat concrete cognitive-neuroscience hypotheses does MICRO make that could be tested empirically?\n\nGiven the small size of the MICROSFT dataset (≈3k examples), how sensitive are your results to its composition or labeling noise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RA8ZPH5FRn", "forum": "m3jztlHDmG", "replyto": "m3jztlHDmG", "signatures": ["ICLR.cc/2026/Conference/Submission20066/Reviewer_D255"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20066/Reviewer_D255"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760823836674, "cdate": 1760823836674, "tmdate": 1762932957573, "mdate": 1762932957573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method (MiCRo) for post-training a transformer to incorporate specialized expert blocks across four domains: language, logic, social, and world knowledge. Inspired by the functional regions observed in brain networks, the approach involves a three-stage training process: (1) duplicating the original transformer blocks to create four expert blocks, each trained on data labeled at the sentence level according to domain; (2) freezing the network parameters and training routers to direct inputs to the appropriate expert blocks; and (3) conducting large-scale supervised fine-tuning. Experimental results demonstrate that, after training, these expert blocks exhibit distinct and specialized functionalities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe proposed method is well-motivated by the four modular functional regions in human brain.\n\n2.\tThe paper is clearly written and easy to follow."}, "weaknesses": {"value": "1.\tThe diversity and scale of tested models can be further enriched. The paper test Llama, SmolLM, and OLMO models of up to 3B parameters. I would recommend testing on 7B scale models from other model families, such as Qwen2.5-7B-Instruct. Small models sometimes behave very different from large models, so experiments on 7B scale models would make the results more convincing.\n\n2.\tThe paper categorizes MMLU as a reasoning benchmark, which is not quite accurate. I would recommend extra experiments on MMLU-Pro (or a subset of it), which contains more reasoning-oriented questions.\n\n3.\tIn Figure 7 and Table 3, the proposed MiCRo architecture does not outperform the dense model baseline on reasoning tasks, even though it has three times more parameters. While MiCRo-Ablation (which shows the best results when up to one expert is ablated) can sometimes achieve better performance than the dense model baseline, it requires four times more compute at test time. Therefore, the improvement of MiCRo on reasoning tasks is still limited."}, "questions": {"value": "1.\tThe paper notes that small MOE models do not show expert specialization like the MiCRo counterpart. Is it due to the limitation of the model scale?\n\n2.\tIf a question is related to more than one field (e.g., a question that is both related to math and social science), what will be the behavior of the MiCRo model? Are there any experiments on this?\n\n3.\tThe paper mentions that using a small amount of data in Stage 1 and 2 suffices to elicit expert specialization behavior and that this specialization remains in the large-scale SFT stage. What is the intuition behind this phenomenon?\n\n4.\tIn some subplots in Figure 4 (e.g., MiCRo-Llama-3B on MMLU_other), ablating any of the logic, social, or world expert can improve performance. It seems weird since it implies all of these experts are detrimental to the task. Are there any explanations on this phenomenon?\n\n5.\tThe meaning of the marks “*” and “ns” in Figure 7 is not clear. One can guess that it shows the significance of the Welch’s t-tests, but it is better to explain them in the caption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9rQbHlOK6C", "forum": "m3jztlHDmG", "replyto": "m3jztlHDmG", "signatures": ["ICLR.cc/2026/Conference/Submission20066/Reviewer_A5jV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20066/Reviewer_A5jV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761629565398, "cdate": 1761629565398, "tmdate": 1762932957143, "mdate": 1762932957143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MICRO (Mixture of Cognitive Reasoners), a novel mixture-of-blocks architecture designed to induce functional specialization across different experts. Unlike conventional MoE models that apply expert routing only to FFN layers, MICRO routes tokens through entire Transformer blocks, leading to emergent domain-specific experts (e.g., language, logic, social reasoning). The work is conceptually inspired by cognitive neuroscience and provides interesting empirical evidence for modularity in large language models."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is thought-provoking and connects cognitive science with model architecture in an elegant way. The results showing emergent specialization patterns and controllable routing behavior are intriguing. Overall, the idea of modeling “cognitive modularity” within Transformers is both fresh and potentially impactful.\n\n- The paper is generally well written, and the experiments are clearly structured. The figures and analyses are helpful for understanding how modularity emerges under the proposed mechanism."}, "weaknesses": {"value": "- While the modular design is inspired by human cognition, the model enforces top-1 routing per layer, meaning each token is processed by only one expert. In contrast, human reasoning typically involves parallel activation and cooperation among multiple brain regions. This exclusive routing assumption may limit the biological and functional plausibility of the approach."}, "questions": {"value": "No."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fb622A8hNQ", "forum": "m3jztlHDmG", "replyto": "m3jztlHDmG", "signatures": ["ICLR.cc/2026/Conference/Submission20066/Reviewer_vTG9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20066/Reviewer_vTG9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728125810, "cdate": 1761728125810, "tmdate": 1762932956608, "mdate": 1762932956608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MICRO (Mixture of Cognitive Reasoners), a modular transformer architecture inspired by the functional specialization of human brain networks for language, logic, and social reasoning. The model partitions layers of a pretrained language model into four expert modules, each aligned with distinct cognitive domains, and is post-trained using a curriculum to promote specialization. MICRO offers three main advantages: (1) its expert modules are interpretable and causally meaningful, (2) it allows dynamic control at inference by routing tokens to domain-specific experts, and (3) it matches or exceeds baseline performance on reasoning benchmarks (e.g., GSM8K, BBH) and human-alignment tasks (CogBench). Overall, MICRO demonstrates that cognitively inspired modularity can enhance both the interpretability and human-likeness of large language models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The novelty of MICRO lies in its cognitively inspired modular design, where a pretrained language model is partitioned into expert modules aligned with human brain networks for language, logic, and social reasoning. This structure enables interpretable, causally meaningful reasoning and dynamic control at inference, while maintaining or improving performance on both reasoning and human-alignment benchmarks.\n\nThe research provides:\n\n•\tInterpretable modules that provide causally meaningful insights into model behavior.\n\n•\tDynamic control at inference, allowing selective routing to domain-specific experts.\n\n•\tStrong empirical performance, matching or exceeding baselines on reasoning and human-alignment benchmarks."}, "weaknesses": {"value": "There are some shortcomings:\n\n•\tIncreased model complexity due to multiple expert modules and modular routing.\n\n•\tPotential scalability issues for very large models or tasks requiring many cognitive domains.\n\n•\tThe work makes a valuable contribution and builds effectively on current advances. However, including a discussion of remaining challenges and possible avenues for future research would strengthen the paper and highlight its long-term potential."}, "questions": {"value": "Please discuss limitations of MICRO and how these limitations may be addressed.\n\nIt is unclear what is going on in Figs 4 and 16. Please explain the figures.\n\nWhat is the key for Figure 5?\n\nPlease discuss the possible impact of this research.\n\nPlease discuss the generalizability of this research."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZjJZ5chOZ7", "forum": "m3jztlHDmG", "replyto": "m3jztlHDmG", "signatures": ["ICLR.cc/2026/Conference/Submission20066/Reviewer_1ukn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20066/Reviewer_1ukn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959930941, "cdate": 1761959930941, "tmdate": 1762932956259, "mdate": 1762932956259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Mixture of Cognitive Reasoners, a modular, transformer-based architecture that built based on neuroscience background with individual language, logic, social, and world experts. The training procedure of the model is composed by the following three stages: Training the experts with a small curated dataset to provide basic inductive bias; Training the router using the same dataset, with the experts frozen; Training the whole model with large instruction-tuning dataset. Experiments shows that the model acquires experts specialization, exhibits alignment to human behaviors, and matches dense and non–brain-aligned modular baselines on several tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and understandable.\n- The neuroscience motivation of the architecture design is very novel.\n- The training procedure is novel.\n- The experimental results are promising, showing that the proposed Mixture of Cognitive Reasoners model lead to strong interpretability and performance gain over other architectures."}, "weaknesses": {"value": "- The language, logic, social, and world experts decomposition might not be optimal."}, "questions": {"value": "I do not have additional questions at this stage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mkKT3s4zTF", "forum": "m3jztlHDmG", "replyto": "m3jztlHDmG", "signatures": ["ICLR.cc/2026/Conference/Submission20066/Reviewer_JAhk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20066/Reviewer_JAhk"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762083144404, "cdate": 1762083144404, "tmdate": 1762932955717, "mdate": 1762932955717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}