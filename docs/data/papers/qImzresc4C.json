{"id": "qImzresc4C", "number": 11907, "cdate": 1758204589850, "mdate": 1759897547292, "content": {"title": "Learning from Peers in Reasoning Models", "abstract": "Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths.\nHowever, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover.\nWe refer to this phenomenon as the *\"Prefix Dominance Trap\"*.\nThis phenomenon indicates that the self-correction ability of LRMs is fragile and can be easily derailed by a poor start.\nThis fragility motivates us to **look beyond internal self-correction**.\nInspired by psychological findings that peer interaction can promote correction ability without negatively impacting already accurate individuals, we propose **Learning from Peers** (LeaP) to address this phenomenon.\nLeaP enables reasoning paths to periodically (every T tokens) summarize and share intermediate reasoning via a routing mechanism, thereby incorporating peer insights.\nFor smaller models that may inefficiently follow summarization and reflection instructions, we introduce fine-tuned **LeaP-T** models.\nExperiments on benchmarks including AIME 2024, AIME 2025, AIMO 2025, and GPQA Diamond demonstrate substantial improvements with LeaP.\nFor example, QwQ-32B with LeaP achieves nearly 5 absolute points higher than its baseline on average and surpasses DeepSeek-R1-671B on three math benchmarks by an average of 3.3 points.\nThe benefits of LeaP also generalize to other domains, such as logic puzzles on the ZebraLogic benchmark.\nNotably, our fine-tuned LeaP-T-7B matches the performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024.\nIn-depth analysis reveals that LeaP provides robust error correction through timely peer insights and exhibits strong error tolerance.\nCode will be open-sourced.", "tldr": "To fix the \"Prefix Dominance Trap\" where a bad start derails LRM reasoning, our LeaP method lets parallel reasoning paths share summaries, significantly improving error correction and allowing models to outperform even larger counterparts.", "keywords": ["Large Language Models", "Large Reasoning Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3e3734205d34fa9d5ba9fc037313aa70f61d622a.pdf", "supplementary_material": "/attachment/cee928c60ec7fa25d9a5ac5d1a2cbf4b7accbf59.zip"}, "replies": [{"content": {"summary": {"value": "The paper “Learning from Peers in Reasoning Models (LeaP)” introduces a collaborative reasoning framework for large language models (LLMs) that enables multiple reasoning paths to exchange information during inference. The authors show a “Prefix Dominance Trap,” where an early flawed reasoning prefix can significantly reduce accuracy, and propose LeaP as a solution. In this method, several reasoning paths periodically summarize their intermediate thoughts and share them through a selective routing process, allowing each path to revise its reasoning based on peer input. Experiments across math and reasoning benchmarks, including AIME and GPQA, show that this approach improves accuracy and robustness across models of different sizes, even when some paths start from weak reasoning prefixes"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The topic is timely and relevant, aligning with current research trends in parallel reasoning\n- The paper is clearly written and easy to follow\n- The core idea—allowing reasoning paths to communicate and cross-correct—is intuitive and conceptually appealing"}, "weaknesses": {"value": "I don’t find the empirical comparison entirely fair.\n- Table 1: It presents Pass@1 for independent reasoning, self-correct prompt, and LeaP. However, each LeaP path benefits from multiple peer paths, so there is no well-defined Pass@1 for LeaP. A more reasonable evaluation, in my view, would be: assuming the width of each LeaP inference is N, run M LeaP inferences and M×N independent inferences, and then report Pass@N for a fair comparison.\n- Table 3: \n    - Following from the previous point, I feel Pass@1 is an unfair metric for comparison. If we focus on Cons@32, I don’t see a significant difference between +SFT and +LeaP.\n    - The paper states:\n    > We synthesize responses by applying LeaP to DeepSeek-R1-Distill-Qwen-32B (Guo et al., 2025) and\nfilter suitable responses using a rule-based selection mechanism. We use supervised finetuning to train\nour LeaP-T models, starting from the 1.5B, 7B, and 14B versions of DeepSeek-R1-Distill-Qwen (Guo\net al., 2025).\n\n      Based on this description, LeaP-T is effectively distilled from a 32B model, which is substantially larger than the 1.5B, 7B, and 14B models reported in Table 3. This makes the comparison not fair.\n\n\n- The routing design appears rather ad hoc to me. In particular, the use of Levenshtein distance, a string-editing metric, seems unjustified, as it measures character-level differences without capturing semantic content, making it unclear how it meaningfully reflects the similarity or divergence of reasoning paths."}, "questions": {"value": "see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xDsV0ogGYC", "forum": "qImzresc4C", "replyto": "qImzresc4C", "signatures": ["ICLR.cc/2026/Conference/Submission11907/Reviewer_46eR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11907/Reviewer_46eR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11907/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761769635867, "cdate": 1761769635867, "tmdate": 1762922917154, "mdate": 1762922917154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LeaP, an aggregation method for reasoning over multiple reasoning traces. LeaP, different from simple majority voting, introduces a summarization module that summarizes the first T tokens of each trace (to reduce context) and then each summarized trace conditions on other traces to further continue the generation. Final answer is obtained by majority voting over these traces. Experiments are conducted with R1-distilled and QwQ models on different reasoning benchmarks to show that LeaP outperforms majority voting."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* LeaP introduces a simple method for learning from multiple reasoning traces. Experiments show that this can outperform majority voting.\n\n* The authors conduct some analysis on how often the summarization module should be invoked and how many summaries should be aggregated."}, "weaknesses": {"value": "* The biggest weakness of this paper is its lack of novelty. Reasoning over multiple samples (or meta-reasoning) is an old concept (e.g., see  [1], [2]) and here the only difference seems to be the summarizer module. I also don't see a clear ablation of this summarizer module to understand its usefulness. I also think that the method is not necessarily a multi-agent method because the experiments are limited to trajectories from the same underlying model. Related work also should not be in the appendix which makes it harder to understand the key contributions of the work.\n\n* Some key details of the method as well as of the experimental setup are also missing from the paper. Since benchmarks like AIME are quite small, the paper should report the number of seeds and variance. Second, the aggregation prompt also seems to be missing. Third, assuming each LeaP block corresponds to one round of aggregation, how many rounds of aggregation are done? If it's > 1, I'd like to see a plot of baseline vs LeaP across rounds. \n\n* The experiments are with relatively older models. To better understand the usefulness of the method on stronger models, it should be applied to newer Qwen3 models in both thinking/non-thinking modes at varying sizes. This will shed some light on how this method scales. Concurrent works have moved away from prompting-based aggregation to adopt RL-trained aggregation, casting doubt on the effectiveness of prompting-based aggregation [3].\n\n[1] Learning to Reason Across Parallel Samples for LLM Reasoning. Qi et al. 2025\n\n[2] Answering Questions by Meta-Reasoning over Multiple Chains of Thought. Yoran et al., 2023\n\n[3] The Majority is not always right: RL training for solution aggregation. Zhao et al. 2025"}, "questions": {"value": "* Unless I have missed it, could you perform a clean ablation of the summarization module? For example, compare LeaP with only multiple rounds of aggregation.\n\n* How many rounds/LeaP modules do you actually use? Where can I find a scaling plot?\n\n* How many seeds did you use to conduct the experiments?\n\n* How well does your method work with the stronger Qwen3 models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MgQMBzErNl", "forum": "qImzresc4C", "replyto": "qImzresc4C", "signatures": ["ICLR.cc/2026/Conference/Submission11907/Reviewer_pUaZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11907/Reviewer_pUaZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11907/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890434310, "cdate": 1761890434310, "tmdate": 1762922916766, "mdate": 1762922916766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LeaP introduces a mechanism through which multiple agents (copies of the same model, but with differing prompts) decode thoughts in parallel and exchange messages regularly between them. The authors also trained a specialized model that achieves strong results compared to DeepSeek."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Some of the key strenghts:\n1. The paper points out a widespread phenomena, that the first tokens (prefixes) can strongly influence the results up completely breaking a decoded sentence.\n\n2. The paper introduces a test-time only modifications (with multiple agents), studying the setup under multiple models and benchmarks (AIME 2024/2025, AIMO 2025, GPQA, ZebraLogic)"}, "weaknesses": {"value": "Related to weaknesses, I would call out:\n1. Limited novelty, very similar to ensembling methods. Due to prompting, it seems rather novel, however a single naive baseline with multiple reasoning traces and vote aggregation would probably clarify better the contribution.\n\n2. Compute comparison would be necessary, given that multiple reasoning chains easily consume a lot of inference cost. The fine-tuning comparison does not strongly make the case either, reducing a 14B to 7B model is not particularly impressive.\n\n3. It would be great to understand how the method approace with models fine-tuning."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EMXeCuAVYF", "forum": "qImzresc4C", "replyto": "qImzresc4C", "signatures": ["ICLR.cc/2026/Conference/Submission11907/Reviewer_7yVd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11907/Reviewer_7yVd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11907/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762124496220, "cdate": 1762124496220, "tmdate": 1762922916172, "mdate": 1762922916172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}