{"id": "NuGAhTDYTd", "number": 15682, "cdate": 1758253842171, "mdate": 1759897289317, "content": {"title": "Scalable In-Context Q-Learning", "abstract": "Recent advancements in language models have demonstrated remarkable in-context learning abilities, prompting the exploration of in-context reinforcement learning (ICRL) to extend the promise to decision domains. Due to involving more complex dynamics and temporal correlations, existing ICRL approaches may face challenges in learning from suboptimal trajectories and achieving precise in-context inference. In the paper, we propose **S**calable **I**n-**C**ontext **Q**-**L**earning (**S-ICQL**), an innovative framework that harnesses dynamic programming and world modeling to steer ICRL toward efficient reward maximization and task generalization, while retaining the scalability and stability of supervised pretraining. We design a prompt-based multi-head transformer architecture that simultaneously predicts optimal policies and in-context value functions using separate heads. We pretrain a generalized world model to capture task-relevant information, enabling the construction of a compact prompt that facilitates fast and precise in-context inference. During training, we perform iterative policy improvement by fitting a state value function to an upper-expectile of the Q-function, and distill the in-context value functions into policy extraction using advantage-weighted regression. Extensive experiments across a range of discrete and continuous environments show consistent performance gains over various types of baselines, especially when learning from suboptimal data.", "tldr": "We propose an innovative framework that harnesses dynamic programming and world modeling to steer ICRL toward efficient reward maximization and task generalization, while retaining the scalability and stability of the supervised pretraining paradigm.", "keywords": ["In-context reinforcement learning", "Q-learning", "advantage-weighted regression", "world model"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/79389c812ed501e629d70600650be0d8a778d4be.pdf", "supplementary_material": "/attachment/fd9a0d1b7c43f1e96023262a69d186fc690db3e9.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes scalable in-context Q-learning (S-ICQL) that adapts implicit Q-learning (IQL) to the in-context reinforcement learning (ICRL) domain in the offline setting. S-ICRL pretrains a world model to capture the dynamics of the environments, which S-ICQL leverages to encode the task context into a more compact form. The empirical study across three benchmarks confirms the effectiveness of S-ICQL."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- S-ICQL outperforms the other ICRL baselines consistently.\n- The paper is overall easy to follow.\n- The empirical study looks rigorous and includes the study of S-ICQL under out-of-distribution tasks and suboptimal data."}, "weaknesses": {"value": "- I feel the comparison with other baselines is not entirely fair. For instance, algorithm distillation (AD) is more of an online method than an offline one. It requires a stream of experience generated by a consistently improving policy. In addition, the decision pre-trained transformer (DPT) requires the actions in the dataset to be optimal, which may not be satisfied in this work.\n- I think there is a considerable overhead of pre-training the world model. From what I understand, S-ICQL only uses the encoder $E_\\phi$ during the IQL process, and the dynamics decoder was discarded after pretraining. In addition, unlike pre-training a language model on a very general corpus, here the world model is only fitted on a particular class of tasks. Therefore, when the task class changes, the world model needs to be retrained.\n- Some equations are ill-written. For example, why is there an inner expectation in equation (3)? If the encoder $E_\\phi$ is a function, where does the stochasticity come from? Also, in equation (3), how is the squared norm of the tuple $[r, s]$ defined?\n- In the ablation study, I don't think removing the Q-learning component recovers DPT. Without optimal actions, the model is likely regressing on the offline dataset and will naturally learn a suboptimal policy if the data quality is poor.\n\nMinor concerns:\n- Though not significantly affecting reading, the writing has several typos and grammatical errors.\n- Since S-ICQL is claimed to be an ICRL algorithm, then the meta RL survey by Beck et al. (2025) and the ICRL survey by Moeini et al. (2025) should be cited for a richer context."}, "questions": {"value": "- Why is pretraining a world model necessary for obtaining a good encoder $E_\\phi$? What prevents it from learning the encoder directly during the IQL?\n- If the main benefit of training the encoder is to save computation by projecting the raw transitions onto a lower-dimensional space, then why do the authors choose GRU instead of a simple linear layer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SqBSguENne", "forum": "NuGAhTDYTd", "replyto": "NuGAhTDYTd", "signatures": ["ICLR.cc/2026/Conference/Submission15682/Reviewer_93W6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15682/Reviewer_93W6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708199829, "cdate": 1761708199829, "tmdate": 1762925936940, "mdate": 1762925936940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new In-Context Reinforcement Learning (ICRL) framework that integrates world modeling and dynamic programming, achieving state-of-the-art results under few-shot settings on DarkRoom, MuJoCo, and Meta-World benchmarks.  \nThe world model is used to learn task-relevant representations, which can be utilized by the learned policy.  \nThe dynamic programming component enables the model to derive improved policies from suboptimal supervised datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  The proposed world model effectively captures task-relevant information, providing a more compact and informative representation compared to previous ICRL approaches.\n    \n2.  The integration of  Q-function learning and advantage-weighted regression (AWR) allows the model to optimize its policy from suboptimal data and obtain better-performing behaviors.\n    \n3.  The experimental results are relatively comprehensive, with comparisons to multiple baselines demonstrating consistent improvements."}, "weaknesses": {"value": "1.  The experimental environments are relatively  simple. \n    It would strengthen the work to include results on more challenging or diverse environments."}, "questions": {"value": "1.  After training the world model, only the encoder is used in subsequent stages.  \n    Are there possible ways to further utilize the world model during later training phases?\n    \n2.  The current setting trains on offline datasets and tests for generalization to OOD tasks.  \n    Would incorporating online learning with environment interaction in  in-distribution task improve the generalization performance on OOD tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pJk0ZU0sA5", "forum": "NuGAhTDYTd", "replyto": "NuGAhTDYTd", "signatures": ["ICLR.cc/2026/Conference/Submission15682/Reviewer_qpLp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15682/Reviewer_qpLp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825446616, "cdate": 1761825446616, "tmdate": 1762925936304, "mdate": 1762925936304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes S-ICQL, an in-context RL framework which encodes small chunks of experience into a compact task representation and then uses that task representation for transformer-based policy learning. Separate heads are used for the policy and value functions. Experiments show improvements over a number of ICRL/meta-RL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Good experimental results which consider a number of relevant baselines\n- Ablations show that each of the components contributes meaningfully to performance\n- Stitching section shows improvements over best returns in the dataset"}, "weaknesses": {"value": "- My main issue is that it is very hard to understand the details of the method. Figure 1 is very complicated; there are many arrows going in different directions and it is not obvious what exactly “precise task representation” and “precise lightweight prompt” are. \n- The method is also quite complicated and not particularly novel. It seems to be a combination of a number of existing components (e.g., AWR, world model, transformer-based policies)"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6LiKyd98YV", "forum": "NuGAhTDYTd", "replyto": "NuGAhTDYTd", "signatures": ["ICLR.cc/2026/Conference/Submission15682/Reviewer_gW7J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15682/Reviewer_gW7J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870796265, "cdate": 1761870796265, "tmdate": 1762925935583, "mdate": 1762925935583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes Scalable In-Context Q-Learning (S-ICQL), a novel in-context reinforcement learning (ICRL) framework that harnesses dynamic programming and world modeling to steer ICRL toward efficient reward maximization and task generalization. In particular, S-ICQL incorporating RL objectives into the supervised pretraining paradigm with a transformer architecture that simultaneously predicts optimal policies and in-context value functions. In addition, S-ICQL pretrains a world model to better capture the task information for more efficient in-context inference. The authors prove the effectiveness of S-ICQL through extension experiments on standard ICRL benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The manuscript is well-written and easy-to-follow. \n- The experiments are comprehensive, including an array of strong baselines. \n- The performance of S-ICQL is strong."}, "weaknesses": {"value": "My biggest concern is about the novelty of the proposed framework S-ICQL. While it has many components, all of its components and insights have appeared in prior works. As a consequence, S-ICQL feels like a unification of ideas from prior works. Specifically, there are two core technical contributions, \n- **Section 3.3.** The world modeling is **identical** to [1]. Both the model design and pretraining process are almost identical to the task-embedding model of [1]. \n- **Section 3.4.** S-ICQL also proposes to estimate in-context (action) value functions and then extract the policy with advantage weighted regressions. The same idea has been implemented in [2]. The **only differences** are that (1) S-ICQL use a shared transformer backbone for the policy and value transformers, and (2) S-ICQL uses a different loss function, which is the same loss from IQL[3].\n\n[1] Meta-DT: Offline Meta-RL as Conditional Sequence Modeling with World Model Disentanglement.\n\n[2] In-Context Reinforcement Learning From Suboptimal Historical Data.\n\n[3] Offline Reinforcement Learning with Implicit Q-Learning"}, "questions": {"value": "- Is it necessary to learn the in-context value functions together with the transformer policy? I think if you can first learn the in-context value functions, use the learned values functions to compute the weights for policy extraction, and lastly do the policy extraction. In this way, you don't have a moving target, i.e., the weights for AWR is fixed and this can improve training stability.\n- I think it is also standard to test offline deployment performance [4], that is, performance of the performance policy conditioned on a given trajectories collected by other behavior policies. \n- In Eq (7), there are no weights for the three losses? This is counter-intuitive and can be problematic if the three losses are not of the same scales.\n\n\n[4] Supervised Pretraining Can Learn In-Context Reinforcement Learning"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ABGelSMa3s", "forum": "NuGAhTDYTd", "replyto": "NuGAhTDYTd", "signatures": ["ICLR.cc/2026/Conference/Submission15682/Reviewer_72Wd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15682/Reviewer_72Wd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982352562, "cdate": 1761982352562, "tmdate": 1762925934949, "mdate": 1762925934949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}