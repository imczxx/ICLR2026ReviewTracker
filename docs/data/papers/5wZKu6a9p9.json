{"id": "5wZKu6a9p9", "number": 12689, "cdate": 1758209511741, "mdate": 1759897493710, "content": {"title": "Poisoning the Inner Prediction Logic of Graph Neural Networks for Clean-Label Backdoor Attacks", "abstract": "Graph Neural Networks (GNNs) have achieved remarkable results in various tasks. Recent studies reveal that graph backdoor attacks can poison the GNN model to predict test nodes with triggers attached as the target class. However, apart from injecting triggers to training nodes, these graph backdoor attacks generally require altering the labels of trigger-attached training nodes into the target class, which is impractical in real-world scenarios. In this work, we focus on the clean-label graph backdoor attack, a realistic but understudied topic where training labels are not modifiable.\nAccording to our preliminary analysis, existing graph backdoor attacks generally fail under the clean-label setting. Our further analysis identifies that the core failure of existing methods lies in their inability to poison the prediction logic of GNN models, leading to the triggers being deemed unimportant for prediction. Therefore, we study a novel problem of effective clean-label graph backdoor attacks by poisoning the inner prediction logic of GNN models.\nWe propose \\textbf{\\method} to solve the problem by coordinating a poisoned node selector and a logic-poisoning trigger generator.\nExtensive experiments on real-world datasets demonstrate that our method effectively enhances the attack success rate and surpasses state-of-the-art graph backdoor attack competitors under clean-label settings. \nOur code is available at https://anonymous.4open.science/r/BA-Logic.", "tldr": "A novel approach to poison the inner prediction logic of Graph Neural Networks for conducting effective clean-label graph backdoor attacks.", "keywords": ["Backdoor Attack", "Logic Poisoning", "Graph Neural Networks"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/42bd354ee3ee0618b8a162feb54ab06bf1c6070e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on the problem of clean-label backdoor attacks against Graph Neural Networks (GNNs). Addressing the limitation of existing graph backdoor attacks—where modifying training labels renders them impractical in real-world scenarios—it proposes a novel attack paradigm centered on \"poisoning the inner prediction logic of GNNs\" and designs the BA-LOGIC framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper establishes a solid theoretical framework to unravel the core challenge of clean-label graph backdoor attacks.\n\n2. BA-LOGIC’s two-core component design (uncertainty-guided poisoned node selector + adaptive logic-poisoning trigger generator) addresses key practical challenges of clean-label attacks while ensuring effectiveness, and comprehensive experiments prove the feasibility of BA-LOGIC."}, "weaknesses": {"value": "Lack of adaptive defenses."}, "questions": {"value": "If defenders are aware of the BA-LOGIC attack, what adaptive defense strategies can be employed to mitigate its impact?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xKzJOMATsD", "forum": "5wZKu6a9p9", "replyto": "5wZKu6a9p9", "signatures": ["ICLR.cc/2026/Conference/Submission12689/Reviewer_dzBj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12689/Reviewer_dzBj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761462687353, "cdate": 1761462687353, "tmdate": 1762923526252, "mdate": 1762923526252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel clean-label graph backdoor attack technique, BA-LOGIC, focusing on poisoning the inner prediction logic of Graph Neural Networks (GNNs) without modifying training labels. The work introduces a logic-poisoning trigger generator and advanced poisoned node selection, demonstrating state-of-the-art attack success rate and robustness across datasets, models, and defense methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The clean-label attack formulation addresses a realistic scenario where attackers cannot tamper with ground-truth labels, highlighting a meaningful threat for deployed GNNs.\n- Theoretical analysis and ablations help clarify why prior techniques underperform and justify the proposed logic-poisoning approach."}, "weaknesses": {"value": "- The contribution builds incrementally on established ideas in trigger design and node selection, with technical optimization at the core instead of a radically new threat model.\n- Reliance on surrogate models and full node feature access may limit realistic, black-box or large-scale attack scenarios.\n- Comparative defense analysis lacks depth, especially regarding future defenses specifically countering logic-poisoning."}, "questions": {"value": "1. To what extent does BA-LOGIC’s attack success depend on graph properties such as high heterophily, noisy node features, or severe class imbalance—does performance deteriorate in these challenging settings?\n\n2. Is there experimental evidence that straightforward explainability regularization or gradient masking can successfully defend against the core logic poisoning strategy, and how robust is the method to such countermeasures?\n\n3. How practical is uncertainty-based poisoned node selection when training node labels are incomplete, partially unavailable, or highly noisy in real graph data?\n\n4. Are there defense techniques (beyond edge pruning) that specifically target logic-poisoning triggers, and how detectable is BA-LOGIC under adaptive defenses?\n\n5. Could collaborative or distributed defense strategies—such as multiple GNN models jointly monitoring subgraph behavior—mitigate the risk of clean-label logic-poisoning attacks?\n\n6. How does the time and computational complexity of BA-LOGIC compare to both competing attacks and real-world defense operation requirements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MDgAU5TLHI", "forum": "5wZKu6a9p9", "replyto": "5wZKu6a9p9", "signatures": ["ICLR.cc/2026/Conference/Submission12689/Reviewer_3HND"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12689/Reviewer_3HND"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889073605, "cdate": 1761889073605, "tmdate": 1762923525829, "mdate": 1762923525829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies clean-label backdoor attacks on graph neural networks. The attacker cannot modify labels and must rely on adding small structure or feature triggers to a subset of training nodes or graphs. The authors argue that many prior backdoor attacks transfer poorly to this setting because the model relies on clean neighborhood cues instead of the trigger. The paper proposes a method that selects which samples to poison and trains triggers so that the model’s internal decision logic places high importance on the trigger. Experiments cover several datasets and backbones, with high attack success and small clean accuracy drops."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Writing is clear and easy to understand.\n2. The problem setup is realistic for clean-label constraints. The objective connects well to the intended failure mode.\n3. Broad empirical study across tasks, models, and several defenses. Attack transfer looks strong while keeping clean accuracy high."}, "weaknesses": {"value": "1. The work investigates a new setting but the novelty is limited. Several components resemble prior methods for trigger generation or importance shaping. Please position the method more sharply against the closest graph backdoor baselines and explain what is new at the objective level and at the algorithmic level.\n2. No adaptive defense is proposed. Since the objective pushes importance onto small subgraphs, an adaptive baseline that penalizes gradient concentration or uses randomized trigger-edge masking during training would be informative.\n3. Theoretical assumptions may not hold on real graphs with strong feature and structure correlations."}, "questions": {"value": "1. Could author help me understand what is the key difference between UGBA-C and your method in the clean-label case?\n2. If labels cannot be changed, in what exact sense is this a backdoor rather than poisoning attack? Is there a formal or operational line the paper uses to separate the two?\n3. Why do methods designed for clean-label settings such as ERBA and ECGBA underperform EBA-C, GTA-C, UGBA-C, or DPGBA-C in your tables? Please provide an analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XnOemBAHbp", "forum": "5wZKu6a9p9", "replyto": "5wZKu6a9p9", "signatures": ["ICLR.cc/2026/Conference/Submission12689/Reviewer_A1Cu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12689/Reviewer_A1Cu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917917259, "cdate": 1761917917259, "tmdate": 1762923525432, "mdate": 1762923525432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to poison a training corpus without needing to flip the label for the data with the injected trigger (so-called clean label setting) but using a surrogate model to tune perturbations to training data (injecting the suitable structural changes or trigger sub-graphs) to data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Consider the more challenging setting of clean label settings (this relaxes attacker threat model)\n- Evaluation across node/graph/edge classification tasks with multiple benchmark datasets\n- Evaluation with a number of defences\n- Consider different combinations of surrogate architectures vs. target architectures"}, "weaknesses": {"value": "- Clean label backdoor attacks against Graph models and the idea of generating triggers (sub-graphs) via a surrogate is not new\n- The value of the theorem to the method is unclear\n- Clarify the threat model in the main paper\n- Unclear if input sanitization type defences were considered"}, "questions": {"value": "- How is the theorem related to the poisoning method? I am unclear about the value of this other than what is already shown, i.e the injected trigger is less effective. I would honestly remove this.\n\n- Fig 8 says BA-Logic (proposed) is lower in the scores in 1 and higher in the other - this seems to contract the statements in the paper, what am I missing? Can you show the same results for other datasets and clean models? This seems more important as the thesis for the study is based on the ineffectiveness of other clean label methods as demonstrated in the ITR measure.\n\n- Clarify threat model (attackers exert control over training or simply poison data and publish for use by a victim?)\n\n- Explain why existing defences do not work\n \n- Did the authors include input sanitization type defences? (Would be good to categorise the defences considered to state which focus on inputs, models and outputs to remove/clean/purify models/inputs.\n\nI will re-consider the scores after the rebuttal, generally, I remain positive about the study."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7hFUYNd3wg", "forum": "5wZKu6a9p9", "replyto": "5wZKu6a9p9", "signatures": ["ICLR.cc/2026/Conference/Submission12689/Reviewer_hegp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12689/Reviewer_hegp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762250664136, "cdate": 1762250664136, "tmdate": 1762923524656, "mdate": 1762923524656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}