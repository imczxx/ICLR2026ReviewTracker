{"id": "vVMT9ODgwR", "number": 24837, "cdate": 1758360909334, "mdate": 1763519280842, "content": {"title": "Oh-A-DINO: Understanding and Enhancing Attribute-Level Information in Self-Supervised Object-Centric Representations", "abstract": "Object-centric understanding is fundamental to human vision and required for complex reasoning. Traditional methods define slot-based bottlenecks to learn object properties explicitly, while recent self-supervised vision models like DINO have shown emergent object understanding. We investigate the effectiveness of self-supervised representations from models such as CLIP, DINOv2 and DINOv3, as well as slot-based approaches, for multi-object instance retrieval, where specific objects must be faithfully identified in a scene. This scenario is increasingly relevant as pre-trained representations are deployed in downstream tasks, e.g., retrieval, manipulation, and goal-conditioned policies that demand fine-grained object understanding. Our findings reveal that self-supervised vision models and slot-based representations excel at identifying edge-derived geometry (shape, size) but fail to preserve non-geometric surface-level cues (colour, material, texture), which are critical for disambiguating objects when reasoning about or selecting them in such tasks. We show that learning an auxiliary latent space over segmented patches, where VAE regularisation enforces compact, disentangled object-centric representations, recovers these missing attributes. Augmenting the self-supervised methods with such latents improves retrieval across all attributes, suggesting a promising direction for making self-supervised representations more reliable in downstream tasks that require precise object-level reasoning.", "tldr": "We show that slot-based and pre-trained DINO representations struggle to retrieve object-level attributes in multi-object scenes, and improve them by augmenting DINO with learned object-centric features.”", "keywords": ["object-centric learning", "self-supervised learning", "multi-object instance retrieval", "representation fusion", "fine-tuning", "representation learning", "pre-trained representations", "latent spaces"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/816e0012d9891c99fa525762e0d3e1362ae1dcc8.pdf", "supplementary_material": "/attachment/e2928c406f6f018ea25f10abe5b38ff320973ff6.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates how well self-supervised and object-centric visual representations preserve fine-grained object attributes necessary for distinguishing multiple objects in complex scenes. While large self-supervised models such as DINO, DINOv2, and CLIP exhibit emergent object understanding, the authors find that these representations mainly capture geometric properties (e.g., shape, size) but fail to retain surface-level cues like color, texture, and material.\n\nTo address this limitation, the paper proposes OH-A-DINO (Object-Aware DINO) that augments DINOv2 features with object latent vectors learned from segmented image patches. Experiments on CLEVR, CLEVRTex, and Stanford Cars show that OH-A-DINO improves multi-object instance retrieval, especially in color and material matching, indicating that object-centric latents is a promising direction for improving downstream tasks that require precise \nobject-level understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Novel yet Simple Method.** The most significant contribution of this paper lies in offering a novel perspective on integrating conventional self-supervised features with Object-Centric features. Previously, although Object-Centric models were favored for their characteristics, they were often criticized for their overly simplistic representation (using a few vectors to represent an image), which was considered insufficient for complex scenarios. The approach proposed in this paper, which involves using Object-Centric latent to enhance self-supervised features on the basis of self-supervised models, provides a solution that takes both aspects into account. The proposed OH-A-DINO introduces a clean, modular enhancement that does not require retraining the backbone. It elegantly combines global DINO features with locally learned VAE latents to recover missing attribute-level details.\n\n- **Strong Empirical Results.** OH-A-DINO achieves large improvements in both single- and multi-attribute retrieval accuracy, particularly for color and material cues, and demonstrates consistent performance gains over all baselines."}, "weaknesses": {"value": "- **Concerns about using PCA for segmenting.** According to my understanding, OH-A-DINO is divided into two functions: 1) extracting object masks, and 2) extracting object regions based on the masks, and using VAE to learn local features of each region to enhance global features. My concern lies in the former, that is, why does it use PCA plus threshold setting, a non-deep learning approach, to segment object patches instead of deep learning methods? For instance, since this paper is centered on Object-Centric, why not directly use Object-Centric methods to achieve segmentation? As far as I know, at least on CLEVR and ClevrTex, current Object-Centric methods have achieved nearly perfect segmentation results, which should be more reliable than PCA plus threshold and do not rely on manual parameter tuning. (Although this article mentions that OC models such as Slot Diffusion may lose some attribute information, it should not affect the application of OC models if they are only used to provide masks.)\n\n- **Real world dataset choice.** Although the Stanford Car dataset was adopted as the real-world benchmark in the paper, a main concern is that the images in Stanford Car are all centered on a single vehicle as shown in Figure 5, which seems inconsistent with the \"multi-object instance retrieval\" task claimed in the paper. Using images with multiple objects, such as COCO which is commonly used in OCL, is obviously a better choice. Furthermore, similar to the previous weakness, in complex real-world scenarios like COCO, can the simple segmentation method of PCA effectively segment the approximate masks of objects? If not, would it be feasible to switch to the mainstream object-centric (OC) model for real-world scenarios, such as DINOSAUR + DINOv2? Even further, if we directly use Segment Anything to provide object masks, could this enhance DINOv2 in real-world scenarios to improve its retrieval capabilities?"}, "questions": {"value": "- Self-supervised models like DINO are typically trained based on the semantic consistency after geometric and color transformations of images, for instance, the features of an image after color transformation should be similar to those of the original image. Is this the reason why the models are insensitive to color, material, and texture? \n\n- What causes the Slot-based model to lose object attribute information? Logically speaking, the goal of SlotDiffusion (or other Slot-based model that reconstructs RGB pixels) is to generate the original image, so all the information in the image should be preserved in its slots, and information should not be lost."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7Vi6yIxTDm", "forum": "vVMT9ODgwR", "replyto": "vVMT9ODgwR", "signatures": ["ICLR.cc/2026/Conference/Submission24837/Reviewer_wQom"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24837/Reviewer_wQom"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761788847693, "cdate": 1761788847693, "tmdate": 1762943214967, "mdate": 1762943214967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the attribute-level information encoded in object-centric representations from models such as CLIP, DINO, and SlotDiffusion. The authors find that the object embeddings produced by these models cannot be directly used for multi-object instance retrieval, a task that aims to retrieve objects sharing the same attributes as a given query object using cosine similarity. To address this limitation, the authors propose a two-step approach: first, they apply PCA to DINO features to segment objects in the scene; then, all DINO features of patches belonging to an object are fed into a VAE to learn disentangled features for each patch. The resulting feature is concatenated with the original DINO feature to obtain the final representation. Experimental results show that this enhanced feature improves multi-object instance retrieval performance on CLEVR and CLEVRTex, and captures color information more accurately than CLIP and DINO, as demonstrated on the Stanford Cars dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well written and easy to read."}, "weaknesses": {"value": "- My main concern about this paper is its unreasonable setting and metric: One major argument made in the paper is that, given a query object, cosine similarities on the **raw** representations produced by baseline models cannot retrieve objects with the same attributes such as color, material, or shape. The authors interpret this as evidence that the model fails to preserve these non-geometric, surface-level cues. This interpretation is not accurate: low cosine similarity does not imply that the information is missing from the representation—it may simply be encoded in a way that is not directly reflected in direct pairwise distances. For example, representations produced by slot-based models such as SlotDiffusion can reconstruct the original image with high fidelity, indicating that attribute-level information is well preserved. Moreover, numerous experiments (see SlotFormer) on VQA have demonstrated the effectiveness of these representations on downstream tasks where surface-level attributes are also relevant. Therefore, the idea of enforcing attribute-level similarity lacks motivation. On the other hand, I would expect representations produced by DINO and CLIP to lack certain information because they are not trained with a reconstruction objective.\n- In addition to the lack of motivation for the proposed multi-object retrieval setting, the paper also offers limited technical contribution. The proposed method relies on simple heuristics of PCA to segment objects in the scene and then leverages a β-VAE to learn disentangled features for each patch. This is essentially a combination of well-known techniques."}, "questions": {"value": "- What is the motivation for using cosine similarity in the multi-object retrieval setting? Why not just train a classifier to predict the attributes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cl1UPDmCHk", "forum": "vVMT9ODgwR", "replyto": "vVMT9ODgwR", "signatures": ["ICLR.cc/2026/Conference/Submission24837/Reviewer_joPZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24837/Reviewer_joPZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891821677, "cdate": 1761891821677, "tmdate": 1762943214124, "mdate": 1762943214124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies fine-grained surface attributes to be essential for multi-object instance discrimination, which is lacked by SSL models like DINO. The authors propose Oh-A-DINO (Object-Aware-DINO), which augments DINOv2 representations with object-centric VAE latents trained on segmented image patches to improve this.\nPCA-based segmentation extracts object regions from DINOv2 embeddings, followed by VAE training on patches to capture fine-grained attributes like color and material.\nEmpirically, Oh-A-DINO significantly improves previous methods across CLEVR, CLEVRTex, and Stanford Cars."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper identified an interesting limitation in current SSL models on object-centric benchmarks that they struggle with surface-level attributes, making this a valuable research direction.\n- The combination of global SSL features with local VAE latents provides a principled way to preserve both geometric and surface attribute information.\n- The evaluation is thorough, covering both synthetic (CLEVR, CLEVRTex) and real-world (Stanford Cars) datasets. They effectively measure the model's ability to distinguish objects based on fine-grained attributes. The performance is strong compared to prior methods. Ablation studies properly isolate the contribution of different components.\n- The delivery of the paper is clear, and I find it easy to follow."}, "weaknesses": {"value": "- The evaluation focuses primarily on color, material, and basic geometric attributes. More complex attributes like texture patterns, semantic relationships, or fine-grained visual details remain unexplored. The generalizability to broader attribute types would be interesting.\n- While CLEVR provides controlled evaluation, real-world evaluation is limited to Stanford Cars. More diverse real-world datasets spanning different domains would strengthen the claims."}, "questions": {"value": "- Can the authors provide more failure case analysis or discussion of when the method might not work well? Understanding the boundaries and limitations would improve the contribution's practical value.\n- How would SSL methods that employ reconstruction-based losses (eg, iBOT, SigLIP2, AM-RADIOv2.5) perform? How would recent advances in SSL (eg, Perception Encoder) and multimodal LLMs (eg, Qwen3-VL) perform on these benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wUmVptAUFT", "forum": "vVMT9ODgwR", "replyto": "vVMT9ODgwR", "signatures": ["ICLR.cc/2026/Conference/Submission24837/Reviewer_D46H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24837/Reviewer_D46H"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972963389, "cdate": 1761972963389, "tmdate": 1762943213790, "mdate": 1762943213790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper study how self-supervised models and slot-based methods understand objects in complex scenes. It find that models like CLIP and DINO good at shape and size, but not so good with color or texture. The authors propose a latent space with VAE regularization to fix this, which improve retrieval results. The idea is interesting and show potential in especially multi-object retrieval tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- originality: 2/5,\n- quality: 2/5,\n- clarity: 4/5,\n- significance: 2/5. Limited to simple image retrieval tasks."}, "weaknesses": {"value": "W1\n---\nTable. 1.\nThe OCL baseline SlotDiffusion is relatively weak. In fact the object representation quality is highly affected by the object discovery (unsupervised object segmentation) accuracy. There are more advanced OCL methods, like SPOT, DIAS and SmoothSA, which should also be included as stronger OCL baselines.\n\n\nW2\n---\nLine 201.\n> collected from **a batch of t images** and apply PCA\n\nThis means an online induction based on multiple $t$ input image samples -- What if there is only one input available during inference?\n\nThis design could also be a bottleneck for real-world complex images like ones from COCO or ImageNet, where the borderline between foreground and background can be quite vague.\n\n\n\nW3\n---\nLine 223,\n> This yields a set of **object-level** latents\n\nTo put it in a rigid way, these are still **patch-level** latents with object/foreground mask augmentation, which is obtained in Section 3.2 (ii) \"Refining object consistency\" operation.\n\n\n\nW4\n---\nLine 228,\n> (CLS token in **DINOs** case)\n\n\"DINOs\" should be \"DINO's\".\n\n\n\nW5\n---\nLine 232,\n\n> Retrieval is then performed by cosine similarity between v and v′ from query and candidate images\nIt is unclear your performance boost comes from the concat of global features (Figure 2, Line 167 and 168) or not. So for fair comparison, OCL representations should also be concatenated with the CLS token from DINO as the strong baseline.\n\n\nW6\n---\nLine 234 or Appendix A:\n> for each query patch vi we retrieve the patch with the highest cosine similarity\n\n> $s_i^{max} = \\max_j S_{ij}$\n\nIntuitively, the max matching should be Hungarian matching. Otherwise, there might be multiple patches $i$ matched to the same $j$.\n\n\nW7\n---\nLine 447:\n> 6 STANFORD CARS: REAL-WORLD INSTANCE RETRIEVAL\n\nThe section label \"6\" should be \"5.4\", parallel to Section 5.2, as results on synthetic and real-world datasets respectively.\n\n\nReferences\n---\n- SPOT: Self-Training with Patch-Order Permutation for Object-Centric Learning with Autoregressive Transformers\n- DIAS: Slot Attention with Re-Initialization and Self-Distillation\n- SmoothSA: Smoothing Slot Attention Iterations and Recurrences"}, "questions": {"value": "N/A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pl09qMyGob", "forum": "vVMT9ODgwR", "replyto": "vVMT9ODgwR", "signatures": ["ICLR.cc/2026/Conference/Submission24837/Reviewer_jkxw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24837/Reviewer_jkxw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762115646926, "cdate": 1762115646926, "tmdate": 1762943213616, "mdate": 1762943213616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Rebuttal"}, "comment": {"value": "Dear Reviewers,\n\nWe want to sincerely thank you for your valuable feedback both positive and negative. We believe your feedback has led us to improve our paper in the revised version. Here is a quick summary of the changes that were made to address the concerns:\n\n**1. Stronger OCL baselines.**  \nWe added **SPOT** and **SmoothSA**, two state-of-the-art object-centric models, to all single- and multi-attribute retrieval evaluations. These models outperform SlotDiffusion on geometric cues but, importantly, still exhibit **consistently low colour/material retrieval**. This strengthens our core finding that OCL bottlenecks preserve surface cues only in reconstruction pathways, not in metrically accessible embeddings.\n\n**2. Conceptual clarification.**  \nWe clarified the difference between **information presence** and **geometric accessibility**, explaining why cosine similarity is appropriate for retrieval-based downstream tasks and how slot models can access surface-level features in VQA via supervised heads but that the standalone representations don't encode this information in a metrically accessible way. We updated the corresponding parts in the new revision of the paper.\n\n**3. Clarified PCA inference and added examples.**  \nWe clarified that PCA does **not** require multiple test inputs at inference. Instead, PCA directions are computed using a **fixed offline memory bank**, and each query retrieves _k_ nearest neighbours from this pool. Moreover, inference uses only a **single image** and our method requires only coarse foreground separation to work. We added segmentation examples in Figure 7 of the appendix. Finally, we clarify that any segmentation method could be used to retrieve a segmentation mask.\n\n**4. Additional augmentation with DINO CLS token.**  \nFollowing suggestions, we evaluated SlotDiffusion **augmented with the DINOv2 CLS token**. This improves geometric attributes slightly but does **not** improve colour/material retrieval, confirming that our gains do not come from global features.\n\nFurthermore, we attempted to address further concerns raised by each reviewer. We hope these additions and clarifications resolve the reviewers’ concerns and better communicate the contribution of our work."}}, "id": "NUY79EnBZX", "forum": "vVMT9ODgwR", "replyto": "vVMT9ODgwR", "signatures": ["ICLR.cc/2026/Conference/Submission24837/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24837/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission24837/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763519209367, "cdate": 1763519209367, "tmdate": 1763519209367, "mdate": 1763519209367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}