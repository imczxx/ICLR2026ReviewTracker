{"id": "oSX9aenbea", "number": 1526, "cdate": 1756889583584, "mdate": 1759898204305, "content": {"title": "MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models", "abstract": "Recent advances in multimodal large language models (MLLMs) have catalyzed transformative progress in affective computing, enabling models to exhibit emergent emotional intelligence. Despite substantial methodological progress, current emotional benchmarks remain limited, as it is still unknown: (a) the generalization abilities of MLLMs across distinct scenarios, and (b) their reasoning capabilities to identify the triggering factors behind emotional states. To bridge these gaps, we present MME-Emotion, a systematic benchmark that assesses both emotional understanding and reasoning capabilities of MLLMs, enjoying scalable capacity, diverse settings, and unified protocols. As the largest emotional intelligence benchmark for MLLMs, MME-Emotion contains over 6,000 curated video clips with task-specific questioning-answering (QA) pairs, spanning broad scenarios to formulate eight emotional tasks. It further incorporates a holistic evaluation suite with hybrid metrics for emotion recognition and reasoning, analyzed through a multi-agent system framework.\nThrough a rigorous evaluation of 20 advanced MLLMs, we uncover both their strengths and limitations, yielding several key insights: (1) Current MLLMs exhibit unsatisfactory emotional intelligence, with the best-performing model achieving only $39.3\\%$ recognition score and $56.0\\%$ Chain-of-Thought (CoT) score on our benchmark. (2) Generalist models (\\emph{e.g.}, Gemini-2.5-Pro) derive emotional intelligence from generalized multimodal understanding capabilities, while specialist models (\\emph{e.g.}, R1-Omni) can achieve comparable performance through domain-specific post-training adaptation. By introducing MME-Emotion, we hope that it can serve as a foundation for advancing MLLMs' emotional intelligence in the future.", "tldr": "We propose MME-Emotion, the first-ever comprehensive benchmark for emotional intelligence in multimodal large language models (MLLMs).", "keywords": ["Multimodal Large Language Models", "Affective Computing"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1ecfe909780669e3233295fc6ee4613a371dc1ea.pdf", "supplementary_material": "/attachment/6605bfd9ae67ddae6bc701b8fd7144862f9025d8.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces MME-Emotion, a new benchmark designed to holistically evaluate the emotional intelligence of MLLMs. The authors argue that existing benchmarks are limited in scope and fail to assess the reasoning capabilities behind emotional state identification. To address this, MME-Emotion provides a large-scale dataset of over 6,000 curated video clips, organized into eight distinct emotional tasks across 27 different scenarios. A key contribution is the proposed holistic evaluation suite, which employs a multi-agent framework to automatically assess model performance. This framework uses a GPT-based judge to score models on three unified metrics: Recognition Score (Rec-S) for accuracy, Reasoning Score (Rea-S) for the quality of step-by-step logic, and a combined Chain-of-Thought Score (CoT-S). The authors conduct a rigorous evaluation of 20 advanced MLLMs, revealing that current models still possess unsatisfactory emotional intelligence. The paper concludes with several key insights, such as the comparable performance of generalist and specialist models and the correlation between deeper reasoning and better performance, paving the way for future research in affective computing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-motivated by identifying an existing gap in the evaluation of emotional intelligence for MLLMs. To address this, it introduces the MME-Emotion benchmark, which includes over 6,000 videos and its structured organization across 8 tasks and 27 scenarios, expanding the scope of previously available datasets in this area.\n\n2. The paper proposes a holistic, automated evaluation suite. A key feature of this framework is its use of a multi-agent, MLLM-as-judge system to assess performance. This approach extends beyond simple recognition accuracy by incorporating metrics designed to measure reasoning quality (Rea-S and CoT-S), offering a multi-faceted view of model capabilities.\n\n3. The paper conducts a large-scale empirical study evaluating the performance of 20 different MLLMs on the proposed benchmark. This comparison provides a broad overview of current model performance on these emotional intelligence tasks. The results highlight the challenging nature of the benchmark for existing models and document performance differences between generalist and specialist approaches."}, "weaknesses": {"value": "1. The benchmark is framed as \"multi-task,\" but the eight defined tasks (e.g., ER-Lab, ER-Wild, Noise-ER, SA, FG-SA) appear to be variants of a single core task: emotion classification. They primarily differ by scenario (e.g., in-the-lab vs. in-the-wild) or label granularity rather than representing fundamentally distinct tasks. Consequently, the analysis in Section 4, which shows performance differences, highlights varying difficulty levels but fails to provide a deeper analysis of why models perform differently across these closely related settings.\n2. A significant concern arises from the reported results in Table 2. Highly capable specialist models like AffectGPT (11.9% Rec-S) and advanced omni-modal models like Qwen2.5-Omni (17.4% Rec-S) achieve scores that are barely above random guessing. This performance is inconsistent with the capabilities demonstrated in their original papers. This discrepancy raises critical questions about the experimental methodology, suggesting potential issues with either the model replication or, more fundamentally, the label quality and accuracy of the MME-Emotion dataset itself. Such poor performance from top models risks undermining the benchmark's validity.\n3. The evaluation framework is heavily centered on CoT reasoning. However, the paper does not provide an ablation study to justify that CoT is necessary or even beneficial for emotion recognition, a task that often relies more on perception than complex reasoning. This approach also introduces a clear bias against models not optimized for step-by-step output. As seen in Table 2, models like HumanOmni and Emotion-LLaMA produce very short outputs (Avg Token = 1.3 and 2.3) and have near-zero Reasoning Scores (Rea-S = 0.3 and 0.4). Evaluating them with the CoT-S metric, which is heavily weighted by Rea-S, is unfair and does not accurately measure their ability to perform direct emotion recognition.\n4. The paper contains some inaccuracies regarding the capabilities of the models evaluated. For instance, Table 2 indicates that the Gemini-2.5-Pro cannot process audio (the 'A' column is not checked). This is factually incorrect, as it is natively capable of processing audio from video inputs."}, "questions": {"value": "1. Could you elaborate on the conceptual distinctions that make your eight task categories fundamentally different \"tasks\" rather than different \"scenarios\" of a single emotion classification task?\n2. The performance of models like AffectGPT and Qwen2.5-Omni is unexpectedly low. To clarify these results, could you provide details on your model replication and the benchmark's label quality (e.g., inter-annotator agreement)?\n3. Have you performed an ablation study comparing model performance with and without the CoT instruction to justify its necessity? Additionally, how do you ensure fair evaluation for models designed for direct answers that are penalized by the CoT-S metric?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X0Llus1d4y", "forum": "oSX9aenbea", "replyto": "oSX9aenbea", "signatures": ["ICLR.cc/2026/Conference/Submission1526/Reviewer_2ztj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1526/Reviewer_2ztj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761395011382, "cdate": 1761395011382, "tmdate": 1762915793146, "mdate": 1762915793146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MME-Emotion, a comprehensive benchmark designed to evaluate the emotional intelligence of large language models (LLMs) across multiple modalities. Comprising over 6,500 curated video clips spanning 27 real-world scenarios and eight distinct emotional tasks, it covers areas such as emotion recognition in laboratory and natural settings, fine-grained and multi-label recognition, sentiment analysis, and intent recognition. The authors also propose a multi-agent evaluation framework that automatically assesses multimodal LLMs (MLLMs) using three unified metrics: Recognition Score (Rec-S), Reasoning Score (Rea-S) and Chain-of-Thought Score (CoT-S). The framework uses GPT-based 'judge' and 'step' agents to evaluate model responses without the need for human-annotated reasoning traces and validates this method through high inter-rater agreement with five human experts. Using this benchmark, the authors evaluate 20 state-of-the-art MLLMs and reveal that even the top models only achieve 39.3% Rec-S and 56.0% CoT-S, highlighting significant room for improvement. The paper also explores the trade-offs between generalist and specialist models, emphasizing the importance of multimodal fusion and reasoning depth."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1) MME-Emotion is the first holistic benchmark to evaluate both the presence of an emotion and the reason for it, offering a novel method that goes beyond mere classification. The multi-agent automated evaluation is a creative solution to the absence of annotated reasoning chains.\n2) The large-scale benchmark comprises 6,500 clips, 27 scenarios and eight tasks, and is balanced in terms of duration and question distribution. Human validation of the automated scoring adds credibility.\n3) The paper is clear and precise, with transparent reporting of model performance and precise definitions of metrics.\n4) By exposing the limitations of current MLLMs, even the top models achieving a score of less than 40% in recognition, the work sets out a clear research agenda. It also shows that, with targeted post-training, specialist models can rival generalists, offering practical guidance for future development."}, "weaknesses": {"value": "1) Although the article acknowledges that specialized models (e.g. Audio-Reasoner) outperform their multimodal counterparts, it does not provide a systematic analysis of the contribution of individual modalities. Ablation experiments (e.g. running the same MLLM in audio-only, video-only and audio+video modes) could reveal whether the problem lies in the modalities' integration being inefficient or in noise/conflict between the modalities. I would like to see more detail on this, either in the form of detailed experiments or a detailed discussion supported by evidence.\n2) Using a set list of emotions in prompts simplifies the task, but may inflate metrics. In real-world scenarios, users do not provide such a list, so the model must generate an open-ended response. Am I correct in understanding that this reduces the applicability of the results to practical tasks, and is therefore a limitation? Or is this an unavoidable fact, and is there no other way to achieve the obtained metrics?\n3) Although the authors acknowledge that the data is multilingual, they do not stratify by language or culture. Emotional expressions and norms can vary greatly between cultures (e.g. East Asian and Western), so ignoring this factor could lead to biased conclusions about the 'universal' emotional competence of models. This is worth mentioning in more detail to avoid any misunderstandings.\n4) Although the average clip length is specified as being greater than 3.3 seconds, there has been no investigation into how performance scales with clip length or temporal complexity (e.g. a change in emotion occurring midway through a clip). This is important for understanding the extent to which models are capable of temporal reasoning, which is one of the key aspects of real emotional perception. More detailed explanations of this should be added.\n5) Am I correct in understanding that the Reasoning Score metric assesses the accuracy of each step, but does not analyze common error patterns? What happens, for example, if the audio context is ignored, microexpressions are misinterpreted, or emotions are mixed up? Can this metric evaluate all these errors and others at once?\n6) From the paper, it seems that the authors use test splits of the original corpus. This may mean that the wording of the questions and candidate labels indirectly reveals information about the data distribution, particularly if the prompts are based on the original annotations. How confident are the authors that this is not happening, and how can this be assessed?\n7) Although there is a high level of agreement between the GPT assessment and the experts (0.953), the experts only evaluated 373 reasoning steps. If a more extensive verification is performed, especially on complex or borderline cases, how can we be sure that the assessment will not be lower and that it is not currently being adjusted for greater consistency?\n8) The CoT-S formula uses a fixed value and does not involve sensitivity analysis. Different tasks may require different balances between recognition and reasoning. For instance, accuracy is more important in clinical diagnostics, whereas explainability is more important in education. So why is the value fixed rather than offering adaptive adjustment?\n9) Table 2 shows 'Avg Token' and 'Avg Step', but how are these metrics analyzed in relation to quality? For example, how effective is a token per point?\n10) Although 'balancing' is mentioned, it is unclear exactly how the even distribution of emotions, scenarios and complexity was achieved. What was the age and gender distribution? How does this affect the final distribution? More details on this matter are needed; otherwise, there will still be many doubts about the samples and how they are used.\n11) There are few details on exactly how noise affects reasoning quality, rather than just final predictions."}, "questions": {"value": "1) Were ablation experiments conducted with the same MLLM in audio-only, video-only, and audio+video modes?\n2) Using a set list of emotions in the prompts makes the task easier than using an open format. Are you aware of this limitation? Do you think the current wording of the task reduces the validity of the results?\n3) Do you plan to analyze how well the model performs across different cultural groups (e.g. East Asian versus Western)? Have you considered that ignoring cultural differences in emotional expression could lead to inaccurate conclusions about 'universal' emotional competence?\n4) Have you conducted a sensitivity analysis to assess the models' ability to reason temporally? If not, how do you evaluate the models' capacity for temporal reasoning using the available data?\n5) Do you plan to categorize common mistakes? For example, can the current evaluation framework distinguish between logically sound reasoning based on incorrect premises?\n6) You claim that you only use test splits of the original corpus. However, the wording of the questions and the candidate labels may indirectly reveal information about the data distribution, particularly if the prompts are based on the original annotations. How did you verify that there was no such leakage?\n7) Did you carry out further checks on complex or borderline cases? Do you expect the level of agreement to decrease (to much lower than the current 0.953) when the sample is expanded?\n8) What are your plans for offering adaptive $\\alpha$ adjustment on a task-by-task basis?\n9) Do you have any data on the effectiveness of long arguments? Could an average step size that is too high sometimes be a sign of redundancy rather than depth?\n10) In terms of emotions, scenarios and complexity, how exactly was balance achieved? How are the characters in the video distributed by age, gender and ethnicity?\n11) Have you analyzed how noise affects reasoning quality (Rea-S)? For instance, can models retain a logically sound argument but draw incorrect conclusions?\n12) Have you conducted any more extensive experiments on agent replacement?\n13) The benchmark does not include chains of reasoning from humans that are verified as correct. Do you plan to collect such data? Wouldn't you agree that comparing with human reasoning (and not just the final label) would provide a deeper understanding of the quality of CoT in models?\n14) Do you plan to introduce asymmetric penalties or weighted metrics that consider the context of use?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jMZ3WSIovn", "forum": "oSX9aenbea", "replyto": "oSX9aenbea", "signatures": ["ICLR.cc/2026/Conference/Submission1526/Reviewer_73cU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1526/Reviewer_73cU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761762065508, "cdate": 1761762065508, "tmdate": 1762915793027, "mdate": 1762915793027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MME-Emotion, a holistic evaluation benchmark for assessing the emotional intelligence of MLLMs, addressing the limitations of existing benchmarks (inadequate scenario coverage and inconsistent protocols that overlook reasoning capabilities). Its core contributions include: 1) constructing the largest benchmark to date, with 6,500 curated video clips paired with task-specific QA pairs, covering 8 emotional tasks across 27 scenarios to test generalization; 2) designing a holistic automated evaluation suite via a multi-agent system, using three unified metrics and validating reliability with 5 human experts; 3) conducting rigorous evaluation of 20 MLLMs, revealing key insights. MME-Emotion serves as a foundation for advancing MLLMs’ emotional intelligence, with source code and data in the Supplementary Material."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. MME-Emotion has the leading benchmark scale and scenario coverage, which includes 6,500 curated video clips with task-specific QA pairs and 8 emotional tasks. This benchmark could enable fine-grained evaluation of model generalization and address the insufficient scenario coverage of existing benchmarks.\n\n2. An automated evaluation suite is proposed. A multi-agent system-based evaluation framework is designed, which could evaluate the performance of MLLMs without manual annotation of reasoning steps. The evaluation suite shows extremely high consistency with human experts.\n\n3. This paper has carried out a comprehensive empirical analysis of 20 state-of-the-art multimodal large language models (MLLMs) based on the self-constructed MME-Emotion benchmark, covering both open-source and closed-source models. It not only reveals the insufficient emotional intelligence of current models but also clarifies the emotional intelligence construction paths of generalist models and specialist models, providing guidance for future research."}, "weaknesses": {"value": "1. Compared with existing emotional intelligence benchmarks, the main contributions of this paper lie in two aspects: first, it incorporates relevant evaluations for emotional reasoning capabilities; second, it designs a large model-based automated evaluation algorithm.\n\n2. The paper only considers various emotion recognition tasks and does not include emotion generation tasks. Can recognition-only tasks sufficiently and comprehensively assess the emotional intelligence of models?\n\n3. The paper evaluates the model's emotional reasoning ability by identifying the triggering factors behind emotional states, which represents a relatively singular perspective.\n\n4. The sensitivity of different MLLMs to prompts may affect the evaluation results.\n\n5. The large model-based automated evaluation method incurs certain costs."}, "questions": {"value": "1. Is the use of a multi-agent architecture for model evaluation sufficiently stable? For instance, can consistent results be obtained across multiple runs?\n\n2. Will updates to the version of the core judge model affect the final evaluation results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rzK4WZgp2V", "forum": "oSX9aenbea", "replyto": "oSX9aenbea", "signatures": ["ICLR.cc/2026/Conference/Submission1526/Reviewer_12fu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1526/Reviewer_12fu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791480632, "cdate": 1761791480632, "tmdate": 1762915792904, "mdate": 1762915792904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces an emotional reasoning dataset and tests a suite of models on it."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a comprehensive testing suite and tests on a wide variety of models. Tests reasoning vs. non-reasoning.\n\nIt is interesting how the authors adapted video data for non-video models."}, "weaknesses": {"value": "It is hard to know if the MLLMs don’t perform well on emotion tasks because they have trouble with modality fusion/their perception system is weaker or if it is truly a problem with emotion recognition. To test this the authors could have used one of the unimodal datasets cited in Table 1 and tested each model on a text-only version of the task or an image-only version of the task to test the model’s ability. That way the reader would be better informed if the issue was modality/modality-fusion or if the model truly is emotionally unaware. \n\nVideo is an inherently difficult modality to work in, so using it to benchmark models on a non-video primitive seems to be a misguided approach. We can get to how well these models recognize and understand emotion from other modalities. If the issue is in the video processing, that may not be an emotion recognition problem at all! It would be interesting to know the disparity as this can show what the lag from not processing videos well might be.\n\nIt would have been best to have a “human-level” benchmark for this benchmark to compare to.\n\nObservation section is interesting but also a bit obvious. e.g. Obs 3, bad visual perception constrains emotional intelligence.\n\nTable 2 “Colsed-source MLLMs”  “Closed-source MLLMs”"}, "questions": {"value": "."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YHBUGkkULH", "forum": "oSX9aenbea", "replyto": "oSX9aenbea", "signatures": ["ICLR.cc/2026/Conference/Submission1526/Reviewer_XCho"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1526/Reviewer_XCho"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921127993, "cdate": 1761921127993, "tmdate": 1762915792780, "mdate": 1762915792780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}