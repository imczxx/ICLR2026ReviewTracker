{"id": "tVu1zfdbhu", "number": 21669, "cdate": 1758320335649, "mdate": 1759896909498, "content": {"title": "Wavelet-Driven Masked Multiscale Reconstruction for PPG Foundation Models", "abstract": "Wearable foundation models have the potential to transform digital health by learning transferable representations from large-scale biosignals collected in everyday settings. \nWhile recent progress has been made in large-scale pretraining, most approaches overlook the spectral structure of photoplethysmography (PPG) signals, wherein physiological rhythms unfold across multiple frequency bands.\nMotivated by the insight that many downstream health-related tasks depend on multi-resolution features spanning fine-grained waveform morphology to global rhythmic dynamics, we introduce Masked Multiscale Reconstruction (MMR) for PPG representation learning -- a self-supervised pretraining framework that explicitly learns from hierarchical time–frequency scales of PPG data.\nThe pretraining task is designed to reconstruct randomly masked out coefficients  obtained from a wavelet-based multiresolution decomposition of PPG signals, forcing the transformer encoder to integrate information across temporal and spectral scales.\nWe pretrain our model with MMR using  ~17 million unlabeled 10-second PPG segments collected from over ~32000 smartwatch users largely in naturalistic field settings, ensuring high variability and ecological validity. \nOn 11 of 13 diverse health-related tasks, MMR trained on large-scale wearable PPG data outperforms or matches state-of-the-art open-source PPG foundation models, time-series foundation models and other self-supervised baselines. \nExtensive analysis of our learned embeddings and systematic ablations underscore the value of wavelet-based representations, showing that they capture robust and physiologically-grounded features. \nTogether, these results highlight the potential of MMR as a step toward generalizable PPG foundation models.", "tldr": "Masked multiscale reconstruction leverages multi-resolution wavelet decomposition to pretrain a large-scale wavelet-driven PPG foundation model, learning rich time–frequency features and transferring with superior performance across health tasks", "keywords": ["Wearables", "Foundation Models", "Masked Reconstruction", "Discrete Wavelet Decomposition", "Self Supervised Learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9b9161b3feb82a65c013750a597509a33502f1fe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Masked Multiscale Reconstruction, a self-supervised pretraining framework for wearable PPG signals. The key idea is to transform each 10-s segment into multi-resolution wavelet coefficients, randomly mask 75% of small temporal patches across sub-bands, and train a ViT encoder–decoder to reconstruct the missing coefficients. The authors pretrain on ~17M PPG segments from ~32K smartwatch users, then evaluate frozen embeddings with simple downstream heads across 13 health-related tasks. They report consistent gains or parity versus strong baselines and competitive/open-source models, with notable improvements on free-living hypertension and PVC detection; they also include data/model scaling and ablations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tOn free‑living hypertension, MMR achieves 68.1 AUROC versus 62.9 for TF‑C and 60.9 for SimCLR; for PVC detection, MMR reaches 82.0 AUROC versus 70–74 for baselines. Across classification tasks, MMR also outperforms PaPaGei and Chronos on average. \n2.\tTreating PPG in the time–frequency domain with wavelets matches physiological non‑stationarity and multi‑scale rhythms, and masked reconstruction across sub‑bands encourages cross‑scale feature sharing. \n3.\tAblations and scaling analyses offer practical guidance: Haar wavelets often perform best; moderate decomposition depth (L3–L4) helps hypertension/PVC; smaller patches (1×25) preserve fine events; and increasing data volume improves key tasks."}, "weaknesses": {"value": "1.\tThe core idea is masked autoencoding in a time–frequency domain with a fixed DWT tokenizer. Closely related frequency-aware MAE variants already exist, and the contribution feels incremental.\n2.\tUsing a predefined DWT front-end may underfit device-specific characteristics and miss discriminative sub-band boundaries. It will be better to explore the learnable filterbanks or jointly trained spectral tokenizers.\n3.\tConverting multi-rate sub-bands into a 2D coefficient map via interpolation risks aliasing and amplitude distortion. The interpolation kernel, anti-aliasing, and per-band normalization choices are not justified or stress-tested.\n4.\tUniform random masking likely biases learning toward low-frequency shortcuts. Missing band-aware, curriculum, event-centric, or rarity-aware masking that would force recovery of high-frequency/rare cues (e.g., ectopy).\n5.\tNo clear rebalancing (e.g., class weights, focal loss), threshold optimization, or cost-sensitive analysis; this can suppress performance on rare events.\n6.\tTables 1–2 show that MMR is not SOTA on several classification endpoints and is sometimes matched or outperformed by MMR-Light or baselines. The paper does not explain these gaps or provide a failure-mode, leaving the causes of underperformance unclear."}, "questions": {"value": "1.\tCould you clarify how your method is substantively different from existing frequency-aware MAE variants and why fixed-DWT masked autoencoding constitutes a novel contribution? \n2.\tWill you evaluate learnable filterbanks or jointly trained spectral tokenizers to address potential underfitting to device-specific characteristics and missed discriminative sub-band boundaries?\n3.\tHow were the interpolation kernel, anti-aliasing, and per-band normalization chosen when converting multi-rate sub-bands into a 2D map, and can you provide stress tests showing minimal aliasing/amplitude distortion? \n4.\tCan you justify uniform random masking and report results for band-aware, event-centric/curriculum, or rarity-aware masking that better targets high-frequency/rare cues?\n5.\tDid you apply class rebalancing (class weights/focal loss), threshold optimization, or cost-sensitive analysis? If not, can you add these and report their impact on rare-event metrics?\n6.\tIn Table 1&2, MMR is not SOTA on several classification endpoints and in a few cases is even matched or exceeded by MMR-Light or baselines (SimCLR/TF-C). Could you explain these gaps and provide a brief failure-mode analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Axvi9D4PLN", "forum": "tVu1zfdbhu", "replyto": "tVu1zfdbhu", "signatures": ["ICLR.cc/2026/Conference/Submission21669/Reviewer_H7f6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21669/Reviewer_H7f6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849588936, "cdate": 1761849588936, "tmdate": 1762941884070, "mdate": 1762941884070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Masked Multi-Scale Reconstruction, a method for PPG representation learning. In this approach, PPG signals are decomposed into wavelet coefficients using a discrete wavelet transform, which are then masked and subsequently reconstructed to learn meaningful representations. The model is trained on a large proprietary dataset and evaluated against several baselines across 13 downstream tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of learning PPG representations through the reconstruction of masked DWT coefficients is quite interesting.\n- The paper includes thorough experiments, with useful case studies and ablation analyses beyond standard downstream evaluations.\n- The paper is well-written and easy to follow."}, "weaknesses": {"value": "**Evaluation:** The diversity and number of devices used are essential for interpreting the results, and reporting these details would not compromise anonymity. However, the use of a closed-source dataset limits the interpretability and reproducibility of the findings. Specifically: (1) the number of datasets from which each downstream task is derived remains unclear, and (2) it is uncertain whether the training and test data originate from the same devices. Furthermore, several public PPG datasets [1] could be utilized for external validation to strengthen the evaluation.\n\nThis point is particularly important because, when the dataset is fixed and only the training method is varied (Table 2), the average performance improvement of the proposed approach is relatively small (\\~1%) compared to open-source models trained on different data (\\~3.5%). This suggests that, if the baselines were trained using the same data, their performance could be comparable to MMR, thereby weakening the claim of improved generalizability.\n\n**Capture multi-resolution characteristics** such as: high-frequency transients, subtle waveform changes, and low-frequency rhythms such as respiration and circadian trend. Is there any supporting evidence or case study demonstrating that some of these characteristics are captured by MMR. Furthermore, many of the downstream tasks involve blood lab measurements, for which such multi-resolution features may not be directly relevant. This suggests a potential mismatch between the motivation of MMR and the choice of downstream tasks used for evaluation. In short, there could be a potential mismatch between the motivation of MMR and and the choice of downstream tasks used for evaluation.\n\n**Experiments:** Many downstream tasks are strongly correlated with demographic variables. Therefore, it would be valuable to evaluate how the proposed approach performs on demographic prediction tasks such as age, sex, and BMI. Moreover, as the model is closed-source, these results and other relevant factors could be compared to the findings reported by Abbaspourazad _et al._ [2] for additional context.\n\nAdditionally, it would be insightful to examine the advantages of wavelet-based reconstruction through MMR compared to directly reconstructing the raw PPG signal. Such an experiment could help clarify the specific benefits of the wavelet decomposition stage and further strengthen the empirical analysis.\n\n**Contribution beyond previous work:** While the proposed methodology is interesting, the contribution beyond prior work appears limited. (1) The approach is closed-source and comparable to [1], which leverages larger datasets to demonstrate that PPG representations can generalize to more than 50 downstream tasks. In [2], an open-source model is introduced using a morphology-aware self-supervised learning (SSL) strategy that generalizes across diverse tasks. Apart from the training framework, it is difficult to clearly distinguish this work in terms of experimental design, task selection, and case studies from previous research to justify publication. Moreover, the performance improvements of MMR are marginal compared to other SSL approaches trained on the same data. Considering these points, I lean toward a reject.\n\n[1] Pillai, A., Spathis, D., Kawsar, F., & Malekzadeh, M. (2024). Papagei: Open foundation models for optical physiological signals. _arXiv preprint arXiv:2410.20542_.\n\n[2] Abbaspourazad, S., Elachqar, O., Miller, A. C., Emrani, S., Nallasamy, U., & Shapiro, I. (2023). Large-scale training of foundation models for wearable biosignals. _arXiv preprint arXiv:2312.05409_."}, "questions": {"value": "- What is the rationale behind using backbones of different sizes in Table 1? Can't MMR be matched to 5M parameters.\n- Were other transforms beyond DWT considered for reconstruction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Zn2tFjdWEC", "forum": "tVu1zfdbhu", "replyto": "tVu1zfdbhu", "signatures": ["ICLR.cc/2026/Conference/Submission21669/Reviewer_ciJy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21669/Reviewer_ciJy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902485071, "cdate": 1761902485071, "tmdate": 1762941883665, "mdate": 1762941883665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MMR, a self-supervised pretraining framework for large-scale PPG foundation models. The approach decomposes PPG signals into hierarchical time–frequency representations via discrete wavelet transforms and trains a ViT-based encoder–decoder to reconstruct masked coefficients across scales. The method uses PPG segments from smartwatch users collected in the field. The pretrained representations are evaluated on 13 downstream cardiovascular and biomarker prediction tasks, where MMR strong results comparing with the baselines. Ablations explore the impact of these different components on the performance of the model.\n\nI really like the future direction of exploring adaptive multiscale strategies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The use of wavelet-based multiscale reconstruction as a masked modeling target is a strong conceptual contribution.\n2. The diversity of the dataset, with 1h30 of data for each patient in unconstrained environments significantly increases applicability over prior foundation models trained on clean, clinical datasets.\n3. The data pre-processing has a good balance between cleaning and maintaining as much data as possible.\n4. The paper benchmarks across 13 diverse downstream tasks (clinical and physiological), both classification and regression.\n5. Well presented ablation study. Embedding visualizations (t-SNE, silhouette) convincingly show physiological structure capture.\n6. MMR-Light variant maintains strong performance with few parameters.\n7. Paper is well written and easy to read. (except figures/tables, see below)"}, "weaknesses": {"value": "1. While effective, MMR’s novelty lies mainly in applying masked reconstruction to wavelet coefficients. The method reuses a ViT backbone with minimal architectural innovations.\n2. Key preprocessing and DWT hyperparameters (e.g., sampling-rate normalization, interpolation scheme for coefficients) could be better detailed for reproducibility.\n3. The baselines could be newer models (eg Chronos-Bolt instead of Chronos)\n4. Clarity on the fixed parameters in the ablation study could be improved (what model size was used for the data scaling ablation and conversely what data size for the model scaling?\n5. It is disputable if this is indeed a diverse population with \"high variability\" as the smart-watch carrying population is rather restricted.\n6. The min-max range is very large, with 10-15% of the value and the models only being within a couple percentage points of each-other.\n\nClarity/formatting:\n- Figure 2 axis titles are hard to read\n- The tables could be improved for readability (grouping T.1+T.2, making a graph, etc.)"}, "questions": {"value": "1. Could MMR generalize to other biosignals (e.g., ECG, accelerometer)? Does the wavelet-based representation transfer across modalities?\n2. How do performance trends change if downstream tasks are fine-tuned rather than frozen?\n3. Were any measures taken to mitigate potential data leakage across users or sessions?\n4. How does computational cost compare in terms of FLOPs and convergence speed?\n5. Was any consideration given to using a diffusion model?\n6. Have the baseline models (eg. Chronos) been finetuned on this task/with this data? Do you think this would be relevant considering the small performance gap between it and MMR?\n7. How does the model's ability to isolate individual patients with higher patient-wise distance affect it's out-of-distribution abilities? Especially on new patients?\n8. Any intuition on why different models performs as they do on the different tasks, as per model task-performance rankings are not consistent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zTaO9Hoqef", "forum": "tVu1zfdbhu", "replyto": "tVu1zfdbhu", "signatures": ["ICLR.cc/2026/Conference/Submission21669/Reviewer_z2s9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21669/Reviewer_z2s9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762318213267, "cdate": 1762318213267, "tmdate": 1762941883149, "mdate": 1762941883149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new self-supervised pretraining framework for PPG foundation models, which applies Discrete Wavelet Transform (DWT) to decompose raw PPG into multiple frequency bands, then trains a Vision Transformer encoder-decoder to reconstruct randomly masked wavelet coefficients. The authors pretrain on approximately 17 million 10-second PPG segments from over 32,000 smartwatch users and evaluate on 13 diverse health prediction tasks, including hypertension detection, arrhythmia classification, and blood biomarker prediction. The results show that MMR achieves competitive performance with existing PPG foundation models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper tests 13 different downstream tasks spanning cardiovascular conditions (hypertension, PVC), metabolic markers (creatinine), and electrolyte imbalances, and provides strong evidence that the learned representations capture broadly useful information."}, "weaknesses": {"value": "1. The core technical contribution lacks clear validation. While wavelets are positioned as the main innovation, the paper never isolates whether DWT actually drives the performance gains. Critically, the paper is missing the essential ablation: MMR with DWT versus MMR without DWT—the same masked autoencoder architecture and training procedure applied to patchified raw PPG time series instead of wavelet coefficients.\n\n2. Results are mixed and claims are overstated. The abstract and conclusions describe MMR as achieving \"superior\" performance that \"outperforms\" baselines, but the actual results tell a more nuanced story.\n\n3. A fundamental limitation undermines the foundation model premise. The ablation studies reveal a critical tension: different tasks benefit from different wavelet decomposition depths. Hypertension classification works best with level 3, while lab biomarkers like WBC and Sodium prefer level 5. This is not a minor implementation detail—it directly contradicts the paper's core premise of learning \"robust, transferable features\" that generalize across diverse tasks. The authors acknowledge this as a \"key limitation\" and suggest exploring \"adaptive multiscale strategies\" in future work, but this limitation is severe enough to undermine the approach's viability. \n\n4. The \"multi-scale\" framing oversells temporal coverage. The paper claims to capture features \"spanning fine-grained waveform morphology to global rhythmic dynamics\" and explicitly mentions \"circadian trends\" (line 81). However, the 10-second window fundamentally cannot capture true long-term physiological scales. The lowest frequency reliably analyzable in 10 seconds is approximately 0.1 Hz—anything slower, including the circadian rhythms mentioned, is completely inaccessible. What the model actually learns are multi-resolution representations within short windows: beat morphology (~1-10 Hz) to respiratory and brief heart rate variability patterns (~0.1-1 Hz)."}, "questions": {"value": "1. Could you explain how variable sampling rates (25-100 Hz) are handled, despite this being essential for interpreting the wavelet decomposition.\n\n2. Do the same coefficient positions correspond to the same physiological phenomena across different sampling rates, or does rate variation change the meaning of features?\n\n3. Will you release the weights of the pre-trained model and datasets used from pre-training and downstream tasks for reproducibility? The current datasets come from proprietary smartwatch data that cannot be disclosed, rendering exact reproduction impossible."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "This paper mainly uses unpublished datasets and does not disclose the data collection process."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7cQo27BDIy", "forum": "tVu1zfdbhu", "replyto": "tVu1zfdbhu", "signatures": ["ICLR.cc/2026/Conference/Submission21669/Reviewer_kLnH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21669/Reviewer_kLnH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762369734155, "cdate": 1762369734155, "tmdate": 1762941882765, "mdate": 1762941882765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}