{"id": "lmVfTPQF3a", "number": 16940, "cdate": 1758270450627, "mdate": 1759897208749, "content": {"title": "Dataset Distillation for Memorized Data: Soft Labels can Leak Held-Out Teacher Knowledge", "abstract": "Dataset distillation aims to compress training data into fewer examples via a teacher, from which a student can learn effectively. While its success is often attributed to structure in the data, modern neural networks also memorize specific facts, but if and how such memorized information can be transferred in distillation settings remains less understood. While this transfer may be desirable in some applications, it also raises privacy concerns, where preventing such leakage is crucial. In this work, we show that students trained on soft labels from teachers can indeed achieve non-trivial accuracy on held-out memorized data they never directly observed. This effect persists on structured data when the teacher has not generalized. To understand this effect in isolation, we consider finite random i.i.d. datasets where generalization is a priori impossible and a successful teacher fit implies pure memorization. Still, students can learn non-trivial information about the held-out data, in some cases up to perfect accuracy. For multinomial logistic classification and single layer MLPs, we show this corresponds to the setting where the teacher can be recovered functionally -- the student matches the teacher's predictions on all possible inputs, including the held-out memorized data. We empirically show that these phenomena strongly depend on the sample complexity and the temperature with which the logits are smoothed, but persist across varying network capacities, architectures and dataset compositions.", "tldr": "", "keywords": ["distillation", "memorization", "generalization", "learning theory", "model transfer", "privacy"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ebd6c858852c002582c853220c6a781027c3eaf1.pdf", "supplementary_material": "/attachment/ba4ef7ba0c5bb0726b6ed5aa9d79b37306a6016e.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies an exotic problem: when a teacher trained to memorize a dataset and a student trained by distillation on a smaller training partition (compared to the teacher), the student generalizes better than a model trained on the same partition by standard cross entropy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors have done a meticulous and rigorous job in terms of clearly defining their hypothesis and constructing a specific set experiments which is used to study its validity.\n- Although the experimental framework is constrained (the tasks studied and the networks used), the results presented seem convincing in terms of proving the authors' hypothesis"}, "weaknesses": {"value": "- Motivation: It is not clear what is the motivation of the presented experiments. Even if the paper's hypothesis is true what does that mean in practice and that are the applications? Why should the community pay attention to the results of this paper? While this is somewhat described in privacy paragraph of section 2, more clarity is needed to understand the paper's motivation.\n- Impact of presented results: Can the paper's results be applied to real-world tasks, datasets and networks? Unfortunately, there's no evidence of that as also outlined in paper's limitations. \n- What's the purpose of training on random data? Can this have some real-world implication or application?\n- There's a very large number of experiments and cases analysed that makes the paper difficult to follow\n- Better paper proof reading: e.g. line 235"}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "aHFWlME4ec", "forum": "lmVfTPQF3a", "replyto": "lmVfTPQF3a", "signatures": ["ICLR.cc/2026/Conference/Submission16940/Reviewer_Z77o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16940/Reviewer_Z77o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810976575, "cdate": 1761810976575, "tmdate": 1762926966767, "mdate": 1762926966767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission concerns information leakage in model distillation: what does the teacher pass to the student? In particular, when the teacher has memorized some labels from its training data, does this information pass through? The paper shows that the answer is yes. The main results are empirical results in controlled synthetic settings. There is some theoretical analysis as well.\n\nHere's the main setup. We have some classification dataset $(X,Y)$ and train a teacher model $f(x)$ it. We split the data into $X=X_1 \\cup X_2$ and $Y=Y_1\\cup Y_2$. We train a student model on $(X,f(X))$. The experiments show how the student model can memorize the labels $Y_2$ without ever directly seeing them, i.e., the information comes solely through the teacher's labeling.\n\nThe teacher's predictions take the form of a probability distribution over classes and are controlled by a temperature parameter: in low-temperature limit the teacher makes hard choices and in the high-temperature limit the distribution is uniform. Clearly, in the high-temp limit, no information about $Y_2$ gets through to the student. On the other hand, if the teacher predicts the correct labels on its training data, then the low-temp limit corresponds to giving the student $(X_1,Y_1)$, which is independent of $Y_2$. But in between the soft labels seem to convey a lot of information!\n\nThe paper has lots of experiments on this phenomenon in different synthetic settings, with lots of interrelated results. It's not clear to me what the one big takeaway is, other than the fact that dramatic leakage can occur."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I enjoyed reading the submission. The experiments seem well-executed and are designed well: they do a good job probing the phenomenon.\n\nI will likely study this paper further, and I expect many other people will find it interesting."}, "weaknesses": {"value": "My main criticism of the paper is that the main takeaway, \"soft labels can leak held-out teacher knowledge,\" appears to be known. Theorem 1 of Phuong and Lampert (2019), who the authors do cite as related work, shows that soft labels in a linear classification setting lead to the student recovering the teacher's weights exactly. \n\nNow, this submission has a lot of results beyond the headline, so the work is still valuable. But the takeaways are less clear-cut. With so many experiments, the main part of this paper should probably be twice as long. The results are quite compressed and we jump around a lot. This paper would improve with additional work to craft the story."}, "questions": {"value": "Is my understanding correct about the implications of Phuong & Lampert's Theorem 1?\n\nHere is another natural setting for these experiments: the teacher trains model $f$ on $(X_T,Y_T)$ and the student trains on $(X_S, f(X_S))$, where $X_S$ and $X_T$ are independently drawn. Have you tried this? Would you expect very different behavior in this setting?\n\nAt points the experiments seem to switch away from cross-entropy, e.g., around line 295. Can you explain why we make that switch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5EpiNsxpAM", "forum": "lmVfTPQF3a", "replyto": "lmVfTPQF3a", "signatures": ["ICLR.cc/2026/Conference/Submission16940/Reviewer_hkBJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16940/Reviewer_hkBJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928558846, "cdate": 1761928558846, "tmdate": 1762926966417, "mdate": 1762926966417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how memorized information can be transferred from a teacher to a student during dataset distillation through soft labels. The authors show that even when the teacher has not generalized, students trained on its soft labels can recover non-trivial information about held-out samples, both in structured and random data settings. Using controlled experiments with transformers, logistic regression, and ReLU MLPs, the paper identifies distinct regimes that separate different forms of information leakage, depending on sample complexity, temperature, and model capacity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "This paper tackles a very cool and relevant topic: whether dataset distillation, specifically using soft labels, can transfer memorized, held-out information from a teacher to a student. The work makes several very interesting observations on this front, and its primary strength lies in the careful experimental design used to isolate this phenomenon. The modular addition experiments, for instance, provide a compelling and clean demonstration of the difference in transfer outcomes between a generalizing teacher and a non-generalizing one. This, combined with the analyses on i.i.d. data using logistic regression and MLPs, creates a strong, controlled environment to probe the core questions of this work. The paper is exciting because it surfaces several intriguing phenomena—such as the clear importance of soft labels highlighted by the shuffling experiment and the complex behaviors hinted at in figures like 4A—which all point toward deeper mechanisms at play in distillation. The initial visualization in Figure 1 is also a very effective and intuitive way to ground the reader in the central concept."}, "weaknesses": {"value": "The paper's primary weaknesses lie in the clarity of its presentation and the limited scope of the experiments.\n\n* **Clarity and Focus:** The paper presents a wide array of experimental setups (modular addition, logistic regression, MLPs, varying temperature, classes, $\\rho$, etc.) but struggles to synthesize them into a cohesive narrative. Many interesting results are condensed into dense paragraphs, making it difficult for the reader to dissect the core takeaways from each experiment.\n* **Suggestions for Focus:** The paper's impact could be significantly improved by streamlining the main body. The authors might consider focusing on a few core experiments and analyzing them more deeply. For example, a clearer narrative could be built around:\n    1.  The modular addition task, emphasizing the difference between generalizing and non-generalizing teachers.\n    2.  The multinomial logistic regression with ReLU MLPs with a more thorough analysis of the role of $\\rho$ (data fraction) and model scale (partially covered by the $\\alpha$ experiment) in these settings.\n    * Other analyses, such as the effect of the number of classes, seem less central and could be moved to the appendix to improve flow.\n* **Unclear Notation and Definitions:** The theoretical discussion is difficult to follow due to ambiguous notation. The symbols $\\alpha_{label}$ and $\\alpha_{id}$ are introduced, but their practical meaning and the distinction between the \"identifiability threshold\" and \"student memorization capacity\" are not clearly explained.\n    * To give an example, quoting: \"*identifiability threshold: The student can identify the teacher using the logits, measured through the mean squared error loss on the teacher logits, which occurs at $\\alpha_{id} = 1/\\rho$, as the input matrix X becomes invertible.*\"\n    * The phrasing \"the student can identify the teacher\" is imprecise. It would be clearer to describe the empirical observation, such as \"the student's predictions on the test set functionally match the teacher's predictions.\"\n    * Similarly, naming $\\alpha$ the \"sample complexity\" is not very intuitive; a term like \"inverse overparameterization factor\" might be more descriptive. A diagram illustrating the experimental setup and the relationship between these different $\\alpha$ thresholds would be highly beneficial.\n* **Figure Density:** Many figures, particularly Figure 4, are very dense and confusing. While Figure 4A seems to hint at a very interesting phenomenon, the key insight does not come through clearly from the visualization or the accompanying text.\n* **Lack of Scale:** The authors acknowledge this as a limitation, but it is a significant one. All experiments are conducted on very small models. Without even medium-scale experiments, it is unknown whether these observed behaviors generalize to the deeper, more complex architectures where distillation is commonly applied."}, "questions": {"value": "1.  Throughout the paper, the authors report the student's train, test, and validation accuracy. To be precise, are these accuracies calculated against the *original* ground-truth labels from the dataset, or against the teacher's (potentially incorrect) labels? The reviewer assumes the former, but it should be written explicitly.\n2.  The paper uses several terms to define the data partitions ( $\\mathcal{D}^{T}$ , $\\mathcal{D}_{train}^{S}$, etc.. ). A small diagram visualizing the relationship and partitioning of these datasets, perhaps in the appendix, would greatly aid in understanding the experimental setup.\n3.  The authors mention training models \"until convergence\" (e.g., in the context of Line 222). Could the authors please specify the exact convergence criteria used? For example, was it a fixed number of steps, a loss-based patience threshold, or another metric?\n4.  In a similar vein to the user note on Line 215, the phrasing \"the size of the complete data distribution\" is ambiguous. Are the authors referring to the cardinality of the sample space, or something else?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iMKz6kvQas", "forum": "lmVfTPQF3a", "replyto": "lmVfTPQF3a", "signatures": ["ICLR.cc/2026/Conference/Submission16940/Reviewer_h94i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16940/Reviewer_h94i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762127971322, "cdate": 1762127971322, "tmdate": 1762926965406, "mdate": 1762926965406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}