{"id": "aoEtzdRkGh", "number": 923, "cdate": 1756823667598, "mdate": 1763742681636, "content": {"title": "SPEED: Scalable, Precise, and Efficient Concept Erasure for Diffusion Models", "abstract": "Erasing concepts from large-scale text-to-image (T2I) diffusion models has become increasingly crucial due to the growing concerns over copyright infringement, offensive content, and privacy violations. In scalable applications, fine-tuning-based methods are time-consuming to precisely erase multiple target concepts, while real-time editing-based methods often degrade the generation quality of non-target concepts due to conflicting optimization objectives. To address this dilemma, we introduce SPEED, an efficient concept erasure approach that directly edits model parameters. SPEED searches for a null space, a model editing space where parameter updates do not affect non-target concepts, to achieve scalable and precise erasure. To facilitate accurate null space optimization, we incorporate three complementary strategies: Influence-based Prior Filtering (IPF) to selectively retain the most affected non-target concepts, Directed Prior Augmentation (DPA) to enrich the filtered retain set with semantically consistent variations, and Invariant Equality Constraints (IEC) to preserve key invariants during the T2I generation process. Extensive evaluations across multiple concept erasure tasks demonstrate that SPEED consistently outperforms existing methods in non-target preservation while achieving efficient and high-fidelity concept erasure, successfully erasing 100 concepts within only 5 seconds.", "tldr": "", "keywords": ["Diffusion Models", "Concept Erasure", "Model Safety"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e900f23b6a3aa451a0f936d4f5d6419e82c83e9d.pdf", "supplementary_material": "/attachment/f89a2af26a645cea2b148108cd57e4ec739eebba.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a SPEED framework for erasing unwanted concepts in diffusion models. Specifically, this paper aims to develop a scalable, precise, and efficient concept erasure method. By formulating the concept erasure task into a null-space constrained optimization problem, the proposed framework could effectively remove the unwanted concepts while successfully preserving the unrelated concepts. This paper also proposes auxiliary strategies, including Influence-based Prior Filtering (IPF), Directed Prior Augmentation (DPA), and Invariant Equality Constraints (IEC), to prevent severe semantic degradation and improve retent set coverage. The experiments are conducted on various concept erasure tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper aims to enhance multiple crucial aspects of concept erasing tasks, such as scaling the setting to handle large-scale multi-concept erasing, precisely removing the target concepts solely, and reducing computation time. The proposed method facilitates the development of trustworthy generative AI.\n2. The proposed SPEED framework is reasonable and technically sound.\n3. The overall paper is easy to follow and well organized."}, "weaknesses": {"value": "1. While this paper demonstrates the ability to effectively erasing the target concepts while preserving the unrelated concepts, it remains unclear whether the model could handle the paraphrased prompts, which is called robustness issues. It would lead to the target concepts being easily recovered by the paraphrase prompts with the same or similar semantics.\n2. In Figure 6, it appears that background changes in the output images. It seems removing the target concepts still affects the other parts of the images."}, "questions": {"value": "1. In Figure 6(b), this approach shows the ability to perform model knowledge editing to edit the target concept to a specific one. Can this ability be extended to multi-concept knowledge editing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IlMWHaNyaB", "forum": "aoEtzdRkGh", "replyto": "aoEtzdRkGh", "signatures": ["ICLR.cc/2026/Conference/Submission923/Reviewer_NrN5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission923/Reviewer_NrN5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760964986993, "cdate": 1760964986993, "tmdate": 1762915641886, "mdate": 1762915641886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a null space to enable multi-concept erasure while preserving non-targeted concepts. To minimize the impact on non-target concepts, it selects the most affected ones based on the prior shift, which quantifies how parameter updates perturb each non-target concept. Additionally, it augments the non-target concepts by perturbing them with directed noise, guiding their embeddings toward closer semantic representations. Furthermore, it imposes constraints on certain invariants during the T2I generation process. Experimental results show that the proposed method achieves superior performance in a short amount of time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The null space–based method demonstrates a strong ability to preserve untargeted concepts, and the closed-form solution offers high computational efficiency.\n2. The paper is well-organized. To enable accurate null-space construction, the authors use concept selection to reduce the rank of the correlation matrix. To address the resulting limited retain coverage, they further augment non-target embeddings using directed noise."}, "weaknesses": {"value": "The main weakness lies in the CLIP Score, a common metric for evaluating concept erasure. \nThe results show that SPEED does not achieve the lowest CLIP Score. Although the authors provide visualizations demonstrating that erasure remains effective without reaching the minimum score, some residual patterns can still be observed. For example, in the left figure of Fig. 7(c), the mouth remains similar after removal, indicating instability in the concept erasure process.\nAlso, the concept erasure results in Table 2 show that SPEED is less effective at removing target concepts compared to other methods."}, "questions": {"value": "Could adjusting the number of non-target concepts improve concept erasure performance and achieve a better trade-off between erasure and preservation of non-target concepts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YgpwZaY5Ux", "forum": "aoEtzdRkGh", "replyto": "aoEtzdRkGh", "signatures": ["ICLR.cc/2026/Conference/Submission923/Reviewer_oMhk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission923/Reviewer_oMhk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865254990, "cdate": 1761865254990, "tmdate": 1762915641752, "mdate": 1762915641752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SPEED, which resolves the issue of reduced generation quality of non-target concepts due to conflicts in optimization objectives through three modules, Influence-based Prior Filtering (IPF), Directed Prior Augmentation (DPA) and Invariant Equality Constraints (IEC). Extensive experiments have proved that the method is scalable, accurate and efficient."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written with clear presentations, and easy to follow.\n2. Extensive experiments show that SPEED consistently outperforms existing methods in prior preservation across various erasure tasks."}, "weaknesses": {"value": "1. The proposed method achieves a lower CS compared to RECE. The authors argue that an excessively low CS, as seen in RECE, results in \"over-erasure\", whereas the higher CS of SPEED is deemed \"adequate.\" How can it be proved that the residual concept has no negative impact on the erasing effect of the model?\n2. Is the IPF sensitive to the initial selection of the retention set? If the retention set is randomly sampled, is the IPF still valid?\n3. Please supplement the experiments to illustrate that SPEED has the lowest computational cost and high operational efficiency."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U2vcjInvJu", "forum": "aoEtzdRkGh", "replyto": "aoEtzdRkGh", "signatures": ["ICLR.cc/2026/Conference/Submission923/Reviewer_cb18"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission923/Reviewer_cb18"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880741688, "cdate": 1761880741688, "tmdate": 1762915641603, "mdate": 1762915641603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an efficient and effective concept erasure method, SPEED. Inspired from the null-space constraints, SPEED edits model weights within the null space of non-target concepts.\nHowever, as the number of non-target concepts grows, finding the appropriate null space becomes challenging. To address this, the paper introduces Influence-based Prior Filtering (IPF), which removes the minimally affected non-target concepts from the retain set. Then, to further enhance preservation, it introduces Directed Prior Augmentation (DPA), which augments the remaining non-target concepts with semantically consistent noise. Finally, the paper proposes Invariant Equality Constraints (IEC) to ensure that the [SOT] token and the null-text embedding remain unchanged during unlearning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides a theoretical analysis showing that the closed-form optimization used in UCE is not optimal, as its preservation error has a non-zero bound, which limits non-target preservation as the number of target concepts increases.\n- The proposed method is effective in non-target preservation and achieves a good balance between concept erasure and non-target preservation for multi-concept erasure.\n- The proposed method is efficient, which only needs 5 seconds to erase 100 concepts on a single A100 GPU.\n- The experiments are comprehensive, covering a wide range of domains including copyrighted content, artistic styles, celebrities, and nudity.\n- The ablation studies are detailed. The paper investigates which matrices in cross-attention layers should be modified, the effect of the filtering threshold in IPF, and the effect of augmentation times $N_A$ and ranks $r$ in DPA."}, "weaknesses": {"value": "- The proposed method is less effective in erasing target concepts. In the copyrighted content erasure results (Table 1 left), SPEED’s CLIP scores are much higher than those of prior methods RECE and MACE. Similarly, in the multi-concept erasure experiments (Table 2), SPEED’s erasure accuracies are also higher than RECE and MACE, indicating weaker erasure performance compared to these baselines."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6nkkuUm2CU", "forum": "aoEtzdRkGh", "replyto": "aoEtzdRkGh", "signatures": ["ICLR.cc/2026/Conference/Submission923/Reviewer_gLg4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission923/Reviewer_gLg4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981537850, "cdate": 1761981537850, "tmdate": 1762915641353, "mdate": 1762915641353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}