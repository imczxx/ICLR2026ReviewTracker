{"id": "uaTqxaRk9V", "number": 21099, "cdate": 1758313726238, "mdate": 1758814614211, "content": {"title": "Desirable Effort Fairness and Optimality Trade-offs in Strategic Learning", "abstract": "\\emph{Strategic learning} studies how decision rules interact with agents who may strategically change their inputs/features to achieve better outcomes. In standard settings, models assume that the decision-maker's sole scope is to learn a classifier that maximizes an objective (e.g., accuracy) assuming that agents will best respond. However, real decision-making systems' goals do not always align \\emph{exclusively} with producing good predictions. They may need to consider the downstream effects of inducing certain incentives, which translates into certain features being regarded as more \\emph{desirable} to change for the decision maker. \nNot only that, but the principal may also need to incentivize desirable feature changes equally across heterogeneous agents.\n\\emph{How much does this constrained optimization (i.e., maximize the principal's objective, while minimizing the disparity in terms of incentivizing desirable effort) cost the principal?} We propose a unified model of principal-agent interaction that captures this trade-off under three additional components: (1) causal dependencies between features, such that changes in one feature affect others; (2) heterogeneous manipulation costs across the agent population; and (3) peer learning, through which agents infer the principal's algorithm. We provide theoretical guarantees on the principal's optimality loss constrained to a particular desirability fairness tolerance for multiple broad classes of fairness measures. Finally, we demonstrate through experiments on real datasets an explicit tradeoff between maximizing accuracy and fairness in desirability effort.", "tldr": "We study strategic learning when learners must induce desirable incentives fairly. We show theoretical and empirical trade-offs between accuracy/social welfare and fairness under desirability constraints.", "keywords": ["strategic classification", "strategic learning", "fairness", "societal ML", "equilibrium analysis", "Stackelberg games", "causality", "peer learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/27ccbbe67399a10b805d2fbd1324f065a8f9c554.pdf", "supplementary_material": "/attachment/97563d520a297150725301eb7c34e085b8f2b759.zip"}, "replies": [], "withdrawn": true}