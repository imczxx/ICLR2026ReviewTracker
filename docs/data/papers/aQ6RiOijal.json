{"id": "aQ6RiOijal", "number": 14398, "cdate": 1758234475670, "mdate": 1759897372613, "content": {"title": "Tucker-KV: Provable Tucker Compression of KV Caches with Monotone Refinement and Near-Optimal Budgeting", "abstract": "Key-Value (KV) caches enable fast Transformer decoding but their memory and compute scale linearly with context length. Prior KV compression works are largely matrix low-rank heuristics, leaving multilinear guarantees underexplored. We present Tucker-KV, a Tucker-based framework with provable properties for compressing KV tensors over (L, S, H). Our analysis establishes: (i) HOSVD-style error upper bounds and monotone refinement via HOOI; (ii) grouped-head separability enabling parallelizable compression; (iii) a (1-1/e) guarantee for greedy budget allocation under mild DR-submodularity; and (iv) robust residual mixing with matrix baselines that never increases error when Tucker fits the residual in least squares. We further characterize the budget regime where Tucker-2 is preferable to full Tucker. On Qwen2.5-7B with RULER at 4k, Tucker-KV matches Full-KV quality (EM/F1 ~ 1.00) while saving 83% KV memory, with perplexity unchanged and favorable prefill throughput. Importantly, Tucker-KV is orthogonal to token-selection methods (sliding/streaming/xKV) and can be stacked with them; our focus is the representation-compression axis with provable monotone refinement and near-optimal budget allocation.", "tldr": "", "keywords": ["KV Cache", "Tucker Decomposition", "Learning to Compress"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/de35c74c032df750326534fd37fe1e505b3f9460.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper analyzes KV Cache from the tensor perspective, where $X \\in \\\\mathbb{R}^{L \\times S \\times H}$. The authors use Tucker compression technique to approximate the original $X$ to reduce the memory size of the kv cache, and they have proven the error convergence (Proposition 1). Another contribution is the authors prove that each HOOI iteration never increases the reconstruction error (Proposition 2), which gives a monotone refinement guarantee for support their Tucker approximation convergence."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "$\\bullet$ Tensor version of KV Cache analysis: This paper addresses the KV cache compression from the tensor perspective, and the authors have proven the multi-linear error control and monotone reconstruction error across iterations. Tensor version gives us more information and more complicated analysis in comparison with the matrix version. \n\n$\\bullet$ Flexible Design: works per head-group can be stacked on top of a strong matrix-SVD baseline via residual mixing which won't increase error. \n\n$\\bullet$ Good performance: TuckerKV achieves large KV memory reductions and keeps task quality on long-context evaluations"}, "weaknesses": {"value": "$\\bullet$ Unrealistic Assumption: In Proposition 5, the authors seems to derive the global optimality based on tensor $X$ is block-orthogonal along the $H$-mode (eg. attention heads) on Line 574. In practice, I think heads are not orthogonal empirically. In [1], it states '' Our novel pruning methods removes the vast majority of heads without seriously affecting performance.'' Moreover, In [2], it states \" most attention heads can be individually removed after training without any significant downside in terms of test performance\". If heads group were orthogonal, you wouldn't be able to to prune most of them with a small loss. Therefore, I think the condition in your Proposition 5 is unrealistic. \n\n$\\bullet$ Poor presentation and organization: This paper is structured differently from most publications at top ML conferences (NeurIPS, ICLR, ICML, etc.). The introduction is broken into too many sections (“This paper”/“Contributions”/“Scope”/“Design rationale”), and the structure is not logical, which makes it harder for the reader to understand what the authors are trying to say. For instance, a better structure could follow: Background → Problem → Gap → Approach → Contributions → Evidence and Implications. The authors could also formulate a clear research question so readers can quickly understand the problem they are trying to solve.\n\nAs far as I understand this paper, one of the main contributions is theoretically proving the convergence of the tensor-version KV-cache Tucker compression, and the empirical results validate the theoretical findings. I think it will be presented better if author can include a main theorem or proposition in the main body rather than putting all theoretical findings in the appendix. The Related Work section should follow the Introduction, and a preliminaries section providing background information typically exists. it is not mandatory to structure a paper like this, but the authors should structure the paper in a logical way so readers can easily follow. I strongly recommend the authors check other publications in related work to improve the overall presentation of this paper.\n\n\n[1] Voita, Elena, et al. \"Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned.\" ACL'19.\n\n[2] Michel, Paul, Omer Levy, and Graham Neubig. \"Are sixteen heads really better than one?\" NeurIPS'19."}, "questions": {"value": "$\\bullet$ Can author explain why the assumption in Proposition 5 is realistic? \n\n$\\bullet$ Does the main result still hold when the assumption in Proposition 5 is removed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h4lnQXm7om", "forum": "aQ6RiOijal", "replyto": "aQ6RiOijal", "signatures": ["ICLR.cc/2026/Conference/Submission14398/Reviewer_HxxC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14398/Reviewer_HxxC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807810935, "cdate": 1761807810935, "tmdate": 1762924810339, "mdate": 1762924810339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Tucker-KV, a method for compressing the key–value caches in Transformers using a Tucker tensor decomposition. Experiments on Qwen2.5-7B with the RULER benchmark report roughly 83% KV memory savings without loss in task accuracy or perplexity, suggesting the method could be an effective drop-in compression scheme."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The idea of using Tucker decomposition for KV-cache compression seems novel, compared to previous approaches that only considers low-rank decomposition using SVD (and not high-order SVD for Tucker decomposition). The approach appears compatible with other KV efficiency strategies such as windowed or sparse strategies."}, "weaknesses": {"value": "Overall I feel that the paper could gain in clarity by further reinforcing the narrative and the motivation at different stages of the paper. Concretely:\n\n* The paper could benefit from greater clarity by properly introducing KV caches at a high level. In particular, it does not motivate why KV compression should work: the reader may wonder what structural properties of KV tensors make them amenable to compression, or whether a low-rank structure is theoretically justified or empirically observed. Without this context, it is difficult to understand why Tucker decomposition is an appropriate modeling choice rather than an arbitrary compression technique.\n* Section 2.2 on online budget allocation is difficult to follow and appears insufficiently motivated at this point in the paper. It is introduced abruptly, before the reader fully understands the problem setting or why such an allocation mechanism is needed. A brief intuitive explanation or contextual motivation earlier on would make this section much easier to grasp.\n* The contribution paragraph in the introduction is currently a list rather than a cohesive narrative. I suggest the authors rework it to better articulate and emphasize the paper’s key novelties.\n* I feel that some technical steps are underspecified, e.g., it is unclear whether the method uses a full Tucker or Tucker-2 decomposition to do KV-cache compression. \n* In section 2.7, it is not clear how the truncated SVD to compute $X_k$ is performed (on which flattened dimensions), or why applying Tucker to the residual $R = X - X_k$ is expected to work. \n* The authors could make the description of the experimental protocol more self-contained by describing a little more the RULER benchmark, along with the concept of \"needle\", and why this benchmark is challenging and relevant to study KV cache compression. Otherwise, the reader might be confused about the specific choice of this dataset to validate the method.\n* The set of baselines in the experimental protocol feels incomplete. While comparisons to sliding-window and sparse methods are informative, they are not the most relevant since, as the authors themselves note, these approaches are orthogonal to Tucker-KV. A more appropriate evaluation would include baselines from the same family of KV-compression methods, such as standard low-rank or SVD-based approaches, which are currently missing from the submission. Without such a comparison, it is not clear whether the proposed approach is relevant in practice.\n* Table 1 reports comparisons on Qwen2.5-7B, but for Llama 3.1 only the Tucker-KV results with a compression ratio of 1.0 (meaning 0% KV saved) are shown. It is unclear why results for other compression settings or baseline methods are missing, which makes the comparison appear incomplete.\n* Overall the paper did not discuss the reconstruction error $ \\|X - \\hat{X}\\| $ where $X$ is the full KV cache encountered in practice during inference, and $\\hat{X}$ is its compressed version. In particular, this leaves unclear how approximation quality relates to model performance during inference.\n* All the theoretical results are presented only in the appendix and not discussed in the main text. This may leave readers uncertain about the relevance and role of these results within the overall contribution of the paper.\n* The authors mention using HOSVD and HOOI to perform the Tucker decomposition, but it is not clearly stated whether this constitutes a new algorithmic contribution or simply the application of existing methods from the literature. The lack of explicit references makes this point ambiguous."}, "questions": {"value": "1. What structural properties of KV tensors justify a low-rank tensor approximation?  \n2. When exactly is the KV cache compressed during decoding, and how is it updated afterward?  \n3. How do you choose between Tucker and Tucker-2 in practice?  \n4. On which dimensions is the cache flattened when performing the SVD for \\(X_k\\) in Algorithm 2?  \n5. Why should applying Tucker to the residual guarantee improvement?  \n6. Why are there no comparisons with other KV-compression methods (e.g., SVD or CP decomposition)?  \n7. Could you report the reconstruction error and discuss its link to downstream metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fJ189enfK9", "forum": "aQ6RiOijal", "replyto": "aQ6RiOijal", "signatures": ["ICLR.cc/2026/Conference/Submission14398/Reviewer_wJa2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14398/Reviewer_wJa2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835015304, "cdate": 1761835015304, "tmdate": 1762924809853, "mdate": 1762924809853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Tucker-KV, a method for compressing transformer KV caches using Tucker tensor decomposition. Unlike prior works that flatten the cache into a matrix and apply SVD, Tucker-KV treats the cache as a 3D tensor (hidden × sequence × head) and compresses along all modes. The authors provide comprehensive theoretical analysis to justify Tucker’s use in KV compression, offering provable guarantees such as monotone refinement and error bounds. While the theoretical exposition is strong, the experimental scope is limited. Experiments on Qwen2.5-7B and Llama-3.1-8B show that Tucker-KV maintains full model quality while saving up to 83% of KV memory, but only under short-context RULER tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Originality:** Applying Tucker decomposition to explore the low-rank structure of KV caches across multi-head and multi-layer architectures is an insightful and novel direction.\n\n**Theoretical foundation:** The paper provides solid theoretical grounding, showing Tucker’s better reconstruction error bound compared to matrix SVD and offering proofs for refinement and safety properties."}, "weaknesses": {"value": "Despite the promising idea, several critical evaluation and design aspects are missing:\n\n+ **No runtime or cost analysis:** The paper does not measure the actual computational or latency cost of Tucker compression. There are no wall-clock timings, FLOPs, or GPU utilization reports for prefill or decoding, making it unclear how expensive Tucker-KV is in practice.\n\n+ **No direct comparison with SVD-only baseline:** Although Tucker-KV includes a residual SVD path and proves that the combination is guaranteed not to degrade accuracy, the paper provides no experimental results directly comparing SVD-only, Tucker-only, and hybrid variants at equivalent compression ratios. Without this, it is difficult to assess Tucker’s practical advantage over well-established SVD methods.\n\n+ **Limited evaluation scope:** The experiments are confined to 4k-context RULER benchmarks. At such short contexts, KV caches contribute only a small portion of total memory, meaning compression is not yet critical. The paper explicitly excludes long-context stress tests (e.g., 16k–128k), where KV compression truly matters, labeling them as out of scope. This omission weakens the empirical significance of the work.\n\n+ **Lack of inference integration details:** The paper does not explain how Tucker-compressed KV caches are used during real inference. For example, does Tucker-KV only apply to prefill caches? Are the decomposed tensors reconstructed for every decoding step? How are the decomposed factors accessed in attention computation? These are crucial practical questions for determining whether Tucker-KV can actually save time or memory in live inference."}, "questions": {"value": "1. Can you provide runtime and latency analysis for Tucker-KV, including prefill and decoding stages, to quantify the overhead introduced by Tucker decomposition compared to standard SVD and Full-KV? It might be good to test it with a different context size. \n\n2. How exactly is the Tucker-compressed cache integrated into the inference process? Is the KV tensor reconstructed before each attention operation, or are the decomposed factors used directly during decoding?\n\n3. Can you include a direct comparison among SVD-only, Tucker-only, and the proposed hybrid residual design at the same compression ratio, to demonstrate whether Tucker provides measurable practical gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iuYAhoHW2f", "forum": "aQ6RiOijal", "replyto": "aQ6RiOijal", "signatures": ["ICLR.cc/2026/Conference/Submission14398/Reviewer_irCz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14398/Reviewer_irCz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962565653, "cdate": 1761962565653, "tmdate": 1762924809355, "mdate": 1762924809355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a KV cache compression framework based on Higher-Order SVD. It provides theoretical guarantees for this compression method and empirically demonstrates that it achieves a favorable trade-off on RULER@4k using the Qwen2.5-7B model."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Provides a theoretical error bound for the KV cache compression method.\n- Proposes a practical online budgeting mechanism."}, "weaknesses": {"value": "**[W1]** Limited empirical validation: The experiments are primarily conducted on the Qwen2.5-7B model. Furthermore, only very basic KV cache compression methods are used as baselines. Although the paper states that the focus is on the representation-compression axis, there exist much more advanced KV cache representation compression methods that should be compared against, such as xKV (https://arxiv.org/abs/2503.18893), KIVI (https://arxiv.org/pdf/2402.02750), and GEAR (https://arxiv.org/abs/2403.05527).\n\n**[W2]** The paper is difficult to follow:\n- It is not immediately clear which dimensions (L, S, H) refer to which component from the abstract without further description.  \n- The font size in Figure 1 is too small, making it difficult to read."}, "questions": {"value": "**[Q1]** L102: What does the success flag represent? The success of which process?\n\n**[Q2]** How would the method perform on reasoning models such as the Qwen3 model family for long-generation tasks like AIME or LiveCodeBench? Demonstrating this would highlight the method’s robustness under long-generation scenarios, which are not captured by long-context retrieval tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6jmzGddiSx", "forum": "aQ6RiOijal", "replyto": "aQ6RiOijal", "signatures": ["ICLR.cc/2026/Conference/Submission14398/Reviewer_atDj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14398/Reviewer_atDj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966014026, "cdate": 1761966014026, "tmdate": 1762924808949, "mdate": 1762924808949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}