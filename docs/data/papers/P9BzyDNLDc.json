{"id": "P9BzyDNLDc", "number": 19411, "cdate": 1758296012387, "mdate": 1759897040366, "content": {"title": "Semantic Structure in Large Language Model Embeddings", "abstract": "Psychological research consistently finds that human ratings of words across diverse semantic scales can be reduced to a low-dimensional form with relatively little information loss. We find that the semantic associations encoded in the embedding matrices of large language models (LLMs) exhibit a similar structure. We show that the projections of words on semantic directions defined by antonym pairs (e.g. kind - cruel) correlate highly with human ratings, and further find that these projections effectively reduce to a 3-dimensional subspace within LLM embeddings, closely resembling the patterns derived from human survey responses. Moreover, we find that shifting tokens along one semantic direction causes off-target effects on geometrically aligned features proportional to their cosine similarity. These findings suggest that semantic features are entangled within LLMs similarly to how they are interconnected in human language, and a great deal of semantic information, despite its apparent complexity, is surprisingly low-dimensional. Furthermore, accounting for this semantic structure may prove essential for avoiding unintended consequences when steering features.", "tldr": "Semantic features have substantial geometric alignment in LLM embeddings that reflect human associations and result in predictable off-target steering effects.", "keywords": ["Large language models", "interpretability", "steering"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b3328480b282ad42391b138e588d91ec51b18bbd.pdf", "supplementary_material": "/attachment/59a39eb19ff69b16c9ceffa1ec35318506723e50.zip"}, "replies": [{"content": {"summary": {"value": "This paper explores the semantic organization of token embeddings in large language models (LLMs) through a cognitive science perspective. The authors present an interesting analysis revealing two main findings: (1) token embeddings show alignment with human semantic ratings, and (2) there is a correlation between embedding features and off-target effects in embedding steering."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and clearly structured. The experimental design is solid and supports the authors’ claims effectively. The findings are interesting and open promising research directions at the intersection of cognitive science and large language models."}, "weaknesses": {"value": "The visualizations could be clearer. For example, the correlation matrices presented in the appendix are difficult to interpret and compare in their current form.\n\nMoreover, the paper’s practical impact appears limited. While the results are conceptually interesting, it remains unclear how these insights could be applied to improve LLM architectures or their downstream performance."}, "questions": {"value": "The authors write:\n\n> Whitening makes the token cloud isotropic and de-correlates directions that were previously allowed to share variance; human evaluative judgments, however, draw on that shared variance, and thus removing these overlaps reduces the representation’s fidelity to psychological and cultural associations.\n\nWhile this interpretation is supported by the results, whitening might also have a regularization effect that improves performance in LLMs. Without targeted experiments, it is difficult to disentangle these two effects. It would be helpful to acknowledge this possibility in the paper.\n\nAdditionally, in Figure 2 and throughout the appendix, the correlation matrices comparing token projections across various models are hard to visually compare. It would strengthen the analysis to include a single aggregate metric summarizing how well these correlations align with human semantic data. A comparison across embedding dimensionality or model size would also provide further value to the analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zen60lyBor", "forum": "P9BzyDNLDc", "replyto": "P9BzyDNLDc", "signatures": ["ICLR.cc/2026/Conference/Submission19411/Reviewer_3DvR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19411/Reviewer_3DvR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761329255609, "cdate": 1761329255609, "tmdate": 1762931327659, "mdate": 1762931327659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper empirically verifies that the spatial arrangement of LLM embeddings aligns with low-dimensional psychological models of meaning, and demonstrates that the geometric alignment (non-orthogonality) of semantic features predicts intervention side effects."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is highly readable and the topic of understanding how LLMs represent meaning, particularly in relation to human cognitive models (Evaluation, Potency, Activity), is of broad interest."}, "weaknesses": {"value": "**TL;DR**: While the paper may be of interest to a more tailored audience, it seems insufficiently substantive for a top-tier machine learning conference. No new theoretical insight is offered as to how semantics are encoded, rather it provides further empirical evidence for the well-known principle that the geometric arrangement of embeddings (angles/vector addition) encodes semantic meaning.\n\nThe central finding that feature \"entanglement\" leads to predictable off-target effects is fundamentally appears to be the expected mathematical consequence of performing linear manipulations in a non-orthogonal feature space. While the authors claim previous studies treated these effects as random, the mechanism seems to confirm the precise problem that prior work in causal interpretability (e.g., Park et al.) sought to solve by defining causally separable concepts.\n\nFurthermore, the paper’s argument against minimizing feature correlation via techniques like the whitening operation constitutes a potentially misdirected comparison (a \"straw-man argument\"), as related literature on feature steering aims to orthogonalize causally separable concepts, not necessarily every possible semantic dimension en masse. Thus naively orthogonalising is perhaps not a good idea and it would be of interest to consider which concepts can/can't be orthogonalised. \n\nThe conclusion that \"the representation of semantic associations is relatively low-dimensional in LLM embeddings\" seems an unsubstantiated generalization. This dimensionality reduction analysis (PCA) was performed only on 28 predetermined scales across a restricted sample of 301 words (tokens), making extrapolation to the full, high-dimensional LLM semantic space unwarranted.\n\n**Detailed points**:\n - referencing is incorrect: use \"citep\" so that references are in brackets (see sample ICLR paper)\n - the paper would be clearer if it explicitly defined the concept of \"almost orthogonal\" vectors. \n - the mechanisms discussed (e.g., superposition at line 110) should include a clearer explanation of why a space of dimension n can represent ≫n \"nearly orthogonal\" vectors. \n - 132: the reason word2vec received attention was also (if not more) due to analogy solvin by vector addition, not only \"angles between features\".\n - 199/208: The origin and grouping of the antonym pairs used to derive the 28 semantic axes are unclear. \n - 229-240: unclear - is this section a segue/intro to the experiments below, if so this could be more clear and give section references\n - 247-251: this is confusing. If the probability over the \"next token\" is taken (253), why the \"Assistant\" part of the prompt?\n - 270: it would be clearer and more reproducible to give an explicit formula for the intervention vector etc.\n - 302: Park et al don't argue for orthogonalising arbitrary concepts (as above) so, this comparison is not necessarily meaningful.\n - Panels A, B, D: it is unclear what to take from this or the logic of the argument. Why is it good/bad that word embeddings are partly aligned with several reference vectors?\n - Panel C: the scale perhaps makes this misleading. Presumably the diagonals have value 1? which is off the (-0.3, 0.3) scale? All values are relatively small (relative to the diagonal) and it would be clearer if the scale reflected this clearly. Also, there doesn't seem any accounting for the phenomenon that in high dimensions, random vectors are more likely to be orthogonal, so orthogonality itself is less \"surprising\"/meaningful.\n - 363: a histogram of values would be more clear than the heatmap (but, as above, perhaps with reference to the distribution one would expect for random vectors).\n - Section 5: this section, describing the intervention experiment, is concise but unclear and could be better placed or clarified to enhance readability."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NUdT4xwOLU", "forum": "P9BzyDNLDc", "replyto": "P9BzyDNLDc", "signatures": ["ICLR.cc/2026/Conference/Submission19411/Reviewer_1Pp7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19411/Reviewer_1Pp7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771891020, "cdate": 1761771891020, "tmdate": 1762942225655, "mdate": 1762942225655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uses well-established research in psychology about human semantic scales to study the internal semantics encoded in the embedding matrices of LLMs. These semantic scales experiments consist of mapping words to key semantic axes, such as “kind-cruel”. The authors consider 28 such axes (which come from the original experiment in social psychology).  \n\nFirst, the authors find that the feature directions from the embedding matrices corresponding to these 28 axes correlate well with the human ratings of the words (on these various semantic scales).\n\nSecond, the authors find that these LLM embedding matrices can also be “summarized” with low-dimensional matrices and that this 3-dimensional subspace roughly corresponds to the three latent dimensions of Evaluation, Potency, and Activity, that were found for humans decades ago. They do so by applying PCA to the embedding matrix.\n\nThird, the authors study the question of model behavior and steering through these same matrices. They test whether intervening on one feature (e.g., “soft-hard”), will also have an effect on other axis, where the intervention is carried out on the model’s token embeddings. They find that the magnitude of these off-target effects is proportional to the cosine similarity between the vectors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It is important to study how LLMs understand semantics and how their internal (low-dimensional) representations work. Using psychology studies and well-established theories on human semantics is a valuable way of doing so, and these kinds of interdisciplinary approaches seem particularly relevant\n- The different kinds of experiments nicely build on each other and feel like a natural progression. First they study the correlation between the human ratings and token embeddings, then they perform PCA and go into low-dimensions (specifically, into a 3-dimensional solution), and lastly they try to steer the model and predict off-target effects. \n- It is a good idea to test the “whitened” embeddings method in order to ensure that the experimental results are more robust \n- The paper is well-written and the authors clearly have a great command of the social psychology literature at hand\n- The experiments are tested across multiple different LLMs"}, "weaknesses": {"value": "I have three main criticisms of the framework and experimental set-up of the paper.\n- First, the paper only focuses on the LLM static embedding matrices, without taking the activations into account. The authors justify this in page 2, saying that “embedding and unembedding matrices also warrant attention”, but it seems too simplifying of an assumption to forget about the context and the actual transformer architecture of the LLM. Particularly because all of the experiments are run on LLMs, the context matters a great deal. \n- Second, and related to the first point, it seems like the methods of the paper are meant to apply to older word embedding approaches like word2vec, but aren’t really updated to today’s architectures. In order words, the experiments are run on LLMs, but the framework of the paper and the set-up of “good-bad” pairs of antonyms seems to be taken directly from the BERT era. \n- Third, I think that Figure 3 isn’t fully demonstrating the conclusions that are claimed. Specifically, the Evaluation, Potency, and Activity separation isn’t that convincing, and the authors also observe this by saying that “the second component admittedly does not correspond to Potency”. Moreover, the results for all of the figures are not really generalizing across different LLMs."}, "questions": {"value": "- See the three main points made in the “weaknesses” section\n- Isn’t it clear a priori that the off-target effect is proportional to the cosine similarity? Why is this an important finding?\n- Given the initial motivations of the Evaluation, Potency, and Activity studies, which were found to generalize across languages, have you thought about replicating your studies to LLMs trained with other languages other than English?\n- Why has the code not been included as part of the submission?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Q5hyyCW3se", "forum": "P9BzyDNLDc", "replyto": "P9BzyDNLDc", "signatures": ["ICLR.cc/2026/Conference/Submission19411/Reviewer_mumt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19411/Reviewer_mumt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967672968, "cdate": 1761967672968, "tmdate": 1762931326969, "mdate": 1762931326969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}