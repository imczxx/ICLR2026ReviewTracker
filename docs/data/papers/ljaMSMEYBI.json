{"id": "ljaMSMEYBI", "number": 23640, "cdate": 1758346672296, "mdate": 1759896803731, "content": {"title": "Deep Neural Networks Divide and Conquer Dihedral Multiplication", "abstract": "We find multilayer perceptrons and transformers both learn an instantiation of the same divide-and-conquer algorithm and solve dihedral multiplication with logarithmic feature efficiency. Applying principal component analyses to clusters of neurons where neurons have similar activation behavior reveals remarkably clear structure: *the neural representations correspond to Cayley graphs.* It's noteworthy that these Cayley graphs are the low-dimensional manifolds networks are predicted to learn under the manifold hypothesis, and thus each neural representation learned by networks corresponds to a manifold. To our knowledge, this is the first work that gives closed-form equations describing how every point in the dataset maps onto each of the possible neural representations that can be learned.", "tldr": "We show deep neural networks learn different implementations of one divide and conquer algorithm across random seeds.", "keywords": ["mechanistic interpretability", "neural representations", "manifold hypothesis", "group multiplication", "divide and conquer", "cayley graph"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/87a1455c411b5ab752884b481f27eb6f4990fd8b.pdf", "supplementary_material": "/attachment/de0c02e1c025dd52a7eee16fadc8cf3cb86225e9.zip"}, "replies": [{"content": {"summary": {"value": "The paper provides evidence for the universality hypothesis by demonstrating that both multi-layer perceptrons (MLPs) and transformers learn the same abstract divide-and-conquer algorithm to solve dihedral group multiplication. The authors employ a methodology using the Group Fourier Transform (GFT) to cluster neurons with similar activation patterns. Subsequent Principal Component Analysis (PCA) on these clusters reveals that the network's distributed neural representations form distinct, geometric structures identified as Cayley graphs. The global algorithm then works by combining the outputs of these different Cayley graph representations—each solving a simpler subproblem—to maximise the logit for the correct answer."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel Perspective on Neural Representations: The shift from analysing single neurons to the emergent structures formed by a neuron cluster is more appropriate under the superposition hypothesis. This goes beyond prior work that modelled individual neuron activations with sinusoidal functions to provide evidence for discrete distributed representation, showing how sinusoidal components collectively build a higher-level geometric object (a Cayley graph).\n2. Strong Methodology: The task of dihedral multiplication is rigorously analysed from a theoretical perspective to arrive at an appropriate hypothesis for the behaviour of these deep neural networks. The subsequent implementation of GFT and PCA provides compelling evidence that the proposed algorithm is being implemented. Furthermore, the replication of results across different architectures, random initialisations, and numerous (i.e., different values of n) problem instances strongly supports the claim."}, "weaknesses": {"value": "1. Claim of Universality: Although the authors consider multi-layer perceptrons and transformers, the consideration of more fine-grained architectural choices (i.e., depth, width, activation function) is not considered. To provide comprehensive evidence for the universality hypothesis, the effects of these architectural choices, as well as optimiser, training hyperparameters, etc, would need to be considered.\n2. Theoretical Justification: There is little discussion as to why deep neural networks learn this particular algorithm compared to other algorithms for performing dihedral multiplication. \n3. Presentation: A significant portion of the paper is devoted to introducing the problem, reviewing related work, and presenting figures. It would perhaps be more appropriate to provide the results of various ablation studies to support the claim of universality."}, "questions": {"value": "1. Does the emergence of this algorithm appear at a particular point in training, say as a grokking effect?\n2. How do other architectural factors – such as activation function, depth and width – influence the emergence of the divide and conquer algorithm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8RoDlx7mJl", "forum": "ljaMSMEYBI", "replyto": "ljaMSMEYBI", "signatures": ["ICLR.cc/2026/Conference/Submission23640/Reviewer_EEmm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23640/Reviewer_EEmm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761410300225, "cdate": 1761410300225, "tmdate": 1762942741687, "mdate": 1762942741687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates how deep neural networks (MLPs and transformers) learn to perform multiplication in the dihedral group, which is \na non-commutative finite group representing the symmetries of an n-gon.\n\nThe authors claim that neural networks learn a divide-and-conquer algorithm for this operation. \nTheir internal representations correspond to Cayley graphs and coset structures of the group. This supports a “universality hypothesis”: networks trained on algebraic tasks consistently discover similar algorithmic structures, \nregardless of architecture or random seed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Very interesting approach to gain insight into learning\n- The “divide-and-conquer” hypothesis is quite reasonable and insightful\n- Comprehensive experimentation: many thousands of training runs were studied\n- High reproducibility: many experiments over random seeds\n- The manuscript is well written"}, "weaknesses": {"value": "The paper is essentially an exploratory analysis rather than a hypothesis-driven study. \nNo explicit, testable hypotheses are formulated or quantitatively evaluated.\nMost conclusions rest on visual inspection of PCA- and Group Fourier Transform -derived figures, whose interpretations are ambiguous.\nIt remains unclear whether these structures reflect genuine inductive biases or are artifacts of the analytical lens itself - especially given that the Group Fourier Transform already encodes the group structure being “discovered.”\nThis raises a risk of circular reasoning.\nTo be more convincing, the authors should (i) define explicit hypotheses about what structures should appear under what conditions, (ii) quantify the strength or prevalence of these patterns statistically, and (iii) include negative or counterexample experiments where no such structure is expected.\nSuch controls would clarify whether the observed patterns genuinely reflect learned algorithmic structure rather than the setup or analysis.\n\nIn this sense, the approach is a visually appealing first step, but scientifically, it is wholly unclear what the results even mean."}, "questions": {"value": "1. Could the authors formulate explicit, testable hypotheses about what structures (e.g., Cayley graphs, cosets, scaling laws) should appear under specific conditions?\n2. How would those hypotheses be falsified -- what results would not support the proposed divide-and-conquer or universality interpretation?\n3. Have the authors considered applying the same training and analysis pipeline to a problem where such structured behavior is not expected (e.g., random multiplication tables, corrupted group laws, or non-algebraic mappings)?\n4. Would the same Cayley-graph patterns and O(log n) scaling appear in those cases?\n5. If so, how would that affect the interpretation of the current findings?\n6. Given that the Group Fourier Transform is defined directly in terms of the dihedral group’s representation theory, how can the authors ensure that the observed structure is not a byproduct of analyzing activations in a basis already aligned with that structure?\n7. Can the authors provide quantitative metrics (e.g., distances between embeddings and Cayley-graph adjacency matrices, reproducibility statistics across seeds) rather than relying primarily on visual inspection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yszqQQki9Q", "forum": "ljaMSMEYBI", "replyto": "ljaMSMEYBI", "signatures": ["ICLR.cc/2026/Conference/Submission23640/Reviewer_RG3S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23640/Reviewer_RG3S"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735411673, "cdate": 1761735411673, "tmdate": 1762942741394, "mdate": 1762942741394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how deep neural networks (DNNs) perform the *dihedral* group operation (the group of symmetries of a regular polygon, which includes rotations and reflections). By visualizing the neural activation using a frequency-based remapping, the authors argue that DNNs learn neural representations that correspond precisely to Caley graphs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- First comprehensive mechanistic analysis of neural networks trained on non-commutative group tasks.\n- The experiments include hundreds to thousands of random seeds, showing robustness across architectures (MLP vs transformer).\n- Results scale across orders of magnitude (2-512), demonstrating the claimed logarithmic feature efficiency."}, "weaknesses": {"value": "- The experimental setup is incompletely described. For example, it is not stated how many layers are used for the MLPs and Transformers, how the sequences in the language are sampled for transformers, etc.\n- The novelty of this work is not properly discussed in the context of existing work. Specifically, the algorithms learned by DNNs to perform group operations have been already studied by [1]. The current work focuses specifically on the non-commutative dihedral group, but this appears to be just a subcase of [1] since every group is isomorphic to a subgroup of a symmetric group (Cayley’s Theorem). It is not clear if the current work has any additional insights compared to [1].\n- The algorithm learned by DNNs to perform the dihedral group operation is not explained with sufficient detail and clarity. The discussion in section 5 relies heavily on cosets and the Chinese Remainder Theorem (CRT), but these concepts are not adequately introduced.\n- The \"algorithm\" recovered by the authors is quite abstract. The authors do not say how the networks weights are able to compute the Caley graph representations. \n\n1. Chughtai, Bilal, Lawrence Chan, and Neel Nanda. \"A toy model of universality: Reverse engineering how networks learn group operations.\" International Conference on Machine Learning. PMLR, 2023. https://arxiv.org/pdf/2302.03025"}, "questions": {"value": "- Is the Group Fourier Transform applied layer-wise or neuron-wise?\n- How are clusters defined quantitatively (e.g., thresholding on frequency similarity, correlation distance)?\n- Could the same procedure be applied to arbitrary networks without access to group labels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QISAoI2vwW", "forum": "ljaMSMEYBI", "replyto": "ljaMSMEYBI", "signatures": ["ICLR.cc/2026/Conference/Submission23640/Reviewer_oD9U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23640/Reviewer_oD9U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928852104, "cdate": 1761928852104, "tmdate": 1762942741115, "mdate": 1762942741115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work is centered around *how* feed forward and transformer neural networks learn dihedral multiplication. The authors present an argument for how the cyclic nature of dihedral multiplication presents an interesting testbed. They follow this up studying the activations and their principal components. This study reveals that the manifold of learned neural representations correspond to Cayley graphs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The PCA approach to neural representation for their test bed of learning group multiplication in the dihedral group $D_n$ is novel and very interesting. I found the exposition instructive, given some background. Their results align perfectly with the hypothesis in toy setup. They position their work in light of previous work very well. They compare to literature on grokking and interpretability using analytically generated datasets. I found the result in section 5: neural networks learn $O(\\log n)$ algorithm for group operation to be very interesting."}, "weaknesses": {"value": "One challenge I faced while reading the work was the background. I believe the authors can present the following topics in the main body of the paper:\n1. Group Fourier Transform: the authors use and re-use this idea throughout but the background is relegated to the Appendix. I believe a brief intro would benefit all readers with varying levels of familiarity with Fourier analysis.\n\n2. An example of a coset (e.g. all with + sign or - sign) would help a reader like me.\n\n3. I am not sure how the learning dynamics fit into the picture? Since the problem is motivated from a Grokking perspective, which has to do with learning dynamics, I would be interested in at what stage in their training do NNs Grok these divide and conquer algorithms.\n\n------------------------------------------\n\nMinor issues:\n\nLine 101-103: I would use citep here to put all the references in ()\n\nLine 432-433: broken reference \"??\""}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "g2wqOxpM4b", "forum": "ljaMSMEYBI", "replyto": "ljaMSMEYBI", "signatures": ["ICLR.cc/2026/Conference/Submission23640/Reviewer_NHvf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23640/Reviewer_NHvf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937922968, "cdate": 1761937922968, "tmdate": 1762942740849, "mdate": 1762942740849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}