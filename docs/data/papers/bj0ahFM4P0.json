{"id": "bj0ahFM4P0", "number": 16212, "cdate": 1758261751102, "mdate": 1763714157127, "content": {"title": "ExpVid: A Benchmark for Experiment Video Understanding & Reasoning", "abstract": "Multimodal Large Language Models (MLLMs) hold promise for accelerating scientific discovery by interpreting complex experimental procedures. However, their true capabilities are poorly understood, as existing benchmarks neglect the fine-grained and long-horizon nature of authentic laboratory work, especially in wet-lab settings. To bridge this gap, we introduce ExpVid, the first benchmark designed to systematically evaluate MLLMs on scientific experiment videos. Curated from peer-reviewed video publications, ExpVid features a new three-level task hierarchy that mirrors the scientific process: (1) Fine-grained Perception of tools, materials, and actions; (2) Procedural Understanding of step order and completeness; and (3) Scientific Reasoning that connects the full experiment to its published conclusions. Our vision-centric annotation pipeline, combining automated generation with multi-disciplinary expert validation, ensures that tasks require visual grounding. We evaluate 19 leading MLLMs on ExpVid and find that while they excel at coarse-grained recognition, they struggle with disambiguating fine details, tracking state changes over time, and linking experimental procedures to scientific outcomes. Our results reveal a notable performance gap between proprietary and open-source models, particularly in high-order reasoning. ExpVid not only provides a diagnostic tool but also charts a roadmap for developing MLLMs capable of becoming trustworthy partners in scientific experimentation.", "tldr": "", "keywords": ["Multimodal Large Language Model", "Video Large Language Model", "Nature Science", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5bb6e72da6d6798fba4942ff048260144dd2f099.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper evaluates the extent to which multimodal large language models (MLLMs) (which have the capacity to analyze videos) can be used for important tasks in wetlab settings. This includes identifying different aspects of experimental tasks from video, understanding sequences of tasks that make up a particular procedure, and drawing conclusions from a sequence of scientific procedures. To this end, the paper introduces a new dataset ExpVid, which aims to capture some of the capabilities required for a video-processing MLLM to be useful in a wetlab setting. The paper breaks these capabilities up into a hierarchy: finegrained perception, procedural understanding, and scientific reasoning. ExpVid contains questions aimed at each of these. After describing how the dataset was constructed, automatically annotated, and verified, it provides the results of evaluating both closed and open models. Broadly, it is found that closed models like GPT5 and Gemini have substantially better performance. The paper ends with some further light analysis of the results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Clarity:** In general, the paper is well-written (but see typos in the Nitpicks section below and the issue of missing examples). It is clear that a lot of work was put into the figures, which are clean and communicate well the points of the paper. The reviewer believes that most participants at ICLR will be able to read and appreciate the results while on the other hand, a good amount of detail on the dataset generation process (usual to experts) was included.\n\n**Problem importance:** The use of MLLMs (not restricted to the video models in this paper) in science is a growing area with several notable start-ups focused on this appearing in the US just this year. Getting a better understanding of the real capabilities of these models in this space is thus highly relevant to current developments in the field. In this sense, benchmarks such as ExpVid, are a welcome contribution."}, "weaknesses": {"value": "**Without access to the videos, it is hard for a reader to gauge the difficulty of these tasks:** Though the reproducibility statement says that an anonymized copy of the dataset would be made available, the reviewer did not see this (please correct me if I just missed it). Without this, it is hard to get a good sense of the actual content of the dataset and therefore challenging to know how to interpret the results. The reviewer would also recommend adding more examples of questions from the dataset (as much as this is possible for a text and image-based paper) in the body of the paper itself. One way to do this would be to add example questions corresponding to each of the categories introduced in Section 3.2.\n\n**The paper would have benefited from deeper analysis of the results:** It is challenging to include all the details in a dataset paper that has a page limit of only 9 pages. Especially when dataset construction was complicated with many different steps (as with this paper). However, thinking about what is most valuable to the average reader, this reviewer believes there is a strong argument for including more analysis of the results, possibly at the expense of moving some details of dataset construction to the appendix. In particular, the reviewer was curious about current limitations of all models in the tasks that the paper captures. What are limitations that the model-building part of the community needs to address? Where are we succeeding?\n\n**Nitpicks**\n- Line 121: “…provide their corresponding analysis…” $\\mapsto$ “…provide an analysis of our results…”?\n- Line 158: “…while most of it in computation and physics are…” $\\mapsto$ “…while most computational examples, or examples from physics…”?\n- Line 195: “Whether records an entire…” missing ‘it’.\n- Line 261: The term ‘MCQ’ is used before it is defined.\n- Line 264: “Identify the appeared tools from the scene…” should be “tools that appear” or something like that.\n- Line 323: “Encouraging multi-blank settings to probing several key points within one question.” What does this mean?"}, "questions": {"value": "- The paper says that an anonymized version of the dataset would be released, but the reviewer could not find any links to this?\n- One of the tasks is ‘quantity recognition’. This reviewer has trouble identifying numbers appearing on devices in a video. How was it verified that this task was feasible? Examples of what this looks like would be very helpful. \n- Line 463: How was the ablation of frames performed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XA6CxjQxLC", "forum": "bj0ahFM4P0", "replyto": "bj0ahFM4P0", "signatures": ["ICLR.cc/2026/Conference/Submission16212/Reviewer_QvUU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16212/Reviewer_QvUU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761517410779, "cdate": 1761517410779, "tmdate": 1762926372726, "mdate": 1762926372726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ExpVid, a new benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on scientific experiment videos, focusing on wet-lab procedures across 13 disciplines. ExpVid is structured around a three-level task hierarchy: (1) Fine-grained Perception, (2) Procedural Understanding, and (3) Scientific Reasoning. The benchmark includes 390 curated experiment videos with detailed annotations from peer-reviewed publications. The study evaluates 19 MLLMs, revealing their strengths in coarse-grained recognition but highlighting limitations in fine-grained tasks and scientific reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Multilevel Evaluation Capacity:** ExpVid introduces a three-tier task hierarchy: fine-grained perception (identifying tools, materials, and actions), procedural understanding (step order and completeness), and scientific reasoning (linking experiments to conclusions). This ensures comprehensive assessment of MLLMs' capabilities across varying complexity levels.\n\n* **Scientific and Realistic Data:** The videos in ExpVid are sourced from the peer-reviewed JoVE journal, ensuring scientific accuracy and reflecting actual experimental processes. The task design closely aligns with real-world experimental workflows, avoiding overly theoretical tests.\n\n* **Vision-Centric Annotation with Expert Validation:** ExpVid uses a vision-based annotation pipeline combined with expert review, minimizing reliance on text knowledge and enhancing the reliability and diversity of tasks. This process ensures accurate and contextually appropriate task creation."}, "weaknesses": {"value": "1. **Input Format Ambiguity:** The paper does not clearly specify the input format of the experiment videos used in the evaluation. It is important to clarify whether the inputs are full videos, images extracted from videos at intervals, or something else. This detail could significantly impact the model's performance, as different input formats may require different processing approaches. Clearer descriptions of the input format would enhance reproducibility and understanding of the benchmark.\n\n2. **Expert Annotation Scale and Feasibility:** While the paper highlights the use of PhD-level expert annotators, the number of experts and their workload distribution across various disciplines are not provided. Given the breadth of scientific fields covered by ExpVid, it would be useful to understand how many experts were involved, how much time they spent, and how the workload was managed across such a diverse set of tasks. This information would help assess the scalability and practicality of the annotation process, which could be a potential bottleneck for future applications of this benchmark.\n\n3. **Typographical Errors:** A minor but notable typographical error occurs on line 282, where \"e.g.\" is incorrectly formatted as \"e.g.\" without a following comma. Ensuring that such errors are corrected would improve the overall presentation of the paper."}, "questions": {"value": "1. **Clarification on Input Format:**\n\n   * Could you clarify the exact input format used in ExpVid? Specifically, are the models provided with full videos, or are the videos split into individual frames or intervals of frames? How does this format affect model performance, and have you considered experimenting with different formats to evaluate potential performance variations?\n\n2. **Expert Annotation Workload:**\n\n   * Given the wide range of scientific disciplines covered in ExpVid, could you provide more details on the number of PhD-level expert annotators involved in the project? How were the tasks distributed across experts, and how much time did they spend on annotation? This information would help understand the scalability of the annotation process and any potential limitations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UsoJdjCnEA", "forum": "bj0ahFM4P0", "replyto": "bj0ahFM4P0", "signatures": ["ICLR.cc/2026/Conference/Submission16212/Reviewer_stVf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16212/Reviewer_stVf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875438826, "cdate": 1761875438826, "tmdate": 1762926372215, "mdate": 1762926372215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ExpVid, a new and significant benchmark for evaluating Multimodal MLLMs on scientific experiment video understanding. Its primary contribution lies in addressing a clear gap, as existing benchmarks fail to capture the fine-grained, long-horizon nature of \"wet-lab\" procedures. Its core contribution is a three-level task hierarchy (Perception, Procedure, Reasoning) which allows for a detailed diagnosis of model capabilities, from basic visual recognition up to complex, long-horizon inference and is built from peer-reviewed JoVE videos paired with their scientific publications. This unique data pairing enables high-level reasoning tasks absent in other datasets. Crucially, the authors employ a rigorous \"vision-centric\" annotation pipeline with PhD-level expert validation, ensuring that tasks require genuine visual grounding and cannot be \"cheated\" by models relying on prior data. Finally, the paper provides actionable findings by evaluating 19 SOTA MLLMs, clearly demonstrating their current limitations and offering a valuable roadmap for future research."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The three-level task hierarchy is a major contribution. This structure is exceptionally well-conceived, as it mirrors the real-world scientific process—from basic observation of tools to high-level reasoning about an experiment's conclusions. This hierarchy allows for a much more granular diagnosis of model capabilities than a single, monolithic task, enabling researchers to pinpoint where models are failing.\n- The choice of data source (peer-reviewed JoVE videos) is excellent. By pairing videos with their corresponding peer-reviewed publications, the authors ensure a high degree of scientific rigor.\n- The \"vision-centric\" annotation pipeline is a critical methodological strength. The authors explicitly designed the tasks to require visual grounding, using semantically and visually plausible distractors. By validating these tasks with multi-disciplinary, PhD-level experts, they have addressed the well-known \"language-prior shortcut\" problem (where models guess the answer from text without processing the video).\n- The paper provides more than just a dataset; it delivers a strong empirical study by benchmarking 19 leading MLLMs. The findings are clear and valuable: all models struggle with fine details and long-horizon reasoning, and a significant performance gap exists between proprietary and open-source models."}, "weaknesses": {"value": "- The benchmark is built on 390 high-quality videos. While the annotation depth (7,800 QA pairs) is impressive, the total number of unique experimental videos is relatively small compared to other large-scale video benchmarks.\n- The data is sourced from JoVE. This is a strength for rigor but a potential weakness for real-world applicability. JoVE videos are highly produced, professionally narrated, and edited for clarity. They do not represent the \"messy\" reality of lab work, which often involves poor lighting, occlusions, camera-phone footage, verbal mistakes, and actual failed steps. Models trained for this benchmark may not generalize well to the real lab videos.\n- The Level-3 (Scientific Reasoning) tasks are evaluated using a \"fill-in-the-blank\" format. This Outcome-Based method effectively tests the outcome of a model's reasoning but fails to evaluate the process. As the authors acknowledge, it doesn't illuminate the chain-of-thought. A model could arrive at the correct answer through flawed logic, or fail for a simple perceptual mistake. This is a missed opportunity for a benchmark on \"reasoning.\"\n- The paper uses non-expert undergraduates as a human baseline. This baseline is difficult to interpret. One would expect non-experts to fail at Level-3 (Scientific Reasoning), making their (unreported) score on this task less informative. A more valuable, albeit much more difficult, baseline would have been to benchmark the performance of PhD-level experts to understand the true gap between models and expert-level scientific comprehension."}, "questions": {"value": "- The Level-3 tasks are \"outcome-based\" (fill-in-the-blank), which checks if the final answer is correct but not if the reasoning process was. How can we be sure models are not arriving at correct answers through flawed logic? Was a \"process-based\" (e.g., chain-of-thought) evaluation considered, and why was it excluded?\n- The paper makes a point that \"Thinking\" mode can hurt performance by causing models to \"drift\" from visual evidence. This is a critical but under explained finding. Could the authors elaborate on how prevalent this was? Does this suggest a fundamental flaw in how current models balance their internal knowledge against real-time visual evidence?\n- The paper states ASR transcripts were collected from JoVE. Is there any data on the quality (e.g., Word Error Rate) of these transcripts? It is ambiguous whether some model failures, especially on Level 2 and 3, could be attributed to errors in the provided ASR text rather than the model's own reasoning."}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dYj1kLg8X3", "forum": "bj0ahFM4P0", "replyto": "bj0ahFM4P0", "signatures": ["ICLR.cc/2026/Conference/Submission16212/Reviewer_1vC1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16212/Reviewer_1vC1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939194051, "cdate": 1761939194051, "tmdate": 1762926371838, "mdate": 1762926371838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ExpVid introduces a benchmark for understanding and reasoning over scientific experiment videos, with a three-level hierarchy: (L1) fine-grained perception (materials, tools, quantities, operations), (L2) procedural understanding (ordering, sequence generation, completeness, next-step prediction), and (L3) scientific reasoning (experimental analysis, discovery). The dataset is curated from peer-reviewed research collection videos and paired papers, using a vision-centric, semi-automatic pipeline (LLM-assisted extraction + expert verification). The released scale is 390 videos across 13 disciplines with 7,800 QA items spanning 10 task types. Baselines across 19 MLLMs show frontier closed-source models outperform open-source ones, with the gap widening at higher-order reasoning; e.g., GPT-5 leads L2/L3, and Gemini-2.5-Flash tops L1. Authors report vision is necessary (ablations w/ vs. w/o frames) and “thinking” can sometimes hurt if it drifts from visuals. Limitations include a focus on wet-lab domains and L3 evaluation via an LLM judge rather than human-graded answers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The task design is clear and hierarchical (perception $\\rightarrow$ procedure $\\rightarrow$ reasoning), aligning with practical laboratory steps and utilizing expert-validated task templates and distractors.\n2. The paper details a transparent curation process, utilizing JoVE videos and expert verification, to create a substantial and diverse benchmark (7,800 QA pairs across 13 disciplines).\n3. The paper provides a comprehensive empirical evaluation by testing 19 MLLMs against human baselines using task-appropriate metrics for comparison."}, "weaknesses": {"value": "1. The Level-3 (Scientific Reasoning) tasks use a \"fill-in-the-blank\" format (Sec 3.3). This format might be simpler or more susceptible to text-matching than open-ended questions. Please discuss the rationale for this choice and how it was designed to test deep reasoning beyond simple keyword extraction.\n2. The Thinking effect observation is insightful but based on one model configuration and qualitative examples. Please Run a 2×2 across ≥3 model families with or w/o think within matched budgets to verify the claim.\n3. L2 shows high Step Ordering yet low Step Prediction/Completeness, but diagnostics aren’t tied to controllable factors (occlusion, clip length, distractor difficulty). Please provide controlled L2 suites that vary occlusion, temporal gap, and distractor closeness, and report item-response curves."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OCARwsLbOH", "forum": "bj0ahFM4P0", "replyto": "bj0ahFM4P0", "signatures": ["ICLR.cc/2026/Conference/Submission16212/Reviewer_Z8zB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16212/Reviewer_Z8zB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944394157, "cdate": 1761944394157, "tmdate": 1762926371263, "mdate": 1762926371263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ExpVid, a new benchmark designed to systematically evaluate Multimodal Large Language Models (MLLMs) on scientific experiment videos. The dataset consists of peer-reviewed laboratory videos from JoVE (an online journal), annotated with a three-level hierarchy: 1. Fine-grained Perception (e.g., identifying tools, materials, quantities, and actions); 2. Procedural Understanding (e.g., step ordering, sequence prediction, completeness verification); and 3. Scientific Reasoning (e.g., connecting experimental procedures to conclusions). The authors use multiple MLLMs (both open- and closed-source) to evaluate the performance on ExpVid. The results reveal a substantial gap between closed-source models (GPT-5, Gemini-2.5) and open-source counterparts in all tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors introduce a dataset for the evaluation of MLLMs for scientific experimental reasoning, a domain previously underserved by benchmarks like MVBench, Video-MMMU, and SCI-VID. The three-level hierarchy (Perception -> Procedure -> Reasoning) mimics the actual cognitive workflow of scientific experimentation and it is interesting. \n2. The annotation pipeline combines automated extraction by LLMs with expert human validation by PhD students. \n3. Empirical evaluation with a large number of LLMs is seen as valuable."}, "weaknesses": {"value": "1. Although an interesting dataset, using only one source to construct the dataset (JoVE) could introduce biases. It is unclear how the models would perform on other types of data (collected from other sources). Are there other sources that can be used to enhance the dataset? (Other journals with such data?) \n\n2. The reliance of LLMs for annotation could potentially introduce some biases. Did the authors/PhD student annotators carefully look for such biases and ensure the dataset is properly annotated? A more thorough discussion on this would strengthen the annotation process. \n\n3. The focus on wet-lab biology, chemistry, medicine could limit the scope of this paper and may not be of interest to a large audience in the ICLR community."}, "questions": {"value": "1. Are there other domains where the construction of datasets in the same fashion can be expanded to ensure this work would target a larger audience from the ICLR community?\n\n2. Can you please provide details on any biases that LLMs may have included in the annotation process (if any)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XyoLABp4Yj", "forum": "bj0ahFM4P0", "replyto": "bj0ahFM4P0", "signatures": ["ICLR.cc/2026/Conference/Submission16212/Reviewer_YjXg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16212/Reviewer_YjXg"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762118358725, "cdate": 1762118358725, "tmdate": 1762926370553, "mdate": 1762926370553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}