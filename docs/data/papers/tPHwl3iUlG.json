{"id": "tPHwl3iUlG", "number": 17644, "cdate": 1758278728931, "mdate": 1763638108754, "content": {"title": "Know Thyself, Know Thy User: Dual-Perspective Reasoning Architecture for Role-Playing Language Models", "abstract": "Current role-playing Large Language Models (LLMs) face a fundamental challenge: balancing character authenticity with user satisfaction. While recent dual-process and dual-perspective approaches have made progress, existing systems still struggle with role-user conflicts where character constraints clash with user expectations. We introduce the KnowSelf-KnowOther Transformer (KSKT), a novel dual-perspective reasoning architecture that addresses this challenge through four integrated innovations: Dual-Stream Axial Attention that processes self-understanding and other-understanding along functionally decoupled dimensions, Bipolar Reasoning combining fast intuitive and slow deliberative pathways, Mutual-Understanding Position Encoding capturing dynamic relational contexts, and Self-Awareness Mixture of Experts specializing in multi-dimensional character comprehension. Unlike previous approaches that treat dual-perspective reasoning as post-hoc optimization or separate modules, KSKT integrates mutual understanding directly into the model architecture. Extensive experiments on CharacterBench demonstrate significant improvements: 6.4% overall enhancement over strong baselines, with particularly notable gains in persona consistency (8.7%) and emotional intelligence (15.2%). Critically, controlled experiments show KSKT maintains balanced dual-perspective reasoning (0.87 self-awareness, 0.87 other-awareness) in role-user conflict scenarios, while baseline models exhibit severe single-perspective bias (0.17 vs. 0.83). These results establish KSKT as an effective architectural framework for role-playing systems that must balance character authenticity with user engagement.", "tldr": "KSKT realizes \"know thyself, know thy user\" via dual-perspective reasoning architecture for role-playing LLMs, achieving balanced self/other awareness and 6.4% performance gains.", "keywords": ["role-playing language models", "axial attention", "dual-perspective reasoning", "mixture of experts", "self-awareness"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/34d4aeaf7fd874954600467e404083ac4ee82dc2.pdf", "supplementary_material": "/attachment/6b563714459ddb2699702216bbd2edcbc21a67dc.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents KSKT (KnowSelf-KnowOther Transformer), a novel architecture for dual-perspective reasoning in role-playing LLMs. The model integrates character-centric (self-awareness) and user-centric (other-awareness) understanding through four key components:\n\n- Dual-Stream Axial Attention\n\n- Bipolar Reasoning Module\n\n- Mutual-Understanding Position Encoding\n\n- Self-Awareness Mixture of Experts (SAMOE)\n\nThe paper addresses the common role-user conflict in RP-LMs and proposes architectural rather than post-hoc solutions. Evaluation on CharacterBench demonstrates improvements in persona consistency (+8.7%) and emotional intelligence (+15.2%) over strong baselines"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Timely and relevant: Tackles the important problem of role-user conflict in role-playing agents, which is under-explored architecturally.\n\n- Clear empirical gains: Significant improvements in both self- and other-awareness (both reaching ~0.87), outperforming strong baselines like Claude-3-opus\n\n- Detailed ablation analysis shows each component's contribution and their specialization patterns"}, "weaknesses": {"value": "- Unclear Baseline Design:\n\nThe Qwen3-4B-Thinking (Base) model is claimed as the foundation, but it’s not clearly stated whether it is fine-tuned or frozen. If not fine-tuned, comparison may be unfair against a fully-trained proposed model\n\n- Possibly Unfair LLM Comparisons:\n\nAppendix D.3 reports comparisons with closed-source LLMs like Claude and GPT-4, but it’s unclear whether those models are given n-shot prompts or access to similar training context. If not, this may underestimate their performance\n\n- Expansion Limitations in SAMOE:\n\nThe Self-Awareness MoE has pre-defined expert types (P/K/E/C), which may limit generalization to domains requiring flexible or emergent roles"}, "questions": {"value": "*(Corresponding to weaknesses above)*\n\n- Do you fine-tune the Qwen3-4B-Thinking base model in your main experiments, or is it frozen?\n\n- Were the closed-source LLMs (e.g., GPT-4, Claude) given in-context examples (n-shot), CoT, or system messages to match your training data in any way?\n\n- Could the current SAMOE expert scheme be extended to non-predefined, dynamic roles? For example, emergent traits like \"ironic narrator\" or \"deceptive agent\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hj4aZ5sxZf", "forum": "tPHwl3iUlG", "replyto": "tPHwl3iUlG", "signatures": ["ICLR.cc/2026/Conference/Submission17644/Reviewer_gq97"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17644/Reviewer_gq97"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17644/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774130392, "cdate": 1761774130392, "tmdate": 1762927500415, "mdate": 1762927500415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents KSKT (KnowSelf-KnowOther Transformer), a novel architecture for dual-perspective reasoning in role-playing LLMs. The model integrates character-centric (self-awareness) and user-centric (other-awareness) understanding through four key components:\n\n- Dual-Stream Axial Attention\n\n- Bipolar Reasoning Module\n\n- Mutual-Understanding Position Encoding\n\n- Self-Awareness Mixture of Experts (SAMOE)\n\nThe paper addresses the common role-user conflict in RP-LMs and proposes architectural rather than post-hoc solutions. Evaluation on CharacterBench demonstrates improvements in persona consistency (+8.7%) and emotional intelligence (+15.2%) over strong baselines"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Timely and relevant: Tackles the important problem of role-user conflict in role-playing agents, which is under-explored architecturally.\n\n- Clear empirical gains: Significant improvements in both self- and other-awareness (both reaching ~0.87), outperforming strong baselines like Claude-3-opus\n\n- Detailed ablation analysis shows each component's contribution and their specialization patterns"}, "weaknesses": {"value": "- Unclear Baseline Design:\n\nThe Qwen3-4B-Thinking (Base) model is claimed as the foundation, but it’s not clearly stated whether it is fine-tuned or frozen. If not fine-tuned, comparison may be unfair against a fully-trained proposed model\n\n- Possibly Unfair LLM Comparisons:\n\nAppendix D.3 reports comparisons with closed-source LLMs like Claude and GPT-4, but it’s unclear whether those models are given n-shot prompts or access to similar training context. If not, this may underestimate their performance\n\n- Expansion Limitations in SAMOE:\n\nThe Self-Awareness MoE has pre-defined expert types (P/K/E/C), which may limit generalization to domains requiring flexible or emergent roles"}, "questions": {"value": "*(Corresponding to weaknesses above)*\n\n- Do you fine-tune the Qwen3-4B-Thinking base model in your main experiments, or is it frozen?\n\n- Were the closed-source LLMs (e.g., GPT-4, Claude) given in-context examples (n-shot), CoT, or system messages to match your training data in any way?\n\n- Could the current SAMOE expert scheme be extended to non-predefined, dynamic roles? For example, emergent traits like \"ironic narrator\" or \"deceptive agent\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hj4aZ5sxZf", "forum": "tPHwl3iUlG", "replyto": "tPHwl3iUlG", "signatures": ["ICLR.cc/2026/Conference/Submission17644/Reviewer_gq97"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17644/Reviewer_gq97"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17644/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774130392, "cdate": 1761774130392, "tmdate": 1763681389732, "mdate": 1763681389732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of balancing character authenticity and user satisfaction in role-playing LLMs, where existing systems often show single-perspective bias and treat dual-perspective reasoning as post-hoc optimization. It introduces the KnowSelf-KnowOther Transformer (KSKT), a Qwen3-4B-Thinking-based architecture with four core components to integrate \"self-understanding (character constraints)\" and \"other-understanding (user intentions)\" into transformer generation. Experiments on CharacterBench show KSKT boosts overall performance by 6.4% (8.7% in persona consistency, 15.2% in emotional intelligence) and maintains balanced self/other-awareness (0.87 each) in conflicts, unlike biased baselines. However, KSKT lags behind Claude-3-opus in English and has 19.5% inference latency overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Architectural breakthrough beyond post-hoc optimization: Instead of treating dual-perspective reasoning as an auxiliary module, KSKT deeply integrates \"self-understanding (character constraints)\" and \"other-understanding (user intentions)\" into the transformer generation process via four core components, enabling real-time balance between character authenticity and user satisfaction.\n2. Rigorous and credible experimental design: Centered on multi-dimensional metrics from CharacterBench, it uses incremental component analysis to verify each module’s contribution and supplements with blind evaluations by 15 experts, ensuring consistency between objective performance gains and improved user experience.\n3. Components with scenario-specific precision: SAMOE resolves persona consistency through four specialized experts, DSAA focuses on enhancing emotional understanding, and Bipolar Reasoning balances efficiency and reasoning depth—all precisely addressing core pain points in role-playing."}, "weaknesses": {"value": "1. Architectural robustness issues in high-conflict scenarios: 42.5% of failures stem from core design flaws—23.5% from incorrect SAMOE expert routing (e.g., assigning a Stoic philosopher query to the Emotional Expert) and 19% from biased Dual-Stream fusion (e.g., a medieval peasant violating knowledge boundaries to help with calculus).\n2. Lack of discussion on generalizability. The design of the method relies heavily on certain prior assumptions (such as personality and knowledge), and it remains unclear whether it is also applicable to other role-playing tasks—for example, mimicking language styles."}, "questions": {"value": "1. Does role-specific data training impair the model's inherent general capabilities and deep reasoning abilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "25tbjhy4Gi", "forum": "tPHwl3iUlG", "replyto": "tPHwl3iUlG", "signatures": ["ICLR.cc/2026/Conference/Submission17644/Reviewer_jKjS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17644/Reviewer_jKjS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17644/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981150767, "cdate": 1761981150767, "tmdate": 1762927499062, "mdate": 1762927499062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents KSKT, a novel transformer-based architecture designed to resolve the persistent tension between character authenticity and user satisfaction in role-playing language models. It introduces four integrated components—Dual-Stream Axial Attention, Mutual-Understanding Position Encoding, Bipolar Reasoning, and Self-Awareness Mixture of Experts—that collectively enable a form of architecturally embedded, dual-perspective reasoning during generation. The work demonstrates measurable improvements on the CharacterBench benchmark, particularly in persona consistency and emotional intelligence, and provides extensive ablation studies and qualitative analyses to support the effectiveness of its design."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper provides a highly systematic and modular breakdown of the KSKT architecture. Each core component (DSAA, MUPE, BRM, SAMOE) is introduced with its design philosophy, mathematical formulation, and specific mechanisms, making the complex integrated system readily understandable. This structured approach, particularly in Section 3 and Appendix A, greatly enhances the clarity for readers to grasp the intricate interplay of its novel components.\n\n2. Figure 1, the KSKT Architecture Overview, is exceptionally well-designed, offering an intuitive visual roadmap of the model's data flow and component interactions. This high-level diagram, coupled with subsequent detailed figures like Figure 3 (Dual-perspective reasoning validation) and Figure 4 (Dynamic activation schematic), significantly aids in conceptualizing the abstract architectural principles and their operational dynamics, making the paper highly accessible.\n\n3. The experimental section is robust, featuring a thorough performance comparison against strong baselines on CharacterBench (Table 1). Crucially, the paper includes detailed ablation studies (Figure 2a) demonstrating the incremental contributions of each KSKT component. This systematic analysis helps validate the individual efficacy of the proposed innovations and provides clear evidence of their targeted capability enhancements.\n\n4. The inclusion of a detailed failure case analysis (Section G, Table 9) is a significant strength. By quantifying failure types and their root causes (e.g., expert routing failures, dual-stream fusion bias), the paper not only acknowledges limitations but also provides clear, actionable insights for future research. This candid and data-driven approach to identifying shortcomings fosters productive follow-up work and demonstrates scientific rigor."}, "weaknesses": {"value": "1. This paper claims DSAA decomposes attention into \"orthogonal streams\" for self- and other-understanding, yet lacks rigorous evidence. It does not demonstrate semantic non-overlap between $A_{\\text{self}}$ and $A_{\\text{other}}$ attention weight matrices or prove linear separability of their outputs. Without analyses like activation pattern differentiation or gradient flow, the claim of \"semantic independence\" remains unproven. Consequently, these streams might merely process similar information with different weightings, potentially reducing efficiency rather than achieving true semantic separation and robustly resolving \"role-user conflicts.\"\n\n2.  This paper states that $B_{\\text{role}}$ and $B_{\\text{intent}}$ bias matrices \"encode role-specific and intent-specific attention patterns\" by being \"initialized to focus on relevant token types.\" However, this initialization primarily relies on grammatical features (e.g., part-of-speech tagging for role traits), rather than deep semantic alignment. There is no clear evidence that the model maintains this semantic alignment throughout training or that the biases precisely target underlying role identity semantics or user intention semantics, beyond mere lexical or grammatical categories. This limited semantic grounding could lead to significant overlap or misalignment between the \"self\" and \"other\" information processed, hindering DSAA's precision in achieving dual-perspective cognition.\n\n3. Without more rigorous empirical or theoretical analysis, DSAA's architectural novelty remains unclear. Structurally, it might be interpreted as simply splitting a single attention layer into two distinct groups of heads (self- and other-understanding heads) with separate projection matrices and biases, followed by fusion. The paper lacks a compelling argument or empirical demonstration that this mechanism fundamentally differs from merely increasing the number of generic multi-head attention heads to enhance model capacity. Consequently, DSAA's observed benefits might be partially attributable to increased parameter count rather than a transformative mechanistic breakthrough for resolving \"role-user conflicts.\"\n\n4. MUPE's reliance on externally pre-processed $R_{\\text{proc}}$ and $U_{\\text{proc}}$ introduces a potential semantic gap between these external representations and the LLM's internal semantic space. The paper does not explain how the projection matrices $W_{\\text{role}}$ and $W_{\\text{intent}}$ effectively bridge this gap to integrate external semantics into the LLM's position encodings. Furthermore, errors from external models (e.g., BERT's intent classification errors) will propagate directly into MUPE, potentially amplifying inaccuracies within the LLM and undermining its robustness. This external dependency might hinder MUPE's ability to precisely inject \"self-other\" relational signals and could lead to inconsistencies if the LLM's internal context understanding diverges from the external inputs.\n\n\n5. SAMOE's \"self-awareness\" (via $q_{\\text{self}}$) heavily relies on the externally pre-processed and potentially static $R_{\\text{proc}}$. If $R_{\\text{proc}}$ is not dynamically updated based on ongoing dialogue, the depth and flexibility of SAMOE's \"self-cognition\" would be limited by the external model's capacity to capture nuanced, implicit role traits. This dependence on a pre-defined external representation for the core \"self-identity\" challenges the paper's claims of \"architecturally emergent 'role reasoning patterns'\" and \"spontaneous emergence.\" The expert routing decisions might become overly rigid or inaccurate, failing to adapt to a character's subtle personality shifts in complex, dynamic conversational contexts, thus weakening the system's claimed \"multi-dimensional persona integration.\""}, "questions": {"value": "1. Could the authors provide further justification for the claim that the two attention streams in DSAA are semantically orthogonal rather than merely independently parameterized?\n\n2. Given that SAMOE’s routing is driven entirely by pre-processed $R_{\\text{proc}}$, how does the system maintain dynamic self-awareness when the character’s internal state evolves through dialogue, beyond what is statically encoded in the initial role description?\n\n3. The results show a significant performance gap in English compared to Chinese, even after architectural innovations. Is this disparity attributed primarily to the base Qwen3 model’s training bias, or does it suggest that the current design of MUPE or SAMOE is more compatible with the syntactic or pragmatic structure of Chinese?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EdLqHWvXlO", "forum": "tPHwl3iUlG", "replyto": "tPHwl3iUlG", "signatures": ["ICLR.cc/2026/Conference/Submission17644/Reviewer_A65J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17644/Reviewer_A65J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17644/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762331307233, "cdate": 1762331307233, "tmdate": 1762927498590, "mdate": 1762927498590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all the reviewers for their thorough evaluation of our paper and their constructive feedback. We believe that these revisions have significantly enhanced the quality of the paper. Below, we address each comment in detail. Here, we summarize the key changes and additional analyses incorporated into the revised manuscript:\n\n*   **Mechanism Verification (Section 4.5 & Appendix B.4):** To address concerns regarding orthogonality and architectural necessity, we conducted Centered Kernel Alignment (CKA) analysis and Gradient Flow visualization. Results show a \"U-shaped\" semantic separation (min CKA $0.34$) and minimal gradient overlap ($18.4\\%$), empirically proving the functional decoupling of the dual streams.\n*   **Failure Mitigation Strategies (Appendix G.5):** Addressing concerns about high-conflict scenarios, we implemented Top-$k$ routing and uncertainty-aware fusion as mitigation strategies. These refinements reduced the failure rate by **$24.5\\%$** on hard cases without significant latency overhead.\n*   **Generalization & Extensibility (Appendix H):** We validated KSKT on out-of-distribution tasks, including linguistic style imitation (Shakespearean, $+15.1\\%$ consistency) and emergent role modeling (via Top-$k$ routing), demonstrating applicability beyond pre-defined personality traits.\n*   **Experimental Clarifications (Section 4.1 & Appendix D.3):** We clarified that the base model was fully fine-tuned (not frozen) to ensure a fair comparison and expanded comparisons with closed-source models (Zero-shot/CoT).\n\nIn addition to these updates, we refined the text throughout the manuscript to align with the reviewers' suggestions."}}, "id": "y4cNOBO31j", "forum": "tPHwl3iUlG", "replyto": "tPHwl3iUlG", "signatures": ["ICLR.cc/2026/Conference/Submission17644/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17644/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17644/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763642092213, "cdate": 1763642092213, "tmdate": 1763642092213, "mdate": 1763642092213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}