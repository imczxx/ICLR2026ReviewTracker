{"id": "QihJMqm2aX", "number": 10419, "cdate": 1758170856782, "mdate": 1759897651838, "content": {"title": "Pushing the limits of unconstrained machine-learned interatomic potentials", "abstract": "Machine-learned interatomic potentials (MLIPs) are increasingly used to replace computationally demanding electronic-structure calculations to model matter at the atomic scale. The most commonly used model architectures are constrained to fulfill exactly a number of physical laws, from geometric symmetries to energy conservation. Evidence is mounting that relaxing some of these constraints can be beneficial to the efficiency and (somewhat surprisingly) accuracy of MLIPs, even though care should be taken to avoid qualitative failures associated with the breaking of physical symmetries. Given the irresistible trend of scaling models up to larger numbers of parameters and training configurations, a very important question is how unconstrained MLIPs behave in this limit. Here we investigate this issue, showing that — when trained on some of the current large-scale datasets — unconstrained models can be competitive in accuracy and superior in speed when compared to physically constrained models. We assess these models both in terms of benchmark accuracy and in terms of usability in practical scenarios, focusing on static simulation workflows such as geometry optimization and lattice dynamics. We conclude that accurate unconstrained models can be applied with confidence, especially given that simple inference-time modifications can be used to recover observables that are fully consistent with the relevant physical symmetries.", "tldr": "Unconstrained machine-learned interatomic potentials achieve state-of-the-art accuracy and efficiency for atomic-scale systems", "keywords": ["machine learning", "machine-learned interatomic potentials", "atomistic simulations", "geometric machine learning", "graph neural networks", "unconstrained", "equivariant"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7cab9462d3645836d7dc40d7e3a8a34beb825cab.pdf", "supplementary_material": "/attachment/d9b0ed182081b32736ce05f4794211b812554674.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the performance of unconstrained machine-learned interatomic potentials (MLIPs) — models that do not explicitly enforce rotational equivariance or energy conservation. The authors scale up a PET-based GNN architecture to hundreds of millions of parameters and evaluate it on large-scale materials (OMat24, MPtrj) and molecular (SPICE) datasets. The results show that unconstrained models can reach comparable accuracy to equivariant ones while offering 2–3× faster inference. The paper also discusses inference-time corrections (rotational averaging, Hessian symmetrization) to recover physical observables."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper is a solid engineering and benchmarking effort but provides limited conceptual novelty."}, "weaknesses": {"value": "The discussion on conservative vs. non-conservative forces and equivariant vs. non-equivariant architectures has already been extensively studied in recent works. \n\nThe idea of combining direct-force pretraining with conservative fine-tuning was already proposed and empirically validated in eSCN rameworks. Therefore, the conceptual novelty of this paper is limited. \n\nThe proposed PET modifications (RMSNorm, SwiGLU, increased node features) are incremental architectural tweaks rather than fundamentally new ideas. The core PET design and inference-time averaging have been previously explored."}, "questions": {"value": "See Section Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "09baJyjGUW", "forum": "QihJMqm2aX", "replyto": "QihJMqm2aX", "signatures": ["ICLR.cc/2026/Conference/Submission10419/Reviewer_3Py5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10419/Reviewer_3Py5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760543237812, "cdate": 1760543237812, "tmdate": 1762921728907, "mdate": 1762921728907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors train a non-rotationally equivariant MLIP on relevant chemical datasets. They show competitive performance compared to equivariant architectures and discuss the role of different constraints in performance and inference speed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Training efficient MLIPs is of interest to the community\n- The discussion surrounding the amount of augmentation and what can be expected with / without constraints is interesting.\n- The paper is well written and clearly motivated"}, "weaknesses": {"value": "The way the paper is currently written, it is difficult to pinpoint the exact contribution that the authors are advocating for. There are a number of existing works that already show strong performance with non-equivariant architectures [1,2,3,4]. It is already known that these non-rotationally equivariant models can perform quite well. I find the discussion in Section 3.1 surrounding how hard different symmetries are to learn, and in fact I think focusing on this discussion with clear experiments would make the paper stronger and more novel. However, as it stands, I have some worries about the relevance of the current experiments:\n\n- While the performance on materials is competitive, non-equivariant architectures have already been shown to perform extremely well on these benchmarks [1,2,3]. \n- Again the performance on SPICE is competitive, but models are already near perfect and there are known issues with the dataset [6], making differences between such accurate models hard to evaluate. It also seems like training resources were not clearly controlled between baselines (I suggest the authors report the speed of a few eSEN / EScAIP / MACE training steps on the same hardware used to train their models to make a clearer comparison). Something like OMol is likely a much better place to evaluate performance on molecules [5].     \n\n[1] Eric Qu, & Aditi S. Krishnapriyan. (2024). The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains.\n\n[2] Mark Neumann, James Gin, Benjamin Rhodes, Steven Bennett, Zhiyi Li, Hitarth Choubisa, Arthur Hussey, & Jonathan Godwin. (2024). Orb: A Fast, Scalable Neural Network Potential.\n\n[3] Benjamin Rhodes, Sander Vandenhaute, Vaidotas Šimkus, James Gin, Jonathan Godwin, Tim Duignan, & Mark Neumann. (2025). Orb-v3: atomistic simulation at scale.\n\n[4] Max Eissler, Tim Korjakow, Stefan Ganscha, Oliver T. Unke, Klaus-Robert Müller, & Stefan Gugler. (2025). How simple can you go? An off-the-shelf transformer approach to molecular dynamics.\n\n[5] Daniel S. Levine, Muhammed Shuaibi, Evan Walter Clark Spotte-Smith, Michael G. Taylor, Muhammad R. Hasyim, Kyle Michel, Ilyes Batatia, Gábor Csányi, Misko Dzamba, Peter Eastman, Nathan C. Frey, Xiang Fu, Vahe Gharakhanyan, Aditi S. Krishnapriyan, Joshua A. Rackers, Sanjeev Raja, Ammar Rizvi, Andrew S. Rosen, Zachary Ulissi, Santiago Vargas, C. Lawrence Zitnick, Samuel M. Blau, & Brandon M. Wood. (2025). The Open Molecules 2025 (OMol25) Dataset, Evaluations, and Models.\n\n[6] Domantas Kuryla, Fabian Berger, Gábor Csányi, & Angelos Michaelides. (2025). How Accurate Are DFT Forces? Unexpectedly Large Uncertainties in Molecular Datasets."}, "questions": {"value": "Smaller comments / questions:\n- Line 190: Where does the 0.1 Å come from? This seems very dependent on the downstream application.\n- Line 48: Something like Gemenet still has many inductive biases and \"constraints\"\n- What hardware was used to run the speed benchmarks in Fig. 3? Is this the single A100 80 GB card mentioned in Appendix F? For a speed benchmarks, theses details are useful to have in the main text for the reader."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kDNL0qfw5l", "forum": "QihJMqm2aX", "replyto": "QihJMqm2aX", "signatures": ["ICLR.cc/2026/Conference/Submission10419/Reviewer_no2Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10419/Reviewer_no2Y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761627888345, "cdate": 1761627888345, "tmdate": 1762921728465, "mdate": 1762921728465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how much inductive-bias is required in machine learned interatomic potentials. These inductive-biases include: rotations, translations, permutations, inversions, and energy conservation. In the paper, an unconstrained model (based on the point edge transformer) is trained on the MPtrj and OAM materials datasets. The model is evaluated on a number of tasks including geometry optimizations, phonons, and the Matbench Discovery leaderboard. Both the PET-MPtrj and PET-OAM models are highly competitive on the Matbench Discovery leaderboard. Similarly, a PET model is trained on SPICE, a dataset for molecules, where it achieves SOTA result against models with similar inference speeds. The paper concludes that rotational equivariance can be learned whereas non-conserving models look good on test set metrics but fail in other settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The work focuses on an important topic.\n- The paper does an excellent job laying out the background information.\n- The paper is technically solid."}, "weaknesses": {"value": "- There is not sufficient novelty presented in the paper. The model architecture is not new; it is based on the point edge transformer, which was published previously. The main conclusions that rotational equivariance can be learned and that energy conservation needs to be included (this can be made more efficient with non-conservative pre-training) has been reported by a number of others (Orb-v3: atomistic simulation at scale, The dark side of the forces: assessing non-conservative force models for atomistic machine learning, The Importance of Being Scalable:Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains, and Learning Smooth and Expressive Interatomic Potentials for Physical Property Prediction). While the empirical results on Matbench Discovery and SPICE are promising and demonstrate clear value, additional contributions would strengthen the case for publication. \n- The paper does not include any molecular dynamics (MD) evaluations. It would be nice to include NVE MD energy drift experiments similar to those in Learning Smooth and Expressive Interatomic Potentials for Physical Property Prediction to see how the unconstrained PET model behaves."}, "questions": {"value": "1. Can you explain the constrained relaxation procedure in more detail? \n2. In Figure 2, is there a DFT reference available?\n3. How much does rotational or O(3) averaging dig into the speed benefits of an unconstrained model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fLiCbTHTGd", "forum": "QihJMqm2aX", "replyto": "QihJMqm2aX", "signatures": ["ICLR.cc/2026/Conference/Submission10419/Reviewer_LXtX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10419/Reviewer_LXtX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972971314, "cdate": 1761972971314, "tmdate": 1762921727898, "mdate": 1762921727898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper looks at an architecture that is a modification of the Point Edge Transformer (PET). It explores this architecture in the context of not building in rotational equivariance or energy conservation. The model is assessed on materials benchmarks such as the ones in Matbench Discovery, as well as on the SPICE molecular dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The analysis of the phonon calculations is interesting, and provides a nice analysis of what the ML community should be looking at"}, "weaknesses": {"value": "- The primary thing that this paper seems to look at is having models that are unconstrained with respect to rotational equivariance and energy conservation, not unconstrained in all scenarios, so the title and the way the model is referred to is already misleading. There are a number of works that have also studied and shown this, and it’s not clear what is new here: the title of this paper should be renamed to reflect these very specific architectural removals (such as rotational equivariance), as it doesn’t seem like it is pushing the limits of a truly unconstrained architecture. On this note, it does not seem like it is “pushing the limits” of an architecture without these standard constraints either.\n\n- This paper does not mention the Open Molecules dataset, which has been out for a number of months before the ICLR deadline, and represents a thorough dataset and benchmark suite. There are lines in the paper saying “applications of MLIPs to molecular systems have not yet seen the development of established benchmark suites” and this is inaccurate. For any new model paper, training and testing on Open Molecules should become standard. It has both energy and force evaluation metrics but also a number of additional evaluations.\n\n- Combining the above points, this seems like an architecture that is only marginally different from before and is not actually a top performing model on any noticeable datasets."}, "questions": {"value": "- The way the architecture is referred to is confusing. The model is called a “Point Edge Transformer” but seems to essentially still be a GNN, as it has message passing and a radius cutoff. There is a line in the paper that says it is “a GNN whose successive layers process the edges through a standard Transformer” but this is unclear: looking at the architecture, it seems like it would be a GNN that uses the attention mechanism. Can you please clarify, and/or update the text to reflect the correct notation?\n\n- What would results be on the Open Molecules 2025 dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "970G6nyNdg", "forum": "QihJMqm2aX", "replyto": "QihJMqm2aX", "signatures": ["ICLR.cc/2026/Conference/Submission10419/Reviewer_xQpZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10419/Reviewer_xQpZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762717526578, "cdate": 1762717526578, "tmdate": 1762921727284, "mdate": 1762921727284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}