{"id": "73w655DPf1", "number": 7963, "cdate": 1758046390694, "mdate": 1759897819592, "content": {"title": "Med-REFL: Enhancing Complex Reasoning via Fine-grained Self-Correction", "abstract": "Large reasoning models excel in domains like mathematics where intermediate reasoning is straightforward to verify, but struggle to self-correct in medicine fields where evaluating intermediate reasoning is cumbersome and expensive. This verification bottleneck hinders the development of reliable AI reasoners for high-stakes application. Here we propose Med-REFL, a novel framework that learns fine-grained reflection without human labels or model distillation. Med-REFL introduces a deterministic structural assessment of the reasoning space to automatically generate preference data for reflection. By globally evaluating all explored reasoning paths in a tree-of-thoughts, our method quantifies the value of corrective actions, enabling the automated construction of  direct preference optimization pairs. This trains the model to recognize and amend its own reasoning fallacies.  Extensive experiments show Med-REFL delivers robust gains across diverse models architectures and medical benchmarks, boosting a general-purpose Llama3.1-8B by +5.82\\% and the state-of-the-art Huatuo-o1 by +4.13\\% on the MedQA benchmark. Our Med-REFL-8B achieves state-of-the-art performance among 7-8B models while even competing with models twice its size. Crucially, targeted ablations prove its success stems from instilling a genuine, prompt-activatable self-correction capability, rather than superficial mimicry. \nUltimately, our framework provides a scalable solution to the verification bottleneck, paving the way for more reliable AI reasoners in high-stakes domains like medicine. Med-REFL has been made publicly available.", "tldr": "Med-REFL enhances LLM reasoning in domains like medicine where step-verification is costly. It deterministically scores reasoning steps to automatically generate preference data, teaching models to effectively self-correct.", "keywords": ["Large Language Models", "Self-Correction", "Reflection Learning", "Medical Reasoning", "LLM Reasoning", "Reasoning", "Tree of Thoughts", "healthcare applications", "medical QA"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5022e83a07bd70be9435968bad755970f8bae4bd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Med-REFL, a novel framework for enhancing complex medical reasoning in large language models through fine-grained self-correction. The key innovation is a deterministic, structure-based evaluation method using tree-of-thoughts (ToT) exploration to automatically generate preference data for direct preference optimization (DPO). The framework addresses the verification bottleneck in medical reasoning by quantifying the value of corrective actions without requiring human labels or expensive process reward models. Experiments show consistent improvements across diverse models and benchmarks, with particularly strong gains on the MedQA dataset (+3.67% average) and GPQA (+3.53% average)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Well-motivated problem formulation**: The paper clearly articulates why existing approaches (outcome-based RL, PRMs) are insufficient for medical reasoning and positions Med-REFL as a solution to the verification bottleneck.\n\n2. **Comprehensive experimental validation**: Testing across 7 diverse models (instruction-tuned, reason-heavy, knowledge-heavy) and 7 benchmarks with consistent improvements demonstrates robustness.\n\n3. **Strong ablation studies**: The experiments comparing Med-REFL against MCTS rollouts and GPT-4.1-as-judge (Table 3) provide compelling evidence for the structural assessment approach. The prompting experiments (Table 5) elegantly demonstrate internalized reflection capability.\n\n4. **Practical impact**: Achieving SOTA in the 7-8B class and competing with 10-20B models (Figure 1d, Table 7) has clear practical value for resource-constrained deployment."}, "weaknesses": {"value": "**Computational cost not addressed**: ToT exploration is computationally expensive. The paper lacks analysis of:\n   - Wall-clock time for data generation compared to alternatives\n   - Trade-offs between exploration breadth/depth and final performance\n   - Scalability analysis - what happens with 10x or 100x more data?\n\n**Scoring model creates confusion**: Table 1 claims Med-REFL is \"Reward Model Free\" but Section 2.3 introduces a 3B scoring model trained on v_sol. While smaller than a full PRM, this contradicts the \"free\" claim and adds training complexity not fully acknowledged.\n\n**Reflection vs. reasoning gains unclear**: Table 10 shows both components help, but the interaction effects and optimal ratios (beyond the 1:2 empirical finding) lack theoretical justification. Why is 1:2 optimal?\n\n**Limited failure analysis**: The paper focuses on improvements but doesn't analyze:\n   - Which question types or medical domains see minimal or negative impact?\n   - Are there systematic biases introduced by the ToT exploration?\n   - Cases where reflection leads to overthinking/correct→incorrect changes"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "0RkL3CPREx", "forum": "73w655DPf1", "replyto": "73w655DPf1", "signatures": ["ICLR.cc/2026/Conference/Submission7963/Reviewer_3fRk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7963/Reviewer_3fRk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883398384, "cdate": 1761883398384, "tmdate": 1762919977555, "mdate": 1762919977555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Med-REFL, a framework that teaches large reasoning models to self-correct without human or reward-model supervision. It evaluates reasoning trees deterministically—assigning step and action values—to build preference pairs between effective and invalid reflections, then fine-tunes via Direct Preference Optimization (DPO).\n\nAcross seven medical benchmarks and multiple 7–8B models (Llama-3.1, Huatuo-o1, AlphaMed, etc.), Med-REFL achieves consistent gains (+3–6 %), reaching SOTA among 8B models.\nAblations show deterministic scoring outperforms MCTS or GPT-4.1 judges, and reflection ability can be explicitly activated by prompting, suggesting genuine self-correction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles one of the central challenges in medical reasoning — the lack of reliable supervision for intermediate steps. This is a genuine bottleneck for developing trustworthy reasoning models in high-stakes domains, and the authors motivate it well.\n\nThe proposed method delivers noticeable and repeatable gains across a wide range of models and benchmarks, not just in one narrow setup. The results look robust, suggesting the approach generalizes beyond a single architecture or dataset.\n\nIncludes thoughtful and convincing ablations. The ablation studies are well designed and help clarify the role of each component in the framework. They make it much easier to trust that the improvements come from the core ideas rather than from implementation details or data artifacts."}, "weaknesses": {"value": "Still somewhat tied to QA-style supervision. Even though the deterministic scoring idea is neat, it ultimately depends on having the correct final answer to judge whether a reasoning path is good or bad. That works well for multiple-choice or QA-type benchmarks, but it might not translate easily to more open-ended tasks like diagnosis summaries or free-form clinical explanations where there isn’t a single “right” answer. It would be helpful if the authors could comment on how their approach might generalize to those cases.\n\nCompute cost isn’t very clear. The paper describes generating full Trees-of-Thought and evaluating every step, which sounds potentially expensive, but it doesn’t quantify how costly that actually is. A brief analysis of runtime, GPU hours, or scaling behavior would make it easier to judge how practical this is compared to standard fine-tuning or PRM methods.\n\nThe authors claim the framework is general, but all experiments are in the medical domain. Even one small example from another reasoning area—say scientific QA or legal reasoning—would help show that the method really transfers beyond the datasets it was built for.\n\nThe conceptual novelty is somewhat incremental. The proposed framework builds on ideas already explored in process reward models (PRMs) and StepWiser-like step-evaluation methods, where each reasoning step is scored or judged to guide model improvement."}, "questions": {"value": "1. How sensitive is the framework to the quality of the base model’s reasoning? Could a weaker base model still generate useful reflection data?\n2. Is the step-value metric robust to partial correctness or multi-label questions?\n3. How much compute was required to generate the 33k DPO pairs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vllrUQQb9U", "forum": "73w655DPf1", "replyto": "73w655DPf1", "signatures": ["ICLR.cc/2026/Conference/Submission7963/Reviewer_oh3a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7963/Reviewer_oh3a"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949232184, "cdate": 1761949232184, "tmdate": 1762919977194, "mdate": 1762919977194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Med-REFL, a deterministic tree-of-thoughts framework that constructs step-wise preference data to DPO-train models, enabling fine-grained self-correction reasoning in medical fields."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Deterministic structural evaluation: This work utilizes step / solution / remaining value metrics and another action value $$v_{act}$$ to provide a clear and label-free signal to assess both states and corrective actions across the entire ToT search space.\n2. Targeted reflection supervision via DPO: This work carefully designs a scheme to construct preference pairs that explicitly contrast effective versus invalid reflections, incentivizing self-correction ability of the  model rather than mere outcome accuracy.\n3. Balanced training data: This work combines reflection-focused pairs with general CoT quality pairs, scored by a lightweight SFT scorer, to maintain broad reasoning proficiency while emphasizing error recovery."}, "weaknesses": {"value": "1. Heuristic sensitivity: Performance may hinge on the quality of LLM-based tools ($E_{\\pi}$, $C_{\\pi}$, $M_{\\pi}$) and hyperparameters (e.g., $\\lambda_1$,$\\lambda_2$,$\\lambda_3$), which could vary across domains or models.\n2. Limited algorithmic contribution: Med-REFL primarily leverages ToT to collect reasoning trajectories and uses carefully designed metrics to construct DPO preference pairs, thus porting self-reflection to the medical domain."}, "questions": {"value": "Please refer to Weakness 1 & 2.\nHere are some other questions:\n1. In Table 2, the improvements of Pure-RL model are noticeably lower than that of other models, and there is even a performance drop on PubMedQA. Since the compared models are roughly similar in parameter size, could the authors explain what is the cause of such phenomenon? Why does your method appear to yield very limited gains on the Pure-RL model, while achieving the best improvements on the reason-heavy Huatuo-o1-8B?\n2. In line 257, the authors mention introducing general-purpose data during training. What is the ratio between this general-purpose data and the reflection data, and is there a trade-off between performance on medical datasets and general-purpose benchmarks? If so, could the authors explain how they balanced the data mixture? Additionally, if the general-purpose data is removed, can the model—after training on well-organized DPO data—maintain its performance on general-purpose benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MqVoZh8h1o", "forum": "73w655DPf1", "replyto": "73w655DPf1", "signatures": ["ICLR.cc/2026/Conference/Submission7963/Reviewer_83j9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7963/Reviewer_83j9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982342110, "cdate": 1761982342110, "tmdate": 1762919976813, "mdate": 1762919976813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Med-REFL is a teacher/PRM-free framework to instill fine-grained, activatable self-correction in medical LLMs. It explores tree-of-thought (ToT) trajectories per question and assigns deterministic, structure-based values to steps and actions using only final-answer supervision.It then constructs DPO preference pairs by contrasting effective with invalid reflections for reflection learning and high-quality with plausible-but-flawed reasoning for reasoning enhancement, using a lightweight scoring model. Fine-tuning on 33k pairs yields consistent gains across 7–8B models on MedQA, GPQA-M, MMLU-Pro, MedXpert, with strong ablations and evidence that reflection can be prompted post-training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear, deterministic valuation of steps enables process-level learning without PRMs or human labels.\n- Explicitly trains “effective vs. invalid reflection,” shifting from best-path selection to error correction.\n- Consistent improvements across diverse models/benchmarks; strongest on reasoning-heavy tasks.\n- Reflection-activation study supports genuine internalization of self-correction."}, "weaknesses": {"value": "- Valuation may be sensitive to ToT exploration distribution and generator choice, limited sensitivity analysis.\n- Evaluation focuses on MCQs,vmodest gains on retrieval-heavy tasks; no process metrics (faithfulness, coherence) or open-ended tests.\n- Fixed λ weights and 1:2 data mix lack broader sensitivity studies.\n- preference data sourced mainly from MedQA."}, "questions": {"value": "- Sensitivity of v_step/v_act to ToT parameters (branching/depth, seeds, temperatures) and λ weights?\n- Any leakage controls between MedQA training-derived pairs and test split?\n- Can you add open-ended/process evaluations (e.g., faithfulness, hallucination rates, calibration) and multistage cases?\n- Is reflection text necessary, or does path preference alone suffice? What are compute/time costs for ToT/data construction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dCYpB8Mq2M", "forum": "73w655DPf1", "replyto": "73w655DPf1", "signatures": ["ICLR.cc/2026/Conference/Submission7963/Reviewer_dN7n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7963/Reviewer_dN7n"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762067802036, "cdate": 1762067802036, "tmdate": 1762919976265, "mdate": 1762919976265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}