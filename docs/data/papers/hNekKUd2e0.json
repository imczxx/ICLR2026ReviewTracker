{"id": "hNekKUd2e0", "number": 3560, "cdate": 1757474859259, "mdate": 1763632161348, "content": {"title": "Can Hypergraph Models Be Strong Baselines For Node-level Tasks? Sloving the Hyperedge Pollution Problem", "abstract": "Compared to hypergraph-based methods, graph transformers (GTs) and graph neural networks (GNNs)-based models have recently dominated node-level tasks, becoming widely adopted baselines in graph representation learning. However, in this work, HPHNN is proposed to address the Hyperedge Pollution (HP) problem in hypergraph construction, enabling classical hypergraph models to significantly outperform existing GT and GNN models and to serve as strong baselines for node-level graph tasks. Experimental results on 11 real-world graph datasets demonstrate that hyperedge-based models not only outperform these baselines but also exhibit more stable and generalizable performance across diverse node-level tasks by mitigating the HP problem. The findings of this paper challenge the prevailing view that GTs and GNNs possess inherent superiority in node-level graph tasks, establishing another strong baseline model for graph representation learning.", "tldr": "", "keywords": ["Graph Neural Networks", "Hypergraph Neural Network", "Graph Node Classification", "Hypergraph Structure Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/65773a5e847bfe555d98736adc8243bae1bd0382.pdf", "supplementary_material": "/attachment/c02b0b84d0f6180dfd65f4fcb3cabbc55d682590.zip"}, "replies": [{"content": {"summary": {"value": "The paper identifies that hypergraph neural networks (HGNNs) lag behind graph neural networks (GNNs) and graph transformers in node-level tasks due to the Hyperedge Pollution (HP) problem, for which the hypergraph results in a high hyperedge heterogeneity due to nodes of different types being incorrectly grouped into the same hyperedge. To address node-level tasks, the paper proposes a HP-based dynamic HGNN, which adaptively updates the hypergraph structure to mitigate pollution. Evaluation on 11 benchmark datasets shows that the proposed method outperforms the baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- Observation that HGNNs fail due to Hypergraph Pollution is \n\n- Experimental results on the benchmark datasets show consistent improvement, outperforming the baselines."}, "weaknesses": {"value": "- While the paper states that HP is the key reason for the low performance of HGNNs, the explanation and experimental justification are insufficient. Why does a high HP score lead to low performance on node-level tasks? What is the correlation between the HP score and the performance on node-level tasks?\n\n- In lines 201-202, the paper claims that a higher HP score reflects greater structural inconsistency. Why? I can't find sufficient justification for this, and this seems to be a core motivation for the proposed method.\n\n- While the proposed HPHNN results in low HP scores and better node-level task performance, how can you say a low HP score led to better performance? I'd like to see an analysis or experimental result for this causality. I find experimental analysis in Appendix A.3., but this does not show any causality.\n\n- Readability of the paper is poor. Reported experimental results are hard to read: For example, numbers in Table 1 or results in Figure 3 are hard to understand due to the small font. Also, readability could be improved by using the \\citep command in latex\n\nSince I may have missed several details and analysis, I'll raise my score if these concerns are sufficiently addressed."}, "questions": {"value": "Please address the questions raised in the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xPfmGscZPU", "forum": "hNekKUd2e0", "replyto": "hNekKUd2e0", "signatures": ["ICLR.cc/2026/Conference/Submission3560/Reviewer_jVJ1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3560/Reviewer_jVJ1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761487169210, "cdate": 1761487169210, "tmdate": 1762916823612, "mdate": 1762916823612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new graph node label prediction method based on a dynamic hyperedge construction mechanism, achieving superior label accuracy over all baselines on 11 datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method achieves consistently higher accuracy than several strong baselines across a diverse set of datasets.\n\n2. The strategy of constructing dynamic hyperedges at each layer based on node similarity appears to be a novel and effective design."}, "weaknesses": {"value": "1. The overall presentation of the paper requires substantial improvement. The abstract is rather confusing, and the details of the general HGNN algorithm should be introduced before presenting the proposed method. Moreover, there is extensive misuse of *citep* and *citet*, which does not comply with the formatting guidelines.\n\n2. Enhancing accuracy on node-level label prediction, where existing baselines already perform extremely well, contributes little practical value. This problem has been extensively studied over the years and is largely considered to be well addressed.\n\n3. HGNN is essentially a variant of GNN. Achieving better performance after addressing the pollution issue does not constitute a paradigm-level advancement."}, "questions": {"value": "no specific questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iJEbUVNqFX", "forum": "hNekKUd2e0", "replyto": "hNekKUd2e0", "signatures": ["ICLR.cc/2026/Conference/Submission3560/Reviewer_RChm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3560/Reviewer_RChm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652322527, "cdate": 1761652322527, "tmdate": 1762916822991, "mdate": 1762916822991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that hypergraph models often underperform GNNs and GTs on node-level tasks due to the Hyperedge Pollution (HP) problem, where nodes of different types are incorrectly grouped within the same hyperedge. The authors propose HPHNN, a dynamic hypergraph framework that formally defines and quantifies this issue using an \"HP score.\" The model then iteratively refines the hypergraph structure to minimize this pollution, improving hyperedge consistency. Experiments on 11 real-world datasets show HPHNN significantly outperforms existing GNN, GT, and hypergraph-based models."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper introduces the problem of hyperedge pollution in hypergraph learning. It proposes a hypergraph neural network that dynamically updates the hypergraph structure. It uses a principled approach of indirectly optimizing the hyperedge pollution (HP) metric that is proposed in this paper. THe HP metric is simple and satisfied some basic theoretical properties as discussed in the Appendix.\n\n2. The paper demonstrates its model's merit through evaluation on 11 diverse benchmark datasets and a diverse set of baseline algorithms.\n\n3. The overall writing is easy to follow and the flow is good."}, "weaknesses": {"value": "However, the paper has several weaknesses.\n\n1. A major claimed novelty of the paper - the heuristic based metric Hyperedge Pollution (HP) score is too generic in nature. It completely ignores the fact that there are networks where dis-similar entities come and form a group. A interdisciplinary research paper coauthored by a set of researchers having very different backgrounds can be an example. There are other types of heterogeneous hypergraphs. A metric which just relies on the cosine similarity between the features (or embeddings) of the nodes within a hypergraph is just too simple to capture such complex use case. There is hardly any theoretical justification on the construction of HP. The theory presented in Appendix on HP is rather trivial and shows some simple properties that HP satisfies. It does not address the basic question of why it is a good metric for hypergraph learning.\n\n2. There are multiple design choices made in the paper without proper justification. For example, HP score (in Eq. 1 and 2) is based on cosine similarities between the embeddings. But k-means algorithm used in Eq. 4 is based on L2 distances. Similarly, L2 distance is again used in the loss function in Eq. 18. There is no explanation on the use of different types of distance functions in different places. This limits the technical contribution and soundness of the approach. \n\n3. In Equation 13, an important hyperparameter \\theta is introduced. But it is not clear how to fix it for a new graph."}, "questions": {"value": "There are few more clarifications needed in the paper.\n\n1. The motivation of the paper could have improved. The direct comparison on the node classification performance between hypergraph and graph is not fair. There are applications where the inherent input is a graph and there are applications where the inherent input is a hypergraph. Yes, researchers often convert graph to hypergraph, or vice-versa. But that is not a suitable motivation for a paper.\n\n2. In Equation 19, what is \\pi(i)?\n\n3. There is a hypergraph attention network which computes attention scores between the nodes within a hyperedge through an indirect formation of a line graph. I suggest the authors to check if that can be used as a baseline: \"Hypergraph Attention Isomorphism Network by\nLearning Line Graph Expansion\" (IEEE Big Data 2020)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1zSYkmboPF", "forum": "hNekKUd2e0", "replyto": "hNekKUd2e0", "signatures": ["ICLR.cc/2026/Conference/Submission3560/Reviewer_jWFd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3560/Reviewer_jWFd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924887906, "cdate": 1761924887906, "tmdate": 1762916822637, "mdate": 1762916822637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors argue that the empirical limitations of hypergraph neural networks (HGNN) largely stem from the “Hyperedge Pollution” (HP) problem, where semantically different nodes are grouped together within hyperedges, leading to heterogeneous representations that inhibit node-level learning. They then propose an HP score as a metric to quantify pollution, and propose a dynamic HGNN called HPHNN that uses an iterative refinement scheme based on a quality score (which leverages the proposed HP score) to dynamically adapt hyperedges over the course of training. HPHNN reports impressive metrics and outperforms GNN, GTs and HGNN baselines across datasets collected from a wide array of domains, albeit at highly increased computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper combines multiple potentially impactful contributions in (a) characterizing the hyperedge pollution problem and connecting it with the limitations of HGNNs, (b) deriving a metric to quantify it, and (c) use the proposed metric to propose a powerful HGNN model. The overall potential contribution of the paper is thus significant.\n2. The paper construction is overall quite sound; despite the issues with clarity regarding specifics (see Weaknesses), the high-level picture is quite easy to follow and the individual contributions tie into each other well.\n3. The empirical results are consistent and convincing. I appreciate the extensive studies provided, in particular the ablation studies and efficiency analysis. The visualizations in Figure 3 are excellent, both visually appealing and highly useful in conveying key results."}, "weaknesses": {"value": "1. Computational inefficiency & fair evaluation: Clearly the biggest drawback of the proposed method is its comparatively _immense_ computational cost: According to Table 2 results, HPHNN is approximately 2 to 5.4 times slower than the next slowest method (TDHNN), and almost 200 times slower than the fastest method (GraphMVM) across datasets. This is clearly by construction: A GNN is used to ‘initialize’ embeddings, which is followed by repeated HP-score and k-means computation; all costly operations on top of a computationally heavy HGNN construction. While the standalone results are impressive, the computational cost of the model represents a clear scalability bottleneck and arguably leads to an unfair evaluation compared with the baselines (see Questions section for suggestions).\n2. The writing is uneven in places, with many typos and ambiguous statements, and terms left unreferenced and clarified. It affects readability to the point that it does have an compounding overall impact on the score. Specifics are as follows:\n   1. Typos (non-exhaustive):\n      1. In the title (!): Sloving -> Solving\n      2. Across the paper (particularly in sections 1, 2 and 4.1) `\\citet` is used in place of `\\citep`, usually also without any preceding or trailing whitespace.\n      3. L281: insatiable -> unsuitable\n      4. L287: learn -> learned\n      5. L226/307: The capitalization of `\\texttt{clustering}` is inconsistent.\n   2. Section 3.2.4, a crucial part of methodology, is missing definitions and references regarding the individual components of Eq. 12. See Questions section for specific clarifications required."}, "questions": {"value": "1. Are there any prior works on characterizing the limitations of HGNNs from a perspective similar to Hyperedge Pollution (or other perspectives, for that matter)? I think the contribution of HP is valuable, but my impression is that while the connections of HP to node heterogeneity is clearly stated, the relationship between hyperedge heterogeneity and node-level task performance seems more or less taken for granted. I would like to see this relationship more clearly stated, with references to prior work where applicable. \n2. How do the parameter counts for HPHNN compare with other baselines? I suspect the long wall-clock times of HPHNN runs have more to do with the repeated application of HP-scoring and k-means, rather than actual parameter counts, when compared with other HGNNs. I have two suggestions to get a better understanding of the computational costs:\n   1. A study to time the individual components of HPHNN to clarify the sources of inefficiency; I think this would also provide good directions for future work (e.g. can one replace the costly components with faster approximations?) and complement other ablation studies. \n   2. To account for a fairer evaluation given the high computational cost of HPHNN, I would like to see how parameter scaling effects HPHNN performance. Can we still improve on the other baselines while using a faster, less parametrized model. This would help clarify the performance-cost trade-off of the HPHNN model and accordingly help better evaluate its utility.\n3. Re: Weakness 2.2:\n   1. It is denoted that Satur($\\mathcal{E}$) refers to hyperedge saturation,  measuring “the number of vertices participating in the hyperedges”, which is abiguous. Is this simply the cardinality of hyperedges (I don’t think so), or a ratio of sorts. How is it computed? Is it proposed by the authors or adapted from another paper?\n   2. The same questions apply to the “silhouette score” Silh($\\hat{\\textbf{Z}}, \\textbf{Y}$), except it is already estsablished as a metric for clustering which the authors fail to mention: The purpose and source of the term [1] should be clearly stated. Ideally, an ablation study on each term of Eq. 12 would help solidify the utility of the individual terms.\n\n**Conclusion:** The paper merits acceptance on the basis of several non-trivial contributions to the subfield of hypergraph learning. However, computational bottlenecks of HPHNN likely inhibit the potential utility of the method, and alongside some issues with clarity, they prevent me from recommending a higher score for the time being. \n\n[1] Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20, 53–65. doi:10.1016/0377-0427(87)90125-7"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BGWLJogv9h", "forum": "hNekKUd2e0", "replyto": "hNekKUd2e0", "signatures": ["ICLR.cc/2026/Conference/Submission3560/Reviewer_fktw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3560/Reviewer_fktw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762307315046, "cdate": 1762307315046, "tmdate": 1762916820181, "mdate": 1762916820181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}