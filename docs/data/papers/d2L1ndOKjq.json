{"id": "d2L1ndOKjq", "number": 17832, "cdate": 1758281037893, "mdate": 1759897151242, "content": {"title": "Foundation Models for Causal Inference via Prior-Data Fitted Networks", "abstract": "Prior-data fitted networks (PFNs) have recently been proposed as a promising way to train tabular foundation models. PFNs are transformers that are pre-trained on synthetic data generated from a prespecified prior distribution and that enable Bayesian inference through in-context learning. In this paper, we introduce CausalFM, a comprehensive framework for training PFN-based foundation models in various causal inference settings. First, we formalize the construction of Bayesian priors for causal inference based on structural causal models (SCMs) in a principled way and derive necessary criteria for the validity of such priors. Building on this, we propose a novel family of prior distributions using causality-inspired Bayesian neural networks that enable CausalFM to perform Bayesian causal inference in various settings, including back-door, front-door, and instrumental variable adjustment. Finally, we instantiate CausalFM and train our foundation models for estimating conditional average treatment effects (CATEs) for different settings. We show that CausalFM performs competitively for CATE estimation using various synthetic and semi-synthetic benchmarks. In sum, our framework can be used as a general recipe to train foundation models for various causal inference settings. In contrast to the current state-of-the-art in causal inference, CausalFM offers a novel paradigm with the potential to fundamentally change how practitioners perform causal inference in medicine, economics, and other disciplines.", "tldr": "", "keywords": ["Causal Inference", "Treatment Effect Estimation", "Foundation Models"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc139807edc77a2e6bf334b7dc2afb3a9250e085.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CausalFM, a framework for training transformer-based Prior-data Fitted Networks (PFNs) as foundation models for causal inference. The approach centers on \"C-SCM-Priors,\" novel prior distributions over Structural Causal Models (SCMs) constructed to explicitly satisfy identifiability assumptions for specific causal settings (e.g., back-door, front-door, IV). The paper presents a theoretical argument that restricting the prior to identifiable SCMs is necessary for the resulting PFN to be \"well-specified\" and yield consistent causal effect estimates. CausalFM is pre-trained on synthetic data from these priors, enabling it to perform Bayesian causal inference (specifically, CATE estimation) via in-context learning on new observational datasets without retraining. Experiments demonstrate competitive performance against specialized baselines in the targeted settings.\n[Note: I have used LLMs to improve my writing and help me answer paper questions]"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides a clear theoretical argument for why PFN priors in this Bayesian approximation context should enforce identifiability for consistent estimation (I am a little skeptical if we should make this assumption, see below - but realizing and demonstrating it is very valuable).\n- Introduces a structured way to build priors over SCMs using BNNs that respect the assumed causal graph structure and identifiability conditions.\n- CausalFM is designed to handle back-door, front-door, and IV settings, offering broader applicability than methods restricted to unconfoundedness, like CausalPFN.\n- Achieves strong average empirical results across multiple benchmarks and settings compared to specialized estimators that require per-dataset training and tuning."}, "weaknesses": {"value": "- Reliance on Correct Identifiability Assumptions: The framework's core premise requires the user to correctly identify the true causal structure and select the appropriate identifiability strategy (back-door, front-door, IV) before applying the model. This is a strong assumption, as determining the correct causal graph and valid adjustment strategy from domain knowledge alone is often a major challenge in real-world applications. CausalFM automates estimation given these assumptions but offers no mechanism to validate them or handle uncertainty about the true causal structure. Potential Mismatch with Reality vs. Uncertainty Modeling: By strictly requiring identifiable priors for consistency (Thm 4.3), CausalFM might struggle or produce overly confident (but potentially wrong) estimates if the real-world DGP is non-identifiable or if the chosen identifiable structure is incorrect. CausalFM prioritizes potential consistency under strong assumptions over explicitly modeling uncertainty arising from potential non-identifiability.\n\n- The effectiveness depends on the specific design of the BNN-based C-SCM-Prior. The paper lacks sensitivity analyses regarding the prior's construction (e.g., BNN architecture, diversity of sampled SCMs) and how well this synthetic prior generalizes to the complexities of real-world data generation processes."}, "questions": {"value": "- How sensitive is CausalFM's performance if the user specifies a causal setting (e.g., assumes back-door identifiability) but the underlying data violates this assumption mildly (e.g., weak unobserved confounding)? Does the model fail catastrophically, or does it offer graceful degradation?\n- How critical are the specific choices made in the BNN architecture and sampling process for the C-SCM-Prior (Sec 4.3)? Were alternatives explored, and how much does performance vary with changes to the prior generation mechanism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "psV9jUhVCV", "forum": "d2L1ndOKjq", "replyto": "d2L1ndOKjq", "signatures": ["ICLR.cc/2026/Conference/Submission17832/Reviewer_V8FG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17832/Reviewer_V8FG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761404148724, "cdate": 1761404148724, "tmdate": 1762927668513, "mdate": 1762927668513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose CausalFM for training foundation models for causal inference based on existing Prior-data Fitted Networks (PFNs). The core idea is to pre-train a transformer model on synthetic data generated from a family of Structural Causal Model (SCM), enabling zero-shot causal estimation in various settings, including back-door, front-door, IV, via in-context learning without retraining. The approach addresses a significant limitation of traditionally causal inference methods. The empirical results show competitive performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Beyond dataset-specific causal inference to in-context causal inference is a promising and important research direction.  In the paper, the authors propose a framework which is not restricted to a single causal setting. It supports back-door, front-door, and instrumental variable scenarios within a single model.\n\nThe introduction of the \"well-specified prior\" concept and the argument for incorporating identifiability assumptions into the prior are valuable theoretical insights.\n\nThe results on synthetic, semi-synthetic (Jobs), and other datasets demonstrate the model's versatility and strong performance."}, "weaknesses": {"value": "The pre-training methodology in this paper appears largely similar to exsiting work in this area \n\nThe proof sketches are quite dense and lacks intuition, then are hard to follow.\n\nThe 24-hour training time on an A100 GPU is mentioned, but there is no discussion of inference speed or comparative training costs of the baselines."}, "questions": {"value": "In fact, some researchers have begun leveraging PFNs to pre-train causal foundation models, such as the following work. The pretraining method used in this paper is very simimlar to the existing work, although the authors have discussed these methods in the paper. In addition, the authors critique the following work for being restricted to back-door adjustment. However, this work can actually handle back-door, front-door, and IV settings via in-context learning without retraining. Since the pre-training methodology in this paper appears largely similar, its claimed novelty over these prior works remains unclear.\n\nJake Robertson, Arik Reuter, Siyuan Guo, Noah Hollmann, Frank Hutter, and Bernhard Scholkopf. Do-pfn: In-context learning for causal effect estimation, arXiv preprint, arXiv:2506.06039, 2025.\n\nIn the back-door training loss (Eq. 9), the model is trained to predict the individual treatment effect directly. What is the rationale behind this and whether you experimented with predicting potential outcomes Y(a) separately and then differencing?\n\nThe prior in Sec. 4.3 is based on a well-specified C-DAG provided by the practitioner. How sensitive is the method to misspecification of this C-DAG?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SxxUbQtRRV", "forum": "d2L1ndOKjq", "replyto": "d2L1ndOKjq", "signatures": ["ICLR.cc/2026/Conference/Submission17832/Reviewer_Zj58"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17832/Reviewer_Zj58"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963905268, "cdate": 1761963905268, "tmdate": 1762927668000, "mdate": 1762927668000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CausalFM, a new framework for training foundation models (transformers) to perform causal inference via prior-data fitting. The authors formalize how to construct Bayesian prior distributions based on structural causal models (SCMs) for various causal inference tasks, and derive necessary criteria to ensure these priors lead to valid, identifiable causal inferences. Using this theory, they propose a novel family of SCM-based priors implemented through Bayesian neural networks, which enables the PFN (Prior-Data Fitted Network) to carry out Bayesian causal effect estimation in-context for settings including back-door (unconfounded observational studies), front-door (mediator adjustment), and instrumental variable (IV) scenarios. The paper then instantiates CausalFM by training transformer models on synthetic data generated from those SCM-based priors, effectively teaching the model to infer causal effects (e.g. Conditional Average Treatment Effect, CATE) without retraining on new datasets. Empirically, CausalFM achieves competitive or superior performance compared to both standard specialized estimators and prior foundation-model baselines, across diverse benchmarks for CATE (unconfounded), IV, and front-door settings. In many cases, the CausalFM model’s in-context predictions match or outperform state-of-the-art alternatives even though those baselines are trained specifically for each task. Overall, the paper’s main result is a general recipe to train a single tabular foundation model for causal inference tasks, offering a new paradigm that could improve flexibility (test-time inference without retraining) in fields like medicine and economics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a novel approach by leveraging prior-data fitted networks for causal inference, which might be the first to provide a comprehensive foundation model (CausalFM) covering multiple settings in one framework. Unlike past works that focus on a single identification strategy (e.g. only back-door criteria) or require task-specific models, CausalFM uses SCM-based priors to train a transformer that can flexibly handle back-door, front-door, and IV adjustments within one model. This is one step closer to a real causal FM: the model effectively learns to “select” the appropriate causal estimation formula from data context, enabling zero-shot inference on new datasets without retraining – a clear paradigm shift from traditional retrain-per-dataset approaches. The introduction of SCM-guided synthetic training data (via a Bayesian neural network prior) is especially novel, as it injects domain causal knowledge into the foundation model’s pretraining process. This combination of foundation models + causal SCM priors is an original contribution that extends the foundation model concept into the causal inference domain in a principled way.\n\n- The paper is grounded in strong theory. The authors formally define conditions under which a causal effect is identifiable and incorporate those into the prior design. In particular, they derive necessary criteria for a valid causal prior, proving that any well-specified prior must assign zero probability to SCMs that violate the identifiability of the target causal query. This result (Theorem 4.3) ensures that the model’s Bayesian posterior cannot mislead us with non-identifiable alternatives, thereby guaranteeing asymptotic consistency of the causal estimates under the given assumptions. Furthermore, by building on a Bayesian PFN framework, CausalFM naturally provides uncertainty quantification and connects to Bayesian consistency results (e.g. invoking a Bernstein–von Mises argument for the posterior on the observational distribution). The paper’s separation of identifiability (ensured via SCM prior design) and statistical estimation (learned by the PFN) follows established causal inference principles (Pearl’s identification-before-estimation philosophy), lending the approach a sound theoretical foundation. Overall, the inclusion of formal definitions, theorems, and proofs about prior construction, consistency, and identifiability demonstrates a high level of theoretical rigor that bolsters confidence in the method’s validity.\n\n- Strong Empirical Performance: The experimental evaluation is thorough and shows competitive or superior performance of CausalFM on a wide range of benchmarks. The authors test their model in standard back-door (observational CATE estimation) scenarios (including synthetic datasets and a semi-synthetic Jobs dataset), in instrumental variable settings (with both binary and continuous instruments), and in front-door mediation scenarios. Across these experiments, CausalFM’s one-model approach achieves error rates (PEHE) that are on par with or better than specialized models. For example, in CATE estimation, CausalFM slightly outperforms several state-of-the-art estimators (e.g. S-learner, T-learner, TARNet) and also improves over prior foundation model baselines: CausalFM attained a lower PEHE (≈0.51) than CausalPFN (≈0.56) or DoPFN (≈0.59) on synthetic CATE benchmarks. In the IV setting, CausalFM similarly achieved the best PEHE among foundation models (0.42 vs. 0.52 for DoPFN) and was comparable to the top specialized IV methods. Notably, in a challenging front-door adjustment task with confounded treatment and mediator, the CausalFM model obtained a PEHE around 0.90, outperforming a baseline foundation model (DoPFN at 1.27) and all but the very best plug-in front-door estimator. These results indicate that CausalFM not only generalizes across different causal inference problems but often matches or exceeds the accuracy of dedicated models, all while requiring no retraining per new dataset. The consistency of CausalFM’s performance across diverse settings and its edge over previous PFN approaches (which were limited to back-door or had no identifiability guarantees) stand out as a major strength. This empirical evidence convinces the reader that the proposed foundation model approach is viable and competitive in practice."}, "weaknesses": {"value": "- While CausalFM is conceptually flexible, there may be practical scalability challenges. Training a PFN of this sort involves generating and learning from a very large number of synthetic datasets drawn from complex SCM priors, which is computationally intensive. The transformer model itself has a fixed context length and model size – this could limit the scale of datasets it can handle at test time (e.g. number of samples or covariates) unless the architecture is scaled up. The paper does not extensively discuss memory or runtime implications; however, foundation models for tabular data (like TabPFN) are known to handle only modest-sized datasets due to the need to input the dataset as context. If one were to apply CausalFM to truly large real-world datasets (say, thousands of units or very high-dimensional covariates), it is unclear if the current model would remain efficient or accurate. In short, questions of scalability (both in training cost and inference on large data) remain a concern. The method’s impressive performance is demonstrated on reasonably sized benchmarks, but its feasibility on substantially bigger or more complex tasks has not been shown, potentially limiting its immediate practical adoption for “big data” causal inference.\n\n- The work would benefit from deeper analysis of why CausalFM works so well. Currently, there is little in the way of ablation studies or interpretability insights. For example, the framework involves several design choices – using a Bayesian neural network prior, simulating counterfactuals during training, particular choices in the SCM parameterization – yet the paper does not present ablation experiments to quantify the contribution of each component (e.g., what if the prior did not enforce certain assumptions, or if the counterfactual simulation was removed?). Without such ablations, it is hard to assess which aspects of the CausalFM design are most critical. Additionally, the interpretability of the PFN’s in-context inference mechanism is not explored. CausalFM essentially acts as a black-box that implicitly decides which causal adjustment to apply (back-door vs front-door vs IV) based on the input data. However, the readers are not shown how the model makes this decision or whether it aligns with known causal formulas in each case. There is no analysis, for instance, of attention weights or internal representations to reveal if the model focuses on certain variables (e.g. an instrument or mediator) to choose the appropriate formula. This lack of interpretability means practitioners might find it hard to trust or validate the model’s reasoning on a given dataset. In summary, the paper’s experiments focus on accuracy, but omit diagnostic analyses: one misses ablation studies to justify design choices and interpretability checks to illuminate the model’s decision-making. This is a limitation because understanding the model’s behavior would greatly enhance confidence and provide insights for future improvements.\n\n- The success of CausalFM hinges on the assumption that the chosen prior distribution (the family of SCMs used in training) accurately captures the relevant causal structure of the target domain. This could make the approach sensitive to mis-specification. If a real-world scenario violates the assumptions baked into the prior (for example, there is an unobserved confounder not accounted for, or the functional relationships differ substantially), the model’s inferences may become unreliable. The authors explicitly restrict the priors to settings where identifiability holds, which is sensible, but it also means the model is not trained to handle scenarios outside those assumptions (e.g. it wouldn’t know how to behave if confronted with a non-identifiable case, except to potentially output high uncertainty). It would strengthen the work to understand how robust the model is to slight violations of its assumptions – this is not addressed in the current evaluation. Moreover, real-world validation is limited in the experiments. While the paper includes a semi-synthetic example (Jobs dataset with simulated outcomes) and numerous fully synthetic benchmarks, it does not demonstrate CausalFM on purely real observational data where ground-truth causal effects are unknown. Consequently, it remains to be seen how the model performs in practice on real datasets, and how one would verify its estimates when the true effect is not available for comparison. The lack of real-world case studies means the approach’s practical utility is still somewhat speculative. In essence, the framework’s generalizability to real, messy data and its robustness to deviations from assumed priors are open questions not fully answered by the paper. These limitations suggest that further work is needed to test CausalFM under less ideal conditions and to guide users in setting an appropriate prior for their specific application."}, "questions": {"value": "- Robustness to Prior Mis-specification: How sensitive is CausalFM to the correctness of its prior assumptions in practice? In a scenario where the true data-generating process deviates from the assumed SCM prior (for instance, an unobserved confounder is present when the model assumed none, or the functional form of relationships is different), what would happen to the model’s performance? Would the in-context learner recognize the model mismatch (perhaps via larger predictive uncertainty), or could it yield biased estimates? It would be helpful if the authors could clarify whether any experiments were done to assess robustness when the test data falls slightly outside the support of the training prior, and if not, how might one ensure reliability of CausalFM in such cases (e.g., by broadening the prior or diagnosing posterior outputs).\n\n- Interpretability and Practical Use: While the model automatically selects an identification strategy based on the data, it operates as a black box from the user’s perspective. Do the authors have insights or plans for making CausalFM’s reasoning more interpretable? For example, is it possible to extract which causal adjustment the model is implicitly using for a given dataset (such as detecting that it’s performing an IV-like computation versus a back-door adjustment)? Understanding this could be important for practitioners to trust the results. Moreover, how would the authors recommend validating CausalFM’s outputs on a real-world problem where the true causal effect is unknown? Are there diagnostic checks or visualizations one can use (perhaps analyzing the learned attention weights or the model’s posterior over causal effects) to ensure the model’s inference aligns with domain knowledge? Clarifying these points would help users apply CausalFM more confidently and transparently in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Jg07ZbKsDj", "forum": "d2L1ndOKjq", "replyto": "d2L1ndOKjq", "signatures": ["ICLR.cc/2026/Conference/Submission17832/Reviewer_r9r3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17832/Reviewer_r9r3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762066219718, "cdate": 1762066219718, "tmdate": 1762927667552, "mdate": 1762927667552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}