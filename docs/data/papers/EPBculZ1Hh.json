{"id": "EPBculZ1Hh", "number": 15075, "cdate": 1758247478006, "mdate": 1759897330595, "content": {"title": "Ready2Unlearn: A Learning-Time Approach for Preparing Models with Future Unlearning Readiness", "abstract": "This paper introduces Ready2Unlearn, a learning-time optimization approach designed to facilitate future unlearning processes. \nUnlike the majority of existing unlearning efforts that focus on designing unlearning algorithms, which are typically implemented reactively when an unlearning request is made during the model deployment phase, Ready2Unlearn shifts the focus to the training phase, adopting a \"forward-looking\" perspective. Building upon well-established meta-learning principles, Ready2Unlearn proactively trains machine learning models with unlearning readiness, such that they are well prepared and can handle future unlearning requests in a more efficient and principled manner. Ready2Unlearn is model-agnostic and compatible with any gradient ascent-based machine unlearning algorithms. We evaluate the method on both vision and language tasks under various unlearning settings, including class-wise unlearning and random data unlearning. Experimental results show that by incorporating such preparedness at training time, Ready2Unlearn produces an unlearning-ready model state, which offers several key advantages when future unlearning is requested, including reduced unlearning time, improved retention of overall model capability, and enhanced resistance to the inadvertent recovery of forgotten data. We hope this study could inspire future work to explore more proactive strategies for equipping machine learning models with built-in readiness towards more reliable and principled machine unlearning.", "tldr": "This paper introduces Ready2Unlearn, a learning-time optimization approach designed to facilitate future unlearning processes.", "keywords": ["Machine unlearning", "Meta-learning", "Data privacy"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f7bb689eb54f50f5b13975731d857687b9ca0d5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Ready2Unlearn, a meta-learning inspired training pipeline that prepare the a particular deep machine learning model to be able to easily unlearn. Ready2Unlearn uses meta-learning principles to train models in a way that makes them easier to unlearn later. Specifically, it simulates future unlearning via gradient ascent during pre-training, and optimizes the model to (1) forget efficiently, (2) retain useful knowledge, and (3) counter relearning forgotten data. Based on the formulation provided the method is model-agnostic. Based on the experiments conducted on image and language tasks show that Ready2Unlearn improves unlearning speed, robustness, and performance retention compared to standard and baseline approaches."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of making models unlearning-ready at training time is a significant conceptual shift from existing reactive methods.\n\n2. Proactive unlearning via meta-objectives is a novel contribution in the unlearning space. While some prior works use meta-learning for unlearning itself, this is the first to integrate it during the initial training phase.\n\n3. Partitioning training data into revocable (likely to be unlearned) and stable categories and incorporating this into the meta-objective is novel, despite its overly-simplified assumption."}, "weaknesses": {"value": "1. While the results are averaged, there are no confidence intervals or statistical significance tests, limiting interpretability and reliability, especially in sensitive areas like privacy.\n\n2. The added cost of meta-learning during training (e.g., outer-loop gradients, more iterations) is not thoroughly quantified or compared. For example, meta learning would be hard to scale to large-scale models, and given the context-dependent and evolving nature of high-risk and low-risk splits it may be hard to adjust the model dynamically at the time of unlearn-aware training.\n\n3. The framework assumes training data can be split neatly into revocable and stable categories, which may not hold in practice. In reality, these categories classification may be context-dependent, evolving, or uncertain. Hence, it seems that making the model to be unlearning aware is a bit of an overcommitment. In practice, knowing what data may be subject to future unlearning isn't always feasible, making the deployment of the framework non-trivial in many real-world applications."}, "questions": {"value": "1. How does Ready2Unlearn perform when the forget set overlaps significantly with the retain set?\n2. Can the meta-objective be extended to support non-gradient ascent based unlearning techniques?\n3. Is it feasible to adapt Ready2Unlearn in settings where the forget set is unknown at training time?\n4. How sensitive is the performance to $\\lambda_1$, $\\lambda_2$, $\\lambda_3$ loss weights, and are there principled ways to set them?\n5. Can this approach scale to very large models and datasets without significant increases in training cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QKug2MJPC4", "forum": "EPBculZ1Hh", "replyto": "EPBculZ1Hh", "signatures": ["ICLR.cc/2026/Conference/Submission15075/Reviewer_kNNt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15075/Reviewer_kNNt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761616573551, "cdate": 1761616573551, "tmdate": 1762925401725, "mdate": 1762925401725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Ready2Unlearn, a meta-learning framework that prepares models to support future unlearning efficiently and reliably. Instead of treating unlearning as a post-hoc reactive process, Ready2Unlearn applies meta-learning optimization during training to anticipate future unlearning requests. The method explicitly optimizes three “forward-looking” objectives, efficiency, retention, and resistance, so that later gradient-ascent-based unlearning is faster, preserves overall performance, and resists recovery of forgotten data. Experiments across image classification (MNIST, PathMNIST) and language tasks (MUSE-Books, MUSE-News, LLaMA/GPT-2) show improved unlearning speed and robustness compared to several baselines (loss reweighting, DP-SGD, NEFTune, etc.)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Novel Research Direction: The paper introduces a genuinely new direction for the machine unlearning community by emphasizing preparation for unlearning during the training stage. This proactive framing, rather than the usual reactive approach, is both novel and conceptually interesting.\n2. Writing Clarity:  The writing is clear and well-organized. The argument flows naturally, and Figure 2 does an excellent job of visually illustrating the key idea. The authors make good use of space by focusing the main text on intuitive explanations and leaving the algorithmic details to the appendix.\n3. Method Simplicity: The method itself is simple and practical. It is well motivated, clearly explained with minimal notation, and appears easy to implement across different architectures and codebases."}, "weaknesses": {"value": "1. Time/Memory Complexity: While the motivation behind the approach is clear and compelling, the paper does not discuss the additional computational or memory costs introduced by the meta-learning framework. Information about training time, memory footprint, and convergence behavior would be valuable for assessing the practical feasibility of the method, especially for large-scale models.\n2. Forget Size Limit: The paper does not clearly address the scalability of the proposed approach with respect to the size or diversity of the forget set. It would be helpful to understand how computational requirements and unlearning performance scale as the number or diversity of forget samples increases.\n3. Unintended Consequences: While the paper does not report any unintended side effects, it would be helpful for the authors to discuss potential consequences of the proposed training structure. For example, could a model trained with unlearning preparedness become less adaptable when learning new tasks, as in standard continual learning scenarios? Additionally, if the model is later fine-tuned on many different tasks or asked to unlearn data outside the original forget set, might this shift it away from the “prepared” loss basin and diminish its unlearning efficiency, retention, or robustness? \n4. Relearning Set Definition: The paper describes the relearning set as data with “similar distributional characteristics, such as stylistically or semantically similar examples,” but this notion remains somewhat vague. It would help if the authors provided a more formal definition or concrete examples of what constitutes such similarity. For instance, how similar can the relearning set be to the forget set before it no longer serves as a valid proxy for recovery evaluation? The boundary between the two is unclear, particularly since the relearning experiment is presented for text generation but not for image classification. Would fine-tuning on digits not included in the forget class, for example, still fall within the intended definition of relearning?\n5. Experimental Clarifications: In Figure 6, the difference between the plateau points for the prepared and unprepared models appears relatively small. A more detailed quantitative analysis, perhaps through an ablation study, could clarify how each component of the final objective, especially the term intended to enhance resistance to relearning, contributes to this effect."}, "questions": {"value": "1. The resilience term is motivated as a way to direct the model toward learning more distinctive, data-specific features of the forget set, thereby preventing easy relearning. Do you observe this effect empirically, for example, through feature visualizations showing that the inclusion of the resilience term makes the forget and retain representations more disjoint?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hq7DAZ1MOZ", "forum": "EPBculZ1Hh", "replyto": "EPBculZ1Hh", "signatures": ["ICLR.cc/2026/Conference/Submission15075/Reviewer_heUx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15075/Reviewer_heUx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797895354, "cdate": 1761797895354, "tmdate": 1762925401284, "mdate": 1762925401284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose Ready2Unlearn (R2U), which prepares models for future unlearning requests proactively during training. They use some MAML inspired dual-loop optimization to optimize for resistance metrics for when unlearning occurs later."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The novel perspective of shifting from reactive to proactive unlearning is interesting and paradigmaticallly different.\n2. The method is model agnostic and doesn't require architecture specific unlearning algorithms.\n3. The visual comparisons and systematic evals across many datasets provide clear evidence of effectiveness in the setup context."}, "weaknesses": {"value": "1. The core assumption of the paper regarding being able to reliably predict which data is \"revocable\" vs \"stable\" is unrealistic, and the paper provides no principled approach for making this classification.\n2. Due to a lack of theoretical foundation, it is hard to understand why this approach should work. There is no convergence analysis as well.\n3. The proposed experimental setups where there are designated forget and retain classes is not realistic. \n4. There is no baseline comparison against existing methods like influence-function based approaches or differential privacy techniques."}, "questions": {"value": "1. How does the method handle cases where the revoke-stable categorization is wrong?\n2. What happens when multiple overlapping unlearning requests occur?\n3. What is the rationale behind using gradient ascent based unlearning when established parallel methods use other approaches?\n4. How does performance change when recovery attacks use the actual forgotten data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WRkIutx4V7", "forum": "EPBculZ1Hh", "replyto": "EPBculZ1Hh", "signatures": ["ICLR.cc/2026/Conference/Submission15075/Reviewer_kqD5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15075/Reviewer_kqD5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807024451, "cdate": 1761807024451, "tmdate": 1762925400786, "mdate": 1762925400786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}