{"id": "TuzsCiHocG", "number": 11170, "cdate": 1758191963452, "mdate": 1759897603061, "content": {"title": "Accurate and Efficient Singular Value Decomposition For LLMs via Decay-aware Rank Allocation and Feature-Preserved Weight Update", "abstract": "Singular Value Decomposition (SVD) provides a hardware-agnostic and effective paradigm for compressing and accelerating Large Language Models (LLMs) by decomposing and truncating weight matrices, followed by weight updates to restore accuracy. However, SVD-based compression faces two major challenges:**(1) Rank Selection Problem:** Optimizing truncation and update ranks constitutes a high-dimensional combinatorial problem. Existing solutions rely on computationally expensive search, leading to both suboptimal performance and diminished efficiency. **(2) Limited Accuracy Restoration:** The sequential weight update strategy employed by state-of-the-art approaches (e.g., SVD-LLM) results in Hessian anisotropic, which hampers accuracy recovery and slows convergence. To overcome these, we introduce DF-SVD, which integrates: **(1) Decay-Aware Rank Allocation:** We derive and validate a correlation between decay characteristics of each weight's singular value spectrum and its importance. This enables dynamic, layer- and weight-specific rank allocation, ensuring high fidelity without costly search. **(2) Feature-Preserved Weight Update:** We introduce a theoretically grounded update strategy that fixes the truncated weight matrix $V^{\\top}S^{-1}$ along with the principal components of $U\\Sigma$,  while updating only the minor components. This design ensures Hessian isotropic, achieving superior accuracy restoration and faster convergence. DF-SVD not only significantly outperforms baselines in accuracy, but also completing compression in just 30 minutes, achieving speedups of $7\\times$, $11\\times$ and $16\\times$ compared to SVD-LLM, ASVD and Dobi-SVD respectively. DF-SVD directly correlates the singular spectrum with training-free rank selection and boosts Hessian isotropy, paving the way for a new paradigm in accurate and efficient SVD-based LLM compression.", "tldr": "Accurate and Efficient Singular Value Decomposition for LLMs via Decay-Aware Rank Allocation and Feature-Preserved Weight Update", "keywords": ["Singular Value Decomposition", "Decay-Aware Rank Allocation", "Feature Preserved Parameter Updating", "Large Language Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1a1f8c1d1b4ea056ff7b14b73dfac887894cbee7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes DF-SVD, a method for compressing LLM using SVD with two main contributions: (1) decay-aware rank allocation that dynamically determines truncation and update ranks based on singular value decay, and (2) feature-preserved weight updates that achieve isotropic Hessian by fixing V^TS^{-1} and selectively updating only minor components of UŒ£. The method achieves much speedup over existing SVD-based methods while maintaining or improving accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong exp results and practical speedup. \n2. Sound theoretical analysis: The Hessian conditioning analysis (section 3.2) is mathematically sound and provides clear intuition for why the proposed reformulation can achieve better convergence properties"}, "weaknesses": {"value": "1. Limited novelty relative to SVD-LLM: The paper heavily builds on SVD-LLM's foundation (Cholesky whitening, sequential optimization framework, experimental setup). Much of the methodology is inherited, making this more of an incremental improvement.\n2. Missing critical comparisons:\nA. No comparison with AdaLoRA: The paper cites AdaLoRA for importance-based rank allocation but never compares against it\nB. No comparison with other methods: Methods like \"Dynamic Low-rank Estimation for Transformer-based Language Models\" (Hua et al., EMNLP 2023 findings) are highly relevant but not discussed or compared\n3. No empirical validation that decay coefficient actually correlates with ground-truth importance (e.g., gradient magnitudes, ablation impact)"}, "questions": {"value": "1. Can you show via experiments that Hessian isotropy causes the speedup (e.g., via iteration counts, convergence curves)?\n2. Why not compare with AdaLoRA, which you cite as inspiration?\n3. What is the correlation between Œª_norm and ground-truth importance metrics (gradients, sensitivity)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p9sKd7wXF7", "forum": "TuzsCiHocG", "replyto": "TuzsCiHocG", "signatures": ["ICLR.cc/2026/Conference/Submission11170/Reviewer_LAV4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11170/Reviewer_LAV4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850157988, "cdate": 1761850157988, "tmdate": 1762922330280, "mdate": 1762922330280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the DF-SVD framework, which aims to simultaneously improve accuracy recovery and compression efficiency in large language models. The authors first identify two key issues with conventional SVD-based compression methods: difficulty in selecting appropriate truncation and update ranks, and limited fine-tuning stability. To address these challenges, the paper proposes two core modules. The first, Decay-Aware Rank Allocation, models the singular value decay rate of each layer‚Äôs weight matrix to dynamically determine both truncation and update ranks, achieving adaptive compression across layers and matrices. The second, Feature-Preserved Weight Update, freezes the dominant components and only updates the minor subspace, thereby preserving critical pretrained features while improving the isotropy of the Hessian for faster convergence. Experimental results show that DF-SVD consistently outperforms existing methods such as SVD-LLM, ASVD, and Dobi-SVD on LLaMA, LLaMA2, LLaMA3, and OPT models under 30‚Äì60% compression, achieving comparable accuracy with 7‚Äì16√ó faster end-to-end compression."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear and practical implementation design.\nThe paper follows the SVD-LLM pipeline with whitening (via Cholesky decomposition) and SVD pre-processing, while confining its innovation to rank allocation and the update subspace. This design choice makes the method easy to reproduce, integrate, and deploy in real-world model compression workflows.\n\n2. Comprehensive experimental evaluation.\nThe experiments cover multiple model families (LLaMA and OPT) and diverse datasets, and report both accuracy and end-to-end compression time. The study also compares DF-SVD against pruning and quantization methods, demonstrating its compatibility and potential for combined use."}, "weaknesses": {"value": "1. Limited novelty.\nThe paper‚Äôs motivation‚Äîimproving rank selection and reducing update time‚Äîtargets a well-studied problem. While the proposed approach is practical, it appears relatively straightforward and lacks deeper theoretical innovation. For example, using singular value decay as a heuristic for rank allocation is intuitive but overlooks inter-layer importance differences; in practice, some critical layers may still require higher ranks even with rapid singular value decay.\n\n2. Insufficient validation of the exponential decay assumption.\nThe core rank allocation mechanism hinges on the assumption that singular values follow an approximately exponential decay pattern and can be modeled by a single parameter Œª. Although the paper provides preliminary theoretical reasoning and empirical evidence, it lacks sensitivity analyses showing how deviations from this assumption affect model performance, as well as more rigorous theoretical justification.\n\n3. Under-examined assumptions in the optimization analysis.\nThe theoretical claim that the Hessian becomes isotropic (ùêª=2ùêº) depends on the assumption of nearly orthogonal, whitened inputs. However, it remains unclear whether this assumption holds under small-sample calibration or distributional shift, and whether it is consistent across different layers or batches. While freezing principal components may preserve pretrained knowledge, it could hinder adaptation in cases of aggressive compression or significant domain shift.\n\n4. Modest empirical gains.\nIn the reported results, DF-SVD achieves only marginal improvements over SVD-LLM in accuracy, which may not be sufficient to demonstrate a strong advantage given that DF-SVD employs mixed-rank allocation whereas SVD-LLM uses a fixed rank. Although DF-SVD shows faster compression compared with Ada-SVD and Dobi-SVD, the performance comparisons are not exhaustive, leaving some uncertainty about its overall effectiveness."}, "questions": {"value": "Refer to the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZR5eGD3luH", "forum": "TuzsCiHocG", "replyto": "TuzsCiHocG", "signatures": ["ICLR.cc/2026/Conference/Submission11170/Reviewer_y1EY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11170/Reviewer_y1EY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886166461, "cdate": 1761886166461, "tmdate": 1762922329780, "mdate": 1762922329780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DF-SVD, a SVD-based compression framework for large language models (LLMs). It addresses two key challenges in existing SVD compression:\n1. Rank Selection Problem ‚Äì current methods rely on costly search or uniform rank allocation. DF-SVD introduces decay-aware rank allocation, which leverages the singular value spectrum‚Äôs decay rate to assign truncation and update ranks per weight matrix dynamically.\n\n2. Limited Accuracy Restoration ‚Äì sequential weight updates in prior work (e.g., SVD-LLM) lead to Hessian anisotropy and slow convergence. DF-SVD proposes a feature-preserved weight update strategy that freezes principal components and only updates minor components, ensuring Hessian isotropy and preserving pretrained knowledge."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation and problem definition: Identifies two fundamental bottlenecks in SVD compression (rank allocation and update inefficiency).\n2. Theoretical contribution: Provides analysis showing Hessian isotropy under the proposed update scheme, linking spectral properties to convergence guarantees."}, "weaknesses": {"value": "1. Generalization to larger models: Experiments are on 7B‚Äì8B scale models; it remains uncertain how well DF-SVD scales to 30B+ models."}, "questions": {"value": "Have you tested DF-SVD on huge models (e.g., Qwen3-30B-A3B-Instruct-2507)? Does the efficiency advantage hold at that scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oaH097o42P", "forum": "TuzsCiHocG", "replyto": "TuzsCiHocG", "signatures": ["ICLR.cc/2026/Conference/Submission11170/Reviewer_CJeg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11170/Reviewer_CJeg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993370517, "cdate": 1761993370517, "tmdate": 1762922329293, "mdate": 1762922329293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DF-SVD to compress Large Language Models using Singular Value Decomposition. It solves two key challenges: the Rank Selection Problem and Limited Accuracy Restoration. There are two innovations: 1. Decay-Aware Rank Allocation, which dynamically assigns truncation and update ranks to each weight based on its singular value decay characteristics, eliminating the need for costly search; 2. Feature-Preserved Weight Update, a theoretically-grounded strategy that freezes key matrix components while only updating minor ones. This update strategy ensures an isotropic Hessian, leading to superior accuracy and faster convergence. The results show that DF-SVD outperforms existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper validates DF-SVD across four different models (LLaMA 1/2/3 and OPT) and eight datasets, consistently demonstrating superior performance.\n2. The authors provide a detailed ablation study that confirms the positive impact of both the Decay-Aware Rank Allocation and the Feature-Preserved Weight Update components.\n3. The method is efficient, completing the entire compression process 7-16 times faster than competing SVD baselines."}, "weaknesses": {"value": "1.The Decay-Aware Rank Allocation method relies on an original truncation position ($ra_{old}$) and update rank ($rank_{old}$). It‚Äôs not clear how these critical baseline values are chosen, which makes the results difficult to reproduce.\n\n2. Lack of theoretical proof for the assumptions (such as the reason of the singular value spectrums follow an exponential decay model should be justified). \n\n3.I was wondering whether the reported wall-clock time includes the LoRA fine-tuning stage, or only the SVD and calibration steps.\n\n4.The update procedure using LoRA in section 3.2 looks quite similar to SVD-LLM .  Could your please articulate the key differences/novelty\n\n5.The analysis of the Hessian (convergence) is based on minimizing reconstruction error, not the model's final task loss. This optimality for the reconstruction objective may not hold for the task objective. Is this a negative impact to the task performance?"}, "questions": {"value": "Please see the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sX1r2k3qNY", "forum": "TuzsCiHocG", "replyto": "TuzsCiHocG", "signatures": ["ICLR.cc/2026/Conference/Submission11170/Reviewer_7BBV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11170/Reviewer_7BBV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762096060868, "cdate": 1762096060868, "tmdate": 1762922328971, "mdate": 1762922328971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}