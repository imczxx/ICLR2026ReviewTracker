{"id": "YaOvgVIGuC", "number": 22235, "cdate": 1758328146933, "mdate": 1759896878489, "content": {"title": "Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains", "abstract": "In sensitive domains, Retrieval-Augmented Generation (RAG) must be interpretable and robust because errors here don't just mislead; they invite lawsuits, undermine scholarly credibility, and breach compliance. Stakeholders require traceable evidence, clear rationale for why specific evidence were selected, and safeguards against poisoned or misleading content. Yet current RAG pipelines use similarity-based retrieval with arbitrary top-k cutoffs, offering no explanation for their selections, and remain vulnerable to data poisoning attacks. We propose METEORA, which replaces these existing drawbacks in RAG pipelines with rationale-driven selection; explicit reasoning that simultaneously guides evidence choice, explains decisions, and remains robust to RAG poisoning. METEORA operates in three stages. First, a general-purpose Large Language Model (LLM) is preference-tuned to generate rationales conditioned on the input query using direct preference optimization. Second, these rationales guide the **Evidence Chunk Selection Engine**, which employs a two-step process: (Step 1) pairing individual rationales with retrieved evidence for query-specific relevance and applying elbow detection (identifying sharp drops in similarity scores) to determine an adaptive cutoff point that eliminates the need for top-k heuristics, thereby acquiring dataset-specific relevance, and (Step 2) optionally performing context expansion by adding neighboring evidence. Lastly, the rationales are used by a **Verifier LLM** to detect and filter poisoned or misleading evidence for safe generation. The framework provides explainable and interpretable evidence flow by using rationales consistently across both selection and verification. Our evaluation across six datasets shows METEORA delivers breakthrough performance on three fronts: (i) it achieves **13.41%** higher recall, and its METEORA w/o Expansion variant achieves **21.05%** higher precision than the best-performing baseline; (ii) it reduces the amount of evidence required to reach comparable recall by **80%**, which directly improves downstream answer generation accuracy by **33.34%**; and (iii) it strengthens adversarial defense, increasing F1 **from 0.10 to 0.44** and making RAG systems more resilient to poisoning attacks. Code available at: https://anonymous.4open.science/r/METEORA-DC46/README.md", "tldr": "", "keywords": ["Retrieval-Augmented Generation (RAG)", "Explainable AI", "Interpretable AI", "Safe AI", "Legal NLP", "Scientific NLP", "Financial NLP"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9becc2c9e90f68d6e921a3e2bea3bd5aecc39b22.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces METEORA, a ranking-free RAG pipeline that (i) DPO-tunes a rationale generator, (ii) uses a two-step Evidence Chunk Selection Engine with elbow detection and optional neighborhood expansion, and (iii) reuses the same rationales in a Verifier LLM to filter poisoned/irrelevant evidence before answer generation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Replaces opaque top-k heuristics with rationale-driven selection; the same rationale frame powers selection and verification, improving auditability for sensitive domains.\n\n2. Easy to implement and tune.\n\n3. Breadth of evaluation, three tasks across six long-document datasets"}, "weaknesses": {"value": "1. In CP, baselines are evaluated at METEORA’s average evidence count rather than each method’s own best-K; a full K-sweep with per-method optima would be more standard and may change relative standings.\n\n2. Design-wise, selection and verification share the same rationale frame, which authors themselves tag as a single-point-of-failure risk under targeted attacks; some verifier/model heterogeneity would help.\n\n3. FinQA case shows lower recall than re-rankers in short-passage settings; the paper attributes this to construction choices, but a deeper failure analysis (tables, chunking, rationale style) would clarify when selection loses to re-ranking."}, "questions": {"value": "1. Section 2.3 (Verifier): Do verifier flags correlate with downstream answer correctness on each dataset? Please report correlations/AUCs, not just counts.\n\n2. What are your failure modes when elbow detection under- or over-selects? Any guardrails you recommend (min/max caps, second-pass knee tests)?\n\n3. For typical r and pool sizes, what fraction of tokens/latency goes to rationale generation and to verifier passes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eq0eaoYPXm", "forum": "YaOvgVIGuC", "replyto": "YaOvgVIGuC", "signatures": ["ICLR.cc/2026/Conference/Submission22235/Reviewer_E7Tq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22235/Reviewer_E7Tq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22235/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761204401076, "cdate": 1761204401076, "tmdate": 1762942128605, "mdate": 1762942128605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to make RAG systems more auditable, interpretable, and robust in sensitive domains like law, finance, and healthcare. Instead of relying on opaque re-ranking and arbitrary top-k retrieval, it introduces METEORA, a framework that uses explicit rationales to guide evidence selection. A rationale generator trained with Direct Preference Optimization (DPO) produces interpretable reasoning sentences, which drive a two-stage Evidence Chunk Selection Engine to adaptively choose relevant evidence and optionally expand context. A Verifier LLM then filters out poisoned or misleading chunks. Experiments show that METEORA improves retrieval quality, downstream answer accuracy, and robustness against corpus poisoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "It offers a fresh perspective on re-ranking by grounding it in explicit reasoning — using a rationale generator to make the evidence selection process more transparent and auditable, providing a cleaner and more interpretable alternative to standard top-k retrieval."}, "weaknesses": {"value": "1. While METEORA shows clear performance gains, the paper doesn’t provide much concrete analysis or evidence to quantify its initial claims around improved interpretability and credibility. It would be helpful to see a more systematic evaluation of these aspects.\n\n2. Methodologically, METEORA feels more like an engineering refinement built on existing techniques. The main novelty seems to be the unsupervised evidence selection approach, but its real advantages aren’t clearly demonstrated — the paper lacks ablation or comparative analysis to justify its effectiveness."}, "questions": {"value": "1. Prior work has shown that correct answers don’t always come from correct reasoning (1). I wonder if the authors analyzed how often this occurs in their Preference Dataset, and whether such cases affect the model’s final performance or rationale quality.\n\n2. The Verifier LLM seems to rely on confidence-based flagging, but model confidence can be unreliable (2). It would strengthen the work to include a comparison against a human or expert baseline to assess how trustworthy these verifications actually are.\n\n3. METEORA’s efficiency gains partly come from using 80% fewer evidence chunks than top-k selection, but it’s unclear whether this still holds for multi-hop reasoning tasks. In such cases, reducing evidence too aggressively might hurt completeness — fewer chunks don’t necessarily mean better outcomes.\n\nRef:\n\n(1) https://arxiv.org/abs/2501.07301\n\n(2) https://cdn.openai.com/papers/simpleqa.pdf"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GmwkZGTZyo", "forum": "YaOvgVIGuC", "replyto": "YaOvgVIGuC", "signatures": ["ICLR.cc/2026/Conference/Submission22235/Reviewer_RMUm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22235/Reviewer_RMUm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22235/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761484092033, "cdate": 1761484092033, "tmdate": 1762942128367, "mdate": 1762942128367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "METEORA is a RAG system designed for sensitive domains that require interpretability and robustness. The system generates rationales (explicit reasoning) via a preference tuned LLM rather than relying on direct similarity computations between query and evidence.  Rationales are then used to guide evidence selection. The approach also includes adaptively similarity cutoffs via elbow detection, context expansion, and verification against poisoned or misleading content. Evaluations indicate that the approach achieves significant improvements over existing approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Use of rationales to both select evidence and to explain that selection clearly to the user. Applying dpo to optimize rationale generation is also interesting. \n\n2. The paper proposes a set of practical optimizations that could be applied to any RAG pipeline that I believe would likely lead to improvements in overall performance. This is a valuable contribution."}, "weaknesses": {"value": "1. I see that a preference tuned LlaMA-3.1-8b was used for rationale generation and evidence verification in the experiments. Is this  LLM available for general use? I didn't see reference to it in the repo.\n\n2. Does METEORA have an sdk that can be used to interface with the framework?\n\n3. As I understand the rationale includes flagging instructions, I would assume that inclusion of flagging instructions may affect the quality of evidence selection? I dont see ablation study that addresses this."}, "questions": {"value": "Is the DPO trained model available?\nSDK available?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2CQFSa7R4K", "forum": "YaOvgVIGuC", "replyto": "YaOvgVIGuC", "signatures": ["ICLR.cc/2026/Conference/Submission22235/Reviewer_y1nd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22235/Reviewer_y1nd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22235/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858942144, "cdate": 1761858942144, "tmdate": 1762942128137, "mdate": 1762942128137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}