{"id": "3e1zmXNdME", "number": 4200, "cdate": 1757631503533, "mdate": 1763028902516, "content": {"title": "Generative Distribution Distillation", "abstract": "In this paper, we formulate the knowledge distillation (KD) as a conditional generative problem and propose the Generative Distribution Distillation (GenDD). A naive GenDD encounters two major challenges: the curse of high-dimensional optimization and the lack of semantic supervision from labels.\nTo address these issues, we introduce a Split Tokenization (SplitTok) strategy, achieving stable and effective unsupervised KD. Additionally, we develop the Distribution Contraction technique to integrate label supervision into the reconstruction objective. \nOur theoretical proof demonstrates that GenDD with Distribution Contraction serves as a gradient-level surrogate for multi-task learning, realizing efficient supervised training without explicit classification loss on multi-step sampling image representations. \nTo evaluate the effectiveness of our method, we conduct experiments on balanced, imbalanced, and unlabeled data. Experimental results show that GenDD performs competitively in the unsupervised setting, significantly surpassing the KL baseline by 16.29\\% on the ImageNet validation set. With label supervision, our ResNet-50 achieves 82.28\\% top-1 accuracy on ImageNet in 600 epochs of training, establishing a new state-of-the-art. Code is available in the Appendix.", "tldr": "", "keywords": ["Generative Learning", "Distillation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/710e5db7f44d37dd2500bc55d2f32ffa3792b58e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed GenDD, a feature-based knowledge distillation method mainly for classification models. GenDD proposed to train a conditional diffusion model with the student's feature as the condition and with the teacher's feature as the generated output. For better class-awareness, GenDD linearly scales the teacher's feature of each sample towards its class centroid."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The experiments cover a wide range of models and datasets.\n2. The writing is clear and easy to follow."}, "weaknesses": {"value": "1. The proof of Theorem 1 is questionable. A critical step of this proof is line 776, where the authors claim $W _y = c _y$. However, this claim is not close to the truth without any other assumptions, and the authors did **not** mention this assumption elsewhere in the main body of the paper. If this problem is not addressed, then the proof can be viewed as invalid.\n2. The motivation of the proposed method is unclear and not well justified. Since the features of both teacher and student models are deterministic, it is not obvious why a generative model (such as a diffusion model) is used to predict the teacher's feature. In other words, it is not clear why it is beneficial to do \"distributional KD\". A simple deterministic neural network is conceptually more favorable to parameterize this mapping, as the baselines did, because it is much easier to train and does not have the non-differentiable problem GenDD has.\n3. Apart from an abbreviation, the authors did not provide any citation or description for each baseline. This makes it difficult to assess the empirical rigor.\n4. The related works section is currently a placeholder. For the KD paragraph, the authors only provided a condensed citation block. The authors did not sufficiently identify how their method is different from existing literature or provide any analysis on conceptually related designs.\n5. According to the ablation study, the method seems to be highly sensitive to hyperparameter choice, especially lambda."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yRGWMFxgT3", "forum": "3e1zmXNdME", "replyto": "3e1zmXNdME", "signatures": ["ICLR.cc/2026/Conference/Submission4200/Reviewer_vNZk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4200/Reviewer_vNZk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4200/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760583672045, "cdate": 1760583672045, "tmdate": 1762917226147, "mdate": 1762917226147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Clarification on questions"}, "comment": {"value": "We thank the reviewers for their valuable comments. Here, we clarify some misunderstandings about our work.\n\n\n1. Sensitivity to hyperparameters.               \n\n    (1) Traditional KD with KL loss has at least 2 hyperparameters: the temperature $\\tau$ and the weight for KL.\n         Different teacher-student configurations can have different optimal weight for KL loss as shown in Figure 2.                    \n         Different $\\tau$ could be employed, regularly 1.0 on ImageNet, 4.0 on CIFAR.                               \n\n    (2) For advanced KD methods, like DKD and IKL-KD, there are often other hyperparameters. Optimal hyperparameters can also be different for different teacher-student configurations. We show it in Appendix A.2.                                                \n\n    (3) The $\\lambda$ controls the degree we use label supervision in GenDD. It can have great effects on model performance.\n          However, we use $\\lambda=0.9$ on ImageNet for all of the teacher-student configurations, demonstrating the robustness of GenDD.\n\n  \n\n\n2. Training settings for the new state-of-the-art on ImageNet with ResNet-50.                \n\n    We evaluate our model on two settings: (1) regular training in 100 epochs with RandomResizedCrop and horizontal flip, (2) strong training settings A1 and A2 that are defined in our paper.\n\n   Our state-of-the-art performance is obtained under the A1 strategy.\n\n3. Addtional parameters for GenDD.\n\n    Our paper presents an alternative generative pipeline for knowledge transfer. Especially, it can also do unsupervised KD effectively.\n    The diffusion head can have extra parameters. We have conducted ablations to show that the performance gains come from the diffusion mechanism rather than extra parameters in Table 8."}}, "id": "x1wJ8ajsBK", "forum": "3e1zmXNdME", "replyto": "3e1zmXNdME", "signatures": ["ICLR.cc/2026/Conference/Submission4200/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4200/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4200/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763028859693, "cdate": 1763028859693, "tmdate": 1763028859693, "mdate": 1763028859693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Thanks for the valuable comments from the reviewers."}}, "id": "PQlvq7JNzp", "forum": "3e1zmXNdME", "replyto": "3e1zmXNdME", "signatures": ["ICLR.cc/2026/Conference/Submission4200/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4200/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763028901844, "cdate": 1763028901844, "tmdate": 1763028901844, "mdate": 1763028901844, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Instead of proposing to just match logits or features, the authors use the student features to condition a diffusion model to generate the teacher features. At test time they then reconstruct the teacher features and classify using the teachers classifier. To address the problem of high dimensional optimisation and improve training stability, they introduce a SplitTok as a different way to condition the diffusion model. Finally, they naturally introduce label supervision as a multi-task problem.\n\nThe authors provide experiments on small and large scale datasets."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method is novel and quite interesting to read. In contrast to prior works, which just introduce an additional alignment loss, they propose to model the problem as a generative task, where the reconstruction loss can be seen as a regularization to align with the teacher. This incorporation of label supervision is very natural. \n\nThere is a good motivation for the SplitTok component and it is clearly ablated to show its importance.\n\nThe ablation experiments are on ImageNet, which is really good to see. Ablations on specific architecture pairs on CIFAR can be misleading."}, "weaknesses": {"value": "In abstract: \"With label supervision, our ResNet-50 achieves 82.28% top-1 accuracy on ImageNet in 600 epochs of training\". This statement is misleading. [1] is a very well known KD paper that also explicitly does label supervision for training a ResNet-50 model and achieves an accuracy is 82.8%. If the authors wish to highlight this result, they should say state-of-the-art within this given training budget.\n\nMissing related work [2, 3, 4], which to my understanding do not have any extensive hyper-parameter tuning. In fact they all show robustness to hyperparameters empirically.\nFurthermore, [4, 5] show better results across all the CIFAR architecture pairs presented. Although the CIFAR benchmark is admittedly quite saturated, omitting a comparison to many of the recent KD methods (especially those for *feature/representation* distillation) is very misleading. It seems the authors only compare against feature methods that are over 4 years old. \n\nThe practical motivation for this work is unclear. Conventionally distillation is used to train smaller models so that they are faster at inference. In this work, they propose a reconstruction based task which incurs inference overheads and additionally use the teachers classifier. Firstly, there is already work suggesting the use of sharing the teachers classifier [6] but they introduced pruning so that this modification did not incur any parameter overhead for a fair comparison to other KD works. Secondly, there are no timing results for the reconstruction during inference and no attempts to make the evaluation match the evaluation cost with other KD methods they compare to. In practice, if there is no interest in the evaluation overhead, how would SplitTok compare to just using a larger student model?\n\nThe way I see it is that this proposed method is theoretically very interesting, but practically not too useful. I would encourage the authors to find a useful practical utility for this method over prior KD works. As it stands, comparing to cheaper (at evaluation time) KD methods is misleading, and claiming that prior works are sensitive to hyperparameters is not true (not just the sensitivity of vanilla logit distillation with the temperature parameter). Related to this point, in figure 4b there is only a noticeable improvement over the baseline KL when 1-\\lambda = 0.9. I'm not sure how this highlights robustness to \\lambda? in fact it shows that a very specific value of lambda is needed to improve upon the baseline KL (71.4% mobilenet).\n\n[1] Knowledge distillation: A good teacher is patient and consistent. CVPR 2022\n\n[2] Understanding the Role of the Projector in Knowledge Distillation. AAAI 2024\n\n[3] Wasserstein Contrastive Representation Distillation. CVPR 2021\n\n[4] Logit Standardization in Knowledge Distillation. CVPR 2024\n\n[5] Multi-level logit distillation. CVPR 2023\n\n[6] Knowledge Distillation with the Reused Teacher Classifier. CVPR 2022\n\nMinor comments:\n\nL086: \"Conditoned\""}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6AXZijmY5c", "forum": "3e1zmXNdME", "replyto": "3e1zmXNdME", "signatures": ["ICLR.cc/2026/Conference/Submission4200/Reviewer_Phrj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4200/Reviewer_Phrj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4200/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761132074409, "cdate": 1761132074409, "tmdate": 1762917225748, "mdate": 1762917225748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper recasts the point-wise discriminative paradigm of knowledge distillation into a diffusion-based generative one. It introduces two techniques, namely Split Tokenization and Distribution Contraction to improve performance. Experimental results on different datasets, models, and settings demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Casting the point-wise discriminative knowledge distillation paradigm into a generative one is conceptually novel and well-motivated.\n\n2.Experimental results show that the proposed method is effective and yields significant gains on different datasets and settings."}, "weaknesses": {"value": "1.Relevant (e.g., MGD [1]) or recent strong KD baselines (e.g., LSKD [2], CRLD [3]) are missing for introduction and comparison. For example, Tables 2 and 5 only present outdated KD methods, and recent strong baselines such as FCFD [4], LSKD [2], CRLD [3], and SDD [5], are absent for comparison.\n\n2.The diffusion process is notorious for its time-consuming multi-step sampling. The 1000-step and 64-step sampling used in the training and inference of the proposed method could make it substantially inefficient during both training and deployment. Meanwhile, detailed benchmarking and discussion of this aspect are not provided.\n\n3.Choices for some diffusion-related hyperparameters are not justified. For example, how is the CFG guidance scale of 2.0 chosen? Why 64-step for inference? What are the effect of using different values?\n\n4.Can the proposed method be applied to tasks beyond classification, for example object detection? The practical value of the proposed method may be limited if it only works for classification.\n\n[1] Yang et al. Masked Generative Distillation. ECCV 2022.\n\n[2] Sun et al. Logit Standardization in Knowledge Distillation. CVPR 2024.\n\n[3] Zhang et al. Cross-View Consistency Regularisation for Knowledge Distillation. ACM MM 2024.\n\n[4] Liu et al. Function-Consistent Feature Distillation. ICLR 2023.\n\n[5] Wei et al. Scale Decoupled Distillation. CVPR 2024."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SKuJqQfpGk", "forum": "3e1zmXNdME", "replyto": "3e1zmXNdME", "signatures": ["ICLR.cc/2026/Conference/Submission4200/Reviewer_VbvG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4200/Reviewer_VbvG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4200/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761615200618, "cdate": 1761615200618, "tmdate": 1762917225354, "mdate": 1762917225354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}