{"id": "f9wIqr87W9", "number": 16504, "cdate": 1758265277833, "mdate": 1759897236698, "content": {"title": "IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs", "abstract": "Large Language Models (LLMs) promise impressive capabilities, yet their multi-billion-parameter scale makes on-device or low-resource deployment prohibitive. Mixed-precision quantization offers a compelling solution, but existing methods struggle when the average precision drops below four bits, as they rely on isolated, layer-specific metrics that overlook critical inter-layer interactions affecting overall performance. In this paper, we propose two innovations to address these limitations. First, we frame the mixed-precision quantization problem as a cooperative game among layers and introduce Shapley-based Progressive Quantization Estimation (SPQE) to efficiently obtain accurate Shapley estimates of layer sensitivities and inter-layer interactions. Second, building upon SPQE, we propose Interaction-aware Mixed-Precision Quantization (IMPQ) which translates these Shapley estimates into a binary quadratic optimization formulation, assigning either 2 or 4-bit precision to layers under strict memory constraints. Comprehensive experiments conducted on Llama-3, Gemma-2, and Qwen-3 models across three independent PTQ back-ends (Quanto, HQQ, GPTQ) demonstrate IMPQ’s scalability and consistently superior performance compared to methods relying solely on isolated metrics. Across average precisions spanning 4 bit down to 2 bit, IMPQ cuts Perplexity by 20 – 80 % relative to the best baseline, with the margin growing as the bit-width tightens.", "tldr": "", "keywords": ["Interpretability", "Explainability and Transparency", "Cooperative Game Theory", "Large Language Models"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bca247c7e5e1027b6fcfb7bc6f170c4c8f37f449.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces IMPQ, a novel framework for mixed-precision quantization of Large Language Models (LLMs) that addresses the critical challenge of deploying massive models on resource-constrained devices. The core innovation lies in modeling quantization as a cooperative game among transformer layers, where the authors propose SPQE (Shapley-based Progressive Quantization Estimation) to capture layer sensitivities and inter-layer interactions through progressive quantization rather than abrupt pruning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The application of cooperative game theory and Shapley values to mixed-precision quantization represents a significant conceptual advance. By framing layers as players in a cooperative game, the authors provide a principled approach to quantifying both individual layer contributions and inter-layer interactions, which existing methods neglect.\n\nSPQE's progressive quantization (from 4-bit to 2-bit) is a clever innovation that maintains model stability during Shapley estimation. This approach effectively avoids the catastrophic performance degradation and high variance associated with layer pruning, enabling more accurate and reliable layer importance assessment."}, "weaknesses": {"value": "While the paper analyzes the impact of Monte Carlo samples, it neglects a thorough examination of other hyperparameters. The diagonal shrinkage parameter $\\alpha$ is fixed at 0.5 without justification or sensitivity analysis. The choice of baseline (4-bit) and target (2-bit) precisions is also not motivated or varied.\n\nExperiments rely primarily on C4 for Shapley estimation and WikiText-2 for evaluation. Testing on more diverse domains (e.g., code, multilingual text) and larger datasets would strengthen claims about generalizability, especially given the domain-specific nature of quantization effects.\nWhile several baselines are included, comparisons with recent Hessian-based methods like HAWQ are limited. The paper also doesn't compare against neural architecture search approaches for quantization, which could provide additional context for the performance gains.\n\n\nI'm mainly interested in the presentation and theoretical parts, with some confusing content below. Please tell me if I was wrong.\nThe paper assumes Monte Carlo sampling provides accurate Shapley approximations without a theoretical analysis of approximation error. The value function $v_{\\text{NLL}}(S) = \\mathbb{E}_{(x,t)\\sim D}[-\\log p(x_{t+1}|x_{\\leq t}; S)]$ in Equation 3 is not justified as the optimal choice for measuring layer contributions in the cooperative game framework. \n\nSection 3.2 begins with a second-order Taylor approximation $\\Delta L \\approx \\sum_{i=1}^{L} g_i^\\top \\epsilon_i + \\sum_{i=1}^{L} \\sum_{j=1}^{L} \\epsilon_i^\\top H_{ij} \\epsilon_j$ but then switches to a Shapley-based approach without reconciling these perspectives. The covariance matrix $C = \\frac{1}{M}(\\Delta v_\\ell - \\hat{\\phi})^\\top (\\Delta v_\\ell - \\hat{\\phi})$ in Equation 8 is proposed as a Hessian proxy without theoretical justification for this equivalence.\n\nThe distribution $D$ in Equation 3 is not clearly specified. The perturbation $\\epsilon_i$ in Section 3.2 is used without defining its relationship to quantization error. The construction of the covariance matrix $C$ in Equation 8 lacks clarity regarding dimensions and the precise meaning of $\\Delta v_\\ell$. It's kindly suggested to check these notations."}, "questions": {"value": "See the weakness, mainly in the exp settings, theory, and notations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sGXfMA8Pxu", "forum": "f9wIqr87W9", "replyto": "f9wIqr87W9", "signatures": ["ICLR.cc/2026/Conference/Submission16504/Reviewer_bM4D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16504/Reviewer_bM4D"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16504/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557840298, "cdate": 1761557840298, "tmdate": 1762926597848, "mdate": 1762926597848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces IMPQ, a mixed precision PTQ method for LLMs that leverages Shapley values to estimate the importance/sensitivity of individual layers in LLMs, and forms a Hessian scaled objective for the mixed-precision problem that can be solved using quadratic integer programming solvers."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method is clear and the evaluation supports its claim of effectiveness over baselines."}, "weaknesses": {"value": "- The complexity of the algorithm needs to be analyzed: The calculation of Shapley value and the Monte Carlo samping seems very computationally expensive for LLMs. A comparison of the computational complexity of the IMPQ method against baselines may be needed given important factors like layer numbers, and sampling numbers\n- Downstream task evaluation is missing. The evaluation section only shows result on wikitext perplexity, but IMPQ method and some baselines/quantization methods needs calibration. It will be more convincible if results on downstream tasks are included. Given this perplexity, how hard it  will be to apply to MoE models?"}, "questions": {"value": "Besides the weakness section, could the author answer the following questions\n\n1. Why the average per-token NLL are used as pay-offs? \n2. Could the author explain/give some intuitive hints on why IMPQ  outperforms baselines?\n3. Apart from the computationaly complexity analysis in Weakness section, could the author give a comparison of quantization time? This will be straightforward for readers to understand the complexity-performance trane off\n4. The selection of hyper params like $\\alpha$ in line 260. Could the author explain why choose this specific value of 0.5?\n\nThere are a few typos in the paper due to citation formating issue like line 109 and 112"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B8QsKACNIn", "forum": "f9wIqr87W9", "replyto": "f9wIqr87W9", "signatures": ["ICLR.cc/2026/Conference/Submission16504/Reviewer_GnxT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16504/Reviewer_GnxT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16504/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927530593, "cdate": 1761927530593, "tmdate": 1762926597176, "mdate": 1762926597176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SPQE, a Shapley-based approach for estimating layer importance via progressive quantization, and IMPQ, a MILP-based method for assigning 2- or 4-bit precision under memory constraints. The authors frame mixed-precision quantization as a cooperative game among layers, capturing inter-layer dependencies more effectively than existing heuristics. Evaluated on several LLMs and PTQ backends, IMPQ achieves significantly lower perplexity, especially under 2-bit constraints, demonstrating strong empirical performance and robustness across models and settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The key strengths lie in the originality of modeling quantization as a cooperative game, the method’s stability under aggressive bit reductions, and the extensive and thorough experimental validation. The results clearly show that accounting for inter-layer interactions leads to better bit allocation and quantized performance than isolated sensitivity measures."}, "weaknesses": {"value": "1. The method carries substantial computational overhead, with SPQE requiring many hours to estimate Shapley values even for mid-sized models. \n\n2. The approach is currently limited to binary 2-bit/4-bit decisions, which restricts its generality, and the MILP formulation, though optimal in theory, raises questions about scalability to larger models or finer-grained bit options. \n\n3. Moreover, the paper does not explore how robust the final assignments are to noise in Shapley estimates, nor does it fully explain implementation details such as memory constraint handling or solver configurations."}, "questions": {"value": "1. It would help to know whether the authors plan to support finer bit precision (e.g., 3-bit or 8-bit layers), and whether SPQE or MILP runtimes could be reduced through approximation or more scalable formulations. \n\n2. Additionally, can this game-theoretic framework extend to other compression tasks, such as structured pruning, where modeling interactions is equally important?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3pQ1xH7bPF", "forum": "f9wIqr87W9", "replyto": "f9wIqr87W9", "signatures": ["ICLR.cc/2026/Conference/Submission16504/Reviewer_TvGU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16504/Reviewer_TvGU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16504/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974039965, "cdate": 1761974039965, "tmdate": 1762926596707, "mdate": 1762926596707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SPQE to obtain accurate Shapley estimates of layer sensitivities and inter-layer interactions. SPQE is based on cooperative game theory. The authors also use IMPQ to find optimal bit-width assignments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. An innovative use of Shapley value analysis and cooperative games among LLM layers to model mixed-precision quantization.\n\n2. Demonstrated performance improvements on Llama-3, Gemma-2, and Qwen-3."}, "weaknesses": {"value": "1. Why is modeling mixed-precision quantization using Shapley value analysis and cooperative games among LLM layers more effective than traditional Hessian-based methods? The authors did not clearly explain the motivation.\n\n2. The experimental setup seems problematic. I believe the paper does not evaluate quantization performance under an accepted standard setting.\n(a) The performance of the full-precision baseline is not stated.\n(b) The bit range defined in the paper (e.g., 2.5–3.0) is overly broad. It is not explained how these bits correspond to specific mixing ratios or group sizes, nor how fairness is maintained across different baselines.\n(c) The statistical results in the paper are inconsistent with previous work. A perplexity (ppl) around 15–25 is too high—much worse than those reported in existing papers. For example, in [1], methods such as OmniQuant and CherryQ achieve ppl < 10 at 2.15 bits. Although the experimental setup in [1] differs from this paper, the discrepancy should not be this large.\n\n3. Continuing from 2.(c), I believe that while parameter sensitivity has room for optimization, improving only the sensitivity is of limited benefit. The optimal sensitivity selection strategy may only result in a small decrease in perplexity. Therefore, the claim in Table 1 that IMPQ reduces ppl by about 10 compared to other baselines may not be credible. The authors may not have obtained the optimal performance for the baselines.\n4. Some writing issues—e.g., the font in Table 1 is too small.\n\n[1] Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K9lcjDzqv5", "forum": "f9wIqr87W9", "replyto": "f9wIqr87W9", "signatures": ["ICLR.cc/2026/Conference/Submission16504/Reviewer_8JN3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16504/Reviewer_8JN3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16504/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762053552588, "cdate": 1762053552588, "tmdate": 1762926596056, "mdate": 1762926596056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}