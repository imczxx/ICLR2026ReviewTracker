{"id": "NT6o1Yv3V5", "number": 24342, "cdate": 1758355949742, "mdate": 1759896770395, "content": {"title": "PRISM: A Principled Framework for Supervised Disentanglement via Bipartite Factorization", "abstract": "Learning structured representations that partition information based on its semantic contents remains a central challenge in deep generative modeling. In light of the established theoretical impossibility of purely unsupervised disentanglement, we address the pragmatic and well-posed objective of bipartite factorization: separating the single factor of variation corresponding to the supervisory label from all other residual sources of variation. We introduce a principled framework that achieves this separation through a learning mechanism that routes most of the intra-class variation into a class-agnostic latent subspace. The design of this mechanism is guided by a formal, information-theoretic analysis, which provides quantitative bounds on the learning outcome. We conduct a series of targeted experiments designed to validate the proposed mechanism, demonstrating its ability to produce a factorized representation with quantifiably low leakage of supervised information into the residual subspace, and illustrating the effectiveness of the resulting factorization on downstream tasks requiring precise latent control, such as targeted attribute swapping and manipulation of stylistic features.", "tldr": "", "keywords": ["supervised disentanglement", "representation learning", "generative modeling", "autoencoders", "deep learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1f9498f0e2ab3e42ef30cc1663ee368c4371ceb5.pdf", "supplementary_material": "/attachment/37cd8ffc7ea56d278d8223b32597542a7a112149.zip"}, "replies": [{"content": {"summary": {"value": "In this work the authors construct set of neural networks and losses to distill two aspects of the latent code of datapoints given a dataset of labels: those that impact the label and those that are independent of it. They demonstrate the effectiveness of the extraction procedure, perform an ablation study, provide supporting theory."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is very clear, from problem statement, through literature review, to theory and experiments\n- The contribution is novel, elaborate, and appears to work\n- The authors perform a meaningful ablation study\n\nBroadly, it just seemed relatively well executed. \n\nI liked the paper, and think it makes a good contribution to ICLR as is. I oscillated between weak acceptance and acceptance. My (minor) negative comments are included in the weaknesses section. Broadly, if the authors could do more to convince me that their method was simply better than alternatives, and if I was certain there were no other more recent models (Fader is 2017) [I don't know this literature well, I will defer to other reviewers on this], I would lean more towards acceptance."}, "weaknesses": {"value": "- Personally, I did not find the maths added very much. The final predictions they ended up with felt relatively intuitive - (i) more reconstruction requires more info in z_0, (ii) less dimensionality of z_1 pushes more info into z_0, (iii) removing non-class info from z_1 pushes it into z_0. It is indeed nice to tie these intuitions to a principled framework with attached assumptions etc. but I didn't get a lot scientifically from it. I'd be interested to hear what the authors think it taught them.\n\n- Could figure 1, especially the text, be made bigger.\n\n- Appendix C seemed very important, personally I would prioritise pushing that into the main paper, at the expense of, for example, section 5.3, section 4, and perhaps some of section 2. Further, is FADER this the only existing comparable method? And the improvements the method achieves over FADER are minimal - the two label leakages are within error bars of one another. Hence, it doesn't seem that the method is necessarily better, just different, muddying the important contribution of this paper?"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d5qK9YhRRY", "forum": "NT6o1Yv3V5", "replyto": "NT6o1Yv3V5", "signatures": ["ICLR.cc/2026/Conference/Submission24342/Reviewer_MCZj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24342/Reviewer_MCZj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761310257795, "cdate": 1761310257795, "tmdate": 1762943050044, "mdate": 1762943050044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes supervised disentangled representation learning by partitioning the latent space into two codes: one that captures the supervised factor of variation and the other that captures the remaining (class-agnostic) information. To this end, the authors design an adversarial training framework that encourages the first latent to be class-relevant while constraining the second to be class-irrelevant. The proposed method is validated on Morpho-MNIST via component-wise ablations and by empirically estimating a lower bound on the conditional residual information which indicates low leakage of supervised information into the residual latent code."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- This paper addresses an important challenge in disentangled representation learning under weak supervision.\n- It provides an information-theoretic analysis with quantitative bounds (e.g., on information leakage and conditional residual information)."}, "weaknesses": {"value": "- The overall presentation is not well organized, which makes it difficult to understand the paper’s main claim and method. The approach is introduced with many equations and components without high-level intuition or justification. (Please see the questions below.)\n- The proposed method introduces too many objectives (at least seven within an adversarial scheme), which overly complicates the framework. It makes hyperparameter search very complex (the method has at least seven loss-weighting coefficients such as\n $\\lambda_x, \\lambda_y, etc$. \n- A major concern is the lack of comparisons to prior work (e.g., [1]) and the absence of evaluation on common disentanglement metrics (e.g., DCI, FactorVAE score, MIG as in [1]), which makes it hard to assess the method’s effectiveness relative to prior work. Instead, the paper reports probe-gap (without reference or justification) and ARI, which appears orthogonal to disentanglement quality. Moreover, the evaluation is limited to a single, simple synthetic dataset (Morpho-MNIST). \n\n**Reference**\n\n[1] Locatello et al., Weakly-supervised disentanglement without compromises, in ICML 2020."}, "questions": {"value": "- Why is the structural loss (L196) necessary for disentanglement? Isn’t this objective orthogonal to disentanglement of the representation?\n- Why do we have to maximize $I(z_0;\\hat {x})$? Doesn’t the reconstruction loss already encourage $z_0$ to contain information for reconstructing $x$?\n- What is $R_X(\\sigma_{rec})$ in L313 and how is it measured?\n- How did the authors select and tune the numerous hyperparameters for the loss-weighting coefficients?\n- Section 3.1 begins by describing the architectures and optimization framework without any overview or justification, which makes it hard for readers to follow the overall flow. From the beginning, it is not even clear why the proposed method requires an adversarial framework. Providing a brief overview would improve clarity.\n- Similarly, Section 3.2 lists many components without sufficient description. For example, it starts by introducing the loss $L_M$​, which consists of seven loss terms, but readers have no context for each term. Although the following sentences group these terms into three categories, the section still lacks justifications or high-level intuitions for the losses—e.g., why the concept-relevant subspace must be structured, why the residual subspace should be shaped, and what role each loss term plays.\n- Please add equation numbering.\n- Please consider redrawing Figure 1 for readability; the text in the figure is too small to read and the figure looks too complex."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kVTtI0BDoC", "forum": "NT6o1Yv3V5", "replyto": "NT6o1Yv3V5", "signatures": ["ICLR.cc/2026/Conference/Submission24342/Reviewer_4kLh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24342/Reviewer_4kLh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919488083, "cdate": 1761919488083, "tmdate": 1762943049751, "mdate": 1762943049751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of bipartite factorization in generative models—separating a supervised semantic factor (e.g., class label) from all remaining variation. The proposed model, PRISM, uses a combination of adversarial training, prototype-based supervision, and information bottlenecks to route intra-class variation into a residual latent subspace. The paper also provides an information-theoretic analysis explaining the conditions under which such routing occurs, and validates these predictions with targeted experiments on Morpho-MNIST and qualitative disentanglement scenarios on CelebA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a well-motivated and conceptually clean solution to the challenging problem of bipartite latent space factorization. By explicitly separating a supervised semantic factor from residual variation, PRISM provides a principled alternative to fully unsupervised disentanglement, which is known to be theoretically ill-posed. The architecture combines prototype-based regularization, adversarial class removal, and mutual information objectives, producing a latent structure that aligns tightly with the intended factorization. This combination is both innovative and well-justified, and the design decisions are guided by a formal information-theoretic framework.\n\nThe experimental validation is strong and carefully targeted. The ablation studies and controlled manipulations of model capacity, concept subspace dimensionality, and structural regularization provide direct empirical support for the theoretical predictions (Lemmas 1 and 2). The qualitative results on CelebA, including attribute swapping and latent traversals, further demonstrate the model’s ability to cleanly isolate concept-relevant features from residual variation. Overall, the work provides a compelling combination of theory, architectural design, and empirical validation that advances the field of structured representation learning."}, "weaknesses": {"value": "Despite its strengths, the proposed approach is fairly complex, involving multiple adversarial networks, prototype updates, and mutual information estimators. This could pose challenges for reproducibility and practical adoption, especially for users without careful hyperparameter tuning or computational resources. Furthermore, while the theoretical analysis provides upper and lower bounds on information leakage and routing, it relies on strong assumptions about equilibrium convergence in multi-agent optimization, which may not hold in practice and could limit the applicability of the formal guarantees.\n\nThe empirical evaluation, though thorough in testing theoretical predictions, is somewhat limited in scope. The primary quantitative evaluations are conducted on synthetic or semi-synthetic datasets (Morpho-MNIST), with real-world datasets (CelebA) evaluated mostly qualitatively. Additionally, while comparisons are made to Fader Networks, the study does not thoroughly benchmark against other recent methods for supervised factorization or disentanglement. Broader quantitative evaluation on natural datasets and more diverse baselines would strengthen the evidence for PRISM’s practical effectiveness."}, "questions": {"value": "- How sensitive is PRISM to the choice of hyperparameters and the stability of the adversarial training? Could slight variations in weights or network capacity lead to a collapse of the bipartite factorization, and how might this affect practical deployment?\n\n- Can the proposed framework generalize to multi-factor supervised settings, where multiple attributes need to be separated simultaneously, and if so, what modifications to the architecture or objectives would be necessary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "iwHQFGsMDI", "forum": "NT6o1Yv3V5", "replyto": "NT6o1Yv3V5", "signatures": ["ICLR.cc/2026/Conference/Submission24342/Reviewer_Wv49"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24342/Reviewer_Wv49"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970639761, "cdate": 1761970639761, "tmdate": 1762943049166, "mdate": 1762943049166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}