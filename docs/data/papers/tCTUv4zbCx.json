{"id": "tCTUv4zbCx", "number": 20115, "cdate": 1758302641792, "mdate": 1759897000897, "content": {"title": "Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving", "abstract": "Large language models (LLMs) have been widely used for problem-solving tasks. Most recent work improves their performance through supervised fine-tuning (SFT) with labeled data or reinforcement learning (RL) from task feedback. In this paper, we study a new perspective: the divergence in solutions generated by LLMs for a single problem. We show that higher solution divergence is positively related to better problem-solving abilities across various models. Based on this finding, we propose solution divergence as a novel metric that can support both SFT and RL strategies. We test this idea on three representative problem domains and find that using solution divergence consistently improves success rates. These results suggest that solution divergence is a simple but effective tool for advancing LLM training and evaluation.", "tldr": "", "keywords": ["Large Language Model", "Solution Divergence", "Fine-tuning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8f6b934b94e75fa948b25b773ba6feb0990d000c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper argues that the diversity of solutions generated by a language model to a given problem is an important indicator of the problem solving ability of the model. It proposes a way to measure, and a way to improve, the solution diversity in models (for both supervised and reinforcement based fine-tuning) based on (the eigenvalues of) the Laplacian of a graph constructed from the solutions generated by a model. It also provides experimental evidence for the benefits of doing so."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is exceptionally well written and very easy to follow. It investigates solution diversity backed with a strong motivation from cognitive science. It proposes an elegant mechanism to measure solution diversity using the Laplacian of a graph constructed from multiple solutions drawn from a model. And it shows that their measure of solution diversity can be beneficial in training models."}, "weaknesses": {"value": "The approach is explored and evaluated on three simple reasoning tasks (math-500, mbpp, and a newly introduced maze task), represented by a sample of 100 test cases each. I am not sure how stable the findings are if one would broaden that set (or even re-ran the experiment on a different set of 100 test cases). This concern is corroborated by the fact that there is no significant performance improvement from leveraging solution divergence in one of the three kinds of task (mbpp) in Section 5.3. \n\nSolution divergence has the most significant positive impact in the Pass@10 evaluation setting (vs. Pass@1 where there is much less of an impact). I am wondering whether this setting obviously benefits from more diverse solutions, and how significance the overall finding therefore is. \n\nA few additional concerns are phrased as questions below."}, "questions": {"value": "(minor comment) The text in Figure 2 is extremely hard to read. I don’t know how one would fix that, but perhaps not labelling every single model and showing just a few ones with larger font-size would work. \n\nWhat is N in the experiment shown in Figure 2? \n\nOne could imagine many ways to incorporate solution divergence into RL rewards, and the formulation in Section 4.2 seems like one somewhat arbitrary way to do so. It would be nice to understand how this particular formulation (over any potential other ways) is motivated. \n\n(minor comment) Wouldn’t “solution diversity” a more fitting term that “solution divergence”? \n\n(line 360) “we tune the hyper-parameters including learning rate…”. How were they tuned and based on what criterion and what data? \n\n(line 365) It is mentioned that self-consistency (Wang et al. 2022) can be leveraged. Was it used, and if so, how? \n\nIn Tables 2 and 3, data with high solution divergence is compared to data with low solution divergence. Shouldn’t there be a comparison to data with “typical” solution divergence, which would be simply a sample drawn from the model. \n\nFurther, as a control study, how would varying solution divergence of the training data by simply varying temperature, rather than based on the proposed method, compare? \n\nThere seems to be a typo in the sentence beginning in line 430."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rV0juGSzIE", "forum": "tCTUv4zbCx", "replyto": "tCTUv4zbCx", "signatures": ["ICLR.cc/2026/Conference/Submission20115/Reviewer_i9wi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20115/Reviewer_i9wi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938695661, "cdate": 1761938695661, "tmdate": 1762933011580, "mdate": 1762933011580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces and investigates the concept of solution divergence — the degree to which large language models (LLMs) generate distinct solutions to the same problem — as a predictor and enhancer of problem-solving performance. The authors show that higher divergence correlates positively with better accuracy across mathematics, programming, and logical reasoning tasks.\nThey formalize solution divergence using pairwise normalized edit distances among generated solutions and compute global and local metrics via the Laplacian spectrum of a relational similarity graph. Empirical studies across multiple open- and closed-source LLMs demonstrate a strong correlation (R² up to 0.94) between global divergence and problem-solving success"}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of quantifying and leveraging solution divergence as a performance signal for LLMs is novel.\n2. The empirical correlation between divergence and performance is rigorously demonstrated across multiple datasets and 17 LLMs.\n3. The theoretical foundation is clear: the Laplacian-based divergence metric ties to spectral clustering theory, providing a principled way to measure diversity in non-binary solution spaces."}, "weaknesses": {"value": "1. Metric simplicity and linguistic bias:\ni.  The divergence metric relies on normalized string edit distance, which can be brittle and insensitive to semantically distinct but lexically similar outputs (especially in math and code domains).\nii.  The authors acknowledge this but still treat it as the core metric; future iterations could integrate semantic embeddings/\n\n2. Improvements in Pass@1 are modest (0.5–1% average in SFT, ~0.3% in RL) despite theoretical appeal, though Pass@10 gains are stronger (≈3–6%). Statistical significance results would be useful."}, "questions": {"value": "check weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9dchzbhZmj", "forum": "tCTUv4zbCx", "replyto": "tCTUv4zbCx", "signatures": ["ICLR.cc/2026/Conference/Submission20115/Reviewer_zYtu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20115/Reviewer_zYtu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983713490, "cdate": 1761983713490, "tmdate": 1762933011008, "mdate": 1762933011008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper positions solution divergence as a simple but powerful new method for understanding and improving LLM reasoning, which provides a bridge between cognitive diversity in human problem solving and data diversity in machine learning. \n\nThe authors hypothesize that models exhibiting greater diversity in their valid reasoning paths tend to have stronger general problem-solving capabilities. They quantify solution divergence using a graph-based metric derived from normalized edit distances among solutions, computed via spectral properties of the Laplacian matrix. In experiment, the authors show a strong positive correlation  between solution divergence and success rate (Pass@1) across multiple LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a conceptual framing. The propose concept is original and intuitive.\n2. Results are consistent across multiple reasoning domains (math, logic, programming) and different model families (GPT-4o, Claude, Gemini, Llama-3, Qwen)."}, "weaknesses": {"value": "1. The paper establishes a strong correlation between solution divergence and model accuracy, but causal mechanisms remain unclear. It remains unknown whether higher divergence causes better reasoning, or whether better models naturally produce both diverse and correct solutions.\n2. Although the experiments span math, logic, and programming, these tasks are still structured and verifiable.\n3. The paper focuses on how diversity improves performance, but doesn’t analyze potential downsides. For example, could encouraging divergence lead to less stable or less factual reasoning?"}, "questions": {"value": "1. Is there a case where some diversities are bad and lead to incoherent or invalid reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "GYkZvxPF3Q", "forum": "tCTUv4zbCx", "replyto": "tCTUv4zbCx", "signatures": ["ICLR.cc/2026/Conference/Submission20115/Reviewer_CKU3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20115/Reviewer_CKU3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762157184443, "cdate": 1762157184443, "tmdate": 1762995327134, "mdate": 1762995327134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces and investigates the concept of \"solution divergence,\" which refers to the existence of different yet equally valid solutions to a single problem. The authors' preliminary study reveals a strong positive correlation between an LLM's ability to generate diverse solutions and its overall problem-solving performance across tasks in mathematics, programming, and logical reasoning.\n\nBuilding on this finding, the paper proposes two methods to leverage solution divergence to improve LLM training:\n\n- Dataset Divergence Metric (for SFT): A method for Supervised Fine-Tuning (SFT) where training data is selected based on its inherent solution diversity. Models trained on datasets with higher divergence showed improved performance.\n- Solution Divergence Fused Reward (for RL): A novel reward function for Reinforcement Learning (RL) that encourages the model to generate solutions that are not only correct but also different from other solutions. This method also led to significant performance gains, particularly in finding a correct answer within multiple attempts (Pass@10).\n\nIn essence, the research demonstrates that solution divergence is a simple but powerful metric that can be effectively integrated into both SFT and RL pipelines to enhance the problem-solving capabilities of large language models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper introduces the novel and underexplored concept of \"solution divergence\" as a key factor in LLM problem-solving performance. And it provides strong empirical evidence demonstrating a positive correlation between a model's ability to generate diverse solutions and its success rate across different domains.\n- It proposes two practical and actionable methods for integrating solution divergence into standard LLM training pipelines: one for SFT and one for RL.\n- The research is grounded in a comprehensive experimental setup, utilizing three distinct datasets (Math-500, MBPP+, and the newly created Maze), a wide array of 17 different LLMs, and robust evaluation.\n- The authors formalize the abstract concept of divergence into a computable metric using graph theory and string edit distance, making the approach replicable."}, "weaknesses": {"value": "- The core metric of \"solution divergence\" relies on a superficial and potentially misleading proxy. The entire paper's foundation rests on measuring divergence using normalized string edit distance. This is a crude, syntactic measure that is blind to semantic, logical, or structural equivalence. Two solutions could be algorithmically identical but written with different variable names or sentence structures, leading to a high, artificial divergence score. Conversely, two genuinely different logical approaches could be expressed with similar vocabulary, yielding an erroneously low score. This fundamental weakness calls into question the validity of the paper's core findings.\n\n- The paper risks conflating correlation with causation. The preliminary study only establishes a correlation: better models tend to produce more diverse solutions. The paper then leaps to the conclusion that forcing more divergence into training causes better performance. An equally plausible hypothesis is that stronger, more capable models naturally develop a wider range of strategies as a byproduct of their advanced reasoning abilities, making divergence a symptom of competence, not its cause.\n\n- The heavy reliance on the novel \"Maze\" dataset for key ablation studies is a significant weakness. The Maze task, created for this study, is an unvetted, toy problem. It is highly structured and path-based, making it uniquely and perhaps artificially sensitive to a syntactic diversity metric. The most compelling results and ablation studies are performed on this dataset, but there is no guarantee these findings would hold for more complex, established benchmarks. The minimal impact on the MBPP+ dataset could be an indicator that the effect is not as general as claimed.\n\n- The method for generating SFT data introduces a major confounding variable. The authors use powerful, state-of-the-art LLMs (GPT-4o, Claude-3.5) to generate the \"diverse\" solutions for their training sets. The subsequent training of smaller models on this data may not be learning \"divergence\" as a principle, but rather is simply a form of knowledge distillation from a superior teacher model. The performance gains could be attributed to the quality of the distilled solutions, not the diversity metric used to select them.\n\n- The practical application and scalability of the proposed SFT method are questionable. The process of creating the high-divergence dataset involves generating a large pool of solutions, enumerating all possible subsets (e.g., all 4-solution subsets), and calculating divergence for each one. This is a combinatorial explosion that is computationally infeasible for any reasonably sized pool of candidate solutions or larger subset selections."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M9nCc5LwKv", "forum": "tCTUv4zbCx", "replyto": "tCTUv4zbCx", "signatures": ["ICLR.cc/2026/Conference/Submission20115/Reviewer_98hL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20115/Reviewer_98hL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762286868059, "cdate": 1762286868059, "tmdate": 1762933009854, "mdate": 1762933009854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}