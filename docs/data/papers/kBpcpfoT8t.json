{"id": "kBpcpfoT8t", "number": 6777, "cdate": 1757995337069, "mdate": 1762920602378, "content": {"title": "LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation", "abstract": "Unified multimodal models have recently shown remarkable gains in both capability and versatility, yet most leading systems are still trained from scratch and require substantial computational resources. In this paper, we show that competitive performance can be obtained far more efficiently by strategically fusing publicly available models. Specifically, our key design is to retain the original VLM and DiT blocks while additionally interleaving multimodal self-attention blocks throughout the network. This double fusion mechanism (1) effectively enables rich multi-modal fusion while largely preserving the original strengths of the base models, and (2) catalyzes synergistic fusion of high-level semantic representations from a ViT encoder with low-level spatial signals from a VAE encoder. By training with only $\\sim35B$ tokens, this approach achieves strong results across multiple benchmarks: 0.89 on GenEval for compositional text-to-image generation, 82.28 on DPG-Bench for complex text-to-image generation, 6.06 on GEditBench, and 3.65 on ImgEdit-Bench for image editing. \nWe will fully release the entire suite of code, model weights, and datasets to support future research on unified multimodal modeling.", "tldr": "Strong unified multimodal models trained with only 35B tokens!", "keywords": ["Unified Multimodal Understanding and Generation Modeling; Text-to-Image Generation; Image Editing"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6018c2e3ac71ee2d1686c2be6b3519bcf83a2a20.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes LightBagel, a unified multimodal model that couples a pretrained VLM understanding branch with a pretrained DiT generation branch by inserting zero-initialized multimodal self-attention after each block in both branches, allowing ViT tokens to carry high-level semantics while VAE tokens preserve spatial detail. The understanding branch remains frozen, the reported training budget is modest, and the system is competitive on GenEval, DPG-Bench, GEdit-Bench, and ImgEdit-Bench under the stated constraints. Despite these merits, the novelty appears limited because similar unified modeling strategies have already been extensively explored by the community, so the contribution reads more as a careful engineering integration than a substantive conceptual advance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "LightBagel presents a clean and practical architecture that interleaves zero-initialized multimodal attention to couple strong pretrained components while preserving their initial capabilities. The method establishes a clear token-level division of labor, with ViT tokens carrying semantics and VAE tokens capturing spatial detail, which aligns well with image editing. Results are competitive relative to the reported training budget, and the paper presents a coherent token-efficiency narrative. Ablations on fusion depth and timestep shift provide concrete and reproducible design guidance."}, "weaknesses": {"value": "1. **Incremental novelty relative to recent unified models:** LightBagel interleaves zero-initialized multimodal attention after each block to connect a frozen VLM and a DiT, but similar deep-fusion and connector-style unification has already been explored in BAGEL and LMFusion, which also preserve pretrained capabilities while enabling generation. To isolate the technical contribution, the paper should add head-to-head ablations against them with strict parity in parameter counts, sequence lengths, training tokens, and data, and analyze where interleaving at matched depth yields measurable advantages over the generalized causal attention or connector designs used previously.\n\n2. **Evaluation clarity and fairness on GenEval:** The reported GenEval number appears to rely on an LLM-rewriter setting, while several baselines mix rewritten and non-rewritten prompts. Since GenEval is sensitive to prompt phrasing, this can inflate relative gains. The paper should report both rewritten and non-rewritten results for LightBagel, mark every baseline consistently.\n\n3. **Insufficient compute and memory accounting:** The method inserts additional attention modules at every layer, yet the paper does not specify hardware, GPU count, training throughput, effective tokens per second, peak memory, or inference latency. Without a cost profile, readers cannot judge efficiency or deployability."}, "questions": {"value": "1. **Novelty and parity:** Could you briefly clarify the concrete technical delta over BAGEL and LMFusion and, if feasible, include one small controlled comparison under roughly matched settings to illustrate when interleaving at matched depth helps?\n\n2. **GenEval settings and fairness:** Please confirm whether your GenEval score uses the LLM-rewriter and whether baselines are aligned. If possible, add a short two-row table showing LightBagel with and without rewriting to indicate sensitivity.\n\n3. **Compute and memory accounting:** Please share indicative numbers for a representative setup (GPU model/count, training hours) and a single-shot inference latency and peak memory at 512 px to contextualize cost."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h1l9MdiWGw", "forum": "kBpcpfoT8t", "replyto": "kBpcpfoT8t", "signatures": ["ICLR.cc/2026/Conference/Submission6777/Reviewer_KSuX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6777/Reviewer_KSuX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761563775908, "cdate": 1761563775908, "tmdate": 1762919053580, "mdate": 1762919053580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Ol6FRTZfjA", "forum": "kBpcpfoT8t", "replyto": "kBpcpfoT8t", "signatures": ["ICLR.cc/2026/Conference/Submission6777/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6777/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762920601008, "cdate": 1762920601008, "tmdate": 1762920601008, "mdate": 1762920601008, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed a unified multimodal framework that bridges understanding and generation in a single architecture. They build upon two pre-trained backbones (one for visual-language understanding and one for diffusion-based image generation), and subsequently interconnect them via a layer-wise fusion module to allow semantic features learned by the understanding branch to flow into the generation branch (and vice versa), such that both high-level semantic visual features and low-level image latent features are jointly leveraged.\n\nThe authors also collected approximately 45 million samples, which are mostly from publicly available datasets, with about 10% supplemented by a synthetic self-curated dataset, to train the model. Empirical evaluations across standard benchmarks show that their model achieves strong performance on both multimodal reasoning and image generation/editing tasks, demonstrating that this kind of “dual-path, dual-fusion” design can efficiently reuse pre-trained models for unified multimodal tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Layer-wise Bidirectional Fusion Design: Authors introduce a systematic, layer-wise fusion scheme that allows bidirectional information exchange between a frozen vision–language understanding backbone and a diffusion-based generation backbone. This integration is clearly described, implemented, and empirically shown to yield improvements across multiple multimodal tasks.\n2. Effective Reuse of Pre-trained Models: The Authors freeze the understanding branch and insert zero-initialized fusion layers, maintaining stability during training.\nThis provides a practical contribution to multimodal system engineering, though it depends heavily on the availability of strong pretrained backbones.\n3. Empirical Validation Across Multiple Benchmarks: The paper reports gains on well-known datasets such as GenEval, DPG-Bench, and GEditBench-EN, indicating that the proposed integration produces measurable practical benefits."}, "weaknesses": {"value": "1. Lack of Theoretical or Representational Analysis: The proposed architecture is more likely an incremental combination of existing pretrained models and fusion layers without introducing a novel learning mechanism. The core contribution feels more like one engineering work, despite that the engineering integration is neat and the results are okay. \n2. Data Efficiency: By inserting only minimal fusion modules and freezing most parameters, the model achieves competitive performance with comparatively limited additional training cost. But the claimed “efficiency” is relative. The framework leverages already-expensive pre-trained components rather than achieving efficiency from first-principles learning design.\n3. Lack of Causal Ablation and Attribution Clarity: The paper claims gains from “Double Fusion,” but fails to show which part (bidirectionality, layer count, or dual-level features) truly matters.  For example, in order to determine whether bidirectional information exchange between the understanding and generation branches is essential or merely redundant, controlled experiments can be set to compare the proposed method with unidirectional and no-fusion variants."}, "questions": {"value": "1. I am not sure what 50% depth means in Section 4.4. 0% depth represents that i-th DiT block is conditioned on i-th VLM block, and 100% represents that every DiT block is conditioned on the final VLM block. Does 50% depth means every DiT block is conditioned on the layer of the middle depth VLM block?\n2. Since ViT tokens are from Qwen2.5-VL and 3D causal VAE from Wan2.2-TI2V-5B, I cannot find them in the experimental results. While authors claimed the proposed method can keep the strong ability of each existing model, it is better to compare LightBagel with them. If I miss the comparison results, authors can point them out in the rebuttals.\n3.  Authors have created a synthetic dataset of 4.5 million samples and used it together with some other public datasets to train the LightBagel model. The experimental results combine the effects of both the dual-fusion architecture and the newly curated dataset, making it difficult to isolate the contribution of each factor."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pe6UZv6j3e", "forum": "kBpcpfoT8t", "replyto": "kBpcpfoT8t", "signatures": ["ICLR.cc/2026/Conference/Submission6777/Reviewer_J38T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6777/Reviewer_J38T"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570251579, "cdate": 1761570251579, "tmdate": 1762919053141, "mdate": 1762919053141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LightBagel, a light-weight unified multimodal model designed to achieve competitive performance in both multimodal understanding and image generation/editing with significantly reduced training cost.\nThe key idea is a double fusion mechanism that interleaves multimodal self-attention blocks between pretrained Vision-Language Models and Diffusion Transformers. This architecture—termed “deep fusion”—preserves the original strengths of each base model (autoregressive reasoning in VLM and high-fidelity rendering in DiT) while enabling rich cross-modal interactions throughout the network.\nLightBagel employs: A dual-pathway architecture (VLM + DiT) augmented with zero-initialized multimodal self-attention modules for stable integration. A layer-wise fusion strategy connecting high-level ViT features and low-level VAE representations. A carefully curated dataset of 45M multimodal samples (public + synthetic) covering text-to-image and image-editing tasks. Trained with only ~35B tokens, it matches or surpasses larger models such as OmniGen2, BAGEL, and UniPic, while using 10×–20× fewer tokens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written, with experiments on multiple benchmarks and thorough ablation studies.\n2. LightBagel achieves competitive or superior results to large-scale UMMs while using an order of magnitude fewer training tokens, demonstrating excellent token efficiency."}, "weaknesses": {"value": "1. The paper lacks comparison with two base models Qwen and Wan (quantitative/qualitative) to show the improvements of design.\n2. The paper focuses heavily on benchmark scores but lacks detailed investigation into why double fusion improves multimodal alignment or where it might fail.\n3. The design seems to only improve generation ability with understanding. The authors are actually adding an understanding module to more accurately generate images."}, "questions": {"value": "I believe that a truly unified framework should not just combine an understanding model and a generative model in the most lightweight way, but should enable mutual enhancement between understanding and generation.\nIn this framework, it seems that only the generative side is being strengthened — what about the other direction?\nI will increase my score if this question is addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZHKoihWTD6", "forum": "kBpcpfoT8t", "replyto": "kBpcpfoT8t", "signatures": ["ICLR.cc/2026/Conference/Submission6777/Reviewer_EPax"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6777/Reviewer_EPax"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707870384, "cdate": 1761707870384, "tmdate": 1762919052765, "mdate": 1762919052765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an efficient fusion strategy that utilizes publicly available unified multimodal models trained for both understanding and generation.  The paper uses the MoT architecture to enable layer-by-layer feature interaction between understanding models and generation models. The proposed method, LightBagel, achieves competitive performance in comprehension, generation, and editing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Unified Multimodal Models are currently a hot topic of research. The training strategy proposed in the paper can efficiently utilize the understanding and generation capabilities of open-source models and achieve unification.\n2. The paper's experimental results are competitive and comparable to the performance of current unified multimodal models."}, "weaknesses": {"value": "1. The architecture proposed by the paper has already been discussed in Bagel, and Bagel's experiments demonstrated the architecture's effectiveness. The paper seemingly just trains open-source understanding and generation models following Bagel's structure.\n2. Insufficient experiments: The paper's core module adopts an MoT structure to fuse the understanding and generation models, but it provides no comparison regarding the choice of the MoT structure or against other fusion methods."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CUHOaM8HtH", "forum": "kBpcpfoT8t", "replyto": "kBpcpfoT8t", "signatures": ["ICLR.cc/2026/Conference/Submission6777/Reviewer_6ety"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6777/Reviewer_6ety"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751225154, "cdate": 1761751225154, "tmdate": 1762919052391, "mdate": 1762919052391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}