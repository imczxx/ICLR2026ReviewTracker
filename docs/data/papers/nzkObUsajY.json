{"id": "nzkObUsajY", "number": 15417, "cdate": 1758251138026, "mdate": 1759897308393, "content": {"title": "How to Get Spiking LLMs? A Dual ANN-to-SNN Conversion with Layer-Wise Calibration", "abstract": "With rising concerns about data privacy, deploying large language models (LLMs) on edge devices rather than relying solely on cloud-based solutions is becoming increasingly essential. Nonetheless, the constrained power and computational capacity of edge hardware frequently hinder the practical deployment of LLMs. Spiking Neural Networks (SNNs) have gained attention as a viable alternative, offering brain-inspired efficiency and low power consumption, making them ideal for edge deployment. Among various SNN training strategies, ANN-to-SNN conversion stands out for its relatively low computational cost compared to training spiking networks from scratch. However, conventional conversion methods still require a specially trained, conversion-friendly ANN, which becomes prohibitively expensive when applied to large-scale models like LLMs. To address this limitation, we propose a novel ANN-to-SNN conversion framework that can be regarded as a dual version of conventional conversion methods. Built on quantized LLMs, our approach eliminates the need to train a dedicated ANN tailored for conversion. A key challenge in such conversions is the temporal dynamics of spike arrivals—commonly known as unevenness error—which can cause significant performance degradation. To mitigate this issue, we introduce a parameter-efficient, layer-wise calibration technique that effectively reduces conversion errors, particularly unevenness error, while keeping computational overhead minimal. Theoretical analysis demonstrates that our calibration method substantially lowers the final conversion error between the original LLM and its spiking counterpart. Extensive experiments on LLaMA models show that our method achieves performance comparable to state-of-the-art quantization techniques, highlighting its effectiveness.", "tldr": "", "keywords": ["calibration", "spiking neuron network", "large language model", "ann-to-snn conversion"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7375ebb9ada44d4f56e43ef53a0b882b0989672a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents an ANN-to-SNN conversion framework for spiking LLMs. It employs the Integer Spiking (IS) neuron, which has multiple thresholds and fires multi-bit spikes. Building on the IS neuron, Quantized ANN (QANN) can be converted to SNN without training another tailored ANN. The proposed conversion method also utilizes the layer-wise calibration to enhance the performance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Theoretical analyses of the error bounds of conversion prove the effectiveness of the layer-wise calibration.\n2. Compared to conventional ANN2SNN conversion approaches, the proposed framework does not require training a tailored ANN, which is important for LLM conversion."}, "weaknesses": {"value": "1. The contributions of this paper are incremental. First, previous work [1] has already proposed layer-wise calibration methods. This paper hides this technical background and avoids citing the prior research. Furthermore, the multi-bit spike neuron (i.e., M-HT neuron [2], Burst spikes [3], etc.) has also been proposed. Additionally, there is also research [4] that converts QANNs to SNNs. I believe the contributions of this paper are limited to using multi-bit spike neurons to convert QANNs to SNNs.\n2. Critical technical details are missing. This paper does not describe how key components of the Transformer, including Layer Norm, Softmax, and matrix multiplication, are converted to a form suitable for SNNs.\n3. Lack of energy efficiency analysis. This paper does not analyze the energy efficiency advantages of the converted SNN over the original QANN. Furthermore, the proposed method employs multi-bit spikes. While this facilitates the conversion of QANN to SNN, it also introduces additional computational overhead in the linear layer. This paper does not analyze this impact either.\n4. This paper lacks comparisons with state-of-the-art conversion methods, such as SpikeZIP-TF [5], in terms of performance and energy efficiency.\n\n\n\n[1] Li, Yuhang, et al. \"A free lunch from ANN: Towards efficient, accurate spiking neural networks calibration.\" *International conference on machine learning*. PMLR, 2021.\n\n[2] Hao, Zecheng, et al. \"LM-HT SNN: Enhancing the performance of SNN to ANN counterpart through learnable multi-hierarchical threshold model.\" *Advances in Neural Information Processing Systems* 37 (2024): 101905-101927.\n\n[3] Li, Yang, and Yi Zeng. \"Efficient and Accurate Conversion of Spiking Neural Network with Burst Spikes.\" *Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence*. 2022.\n\n[4] Yang, Yuchen, et al. \"NeuBridge: bridging quantized activations and spiking neurons for ANN-SNN conversion.\" *Neuromorphic Computing and Engineering* 5.2 (2025): 024018.\n\n[5] You, Kang, et al. \"SpikeZIP-TF: Conversion is All You Need for Transformer-based SNN.\" *International Conference on Machine Learning*. PMLR, 2024."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OGTkOUhwLP", "forum": "nzkObUsajY", "replyto": "nzkObUsajY", "signatures": ["ICLR.cc/2026/Conference/Submission15417/Reviewer_jzES"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15417/Reviewer_jzES"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761565010122, "cdate": 1761565010122, "tmdate": 1762925695134, "mdate": 1762925695134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a two-stage ANN-to-SNN conversion framework that improves scalability to large language models. The authors propose a layer-wise calibration method that significantly reduces conversion errors. The framework is demonstrated through converting pre-trained LLaMA models into spiking large language models (SNN-LLMs), achieving performance comparable to state-of-the-art quantization methods."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper explores ANN-to-SNN conversion in the context of very large models.\n- The proposed layer-wise calibration approach is theoretically motivated and empirically validated."}, "weaknesses": {"value": "- The paper claims to be “training-free,” but the calibration step still appears to involve optimization via BPTT or similar gradient-based tuning of neuronal thresholds. This step could reintroduce significant computational costs, especially for LLM-scale models.\n- There is a mismatch between the stated motivation and the evaluation metrics. The central motivation of ANN-to-SNN conversion is energy efficiency and neuromorphic deployability, yet the experiments focus solely on accuracy and perplexity benchmarks. The paper would benefit from including energy, latency, or hardware feasibility analyses.\n- The proposed models (tested on LLaMA-2 and LLaMA-3) are far beyond the capacity of existing neuromorphic chips (e.g., Loihi 2, TrueNorth). The authors should clarify which hardware platforms are envisioned for deployment, and whether the approach offers any practical energy advantage over quantized or sparsified ANN-based LLMs.\n- The paper employs graded spikes. This design choice should be explicitly stated early in the paper and discussed in terms of hardware constraints, as graded spikes may complicate neuromorphic implementation."}, "questions": {"value": "See in Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5K31PZpp9Z", "forum": "nzkObUsajY", "replyto": "nzkObUsajY", "signatures": ["ICLR.cc/2026/Conference/Submission15417/Reviewer_VqZM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15417/Reviewer_VqZM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893688805, "cdate": 1761893688805, "tmdate": 1762925694609, "mdate": 1762925694609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel \"dual ANN-to-SNN conversion\" framework to address the challenge of efficiently converting Large Language Models into Spiking Neural Networks. The core idea is to introduce a Quantized ANN as an intermediate model, decoupling the conversion process into two stages: ANN→QANN and QANN→SNN. To achieve precise modeling, the authors design a new Integrate-and-Spike with Reset neuron and provide a rigorous theoretical proof of its functional equivalence to standard Integrate-and-Fire neurons, which lays a solid foundation for hardware implementation. The paper also conducts theoretical error decomposition and proposes a layer-wise calibration method to optimize performance. Experiments on LLaMA models demonstrate the effectiveness of the approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Decoupling the ANN-to-SNN conversion into \"quantization\" and \"temporal dynamicization\" sub-problems via a QANN intermediate bridge is a ingenious approach.\n\n2.The proposed IS neuron model and its equivalence proof to IF neurons are proposed."}, "weaknesses": {"value": "1.The method heavily relies on the discrete integer outputs of the QANN. For modern LLMs using non-uniform quantization or containing complex activation functions (e.g., SwiGLU), their discretized values may be difficult to precisely reconstruct with a finite number of spike timesteps, potentially limiting the method's generalizability.\n\n2.Although the equivalence between IS and IF neurons is proven, this implementation requires a large number of IF neurons and timesteps to simulate a single IS neuron. The paper completely ignores the area overhead (requiring more neurons) and time overhead (requiring more timesteps) introduced by this mapping, which is crucial for evaluating the actual energy efficiency of the SNN."}, "questions": {"value": "As in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GmEYRbYQEi", "forum": "nzkObUsajY", "replyto": "nzkObUsajY", "signatures": ["ICLR.cc/2026/Conference/Submission15417/Reviewer_pCuY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15417/Reviewer_pCuY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985986641, "cdate": 1761985986641, "tmdate": 1762925693771, "mdate": 1762925693771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel method for efficiently converting LLMs into SNNs.. The core methodology includes: (1) introducing a membrane potential alignment mechanism during ANN-to-SNN conversion to reduce activation distribution mismatches; (2) adopting a parameter-efficient fine-tuning strategy that only learns neuronal firing thresholds and initial membrane potentials per layer, while keeping pretrained weights frozen; and (3) further incorporating grouped activation scaling, where a small number of learnable parameters are shared across activation groups to compensate for quantization and clipping errors. Experiments on LLaMA show significant improvements over conventional SNN conversion baselines, achieving competitive zero-shot accuracy and language modeling performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper goes beyond empirical heuristics by providing formal analysis, which bounds the approximation error between SNN outputs and ANN activations within a finite number of timesteps.\n\n2.The proposed membrane potential alignment and grouped activation fine-tuning are theoretically grounded and achieve a good trade-off between parameter efficiency and performance."}, "weaknesses": {"value": "1.Despite claims about low-power deployment, all experiments are simulation-based. No energy, latency, or throughput measurements are reported on neuromorphic hardware such as Loihi, making it impossible to validate the practical feasibility of edge deployment.\n\n2.The evaluation is only on LLaMA-2 and LLaMA-3, with no testing on other mainstream LLM architectures."}, "questions": {"value": "1.The IS neurons produce only non-negative and bounded activation. How can they effectively emulate SwiGLU, which involves negative activation values?\n\n2.Although the paper establishes functional equivalence between IS and IF neurons, the practical implementation requires multiple IF neurons and a large number of timesteps to emulate a single IS neuron. Have the authors considered the resulting area overhead due to increased neuron count? How do these factors impact the actual energy efficiency and throughput when deployed on real neuromorphic hardware?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n1wwWTXbTe", "forum": "nzkObUsajY", "replyto": "nzkObUsajY", "signatures": ["ICLR.cc/2026/Conference/Submission15417/Reviewer_S5K6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15417/Reviewer_S5K6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762485104643, "cdate": 1762485104643, "tmdate": 1762925693449, "mdate": 1762925693449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}