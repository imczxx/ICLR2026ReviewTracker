{"id": "rPc5AkREUR", "number": 1223, "cdate": 1756866207770, "mdate": 1759898220394, "content": {"title": "SCALER: Fast and Effective Graph Anomaly Detection via Dual-Level Synergistic Contrastive Learning", "abstract": "Unsupervised graph anomaly detection (UGAD) is crucial for identifying anomalous behavior in graph-structured data. However, recent deep learning-based UGAD methods, while effective, suffer from long inference times due to neighborhood aggregation, which limits their applicability in real-world scenarios. \nMoreover, current contrastive-based approaches are constrained by the limitations of node–subgraph contrast and the limited use of edge-level contrastive signals.\nTo address these issues, we propose SCALER, a self-supervised MLP-GNN learning framework that trains a structure-aware multilayer perceptron (MLP) for UGAD without requiring costly graph neural network (GNN) aggregation during inference.\nSCALER introduces a dual-level contrastive learning network that combines node-level and edge-level contrast to effectively guide MLP training.\nThe edge-level contrastive strategy leverages rich relational information embedded in edges to enhance node representations and improve anomaly detection.\nFurthermore, a neighborhood entropy-guided anomaly score correction module is incorporated to further improve robustness against anomalous nodes with low neighborhood entropy.\nExtensive experiments on eight real-world benchmark datasets, including a large-scale OGB dataset, against thirteen state-of-the-art baselines demonstrate that SCALER significantly improves detection performance across three metrics, particularly achieving an average gain of 19.6\\% in AUPRC, while reducing inference time to the order of seconds. These results validate the effectiveness and efficiency of SCALER.", "tldr": "", "keywords": ["Attributed Networks", "Anomaly Detection", "Contrastive Learning", "Self-supervised"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/044338127717b24e1873095a87e4945e4aff25a5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of UGAD, where existing deep learning methods suffer from long inference times due to neighborhood aggregation and limitations in contrastive learning strategies. The authors propose SCALER, a self-supervised MLP-GNN framework that trains a structure-aware MLP through dual-level synergistic contrastive learning. The method incorporates node-level and edge-level contrastive networks to capture structural patterns, and introduces a neighborhood entropy-guided anomaly score correction module."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Using only MLP for inference is an interesting approach, and experimental results show that it requires only a small amount of runtime.\n2. The paper designs an anomaly score correction module based on neighbor entropy, optimizing for low-entropy anomaly nodes and avoiding the limitations of traditional methods that rely solely on consistency scores.\n3. The experimental design is detailed and comprehensive, and the results are excellent."}, "weaknesses": {"value": "1. Multi-scale graph contrastive learning has achieved excellent results, and the differences between dual-level contrastive learning and existing methods are worth discussing.\n2. How should we understand \"dual-level synergistic\"? It seems to be just a simple combination of different perspectives.\n3. Dual-level contrastive learning is all about the design of low-order neighbors. If the graph contains long-range dependencies, MLP may not perform well."}, "questions": {"value": "Have the authors considered contrastive learning at the sub-graph level?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V1fvUFhNW4", "forum": "rPc5AkREUR", "replyto": "rPc5AkREUR", "signatures": ["ICLR.cc/2026/Conference/Submission1223/Reviewer_pyNh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1223/Reviewer_pyNh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761481948725, "cdate": 1761481948725, "tmdate": 1762915713134, "mdate": 1762915713134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of Unsupervised Graph Anomaly Detection (UGAD), particularly the high inference latency of existing deep learning methods due to neighborhood aggregation and limitations in current contrastive learning strategies, such as node-subgraph contrast. The authors propose SCALER, a self-supervised MLP-GNN learning framework that trains a structure-aware Multilayer Perceptron (MLP) for fast anomaly detection without requiring costly Graph Neural Network (GNN) aggregation during inference. Extensive experiments on eight benchmark datasets show that SCALER significantly improves detection performance (e.g., average gain of $19.6\\%$ in AUPRC) while reducing inference time to the order of seconds, demonstrating high effectiveness and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* High Efficiency with Performance Retention: The MLP-GNN framework successfully decouples costly GNN aggregation from the inference phase, leading to orders of magnitude faster anomaly estimation while achieving superior detection performance, a key practical advantage.\n* Novel Dual-Level Contrastive Learning: The introduction of a more robust node-level neighborhood contrast (using first-order neighbors as positive samples) and a novel edge-level contrast effectively addresses the limitations of previous contrastive UGAD methods like sampling-based node-subgraph contrast.\n* Strong Empirical Results: The method achieves state-of-the-art results across various metrics (AUROC, AUPRC, Recall@K) and datasets, demonstrating robust and generalizable detection capability. The performance is notably strong in the challenging AUPRC metric.\nScalability: The framework's ability to handle the large-scale ogbn-Arxiv dataset, where many baselines encounter out-of-memory issues, confirms its practical scalability."}, "weaknesses": {"value": "* Complexity of Training Objective: The joint training objective involves three main loss terms (\\mathcal{L}_{node}, \\mathcal{L}_{edge},  \\mathcal{L}_{reg})  controlled by three hyperparameters (\\alpha, \\beta, \\gamma). Although a hyperparameter analysis is provided, the need to carefully tune three trade-off parameters adds complexity to deployment and reproduction on new datasets.\n* Generalization Analysis Clarity: While the generalization analysis in Appendix E.5 is valuable, its description, especially the “Anomaly detection performance on extended subgraphs” section (E.5.2), could be clearer about the specific experimental setup and the connection between $G_1$ and $G_2$ (the dashed lines in Figure 13 are only present in the first diagram in the appendix, not the second that is referenced). The performance of the MLP trained only on $G_1$ on the unseen $G_2$ subgraph should be a strong point, but the comparison with HUGE is a bit obscured in the text"}, "questions": {"value": "* AUPRC vs. AUROC Trade-off on Reddit: On the Reddit dataset, SCALER’s AUROC is slightly below the best baseline (0.5937 vs. 0.5980 for GRADATE), but its AUPRC is the best (0.0523 vs. 0.0509 for HUGE). Given AUPRC is generally a better indicator for imbalanced anomaly detection, could the authors provide a more detailed interpretation of this slight discrepancy in AUROC, and how the dual-level contrastive strategy specifically contributes to the higher AUPRC and Recall@K on this dataset?\n* Necessity of $\\mathcal{L}_{reg}$ for Organic Anomalies: For the organic anomaly datasets Reddit and YelpChi, the best performance is achieved when the regularization weight $\\gamma=0$. This suggests that for these graphs, the distribution shift is weak, and the regularization term is unnecessary or even detrimental. Could the authors expand on why the self-supervised training on these organic anomaly datasets introduces a weaker distribution shift compared to the synthetic anomaly datasets?\n* Edge Feature Utilization: The edge embedding is simply defined as the average of its two endpoint node embeddings $E_{ij}^{GNN}=\\frac{H_i^{GNN}+H_j^{GNN}}{2}$. For richer relational information, have the authors considered a more complex edge embedding function that might involve edge attributes (if available) or a dedicated edge feature learning module, and how might this impact overall performance and efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dqvo3W97db", "forum": "rPc5AkREUR", "replyto": "rPc5AkREUR", "signatures": ["ICLR.cc/2026/Conference/Submission1223/Reviewer_ndWk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1223/Reviewer_ndWk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880468530, "cdate": 1761880468530, "tmdate": 1762915712996, "mdate": 1762915712996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SCALER, a self-supervised graph anomaly detection framework that combines MLP and GNN. SCALER achieves high performance and fast inference without costly GNN aggregation, even a large-scale OGB dataset. It uses a dual-level contrastive learning network, leveraging both node-level and edge-level contrast to train the MLP. Additionally, SCALER includes a neighborhood entropy-guided anomaly score correction module to enhance robustness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The framework is well-designed and comprehensively evaluated. The experiments cover multiple datasets and metrics, demonstrating the effectiveness and efficiency of SCALER.\n* The neighborhood entropy-guided anomaly score correction module is interesting to me.\n* The proposed method avoids costly GNN neighbor aggregation during inference, thus speeding up the process and reducing GPU memory usage, especially for large-scale datasets. This is crucial for practical deployment."}, "weaknesses": {"value": "* In Fig. 3, is the \"structure-aware MLP\" the same as the \"MLP\" used in training? Both are shown in orange boxes. It would be helpful to add clear annotations to the figure. Also, why use different names? This would make it easier for readers to understand.\n* The paper doesn't seem to mention any work on accelerating GNN aggregation. This relevant work should be discussed.\n* Have there been attempts at better methods for the global negative edge pool, such as adaptive numbers and targeted selection instead of fixed numbers and random selection?\n* In Table 1, why are the AUPRC for the comparison methods on the Reddit and YelpChi datasets very low? This doesn't match the results in [1].\n\n[1] GCTAM: Global and Contextual Truncated Affinity Combined Maximization Model For Unsupervised Graph Anomaly Detection. International Joint Conference on Artificial Intelligence (2025)."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m8zZ91FL4A", "forum": "rPc5AkREUR", "replyto": "rPc5AkREUR", "signatures": ["ICLR.cc/2026/Conference/Submission1223/Reviewer_r86T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1223/Reviewer_r86T"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010644393, "cdate": 1762010644393, "tmdate": 1762915712869, "mdate": 1762915712869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SCALER, a UGAD framework that transfers structural knowledge from GNNs into MLPs, combined with dual-level contrastive learning (node- and edge-level) and entropy-guided anomaly score correction. The goal is to retain strong anomaly detection performance while achieving fast inference without GNN aggregation. Experiments on benchmark datasets demonstrate improved AUROC/AUPRC and significantly faster inference compared to existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. This paper addresses a practical UGAD bottleneck by enabling fast inference without GNN aggregation.\n\nS2. It proposes a dual-level contrastive learning design that effectively improves over existing subgraph contrast approaches (e.g., CoLA).\n\nS3. It provides strong empirical results, showing consistent performance gains and large inference speedups across various real-world datasets."}, "weaknesses": {"value": "W1. This paper shows limited novelty, as it mainly refines existing ideas rather than introducing a fundamentally new UGAD approach.\n\nW2. This paper uses relatively high anomaly ratios (3-6%), which weakens claims of real-world applicability.\n\nW3. This paper lacks training-time comparisons, despite requiring a GNN and dual contrastive objectives during training.\n\nW4. This paper relies on a heuristic entropy-based score correction without validating alternatives."}, "questions": {"value": "Q1. What is the core novelty that prior node-neighborhood contrast and GNN-to-MLP transfer methods do not already provide?\n\nQ2. Can you report results under much lower anomaly ratios (e.g., <1%) to support real-world applicability?\n\nQ3. How does the training time and memory compare to baselines, given the need for a GNN and dual contrast during training?\n\nQ4. Why use a heuristic entropy-based correction, and how does it compare to a simple learnable alternative or show robustness via sensitivity analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aVBU2b13Ma", "forum": "rPc5AkREUR", "replyto": "rPc5AkREUR", "signatures": ["ICLR.cc/2026/Conference/Submission1223/Reviewer_CCsg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1223/Reviewer_CCsg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762151521183, "cdate": 1762151521183, "tmdate": 1762915712757, "mdate": 1762915712757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}