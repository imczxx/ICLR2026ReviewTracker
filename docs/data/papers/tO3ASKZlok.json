{"id": "tO3ASKZlok", "number": 16479, "cdate": 1758264987743, "mdate": 1759897238149, "content": {"title": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate", "abstract": "Vector quantization, a problem rooted in Shannon's source coding theory, aims to quantize high-dimensional Euclidean vectors while minimizing distortion in their geometric structure. We propose TurboQuant to address both mean-squared error (MSE) and inner product distortion, overcoming limitations of existing methods that fail to achieve optimal distortion rates. Our data-oblivious algorithms, suitable for online applications, achieve near-optimal distortion rates (within a small constant factor) across all bit-widths and dimensions. TurboQuant achieves this by randomly rotating input vectors, inducing a concentrated Beta distribution on coordinates, and leveraging the near-independence property of distinct coordinates in high dimensions to simply apply optimal scalar quantizers per each coordinate. Recognizing that MSE-optimal quantizers introduce bias in inner product estimation, we propose a two-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL (QJL) transform on the residual, resulting in an unbiased inner product quantizer. We also provide a formal proof of the information-theoretic lower bounds on best achievable distortion rate by any vector quantizer, demonstrating that TurboQuant closely matches these bounds, differing only by a small constant ($\\approx 2.7$) factor. Experimental results validate our theoretical findings, showing that for KV cache quantization, we achieve absolute quality neutrality with 3.5 bits per channel and marginal quality degradation with 2.5 bits per channel. Furthermore, in nearest neighbor search tasks, our method outperforms existing product quantization techniques in recall while reducing indexing time to virtually zero.", "tldr": "", "keywords": ["Vector Quantization", "KV Cache Compression", "Nearest Neighbor Search", "Similarity Search Acceleration", "Online Compression Algorithms"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7d33913c9a4f47c8abb294d6beb85d30124747ca.pdf", "supplementary_material": "/attachment/22147a262e2a80edd06730de1362350bbc8d4d13.zip"}, "replies": [{"content": {"summary": {"value": "The author introduce a new method for online vector quantization, which does not assume data-specific tuning, which is especially useful for applications such as KV cache compression in LLMs. Their method consist of applying a random rotation to map the (unit-normalized) distribution to a representation with near-independent coordinates whose distribution converges to a known distribution in high dimensions. This allows them to pre-compute per-coordinate $b$-bits codebooks optimized for this distribution, that are suitable for online usage. They further combine this method with QJL to provide unbiased inner product, which is important for KV cache compression."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is clearly explain and simple to use.\n- The proposed method is online, making it easily applicable to KV cache compression.\n- The authors provide theoretical justifications and bounds.\n- The proposed method has potential for broad application on LLM inference"}, "weaknesses": {"value": "While the presented method has undeniable strengths, they are undermined by weak experimental validations.\n- For KV compression, all compared methods use a different number of bits, making comparison of the accuracy-compression ratio harder to read. Better analysis of where this method takes place on this trade-off is needed.\n- KL compression is compared to only two other comparison methods, ignoring some other methods, including RotateKV which is cited by the authors. Comprehensive comparison is needed.\n- Authors propose “TurboQuant”, which revolves around the random matrix multiplication followed by per-bit quantization. The QJL and two-tier channel-wise quantization strategy parts come from other work and are generally applicable to some other methods, but are used in the comparison. Analysis of the individual component introduce by the paper would make results more convincing.\n- This method is only compared to existing ones on a single benchmark, and single model.\n\nThe near neighbor search experiments are the least convincing:\n- The method is only compared to PQ, and another method that barely surpasses PQ. For this kind of data-dependant method already surpass these baselines, including codebook-based methods (such as OPQ, RQ, LSQ++) or neural-network based (such as QINCo2 or UNQ).\n- An argument is made about the time needed to quantize the training set, but this has very limited impact. Real-world ANN settings are usually constrained mostly by the search speed and accuracy on CPU, and the limited memory, more than the 1-time quantization. This part lacks justification."}, "questions": {"value": "The method is interesting, and its simplicity is a strong argument for applying it. I would ask the authors to clarify their results and clearly justify the quality of the quantization in either KV compression or ANN settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9YkPjGeRbr", "forum": "tO3ASKZlok", "replyto": "tO3ASKZlok", "signatures": ["ICLR.cc/2026/Conference/Submission16479/Reviewer_k6Lz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16479/Reviewer_k6Lz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761496303780, "cdate": 1761496303780, "tmdate": 1762926583144, "mdate": 1762926583144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TurboQuant is proposed to minimize both mean-squared error and inner product distortion in vector quantization by using random rotation and optimal scalar quantizers, achieving near-optimal distortion rates across all bit-widths and dimensions, with a two-stage approach for unbiased inner product estimation, and experimental results validate its superiority over existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "As detailed below, I suspect there may be issues with the  two major innovations regarding mean-squared error and inner product distortion."}, "weaknesses": {"value": "The two major contributions claimed in the paper appear theoretically and logically untenable.\n\n1) **$Q_{mse}$:** This paper does not impose restrictions on the distribution of vector x, nor does it specify whether the elements of x are independent. If the elements are not independent (commonly seen in natural data), then multiplying x by a random rotation matrix cannot guarantee mutual independence among the resulting vector's elements (contrary to the statement in Line 215), rendering simple scalar quantization methods (which quantize elements individually) inapplicable. Consequently, the paper's claimed optimal mse quantization would not hold.\n\n2) **Unbiased $Q_{prod}$:** The multiplicative bias of $2/\\pi$  (derived in Line 296) for the inner product $⟨x,y⟩$ is a constant and does not affect comparisons between different inner products. Therefore, I question the necessity of eliminating this bias by introducing complex quantization schemes for the residual vectors."}, "questions": {"value": "Besides the two major concerns mentioned above, I have the following questions:\n\n1) Line 210: Why is multiplication with a random rotation matrix employed? Is the purpose to achieve a beta distribution? Additionally, is this approach originally introduced in this paper?\n\n2) The experiments are insufficient, as comparisons are made with only two quantization methods, and only a few bit-width cases are examined. \n\n3) The “online” property claimed in the title is not explicitly demonstrated in either the theoretical analysis or experimental results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "46LMgJSN4w", "forum": "tO3ASKZlok", "replyto": "tO3ASKZlok", "signatures": ["ICLR.cc/2026/Conference/Submission16479/Reviewer_Autm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16479/Reviewer_Autm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570401352, "cdate": 1761570401352, "tmdate": 1762926582651, "mdate": 1762926582651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TurboQuant as method for low-distortion vector quantization. To minimize MSE, TurboQuant first randomly rotates the input vector and then uses k-means generated codebook for each dimension. For an unbiased estimation of inner product, TurboQuant combines the random rotation method for MSE with an unbiased 1-bit quantization method via a residual quantization approach. Theoretical results show that the quantization errors of TurboQuant are close to the optimization. Empirical results show that TurboQuant provides good performance for both KV cache compression and nearest neighbor search."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Thank you for the interesting paper! I think this paper is among the best vector papers I have read in this year, and I learned a lot from the paper.\n\nS1: A comprehensive discussion for the applications of vector quantization is provided in the introduction.\n\nS2: TurboQuant has a low quantization complexity and yet strong performance.\n\nS3: The theoretical analysis is in-depth, and the theoretical results are strong.\n\nS4: Combing different vector quantization methods via a residual quantization like approach, despite straightforward, is interesting and makes sense.\n\nS5: The experiment results are strong and cover both KV cache compression and nearest neighbor search.\n\nS6: The presentation is fluent. Although the paper is heavy on math, the author makes it easy to read even for readers that may not be familiar with this area."}, "weaknesses": {"value": "The paper can be enhanced with more detailed discussions and experiments to compare TurboQuant with RaBitQ and variants (e.g., [1]). Note that I do not think RaBitQ affects the novelty of TurboQuant since the theorical results of TurboQuant are much stronger.\n\nD1: RaBitQ is discussed in the appendix but the discussions are far from insufficient. The original RaBitQ quantization is slow due to the joint search for the optimal quantized bits over dimensions but a recent work [1] solves the problem. Moreover, [1] also uses PCA to significantly reduce the quantization error of RaBitQ. RaBitQ and variants are similar to TurboQuant in that they all use random projection; but they are also different in crucial aspects, (1) they search for quantized bits while TurboQuant uses k-means generated codebook independently for each dimension; (2) they use a re-normalization trick for unbiased inner product estimations while TurboQuant combines random rotation with another vector quantization method. A crucial question is that which design is better for the two purposes, and giving clear answers will be valuable for this area. Some ablation experiments can be added for these design choices, e.g., check the quantization error by using different design combinations. It will even better if some theorical analysis can be conducted.     \n\n[1] SAQ: Pushing the Limits of Vector Quantization through Code Adjustment and Dimension Segmentation"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sNp6ee9fzN", "forum": "tO3ASKZlok", "replyto": "tO3ASKZlok", "signatures": ["ICLR.cc/2026/Conference/Submission16479/Reviewer_WFrV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16479/Reviewer_WFrV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915557449, "cdate": 1761915557449, "tmdate": 1762926582208, "mdate": 1762926582208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TurboQuant, an online vector quantization algorithm designed to speed up nearest-neighbor retrieval and vector search by continuously updating codebooks as new data arrives. The core idea is to maintain good quantization quality in a streaming setting by using lightweight updates rather than full re-training. The method provides theoretical guarantees on quantization error and memory usage and is evaluated on several ANN benchmarks to show improved trade-offs between accuracy, latency, and memory."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1 Addresses a relevant and timely problem: efficient vector quantization in streaming/online settings, which is important for large-scale retrieval systems.\n\nS2 Includes theoretical analysis that supports the stability and bounded error of the online updates.\n\nS3 Empirical results show clear improvements over static or periodically-retrained quantization baselines in both speed and accuracy.\n\nS4 Paper is generally well-written, and motivations for online updates vs. periodic retraining are clearly explained."}, "weaknesses": {"value": "The paper is in general quite solid. My question is that the paper seems missing comparisons with stronger or more recent quantization methods, such as those using residual or product quantization variants with adaptive codebook updates, or learned quantizers from recent literature. It’s not entirely clear how TurboQuant performs relative to the current state of the art. Can you provide more literature review and comparisons?"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yG1r6xI5FA", "forum": "tO3ASKZlok", "replyto": "tO3ASKZlok", "signatures": ["ICLR.cc/2026/Conference/Submission16479/Reviewer_X81d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16479/Reviewer_X81d"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985307513, "cdate": 1761985307513, "tmdate": 1762926581806, "mdate": 1762926581806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}