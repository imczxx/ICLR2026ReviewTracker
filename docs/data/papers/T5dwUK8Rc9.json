{"id": "T5dwUK8Rc9", "number": 10623, "cdate": 1758177860637, "mdate": 1763363425189, "content": {"title": "Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric Problems", "abstract": "Evaluating the mathematical capability of Large Language Models (LLMs) is a critical yet challenging frontier. Existing benchmarks fall short, particularly for proof-centric problems, as manual creation is unscalable and costly, leaving the true mathematical abilities of LLMs largely unassessed. To overcome these barriers, we propose Proof2Hybrid, the first fully automated framework that synthesizes high-quality, proof-centric benchmarks from natural language mathematical corpora. The key novelty of our solution is Proof2X, a roadmap of converting mathematical proofs into various kinds of questions that are easy to verify. Instructed by this roadmap, we propose a new type of hybrid-formatted questions, named ``$m$-out-of-$n$ multiple judge questions'', specifically designed to enable robust, automatic evaluation while being resilient to guessing and superficial pattern matching inherent in traditional formats. As a demonstration of our framework, we introduce AlgGeoTest, a benchmark for algebraic geometry—a frontier domain of modern mathematics—comprising 456 challenging items. Our extensive evaluations on state-of-the-art LLMs using AlgGeoTest reveal profound deficits in their comprehension of algebraic geometry, providing a more precise measure of their true mathematical capabilities. Our framework and benchmark pave the way for a new wave of in-depth research into the mathematical intelligence of AI systems. Our code and data are provided in the supplementary material.", "tldr": "", "keywords": ["Large Language Models", "Benchmarks", "Mathematics", "Datasets"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5ff74cf947172229a60dea92babe1b815d28b5a5.pdf", "supplementary_material": "/attachment/b56a9a64a5294636b8cbb12c261361f03d79c976.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Proof2Hybrid, a novel, fully automated framework designed to synthesize high-quality, \"proof-centric\" mathematical benchmarks. The authors identify a critical gap in current LLM evaluations, which primarily focus on \"number-centric\" problems with definite numerical answers, rather than the logical deduction required in advanced mathematics. Manual creation of proof-based benchmarks is costly and unscalable. The core of Proof2Hybrid is a Proof2X roadmap, which converts mathematical proofs from natural language corpora into easily verifiable questions. The framework's key innovation is a new \"m-out-of-n multiple judge question\" format. This format presents models with $n$ items (each a proposition-proof pair or definition) and states that exactly $m$ are correct. The workflow includes Seed Item Filtration, Distractor Generation, Distractor Filtration, Aggregation. As a demonstration, the authors create AlgGeoTest, a benchmark for algebraic geometry comprising 456 challenging 2-out-of-6 questions. Evaluations show that AlgGeoTest is extremely difficult for SOTA LLMs, with the best model scoring only around 60%."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles the significant challenge of evaluating LLMs on proof-centric problems, which are far more representative of advanced mathematical reasoning than simple calculation tasks. Its fully automated approach is a major contribution toward scalable and cost-effective benchmark creation\n\n2. The \"m-out-of-n\" format is a key strength. It is highly resilient to random guessing (unlike true/false questions).\n\n3. The resulting benchmark, AlgGeoTest, proves to be extremely challenging, with SOTA models performing poorly, which underscores its difficulty."}, "weaknesses": {"value": "1. While the framework is described as \"domain-agnostic\" , it is only demonstrated on a single, highly-structured data source (\"The Stacks project\") in one advanced domain (algebraic geometry). It remains to be seen how effectively this pipeline would adapt to other mathematical fields where \"subtle flaws\" might manifest differently.\n2. The benchmark, AlgGeoTest, is built using \"The Stacks project\" as its data source , which is a well-known, public, and open-source mathematical reference work. There is a high probability that this corpus was included in the training data for the very SOTA Large Language Models being evaluated. This creates an inherent risk of data contamination"}, "questions": {"value": "See my comments on weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bfEzOuVtJO", "forum": "T5dwUK8Rc9", "replyto": "T5dwUK8Rc9", "signatures": ["ICLR.cc/2026/Conference/Submission10623/Reviewer_oxHQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10623/Reviewer_oxHQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638133879, "cdate": 1761638133879, "tmdate": 1762921883318, "mdate": 1762921883318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1) Domain and the core question\n- Domain. Automated evaluation of mathematical proof understanding in large language models (LLMs), with a concrete instantiation in algebraic geometry.\n\n- Core question. Can we automatically synthesize high-quality, proof-centric benchmarks from natural-language math corpora, such that the resulting tasks are easy to verify, hard to game, and informative about models’ proof comprehension?\n\n- Does the paper answer it? Largely yes. The paper introduces Proof2Hybrid, a pipeline that turns proof texts into verifiable questions (via a “Proof2X” roadmap) and proposes a new m-out-of-n multiple-judge format to cut down guessing and format-specific artifacts. It then instantiates the pipeline on the Stacks Project to produce AlgGeoTest (456 items; each item has six sub-statements with exactly two true), and shows that many strong models score modestly, suggesting the benchmark captures real difficulty rather than format quirks.    \n\n2) Summary of what is proposed\n- Method. The pipeline first filters seed items (definitions or proposition-proof pairs) using several top models with repeated judgments; only items judged correct often enough survive (m₁ models × n₁ runs per model; keep items with ≥k₁ “correct”). It then generates distractors with another set of models, followed by a distractor filter using multiple judges with thresholds k₃–k₄ for “incorrect” votes to retain only subtly wrong variants. Finally, it assembles hybrid questions containing m correct seeds and n-m filtered distractors, with all n coming from different seeds. In their main setting, (m,n)=(2,6).      \n\n- Question format. The m-out-of-n design reduces random success from (1/2) (plain T/F) to (1/\\binom{n}{m}) (e.g., (1/15) when (m=2,n=6)), and prevents within-question “compare two versions of the same statement” tricks by ensuring all options come from different seeds. The authors also give a perplexity-based alternative for base models.   \n\n- Benchmark. AlgGeoTest is built from the Stacks Project. It contains 456 items, each with six sub-statements; exactly two are true. \n\n- Findings. Under the generation-based protocol, even the best model is around 60, and many scores are <20; reasoning-oriented models tend to do better. Base-model perplexity scores scale with size across Qwen and Llama families. The benchmark correlates only moderately with MATH-500 and AIME24 (R² ≈ 0.42, 0.51), indicating it captures different skills.   \n\n- Quality control. A human audit reports >98.75% of distractors are truly incorrect yet plausible, and >95% of questions meet the same bar."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A concrete answer to a real gap. Prior math benchmarks skew to numeric answers; proof-centric evaluation at scale is missing. The paper directly targets this gap with an automatic pipeline over a natural-language corpus rather than formal systems only. \n- Format innovation with clear rationale. The m-out-of-n format is well-motivated: it reduces chance accuracy, blocks option-comparison shortcuts, and reframes evaluation as relative correctness ranking, which can reduce sensitivity to each model’s internal “proof strictness” threshold. The math behind guessing risk and the construction constraint are explicit.  \n- Multi-model generation and judging. Using separate model pools for crafting and vetting reduces single-model bias; the thresholds (k_1,k_3,k_4) are specified, which is important for reproducibility.   \n- Non-trivial empirical signal. State-of-the-art models do not saturate, and the moderate correlation with number-centric math benchmarks suggests alg-geo proof understanding is not just a proxy for contest arithmetic.  \n- Human audit. The added manual check is valuable, given that the pipeline relies on LLM judges internally."}, "weaknesses": {"value": "A. Single-domain instantiation. The method is positioned as domain-agnostic, but the paper only shows algebraic geometry. To support generality, at least one additional area (e.g., commutative algebra or topology) would strengthen the claim. \n\nB. Style and memorization confounds. The true items are original seeds from the Stacks Project, while false items are model-generated edits. Well-trained models may recognize the “house style” of Stacks and prefer those options. A control where true items are paraphrased (or both true and false are style-matched) would test for such cues. The paper’s argument that “original items are more ‘correct’ than twisted ones” is intuitive, but it does not rule out style/memorization signals.  \n\nC. Assumptions in seed filtering. The paper asserts that hard but correct seeds will not be filtered out because models mark them “correct” when they cannot spot flaws. That is plausible but needs evidence (e.g., a labeled subset with known difficulty, and false-negative rates under the m₁,n₁,k₁ scheme). \n\nD. Bias from closed models and shifting APIs. The pipeline depends on several closed-source systems for generation and judging. This can affect reproducibility when APIs, safety rules, or model versions change. The release of final data helps, but end-to-end re-runs may be hard to replicate.  \n\nE. Metric clarity and baselines. For the generation-based protocol, two scoring rules (loose vs. tight) are defined, but there is no analysis of expected random scores under the loose rule or of calibration effects when models are forced to pick exactly m truths. For the perplexity protocol, “lowest perplexity” may reward surface familiarity with Stacks phrasing; a style-neutral rewording control would clarify this.  \n\nF. Limited ablations. We do not see ablations over (m,n), judge count, or thresholds (k_1,k_3,k_4). Such ablations would show how difficulty and reliability change with the pipeline’s knobs.  \n\nG. Cost accounting. The paper does not report the computational or monetary cost of building 456 items under m₁,n₁ and m₃,n₃ repeated judging, which matters for scalability claims."}, "questions": {"value": "Suggestion for Revision\n\n1.Second domain. Build a smaller second benchmark (e.g., 150–200 items) from another advanced area to confirm the method transfers beyond algebraic geometry. \n\n2.Style-matched controls. For a held-out slice, paraphrase the true items and lightly paraphrase the false ones to match tone and structure. Re-evaluate to measure any drop from losing “source style” cues. \n\n3.Leakage and recency check. Use Stacks entries added after the training cutoff of several models, or redact tell-tale tag IDs, to estimate memorization effects.\n\n4.Ablations. Vary (m,n), the judge pool size, and (k_1,k_3,k_4), and report how item acceptance rate, human-audit pass rate, and model scores shift.  \n\n5.Chance-level analysis. Report closed-form chance scores for both tight and loose grading, and include a random pick among (\\binom{n}{m}) baseline so readers can normalize results. \n\n6.Cost and reproducibility. Include prompt templates, seeds, model versions, and a cost table for each stage; add an “all-open” variant using only open models for both generation and judging.\n\n7.Human audit details. Report the number of auditors, guidelines, inter-rater agreement (e.g., Cohen’s κ), and a taxonomy of typical distractor flaws."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6EKUs2P4P9", "forum": "T5dwUK8Rc9", "replyto": "T5dwUK8Rc9", "signatures": ["ICLR.cc/2026/Conference/Submission10623/Reviewer_CTT8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10623/Reviewer_CTT8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820866446, "cdate": 1761820866446, "tmdate": 1762921882965, "mdate": 1762921882965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an agentic framework that autonomously converts free-flowing, natural-language mathematics to multiple choice questions. Two types of items are the main point of focus: definitions and theorems/propositions and their proof.\n\nThe agentic Framework is thus domain-agnostic and relies on a three-staged process: extracting the relevant item, creating variations of them, and then correcting the variations. All of these are carried out solely using LLMs.\n\nThe authors instantiate their approach on the Stacks project to obtain a benchmark on questions from algebraic geometry."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The question generation pipeline that is model agnostic has a lot of potential."}, "weaknesses": {"value": "- I have some misgivings about an  entirely LLM-assisted pipeline. This may propagate LLM biases in unexpected ways. \n\n- there is a single figure with results. These seem hard to read, and to take home information."}, "questions": {"value": "Paragraph 4 3 seems to weak to really draw any meaningful conclusions - some correlation, but not a strong one.\n\nCould this be improved, or the paragraph removed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D6wJTD0r9Q", "forum": "T5dwUK8Rc9", "replyto": "T5dwUK8Rc9", "signatures": ["ICLR.cc/2026/Conference/Submission10623/Reviewer_7M46"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10623/Reviewer_7M46"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762544100367, "cdate": 1762544100367, "tmdate": 1762921882488, "mdate": 1762921882488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the high cost of manual annotation in existing mathematical ability verification benchmarks by proposing the Proof2Hybrid framework for synthesizing proof-centric benchmarks. It also introduces AlgGeoTest, a benchmark based on a hybrid question format. However, the paper lacks clarity in its technical descriptions and provides insufficient experimental validation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper offers interesting insights into the synthetic framework for mathematical competence verification benchmarks and introduces AlgGeoTest, a noteworthy algebraic geometry benchmark featuring hybrid-format problems.\n- It provides a comprehensive overview of relevant research on mathematical benchmarks."}, "weaknesses": {"value": "- Writing clarity: The paper lacks clear presentation, making it difficult to identify key insights.\n- Insufficient data examples: Figure 1 and the question format comparison in Figure 3 are not adequately described or supported with clear examples, weakening the persuasiveness of the paper's core contributions.\n- Limited benchmark comparisons: The paper does not provide sufficient comparisons with other mathematical benchmarks (e.g., those listed in Table 1). An analysis of performance variations across different benchmarks under various LLM architectures is recommended.\n- Unclear technical details: Many aspects of the synthesis path are poorly explained; see the Questions section for further specifics."}, "questions": {"value": "- Why is the model used for FILTRATION different from the one used in the generation stage? What considerations or empirical insights informed this decision? For instance, have differences in LLM preferences for generating versus filtering interference items been analyzed, or are there relevant findings from prior research that support this choice?\n\n- How does AlgGeoTest perform compared to other mathematical benchmarks when evaluated with different LLM architectures? Including data case studies across benchmarks would help demonstrate the rationale and superiority of the proposed synthesis method.\n\n- How to understand \"large-scale\" and \"automatic\" in Table 1? It is recommended to add relevant explanations, such as how many data points are considered large-scale, or whether no human intervention is required in specific generation or verification stages.\n\n- Lines 145 and 151: What are IMO and IMO-level math questions?\n\n- The framework description in Part 3 lacks detailed explanations and references to each stage in Figure 1.\n\n- As described in 3.1, the collection of seed items focuses on mathematical definitions and mathematical proposition-proof pairs. What are their specific sources and presentation forms?\n\n- Line 208: How to understand the \"iterative refinement\" process of Proof2Hybrid? Where is it reflected in Figure 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2RFqRBnjmk", "forum": "T5dwUK8Rc9", "replyto": "T5dwUK8Rc9", "signatures": ["ICLR.cc/2026/Conference/Submission10623/Reviewer_r7MJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10623/Reviewer_r7MJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762796076214, "cdate": 1762796076214, "tmdate": 1762921882206, "mdate": 1762921882206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}