{"id": "1K9wMaRlnw", "number": 15181, "cdate": 1758248638801, "mdate": 1759897322947, "content": {"title": "CLAMP: A Chebyshev-Weighted Multi-Gradient Approach for Multi-Objective LLM Alignment", "abstract": "Alignment in large language models (LLMs) is crucial for enhancing their capabilities to align with human preferences. \nTo date, many existing alignment approaches, such as reinforcement learning from human feedback (RLHF)-based and reinforcement learning-free methods (e.g., direct preference optimization (DPO)), assume homogeneous human preferences. \nIn practice, however, human preferences are inherently heterogeneous and even conflicting, rendering traditional LLM alignment techniques inapplicable. \nToward this end, multi-objective alignment (MOA) methods have been developed to accommodate this diversity.\nYet, most of them rely on simple heuristics to address conflicting objectives, hence struggling to efficiently explore the full Pareto front and handle non-convex LLM alignment objective landscapes. \nAlthough there have been other alignment techniques attempt to address these issues, they still depend heavily on reinforcement learning (RL) or pre-trained reward models, resulting in computational inefficiency and susceptibility to reward-model-induced biases.\nIn this work, we propose the CLAMP (**C**hebyshev-weighted **L**LM **a**lignment with **m**ulti-objective **p**references), a new multi-objective alignment algorithmic framework that is both RL-free and reward-model-free. \nOur method integrates Chebyshev-weighted scalarization with multi-gradient descent algorithms, efficiently finding Pareto-stationary solutions and effectively capturing diverse human preference trade-offs. \nWe theoretically establish finite-time convergence rate guarantees for our CLAMP framework, which is independent of the number of alignment objectives. \nExperimental results further validate the effectiveness of CLAMP in aligning LLMs to heterogeneous human preferences, significantly improving previous methods.", "tldr": "", "keywords": ["LLM Alignment", "Multi-Objective Alignment", "Chebyshev-Weighted Multi-Gradient Approach"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f4d4a61d8c7e6f87cf275d0304a13d5c1915f0a5.pdf", "supplementary_material": "/attachment/b42613feba6e7bd9301be85d5d032079a0ca6be6.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces CLAMP, a multi-objective alignment framework that operates without an explicit reward model. It utilizes distinct preference datasets for various human preference dimensions to optimize a vector-valued objective function.\n\nThe framework integrates weighted Chebyshev scalarization with multi-gradient descent algorithms to find Pareto-stationary solutions. The authors also provide a theoretical guarantee for a finite-time convergence rate for the framework, which is notably independent of the number of alignment objectives. Experimental results confirm CLAMP's effectiveness in aligning LLMs to heterogeneous human preferences, showing significant improvement over existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper introduces the CLAMP framework and establishes a theoretical guarantee for solving the Multi-Objective LLM Alignment problem.\n\n2. Experimental results validate the efficiency of the proposed method."}, "weaknesses": {"value": "My main concern is the concept of Pareto Optimality and the Pareto Front. Though the consideration for Multi-Objective LLM Alignment across different preference functions is reasonable, the Pareto Optimality seems too weak. In detail, a Pareto optimal solution only requires that the solution is not dominated by others.\n\nBased on this definition, if we consider a summation reward function $f(\\theta)=f_1(\\theta)+\\dots+f_m(\\theta)$ and only try to maximize this reward function, then the optimal solution for this summation reward function will naturally be Pareto Optimal. This is because, for any other solution, this optimal solution must have a larger value for at least one objective (or agent $m$) to achieve a higher summation.\n\nIn an extreme case, when one of the the function $f_i(\\theta)$ is continuous, it usually does not have the same value for different solutions. In this case, only maximizing the function $f_i(\\theta)$ can still find a Pareto Optimal solution.\n\nOverall, the Pareto Optimality seems not to capture the fundamental intuition for the Multi-Objective LLM Alignment, and the calculation of the Pareto Optimal solutions can be easily reduced to the Single-Objective LLM Alignment problem, which highly challenges the contribution of this paper."}, "questions": {"value": "1. The current experimental results are limited to LoRA-based fine-tuning. Could the authors provide results using full fine-tuning to give a more complete performance comparison?\n\n2. The two primary multi-objective tasks (\"Helpfulness-Harmlessness\" and \"Helpfulness-Honesty-Instruction-Following\") appear similar in nature. Do the authors have any results demonstrating the divergence or degree of conflict between the objective functions in these two evaluation settings, which is essential for reflecting the difficulty of true multi-objective problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6sbQ3rzRMA", "forum": "1K9wMaRlnw", "replyto": "1K9wMaRlnw", "signatures": ["ICLR.cc/2026/Conference/Submission15181/Reviewer_5DcQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15181/Reviewer_5DcQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760828761515, "cdate": 1760828761515, "tmdate": 1762925487531, "mdate": 1762925487531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CLAMP (Chebyshev-Weighted Multi-Gradient Alignment), a new framework for multi-objective LLM alignment that does not rely on reinforcement learning or reward models. The method addresses the challenge of aligning models with multiple, potentially conflicting objectives by formulating training as a multi-objective optimization problem.\n\nCLAMP defines a Chebyshev-weighted loss, which minimizes the maximum deviation among all objectives, effectively prioritizing the worst-performing one at each step. The optimization combines this scalarization with the Multi-Gradient Descent Algorithm (MGDA) to compute a single update direction that achieves Pareto-stationary improvements across objectives. The approach includes theoretical analysis proving an (O(1/T)) convergence rate independent of the number of objectives, indicating good scalability.\n\nEmpirically, CLAMP is tested on multi-preference alignment benchmarks and compared with MORLHF and related baselines. Results show improved Pareto front coverage, alignment stability, and task trade-offs with minimal computational overhead. The paper claims CLAMP offers a theoretically principled and efficient alternative for balancing multiple alignment goals in LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses a problem in multi-objective alignment for LLMs and proposes a RL-free framework that is both theoretically motivated and empirically supported. In terms of originality, the integration of Chebyshev-weighted scalarization with multi-gradient descent (MGDA) offers an interesting combination of classical multi-objective optimization principles and modern LLM alignment methods. The formulation provides a clear geometric interpretation of balancing conflicting objectives and offers an alternative to traditional RLHF-based approaches.\n\nRegarding quality, the paper includes formal convergence analysis and claims an O(1/T) convergence rate independent of the number of objectives, suggesting theoretical soundness and scalability. The optimization strategy is simple yet mathematically grounded, and the experimental evaluation demonstrates that CLAMP can achieve balanced alignment across objectives while maintaining low computational overhead.\n\nIn terms of significance, the method contributes to a growing line of research aiming to reduce reliance on RL and reward modeling in preference alignment. The focus on Pareto-stationary updates aligns well with real-world alignment challenges where multiple preferences must coexist. Although the implementation could be clarified, the framework itself has potential for broader applicability in multi-objective fine-tuning of LLMs."}, "weaknesses": {"value": "1. While the paper introduces a theoretically motivated framework, several issues limit its clarity and empirical strength. First, the core loss formulation in Equation (3) appears mathematically equivalent to the MaxMin-RLHF objective when incorporating the preference vector p. However, the paper only compares with MORLHF and does not include MaxMin-RLHF. This omission makes it difficult to assess whether CLAMP’s improvements arise from the algorithm itself or simply from reparameterization of an existing loss.\n\n2. The proposed loss function does not have a unique solution. The Chebyshev-weighted max-min scalarization naturally admits multiple Pareto-stationary points, depending on initialization and gradient geometry. The paper does not provide any analysis or experiments on how these different solutions behave in practice. Multiple training runs could yield models representing distinct trade-offs on the Pareto front, but the paper lacks results or visualizations demonstrating this diversity or consistency. Clarifying how these solutions differ and whether they produce meaningful alignment trade-offs would substantially strengthen the empirical section."}, "questions": {"value": "1.  Comparison with MaxMin-RLHF and Computational Advantage: Equation (3) appears to define the same loss function as the MaxMin-RLHF formulation when incorporating the preference vector $p$, i.e., minimizing \n$\\min_\\theta \\max_m \\{ p_m f_m(\\theta) \\}$. \nHowever, the paper only compares CLAMP with MORLHF rather than with this closely related MaxMin-RLHF method, which shares the same scalarized objective. Could the authors clarify how CLAMP provides a meaningful improvement, either theoretically or empirically, over MaxMin-RLHF, given that both methods optimize an equivalent objective but differ in optimization dynamics?\n\n2. Could the authors include any experimental results or analysis comparing multiple runs to demonstrate whether the algorithm consistently converges to similar solutions or explores diverse regions of the Pareto front? The loss function defined in Equation (3) does not appear to yield a unique optimal solution, as multiple Pareto-stationary points can exist depending on initialization or gradient geometry. It would also be helpful to visualize or quantify the diversity of solutions obtained from different random seeds to confirm the robustness and stability of CLAMP’s optimization process."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1qiuzeC3ok", "forum": "1K9wMaRlnw", "replyto": "1K9wMaRlnw", "signatures": ["ICLR.cc/2026/Conference/Submission15181/Reviewer_y3m2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15181/Reviewer_y3m2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761660563534, "cdate": 1761660563534, "tmdate": 1762925487164, "mdate": 1762925487164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CLAMP (Chebyshev-weighted LLM alignment with multi-objective preferences), a method that integrates stochastic multi-gradient-based and Chebyshev-weighted techniques to achieve multi-objective alignment for LLMs. Experimental results demonstrate that the proposed approach improves multi-objective alignment compared to existing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is theoretically grounded.\n\n- The method is RL-free and reward model-free, and its training time remains unaffected by the number of objectives."}, "weaknesses": {"value": "- The clarity of the paper needs improvement. It took me some time to understand the methodology, and after reading, it remains unclear how to compute the multi-objective loss. For example, given a single sample (x, y_w, y_l) to optimize and three objectives, how is the loss function computed for each objective?\n\n- The novelty appears limited, as the method primarily applies previous theories to the multi-objective alignment problem.\n\n- Comparing with more recent baselines, such as those mentioned (e.g., MO-GRPO) in the related work, would enhance the overall quality of the paper.\n\n- The term \"heuristics\" could be misleading. Are all existing multi-objective alignment algorithms heuristic-based? The authors should provide a clearer explanation of this.\n\n- The proposed method is sensitive to the hyperparameter μ.\n\n- The experimental settings are not clearly stated. For instance, the test set and the reward models used should be explicitly specified, with direct references to the appendix where applicable.\n\n- The paper does not validate whether the proposed algorithm performs well with other methods, such as IPO or SimPO.\n\n- A concern is the notably poor performance of MORLHF reported in the paper. In my own experience, using multi-objective reward-weighted PPO/GRPO often yields better results than DPO. Could the weak performance be due to the reward models used? It would be beneficial if the authors could report the results of training with more advanced reward models, such as Skywork-llama3-8b-v2 on UltraFeedback.\n\n- Formatting issues: The citation format is incorrect. For example, \"Reinforcement learning from human feedback (RLHF) Christiano et al. (2017);\n\n- Typo:  Line 232: “meta-algorithm.” should be “meta-algorithm”."}, "questions": {"value": "- Can CLAMP be applied to the online RL setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Xi1kchCBTA", "forum": "1K9wMaRlnw", "replyto": "1K9wMaRlnw", "signatures": ["ICLR.cc/2026/Conference/Submission15181/Reviewer_GDJ6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15181/Reviewer_GDJ6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802799881, "cdate": 1761802799881, "tmdate": 1762925486749, "mdate": 1762925486749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}