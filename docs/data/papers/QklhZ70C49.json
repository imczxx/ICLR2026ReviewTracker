{"id": "QklhZ70C49", "number": 8779, "cdate": 1758098002947, "mdate": 1759897764474, "content": {"title": "PO-Dreamer: Memory Guided World Models for Partially Observable Reinforcement Learning", "abstract": "World models predict future states and rewards by learning compact state representations of the environment, thereby enabling efficient policy optimization. World-model-based reinforcement learning (RL) algorithms have demonstrated significant advantages in complex tasks. However the scenarios in real world application are always partially observable (i.e., image based RL and multi-agent RL), and contain non-stationary dynamics. To address the challenges in Partially Observable Markov Decision Processes (POMDP) scenarios,  we propose a novel memory guided world model named PO-Dreamer. Besides current observation, we adaptively extract meaningful cues from memory which is helpful to model the environmental dynamics. Then, the features of current observation and memory are fused by the fusion mechanism to predict state transition and future rewards. Extensive experiments on both single-agent (Atari 100K) and multi-agent (SMAC) tasks demonstrate that our method achieves state-of-the-art (SOTA) performance compared to existing strong baselines.", "tldr": "", "keywords": ["reinforcement learning", "world model"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88e81f4f6a4eaf9a77f49bfba4c9a40edf6c159c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces PO-Dreamer, an extension of the Dreamer algorithm designed to address challenges posed by partial observability. This enhancement incorporates a novel internal memory state intended to emulate the behavior of an external memory, which aggregates past observations from both the current and previous episodes. The central idea is to enable Dreamer to more effectively model non-stationary and partially observable dynamics by leveraging this memory mechanism. Empirical results on the Atari 100k benchmark demonstrate that the inclusion of the memory state boosts the performance of Dreamer-V3. Furthermore, in the multi-agent StarCraft environment, PO-Dreamer surpasses several state-of-the-art approaches."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a theoretically sound extension of Dreamer by integrating an external memory module, although I have some concerns regarding the observation selection strategy outlined in Equation 4. An innovation in PO-Dreamer is its inclusion of observations from previous episodes in the memory, which differentiates it from earlier approaches. Empirical results show that PO-Dreamer achieves strong performance on both the Atari 100k and SMAC benchmarks, surpassing competitive baselines such as DreamerV3 in the Atari domain."}, "weaknesses": {"value": "It is unclear why the proposed method works. The paper asserts that augmenting Dreamer with external memory improves its ability to handle partial observability. However, the environments used to evaluate PO-Dreamer are generally regarded as fully observable in the existing literature. Partial observability typically poses a challenge in reinforcement learning when tasks involve long-term dependencies that require the agent to retain information over time to act optimally. For example, Hausknecht & Stone (2015) introduced artificial occlusions—such as random frame dropping—to simulate partial observability in Atari games. Another well-known case is the T-maze (Esslinger et al., 2022, Fig. 3a), where the agent receives a clue at the beginning of the episode that it must remember to make the correct decision later. As far as I can tell, all the works cited in Section 2.2 deliberately evaluate their PO agents in settings where memory is essential for task completion. This does not appear to be the case for PO-Dreamer. Consequently, I find the evidence insufficient to support the paper’s first claimed contribution.\n\n> \"A memory guided world model, PO-Dreamer, is proposed to extract key temporal information to infer unobservable aspects of the environment, effectively addressing the challenges of non-stationarity in POMDPs\"\n\nI find it difficult to see how the experiments substantiate the claim that PO-Dreamer effectively extracts key temporal information to infer unobservable aspects of the environment. This concern arises primarily because the method was not evaluated on environments where temporal information is critical for solving the task—beyond simply recalling the last few frames, as is typical in Atari. I believe this also largely applies to the SMAC domains used in the evaluation.\n\nFurthermore, the mechanism used to populate the external memory (Equation 4) appears to contradict the goal of retaining important past information to address partial observability. If I understood correctly, the memory is populated with observations that are similar to the current one, based on cosine similarity between their embedding vectors. In genuinely partially observable settings, where success depends on recalling information from the distant past, retrieving observations similar to the current one is not helpful. Such observations are likely redundant, as they reflect what the agent is already seeing. What is needed instead is access to information that is currently missing but crucial for decision-making—such as the initial clue in the T-maze task.\n\nAdditionally, the decision to include observations from previous episodes seems misaligned with the goal of uncovering unobservable aspects of a POMDP. Consider again the T-maze: the clue changes in every episode. Therefore, retrieving observations from past episodes could mislead the agent, as it might attend to outdated or irrelevant clues, rather than the one pertinent to the current episode.\n\nIn summary, while the paper presents strong evidence that PO-Dreamer performs well in fully observable domains, I am not convinced that its success is attributable to improved handling of partial observability. It seems more plausible that the gains stem from increased model capacity or richer learning signals, rather than from mechanisms that effectively address partial observability."}, "questions": {"value": "1. What accounts for PO-Dreamer's superior performance over DreamerV3 in the Atari 100k benchmark? Given the concerns raised earlier regarding partial observability, it seems unlikely that the observed performance gains are primarily due to improved inference of unobserved aspects of the environment. \n\n2. What is the rationale for including observations from different episodes in the external memory? In tasks where episode-specific information is critical (e.g., the T-maze), retrieving observations from other episodes could introduce noise or confusion rather than aid decision-making.\n\n3. What is the motivation behind the observation selection strategy described in Equation 4? Specifically, why is similarity to the current observation—measured via cosine similarity between embeddings—a meaningful criterion? Would alternative strategies, such as random sampling or task-specific heuristics, perform better in partially observable settings?\n\n4. How does PO-Dreamer perform in environments that are explicitly designed to test partial observability? For example, how does it compare to prior methods on benchmarks used in the works cited in Section 2.2, where memory is essential for task success?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kI3JSfI03P", "forum": "QklhZ70C49", "replyto": "QklhZ70C49", "signatures": ["ICLR.cc/2026/Conference/Submission8779/Reviewer_w5xG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8779/Reviewer_w5xG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761166535325, "cdate": 1761166535325, "tmdate": 1762920555119, "mdate": 1762920555119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PO-Dreamer, an extension of Dreamer that incorporates a memory module to improve performance in partially observable environments. The added memory stores a subset of past observations and actions, which may come from either the current or previous episodes. During interaction, the model selectively retains a smaller subset of this memory and performs memory lookup using a Transformer, with the current observation serving as the query. The retrieved memory is then fused to augment the latent state in Dreamer. The approach is evaluated on single-agent Atari 100K and multi-agent SMAC benchmarks, showing improvements on some tasks compared to selected baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The core problem addressed in the paper, namely the memory limitations in model-based reinforcement learning, is both relevant and significant.\n- The concept of leveraging cross-episode data during inference is novel.\n- Overall, the paper is well organized and easy to follow.\n- Ablation studies are included and help clarify the contribution of different components of the method."}, "weaknesses": {"value": "- The main motivation of the paper comes from the partial observability of the environment. However, in the selected single-agent tasks, the observation function is only mildly partial with deterministic transitions, limiting evidence for the method’s effectiveness in harder scenarios.\n\n- While the experiments demonstrate some improvement over existing baselines, they do not provide clear evidence of stronger memory capabilities. To support this claim, the authors could consider long-horizon tasks that require the agent to retain information from the distant past.\n\n- No error bars or statistical significance are reported for Atari 100K, making results less reliable.\n\n\n- Table 2 suggests that increased model capacity (CO+CO) improves performance compared to Dreamer. This raises the question of how Dreamer with a larger latent state size would perform and whether the observed gains are primarily due to increased model capacity rather than the method itself.\n\n- The proposed method requires storing raw observations in memory and processing the entire memory at each timestep, which introduces additional time and memory complexity. This overhead should be analyzed and discussed in depth in the paper.\n\n- Although the authors claim to address non-stationarity in POMDPs, both the transition function and rewards are assumed to be stationary in the proposed method and in the experimental environments.\n\n- The notations are somewhat overcomplicated, and some equations require minor adjustments (e.g., indexing in Eq. 4).\n\n- Some important detail of the method are missing. Please see the questions."}, "questions": {"value": "- Why do you encode timesteps in memory embeddings? My intuition is that the improvement observed from adding timesteps to the memory embeddings might be task-specific. If that’s the case, comparing these results with baselines that don’t use timesteps could be unfair (though I’m not sure whether all of them exclude timesteps).\n\n- What is the trade-off being referred to in line 193?\n\n- Do you stop gradients when computing (4)?\n\n- What is the intuition behind using cross-attention in (8)?\n\n- How do you fill memory in training and inference?\n\n- Why is DreamerV3 missing in Figure 3?\n\n- What is the connection between the exploration–exploitation discussion in line 474 and Figure 4 (c)–(d)?"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "There are some unusual hyperref usages in the paper. For example, on page 2, the page number is linked; on page 3, the \"Under review as a conference paper at ICLR 2026\" header and an invisible rectangle on the left side of the page also appear as hyperlinks."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z74Ej4bagm", "forum": "QklhZ70C49", "replyto": "QklhZ70C49", "signatures": ["ICLR.cc/2026/Conference/Submission8779/Reviewer_Cf3s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8779/Reviewer_Cf3s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761198936408, "cdate": 1761198936408, "tmdate": 1762920554378, "mdate": 1762920554378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PO-Dreamer, which proposes an architecture to extract \"important\" observations to attempt to address the partial observability problem within the model-based RL framework.\nSpecifically, the authors propose to maintain an additional memory latent and recurrent state $(z^{mg}, h^{mg})$ to address the issue of partial observability. They propose a mechanism of *adaptive dropout* where important observations are retained by computing cosine similarity between memory embeddings and the current obs. embedding. They evaluate their performance on the ATARI 100k benchmark and on the StarCraft multi-agent challenge (SMAC) and show better performance of PO-Dreamer."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Most MBRL frameworks don't address partial-observability in an intelligent way (most of them employ recurrent networks) -- it is interesting to see a direction in MBRL where \"relevant\" observations are kept for dynamics prediction.\n\n2. It is nice to see the open-sourcing of the code base. I've gone through it briefly (haven't run it) and the structure generally makes sense."}, "weaknesses": {"value": "1. **[Experimental Setting]** I find the ATARI 100k experimental setting to be a bit underwhelming since ATARI is not necessarily the best benchmark for testing partial observability. This is because stacking frames in ATARI / using recurrent network/transformers (naively) can still be equally performant or, in some games, better, as clearly shown in Table 1. Instead, I'd like PO-Dreamer to be tested on more complex POMDP tasks. One good start could be the Flickering Atari Games from [1] -- perhaps rather than blanking out frames with p=0.5, you could blank a series of frames together with some probability so as to test the adaptive dropout mechanism.\n\nOther benchmarks, such as Crafter [2], no-inventory Crafter [3], and BabyAI environments [4], are a much better suite of tasks to test PO-Dreamer on.\n\n2. **[Analysis on the selected frames]** One of the key components of PO-Dreamer is the selection of the \"key observations\" using the Adaptive Dropout. A visual analysis of what frames are being selected is important. How far back is the memory encoding selecting (among the 800 frames)? This perhaps isn't super crucial for the ATARI benchmark as opposed to StarCraft or when you blank out ATARI frames [1]. It'd be interesting to see if it selects the non-blank frames or not.\n\n\n-----\n**References**\n\n[1] Deep Recurrent Q-Learning for Partially Observable MDPs, Matthew Hausknecht and Peter Stone, AAAI Symposia, 2015 \n\n[2] Benchmarking the Spectrum of Agent Capabilities, Danijar Hafner, ICLR 2021\n\n[3] Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains, Ruo Yu Tao et al., RL-C 2025\n\n[4] https://minigrid.farama.org/environments/babyai/index.html\n\n\n----\n**Rationale for voting**\n\nI'm currently voting for a rejection of the paper as my major concern is lack of experiments that have higher level of partial-observability. The core claim of the paper the memory-guided world model which can effectively address challenges of non-stationarity in POMDPs. I will base my final decision on authors' rebuttal and comments from other reviewers."}, "questions": {"value": "3. On Table 1, STORM is marked with an asterisk that says \"These methods are implemented with advanced network backbones such as Transformer and Diffusion for world models.\" I would like to point out that the number of transformer layers used in PO-Dreamer in the cross-attention is 3, while STORM only uses 2 layers (both have the same feature dim of 512) -- so I don't believe that STORM is a more complex model than PO-Dreamer. Please confirm if this argument is correct.\n\n\n4. It is unclear to me as to what is preventing the collapse of $\\hat{f_t}$ to a trivial solution? For instance, $\\hat{z_t}^{mg}$ can be zeros (and so can $f_t$), and effectively this reduces to DreamerV3. It would be helpful if the authors could clarify this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mkCCIhOlCu", "forum": "QklhZ70C49", "replyto": "QklhZ70C49", "signatures": ["ICLR.cc/2026/Conference/Submission8779/Reviewer_q9n2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8779/Reviewer_q9n2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895499974, "cdate": 1761895499974, "tmdate": 1762920553926, "mdate": 1762920553926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission proposes PO-Dreamer, a memory-guided world model for partially observable reinforcement learning. The key proposa is a dual-encoder architecture that separately processes current observations and historical memory, then fuses them through attention mechanisms to predict state transitions and rewards. The memory encoder uses adaptive dropout to select relevant frames from a memory window based on cosine similarity, and models memory features as a Gaussian distribution via VAE. Experiments on Atari 100K and SMAC demonstrate performance improvement and strong results on multi-agent tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The submission is generally well-written with clear figures and good organization.\n2. The experiment is comprehensive, showing improved performance across diverse Atari and multi-agent benchmark tasks and compared against a wide array of strong MBRL baselines.\n3. The proposed architecture is properly motivated to addresses partial observability by explicitly modeling memory alongside current observations and fusion mechanism using attention.\n4. Comprehensive ablation studies were perform to validate each component choice and hyperparameter sensitivity."}, "weaknesses": {"value": "1. Performance results is not reported with confidence interval, making it hard to assess whether the difference is meaningful or within the range of variance. Please include confidence interval for both main results and ablation studies.\n2. Computational cost was not analyzed: No discussion of wall-clock time, memory usage, or training efficiency compared to baselines. How is the computational overhead compared to methods like Dreamer-v3. Given τ=800 and attention mechanisms, computational overhead could be significant.\n3. There is little analysis on when memory helps. The submission claims benefits for adding memory but provides limited systematic analysis of which task characteristics benefit from memory. Can you discuss when memory provides the largest benefits? What task characteristics correlate with memory usefulness? For instance, in atari games, it seems like baseline models perform even better in many games.\n4. Limitation of the work is not discussed."}, "questions": {"value": "1. Cross-episode vs. current episode: Given \"CE Only\" performs reasonably well (1.781, vs. 1.865 for the full model), how much value does cross-episode memory actually add? Is the 800-step window typically within a single episode for most games?\n2. Is there any way to measure whether the world model learned by the proposed PO-Dreamer is better than other method say Dreamer-v3? Is this the major driver for improved policy learning? Or something else?\n3. The cosine similarity-based memory selection seems relatively simple and arbitrary, and not well justified against alternatives.\n4. Line 72 typo: \"environmentment's\" → \"environment's\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S5ES2gSrYb", "forum": "QklhZ70C49", "replyto": "QklhZ70C49", "signatures": ["ICLR.cc/2026/Conference/Submission8779/Reviewer_GV13"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8779/Reviewer_GV13"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988152973, "cdate": 1761988152973, "tmdate": 1762920553421, "mdate": 1762920553421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}