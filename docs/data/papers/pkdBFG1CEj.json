{"id": "pkdBFG1CEj", "number": 13620, "cdate": 1758219930907, "mdate": 1759897424403, "content": {"title": "Aligning Multilingual Reasoning With Verifiable Semantics From A High-Resource Expert Model", "abstract": "While reinforcement learning has advanced the reasoning abilities of Large Language Models (LLMs), these gains are largely confined to English, creating a significant performance disparity across languages. To address this, we introduce Pivot-Based Reinforcement Learning with Semantically Verifiable Rewards (PB-RLSVR), a novel framework that enhances multilingual reasoning by circumventing the need for human-annotated data in target languages. Our approach employs a high-performing English LLM as a \"pivot\" model to generate reference responses for reasoning tasks. A multilingual model is then rewarded based on the semantic equivalence of its responses to the English reference, effectively transferring the pivot model's reasoning capabilities across languages. We investigate several cross-lingual semantic reward functions, including those based on embeddings and machine translation. Extensive experiments on a suite of multilingual reasoning benchmarks show that our method significantly narrows the performance gap between English and other languages, substantially outperforming traditional PPO baselines. Specifically, our PB-RLSVR framework improves the average multilingual performance of Llama-3.1-8B-Instruct and Qwen3-32B by 16.41\\% and 10.17\\%, respectively, demonstrating a powerful and data-efficient approach to building truly multilingual reasoning agents.", "tldr": "We developed a method to improve the reasoning skills of AI models in various languages by using their strong English abilities as a guide.", "keywords": ["Multilingual", "LLMs", "Reinforce Learning", "Expert Rewards"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7ce2a1f582b9087d6d60af01c123ae05eecbf02c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Using a high-performing English model as a “pivot” to generate reference reasoning, and rewards target-language responses based on their semantic similarity to these English references."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tExploring an important issue of multilingual reasoning disparity"}, "weaknesses": {"value": "1.\tCore Concern: Line 273-The responses used for guidance were generated by Qwen3-235B-A22B. Therefore, the training stage, this process can partially be viewed as data distillation from Qwen3-235B-A22B (bigger and stronger model) to other (smaller and weaker) models. The improvements achieved in this paper cannot be fully attributed to the proposed framework.\n2.\tThe framework effectively relies on expert-generated pseudo-labels, similar to cross-model distillation, yet the paper positions it as reinforcement learning with verifiable rewards.\n3.\tSuggestion: A acceptable framework should be to leverage the model’s inherently stronger capabilities in English relative to other languages for alignment and enhance reasoning capabilities in non-English languages.\n4.\tThe paper does not quantify how translation or embedding errors affect reward accuracy and final performance.\n5.\tThe method requires generating and storing reference reasoning from a stronger model and repeated semantic comparisons. How about its computational cost and practicality."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IJPJHXKahR", "forum": "pkdBFG1CEj", "replyto": "pkdBFG1CEj", "signatures": ["ICLR.cc/2026/Conference/Submission13620/Reviewer_bSs8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13620/Reviewer_bSs8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761394276200, "cdate": 1761394276200, "tmdate": 1762924200374, "mdate": 1762924200374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PB-RLSVR, a method that uses a high-quality English \"pivot\" answer to generate a verifiable reward signal for reinforcing multilingual reasoning.\nThe model's non-English output, including its reasoning chain, is semantically compared against the English reference. This consistency score, a hybrid reward evaluating the final answer (with COMET) and the reasoning steps (with vector similarity), is then used to fine-tune the model via reinforcement learning. The approach significantly boosts performance on multilingual benchmarks like MGSM and shows strong generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method is clear and straightforward, achieving performance improvements."}, "weaknesses": {"value": "1. Novelty overlap & missing related work. The paper largely overlaps with prior multilingual alignment work and avoids direct discussion/comparison. In particular, MAPO[1] already uses an English pivot and a translation-model likelihood–based alignment preference signal optimized via PPO/DPO. A head-to-head MAPO-style baseline and analysis are needed. Beyond MAPO, there is a substantial body of multilingual reasoning/alignment research [2][3][4].\n\n2. Writing & reporting inconsistencies. Numerous terminological and numerical mismatches hurt clarity and credibility: the method name GRPO is written inconsistently (“Group Relative…”, “Group Reward…”, even “Group Regularized…”), and Qwen3-32B baseline accuracy is inconsistent (e.g., 74.3 in line369 vs 72.8 in table2).\n\n3. Heavy dependence on a strong external expert, without ablations. The approach relies on Qwen3-235B to produce English CoT “anchors,” yet the paper neither reports that expert’s multilingual performance nor provides ablations with a weaker expert or self-anchored outputs. A straightforward baseline is to use Qwen3-235B to generate multilingual SFT data and train the model on them.\n\n[1] MAPO: Advancing Multilingual Reasoning through Multilingual Alignment-as-Preference Optimization\n\n[2] Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations\n\n[3] Question Translation Training for Better Multilingual Reasoning\n\n[4] LangBridge: Multilingual Reasoning Without Multilingual Supervision"}, "questions": {"value": "No further questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "t3LSV6BHKu", "forum": "pkdBFG1CEj", "replyto": "pkdBFG1CEj", "signatures": ["ICLR.cc/2026/Conference/Submission13620/Reviewer_KSWQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13620/Reviewer_KSWQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724552741, "cdate": 1761724552741, "tmdate": 1762924199988, "mdate": 1762924199988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the persistent gap between English and non-English reasoning in LLMs by proposing Pivot-Based Reinforcement Learning with Semantically Verifiable Rewards (PB-RLSVR), which uses a strong English model to generate reference chains of thought and then rewards a multilingual policy for producing semantically equivalent answers and reasoning in other languages. The method shows consistent gains over several benchmarks across two model families."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem formulation is clear and timely: it targets the concrete multilingual reasoning drop documented on MGSM and MMLU-ProX and turns a widely available asset (strong English reasoning) into supervision for low-resource languages.\n2. The reward design is thoughtful: separating answer-level semantic fidelity (COMET) from reasoning-level semantic coherence (dual embedding signals) and showing, through ablations, that the hybrid variant outperforms single metrics adds credibility to the proposed framework and explains the reported gains over SFT/PPO."}, "weaknesses": {"value": "1. The method is tightly coupled to the quality and availability of an English pivot; robustness to imperfect or domain-mismatched English reasoning is not analyzed.\n2. The reward pipeline looks computationally heavy (MT + COMET-style scoring + embeddings), but the paper does not report overhead, failure rates, or reward sparsity, so practical applicability is unclear.\n3. Since the training data relies on MT’ed resources and filtered generations, the paper does not convincingly rule out that the gains mainly come from cleaning MT artifacts rather than learning general multilingual reasoning.\n4. The evaluation scope is narrower than the ambition: there is no human analysis of cross-lingual faithfulness, no setting without a strong English pivot, and no study of negative transfer, so the final claims read slightly stronger than the presented evidence."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8H3iq6bmzr", "forum": "pkdBFG1CEj", "replyto": "pkdBFG1CEj", "signatures": ["ICLR.cc/2026/Conference/Submission13620/Reviewer_Q6sL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13620/Reviewer_Q6sL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868629325, "cdate": 1761868629325, "tmdate": 1762924199436, "mdate": 1762924199436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the performance gap in multilingual reasoning tasks for large language models by proposing a novel framework called pivot-based semantically verifiable reward reinforcement learning. The core of this method lies in leveraging high-performance English models as “pivots” to generate reference responses, thereby eliminating the need for manually annotated data in target languages. A hybrid semantic reward function integrating answer accuracy assessment and semantic consistency evaluation is designed to provide verifiable cross-lingual reward signals for reinforcement learning training. Empirical results across multiple multilingual reasoning benchmarks demonstrate that this framework significantly narrows the performance gap between English and non-English languages."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper highlights the performance gap in current large language models for multilingual reasoning tasks, emphasizing the overreliance of existing approaches on English and their limitations in resource-poor languages.\n\n2. The method was evaluated across multiple multilingual reasoning benchmarks, including MGSM, MMLU-ProX, INCLUDE, and M-LoGiQA, covering a wide range of languages and task types.\n\n3. The paper not only evaluated performance within the training language but also specifically tested zero-shot generalization capabilities on unseen languages, demonstrating that the enhanced reasoning abilities of this method exhibit a degree of language independence.\n\n4. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. The paper primarily compares with SFT and standard PPO. To enhance the paper's persuasiveness, it is recommended to compare with some fine-tuning or alignment methods specifically designed for multilingual scenarios, such as:\n(1) Cross-lingual Thought Prompting for Zero-shot Reasoning in Low-resource Languages.\n(2) MALA: Cross-lingual Transfer with Memory-assisted Language Agnostic Meta-learning\n\n2. Reward mechanisms may inadvertently encourage models to generate more “English-like” reasoning styles and expressions, rather than preserving the natural reasoning patterns of the target language.\n\n3. The performance ceiling of the method is constrained by the selected pivot model. If the pivot model generates errors or biases on certain tasks, these errors may propagate through the reward signal and be amplified to the multilingual model under training."}, "questions": {"value": "1. Using “semantic equivalence with English reference responses” as the core reward signal may inadvertently encourage models to mimic English reasoning styles and expression habits rather than truly master language-independent reasoning abilities. How can we distinguish whether the model has learned reasoning itself or merely generates text similar to English expressions?\n\n2. The core assumption of this method is that “high-quality English reasoning” can serve as a cross-linguistic gold standard. However, for reasoning patterns or knowledge specific to certain languages or cultures, English references may not be applicable or optimal. Should performance on such language-specific reasoning tasks be evaluated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "A2BrfrLyfE", "forum": "pkdBFG1CEj", "replyto": "pkdBFG1CEj", "signatures": ["ICLR.cc/2026/Conference/Submission13620/Reviewer_hovV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13620/Reviewer_hovV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919583761, "cdate": 1761919583761, "tmdate": 1762924198778, "mdate": 1762924198778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}