{"id": "XCqrMBh1Uj", "number": 11457, "cdate": 1758199460084, "mdate": 1763719707458, "content": {"title": "REAL: REtrieval-Augmented and Logic-constructed Attention Behaviors for Robust KV Cache Compression", "abstract": "The growing input sequence length of large language models (LLMs) places increasing pressure on key-value (KV) cache storage, making efficient inference challenging. Existing retrieval-based compression methods neglect the impact of distracted, biased, and widespread attention behaviors, raising robustness concerns. To address these challenges, this paper proposes REtrieval-Augmented and Logic-constructed (REAL) KV cache compression that implements a robust, low-cost, training-free method, capturing diverse attention behaviors. REAL introduces an attention weight confusion matrix (AWCM) to categorize attention behaviors and an inference score (INFsc) that balances retrieval and logic for head-wise dynamic budget allocation with an empirical per-layer safeguard. Experiments on long-sequence QA and non-QA tasks show that REAL achieves more robust compression than state-of-the-art baselines and even surpasses FullKV in certain situations. To our knowledge, REAL is the first approach to compress KV caches by attention behavior analysis, offering a new perspective.", "tldr": "REAL is an attention behavior-based approach with minimal model modification and without compromising accuracy, as in fullKV.", "keywords": ["Large language model", "Key-value", "Attention behavior"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0551deed4704aca4cf2d51e98a97e3163cf33cc9.pdf", "supplementary_material": "/attachment/0aab97289e8777af4cf9f1c1ddfdf60bf284a78a.zip"}, "replies": [{"content": {"summary": {"value": "This paper mainly addresses the issues of KV cache efficiency and inference performance in long-text scenarios, and proposes a dynamic cache allocation method based on matrix computation metrics."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written, with well-defined problems and a well-explained methodology. The addressed problem is practically valuable, and the main experiments are relatively sufficient."}, "weaknesses": {"value": "1.\tThe method relies on a large number of hyperparameters, and its generalizability has not been sufficiently validated. For example, if a different, more specialized dataset is used, it is unclear whether the current hyperparameters would still perform well, or if they would need to be re-tuned. There seem to be quite a few hyperparameters in total.\n2.\tThe ablation studies and comparisons are not entirely comprehensive. For instance, it would be helpful to compare against some simple alternative sorting schemes to demonstrate the effectiveness of the proposed matrix-based metrics. It is possible that even simple sorting strategies could achieve similar results."}, "questions": {"value": "See weekness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oGByrPE2ti", "forum": "XCqrMBh1Uj", "replyto": "XCqrMBh1Uj", "signatures": ["ICLR.cc/2026/Conference/Submission11457/Reviewer_BuNp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11457/Reviewer_BuNp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850099924, "cdate": 1761850099924, "tmdate": 1762922568461, "mdate": 1762922568461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training-free KV cache compression method REAL, which is designed by analysing the different attention behaviours. The experiments on various models and tasks show the method can compress the KV cache without decreasing performance too much."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Designing the KV cache compression method based on the different attention behaviours is intuitive. The method is training-free and thus could be applied to large models without heavy fine-tuning."}, "weaknesses": {"value": "It is unclear why the four attention behaviours (retrieval-augmented, distracted, biased, and widespread) are considered. Is there any other behaviour? Some implementation details of these behaviours are spread in the introduction, but the formal definitions are missing.\n\nThe ambiguous term \"robustness\" is heavily used in the paper. It is unclear in which aspects the proposed method improves the performance. It would be better to include a case study to explain.\n\nDo you use the same compression ratio for comparing different methods in Table 2? It is also important to present how the performance changes with the compression ratio. Please refer to the experimental setting in: https://github.com/NVIDIA/kvpress\n\nIn Figure 10, it doesn't make sense that every method has the same decoding time (except FullKV), because different methods have different strategies for calculating importance; at least, the first token latency is different. \n\nThe definition of the budget $B$ is not clear. Is it the number of KV pairs? What is the definition of $b_{base}$, and what is \"predefined ratio $\\beta$\"?"}, "questions": {"value": "What does \"dynamic eviction is constrained by model dimensionality\" mean?\n\nThe explanation after eq1: $Q \\cdot K_{\\text{Retrieval-Augmented}}$ and other $K$ are amplified by exp. But the softmax will further decrease the small value in a vector. What information do you want to convey?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1pABqsFiJF", "forum": "XCqrMBh1Uj", "replyto": "XCqrMBh1Uj", "signatures": ["ICLR.cc/2026/Conference/Submission11457/Reviewer_a1fg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11457/Reviewer_a1fg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968121005, "cdate": 1761968121005, "tmdate": 1762922568072, "mdate": 1762922568072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a KV cache compression method that constructs a confusion matrix by classifying tokens based on two criteria: whether their attention region lies within or outside the “needle” part, and whether their attention rank belongs to the Top-k or non-Top-k group. The method then computes the harmonic mean of the ratios derived from this confusion matrix and uses the resulting value as an importance metric for KV cache pruning. Experimental results demonstrate that the proposed approach achieves superior performance compared to SnapKV and PyramidKV across a diverse set of benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a novel KV cache importance metric and demonstrates superior performance compared to existing baselines."}, "weaknesses": {"value": "- The main concern lies in the clarity of writing and presentation. Specifically, the description of how the needles are generated and inserted is difficult to follow. Are the authors synthetically inserting needles to determine which attention heads to retain? If so, how is this synthetic needle data constructed?\n- Additionally, is the resulting head-wise compression static (i.e., the same pattern applied across all evaluation data) or dynamic (where compression is performed separately for each evaluation instance)? If it is the latter, this could introduce significant computational overhead, as it needs to compute attention patterns for multiple different positions.\n- It is also unclear how many calibration samples (synthetic needles) were used and what process was followed to construct them.\n- Are token-level compression methods such as SnapKV implemented dynamically, with patterns varying across evaluation samples?\n- If the proposed method instead performs static head-level compression, a direct comparison with DuoAttention [1] would be more appropriate and needed.\n\n[1] Xiao, Guangxuan, et al. \"Duoattention: Efficient long-context llm inference with retrieval and streaming heads.\" ICLR 2025."}, "questions": {"value": "See the above weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I do not have ethics concern regarding this paper."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cuGXp6TL5d", "forum": "XCqrMBh1Uj", "replyto": "XCqrMBh1Uj", "signatures": ["ICLR.cc/2026/Conference/Submission11457/Reviewer_cDkG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11457/Reviewer_cDkG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998561303, "cdate": 1761998561303, "tmdate": 1762922567640, "mdate": 1762922567640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overview of Revision"}, "comment": {"value": "We sincerely thank the reviewers for their thorough reading and valuable feedback. We highlight the important work during the rebuttal from the following two aspects.\n\n1. **Research position of REAL in kv cache compression direction:**\n\nConsidering the concept “Signal-to-Noise Ratio” in information theory and LLM also belongs to information propagation, **REAL** **proposes the metric $INFsc$ for kv budget allocation** to enhance computational efficiency in long-context scenarios. Based on common retrieval-head criteria, REAL designs needles inspired by fast-slow thinking, where previous works focus more on the retrieval head that corresponds to fast thinking, but lack identifying active bias and passive distraction to correspond to slow thinking. REAL then builds AWCM and proposes $INFsc$ for each head, making the KV cache compression more outperformed and robust in different tasks. REAL's first exploration of attention behavior can offer a new perspective in LLM's LLM-related research, like hallucination.\n\n2. **We updated our manuscript for** \n\n- hand-designed needles to clarify four attention behaviors with different colors  (Appendix F).\n\n- mathematical formulas and pseudo-code for calculating the Attention Weight Confusion Matrix (AWCM) (Appendix G).\n- additional baselines for head importance: Random_Head, Max_Attention_Score_Head, and Mean_Attention_Score_Head  (Appendix H).\n\n- Performance comparison between REAL and DuoAttention  (Appendix I).\n- Definition of four attention behaviors (Appendix J).\n\n- We revised the description on line 121.\n\n to show the superiority of REAL. These additions have indeed made our paper more comprehensive and complete.\n\nWe hope these updates and clarifications address your concerns and highlight REAL's contributions and strengths. Please let us know if you have further questions or require additional information, as we are committed to providing any needed clarifications. Thank you againnfor your valuable feedback and thoughtful review!"}}, "id": "6N7vfnWSFW", "forum": "XCqrMBh1Uj", "replyto": "XCqrMBh1Uj", "signatures": ["ICLR.cc/2026/Conference/Submission11457/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11457/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11457/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763714246398, "cdate": 1763714246398, "tmdate": 1763714246398, "mdate": 1763714246398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose REAL, a method that uses an attention weight confusion matrix (AWCM) and an inference score (INFsc) to balance retrieval, distraction, and bias signals when allocating KV cache budgets on a per-head and per-level basis. Authors use synthetic needles-in-a-haystack (NIAH) to profile heads, and REAL dynamically redistributes the KV cache capacity across different attention types. Results show improvements on long-context benchmarks (LongBench, LongBench v2, LooGLE) over strong baselines like PyramidKV and SnapKV."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The differentiation among different types of attention for allocating KV cache budgets seems novel, interesting, and potentially cognitively justified (e.g. see the work on System 1/2 reasoning), and AWCM/INFsc seems a sound way to realise this\n- Significant (although it would be nice to have statistical significance tests) improvements in QA and non-QA tasks in terms of downstream accuracy vs latency/memory trade-offs in comparison with very competitive baselines"}, "weaknesses": {"value": "- Computing the AWCM seems computationally heavy -- how does that impact the applicability of the method?\n- Not sure results are statistically significant (e.g. in the case of the comparison with tuned HeadKV variants)\n- How robust are results to the choice of e.g. $\\beta$ ?"}, "questions": {"value": "Please see my \"weaknesses\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G99gJO5IjB", "forum": "XCqrMBh1Uj", "replyto": "XCqrMBh1Uj", "signatures": ["ICLR.cc/2026/Conference/Submission11457/Reviewer_sxyK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11457/Reviewer_sxyK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762247330096, "cdate": 1762247330096, "tmdate": 1762922567175, "mdate": 1762922567175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}