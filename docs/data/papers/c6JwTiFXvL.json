{"id": "c6JwTiFXvL", "number": 22190, "cdate": 1758327456816, "mdate": 1759896881324, "content": {"title": "FlashHead: Efficient Drop-In Replacement for the Classification Head in Language Model Inference", "abstract": "Language models are increasingly adopting smaller architectures optimized for consumer devices. In this setting, inference efficiency is the primary constraint. Meanwhile, vocabulary sizes continue to grow rapidly, making the classification head a critical bottleneck that accounts for up to 60\\% of model parameters, and 50\\% of inference compute. We introduce FlashHead, the first efficient drop-in replacement for the dense classification head that is training-free and hardware-friendly. FlashHead builds on principles from information retrieval, where the output head is viewing the output head as a retrieval problem rather than a dense computation. FlashHead introduces four key innovations: (1) equal-sized clustering of embeddings, (2) multi-probe retrieval to model heads, (3) a novel inference-time sampling mechanism, and (4) selective quantization, enabling effective low-bit computation in the head. Experiments on Llama-3.2, Gemma-3, and Qwen-3 show that FlashHead delivers model-level inference speedups of up to 1.75x while maintaining output accuracy compared to the original head. By overcoming the classification head bottleneck, FlashHead establishes a new benchmark for efficient inference and removes a key barrier to developing smaller, capable models for consumer hardware.", "tldr": "We introduce FlashHead, an inference-efficient replacement for the dense classification head that is both training-free and hardware-friendly.", "keywords": ["Classification", "Efficient Inference Methods", "Embedding Approaches", "Hardware and Systems", "Natural Language Processing", "Representation Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/84f44b083522d6672f340d5b1da323ae661f9e1a.pdf", "supplementary_material": "/attachment/2001c0f510a24422b6733b4f0a1bb273fec95b07.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces FlashHead, a training-free, two-stage replacement for the dense classification head in LLMs. The method aims to reduce head-level computation and improve inference latency without re-training. Experiments on several open LLMs (Llama 3.2, Gemma 3, Qwen 3) claim up to 1.7× speed-up with comparable perplexity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method effectively improves the efficiency of the LM head computation, which constitutes a significant portion of inference cost in small and medium-sized language models (SLMs), without any additional training cost.\n\nBy enforcing equal-sized clustering within the LM head, the approach achieves a hardware-friendly design that balances GPU workloads and minimizes accuracy degradation.\n\nThe proposed procedure — spherical k-means combined with multi-probe retrieval — is conceptually simple, easy to implement, and requires only a few hyperparameters."}, "weaknesses": {"value": "The experiments are somewhat fragmented across different axes, making it difficult to assess the overall advantage of FlashHead in terms of cost, performance, and latency. A unified comparison table against retraining-based methods would strengthen the paper’s quantitative clarity.\n\nIt is unclear whether the comparisons were made under equivalent experimental conditions. Since hyperparameters such as clustering size and the number of probes can significantly affect performance, the paper should verify that the comparison targets were fairly and optimally tuned.\n\nThe choice of benchmarks and models varies across tables and remains limited in scope. Broader and more consistent evaluations, possibly in the appendix, would enhance the reliability of the results.\n\nThe benefit of using spherical k-means is not isolated through ablation, and the equal-sized clustering analysis is minimal. More comprehensive experiments across diverse models and tasks are needed, along with clearer explanations of which hardware aspects and accuracy factors benefit from this constraint.\n\nA direct comparison between fully INT4-quantized models (including the LM head) and those using FlashHead with INT4 transformer layers is missing. Reporting the quantization bit configuration of each stage (coarse vs. fine) and latency under optimized runtime environments would help readers more accurately gauge the improvement.\n\nRelated research on LM head efficiency—such as VQ-Logit (arXiv:2505.10202) and other compression or retrieval-based approaches—should be discussed for a more balanced perspective.\n\nOverall, the paper would benefit from a more systematic and fair evaluation across model scales, benchmarks, and metrics (latency, performance, cost), which would significantly improve its completeness and credibility."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3i68RCoaN7", "forum": "c6JwTiFXvL", "replyto": "c6JwTiFXvL", "signatures": ["ICLR.cc/2026/Conference/Submission22190/Reviewer_KxkH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22190/Reviewer_KxkH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837049560, "cdate": 1761837049560, "tmdate": 1762942109142, "mdate": 1762942109142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FlashHead, which replaces the dense classification head in small language models (SLMs) with a clustering-based retrieval mechanism, in order to alleviate the bottleneck in SLM inference. FlashHead combines equal-sized clustering, multi-probing, probabilistic sampling, and selective quantization. Experimental results demonstrate that FlashHead achieves high speedup while maintaining accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(S1) Timely and important topic. While most papers focus on the efficiency of large language models (LLMs), improving the efficiency of SLMs can help democratize the use of powerful AI models.  \n\n(S2) Sound empirical results. The results show that FlashHead gives up to 1.75x of speedup, while the impact on model accuracy is minor."}, "weaknesses": {"value": "(W1) Limited technical contributions\n\nThe 4 key innovations are built upon existing literature and the paper does not sufficiently clarify the insights that they could introduce.\n- Equal-sized Clustering: As mentioned in this paper, using K-means to make the vocab compact has been proposed in existing literature, and this work only makes a difference that it requires all clusters to be equal-sized. For me, it is primarily an implementation-level optimization. The paper presents it as a key innovation, but its contribution appears incremental. \n- Efficient Multi-Probing: Why we retrieve one cluster, then by default all tokens in this cluster are probed, so it is already multi-probing. The paper does not discuss the necessity to probe multiple clusters.\n- Probabilistic Probe Sampling: I can’t find any specific description of the “multinomial sampling step”. It is unclear how it is performed efficiently or how it differs meaningfully from the standard top-k sampling or hierarchical softmax.\n- Selective quantization: Applying quantization to a sub-module of a model is quite a standard practice and it is somehow orthogonal to this work. Additionally, it is described within a short paragraph, therefore I can hardly tell it is a core contribution.\n\n(W2) Insufficient experiments\n\nThe experiment section does not provide sufficient justification on several claims. Below I list a few.\n- It is reasonable that using equal cluster size gives better efficiency. However, it is unclear why it improves accuracy (Table 6).\n- Increasing the number of probes decreases efficiency. However, its impact on the accuracy is not directly assessed (Table 7 only provides top-1 containment, not benchmark scores). Besides, there is no comparison against single-probing.\n- There are no experiments comparing the decoding methods (greedy and sampling).\n\n(W3) The paper is not well-written\n\nThe paper is not well-written. To begin with, the introduction section criticizes previous works without any analysis or empirical results. The single-sentence statement “these methods\ninherit the same drawbacks noted above” gives us nothing. The methodology section fails to pinpoint the key limitations of prior arts nor the essential differences made in this work. Worse still, many technical details are not clarified, including the multinomial sampling step, the step-by-step deduction of the two-step retrieval process, and the quantization process.\n\nMinor issues: \n- In Algorithm 1, $x$ is not defined in $z^\\prime \\in \\mathbb{R}^x$.\n- In the caption of Figure 1, there are two full stops."}, "questions": {"value": "All the weaknesses above should be addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t95nKiRw7b", "forum": "c6JwTiFXvL", "replyto": "c6JwTiFXvL", "signatures": ["ICLR.cc/2026/Conference/Submission22190/Reviewer_d65s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22190/Reviewer_d65s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925424245, "cdate": 1761925424245, "tmdate": 1762942108862, "mdate": 1762942108862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "FlashHead addresses the computational bottleneck of classification heads in small language models (SLMs) by treating final layer logit prediction as a retrieval problem. The method uses equal-sized spherical k-means clustering, multi-probe retrieval, probabilistic sampling, and selective quantization to achieve up to 1.75× model-level speedups while maintaining accuracy on important tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Clear algorithmic description and implementation details. Hyperparameters are fully specified and the two stage process is well explained with neat figures.\n- The paper includes diverse benchmarks with different model families along with latency measurements showing practical speedups.\n- A fast training-free approach with near perfect top-1 containment.\n- Equal sized clustering helps efficient memory access by avoiding ragged tensor operations."}, "weaknesses": {"value": "**Literature Survey is Generally lacking**\n- Sample relevant works not discussed.\n  - HALOS: Hashing Large Output Space for Cheap Inference ([paper](https://proceedings.mlsys.org/paper_files/paper/2022/file/b059dd6da6b9a86180fbc32a799766cc-Paper.pdf))\n  - HiRE: High Recall Approximate Top-k Estimation for Efficient LLM Inference ([paper](https://arxiv.org/abs/2402.09360))\n  - VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits ([paper](https://arxiv.org/abs/2505.10202))\n  - Fast Vocabulary Projection Method via Clustering for Multilingual Machine Translation on GPU ([paper](https://arxiv.org/abs/2208.06874))\n- Line 64 “..first LLM head that clusters token embeddings into strictly equal sized spherical clusters..”. I am not sure this is true. KMeans on the logit projection matrix is a well known method for efficient inference.\n- Without proper positioning against recent work, it's unclear whether FlashHead represents a genuine advance or an incremental engineering improvement.\n\n  \n**Evaluations not presented properly**\n-   Results are normalized to /100 which makes the comparison difficult and hides actual variance/uncertainty. Authors should report mean +- std across different runs.\n-   The main results are split across 5+ tables making comparison difficult. Provide a consolidated table similar to table 9.\n\t- Table 1: Benchmark accuracy (no latency)\n\t- Table 2: Top-k containment (no latency)\n\t- Table 3: Latency only (no accuracy)\n\t- Table 4: One benchmark + latency across models\n\t- Table 7: Accuracy-latency tradeoffs for hyperparameters\n-   Why does table 5 only show one evaluation and head latency?\n    \n\n**Memory requirements not discussed**\n\n-   The paper emphasizes edge deployment on consumer devices where memory is constrained, yet provides no memory profiling.\n-   In the two stage process, Stage-1 computes centroid logits, stage-2 gathers token embeddings and computes token logits, can you present the peak activation memory?\n-   Table 8(c,) provides memory usage of parameters but it does not always equal the runtime memory requirement."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HoaqCv8iby", "forum": "c6JwTiFXvL", "replyto": "c6JwTiFXvL", "signatures": ["ICLR.cc/2026/Conference/Submission22190/Reviewer_MNME"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22190/Reviewer_MNME"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950159146, "cdate": 1761950159146, "tmdate": 1762942108591, "mdate": 1762942108591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FlashHead, a training-free, hardware-friendly replacement for the dense LLM classification head. Instead of computing full logits $z = Eh$ with $E\\in\\mathbb{R}^{v\\times d}$ and $h\\in\\mathbb{R}^{d}$, FlashHead clusters token embeddings into $c$ equal-sized spherical K-means clusters, then performs two-stage retrieval: (1) score centroids $Ch$ and select or sample $p$ probes; (2) gather the corresponding token embeddings $\\widetilde{E}$ and compute logits $\\widetilde{E}h$. Complexity drops from $O(vd)$ to $O(cd + pbd)$ with $b = v/c$. Equal-sized clustering enables a dense cluster-to-token map for fast gathers; a probabilistic probe-sampling step unifies retrieval and decoding; and the centroid stage can be safely quantized (e.g., int4) because the final token probabilities are recomputed at higher precision. Experiments across Llama-3.2, Gemma-3, and Qwen-3 show up to $1.75\\times$ end-to-end speedups with near-baseline accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality. Equal-size spherical clustering for the head plus aggressive multi-probe and probabilistic sampling is novel and tailored to accelerators.\n\n2. Strong evidence: near-perfect top-$k$ containment and consistent model-level speedups; int4 stage-1 retains accuracy.\n\n3. Method and complexity are explicit; pseudocode and ablations make design choices convincing.\n\n4. Significance. Reduces a major bottleneck for SLMs; drop-in and training-free lowers adoption barriers."}, "weaknesses": {"value": "1. Likelihoods & evaluation. No closed-form full-vocabulary distribution; relies on Monte-Carlo for likelihood metrics.\n\n2. Equal-size constraint. Requires $c\\mid v$; effect on semantic purity of clusters vs. unequal sizes could be further theorized.\n\n3. Deployment knobs. Sensitivity of $(c,p)$ and memory overheads (centroids, C2T map) under tight device budgets could be quantified more deeply."}, "questions": {"value": "1. How does accuracy/latency change as $c$ grows beyond the default with fixed memory limits?\n\n2. What is the exact storage overhead for $C$ and dense C2T across vocab sizes?\n\n3. Any observed interactions with beam search, speculative decoding, or KV-cache reuse?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TAa68RVkgj", "forum": "c6JwTiFXvL", "replyto": "c6JwTiFXvL", "signatures": ["ICLR.cc/2026/Conference/Submission22190/Reviewer_Ye6p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22190/Reviewer_Ye6p"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972456078, "cdate": 1761972456078, "tmdate": 1762942108351, "mdate": 1762942108351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}