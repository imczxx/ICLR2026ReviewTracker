{"id": "5qfX9zJDi2", "number": 6018, "cdate": 1757950704640, "mdate": 1759897939414, "content": {"title": "RRVF: Visual Reinforcement Learning with Reasoning, Rendering, and Visual Feedback", "abstract": "Multimodal Large Language Models (MLLMs) exhibit impressive performance across various visual tasks. Subsequent investigations into enhancing their visual reasoning abilities have significantly expanded their performance envelope. However, a critical bottleneck in the advancement of MLLMs toward deep visual reasoning is their heavy reliance on curated image-text supervision. To solve this problem, we introduce a novel framework, “Reasoning-Rendering-Visual-Feedback” (RRVF), that enables MLLMs to learn complex visual reasoning from only raw images. This framework builds on the “Asymmetry of Verification” principle, i.e., verifying the rendered output against the source image is substantially easier than performing deep visual reasoning to generate a faithful, structured representation such as code. We demonstrate that this relative ease provides an ideal reward signal for optimization via Reinforcement Learning (RL), thereby reducing reliance on image-text supervision. RRVF implements a closed-loop iterative process encompassing reasoning, rendering, and visual feedback components, enabling the model to perform complex reasoning, including self-correction through multi-turn interactions. This process is optimized end-to-end using the GRPO algorithm. Extensive evaluations are conducted on image-to-code generation across two diverse domains: data charts and web interfaces. The RRVF-trained model not only outperforms existing similarly sized open-source MLLMs and supervised fine-tuning baselines but also exhibits superior generalization. Notably, the model outperforms the more advanced MLLM used to generate visual feedback during training.", "tldr": "", "keywords": ["MLLM", "Visual Reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3f5787470213edbfa2793e8259e9915899f07905.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the image-to-code problem via \"Reasoning-Rendering-Visual-Feedback\". This framework does not require manually labeled text instructions. It simply renders the generated code and uses an MLLM to verify the original image and the rendered image. Experiments under various protocols demonstrate that the proposed method brings significant improvements over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is overall well-written and easy to follow.\n2. Both the motivation and the solution are quite clear and reasonable.\n3. Improvements are quite significant."}, "weaknesses": {"value": "I only have one minor concern:\n\n1. The generalization of this method. The trained model is excellent at code generation. How about other reasoning-related benchmarks, e.g, mathvista, mathverse, logicvisa, etc. Do these advanced code generation capabilities implicitly contribute to better general reasoning capabilities?"}, "questions": {"value": "N/A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rAmLMjkBvu", "forum": "5qfX9zJDi2", "replyto": "5qfX9zJDi2", "signatures": ["ICLR.cc/2026/Conference/Submission6018/Reviewer_Ztqe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6018/Reviewer_Ztqe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828241146, "cdate": 1761828241146, "tmdate": 1762918417204, "mdate": 1762918417204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reinforcement learning framework named \"Reasoning-Rendering-Visual-Feedback\" that only utilizes image data rather than image-text supervision. The framework only utilizes outcome reward by the GRPO algorithm. The training framework enables models to have multi-turn reasoning and tool-call abilities. The experiments show that the framework brings more stable improvement than SFT on Qwen2.5-VL-7B."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper designs the understanding of icon-type images as a rendering and verifying process, which is an interesting and reasonable idea.\n\n2. The paper designs the training of multimodal reasoning as a multi-round rendering and verifying process, so that the model only needs to rely on images and does not require additional text annotations. This alleviates the dependence of multimodal reasoning training on text annotations.\n\n3. The authors' experiments show that the proposed framework brings more significant advantages than supervised training (SFT)."}, "weaknesses": {"value": "1. The proposed framework requires multiple rounds of inference and tool calls during both training and inference. How long does training take? How much does inference time increase compared to not using tools?\n\n2. The applicability of this framework in visual inference is limited. It can only be used in chart-to-code and web-to-code scenarios. The authors did not explore how this framework adapts to general image inference.\n\n3. In Tables 1 and 2, did the authors only conduct in-the-domain experiments on the ChartMimic and Plot2Code benchmarks? I think generalization experiments should be added to demonstrate that reinforcement learning algorithms like GRPO, in addition to performance improvements, also enhance the model's generalization ability for SFT.\n\n4. The authors only conducted SFT and GRPO experiments on Qwen2.5-VL-7B-Instruct, without experimenting with other baseline models, failing to demonstrate the framework's generality.\n\n5. The authors did not conduct ablation experiments on the proposed framework. For example, using different maximum rounds, removing certain proposed components, etc., can validate the effectiveness of the entire framework."}, "questions": {"value": "1. Refer to the issues raised in the weakness section.\n2. How many steps were trained for the reinforcement learning fine-tuning? The curve shown in Figure 3 only shows 100 training steps, which seems to indicate the possibility of overfitting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QfPxp3c0Sm", "forum": "5qfX9zJDi2", "replyto": "5qfX9zJDi2", "signatures": ["ICLR.cc/2026/Conference/Submission6018/Reviewer_WLZB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6018/Reviewer_WLZB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996999016, "cdate": 1761996999016, "tmdate": 1762918416837, "mdate": 1762918416837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel paradigm for multimodal reasoning training by eliminating the need for paired image-text/code supervision. Instead, the model learns solely from raw images via iterative rendering and visual comparison. This formulation shifts the paradigm from supervised imitation learning to self-supervised verification-driven RL. The authors introduce a closed-loop reasoning framework that iteratively generates code, executes it, and incorporates visual feedback for refinement. This mechanism allows the model to progressively correct its outputs based on rendered results rather than relying solely on single-pass generation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n\n2. The research direction is impactful because collecting high-quality program annotations for visual tasks is expensive and often subjective.\n\n3. The model achieves a high code execution rate on ChartMimic and performs competitively on WebSight, without requiring paired text supervision. It also shows better performance than supervised fine-tuning and comparable open-source baselines."}, "weaknesses": {"value": "1. The framework depends on multi-round generation and external tool execution during training and inference. This raises questions regarding computational cost, latency, and scalability. The paper does not report training time, tool-call frequency, or inference speed compared to standard single-pass models.\n\n2. The method is only evaluated on chart-to-code and web-to-code settings, which are structured scenarios with clearly defined rendering engines. It remains unclear whether the approach can extend to broader visual inference tasks (e.g., general scene understanding, reasoning, VQA).\n\n3. The method exhibits a strong reliance on reward shaping and prompt design. The format reward, tool-use reward, and their weighting require manual tuning, yet the paper does not provide systematic analysis of reward stability or robustness. Additionally, no ablation or sensitivity study is conducted on the reward components, making it difficult to assess how much each part contributes to the final performance."}, "questions": {"value": "1. Training cost & efficiency: How long does training take, and how many RL steps are used? What is the total computational budget?\n\n2. Inference overhead: How much slower is inference when using the iterative tool-based framework? Is there a version that can operate efficiently without tool calls at test time?\n\n3. Generalization evaluation: Did you conduct any experiments on tasks outside chart/web code generation? Could you add out-of-domain tests to verify generalization benefits of GRPO over SFT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gr33Ymr575", "forum": "5qfX9zJDi2", "replyto": "5qfX9zJDi2", "signatures": ["ICLR.cc/2026/Conference/Submission6018/Reviewer_9ChV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6018/Reviewer_9ChV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762401713905, "cdate": 1762401713905, "tmdate": 1762918416517, "mdate": 1762918416517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}