{"id": "rG3rXSdzl1", "number": 1370, "cdate": 1756876332670, "mdate": 1759898212287, "content": {"title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification", "abstract": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR).\nThis advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations.\nTherefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. \nTo empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs.\nSpecifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round.\nExtensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3.\nFurthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.", "tldr": "We propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation.", "keywords": ["large languge model", "chain of thought", "process verification", "reinforcement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/243fd2332475e735ef9a7d2c88046939851ee58a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an OPV for long-form reasoning. Instead of scoring raw, lengthy CoT traces directly, the method first compresses them into concise, linear “summaries” and then predicts whether each summarized step is correct, aiming to localize the first erroneous step and provide a short explanation. Training combines RFT-style supervision with an active learning loop that queries experts for the most uncertain cases and further improves the verifier. The authors position this as a practical route to reliable process supervision, claiming it scales better than prior PVs that operate on full CoTs and can be used for dataset cleaning and improved test-time selection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Motivation & intuition for process supervision:** The paper tackles the central challenge of reliable process supervision. Using summaries as a proxy for long CoTs is intuitively appealing: it can reduce noise, strip incidental verbosity, and make step-level correctness more predictable.\n- **Practical training framework:** The active learning setup is a sensible pairing with the proposed verifier—both for label efficiency and for turning process supervision into a usable training pipeline rather than a one-off annotation exercise."}, "weaknesses": {"value": "- **Missing PV baselines:** The evaluation omits a set of established process reward models (PVs) as baselines. Including them would allow clearer comparisons and help assess whether the gains come from summarization, the active learning loop, or other factors. It is unclear what prevented their inclusion.\n- **Uncertainty measurement in active learning:** The uncertainty measure appears to mix aleatoric and epistemic components, which can misguide acquisition. I assume active learning aims to learn from the samples with high epistemic uncertainty?"}, "questions": {"value": "- **On baselines (W1):** Which prior PVs were considered and why were they excluded? Could the authors report results for representative PVs under a matched training budget?\n- **On uncertainty (W2):** How, if at all, do the authors separate or control for aleatoric vs. epistemic uncertainty in their selection criterion? Have they tested alternative acquisition functions (e.g., variance based approach)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5Ki1nPQw35", "forum": "rG3rXSdzl1", "replyto": "rG3rXSdzl1", "signatures": ["ICLR.cc/2026/Conference/Submission1370/Reviewer_CTy4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1370/Reviewer_CTy4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665409851, "cdate": 1761665409851, "tmdate": 1762915749830, "mdate": 1762915749830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an outcome based process verifier (OPV), aiming to bridge in between the process reward model and the outcome reward model. The framework uses iterative data refinement: human experts improve process annotations each round, and the model is retrained by using both human-labeled and model-generated data. Through this loop, OPV enhances both model performance and annotation quality over time."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Existing PRM approaches use Monte Carlo rollouts which are noisy. Different from previous automated rollouts, this paper utilizes humans in the loop to improve the data and model quality iteratively.\n\n+ Curate and iterate to create the OPV-Bench dataset, which could be valuable for process verification methods."}, "weaknesses": {"value": "- Missing model comparison. This work is positioned in-between the ORM and PRM, but does not provide direct comparisons with existing methods (e.g., Qwen PRM or other ORM baselines). For the benchmark it would be important to understand 1. How do the other models compare on the process bench 2? How do the other models compare with the OPV-BENCH? \n\n- Missing Dataset construction details. Authors only mentioned problem curation as K-12 education, high-school competitions, and undergraduate-level mathematics in A.1. There should be more specific sources on the problem curation. \n\n- Since OPV is trained on the dataset with similar distribution as OPV-Bench, performance gains may reflect dataset alignment. The model needs to be evaluated on more datasets other than Process Bench, such as full GSM8K, MATH, Minerva, or MMLU, etc. In addition,  Process Bench should have subcategory results of the GSM8K, MATH, Olympiad, and Omni-MATH. \n\n- The pipeline is rather complex. It requires multiple stages for training, and for each training it has expert iteration then online RL. What would the total training cost with this approach? It is important to compare this with Monte Carlo rollout methods. Also it would be important to understand the human annotation cost."}, "questions": {"value": "(1) L190-209: how does OPV handle cases where the model exhibits high confidence yet produces consistent but incorrect reasoning, which is a common issue in RL-trained models?\n\n(2) Best-of-N ranking (Section 4.2, L402): Could the authors clarify how ranking is determined? Is OPV used directly to score and rank the outputs through the index of the first incorrect answer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8YPPhn9Inn", "forum": "rG3rXSdzl1", "replyto": "rG3rXSdzl1", "signatures": ["ICLR.cc/2026/Conference/Submission1370/Reviewer_k3ry"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1370/Reviewer_k3ry"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758958656, "cdate": 1761758958656, "tmdate": 1762915749607, "mdate": 1762915749607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work tackles the problem of verifying thoughts (CoTs) generated by models along with the final answers. Given that a large portion of the difficulty in verifying long CoTs comes from the verbosity and noisiness of CoTs, the authors propose to summarize CoTs by keeping only the critical steps that are relevant to the final answer and introduce outcome-based process verifiers (OPV). As this can effectively reduce the size of the inputs to the verification problem, they make use of a human-in-the-loop framework to gather human annotations in stages to improve their OPV models. By demonstrating the performance of the OPV models on ProcessBench and the newly introduced OPV-Bench, they suggest that the proposed approach to summarizing CoTs may make utilizing human annotations for improving the verification performance more viable."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Reasonable methodological approach  \nAs the complexity of problems increases, the difficulty of verifying thought processes (CoTs) rises, as well. Therefore, linearization and simplification of long CoTs can make the verification problem much smaller and easier, and the proposed summarization approach sounds like a fair approach in this line.\n\n2. Human-in-the-loop pipeline for practical applicability  \nTaking advantage of the simplified verification process thanks to the summarization, the authors propose an iterative framework to update and improve the verifier model with human interventions. This may encourage practical adoption of the proposed approach by demonstrating how cost-effective human annotations can help improve the verifier.\n\n3. Comprehensive experiments and analyses  \nThis work presents a comprehensive set of experimental results. It covers a fair number of baseline verifiers as well as evaluations on one existing and one in-house benchmarks. Additionally, it provides further analyses and statistics, which can give insights from different angles."}, "weaknesses": {"value": "1. Dependency of OPV on summarization accuracy  \nViewing the originally generated CoTs (+ the final answers) as inputs, this work ultimately proposes to factor a process-based verifier into two components: (a) the summarizer and (b) the summarized process-based verifier. However, this work primarily focuses on (b). While the authors stress the importance of the correct summarization and mention re-summarization, it remains at a qualitative level. Importantly, it looks that the summarized CoTs are kept the same for the entire process, making only (b) the target of the updates/optimization. This can pose a concern that this work tackles a subproblem of the original problem (i.e., optimizing (b)) with an assumption of the existence of a reasonable summarizer (i.e., (a)).\n\n2. Potential subjectivity in determining outcome-relevant steps  \nDepending on the domain, problem, or specific CoT, there may be some subjectivity in whether each step should be considered a \"key step\" toward the final answer and kept through the summarization. For instance, some steps may not be directly related to the final answer but are still part of the needed exploration to find the correct path to the final answer. Given that this work suggests verifying each of the key steps that are retained in the summarized CoTs, this potential subjectiveness may affect whether each generation should be considered correct or not."}, "questions": {"value": "1. Can you include the ProcessBench performance numbers with OPV but without fine-tuning using human annotations? That could provide a better picture of how the summarization helps with verification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "arrQR7u0hO", "forum": "rG3rXSdzl1", "replyto": "rG3rXSdzl1", "signatures": ["ICLR.cc/2026/Conference/Submission1370/Reviewer_WDkH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1370/Reviewer_WDkH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941122972, "cdate": 1761941122972, "tmdate": 1762915749453, "mdate": 1762915749453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes OPV, which verifies reasoning by first using DeepSeek-V3 to summarize long CoTs into concise solution paths, then performing step-by-step verification on the summaries. \n\nAn iterative active learning framework selects uncertain cases for expert annotation. The authors collect 40k annotated solutions and create OPV-BENCH (2.2k samples). \n\nOPV (32B) reportedly outperforms larger models and improves policy model performance on AIME2025 from 55.2% to 73.3% accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear Problem Motivation: The paper identifies real limitations of OV (ignores process) and PV (expensive for long CoTs), motivating the need for alternative approaches.\n\n2. Substantial Data Collection Effort: Curating 40k expert-annotated solutions with rigorous three-expert consensus protocol represents significant engineering work."}, "weaknesses": {"value": "### 1. Unsubstantiated Efficiency Claims\n\nTotal cost = Summarization (671B DeepSeek-V3) + Verification (32B OPV)\n\nPaper provides ZERO computational cost measurements: no latency, FLOPs, or token counts. It claims \"efficient\" repeatedly but likely more expensive than vanilla PV. This undermines the entire motivation\n\n### 2. Unfair Experimental Comparisons\n\n- Baselines: Qwen3-Max, R1, etc. use zero-shot prompting\n- OPV: Uses 40k expert annotations + iterative training + RL\n\nThis compares \"supervised training with massive expert data\" vs. \"no training,\" NOT verification paradigms\n\nMissing critical ablations: \n- (1) Same 40k data training full-CoT PV (the real baseline)\n- (2) Same 40k data training simple OV\n- (3) Summarization contribution quantification (0%? 50%? Unknown)\n\n### 3. Strong Evidence of Data Leakage\n\nFirstly, OPV-BENCH construction and train/test split completely unspecified. The paper says: \"Curated from widely-used benchmarks\", but  which ones? What overlap?\n\nSecondly, Suspicious performance pattern:\n- ProcessBench: OPV 94.4 vs. gpt-oss-120b 93.5 (marginal +0.9)\n- OPV-BENCH: OPV 86.2 vs. gpt-oss-120b 77.8 (huge +8.4)\n\n### 4. Active Learning Theatre\n\nZero experiments comparing active learning vs. random sampling at equal budgets. No justification for this specific uncertainty metric."}, "questions": {"value": "1. If DeepSeek-V3 can perfectly extract key steps, why not use it directly as the verifier?\n2. Why does OPV only dominate on its own benchmark? Likely overfitting/leakage"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hYBKabDbzR", "forum": "rG3rXSdzl1", "replyto": "rG3rXSdzl1", "signatures": ["ICLR.cc/2026/Conference/Submission1370/Reviewer_XyCa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1370/Reviewer_XyCa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762018327740, "cdate": 1762018327740, "tmdate": 1762915749326, "mdate": 1762915749326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}