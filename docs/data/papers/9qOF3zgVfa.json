{"id": "9qOF3zgVfa", "number": 63, "cdate": 1756728359744, "mdate": 1759898277198, "content": {"title": "A Needle In A Haystack: Referring Hour-Level Video Object Segmentation", "abstract": "Long-term videos over minutes are ubiquitous in daily life while existing Referring Video Object Segmentation (RVOS) datasets are limited to short-term videos with a duration of only 5-60 seconds. \nTo unveil the dilemma of referring object segmentation towards hour-level videos, we construct the first Hour-level Referring Video Object Segmentation (Hour-RVOS) dataset characterized by \n(1) any-length videos from seconds to hours, (2) rich-semantic expressions with double length, and (3) multi-round interactions according to target change. \nThese unique characteristics further bring tough challenges including  (1) **Sparse object distribution**:  Segmenting target objects in sparse-distributed key-frames from massive amounts of frames is like finding a needle in a haystack. (2) **Long-range correspondence**: Intricate linguistic-visual associations are required to establish across thousands of frames. \nTo address these challenges, we propose a semi-online hierarchical-memory-association RVOS method for building cross-modal long-range correlations. \nThrough interleaved propagation of hierarchical memory and dynamic balance of linguistic-visual tokens, our method can adequately associate multi-period representations of target objects in a real-time way. \nThe benchmark results show that existing offline methods have to struggle with hour-level videos in multiple stages, whereas our proposed method without LLMs can achieve over $15\\%$% accuracy improvements compared to Sa2VA-8B when handling any-length videos with multi-round and various-semantic expressions in one-stage.", "tldr": "", "keywords": ["Referring Video Object Segmentation", "Hierarchical Memory"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3b643a86f53d4d2476c0f3ea238941b545fde51e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the limitation of existing Referring Video Object Segmentation (RVOS) datasets, which only cover short-term videos (5-60 seconds), by constructing the first Hour-level RVOS (Hour-RVOS) dataset. To tackle the unique challenges of Hour-RVOS, such as sparse object distribution and long-range correspondences, the authors propose a semi-online Hierarchical-Memory-Association method (Memory-RVOS), which includes a Hierarchical Memory Interleaved Propagation module and a Linguistic-Visual Dynamic Balance module. The method is designed to retain core target information and balance cross-modal tokens, enabling real-time segmentation of any-length videos. Comprehensive experiments compare Memory-RVOS with existing offline RVOS methods and Multimodal Large Language Models (MLLMs) on Hour-RVOS."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Reasonable idea and great motivation: The proposal of Hour-RVOS and Memory-RVOS directly targets real-world scenarios where long-duration videos (e.g., daily records, movies) require accurate referring segmentation, making the research both practical and innovative.\n2. The authors conduct an in-depth analysis of Hour-RVOS, including comparisons with 6 existing RVOS datasets and fine-grained statistics, which fully present the dataset’s complexity and representativeness.\n3. The proposed method, Memory-RVOS, is highly tailored to the challenges of Hour-RVOS. Ablation studies (Tables 3, 4) further confirm that each component contributes to performance improvements.\n4. The experiments cover multiple dimensions: benchmarking against diverse baselines on Hour-RVOS; verifying generalization on existing short-term RVOS datasets; conducting comprehensive ablation studies."}, "weaknesses": {"value": "1. The paper uses \"A NEEDLE IN A HAYSTACK\" to metaphorically describe the challenge of sparse object distribution in hour-level videos, but it lacks in-depth discussion. For example, it does not compare this challenge with similar \"needle-in-haystack\" problems in other video tasks.\n\n   - Needle In A Video Haystack: A Scalable Synthetic Evaluator for Video MLLMs.\n   -  Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models.\n   - Re-thinking Temporal Search for Long-Form Video Understanding.\n\n2. The paper introduces hierarchical memory, but it overlooks a significant body of prior work on hierarchical memory systems. For instance, earlier studies have investigated hierarchical memory in the domains of video segmentation.\n\n   - XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model."}, "questions": {"value": "1. Table A3 shows that when clip size increases from 4 to 16 (a 4x increase), FPS only rises from 22 to 31 (less than a 1.5x increase), but the paper does not explain this discrepancy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Zq6tW8NwLD", "forum": "9qOF3zgVfa", "replyto": "9qOF3zgVfa", "signatures": ["ICLR.cc/2026/Conference/Submission63/Reviewer_iizf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission63/Reviewer_iizf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission63/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760871507444, "cdate": 1760871507444, "tmdate": 1762915444613, "mdate": 1762915444613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Hour-RVOS, the first referring video object segmentation dataset with any-length videos up to hour with richer language annotations (avg. 18.3 words) and multi-round interactions. The proposed dataset identifies two core challenges of long video RVOS: sparse object occurrence and long-range language-vision relationship. The paper further proposed a new semi-online method Memory-RVOS with hierarchical memory and a language-visual balance module. Experiments show that the proposed method outperforming current baselines on the new proposed Hour-RVOS dataset."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The RVOS dataset proposed is substantially larger than existing datasets in average sequence length, total duration, number of masks, description complexity, and overall difficulty. It is also the first dataset to include multi-round semantic descriptions, which supports training and evaluating complex semantic segmentation in long videos.\n\n- Evaluation on existing methods indicate the proposed dataset is sufficiently challenging.\n\n- The overall logic of the paper looks good (though there are significant issues in its methodological clarity; see comments below)."}, "weaknesses": {"value": "- According to Tab. 1, the proposed Hour-RVOS contains similar / fewer objects and categories than some existing datasets. This may limit the generalization of models trained or evaluated on Hour-RVOS to open-world object scenarios. A deeper discussion of this limitation would strengthen the paper.\n\n- The method description is unclear. Sections 3.2 and 3.3 present an imbalanced and sometimes illogical exposition of the pipeline. I recommend reorganizing these sections for clarity. Several design considerations are neither clearly justified in the method nor supported by targeted ablations. Please see “Questions” for my specific concerns.\n\n- Many typos and figure issues, naming a few:\n\nL137: “GoundingDINO” → “GroundingDINO”.\n\nFig. 1(b) caption: “Average expression length” → “Average duration”.\n\nFig. 4 lacks legends and annotations, making it difficult to match components to symbols in the text.\n\nFigs. 4 and 5: symbol rendering errors (displayed as question marks).\n\nOverall, figure notation and typography errors are frequent. A thorough proofreading pass is strongly recommended."}, "questions": {"value": "**About the Hour-RVOS dataset**\n\n- Per L343, the category distributions are consistent across train/val/test. In contrast, established VOS benchmarks (e.g., YouTube-VOS [1]) include both seen (overlapping with train) and unseen (not seen at all during training) categories in the test set to evaluate generalization abality to open-world objects. Given the proposed Hour-RVOS is not dramatically larger than YouTube-VOS w.r.t. object/category counts, could you consider a similar seen/unseen split (or an auxiliary evaluation) to assess more general generalization?\n\n**About the proposed RVOS method**\n\n- Section 3.2 (L241–L245) states that text features and object queries are fused and fed to MaskFormer; the resulting object tokens are then fused with memory tokens in the Hierarchical Memory Interleaved Propagation module, and the fused target object tokens are decoded into masks. However, Fig. 4 seems not consistent with this description. In Fig. 4, which components correspond to the object queries Q?, which one correspond to the “potential object tokens”?, and where are the “frame-mask level features”?\n\n- Section 3.3 (L262): How exactly is the short-term memory updated? Please provide a precise description.\n\n- Section 3.3 (L265): The “permanent memory” select tokens with the highest feature similarity from the long-term memory and also impose a window-size limit. This seems quite sparse—how do you ensure the selected tokens preserve sufficient object semantics?\n\n- Section 3.3 (L282): The permanent tokens are compressed in the query encoding stage. What is the motivation for this compression, and how do you mitigate potential information loss?\n\n- Section 3.3 (L295): “We place different memory tokens into different encoding process to adequately match visual-linguistic tokens in different temporal scale.” To my understanding, short-term tokens and the compressed permanent memory are fused with object queries during query encoding, while long-term memory tokens are fused later via cross-attention. What motivates this asymmetric design?\n\nReferences\n[1] Xu, Ning, et al. “YouTube-VOS: A large-scale video object segmentation benchmark.” arXiv:1809.03327 (2018)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J5PXf7cYRt", "forum": "9qOF3zgVfa", "replyto": "9qOF3zgVfa", "signatures": ["ICLR.cc/2026/Conference/Submission63/Reviewer_h6TK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission63/Reviewer_h6TK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission63/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540958729, "cdate": 1761540958729, "tmdate": 1762915444433, "mdate": 1762915444433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an Hour-level RVOS dataset, which features sparse object distributions and long-range temporal correspondence across videos. To address the long-range object association challenge, the authors also propose a semi-online hierarchical memory association method (Memory-RVOS). The proposed approach achieves significantly better performance than existing RVOS methods on their constructed benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The introduced Hour-RVOS dataset is the first benchmark designed specifically for hour-level RVOS tasks.\n- The paper proposes a hierarchical memory association framework tailored for long-range object tracking and segmentation in RVOS. The method achieve much better performance than exisiting methods in the Hour-RVOS benchmark."}, "weaknesses": {"value": "Dataset\n- As a benchmark, the dataset size is relatively small (only 300 videos), compared with thousands in existing long-RVOS datasets (e.g., Long-RVCOS, MeViS, Ref-YouTube-VOS). Although the total number of frames is larger, the number and diversity of object instances remain limited. Moreover, it only contains 90 test videos.\n- The duration statistics show that most videos are under 10 minutes, with only a small portion exceeding 30 minutes.\n- The role of rich textual expressions is unclear. For RVOS benchmarks, it is debatable whether detailed multi-attribute language descriptions are necessary. In real-world use, expressions are often short and simple. Thus, metrics such as Average Words (Table 1) may not be meaningful. Similarly, the multi-round expressions could blur the boundary between visual reasoning and language-based re-identification. They may aid long-range association through text rather than through the RVOS model itself.\n\nMethod\n- The description of Memory-RVOS is unclear and under-specified. For example, it is not specified which modules are trained, e.g., the linguistic encoder, Mask2Former, or other components? The loss functions are not detailed either.\n- The hierarchical memory update and dynamic balance mechanisms are ambiguously described: are they applied during inference only, or during both training and inference?\n- In Fig. 4(b), the design rationale for placing visual short-term and permanent memory in query encoding, while long-term memory and linguistic features are used as keys/values, is not well justified. Would switching permanent and long-term memory improve performance?\n\nExperiments\n- It is unclear whether the competing methods in Table 2 were fine-tuned on the Hour-RVOS dataset; if not, the comparison may be unfair.\n- Ablation studies are conducted only on the validation set (30 videos) rather than the full test set.\n- From Table 3, even the short-term memory variant outperforms previous SOTA methods, suggesting that gains may arise from dataset bias (e.g., rich semantics or multi-round expressions) or fine-tuning effects, rather than the proposed method itself.\n- Although the authors claim real-time performance (Line 364), no inference time results are reported in the main paper."}, "questions": {"value": "Stated in Weakness part."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The collected videos need ethics review."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QbeMrR8CWI", "forum": "9qOF3zgVfa", "replyto": "9qOF3zgVfa", "signatures": ["ICLR.cc/2026/Conference/Submission63/Reviewer_Fj5M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission63/Reviewer_Fj5M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission63/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571517485, "cdate": 1761571517485, "tmdate": 1762915444269, "mdate": 1762915444269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Hour-RVOS, which targets second-to-hour videos (100.4h, 300 videos, 9,114 expressions). It highlights two challenges: Sparse object distribution and Long-range correspondence. To address this, the authors propose a semi-online method with hierarchical memory and language visual dynamic balance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The dataset is well-designed, featuring videos of arbitrary length and multi-round semantic interactions, effectively capturing the complexity of real-world scenarios.\n2. The proposed semi-online hierarchical memory association framework (Memory-RVOS) demonstrates clear effectiveness in modeling long-term cross-modal dependencies.\n3. Experimental results provide strong evidence for the effectiveness of the proposed approach."}, "weaknesses": {"value": "1. As shown in Figure 1(a), fewer than ten videos have a duration of one hour, and the average length is around 20 minutes. So, describing the dataset as hour-level appears to be somewhat of an overclaim.\n2. Lines 313–314 state that the model uses “newly arrived linguistic features to highlight the visual tokens stored in memory”. However, this mechanism is not further elaborated — only the procedures for pruning noise tokens and reweighting linguistic tokens are discussed.\n3. The semantic richness of the dataset is limited — the number of videos, expressions, and object categories does not offer a significant advantage over existing datasets.\n4 .The paper lacks visualization results to demonstrate that the proposed modules effectively address the issues of sparse object distribution and long-range correspondence."}, "questions": {"value": "1. What is the total number of words in Hour-RVOS and other RVOS dataset? Does the so-called rich-semantic expressions mainly refer to the average length of each expression?\n2. How are the newly arrived linguistic features used to highlight the visual tokens stored in memory? Could the authors provide more detailed explanations?\n3. Multi-round expressions are a key feature of Hour-RVOS. Could the authors provide statistics on how many referring expressions each video contains？\n4. The Long-RVOS dataset also focuses on long-term referring video object segmentation. What are the main differences between Long-RVOS and Hour-RVOS?\n5. When generating expressions using LLMs, what kinds of prompts were used? Were there fixed templates, and how were multiple aspects of an object combined into a single complete expression?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A8aYitz5ZK", "forum": "9qOF3zgVfa", "replyto": "9qOF3zgVfa", "signatures": ["ICLR.cc/2026/Conference/Submission63/Reviewer_kwao"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission63/Reviewer_kwao"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission63/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835787371, "cdate": 1761835787371, "tmdate": 1762915444106, "mdate": 1762915444106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}