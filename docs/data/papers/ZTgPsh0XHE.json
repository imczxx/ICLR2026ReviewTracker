{"id": "ZTgPsh0XHE", "number": 15707, "cdate": 1758254177945, "mdate": 1759897287321, "content": {"title": "KA2L: A Knowledge-Aware Active Learning Framework for LLMs", "abstract": "Fine-tuning large language models (LLMs) with high-quality knowledge has been shown to enhance their performance effectively. However, there is a paucity of research on the depth of domain-specific knowledge comprehension by LLMs and the application of targeted active learning to improve their expertise. To address this gap, we introduce the **Knowledge-Aware Active Learning (KA2L)** framework. This framework assesses LLMs' mastery of specific knowledge points to aid in constructing unanswerable or unknowable questions through latent space analysis. This active learning strategy enhances training efficiency by focusing on knowledge the model has yet to master, thereby minimizing redundancy in learning already acquired information. This study innovatively employs a knowledge distribution probing technique to examine the hidden states of specific Transformer layers and identify the distribution of known and unknown knowledge within the LLM. Additionally, a hidden-state decoding method is proposed to generate numerous unknown questions in natural language from the latent knowledge space. In our experiments, we selected nine open-source LLMs to validate the effectiveness of the proposed framework. Results indicate that KA2L not only significantly reduces 50\\% annotation and computation costs across two open-domain and one vertical-domain dataset but also achieves better performance, offering valuable insights into active learning strategies for LLMs. The code is available at https://anonymous.4open.science/r/KA2L-F15C", "tldr": "An active learning framework for LLMs that identifies what the LLM does not know and purposefully constructs a repository of unanswerable or unknowable questions.", "keywords": ["Large Language Models", "Active Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d7d23ff5bc0f736463b13b6bc56fac2edecddcc7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an active learning method named KA2L, which leverages Semantic Entropy (SE) on hidden representations and the output of LLM responses to determine whether the model possesses knowledge about a given input query. It then fine-tunes the LLM using (query, answer) pairs that are deemed unknown to the model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly presented, and the proposed method is well-explained. Additional details in the appendix enhance the paper’s credibility.\nThe underlying motivation is clear and well-justified."}, "weaknesses": {"value": "1. The research contributions are not clearly articulated. Contributions 1 and 2 (lines 079–087) both claim novelty in the proposed framework, particularly in integrating LLM knowledge distribution probing with hallucination detection. However, Contribution 2 appears to be a subset of Contribution 1. It remains unclear which components of the framework are truly novel—for example, the use of SE, or the extraction and concatenation of the last token’s representations across layers.\n2. The experiments lack comprehensiveness. There should be a comparison of efficiency and computational cost between KA2L and the baselines. This is especially important given that KA2L only marginally outperforms the baseline Coreset in BLEU (22.29 vs. 22.04) and BERTScore (91.08% vs. 90.06%), as shown in Table 3.\n3. KA2L relies on a threshold to determine whether questions are KNOWN or UNKNOWN to the model. This threshold is rigid and costly to estimate.\n4. The concept of this threshold—and the binary partitioning of questions into KNOWN and UNKNOWN—is not well justified. A small change in the threshold could easily flip a question from one category to the other."}, "questions": {"value": "1. What is the justification for using the last token’s representations across all layers? Shouldn’t only the representation from the final layer, which is used for decoding and generating answers, be considered?\n2. Lines 124–125 state: “Q_{unk} represents the set of questions for which the model's answers are uncertain.” What happens if the LLM provides answers with high certainty that are actually incorrect? Is this scenario considered? Would such questions be classified as KNOWN or UNKNOWN?\n3. Where is the definition of SE+i mentioned in line 208?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gcQZbVYm7C", "forum": "ZTgPsh0XHE", "replyto": "ZTgPsh0XHE", "signatures": ["ICLR.cc/2026/Conference/Submission15707/Reviewer_SJ6L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15707/Reviewer_SJ6L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761414151712, "cdate": 1761414151712, "tmdate": 1762925957190, "mdate": 1762925957190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a novel method for selecting valuable samples for LLM training based on answer consistency, splitting the unlabeled dataset into known and unknown samples. In addition, a novel augmentation technique was proposed that further enhances model performance with limited resources."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper performs robust evaluation in its experiments on three datasets with nine different LLMs.\n- Clear Guidance through the experimental section via research questions.\n- The paper proposes a simple yet efficient approximation scheme for \"Knowledge\" of the LLM via a binary MLP classifier and proposes a method for hyperparameter selection."}, "weaknesses": {"value": "- The proposed method is compared to well-known AL methods. However, the setting doesn't really involve an iterative selection scheme but a one-time selection, which does not favor any of the compared AL strategies and leads to heavy overlap in informativeness of selected samples (one-time selection of 5000 samples out of 10000 samples). As a result, the provided empirical evidence in sec. 5.3 is rather meaningless.\n- While the adaptation of CoreSet seems appropriate, the adaptation of BADGE seems misleading, adding to the confusion of the first weakness.\n- Other, newer, and more powerful DAL strategies have not been considered, especially strategies that can work on hidden representations (e.g., TypiClust)."}, "questions": {"value": "- What was the motivation behind framing this work as an active learning method? I would see this more in a filtering-methodology.\n- Methods should be compared to known SOTA methods, where BADGE and especially CoreSets have been replaced several years ago. Did you consider other methods?\n- Is it possible to perform classic active learning with multiple iterations and updating the representations the strategies work with for a fairer comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WE41INmojB", "forum": "ZTgPsh0XHE", "replyto": "ZTgPsh0XHE", "signatures": ["ICLR.cc/2026/Conference/Submission15707/Reviewer_tzEH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15707/Reviewer_tzEH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904663218, "cdate": 1761904663218, "tmdate": 1762925956556, "mdate": 1762925956556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes KA2L, a knowledge-aware active-learning framework that probes the hidden states of an LLM to partition questions into “known” and “unknown”, then fine-tunes only on the unknown subset plus decoder-generated augmentations. Across nine open-source models and three QA datasets the method cuts annotation/compute costs by ~50 % while matching or exceeding full-data performance, and consistently outperforms adapted classical AL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Thorough empirical validation—extensive ablations, layer-wise probes, traditional-AL comparisons, and robustness checks across diverse model families. \n\n- Practical impact—simple MLP probe and T5-based decoder add negligible inference cost yet yield large savings, with reproducible code provided. \n\n- Clear writing and well-motivated research questions."}, "weaknesses": {"value": "- The novelty should be highlighted. Adding a small model for LLM active learning is not quite new. What are the most significant differences between the current method and the existing ones, e.g., FreeAL? The semantic entropy here is more like a prediction confidence, please refer to DeepConf (https://arxiv.org/abs/2508.15260) for the related work.\n\n- Scope is limited to factual closed-book QA; unclear how well the probe transfers to open-ended or reasoning-heavy tasks.\n\n- Generated QA pairs still need external annotation; the paper does not quantify human effort for this step."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JTnibeuDi5", "forum": "ZTgPsh0XHE", "replyto": "ZTgPsh0XHE", "signatures": ["ICLR.cc/2026/Conference/Submission15707/Reviewer_EJNQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15707/Reviewer_EJNQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925665815, "cdate": 1761925665815, "tmdate": 1762925956222, "mdate": 1762925956222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Summarize: This paper proposed the Knowledge-Aware Active Learning (KA2L) framework. It leverages the language model hidden states and the Semantic Entropy metric on sampled outputs to train a classifier that determines whether the model has the knowledge for a given question. The result shows that training on data with unknown questions is more effective, saving annotation and compute costs. The paper also employs a latent space decoding technique from LLM interpretability research to augment the unknown questions. KA2L also outperforms adapted classic active learning methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of using semantic entropy to guide data selection in active learning is novel.\n\n2. The result is validated with detailed experiments on nine open-source LLMs."}, "weaknesses": {"value": "1. A core claim of this paper is that “a higher SE value suggests greater semantic divergence, indicating that the model has not mastered the knowledge associated with the question” (Sec 3.2.2), but there are no experiments explicitly validating that a higher SE score is directly related to a lower performance metrics, making this claim questionable.\n\n2. In the experiments, this paper constructs a D_combine dataset “simulating a standard, unfiltered dataset collected without an active learning strategy” by mixing D_unk and D_k equally. The question is why not directly sample data from the original pool? D_unk samples and D_k samples may not be equally distributed.\n\n3. The paper claims that KA2L can “cut annotation and computational costs by approximately 50% while maintaining high performance” (Sec 5.1) by comparing 5k Unknown and 10k Combine sets. This ignores the computation overhead of KA2L’s workflow: probing (training and inference) and possible augmentation. Claiming a napproximately 50% computational costs reduction could be an unfair comparison."}, "questions": {"value": "The code repo link provided by the authors is not working as every file shows “The requested file is not found.”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C8P9Z8aE5m", "forum": "ZTgPsh0XHE", "replyto": "ZTgPsh0XHE", "signatures": ["ICLR.cc/2026/Conference/Submission15707/Reviewer_5ogy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15707/Reviewer_5ogy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959937414, "cdate": 1761959937414, "tmdate": 1762925955818, "mdate": 1762925955818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}