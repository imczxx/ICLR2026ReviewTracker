{"id": "hagNoZpM11", "number": 15363, "cdate": 1758250595227, "mdate": 1763727515337, "content": {"title": "TEFormer: A Topology-Enhanced Transformer for Architecture Performance Prediction", "abstract": "Evaluating architecture performance is a crucial step in neural architecture search (NAS) but remains computationally expensive. Performance predictors offer an efficient alternative by learning from a limited set of architecture-performance pairs. However, previous predictors tend to oversimplify the topological structure of neural architectures using adjacency matrices, node depths, or computation flow, which fail to fully capture topological features of architectures, leading to poor generalization. To address this limitation, we propose TEFormer, a Topology-Enhanced Transformer that integrates both local and global topological information beneficial to performance prediction. Specifically, we employ a topology-aware flow encoding module that incorporates local topological characteristics via a learnable structural encoding and a flow-based encoder. At the global level, we design a hierarchical attention mechanism to jointly model intra-flow and inter-flow interactions within the architecture. To further improve generalization, we propose an architecture augmentation strategy that synthesizes additional samples by interpolating similar architectures in the latent space. Extensive experiments on computer vision, graph learning, and automatic speech recognition tasks demonstrate that TEFormer consistently outperforms state-of-the-art predictors and exhibits superb performance across diverse search spaces.", "tldr": "", "keywords": ["neural architecture search", "performance predictor"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5a23ced475d4e47d353edd24d556b0deb663d2a5.pdf", "supplementary_material": "/attachment/0d2155f5ba945c604dd1bffeecc34d025d10109b.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes TEFormer, or Topology-Enhanced Transformer for Neural Architecture Search (NAS) performance prediction. TEFormer continues a line of work on flow-based predictors [1-3] for NAS. TEFormer augments these approaches by encoding the local and global features through a specialized attention mechanism and positional encoding. TEFormer is evaluated on several NAS-Benchmarks for computer vision and graph prediction."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper makes advances in NAS performance prediction.\n- The experimental setup and execution is solid. \n- The method is clear and easy to understand.\n- Ablation studies are provided.\n- The evaluation is not simply limited to NAS-Bench-{101, 201, 301}, but other benchmarks."}, "weaknesses": {"value": "- The method is incredibly incremental. Table 6 proves this with how little, arguably not statistically significant change takes place during the ablation study. \n- TEFormer primarily stems from an existing, but very narrow line of work on Flow-based predictors [1-3] and the contribution is just slight increases in Kendall's Tau on DARTS which is not noteworthy when we've already been able to leap frog over the best DARTS architectures on ImageNet for several years [4]. \n- The statement in lines 224-225 \"Considering that both forward and backward passes are essential for accurately modeling neural architectures (...), we encode the bidirectional computational flow\", should be removed or heavily revised, as it is essentially ignoring other advances in performance prediction [5, 6, 7] that are tangential to [1-3]; it essentially reads as if the method of [1-3] is the only correct way to achieve performance prediction, which is not true."}, "questions": {"value": "Can the local and global level features be used to extract information about the structure of good/bad architectures as [9, 10] do?\n\nReferences:\n\n[1] https://arxiv.org/abs/2004.01899\n\n[2] https://proceedings.neurips.cc/paper_files/paper/2022/file/d0ac28b79816b51124fcc804b2496a36-Paper-Conference.pdf\n\n[3] https://arxiv.org/abs/2403.12821\n\n[4] https://arxiv.org/abs/1812.00332\n\n[5] https://arxiv.org/abs/2506.04001\n\n[6] https://arxiv.org/abs/2210.03230\n\n[7] https://proceedings.neurips.cc/paper_files/paper/2022/hash/572aaddf9ff774f7c1cf3d0c81c7185b-Abstract-Conference.html"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cLaYbqYLM3", "forum": "hagNoZpM11", "replyto": "hagNoZpM11", "signatures": ["ICLR.cc/2026/Conference/Submission15363/Reviewer_FtPd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15363/Reviewer_FtPd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761256588585, "cdate": 1761256588585, "tmdate": 1762925648669, "mdate": 1762925648669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TEFormer, a novel Transformer-based predictor for neural architecture performance estimation. The core innovations include a topology-aware flow encoding module that incorporates bidirectional computation flows with learnable structural encodings based on random walks, a hierarchical attention mechanism to model both intra-flow and inter-flow dependencies, and an interpolation-based architecture augmentation strategy to combat data scarcity. The method is evaluated on multiple NAS benchmarks across computer vision, graph learning, and speech tasks, demonstrating highly competitive ranking performance and search results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of explicitly modeling bidirectional computation flows (forward and backward) is well-motivated and differentiates the work from many existing predictors that rely on static graph encodings. The integration of learnable structural encodings from random walks is a principled approach to capture rich topological information.\n2. The authors evaluate on many standard NAS benchmarks and provide ablation studies and sensitivity analyses, showing consistent gains and some robustness to hyperparameters."}, "weaknesses": {"value": "1. Some design choices are not sufficiently explained, especially from Eq. (2) to Eq. (4). The rationale behind these formulations is not intuitive.\n2. In Section 4.2, the underlying motivation for computing attention only between nodes connected by a directed path or within the same topological group is not well-justified. The authors' explanations read more like descriptions of the rules' effects rather than a justification of the underlying design principles.\n3. The interpolation-based augmentation strategy, presented as a major contribution, appears potentially risky. A more comprehensive evaluation is required to substantiate its value, such as conducting ablations in both chain-based and cell-based search spaces and studying the impact of the number of augmented samples.\n4. The experimental comparisons lack several important baselines, such as [1], [2], [3], and [4]. Specifically, a direct comparison with the transformer-based predictor [1] is missing on CIFAR-10, and on ImageNet, only the cell-based results of [1] are compared, omitting its chain-based results. Furthermore, the results of [2], [3], and [4] appear to surpass those reported in this work.\n5. For the results on CIFAR-10, it is necessary to report both the mean and standard deviation.\n\n[1] PINAT: A Permutation INvariance Augmented Transformer for NAS Predictor\n[2] CARL: Causality-guided Architecture Representation Learning for an Interpretable Performance Predictor\n[3] HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork\n[4] Computation-friendly Graph Neural Network Design by Accumulating Knowledge on Large Language Models"}, "questions": {"value": "see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mUgPRi6vho", "forum": "hagNoZpM11", "replyto": "hagNoZpM11", "signatures": ["ICLR.cc/2026/Conference/Submission15363/Reviewer_mjum"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15363/Reviewer_mjum"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717256892, "cdate": 1761717256892, "tmdate": 1762925648159, "mdate": 1762925648159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TEFormer is a novel Transformer model designed for NAS performance prediction. It precisely captures the complex topological information of neural architectures by combining Topology-aware Flow Encoding and a Hierarchical Attention Mechanism. Additionally, the model employs an interpolation-based augmentation strategy in the latent space to enhance generalization in few-shot scenarios. TEFormer achieves SOTA performance across multiple NAS benchmarks and includes detailed ablation studies confirming the effectiveness of its components."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper provides extensive experimental validation.\n2.  The paper includes detailed sensitivity analysis and ablation studies."}, "weaknesses": {"value": "1.Overall, the core of this paper's contribution is essentially the introduction of a new loss function, which is derived by combining several typical neural network modules, thus lacking novelty.\n2. The paper lacks corresponding theoretical proof. I believe providing a relevant theoretical analysis for the proposed loss function would have been a significant addition to this paper."}, "questions": {"value": "Can you try to theoretically analyze why this network architecture was chosen?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mGHca2olR8", "forum": "hagNoZpM11", "replyto": "hagNoZpM11", "signatures": ["ICLR.cc/2026/Conference/Submission15363/Reviewer_eGKW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15363/Reviewer_eGKW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728051236, "cdate": 1761728051236, "tmdate": 1762925647513, "mdate": 1762925647513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}