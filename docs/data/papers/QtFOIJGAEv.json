{"id": "QtFOIJGAEv", "number": 9869, "cdate": 1758145216647, "mdate": 1759897690580, "content": {"title": "DDI-Aware Domain Adaptation for Cross-Domain Drug Combination Representation Learning via Contrastive Embedding", "abstract": "Drug–drug interaction (DDI)–aware representation learning for combination therapy remains challenging under distribution shift: prevailing approaches tend to align marginals while neglecting the pairwise interaction structure that should be preserved across domains. We address these gaps with a DDI-aware, domain-adaptation–style framework that cleanly separates a population-level coupling objective from a tractable map-level surrogate. We present a conservative and fully specified framework integrating drug-drug interaction (DDI) structure into a domain-adaptation-style (DA-style) alignment view for association combination representation learning. Our composition has two purposely separated layers. In the paper coupling layer we add to classical optimal transport (OT) a structure-preserving DDI penalty that induces congruency between pairwise interaction structure between domains and we prove the existence of minimizers under standard measure theoretic conditions, without making any claims on determinism of the maps, called Monge maps. Under Dutch map layer and locally Lipschitz map layer and linear growth global well-posedness of induced preconditioned descent in map layer energy monotonicity is stated only under an explicit optional assumption of monotonicity of preconditioner not covering adaptive optimizers such as Adam. We present so-called Eulerian (continuity equation) view for help in interpretation but with clear indication of the scope of rigor. Empirically, we utilize the TWOSIDES proxy dataset to investigate whether DDI-aware pretraining will lead to better interaction-aware feature learning through the use of a proxy data-set (adverse-interaction). We emphasize that adverse DDI labels are not clinical non-synergy but conclusions are limited to the proxy discrimination task. On a variety of graph backbones and embedding baselines, supervised contrastive learning (SCL) with DA-style marginal alignment approaches leads to improvements in terms of Accuracy and Precision with competitive Recall.", "tldr": "", "keywords": ["Domain adaptation", "RKHS MMD alignment", "Supervised Contrastive Learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/83135f5c48c2e9a60b8e7c8f15fab1784bc2d247.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses drug–drug interaction (DDI)-aware representation learning for combination therapy modeling. It argues that, while combination therapy discovery benefits from multi-drug data, it is complicated by heterogeneous sources, domain shifts, and safety constraints. The authors propose to treat interaction structure (how drugs interact) as an inductive bias that regularizes learned representations — using concepts from optimal transport (OT) and domain adaptation (DA)"}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The idea of introducing a DDI-aware regularization term inspired by domain adaptation is potentially useful.\n2. The work could be significant for two reasons. First, it contributes to the methodological development on structure-preserving learning under domain shifts. Second, by grounding DDI-aware learning in transport theory, it opens directions for more mathematically interpretable safety-aware machine learning models in biomedicine."}, "weaknesses": {"value": "1. **Clarity is lacking:** The paper (particularly the abstract, introduction and methods) are written in an overly formal and opaque style. Sentences are long, terminology is inconsistent, and the logical flow between sections is unclear. It’s often difficult to discern the actual research question or technical novelty. For example, the introduction abruptly introduces the following without even setting context for why they are necessary or relevant. Only later in Methods section they are introduced again\n\na) `The coupling objective gives pairwise structure alignment over domains and the map-level surrogate governs dispersion of the target structure evaluation over pushforward pairs and marginals via MMD.` In introduction, it is not clear what is the domain or map-level surrogate\n\nb) `The divide of the coupling formulation and the map-based surrogate between population-level judgments of optimality and algorithmic approximations about how the surrogate should be adjusted for the process being optimized allows the optimality kernels and the algorithms to be treated separately.` In the introduction, it is unclear what are optimality kernels.\n\nc) `We adopt DA-style alignment as a regularizer; our experiments in this paper are still single-dataset and is not evaluating the cross-domain transfer.` Although it becomes clear later reading methods and experiments, writing this in introduction section feels confusing because the paper title suggests the work will discuss cross-domain drug combination which this statement seems to contradict on first glance.\n\nThese are only a few examples but I find that the Introduction section in general is quite hard to follow and lacks clarity. Same is the case for abstract section.\n\n2. **Weak connection between theory and application:** In the introduction, the focus is heavily on DDI motivation. But after introduction this completely disappears and the methods section which follows does not relate the methods to the DDI problem introduced earlier.  The “DDI operator” and “structure template” are defined abstractly, but it is never explained how these correspond to real biological or chemical quantities. \n3. **Mathematical problems / ambiguities:** \n\na) DDI is defined as DDI : B × B → B (vector-valued). But the penalty uses norms of differences, so it behaves like a map to a normed vector space or to scalars. Why map into B rather than ℝ or a specific feature space H_struct?\n\nb) The paper defines the DDI operator but doesn't elaborate on its concrete form. What exactly is this operator? Is it a simple distance metric, a neural network, or a specific graph-based function?\n\nc) The V_DDI penalty is introduced as a \"tractable single-domain surrogate\" that uses a fixed constant symmetric template S_t. What is S_t in practice? Is it a pre-computed or learned ideal interaction structure? The choice of this constant template is vital to how the representations are regularized.\n\n4. Although TWOSIDES data is mentioned, the methods section gives no description of how the proposed objectives are computed in practice or how “DDI structure” is represented in the data.How to reproduce or interpret the experiments section and relate it to Methods?\n\n5. The experiments section is substantially weak with experiments presented on single dataset.\n\n6. The Figures are complex and the captions are non-informative (For example, I find Fig.1 hard to interpret on its own and the caption does not explain any of the components shown)\n\n7. The writing is full of typos and grammatical errors such as, \n\na) In introduction in contributions section `constant templatespan` in L94 misses space, \n\nb) In introduction Line 88-91, three sentences have been written in one without punctuation `Adverse DDIs is not the absence of synergy Our findings are related to proxy discrimination, not therapeutic benefit Throughout, we put a lot of emphasis on verified assumptions,\ncautious claims and clear separation of theory vs. practice.`\n\nc) In line 817-820 in Usage of LLMs section, the sentences are repeated `Additionally, LLMs were used to polish the English grammar without altering the semantics, substantive meaning, or originality of the initial draft. Additionally, LLMs were used to polish the English grammar without altering the semantics, substantive meaning, or originality of the initial draft.`\n\nd) The full form of some abbreviations are never introduced in the paper (such as MMD in L53, RKHS MMD in L77, LaD in L97). There are many more abbreviations unintroduced and these are just a few examples. Although some of these are well-known such as MMD, but it is a common practice to introduce abbreviations and background in ML papers to make the paper complete and comprehensive in its own.\nUnfortunately, these are only few examples of the grammatical errors and typos and the paper is replete with these which make reading the paper hard.\n\nIn light of substantial issues with writing of many sections and well as technical ambiguities in the presented mathematical setup, I believe that the paper needs substantial rewriting of most sections to be considered for publication in a ML venue."}, "questions": {"value": "See the weaknesses above. Those pose many questions already. I believe the work needs a significant rewriting to make it comprehensible and exhaustive as a piece of research which can be adapted for DDI based representation learning in biomedicine."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EHEzir1NSZ", "forum": "QtFOIJGAEv", "replyto": "QtFOIJGAEv", "signatures": ["ICLR.cc/2026/Conference/Submission9869/Reviewer_ePAe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9869/Reviewer_ePAe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825902724, "cdate": 1761825902724, "tmdate": 1762921340106, "mdate": 1762921340106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a learning framework that can benefit DDI-aware combination therapy representation learning. They attempt to show its validity with different GNN backbones in the TWOSIDES dataset."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Learning representations of drug combinations that capture the constraints imposed by known DDIs is an important and well-motivated problem, as clearly explained in the first part of the introduction. The proposed framework is flexible and can be implemented with different GNN backbones."}, "weaknesses": {"value": "## Major\n\n* The paper assumes that the reader is already familiar with many mathematical concepts, without defining them or explaining their relevance to the work. This makes parts of the text difficult to follow. For example, in the sentence “Align marginals while neglecting pairwise interaction structures that should be maintained across domains,” it is unclear which marginals are being aligned, which interaction structures should be maintained, and what domains are being referred to.\n\n* Section 3 focuses on the assumptions and theorems supporting the proposed framework. However, it does not describe how to implement the framework in practice, as there is no explicit loss function, training strategy, or architectural description. It is not obvious to me how to implement the author's proposal. Under these conditions, I cannot properly verify the experimental results.\n\n* I believe that the objective of the experiments was to show that the proposed framework can significantly enhance existing architectures. However, it's difficult to assess the impact of the proposal without any comparisons with state-of-the-art models.\n\n* Furthermore, I am really worried about the fact that the GNN backbones and other architectures in the tables are not properly referenced. I observed that the author chose to reference recent surveys instead of the actual papers, below are some examples:\n\nProper references:\n\n1. GraphSAGEHamilton (2017): *Inductive representation learning on large graphs.* (https://proceedings.neurips.cc/paper_files/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf)\n\n2. GATVelickovic et al. (2018): *Graph attention networks.* (https://arxiv.org/abs/1710.10903)\n\n3. GCNKipf & Welling (2016): *Semi-supervised classification with graph convolutional networks* (https://arxiv.org/abs/1609.02907)\n\nProvided references (surveys):\n\n1. GraphSAGEMoorthy & Jagannath (2024): *Survey of Graph Neural Network for Internet of Things and NextG Networks* (https://arxiv.org/abs/2405.17309)\n\n2. GATVrahatis et al. (2024): *Graph Attention Networks: A Comprehensive Review of Methods and Applications.* (https://www.mdpi.com/1999-5903/16/9/318)\n\n3. GCNSadasivan et al. (2025): *A Systematic Survey of Graph Convolutional Networks for Artificial Intelligence Applications.* (https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.70012)\n\n* I agree that Domain Adaptation is an important strategy for representation learning in this context. But I was not convinced of why a DA regularizer is also a valuable inductive bias in the single-dataset case.\n\n* Figure 1 is not referenced in the text a single time, therefore it is not used to attempt to facilitate the description of the overall framework."}, "questions": {"value": "* What are the modifications and learning strategies that need to be implemented over the existing methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "um4O4P0l9v", "forum": "QtFOIJGAEv", "replyto": "QtFOIJGAEv", "signatures": ["ICLR.cc/2026/Conference/Submission9869/Reviewer_YPZV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9869/Reviewer_YPZV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856694159, "cdate": 1761856694159, "tmdate": 1762921339561, "mdate": 1762921339561, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles important and challenging problem- predicting drug–drug interactions in a way that stays consistent when data distribution changes. To do this, they propose to use optimal transport to align two domains and add a regularization term that preserves pairwise drug–drug relationships between domains. They also design a simplified, trainable objective that approximates this objective. They test the model on the TWOSIDES dataset and find slightly better accuracy and precision compared to baseline. The theory sections discuss existence and stability of solutions but have limited practical impact."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel approach to domain adaptation for drug–drug interaction (DDI) prediction –\nThe idea of applying optimal transport with structure -preserving DDI penalty to align domains and ensure consistency across distribution shifts is an elegant and theoretically grounded idea that addresses a real challenge in biomedical data (heterogeneous sources, batch effects, etc.).\n2. Simplified, trainable objective –The authors’ effort to design a computationally efficient approximation of optimal transport makes the method more practical and easier to train compared to directly optimizing the full formulation."}, "weaknesses": {"value": "1. Disconnection between theory and application\nThe theoretical part of the paper is presented in a highly abstract and complex manner, with limited intuition or connection to the actual problem being solved. The theoretical and empirical sections do not align well. The paper would benefit from a more coherent structure where the theory is explicitly motivated by the application and tied back to the empirical results.\n2. Limited experimental validation\nThe experimental evaluation is very limited — conducted only on a single dataset (TWOSIDES) with no cross-domain validation. Although the paper discusses domain adaptation, the experiments fail to demonstrate actual cross-domain generalization. The significance of the reported improvements is also unclear.\n3. Thermotical contributions appear incremental\nThe first claimed contribution — the existence theorem for the DDI-regularized OT formulation — is theoretically sound but rather standard, as similar results follow under mild assumptions in optimal transport theory. The second contribution, concerning the well-posedness of the surrogate objective, appears to hold only under restrictive conditions that do not reflect practical training scenarios using adaptive optimizers such as Adam.\n4. Writing quality and clarity\nThe paper’ presentation could be substantially improved. It is a bit difficult to follow due to inconsistent terminology, and excessive use of formal mathematical language without sufficient intuitive explanation. Several typos and long, vague sentences further reduce readability. The writing would be improved by using simpler, clearer, and more concise phrasing to convey key ideas.\n5. Theory–practice gap\nThe coupling-level theory and Eulerian derivations formulation feels detached from the practical implementation. The surrogate loss function does not clearly connect to the theoretical results, leaving it unclear whether the observed improvements arise from the theoretical framework or from contrastive pretraining. \n6. Weak related work section\nThe Related Work section lacks focus and does not provide a clear or accurate overview of prior research relevant to the proposed method. Much of it consists of generic summaries of standard graph neural networks (e.g., GCN, GIN, GraphSAGE, GAT) without connecting them to the DDI-aware alignment framework. Some cited works (e.g., “Meta-learning for drug combination synergy prediction,” ICML 2024, and “Multimodal deep learning for drug combination therapy prediction,” Nature Methods) appear untraceable. This section would benefit from a concise and technically grounded comparison to recent studies on DDI prediction, domain adaptation, and structure-preserving representation learning rather than broad historical overviews."}, "questions": {"value": "1 Source and Target Domains\nCould you clarify what the source and target domains represent in your experiments, given that TWOSIDES is a single dataset? How are these domains defined or separated in practice?\n\n2 Definition of the DDI-Variance Term\nHow exactly is the DDI-variance term is computed during training? In Equation (3), what is the “constant template” S_tused in V_\"DDI\" ? How is it calculated—does it correspond to an average DDI pattern in the dataset? Since the original structure-preserving regularizer involves both source and target domains, but the surrogate version (while being an upper bound on Eq. 1) depends only on a single domain, how does this surrogate actually enforce structure preservation between two domains? Additionally, how tight is this bound in practice?\n\n3 Connection Between Theory and Implementation\nHow does the theoretical coupling-level formulation quantitatively relate to the implemented loss function?  The theoretical framework (coupling-level and map-based formulations) seems disconnected from the experimental setup, which involves contrastive pretraining (“SCL+DA”). Could you clarify whether this implemented configuration directly corresponds to your formal objective in Equation (4), or whether it serves as an empirical approximation inspired by that formulation?\n\n4 Ablations on the DDI-Variance Term\nDo you provide any ablation studies or results that isolate the contribution of the DDI-variance term with MMD compared to a simpler MMD-based alignment? Also, this MMD term was never discussed in the first coupling level problem (Equation 2), does the adding of MMD would affect the existence of coupling minimizer? \n\n5 Intuition Behind the DDI Operator\nCould you elaborate on the intuition behind the DDI operator \"DDI\"(x,x^')? For example, does it encourage pairs of drugs with similar interaction profiles to remain close across domains? How is this operator defined in practice, and which drug features or representations does it depend on?\n\n6 Interpretation of the Structure-Preserving Term\nThe coupling-level problem in Equation (2) extends the classical OT objective with a structure-preserving component. How should this term be interpreted intuitively—does it act as a regularizer to maintain pairwise similarity between drugs with similar interaction profiles, or serve another purpose?\n\n7 Concrete Meaning of Domains\nCould you provide an example of what the source and target domains concretely represent in your problem setup (e.g., drug–drug interaction data from different assays, populations, or conditions)?\n\n8 Weight Selection in the Surrogate Objective\nIn Equation (4), the surrogate loss combines three terms (transport cost, MMD, and DDI variance). How are the relative weights λand μchosen? Are they tuned empirically, or determined by theoretical considerations?\n\n9 Sensitivity to Kernel Choice\nHow sensitive is the training procedure to the choice of kernel used in the MMD term?\n\n10 Training Stability and Well-Posedness\nHave you empirically observed stable convergence during training that supports your theoretical claim of well-posedness?\n\n11 Interpretation of Evaluation Metrics\nSince TWOSIDES only provides adverse DDI labels, how should improvements in your reported metrics be interpreted? Do they indicate better modeling of interaction risk or more general DDI representation learning capability?\n\n12 Clarification of “Domain Adaptation”\nThe paper repeatedly describes the method as “domain-adaptation–style,” yet all experiments are conducted on TWOSIDES alone. Could you clarify what domain adaptation specifically refers to in this context? Are the domains defined as distinct subsets of TWOSIDES, or is the domain-adaptation framing primarily conceptual (i.e., as a regularization perspective)?\n\n\n\nSome comments:\n1.\tWriting style and clarity\nThe paper’s writing style could be made substantially clear. Many ideas are expressed in highly abstract which could be expressed in a more intuitive and straightforward wayto make the paper more easy to follow. \nFor example, in the abstract, the sentence “We address these gaps with a DDI-aware, domain-adaptation–style framework that cleanly separates a population-level coupling objective from a tractable map-level surrogate” is quite opaque.\nDo you mean that the “population-level coupling objective” refers to a theoretical objective defined over data distributions, while the “tractable map-level surrogate” denotes a practical, trainable surrogate that approximates this objective via a neural mapping?\nSimilarly, in the sentence “We present a conservative and fully specified framework integrating drug–drug interaction (DDI) structure into a domain-adaptation-style (DA-style) alignment view for association combination representation learning”, does this mean your method explicitly incorporates DDI structure into the process of aligning data distributions (i.e., domain adaptation)? And does combination representation learning refer to representation learning for drug–drug interactions in combination therapy?\nAdditionally, the sentence “Under Dutch map layer and locally Lipschitz map layer and linear growth global well-posedness of induced preconditioned descent in map layer energy monotonicity is stated only under an explicit optional assumption of monotonicity of preconditioner not covering adaptive optimizers such as Adam” is extremely difficult to parse. What does “Dutch map layer” mean? I assume, in simpler terms, you mean something like: “We prove limited convergence properties under strict assumptions (e.g., Lipschitz continuity, monotonicity), but these do not apply to adaptive optimizers such as Adam.”?\n2.\tUndefined terminology\nOn line 97, LaD is not defined. Please clarify this acronym when first introduced.\n3.\tFigure 1 caption\nThe caption for Figure 1 (“Overview of the method”) would benefit from more explanation. It should clearly describe what each block and arrow represents to make the workflow understandable without referring back to the main text.\n4.\tExperimental robustness and data leakage concerns\nReporting  standard deviations and  significance analyses would strengthen the in the results. Furthermore, the relatively high baseline accuracy (≈ 0.9) on TWOSIDES suggests potential data leakage or label correlation, particularly since random pair splits may allow overlap in drug components between the training and test sets. While this issue is briefly acknowledged, it is not experimentally addressed. A scaffold-level or disjoint-compound split would provide a more rigorous evaluation of generalization.\n5.\tDisconnect between theory and experiments\nThe Limitations section restates theoretical caveats but does not integrate them with experimental observations. The coupling-level theory, map-based surrogate, and Eulerian derivations are never empirically linked, leaving it unclear whether the observed improvements arise from the proposed theoretical framework or simply from the addition of contrastive pretraining.\n6.\tData preprocessing details\nThe paper lacks a clear description of data preprocessing steps. Specifically, it is not explained how drug graphs are constructed, how DDI-related features are computed, or how negative drug pairs are generated. Providing these details would greatly improve reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NSbcSZJOrH", "forum": "QtFOIJGAEv", "replyto": "QtFOIJGAEv", "signatures": ["ICLR.cc/2026/Conference/Submission9869/Reviewer_tppf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9869/Reviewer_tppf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910542308, "cdate": 1761910542308, "tmdate": 1762921339303, "mdate": 1762921339303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper titled *“DDI-Aware Domain Adaptation for Cross-Domain Drug Combination Representation Learning via Contrastive Embedding”* appears to be largely generated by a large language model (LLM) rather than written by human authors. \n\nWhile the structure imitates a legitimate research paper (Abstract–Introduction–Theory–Experiments–Conclusion), multiple indicators point to fabricated content:\n- Dozens of non-existent or chronologically impossible references (e.g., “Kipf & Welling, 2024” for GCN; “Hamilton et al., 2022” for GraphSAGE; “Tatonetti et al., 2023” TWOSIDES update).\n- Inconsistent and incoherent theoretical sections mixing unrelated concepts such as CycleGAN surrogates, optimal transport, and Eulerian PDEs.\n- No reproducible code, data, or valid experimental design.\n- Appendix A.28 explicitly admits LLM usage for literature retrieval and writing assistance, yet many cited works are hallucinated.\n\nGiven these issues, the submission raises serious **research integrity** and **authorship authenticity** concerns. It does not meet the minimum standards of scientific validity or reproducibility for ICLR.\n\n**Recommendation:** Reject and flag for ethics and research integrity review."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The only positive aspect of this submission is its surface-level adherence to a paper structure commonly used in machine learning venues. The formatting, section organization (Abstract–Introduction–Methods–Experiments–Conclusion), and citation style mimic a legitimate research article. \n\nHowever, this structural resemblance appears to be automatically generated and does not reflect genuine scientific merit. There are no real strengths in terms of originality, soundness, or contribution.\n\nThe paper’s only strength lies in its formal adherence to conference formatting requirements."}, "weaknesses": {"value": "The weaknesses of the submission are fundamental and severe:\n\n1. **Fabricated References:** The paper cites numerous non-existent or chronologically impossible works (e.g., “Kipf & Welling, 2024”, “Hamilton et al., 2022”, “Tatonetti et al., 2023”), indicating fabricated bibliographic content.  \n2. **Incoherent Methodology:** The theoretical framework mixes unrelated concepts (CycleGANs, optimal transport, and Eulerian dynamics) in a way that suggests automatic text generation rather than genuine reasoning.  \n3. **No Reproducibility:** No code, datasets, or experimental setup are available or verifiable. Results cannot be reproduced.  \n4. **LLM-Generated Text:** Appendix A.28 explicitly admits to using large language models for literature retrieval and writing. Combined with the hallucinated references, this strongly implies large-scale AI authorship.  \n5. **Ethical and Integrity Concerns:** The work misrepresents AI-generated content as original research, violating basic research integrity standards.\n\nThese weaknesses are not superficial — they completely undermine the scientific validity of the paper."}, "questions": {"value": "1. Can you provide verifiable sources (DOIs, arXiv IDs, or URLs) for the cited works such as “Kipf & Welling, 2024”, “Hamilton et al., 2022”, and “Tatonetti et al., 2023”?  \n2. Was any part of the text, including the abstract or theoretical sections, generated using a large language model (LLM)? If yes, please clarify the extent of AI assistance.  \n3. Can you share the code, data, and experimental setup that produced the reported results?  \n4. What real-world datasets were used for the experiments? The “TWOSIDES (2023)” version mentioned does not appear to exist.  \n5. How do you ensure that the mathematical derivations correspond to actual implemented algorithms, given the lack of any empirical verification?\n\nThese clarifications are necessary to determine whether this paper represents original research or synthetic content."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)", "Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "After a detailed review of the manuscript, I found multiple strong indicators that this submission is **partly or fully generated by a large language model (LLM)** rather than being a legitimate scientific work. Here why I think that:\n\n---\n\n### 1. Fabricated and Chronologically Impossible References\n\nThe paper cites several works that either do not exist or are misdated:\n\n| **Cited Reference** | **Real Publication** | **Issue** |\n|----------------------|----------------------|------------|\n| Kipf & Welling, 2024 (GCN) | 2017 | Misdated |\n| Hamilton et al., 2022 (GraphSAGE) | 2017 | Misdated |\n| Velicković et al., 2023 (GAT) | 2018 | Nonexistent variant |\n| Xu et al., 2022 (GIN) | 2019 | Incorrect year |\n| Tatonetti et al., 2023 (TWOSIDES update) | No such update | Fabricated reference |\n| Liu et al., 2025 (Nature Methods) | No record | Hallucinated citation |\n\nSuch a pattern is characteristic of LLM-generated text where references are syntactically plausible but factually false.\n\n---\n\n### 2. Language and Stylistic Indicators of LLM Generation\n\nThe writing style strongly matches automated generation patterns:\n\n- Frequent self-referential disclaimers (e.g., “We are transparent about our assumptions…”, “We emphasize that…”).\n- Excessive repetition of phrases like “our framework,” “structure-preserving,” and “conservative assumptions.”\n- Paragraphs that follow a rigid academic template (Abstract–Intro–Theory–Experiments) but lack semantic coherence.\n- Overuse of connective phrases and soft hedging typical of language model outputs.\n\n---\n\n### 3. Internal Logical and Mathematical Inconsistencies\n\n- Theoretical sections mix unrelated concepts (e.g., *CycleGAN surrogate* with *optimal transport* theorems).\n- Use of advanced terminology (“reflexive Banach space,” “Eulerian continuity equation”) without mathematical necessity.\n- “Theorem 3.2” refers to preconditioned descent but discusses the Adam optimizer — irrelevant to the stated framework.\n- No concrete proofs, datasets, or code are provided.\n\nThese are hallmarks of an AI-generated pseudo-paper where structure is imitated but content coherence is lacking.\n\n---\n\n### 4. Explicit Admission of LLM Usage\n\n> “In preparing this work, we used large language models (LLMs) to support literature retrieval and polish the English grammar…”\n\nHowever, since many of the cited works demonstrably do not exist, this statement indicates uncontrolled or inappropriate use of generative models during manuscript preparation.\n\n---\n\n### 5. Assessment of Scientific Integrity\n\n| **Criterion** | **Status** |\n|---------------|------------|\n| Reference validity | Fabricated or incorrect |\n| Reproducibility (code/data) | None provided |\n| Mathematical rigor | Superficial, internally inconsistent |\n| Writing style | Template-based, repetitive, low cohesion |\n| Declared LLM use | Confirmed by authors |\n| Overall reliability | Extremely low |\n\n---\n\n### 6. Conclusion and Recommendation\n\nBased on the above, I strongly suspect this submission to be largely AI-generated and scientifically unreliable.\n\nI respectfully suggest that the reviewing committee investigate this paper for potential breach of authorship and academic integrity policies before further consideration."}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aGHhkwXpan", "forum": "QtFOIJGAEv", "replyto": "QtFOIJGAEv", "signatures": ["ICLR.cc/2026/Conference/Submission9869/Reviewer_QBTu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9869/Reviewer_QBTu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917475929, "cdate": 1761917475929, "tmdate": 1762921338973, "mdate": 1762921338973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a DDI-aware domain adaptation framework that integrates drug-drug interaction structure into representation learning through a two-layer approach: a coupling-level optimal transport formulation with structure-preserving penalty, and a map-level surrogate with RKHS MMD and single-domain DDI variance regularizer. The authors evaluate their approach on TWOSIDES using adverse interactions as a proxy task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "DDI-aware representation learning for combination therapy is an important and challenging problem in computational drug discovery."}, "weaknesses": {"value": "- The paper's title and abstract emphasize \"domain adaptation\" and \"cross-domain,\" but:\nNo actual cross-domain experiments are conducted (all experiments are single-dataset on TWOSIDES)\nThe authors admit: \"We adopt DA-style alignment as a regularizer; our experiments in this paper are still single-dataset and is not evaluating the cross-domain transfer\" (lines 69-70)\nThis creates misleading expectations. The paper should either conduct true cross-domain experiments (e.g., train on one DDI database, test on another) or reframe the contribution as \"DA-style regularization for single-domain representation learning\"\n\n- The paper compares against embedding methods (node2vec, edge2vec, etc.) but these are not state-of-the-art DDI prediction methods. Missing comparisons with recent DDI-specific baselines such as: [1] CARMEN: Context-Aware Safe Medication Recommendations with Molecular Graph and DDI Graph Embedding (AAAI 2023); [2] SSF-DDI: a deep learning method utilizing drug sequence and substructure features for drug-drug interaction prediction (2024); [3] DSN-DDI: an accurate and generalized framework for drug-drug interaction prediction by dual-view representation learning (2023); [4] Learning motif-based graphs for drug-drug interaction prediction via local-global self-attention.\n\n- The paper's evaluation protocol lacks clarity regarding generalization capabilities:\n\n1). Missing S0/S1/S2 breakdown: Standard DDI prediction papers evaluate three scenarios: S0 (transductive: both drugs seen in training), S1 (semi-inductive: one new drug), and S2 (fully inductive: both drugs unseen). The paper does not explicitly report results for these settings.\n\n2). Pair-level random splits without control: The authors acknowledge using \"pair-level random splits\" for comparability with baselines, but do not clarify whether they control for drug-level overlap between train and test sets. Without such control, a) Training and test sets may contain different pairs of the same drugs; b) Models can memorize single-drug properties rather than learning interaction mechanisms; c) Performance may be artificially inflated, especially in S0 settings.\n\n3). Scaffold splits dismissed too quickly: While the authors justify avoiding scaffold splits for \"comparability,\" this choice undermines generalization claims. At minimum, the paper should report S0/S1/S2 performance separately to demonstrate where the improvements come from. If gains only appear in S0 but not S1/S2, it suggests memorization rather than interaction learning.\n\n\n- The paper provides a global convergence result for the preconditioned descent method (Theorem 3.2)  but explicitly states: “do not cover common state-dependent adaptive optimizers such as Adam” (line 257-258). Yet the experiments use Adam optimizer (section 4.2). This means the theoretical guarantees do not apply to the actual optimization procedure used in practice."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i1toz6s3Uz", "forum": "QtFOIJGAEv", "replyto": "QtFOIJGAEv", "signatures": ["ICLR.cc/2026/Conference/Submission9869/Reviewer_fy9j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9869/Reviewer_fy9j"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951277016, "cdate": 1761951277016, "tmdate": 1762921338638, "mdate": 1762921338638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}