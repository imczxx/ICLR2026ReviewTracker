{"id": "avgMb57IP5", "number": 22612, "cdate": 1758333533846, "mdate": 1763603776049, "content": {"title": "New Hybrid Fine-Tuning Paradigm for LLMs:  Algorithm Design and Convergence Analysis Framework", "abstract": "Fine-tuning Large Language Models (LLMs) typically involves either full fine-tuning, which updates all model parameters, or Parameter-Efficient Fine-Tuning (PEFT), which adjusts a small subset of parameters. However, both approaches have inherent limitations: full fine-tuning is computationally expensive, while PEFT often struggles to learn new knowledge and exhibits suboptimal performance.  To overcome these issues, we propose a novel *hybrid fine-tuning* approach that jointly updates both  LLMs and PEFT modules  using a combination of zeroth-order and first-order optimization methods. To analyze our new algorithm, we develop a theoretical framework centered on the concept of *hybrid smoothness condition*, which accounts for the heterogeneous nature of the optimization landscape in joint LLM and PEFT training. We derive a rigorous convergence analysis for the convergence of reshuffling-type SGD algorithm under multiple learning rates and demonstrate its effectiveness through extensive empirical studies across various downstream tasks and model architectures. On the practical side, our results demonstrate consistent performance improvement, making the approach a viable solution for large-scale language model fine-tuning.", "tldr": "We identify a smoothness mismatch in LLM fine-tuning and propose a solution to mitigate it.", "keywords": ["peft", "large language models", "fine tuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d4bbdfe6e5adfe184fcd2130c1721b6accb4f07b.pdf", "supplementary_material": "/attachment/709c6cc4399dbc6ace637500bfbb783990ffa792.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies hybrid fine-tuning for large language models where two sets of parameters are trained together: 1) The base LLM parameters x, and a small PEFT module y. The authors observe that using the same learning rate for both is usually problematic: if the learning rate is tuned for the base model, the PEFT module learns too slowly but if it is tuned for the PEFT module, the base model becomes unstable and diverges. To explain this, they introduce a hybrid smoothness condition, which says that the loss landscape behaves very differently along the x and y directions. Using this assumption, they prove that convergence requires different learning rates for the two parameter blocks. They give upper bounds for each learning rate and show that the best practice in real LLM fine-tuning (small LR for x, larger LR for y) has theoretical justification. They conduct experiments to support the theory using prompt tuning and full fine-tuning baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1, The paper explains a real and common painpoint in LLM training: why PEFT often requires a higher learning rate than full fine-tuning. The theory directly explains the practical behavior seen in experiments.\n2, Their work reveals that “use different learning rates for base and PEFT parameters”, which is highly practical and easy for practitioners to apply.\n3, The loss curves clearly show the need for asymmetric learning rates and support the theory."}, "weaknesses": {"value": "1, The hybrid smoothness assumption is new, but the paper doesn’t fully explain where it is justified. Moreover, applying a similar argument as in the paper, one can even claim that different modules of the backbone model favors different learning rates. The paper however doesn’t touch on this point.\n2, All experiments use prompt encoder tuning on a sentiment dataset. It is not clear whether the theory holds on other tasks or other PEFT methods.\n3, The learning rate bound for y in Thm. 1 doesn’t depend on d_y, while the bound for x depends on d_x. It’s unclear from the statement why this is the case. After all, as d_y becomes larger this likely will stop to hold at some point.\n4, Many existing recipes already use different learning rates for different parameters groups. So the paper should clarify what is new about its methodology."}, "questions": {"value": "1, How confident are you in the hybrid smoothness assumption? Can you provide empirical evidence showing that geometry for x is worse than for y across multiple datasets and models?\n2, What happens if the PEFT dimension grows? Does the theory still guarantee stability when dy​ becomes large (e.g., high-rank LoRA, prefix tuning, or side-tuning)? If not, how should the learning-rate bound be modified?\n3, Can you validate on more realistic settings? Would you extend the results to common RLHF/SFT datasets, and at least one (more recently released publicly available) model larger than OPT-1.3b?\n4, Can you give a recommended ratio between learning rates of x and y, or a heuristic based on model size? How should practitioners choose the two learning rates in practice?\n5, How does the theory interact with optimizers? Would adaptive optimizers like AdamW weaken or strengthen the LR asymmetry?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ipP2Kk30Bk", "forum": "avgMb57IP5", "replyto": "avgMb57IP5", "signatures": ["ICLR.cc/2026/Conference/Submission22612/Reviewer_o27F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22612/Reviewer_o27F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761539632251, "cdate": 1761539632251, "tmdate": 1762942303246, "mdate": 1762942303246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel hybrid fine-tuning paradigm for LLMs that combines zeroth-order optimization (ZO) for the base LLM parameters with first-order optimization (FO) for PEFT modules. The authors introduce a new theoretical framework called the \"hybrid smoothness condition,\" which accounts for the heterogeneous optimization landscape of joint LLM and PEFT training. The paper provides convergence analysis for SGD under this hybrid condition and conducts extensive experiments across six NLP tasks and three LLM architectures. The proposed method demonstrates improved efficiency and competitive performance compared to traditional fine-tuning methods while maintaining memory efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The hybrid smoothness condition and the convergence analysis for SGD under this condition are significant advancements. The authors also provide sharper complexity bounds compared to prior work.\n\nThe paper evaluates the proposed method across six NLP tasks, three LLM architectures, and multiple PEFT techniques, demonstrating its broad applicability."}, "weaknesses": {"value": "Hybrid LoRA does not show significant improvements over standard LoRA in many tasks. This raises concerns about whether the added complexity of hybrid fine-tuning is justified.\n\nThe paper does not compare memory usage and performance with LoRA + Adam, which could provide a more nuanced understanding of the trade-offs between performance and efficiency."}, "questions": {"value": "Could authors provide some more experiments on LoRA + Adam (performance and memory)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eU8knxhHnq", "forum": "avgMb57IP5", "replyto": "avgMb57IP5", "signatures": ["ICLR.cc/2026/Conference/Submission22612/Reviewer_6rr7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22612/Reviewer_6rr7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761691983163, "cdate": 1761691983163, "tmdate": 1762942302987, "mdate": 1762942302987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes \"hybrid fine-tuning,\" a novel method that jointly updates a base LLM ($x$) and a PEFT module ($y$). The core idea is to use a zeroth-order (ZO) optimizer for the large base model ($x$) and a standard first-order (FO) optimizer for the small PEFT module ($y$). The authors motivate this with the \"hybrid smoothness condition\" and provide a convergence analysis, along with strong empirical results showing performance gains over FO-PEFT methods without additional memory overhead."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Method with Excellent Motivation: The core idea of combining ZO (for the base model) and FO (for the PEFT module) is a novel and highly insightful solution. The paper provides an exceptionally clear and compelling motivation, empirically demonstrating the vastly different smoothness landscapes that necessitate this hybrid, multi-learning-rate approach.\n\n2. Significant Practical Value and Strong Empirical Gains: A key achievement is that this method provides superior performance without incurring any additional GPU memory overhead (Table 2). This practical value is backed by strong and remarkably consistent experimental results.\n\n3. Solid Theoretical Contribution: The introduction of the \"hybrid smoothness condition\" (Definition 1) is a solid contribution to the optimization literature. It provides a formal language to analyze this new class of heterogeneous optimization problems, even if the resulting bounds are not yet perfectly tight."}, "weaknesses": {"value": "1.  Clarification Needed on Wall-Clock Time: The paper's efficiency claims are focused on \"steps to converge\" (Fig. 4). However, the ZO estimator (Eq. 2) requires two forward passes, implying a $\\sim$2x computational cost per step. A clarification on the real-world wall-clock time trade-off would strengthen the paper's practical claim.\n\n2. Gap Between Theoretical Bounds and Practice: There appears to be a gap between the derived theoretical bounds and the practical implementation. The proof (Theorem 2) suggests a dependency on model dimension $d_x$ (e.g., $\\eta_x \\propto 1/d_x$), which would lead to an extremely small learning rate. However, the experiments successfully use a much larger constant $\\eta_x = 10^{-6}$. This suggests the theory, while proving convergence, does not yet fully capture the practical power of the method."}, "questions": {"value": "Q1:  Regarding W1: Could the authors provide some insight into the wall-clock time comparison? Given the $\\sim$2x FLOPs per step, how does the impressive reduction in steps (e.g., in Fig. 4) translate to actual training time savings?\n\nQ2: Regarding W2: Could the authors comment on the gap between the theoretical bounds (e.g., $\\eta_x \\propto 1/d_x$) and the practical learning rates used (e.g., $\\eta_x = 10^{-6}$)? Does this suggest the practical loss landscape has a structure (e.g., sparsity, low-rank) that the theory does not yet exploit?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aRxyMdU3iF", "forum": "avgMb57IP5", "replyto": "avgMb57IP5", "signatures": ["ICLR.cc/2026/Conference/Submission22612/Reviewer_ASAc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22612/Reviewer_ASAc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932119906, "cdate": 1761932119906, "tmdate": 1762942302802, "mdate": 1762942302802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}