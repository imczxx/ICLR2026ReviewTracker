{"id": "hSnsDdSrtK", "number": 15391, "cdate": 1758250882961, "mdate": 1759897309969, "content": {"title": "Fast and Global Equivariant Attention", "abstract": "The global attention mechanism is one of the keys to the success of transformer architecture, but it incurs quadratic computational costs in relation to the number of tokens. On the other hand, equivariant models, which leverage the underlying geometric structures of problem instance, often achieve superior accuracy in physical, biochemical, computer vision, and robotic tasks, but at the cost of additional compute and memory requirments. As a result, existing equivariant transformers only support low-order equivariant features and local context windows, limiting their expressiveness and performance. This work proposes a global linear time equivariant attention, achieving efficient global attention by a novel Clebsch-Gordon Convolution on $\\SO(3)$ irreducible representations, combined with a local dense attention. Our method enables equivariant modeling of features at all orders while achieving ${O}(N \\log N)$ token complexity. Additionally, the proposed method scales well with high-order irreducible features, by exploiting the sparsity of the Clebsch-Gordon matrix. Lastly, we also incorporate optional token permutation equivariance through either weight sharing or data augmentation. We benchmark our method on a diverse set of benchmarks including QM9, n-body simulation, ModelNet point cloud classification, a geometric recall dataset and a robotic grasping dataset, showing clear gains over existing equivariant transformers in GPU memory size, speed, and accuracy.", "tldr": "This work proposes a global linear time equivariant attention mechanism, which has asymptotically optimal O(N log N) token complexity.", "keywords": ["equivariant machine learning", "point cloud transformers"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/15a7aafa7875b3c44fb006e3a5a48bbe5fdf1fdb.pdf", "supplementary_material": "/attachment/04378445da6dec9e9beb83c0887a0408a981742d.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the Clebsch-Gordan Transformer on top of the original SE(3)-Hyena, which fixes the missing permutation equivariance and enables its application to higher-order irreducible representations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I have read the theoretical derivation in this article and believe there are no problems with it; I generally believe it is correct."}, "weaknesses": {"value": "The shortcomings of this article are numerous and can be summarized as follows.\n\nFirst, the paper claims to provide open-source code at https://anonymous.4open.science/r/CG-Transformer-0D1C/. However, no implementation of CG-Transformer can be found there. The repository appears to contain code related to SE(3)-HyperHyena, but even that lacks the core model components. Such a practice—appearing to open-source code while effectively withholding it—raises serious concerns about the reproducibility of the results and undermines the credibility of the work.\n\nSecond, the motivation of the model is not entirely convincing. The authors claim two primary contributions: (1) reducing the complexity of attention, and (2) reducing the complexity of tensor product operations. Both claims are problematic. The E2Former model has already demonstrated that tensor product computations can be localized to nodes, and with global attention, the overall complexity remains linear. Moreover, it remains unclear whether tensor product operations genuinely enhance performance. Recent spherical-scalarization models (e.g., ViSNet, HEGNN, GotenNet) have shown that relying solely on inner products—whose complexity is only $\\mathcal{O}(L^2)$—is sufficient to achieve strong results. The tensor product design here therefore feels more like “innovation for its own sake” rather than a justified modeling necessity.\n\nThird, the paper fails to engage adequately with relevant prior work. Key related models such as VN-EGNN, FastEGNN (which consider global contributions), as well as recent equivariant Transformers like GotenNet and eSEN, are not discussed. This omission weakens the contextual grounding of the paper’s contributions.\n\nFinally, the experimental evaluation is insufficient. The paper omits important baselines, such as GotenNet and EquiformerV2 on QM9, and CGENN and its variants on N-body systems. For a paper emphasizing time complexity, the experiments are limited to small-scale datasets. A convincing demonstration would require evaluations on large-scale systems such as OC20 or MPtrj, where the claimed efficiency gains could be properly validated."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZMeRhH97Hl", "forum": "hSnsDdSrtK", "replyto": "hSnsDdSrtK", "signatures": ["ICLR.cc/2026/Conference/Submission15391/Reviewer_YhRc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15391/Reviewer_YhRc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723011385, "cdate": 1761723011385, "tmdate": 1762925672598, "mdate": 1762925672598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Clebsch–Gordan Transformer, a method for implementing fast, global SO(3)-equivariant attention.  \nThe key idea is to extend SE(3)-Hyena  to arbitrary irreducible representations by replacing the vector cross product with a CG tensor product. The resulting CG convolution allows the model to process higher-order features beyond scalars and type-1 tensors.\nThe authors also exploit the sparsity structure of CG coefficients to reduce computational complexity to $O(L^3)$, and employ FFT/iFFT along the token dimension to achieve $O(N log N)$ scaling."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a relevant and challenging question: achieving global and high-order equivariant attention efficiently.\n2. The exploration to the sparsity of CG coefficient is interesting."}, "weaknesses": {"value": "1. The main contribution—replacing SE(3)-Hyena’s vector cross product with general Clebsch–Gordan tensor products—is conceptually straightforward and incremental. The proposed approach is essentially a natural generalization of SE(3)-Hyena to higher-order irreducible representations, rather than a fundamentally new mechanism for equivariant attention.\n2. Although the authors claim that the CG Transformer is fast and efficient, there are no experiments demonstrating performance on long sequences or tasks with high-order input/output features. Such experiments would be necessary to substantiate the paper’s claim of scalability and efficiency.\n3. The reported results on QM9, N-Body, and ModelNet40 are not particularly strong compared to established baselines such as EGNN or SEGNN. Many more recent equivariant architectures achieve higher accuracy or lower energy prediction error, which weakens the empirical significance of this work. I would suggest authors to explore other benchmarks as these ones are more or less saturated."}, "questions": {"value": "Since the proposed method is more or less built upon SE(3)-Hyena, it would be important to provide direct comparisons with SE(3)-Hyena across all reported benchmarks.  Specifically, it would be nice if the paper include detailed results comparing (a) model accuracy, (b) computational efficiency (runtime), and (c) memory consumption. This would clearly demonstrate whether the proposed Clebsch–Gordan extension offers practical advantages by supporting higher-order representations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zdxQmyMZxV", "forum": "hSnsDdSrtK", "replyto": "hSnsDdSrtK", "signatures": ["ICLR.cc/2026/Conference/Submission15391/Reviewer_GpCg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15391/Reviewer_GpCg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906604234, "cdate": 1761906604234, "tmdate": 1762925672182, "mdate": 1762925672182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper generalizes the SE(3)-Hyena method from Moskalev et al. 2024 to support higher order spherical harmonics and efficent Clebsh-Gordan transforms to combine them."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem considered is relevant and the proposed solution is potentially impactful.\nIn this sense, the paper is well motivated and the pursued direction very interesting.\nUnfortunately, I see many major flaws in the current manuscript."}, "weaknesses": {"value": "Unfortunately, I think the current manuscript has a few major issues.\n\nFirst of all, it is claimed throughout the paper that this work improves SE(3)-Hyena by maintaining its O(N log N) complexity while also preserving permutation equivariance (a key property lost in SE(3)-Hyena). I think this could be the most impactful contribution of the paper. Unfortunately, the main paper never explains how this is achieved.\nOnly in Appendix C (never referred to from the main paper), it is explained this is done by either:\n\n  1) using the Graph Fourier Transform (instead of Hyena's FFT), which preserves the permutation equivariance. However, as far as I know, this also comes with a O(N^3) scaling to compute the eigendecomposition of the graph Laplacian, making this solution much less scalable than the attention baseline. Unfortunately, this is never discussed.\n\n  2) simply adopting SE(3)-Hyena's FFT-based solution together with data augmentation, which only provides approximate equivariance.\n\nHence, I feel like the claims in the main paper are quite misleading.\n\nMoreover, Sec 5.2 and 5.3 test the scalability of the proposed model on relative small graphs (N<=40 in 5.2 and N<=9 in 5.3). While the results are promising, this doesn't seem like a suitable benchmark to demonstrate the high scalability of the proposed method to large N. As a side note, if I remember correctly, model performance on QM9 benchmark has essentially saturated, so I am not sure it is a great benchmark anymore. Results on larger graphs like ModelNet in Sec 5.4 are instead less promising.\n\nThis makes me wonder if replacing a standard transformer in favor of the proposed method and, therefore, trading off the permutation equivariance of standard attention and enforcing SO(3) equivariance, is really a good design strategy. Indeed, the permutation group can be extremely large (N!), while the rotation group SO(3), despite being theoretically infinite, acts very smoothly on the features and is only a 3-dimenisonal Lie group; somehow, the N!-size permutation group seems to be much more relevant here."}, "questions": {"value": "Line 177: Is $W_K^l$ supposed to be $W_K^{l, l'}$ ?\n\nSec 4.3: the bra-kets notation is introduced with no proper definiton. What does it mean $|lm\\rangle$ is the basis for the $l$ representation\"? \nSec 4.3: Soon later, what is $J^2$? Is that the tensor product of the $J$-th irrep with itself? What is $J_z$? It was never defined earlier\nSec 4.3: $C^{JM}_{lm, l'm'}$ never defined properly (only $Jll'$ indeces used before).\n\nFig 3: can you fix the image size and the font? It is very hard to read it currently\n\nLine 402: \"We 3 perform...\" is there a typo maybe? The link seems also broken\n\nSec 5.4: results on ModelNet (which has many more nodes per graph) don't seem very promising, especially in ModelNet40, even when comparing with other equivairant baselines. Could the authors comment on that?\n\nIt is claimed in the main paper the somehow the permutation equviariance is preserved by the FFT-based attention but never really explained how. This seesm to be disucssed in Appendix C which is however never referred to from the main paper. Moreover, appendix C refers to appendix E for comparisons between numerous methods to enforce permutation equivariance but I couldn't find them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0TbMLDpUmd", "forum": "hSnsDdSrtK", "replyto": "hSnsDdSrtK", "signatures": ["ICLR.cc/2026/Conference/Submission15391/Reviewer_EptU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15391/Reviewer_EptU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920678824, "cdate": 1761920678824, "tmdate": 1762925671671, "mdate": 1762925671671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Clebsch-Gordan Transformer, a novel method for efficient global equivariant attention for geometric systems. It achieves this through Clebsch-Gordan long convolution, enabling log-linear scaling with context length and efficient tensor products through utilising the sparsity of the CG-product. The method also incorporates optional permutation equivariance via weight sharing or data augmentation. Experiments on tasks like n-body, QM9, ModelNet classification, and robotic grasping show improved performance and memory efficiency over some of the existing equivariant models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe context and computational efficiency of equivariant transformers severely limit their practical applicability and diminish the added value of equivariance by significantly increasing training and deployment costs. The paper thus tackles a very important problem and addresses a key limitation of modern equivariant self-attention architectures.\n2.\tThe sparsity-centric implementation of CG-product is highly advantageous for the large-scale applications where higher-order features are desirable. Especially, given that the original work of (Moskalev et al. 2025) Geometric Hyena Networks, does not focus on sparse implementation of steerable long convolutions.\n3.\tThe method potentially allows incorporating limited permutation equivariance to the convolutional mechanism, which makes it applicable to a broader class of geometric systems.\n4.\tThe proposed method is tested in a diverse array of tasks, ranging from molecules to robotics, and experimental results support the advantages of the proposed method. In addition, the paper provides extensive ablations on most of the design choices."}, "weaknesses": {"value": "1.\tThe first contribution of the work is “generalise SE(3)-Hyena method of (Moskalev et al., 2024) to include equivariant features of all types”. This is already outlined in the original follow-up work, see Appendix A5 in Geometric Hyena Networks (Moskalev et al., 2025). With this, the generalisation of SE(3)-Hyena to equivariant features of all types can not be claimed as a novel and original contribution of this work. This should be made clear throughout the paper.\n2.\tThe paper never refers Geometric Hyena Networks, which outlines steerable long-convolutional mechanism that enables all equivariant types.\n3.\tAlthough the method is designed to tackle large-scale equivariant learning, the presented experiments are rather small-scale. N-body: 40 particles, QM9: up to 9 heavy atoms, ModelNet: up to 1024 points. Only a moderate-scale experiment in the paper is Object Grasping with up to 4096 points, which is still far away from the scale where log-linear significantly outpaces quadratic complexity in terms of runtime. This creates a mismatch where paper positions itself as the method for equivariant-learning tasks that require context and compute efficiency, but only provides experiments on relatively small-scale datasets.\n4.\tThe experimental comparison against existing equivariant methods is very limited. The paper only test the proposed approach against SE(3)-Transformer, SEGNN, EGNN, and EGNN among equivariant methods.\n5.\tThe permutation equivariance section should be in the main text, as it is a significant contribution on top of long convolutions for point clouds.\n6.\tThe paper will significantly benefit from wall clock runtime and memory usage analysis (over the context length) of the proposed model over the existing methods."}, "questions": {"value": "1. The introduction states the computational complexity of NlogN, but related work (096-097) says that the proposed method is linear time. Is it a typo? Should it be log-linear?\n2. The appendix mentions data augmentation with regularisation as one of the optimal ways to achieve permutation equivariance, but does not provide any further details. Can authors elaborate on this?\n3. The proof of permutation equivariance for Fourier space attention assumes that the Laplacian eigenvectors transform canonically under permutations. Does this hold as well in cases of eigenvalue degeneracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LMqdqCsCSF", "forum": "hSnsDdSrtK", "replyto": "hSnsDdSrtK", "signatures": ["ICLR.cc/2026/Conference/Submission15391/Reviewer_TDu2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15391/Reviewer_TDu2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941827397, "cdate": 1761941827397, "tmdate": 1762925671172, "mdate": 1762925671172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}