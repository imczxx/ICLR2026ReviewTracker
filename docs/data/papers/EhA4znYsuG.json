{"id": "EhA4znYsuG", "number": 9269, "cdate": 1758116834108, "mdate": 1763709813528, "content": {"title": "EmoPrefer: Can Large Language Models Understand Human Emotion Preferences?", "abstract": "Descriptive Multimodal Emotion Recognition (DMER) has garnered increasing research attention. Unlike traditional discriminative paradigms that rely on predefined emotion taxonomies, DMER aims to describe human emotional state using free-form natural language, enabling finer-grained and more interpretable emotion representations. However, this free-form prediction paradigm introduces new challenges regarding its evaluation. Previous works depend on ground-truth descriptions, but emotions are inherently tied to diverse human behaviors, and generating a comprehensive and accurate description is inherently demanding. Other researchers reformulate this problem into a more tractable human preference learning task, but pairwise preference annotation involves substantial manual effort. This leads to a question: *can we leverage multimodal LLMs (MLLMs) to achieve more cost-efficient preference annotation?* To answer this, we propose **EmoPrefer**, a pioneering work exploring the potential of LLMs in decoding human emotion preferences. Specifically, we construct the first emotion preference dataset, **EmoPrefer-Data**, featuring high-quality preference annotations from experts. Additionally, we introduce **EmoPrefer-Bench**, which evaluates the performance of various MLLMs and prompting techniques in preference prediction, while also revealing new strategies to enhance their performance. To the best of our knowledge, this is the first work exploring the capabilities of LLMs in understanding human emotion preferences. Our work advances the field of DMER and lays the foundation for more intelligent human-computer interaction.", "tldr": "", "keywords": ["multimodal emotion recognition", "descriptive emotions", "EmoPrefer", "EmoPrefer-Data", "EmoPrefer-Bench"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b56bee2056a0bcf807a78093aace21c60c21c501.pdf", "supplementary_material": "/attachment/5f8748ac73c27c21a6d4549894c5bcba1e24eb1a.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces EmoPrefer, a benchmark designed to evaluate whether large language models can understand human emotion preferences.\nThe authors first construct a multimodal dataset, EmoPrefer-Data, by pairing two different emotion descriptions for the same video and collecting human preference annotations indicating which description better reflects the perceived emotion.\nBased on this dataset, they develop EmoPrefer-Bench, which assesses the ability of multimodal large language models to act as “judges” of emotional preference—that is, to decide which of two emotion descriptions aligns more closely with human judgment.\nThe benchmark measures both recognition accuracy and swap consistency, testing whether a model’s decision is invariant to input order."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Novelty and scope\n Proposes a first benchmark targeting emotion preference with MLLMs-as-judges, plus a new multimodal preference dataset and a clear pipeline (data, bench, metrics).\n\n2. Dataset curation\nUses overlapping videos from two descriptive MER sources, applies annotator screening and consensus filtering, and reports inter-annotator agreement; tie handling is explicitly discussed.\n\n3. Evaluation design\nDefines two complementary metrics—recognition performance (2-class/3-class WAF/ACC) and swap consistency—to capture both accuracy and order-invariance."}, "weaknesses": {"value": "1. Potential LLM–LLM evaluation loop\nThe compared descriptions originate from model-generated datasets; humans provide pairwise preferences rather than authoring gold descriptions, risking measurement of model self-consistency more than human preference.\n\n2. Limited human supervision\nSmall, filtered annotator pool and consensus-only retention reduce diversity; the reported human upper bound is modest and ties lower agreement further, constraining ceiling estimates.\n\n3. Cost and scalability\nNo systematic reporting of inference cost across S1–S4, nor analysis of error accumulation vs. chain length or budgeted evaluation settings.\n\n4. Missing ablations\nNo systematic per-modality or prompt-sensitivity ablations (e.g., audio-off, video-off, text-only), and limited analysis of temperature/seed effects for judge stability.\n\nOverall, the paper addresses a novel and relevant topic that bridges affective computing and large model evaluation.\nIt provides a useful dataset and benchmark to analyze how well MLLMs capture subjective human emotional judgments.\nHowever, since the emotion descriptions used for annotation originate from other model-generated datasets, the study risks forming a closed LLM–LLM evaluation loop, where models are effectively judging content generated by other models rather than real human descriptions. I think the author can reconsider the data construction process. It is difficult for the community to be convinced by the purely synthetic data benchmark."}, "questions": {"value": "1. Inconsistent variable naming in the swap-consistency formula\nIn the definition of the swap-consistency metric (Equation (1)), the variables “onnormal” and “onswapped” are used inside the mathematical expression. These concatenated English words are non-standard and may confuse readers.\n\n2. Unmatched quotation marks in the prompt template\nIn Figure 17 (Appendix D.3, “Details of S4”), the word 'Tie’ is enclosed with mismatched quotation marks—one straight quote and one curly quote.\n\n3. Redundant wording in contribution statements\nThe sentence “Additionally, we introduce additional strategies ...” repeats the word “additional”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "0oUBGHVFkz", "forum": "EhA4znYsuG", "replyto": "EhA4znYsuG", "signatures": ["ICLR.cc/2026/Conference/Submission9269/Reviewer_yfmT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9269/Reviewer_yfmT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761542469279, "cdate": 1761542469279, "tmdate": 1762920918276, "mdate": 1762920918276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a gap in Descriptive Multimodal Emotion Recognition: the lack of cost-efficient, reliable evaluation methods for open-ended emotion descriptions. By proposing EmoPrefer—a framework that leverages Multimodal Large Language Models to decode human emotion preferences—the work makes three notable contributions: (1) EmoPrefer-Data, the first multimodal dataset for emotion preference annotation with expert consensus; (2) EmoPrefer-Bench, a benchmark to evaluate MLLMs/prompting strategies for preference prediction; and (3) empirical insights into MLLM behavior."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- High-Quality Dataset Construction. EmoPrefer-Data addresses common pitfalls in preference datasets.\n- EmoPrefer-Bench is well-designed.\n- Two complementary metrics—recognition performance (alignment with human annotations) and swap consistency (robustness to input order)—address both accuracy and reliability, a rare focus in LLM-as-judge work."}, "weaknesses": {"value": "- The paper reports 1,368 video samples, but it is unclear how many final annotated pairs are retained after filtering for annotator consensus. For example, if ties (which have lower agreement, 59.23%) are excluded, the effective dataset size may be smaller—this impacts the statistical power of MLLM evaluations.\n- The paper does not specify the range of emotions in EmoPrefer-Data (e.g., is it balanced across positive/negative/neutral, or dominated by common emotions like anxiety/frustration?). A skewed emotion distribution could bias MLLM evaluation."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2eh6L8OT2O", "forum": "EhA4znYsuG", "replyto": "EhA4znYsuG", "signatures": ["ICLR.cc/2026/Conference/Submission9269/Reviewer_evbf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9269/Reviewer_evbf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882494843, "cdate": 1761882494843, "tmdate": 1762920917569, "mdate": 1762920917569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed EmoPrefer, which presents a pioneering exploration into whether MLLMs can serve as cost-effective evaluators of human emotional understanding.\nThis work constructed EmoPrefer-Data, EmoPrefer-Bench, and evaluation metrics. These will contribute to the study of this area.\nThe experiments show that open-source models such as Qwen2.5-Omni can rival or surpass closed-source ones (GPT-4.1, Gemini 2.5), achieving 67.2% WAF and 79.1% swap consistency. Model-based crowdsourcing further improves both performance and robustness. The authors argue this lays groundwork for emotion-aware reward modeling and emotionally intelligent AI."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is working on a novel research problem. It also presents a EmoPrefer-Data and EmoPrefer-Bench to support the research for this area.\n2. Experimental Setup is comprehensive with insightful analyses."}, "weaknesses": {"value": "1. Limited Dataset Scale and Diversity. Only 1,368 annotated pairs. The videos are only sourced from MER2024.\n2. Lack of Downstream Evaluation. No demonstration of how emotion preference prediction could enhance practical applications."}, "questions": {"value": "1. Is “swap consistency” sensitive to prompt wording or model randomness?\n2. Could the authors elaborate on how cultural bias in EmoPrefer-Data might affect model generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hoZOYrqM2k", "forum": "EhA4znYsuG", "replyto": "EhA4znYsuG", "signatures": ["ICLR.cc/2026/Conference/Submission9269/Reviewer_y5v9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9269/Reviewer_y5v9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894219004, "cdate": 1761894219004, "tmdate": 1762920916786, "mdate": 1762920916786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenges in Descriptive Multimodal Emotion Recognition (DMER), which uses free-form natural language to describe emotions for finer-grained representation. To tackle evaluation difficulties and reduce the manual effort in preference annotation, the authors propose EmoPrefer, the first exploration of leveraging multimodal large language models (MLLMs) for emotion preference decoding. They introduce EmoPrefer-Data, a high-quality expert-annotated preference dataset, and EmoPrefer-Bench, a benchmark to evaluate MLLMs and prompting techniques."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The results of the experiment are promising."}, "weaknesses": {"value": "1. The dataset's sample source is too narrow and the scenarios are too limited. All original videos are from the MER2024 dataset, and the scenarios are restricted to \"a single character facing forward with complete audio,\" lacking diverse scenarios such as multi-person interaction and complex environmental background interference.\n2. This paper does not explain the theoretical basis for choosing binary WAF as the default metric. Further correlation analysis between binary and tri-class performance should be provided (e.g., whether the performance rankings of different MLLMs are consistent under both settings) to confirm that the default metric does not distort model comparisons.\n3. Exchange consistency only quantifies whether the results are consistent before and after the order is swapped, but it does not explore the root cause of order bias. For example, is this bias caused by the model's attention mechanism or the modality fusion logic?\n4. The paper designs four prompting strategies (S1-S4) with increasing reasoning chain complexity, but does not clarify why external LLMs improve performance in S3/S4. It only attributes it to \"MLLMs may have limited language understanding,\" but does not verify whether the improvement comes from the external LLM’s stronger language reasoning ability or the multi-step reasoning’s error reduction."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XjiNcI6vBi", "forum": "EhA4znYsuG", "replyto": "EhA4znYsuG", "signatures": ["ICLR.cc/2026/Conference/Submission9269/Reviewer_uypZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9269/Reviewer_uypZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894289397, "cdate": 1761894289397, "tmdate": 1762920916268, "mdate": 1762920916268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores an interesting topic of human emotion preference, aiming to determine whether large language models can understand human preferences on different emotional representations. To this end, they construct EmoPrefer and introduce a new dataset (EmoPrefer-Data) and benchmark (EmoPrefer-Bench), supported by extensive experiments that reveal the upper-bound performance of current models. This work not only provides evaluation metrics for descriptive emotions but also advances future research toward developing more emotion-intelligent LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.This paper proposes EmoPrefer, dedicated to human emotion preference.\n\n2.This paper introduces a new dataset and a new benchmark, laying the foundations for this field.\n\n3.This paper conducts extensive experiments, revealing the upper-bound performance of current models, and proposes techniques to enhance the performance on emotion preference prediction.\n\n4.This paper has promising applications in descriptive emotion understanding and MLLM training."}, "weaknesses": {"value": "1.\tThis paper aims to investigate whether MLLMs can replace humans in decoding emotion preferences. Besides experiments on EmoPrefer-Data, it would be beneficial to further discuss the relationship between MLLMs and humans in practical applications (mentioned in Figure 12).\n\n2.\tFor EmoPrefer-Data, the authors primarily use samples with unanimous preference annotations. To better reveal human preferences, an analysis of which descriptions humans prefer most—considering factors such as emotion richness, description length, and modality coverage—would be necessary.\n\n3.\tThe paper employs win/lose/tie counts and the Bradley-Terry algorithm to derive final rankings. To the best of our knowledge, alternative approaches exist—such as using binary 1/0 scores for win/lose without detailed counts. A discussion on why this ranking method was chosen in this paper should also be provided.\n\n4.\tFigure 10 shows that recognition performance is highly dependent on the choice of k. An explanation for this phenomenon would strengthen the analysis.\n\n5.\tFigure 7 indicates that the larger LLM (Qwen3-14B) underperforms compared to the smaller LLM (Qwen2.5-7B). This unexpected finding needs further explanation.\n\n6.\tThe paper evaluates MLLMs in human emotion decoding under a zero-shot setup, with current models achieving around 70% recognition performance. While the focus of this paper is on evaluation, it would be valuable to discuss potential solutions for improving model performance in this task. Such discussion should be included in the future work."}, "questions": {"value": "See wkes"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NEM0RZXN15", "forum": "EhA4znYsuG", "replyto": "EhA4znYsuG", "signatures": ["ICLR.cc/2026/Conference/Submission9269/Reviewer_ZiTv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9269/Reviewer_ZiTv"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762402657873, "cdate": 1762402657873, "tmdate": 1762920915796, "mdate": 1762920915796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}