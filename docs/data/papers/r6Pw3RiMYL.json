{"id": "r6Pw3RiMYL", "number": 16664, "cdate": 1758267429434, "mdate": 1763729914809, "content": {"title": "On the Direction of RLVR Updates for LLM Reasoning: Identification and Exploitation", "abstract": "Reinforcement learning with verifiable rewards (RLVR) has substantially improved the reasoning capabilities of large language models. \nWhile existing analyses identify that RLVR-induced changes are sparse, they primarily focus on the **magnitude** of these updates, largely overlooking their **direction**. \nIn this work, we argue that the direction of updates is a more critical lens for understanding RLVR's effects, which can be captured by the signed, token-level log probability difference $\\Delta\\log p$ between the base and final RLVR models.\nThrough statistical analysis and token-replacement interventions, we demonstrate that $\\Delta\\log p$ more effectively identifies sparse, yet reasoning-critical updates than magnitude-based metrics (e.g., divergence or entropy).\nBuilding on this insight, we propose two practical applications:\n(1) a *test-time extrapolation* method that amplifies the policy along the learned $\\Delta\\log p$ direction to improve reasoning accuracy without further training;\n(2) a *training-time reweighting* method that focuses learning on low-probability (corresponding to higher $\\Delta\\log p$) tokens, which improves reasoning performance across models and benchmarks.\nOur work establishes the direction of change as a key principle for analyzing and improving RLVR.", "tldr": "", "keywords": ["RLVR", "LLM reasoning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3b1eb12a0bf94a52188e98c7c8bef736b8019329.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper identifies the direction of policy updates as a critical yet underexplored factor underlying RLVR’s effects. The authors show that reformulating the sampling policy using $\\Delta \\log p$ between the RL policy and the base policy yields improved performance on math-focused datasets. They also provide a concise theoretical explanation for this phenomenon and introduce a simple, training-based method that further strengthens the model.\n\nOverall, the paper is well organized and easy to follow. I enjoyed reading it and believe it offers useful insights to the community. However, the experimental evaluation is limited to math-related tasks. Expanding the experiments to other domains—such as coding, agentic tasks, or logical reasoning—would substantially strengthen the work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n2. The observed phenomenon is important and, to my knowledge, has not been systematically explored; it offers meaningful insight.\n3. The theoretical explanation is concise yet sufficiently direct to convey the key ideas. The references are appropriate, and I did not notice overclaims.\n4. The proposed methodology is simple to implement and appears effective. If the results generalize, it could be broadly useful to practitioners."}, "weaknesses": {"value": "1. The experimental setup is restricted to math-related tasks. It is unclear whether the gains extend to other tasks such as coding, agent-based evaluation, or logical reasoning.\n2. Minor issues (easy to address):\n   - The “averaged KL divergence” defined in lines 133–134 corresponds to the Jensen–Shannon divergence, which has a standard name.\n   - Figure 1(c) uses “token replacement,” but the corresponding setup is only introduced in Section 3.2. I recommend adding a hyperlink or forward reference around lines 073–077 to improve readability."}, "questions": {"value": "1. Could the authors clarify the y-axis in Figure 1(b)? It appears to be neither raw counts nor frequency (since it exceeds 1 at some points). In addition, how were the tokens collected—how many tokens, how many RL training steps, which tasks, and which algorithm?\n2. In Figure 2, replacement stops when performance reaches the RLVR value. What happens if the RLVR replacement ratio is increased further for the three curves?\n3. What is the effect of applying the test-time method on the model trained with your training-based method? Does the combination yield additional gains?\n4. Could the authors provide results on at least one non-math task, as noted above? Adding another domain would significantly strengthen the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0B5wUCNgjl", "forum": "r6Pw3RiMYL", "replyto": "r6Pw3RiMYL", "signatures": ["ICLR.cc/2026/Conference/Submission16664/Reviewer_yncQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16664/Reviewer_yncQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760883352216, "cdate": 1760883352216, "tmdate": 1762926723671, "mdate": 1762926723671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper emphasizes the direction of RLVR updates via a token-level signed metric Δlog p and shows it better localizes sparse, high-impact tokens than entropy or KL. Building on this, the authors propose (i) selective test-time extrapolation that nudges the policy along Δlog p at a small set of salient tokens, and (ii) a training-time advantage reweighting that emphasizes low-probability, reasoning-critical tokens. On math-reasoning benchmarks, both strategies improve over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Clear idea. The paper puts the direction of change (signed Δlog p) at the center, not just the size.\n2. Practical methods. The two tools—selective test-time extrapolation and training-time reweighting—are straightforward and easy to try.\n3. Low integration cost. The approach needs only log-probs from a base and an RL model; it’s a small code change with no extra labels.\n4. Token-level evidence. The replacement test shows that a small set of tokens drives most of the gains, making the story concrete."}, "weaknesses": {"value": "1. The paper asserts prior work focuses on magnitude and neglects direction, which is only partly true. While \"Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning\"（arXiv:2506.01939) is largely magnitude-centric (entropy-based selection), Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs (arXiv:2505.12929) explicitly measures for positive-advantage tokens and proposes methods to reduce direction errors(Fig3). Please revise the positioning to acknowledge signed-direction analysis and clarify that your contribution is to directionality (e.g., Δlog p) more directly.\n2. The comparison in the token‐replacement experiment may misinterpret prior works’ use of entropy or KL divergence, which aimed to guide optimization directions during training rather than to describe post‐training entropy distributions — please clarify this distinction.\n3. Hyperparameter robustness. The method relies on a gate τ (token selection) and an extrapolation strength γ (fixed at 0.1). A lack of sensitivity analysis leaves open whether gains are robust across tasks or temperatures."}, "questions": {"value": "1. Direction correctness：Could you show the rate at which positive-advantage tokens are increased and negative-advantage tokens are decreased (signed Δlog p), tracked over training, and compare this to arXiv:2505.12929\n2. Sensitivity to τ and γ (robustness): Could you share the performance across a grid of τ and γ to identify a stable operating region rather than a single tuned point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1gq1V2An1R", "forum": "r6Pw3RiMYL", "replyto": "r6Pw3RiMYL", "signatures": ["ICLR.cc/2026/Conference/Submission16664/Reviewer_L9QX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16664/Reviewer_L9QX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937348284, "cdate": 1761937348284, "tmdate": 1762926723358, "mdate": 1762926723358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that the direction of token-level updates, measured by the signed log-probability difference between base and RLVR models, is more informative than magnitude-based metrics for understanding RLVR's effect on LLM reasoning. The authors propose and validate two methods, test-time extrapolation and training-time advantage reweighting, which exploit the directional insight to improve reasoning performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a novel and intuitive directional metric that effectively captures sparse, reasoning-critical updates. The metric is supported by rigorous token-replacement experiments and gradient analysis.\n\nThe proposed methods are simple yet effective, with consistent gains across multiple models and benchmarks without the need for additional training data."}, "weaknesses": {"value": "The test-time extrapolation method requires access to both the base and RLVR models, which may limit its practicality in settings where only the fine-tuned model is available.\n\nThe paper focuses primarily on mathematical reasoning benchmarks (e.g., AIME, AMC); it remains unclear whether the findings generalize to other reasoning domains or more diverse tasks.\n\nTheoretical justification in Theorem 4.1 relies on a simplified tabular softmax bandit setting, which may not fully reflect the complexity of modern LLM training dynamics."}, "questions": {"value": "How does the proposed direction-based extrapolation perform in non-mathematical reasoning tasks?\n\nThe token-replacement experiment convincingly shows that \\Delta\\log p identifies critical tokens. However, does this intervention sometimes hurt the performance? \n\nIn which instances does replacing a base model's token with the RLVR model's choice result in an incorrect answer, and what characterizes these tokens?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K8yQQwHxaG", "forum": "r6Pw3RiMYL", "replyto": "r6Pw3RiMYL", "signatures": ["ICLR.cc/2026/Conference/Submission16664/Reviewer_aDRb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16664/Reviewer_aDRb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762181196328, "cdate": 1762181196328, "tmdate": 1762926722907, "mdate": 1762926722907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes reinforcement learning with verifiable rewards through how probabilities shift between the base and RLVR-trained models. The paper finds that upweight advantages of low-probability tokens during RL training improves reasoning accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed advantage reweighting methods is straightforward to implement. Despite the simplicity, it consistently improves reasoning performance across multiple models (Qwen2.5, Qwen3) and benchmarks (AIME, AMC)."}, "weaknesses": {"value": "1. How sensitive are performance gains to γ and τ remain unclear."}, "questions": {"value": "1. Can you provide examples of tokens with large ∆log p? What qualitative insights do they reveal?\n2. Will amplifying the penalty of negative tokens also bring benefits as indicated in this work [1]?\n\n[1] The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning. NeurIPS 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WyGExCriFp", "forum": "r6Pw3RiMYL", "replyto": "r6Pw3RiMYL", "signatures": ["ICLR.cc/2026/Conference/Submission16664/Reviewer_zikJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16664/Reviewer_zikJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16664/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762931858409, "cdate": 1762931858409, "tmdate": 1762931858409, "mdate": 1762931858409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Revision uploaded"}, "comment": {"value": "We thank all reviewers for their constructive feedback. In response, we have updated our manuscript to include additional experimental results and analyses. The new or revised content is highlighted in blue in the updated version. The main changes are summarized below:\n\n1. **Appendix A.2 (Figure 8): added per-problem accuracy curves for the token replacement experiment**, allowing a more fine-grained analysis of how increasing the replacement ratio affects model performance.\n2. **Appendix A.3 (Table 5): a new section of hyperparameter sensitivity**, where we report different hyperparameter settings and the corresponding extrapolation performance.\n3. **Appendix C (Table 6,7, and Figure 9): a new section of performance beyond pure-math reasoning tasks**, where we evaluate the proposed extrapolation and reweight method on non-math (STEM) reasoning tasks.\n4. **Appendix E (Figure 10): added word clouds of high-$\\Delta\\log p$ tokens**, providing more intuitive insight into what these influential tokens correspond to.\n5. **Rephrased texts in Section 1 and 5** to enhance clarity when comparing existing work.\n\nPlease let us know if you have any additional comments or suggestions!"}}, "id": "3vZ7lbNvHr", "forum": "r6Pw3RiMYL", "replyto": "r6Pw3RiMYL", "signatures": ["ICLR.cc/2026/Conference/Submission16664/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16664/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission16664/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763727100088, "cdate": 1763727100088, "tmdate": 1763727100088, "mdate": 1763727100088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}