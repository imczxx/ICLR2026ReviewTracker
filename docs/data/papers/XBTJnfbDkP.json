{"id": "XBTJnfbDkP", "number": 10179, "cdate": 1758163158191, "mdate": 1759897668995, "content": {"title": "Transitive RL: Value Learning via Divide and Conquer", "abstract": "In this work, we present Transitive Reinforcement Learning (TRL), a new value learning algorithm based on a divide-and-conquer paradigm. TRL is designed for offline goal-conditioned reinforcement learning (GCRL) problems, where the aim is to find a policy that can reach any state from any other state in the smallest number of steps. TRL converts a triangle inequality structure present in GCRL into a practical divide-and-conquer value update rule. This has several advantages compared to alternative value learning paradigms. Compared to temporal difference (TD) methods, TRL suffers less from bias accumulation, as in principle it only requires $O(\\log T)$ recursions (as opposed to $O(T)$ in TD learning) to handle a length-T trajectory. Unlike Monte Carlo methods, TRL suffers less from high variance as it performs dynamic programming. Experimentally, we show that TRL achieves the best performance in highly challenging, long-horizon benchmark tasks compared to previous offline GCRL algorithms.", "tldr": "", "keywords": ["reinforcement learning", "goal-conditioned rl", "offline rl"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff5706716718c08f370628bab0972b93395eaf78.pdf", "supplementary_material": "/attachment/d00ee95c14c14e35d485f82dc13bd1ec01bc053a.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a method called Transitive RL for offline goal-conditioned RL. It introduces a novel divide-and-conquer approach to learn a value function with reduced accumulated error. On the recent OGBench benchmark, it demonstrates superior performance compared to well-established baselines such as IQL and CRL."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper provides an insightful connection between $V(s, g)$ and $\\max_w V(s, w) V(w, g)$. Accordingly, the authors derive a novel divide-and-conquer value function to replace the target in Eq. 10.\n\n2. The proposed algorithm is simple and achieves strong performance on recently proposed benchmarks."}, "weaknesses": {"value": "The authors‚Äô main hypothesis is that a divide-and-conquer algorithm *might* provide a path to overcoming the curse of horizon (line 55). I would argue that this is not a well-defined hypothesis. It would be more appropriate to state that a divide-and-conquer value learning algorithm can mitigate the curse of horizon in offline goal-conditioned tasks. Accordingly, **the paper‚Äôs framing should be revised to emphasise the offline goal-conditioned setting, rather than starting broadly from off-policy RL**.\n\nI find this submission interesting; however, the presentation and narrative could be improved. The paper lacks a smooth transition from the problem statement (and background) to the proposed idea, and then to the corresponding solution."}, "questions": {"value": "1. In Table 1, TRL significantly outperforms the selected baselines. However, I encourage the authors to include more recent offline GCRL methods, such as HIQL [1] and WGCSL [2].\n\n2. It would be preferable to combine Sections 5.1 and 5.2, as they largely overlap, and move the ablation studies to the main pages.\n\n3. Section 5.1 shows that TRL performs well on large-scale, long-horizon tasks. However, this evidence alone is insufficient to demonstrate that the performance gain arises from the divide-and-conquer value learning. The authors should design more detailed experiments to verify that the value function learned by the proposed method is indeed more accurate and less prone to overestimation, as stated in line 256.\n\n**References**\n\n[1] Park, S., Ghosh, D., Eysenbach, B., \\& Levine, S. (2023). HIQL: Offline Goal-Conditioned RL with Latent States as Actions. NeurIPS. \n\n[2] Yang, R., Lu, Y., Li, W., Sun, H., Fang, M., Du, Y., ‚Ä¶ Zhang, C. (2022). Rethinking Goal-Conditioned Supervised Learning and Its Connection to Offline RL. ICLR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UJr6yASeWH", "forum": "XBTJnfbDkP", "replyto": "XBTJnfbDkP", "signatures": ["ICLR.cc/2026/Conference/Submission10179/Reviewer_6EwX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10179/Reviewer_6EwX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760709367829, "cdate": 1760709367829, "tmdate": 1762921546512, "mdate": 1762921546512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method called TRL for GCRL. The idea is to long horizon tasks by formulating them as GCRL which then allows \"dividing and conquering\" to stitch together shorter plan segments to avoid the small errors that normally stack up and ruin long-term plans."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea seems sensible and the results seem strong. It seems to make the idea of triangle inequality for value learning work, although I am not very familiar with current GCRL literature."}, "weaknesses": {"value": "**W1.** Complicated algorithm with lots of moving parts: eta-quantile, M subgoals, expectable loss, reweighting, separate \"oracle distillation\" network, policy extraction. This is many more \"moving parts\" than standard algorithms like IQL or TD-n, which could make it difficult to tune and implement.\n\n**W2.** As far as I understand it relies on oracle representations (see Appendix D.2). This seems like a significant weakness.\n\n**W3.** Structure: The triangle inequality is referred to in the abstract, introduction, related work (where there‚Äôs a whole subsection on it) etc, but only properly introduced in the middle of page 4. The triangle inequality really needs to be described earlier - in or before the dedicated discussion paragraph in related work.\n\n**W4.** The paper tests on one suite of benchmarks. It would benefit from results on another set of benchmarks.\n\n**W5.** Limited to GCRL and deterministic environments.\n\nOther weaknesses: See questions."}, "questions": {"value": "**Q1.** The authors state they take some baseline results from a prior paper (Table 1) but re-run others (Table 2). For the re-run baselines, how was fair tuning ensured? For example, the SGT-DP and COE baselines (Table 1) are also triangle-inequality methods that perform poorly - how can the authors be sure this is not due to a poor implementation or unfair tuning on their part?\n\n**Q2.** The oracle distillation seems important but it is only mentioned in Appendix D.2. Could you do an ablation of this?\n\n**Q3.** Using the BCE loss over the MSE loss seems unusual. Why was this chosen? Did you try MSE as well?\n\n**Q4.** The authors claim \"we experimentally find that behavioural subgoals can still be highly effective even when the dataset consists of uniformly random atomic motions\". Which experiments does this refer to? It would strengthen the paper to have results across different dataset types - \"random,\" \"medium,\" and \"expert\" - to support this claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9scAvzxn5K", "forum": "XBTJnfbDkP", "replyto": "XBTJnfbDkP", "signatures": ["ICLR.cc/2026/Conference/Submission10179/Reviewer_KdC6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10179/Reviewer_KdC6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763590951, "cdate": 1761763590951, "tmdate": 1762921546191, "mdate": 1762921546191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Transitive Reinforcement Learning (TRL), a new divide-and-conquer framework for value learning in offline goal-conditioned reinforcement learning (GCRL). The core idea is to exploit the triangle inequality of temporal distances to decompose long-horizon value estimation into shorter, composable subproblems. They replace the hard maximization over subgoals with soft expectile regression to mitigate overestimation bias and restrict subgoal selection to in-trajectory behavioral subgoals, which further stabilizes learning in the offline setting."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. TRL‚Äôs key strength lies in its scalability to long-horizon tasks (up to 4000 steps), where it consistently outperforms or matches the best TD- and MC-based baselines. By reducing the Bellman recursion depth to logarithmic complexity, TRL fundamentally mitigates the bias accumulation problem that plagues TD methods over long trajectories.\n2. In contrast to TD-n approaches, TRL attains superior performance without the need for laborious, task-specific tuning of the horizon parameter ùëõ, offering a more robust and parameter-free alternative for long-horizon value learning."}, "weaknesses": {"value": "1. The proposed approach fundamentally depends on deterministic environment dynamics for the triangle inequality assumption to hold. Extending the framework to learn unbiased value functions in stochastic environments remains an important and open avenue for future research.\n\n2. As shown in the ablation study, the results are highly sensitive to subgoal selection, which may introduce additional instability when applied to stochastic or noisy environments."}, "questions": {"value": "1. The ablation study indicates that restricting subgoals to behavioral in-trajectory states is essential for achieving stable performance, whereas using random subgoals leads to a substantial degradation. This sensitivity suggests a strong dependence on subgoal quality and spatial distribution. Would the authors consider exploring an adaptive or learned subgoal sampling mechanism that dynamically selects informative intermediate goals during training?\n\n2. It would be valuable to understand how TRL performs when extended beyond goal-conditioned reinforcement learning to more general reward-based RL tasks. Do the authors expect the divide-and-conquer principle to remain effective in such settings, and have any preliminary experiments been conducted in that direction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1mlm9lDTbc", "forum": "XBTJnfbDkP", "replyto": "XBTJnfbDkP", "signatures": ["ICLR.cc/2026/Conference/Submission10179/Reviewer_EYPx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10179/Reviewer_EYPx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890261742, "cdate": 1761890261742, "tmdate": 1762921545832, "mdate": 1762921545832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel goal-conditioned RL method with D&C updates. The discussion is restricted to discrete, deterministic RL problems, but I still find the work interesting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. The proposed idea is novel and interesting. I agree with the authors that this paper is a first step towards a promising direction."}, "weaknesses": {"value": "1. The authors restrict the discussion to discrete, deterministic environments with trajectory data of equal lengths, although they claim that their proposal can be extended to continuous, stochastic environments and various-length trajectories. I suggest the authors actually do such extensions and present the extended version.\n2. The tasks used in the experiments are synthetic without clear real-life purposes.\n3. Appendix A is unnecessary. It's just homework-level math."}, "questions": {"value": "1. It is mentioned that ‚Äúin practice, different trajectories can have different lengths.‚Äù I wonder about the technical details of handling trajectories of various lengths.\n2. Why is Eq. 7 a multiplication?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FQTGKKz9D7", "forum": "XBTJnfbDkP", "replyto": "XBTJnfbDkP", "signatures": ["ICLR.cc/2026/Conference/Submission10179/Reviewer_7UcZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10179/Reviewer_7UcZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928543880, "cdate": 1761928543880, "tmdate": 1762921545255, "mdate": 1762921545255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}