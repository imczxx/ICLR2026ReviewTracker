{"id": "5sAsjb2jCb", "number": 13407, "cdate": 1758217469538, "mdate": 1759897439824, "content": {"title": "PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts", "abstract": "Puzzlehunts are a genre of complex, multi-step puzzles lacking well-defined problem definitions. In contrast to conventional reasoning benchmarks consisting of tasks with clear instructions and constrained environments, puzzlehunts requires discovering the underlying problem structure from multimodal evidence and iterative reasoning, mirroring real-world domains such as scientific discovery, exploratory data analysis, or investigative problem-solving. Despite progress in foundation models, their performance on open-ended settings remains largely untested. We introduce PuzzleWorld, a comprehensive benchmark of 667 puzzlehunt-style problems designed to assess step-by-step, open-ended, and creative multimodal reasoning. Each puzzle is annotated with the final solution, detailed reasoning traces, and cognitive skill labels, enabling holistic benchmarking and fine-grained diagnostic analysis. Most state-of-the-art models achieve only 1-4\\% final answer accuracy. On PuzzleWorld, the best model solves only 14\\% of puzzles and reaches 40\\% stepwise accuracy, matching human puzzle novices but falling significantly behind puzzle enthusiasts. To demonstrate the value of our reasoning annotations, we show that fine-tuning a small model on reasoning traces boosts stepwise accuracy from 4\\% to 11\\%, which translates to improvements in downstream visual reasoning tasks. Our detailed error analysis reveals that current models exhibit myopic reasoning, are bottlenecked by the limitations of language-based inference, and lack sketching capabilities crucial for visual and spatial reasoning. We will publicly release PuzzleWorld to support future work on building more general, open-ended, and creative reasoning systems.", "tldr": "We introduce PuzzleWorld, a benchmark of 667 puzzlehunt problems to test open-ended, multimodal AI reasoning, and find that models perform poorly, highlighting gaps in their creative problem-solving abilities in non-constrained environments.", "keywords": ["Benchmarking", "Foundation Models", "Multimodal Reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/829b648826e2037df676a503b9869423d65533a0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "PUZZLEWORLD introduces a benchmark of 667 puzzlehunt-style problems designed to evaluate multimodal, open-ended reasoning in AI systems. Unlike conventional benchmarks with well-defined tasks, puzzlehunts require discovering problem structures from ambiguous, multimodal clues, mirroring real-world challenges like scientific discovery or investigative problem-solving. The dataset includes detailed annotations of solutions, step-by-step reasoning traces, and cognitive skill labels. Evaluations show state-of-the-art models achieve only 1–14% final answer accuracy, with the best model (GPT-o3) matching novice human performance but lagging behind enthusiasts and experts. The benchmark also enables diagnostic analysis, revealing model limitations in backtracking, visual reasoning, and sketching capabilities. Fine-tuning on reasoning traces improves stepwise accuracy, demonstrating the utility of PUZZLEWORLD for advancing general-purpose reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. PUZZLEWORLD fills a critical gap by focusing on open-ended, discovery-driven problems rather than constrained tasks. Its emphasis on multimodal clues and unstructured problem-solving aligns with real-world scenarios, providing a more holistic evaluation of reasoning capabilities.\n2. The dataset includes meticulously curated step-by-step reasoning traces, cognitive skill labels, and modality tags. These annotations support fine-grained diagnostics, error analysis, and model training, surpassing existing benchmarks in depth and utility.\n3. The study comprehensively tests frontier models (e.g., GPT-o3, Claude Opus) and human baselines, highlighting significant performance gaps. Stepwise accuracy metrics offer nuanced insights beyond final answers, revealing intermediate reasoning failures."}, "weaknesses": {"value": "1. I think the experimental analyses are not sufficient. For example, we see that almost all MLLMs perform poorly, but the reasons behind this are worth further investigation. Is the model's poor performance due to poor performance in the visual modality or poor performance in the text modality? In my experience, most MLLMs are unable to perform well in downstream tasks due to their insufficient ability to recognize text in images.\n2. The article lacks discussion of more related work, such as [1].\n3. I look forward to seeing more case studies.\n\n[1] LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles."}, "questions": {"value": "See the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G5xYzcYoSO", "forum": "5sAsjb2jCb", "replyto": "5sAsjb2jCb", "signatures": ["ICLR.cc/2026/Conference/Submission13407/Reviewer_s7Za"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13407/Reviewer_s7Za"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761101789330, "cdate": 1761101789330, "tmdate": 1762924039237, "mdate": 1762924039237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a dataset of 667 \"puzzle hunt\"-style puzzles, complete with detailed annotations of reasoning for how to properly solve the puzzle. The authors discuss how this dataset is constructed, and how the annotations are carefully done to ensure no ambiguity in puzzle answers. They evaluate frontier models on the puzzles, and report the performance of both closed- and open-source models. The paper then investigates the effect of fine-tuning on reasoning traces on better performance on other puzzles, and expounds a detailed error analysis of current-generation VLMs."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I really like this dataset, both in terms of the intrinsic open-endedness of the puzzles, and the fact that the puzzles have an unambiguous and easy-to-evaluate final answer; I think this is a strong contribution to the field. I also appreciate the attention paid to making the dataset correct and unambiguous. The paper itself is well-written, and the figures are informative. I like the detailed analysis in section 5.4, and appreciate the contamination check in C.1. In all, this is a valuable dataset which was constructed carefully, and an excellent paper analyzing current performance."}, "weaknesses": {"value": "No obvious weaknesses beyond the classic advice that more data points would be better :) Also, it would be nice to re-evaluate on all the new models that came out since the submission deadline, if that's not too hard!"}, "questions": {"value": "Would the MIT Mystery Hunt archives be a good source to expand a dataset like this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tl2ZPvx1Xe", "forum": "5sAsjb2jCb", "replyto": "5sAsjb2jCb", "signatures": ["ICLR.cc/2026/Conference/Submission13407/Reviewer_vb72"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13407/Reviewer_vb72"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891218282, "cdate": 1761891218282, "tmdate": 1762924038838, "mdate": 1762924038838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PUZZLEWORLD, a benchmark of 667 real-world puzzlehunt problems for evaluating multimodal, open-ended, multi-step reasoning. Each puzzle includes detailed reasoning traces, modality and cognitive-skill labels, enabling fine-grained diagnostic analysis. Experiments show that even frontier models achieve only 1–14% final accuracy, while fine-tuning on reasoning traces improves stepwise accuracy from 4.8% to 11%. The benchmark is clear, rigorous, and highly relevant to general reasoning research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) Proposes the first large-scale open-ended puzzlehunt benchmark, transforming real Puzzled Pint puzzles into machine-readable tasks that test discovery-driven reasoning.\n\n(2) A two-stage GPT-4o + human verification pipeline ensures 96.5% correctness and no contamination.\n\n(3) Evaluates eight major models with both final and stepwise metrics, identifying systematic weaknesses (myopic reasoning, language bottleneck, lack of sketching).\n\n(4) Reasoning-trace supervision improves intermediate reasoning and transfers to Rebus and MathVista tasks."}, "weaknesses": {"value": "(1) The paper omits discussion of FINEREASON (Chen et al., 2025; arXiv:2502.20238), which also studies reflective puzzle reasoning through step decomposition. Comparing PUZZLEWORLD’s open-ended, multimodal puzzles with FINEREASON’s structured logic ones would clarify its unique scope.\n\n(2) Limited ablations on annotation noise and prompting baselines; fine-tuning details appear only in the appendix.\n\n(3) Dataset excludes audio/video modalities and depends on OCR, limiting breadth."}, "questions": {"value": "– Please add a short discussion contrasting PUZZLEWORLD with FINEREASON, emphasizing that PUZZLEWORLD extends structured reflective reasoning to multimodal, discovery-driven contexts.\n\n– Evaluate robustness to imperfect annotations and potential cross-benchmark transfer.\n\n– Future work could merge PUZZLEWORLD’s multimodal puzzles with reflective reasoning frameworks for richer diagnostics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GSw7EGIiUt", "forum": "5sAsjb2jCb", "replyto": "5sAsjb2jCb", "signatures": ["ICLR.cc/2026/Conference/Submission13407/Reviewer_NwBy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13407/Reviewer_NwBy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006090620, "cdate": 1762006090620, "tmdate": 1762924038483, "mdate": 1762924038483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PUZZLEWORLD, a benchmark compiles 667 real puzzlehunt problems to test open-ended multimodal reasoning, adding stepwise solution traces, modality and skill tags, and an LLM-judge for intermediate scoring; frontier models perform poorly overall (best ≈14% answer accuracy, ≈40% stepwise), fine-tuning on reasoning steps improves stepwise scores but not final accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- open-ended puzzles obtained from real world, filling a current research gap. \n\n- the paper demonstrated a clear performance gap that stresses present models; best model only has ~14% answer accuracy"}, "weaknesses": {"value": "- the work is very interesting, but unfortunately not very thoroughly done, for examples:\n    - The “easy/medium/hard” tags come from original puzzle metadata rather than a benchmark-defined rubric. There is no formal difficulty calibration, no solver-time distribution analysis, and no additional validation along the annotation. \n\n   - Humans are grouped into novice/enthusiast/expert tiers, but there is no modality-level breakdown and no step-level scoring symmetry with models. In addition, the human evaluation is only for 5% of the samples, thus the comparison in table 2 is not as informative as it appears. \n\n- The benchmark comes with one gold standard chain per puzzle , yet there is a chance the puzzlehunts can have multiple legitimate solve paths. There is no mechanism for crediting alternative correct partial chains or heuristic leaps, thus the evaluation of the reasoning paths can be quite limited.\n\n- the significance of the benchmark may be limited in the sense that the problems and solutions are quite scoped with the culture of puzzlehunt. These specialized cognitive challenges might not well generalize to a broader scope."}, "questions": {"value": "I will recommend the authors to increase the rigor of the benchmark, as discussed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YhRW0AKjyq", "forum": "5sAsjb2jCb", "replyto": "5sAsjb2jCb", "signatures": ["ICLR.cc/2026/Conference/Submission13407/Reviewer_8chG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13407/Reviewer_8chG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088318309, "cdate": 1762088318309, "tmdate": 1762924038090, "mdate": 1762924038090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}