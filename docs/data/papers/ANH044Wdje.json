{"id": "ANH044Wdje", "number": 14126, "cdate": 1758228821480, "mdate": 1759897388559, "content": {"title": "Generalizing Linear Autoencoder Recommenders with Decoupled Expected Quadratic Loss", "abstract": "Linear autoencoders (LAEs) have gained increasing popularity in recommender systems due to their simplicity and strong empirical performance. Most LAE models, including the Emphasized Denoising Linear Autoencoder (EDLAE) introduced by (Steck, 2020), use quadratic loss during training. However, the original EDLAE only provides closed-form solutions for the hyperparameter choice $b = 0$, which limits its capacity. In this work, we generalize EDLAE objective function into a Decoupled Expected Quadratic Loss (DEQL). We show that DEQL simplifies the process of deriving EDLAE solutions and reveals solutions in a broader hyperparameter range $b > 0$, which were not derived in Steckâ€™s original paper. Additionally, we propose an efficient algorithm based on Millerâ€™s matrix inverse theorem to ensure the computational tractability for the $b > 0$ case. Empirical results on benchmark datasets show that the $b > 0$ solutions provided by DEQL outperform the $b = 0$ EDLAE baseline, demonstrating that DEQL expands the solution space and enables the discovery of models with better testing performance.", "tldr": "", "keywords": ["linear autoencoders", "recommender system", "closed-form solution", "expected quadratic loss"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bb4fb91f22b1016a5c2b80c624182ab6bfc2cece.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses a key limitation in a state-of-the-art linear recommendation model, the 'Emphasized Denoising Linear Autoencoder' (EDLAE). The original EDLAE work only provided a closed-form solution for the specific hyperparameter case of b=0, which limited the model's capacity.\n\nThe authors introduce the 'Decoupled Expected Quadratic Loss' (DEQL), a generalized objective function that reformulates the EDLAE problem from a statistical perspective. The main contributions of this work are:\n\n1. **Theoretical Generalization:** Using the DEQL framework, the authors derive a general closed-form solution (minimizer) for the expected quadratic loss.\n2. **Expanded Solution Space:** They apply this general solution back to the EDLAE objective, successfully deriving closed-form solutions for the previously unexplored hyperparameter regime of b>0.\n3. **Efficient Algorithm:** Recognizing that a naive computation of these new b>0 solutions is computationally prohibitive ($O(n^4)$), the authors propose a novel and efficient algorithm based on the Miller matrix inversion theorem.\n4. **Complexity Reduction:** This algorithm reduces the complexity to $O(n^3)$, making it practical and comparable to existing methods like EASE and the b=0 EDLAE baseline.\n5. **Empirical Validation:** Extensive experiments on six benchmark datasets show that the newly derived solutions for b>0 (especially when combined with $L_2$ regularization, termed DEQL(L2)) consistently and significantly outperform the original b=0 EDLAE baseline.\n6. Novel Insight: The paper also provides an interesting insight that the explicit zero-diagonal constraint is not strictly necessary for strong performance when b>0."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **Originality and Theoretical Quality:** The generalization of the EDLAE objective into the Decoupled Expected Quadratic Loss (DEQL) is an elegant and novel theoretical contribution. It provides a clearer and more principled statistical framework for understanding this class of models. Deriving the closed-form solution for the b>0 case is a non-trivial and significant extension of prior work.\n- **Practical Algorithmic Contribution:** A major strength is that the authors did not stop at the computationally intractable $O(n^4)$ solution. The development of the $O(n^3)$ algorithm using the Miller matrix inversion theorem is a crucial contribution that makes the entire theoretical framework practical and usable. This new algorithm maintains the same complexity as the original EASE and EDLAE (b=0) models.\n- **Clarity and Presentation:** The paper is extremely well-written and easy to follow. The motivation is clear, and the logical flowâ€”from the limitation of EDLAE, to the general DEQL framework, to the specific b>0 solution, and finally to the efficient algorithmâ€”is impeccable. The mathematical derivations are clean and well-explained.\n- **Significance and Empirical Results:** The empirical validation is thorough and convincing. The proposed b>0 solution (specifically DEQL(L2)) demonstrates consistent and sometimes substantial performance gains over the strong EDLAE baseline across all six datasets. The paper also reinforces the value of simple linear models, showing they remain competitive and, in some cases, superior to complex deep learning-based models while offering significantly faster (CPU-based) training.\n- Novel Insight: The finding (RQ2) that the zero-diagonal constraintâ€”essential in models like EASE and EDLAEâ€”is not strictly necessary when b>0 and $L_2$ regularization is applied is an important insight for future model development in this area."}, "weaknesses": {"value": "1. **Lack of Theoretical Guidance on Hyperparameters:** The DEQL framework successfully expands the solution space to b>0, but it does not provide theoretical guidance on *how* to select the optimal hyperparameters (e.g., the ratio b/a, p, or the $L_2$ coefficient $\\lambda$). Navigation of this newly expanded solution space still relies on empirical grid search, which can be costly. The paper proves *that* b>0 solutions are better, but not *which* b>0 solution is optimal a priori.\n2. **Computational Bottleneck:** While the $O(n^3)$ algorithm is a significant achievement, it still relies on the inversion of an $n \\times n$ matrix ($H_0$ or $H_0 + \\lambda I$). For datasets with a very large number of items (n), this $O(n^3)$ step remains a significant bottleneck, limiting its scalability into the high tens of thousands. This is an inherent limitation of this class of closed-form LAE models, but it remains a practical weakness.\n3. **'Pure' DEQL Performance:** The core theoretical contribution is the 'pure' DEQL solution (Eq. 9). However, the experimental results in Table 1 clearly show that this pure DEQL model performs poorly and is often worse than the EASE/EDLAE baselines. The strong performance gains are only realized *after* adding $L_2$ regularization (DEQL(L2)) or both $L_2$ and the zero-diag constraint. This suggests the primary practical contribution is not DEQL *per se*, but *regularized* DEQL. This is not a major flaw, but the paper's framing could be slightly more precise to reflect that the b>0 solution *requires* regularization (like $L_2$) to be effective, though this is standard practice anyway.\n4. The work's key contribution is performance uplift. However, the experimental gains shown in Table 1 are not statistically significant. For example, on ML-20M, Recall@20 improves from 0.3925 to 0.3934, and NDCG@20 from 0.3421 to 0.3426. On Netflix, the performance is identical. On MSD, the b=0 EDLAE baseline actually outperforms the proposed DEQL(L2+zero-diag). These minor and inconsistent gains hardly justify adopting a new solution, let alone the extra hyperparameter tuning for b.\n5. Concurrent research is working to break this cubic complexity bottleneck, e.g., via scalable iterative methods, graph-based approximations, or randomized linear algebra. The paper's contribution lacks some theoretical or experimental comparisons in terms of effectiveness and complexity with these works.\n6. The paper adds a new hyperparameter (b) to an already complex tuning space (dropout p, $L_2$ $\\lambda$). This runs counter to the broader research trend of seeking simplification for robust performance. Other recent work suggests that the dropout and $L_2$ terms already provide sufficient regularization, making an explicit b parameter slightly redundant."}, "questions": {"value": "1. **Sensitivity to b:** Related to Weakness 1, how sensitive is the model's performance to the choice of b (or the b/a ratio)? The grid search ranged from [0.1, 2.0]. Could you provide a brief analysis (perhaps with a plot) on the validation set showing how Recall@20 varies as b changes? Is there a clear 'sweet spot,' or is performance relatively flat across a wide range of b>0?\n2. **Statistical Significance:** The experimental gains in Table 1 (ML-20M, Netflix) appear to be well within the margin of error. Can the authors please provide statistical significance tests (e.g., paired t-tests) across multiple runs to demonstrate that the b>0 solutions are *truly* superior to the b=0 baseline? As it stands, the evidence suggests the gains are just noise from tuning an extra parameter.\n3. **Learned Diagonal Values:** You argue convincingly in RQ2 that the zero-diagonal constraint is not strictly necessary when b>0 and $L_2$ regularization is used (DEQL(L2)). This is a very interesting finding. In the DEQL(L2) model, what do the diagonal entries of the learned $W^*$ matrix look like? Are they small values close to zero (suggesting the $L_2$ norm effectively pushes them down), or do they take on meaningful non-zero values? This could offer insight into *why* the hard constraint is no longer needed.\n4. **Ablation of b vs. $\\lambda$:** The 'naive' DEQL (Eq. 9) performs poorly, and strong results are only obtained with DEQL(L2). This suggests the $L_2$ regularization is doing the heavy lifting, not the b>0 formulation. Could the b=0 EDLAE baseline achieve the same performance as DEQL(L2) if its $L_2$ hyperparameter ($\\lambda$) was simply tuned more aggressively over a wider range?\n5. **Scalability and Approximation:** The $O(n^3)$ inversion of $H_0$ is the main bottleneck. Have the authors considered or can they comment on using approximation techniques for $(H_0 + \\lambda I)^{-1}$? For example, would an iterative solver (e.g., Conjugate Gradient) or a low-rank approximation of $H_0$ be a viable path for scaling this method to n >> 50,000 items? This would break the pure 'closed-form' nature but might offer a practical path to larger datasets.\n6. Generality of the Algorithm: The efficient $O(n^3)$ algorithm relies on $H^{(i)}$ being a low-rank (specifically, rank-2) update to a common matrix $H_0$. This structure arises from the specific dropout/emphasis scheme of EDLAE. Is this low-rank structure a fundamental property of DEQL, or just a property of this specific instantiation? In other words, could other formulations of DEQL (i.e., different choices for $\\mathcal{D}^{(i)}$ in Def 3.1) lead to $H^{(i)}$ matrices that do not have this structure, making the Miller theorem speedup inapplicable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pp4SS4zIRa", "forum": "ANH044Wdje", "replyto": "ANH044Wdje", "signatures": ["ICLR.cc/2026/Conference/Submission14126/Reviewer_9r8b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14126/Reviewer_9r8b"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761307741740, "cdate": 1761307741740, "tmdate": 1762924593228, "mdate": 1762924593228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript proposes a generalization of EDLAE named Decoupled Expected Quadratic Loss (DEQL), which extends the theoretical formulation and provides closed-form solutions for a wider range of hyperparameters range b>0. Moreover, this manuscript propose an efficient algorithm based on Millerâ€™s matrix inverse theorem to to reduce the computational complexity for the b > 0 case. Extensive experiments demonstrate the advantages of DEQL on multiple benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly identifies a key limitation of the existing EDLAE model, the lack of analytical solutions for the case b>0. By introducing the DEQL formulation, the authors successfully generalize EDLAE and provide new theoretical insights into the design of expected quadratic loss functions. The proposed formulation is both mathematically and conceptually well-grounded.\n2. The authors provide detailed derivations, lemmas, and proofs to support the theoretical claims. The transition from the expected quadratic loss formulation to the closed-form solution is rigorous and contributes to a deeper understanding of linear autoencoder objectives. \n3. The introduction of the Miller-based algorithm significantly improves computational efficiency, reducing complexity from O(n4) to O(n3). This makes DEQL practical for large-scale recommendation datasets.\n4. The datasets used are publicly available, and the implementation details are well described. The code link is also provided, which enhances transparency and reproducibility."}, "weaknesses": {"value": "1. The entire paper contains no figures or diagrams to illustrate the overall framework, problem formulation, or intuition behind DEQL. The heavy use of equations makes it difficult for readers to intuitively understand the workflow and conceptual contribution. Adding schematic diagrams of the model or algorithm would greatly improve readability.\n2. As the proposed method is a linear autoencoder-based (LAE-based) method, the most recent baseline considered in the paper is EDLAE (2020). Including more up-to-date LAE-based models for comparison would make the experimental evaluation more convincing and better demonstrate the advantages of the proposed DEQL framework..\n3. The paper mentions in the experimental section that only the ratio b/a affects the closed-form solution. Therefore, the authors fix ð‘Ž a and perform a grid search over b. Since the paper emphasizes that bâ‰¥0 enables exploration of a larger solution space, it would be important to further analyze how different values of b influence the experimental results. A detailed discussion or sensitivity analysis on b would strengthen the empirical section and clarify its effect on model performance."}, "questions": {"value": "1. The entire paper contains no figures or diagrams to illustrate the overall framework, problem formulation, or intuition behind DEQL. The heavy use of equations makes it difficult for readers to intuitively understand the workflow and conceptual contribution. Adding schematic diagrams of the model or algorithm would greatly improve readability.\n2. As the proposed method is a linear autoencoder-based (LAE-based) method, the most recent baseline considered in the paper is EDLAE (2020). Including more up-to-date LAE-based models for comparison would make the experimental evaluation more convincing and better demonstrate the advantages of the proposed DEQL framework..\n3. The paper mentions in the experimental section that only the ratio b/a affects the closed-form solution. Therefore, the authors fix ð‘Ž a and perform a grid search over b. Since the paper emphasizes that bâ‰¥0 enables exploration of a larger solution space, it would be important to further analyze how different values of b influence the experimental results. A detailed discussion or sensitivity analysis on b would strengthen the empirical section and clarify its effect on model performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0dASQMLGQ0", "forum": "ANH044Wdje", "replyto": "ANH044Wdje", "signatures": ["ICLR.cc/2026/Conference/Submission14126/Reviewer_bLSH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14126/Reviewer_bLSH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833635583, "cdate": 1761833635583, "tmdate": 1762924592800, "mdate": 1762924592800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a specific limitation in a previously published model (EDLAE). The main contribution is the extension of the model's analytical solution to a broader set of conditions ($b>0$), which was not covered in the original work. This extension required a derivation and the development of a computationally feasible algorithm. The empirical results, showing improved performance, suggest that this extension is not merely a theoretical exercise but has practical benefits. The findings are of potential interest to researchers working on computationally efficient and effective models.\n\nThe paper's claims are supported by the presented derivations and experiments. The derivation of a closed-form solution for the EDLAE objective with $b>0$ is followed step-by-step, and the algorithm to reduce its complexity is well-explained. The experimental setup is described with sufficient detail to understand the procedure: the authors use publicly available datasets, specify the evaluation metrics (Recall@20, NDCG@20), and list the baseline models they compare against. This provides a basis for evaluating the empirical claims.\n\nThe paper is structured logically and is generally well-written. It begins with a clear problem statement, moves through the proposed mathematical solution and algorithmic details, and concludes with empirical results. The authors provide context by citing and briefly describing related works in linear autoencoders. However, the clarity could be slightly improved in a few areas. The presentation could be improved by adding more explicit motivation for certain design choices, such as the preference for a closed-form solution. The introduction of the DEQL framework could also be clarified by first showing how the underlying property of decoupling arises from the original objective function's structure."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper successfully generalizes the closed-form solution of the EDLAE objective from the special case of $b=0$ to the more general case of $b>0$, which increases the modelâ€™s flexibility.\n\nThe authors developed a practical algorithm for their proposed solution. They designed a method that reduces the computational complexity from $O(n^4)$ to $O(n^3)$, which makes their approach applicable to the datasets used in their experiments.\n\nThe claims are supported by experiments on multiple public datasets. The results, as shown in Tables 1 and 2, consistently demonstrate performance gains for the proposed method over the primary baseline (EDLAE) and show its competitiveness against other listed models.\n\nThe work provides further evidence that properly formulated linear models can be highly effective for recommendation tasks, sometimes outperforming more complex deep learning models, particularly on sparse data. This is a relevant finding for the field."}, "weaknesses": {"value": "Limited Motivation for the Chosen Approach: The paper does not explicitly justify why a closed-form solution is preferable to a standard gradient-based optimization for this problem. While experts in the subfield might understand the rationale, a broader audience would benefit from a discussion of the trade-offs (e.g., computational cost, guarantee of finding a global optimum, reproducibility).\n\nLack of Analysis of the Key Hyperparameter: While the authors state in Section 5.2 that they perform a grid search over the hyperparameter $b$ to find the optimal values for their experiments, the paper does not present an analysis of these results. The reader is not shown how the model's performance changes as the $b/a$ ratio is varied. An empirical sensitivity analysis would be necessary to understand the practical importance of this new degree of freedom and to provide guidance on how to tune it.\n\nPotentially Unclear Framing of the \"Decoupling\": The DEQL framework is presented as a novel formulation. However, the decoupling of the loss function is an intrinsic property of the Frobenius norm and the structure of the EDLAE objective itself. The presentation could be improved by first demonstrating this property in the original objective and then introducing DEQL as a generalized term for such decoupled loss functions."}, "questions": {"value": "Can you elaborate on the practical advantages of finding a closed-form solution versus using an iterative method like SGD for this specific problem, particularly concerning training time, final model performance, and ease of tuning?\n\nHow sensitive is the model's performance to the choice of the $b/a$ ratio? Could you provide an empirical analysis (e.g., a plot of performance vs. $b/a$ on a validation set for one or two datasets) to illustrate the impact of this new hyperparameter?\n\nThe statistics provided under Tables 1 and 2 are helpful. To further support the discussion on how data characteristics like sparsity influence model performance, could you add a density (or sparsity) column to these existing summary tables? This would allow readers to directly correlate the quantitative characteristics of the datasets with the empirical results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1o32DtiZkm", "forum": "ANH044Wdje", "replyto": "ANH044Wdje", "signatures": ["ICLR.cc/2026/Conference/Submission14126/Reviewer_XT64"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14126/Reviewer_XT64"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983273888, "cdate": 1761983273888, "tmdate": 1762924592306, "mdate": 1762924592306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper generalizes the Emphasized Denoising Linear Autoencoder (EDLAE) by introducing Decoupled Expected Quadratic Loss (DEQL), which reformulates EDLAE under a statistical expectation perspective and extends the solution space to the full hyperparameter range $b\\geq 0$. Unlike the original EDLAE, which only derived a closed-form solution for $b=0$, the authors provide theoretical derivations for $b>0$ and propose an efficient algorithmâ€”based on Millerâ€™s matrix inverse theoremâ€”to reduce computational complexity from $O(n^4)$ to $O(n^3)$ . Experiments on standard recommendation datasets show modest improvements over EDLAE and other linear baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear theoretical generalization of EDLAE. Reformulating EDLAE as an expected loss (DEQL) provides a cleaner statistical foundation and yields closed-form solutions for b>0, which were missing in prior work.\n\n2. Efficient algorithmic adaptation. The use of Millerâ€™s theorem reduces computation from $O(n^4)$ to $O(n^3)$, making the extended solution computable in practice.\n\n3. Empirical improvements. Experimental results show that DEQL with $b>0$ offers consistent but moderate accuracy gains over EDLAE."}, "weaknesses": {"value": "1. Limited novelty in the ICLR context. The main contribution is an extension of an existing model (EDLAE) from $b=0$ to $b>0$. While theoretically meaningful, this is somewhat incrementalâ€”more aligned with recommender system venues (KDD, SIGIR, RecSys) than a general ML conference like ICLR.\n\n2. Utilization of existing mathematical tools. The core efficiency improvement relies on Millerâ€™s matrix inverse theorem rather than a new theoretical contribution. The innovation lies in applying this theorem to EDLAEâ€™s structure, not in developing a new algorithmic principle.\n\n3. Scalability remains unsolved. Even after reducing the complexity to $O(n^3)$, the proposed method still requires computing and storing an $N\\times N$ item-item co-occurrence dense matrix $R^\\top R$, similar to its peers like EASE or SLIM. For large item catalogs, the method remains computationally and memory prohibitive. Thus, the method does not overcome the fundamental scalability bottleneck of linear autoencoders.\n\n4. Limited practical relevance of SAE-based recommenders in modern RS community. While the paper extends the family of Sparse/Linear Autoencoder-based models (SAE4Rec), such methods have largely fallen out of favor in industry-scale recommender systems. In practice, deep learning-based recommenders have generally surpassed linear autoencoders in both representation power and real-world deployment. Although proposes selective empirical results of deep-model baselines, the paper does not address whether DEQL can bridge this gap, scale to large industrial settings, or offer practical advantages beyond incremental improvements over existing SAE models."}, "questions": {"value": "Please refer to Weakness. Besides, the reviewer would also like to discuss the following interpretability potential: Linear/Sparse Autoencoders are recently used as interpretable models rather than competitive recommenders. Could DEQL be used as an explanation module? Does the closed-form solution provide interpretable weight structures or concept mappings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Y1hfREBWs7", "forum": "ANH044Wdje", "replyto": "ANH044Wdje", "signatures": ["ICLR.cc/2026/Conference/Submission14126/Reviewer_vBqm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14126/Reviewer_vBqm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992807948, "cdate": 1761992807948, "tmdate": 1762924591791, "mdate": 1762924591791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}