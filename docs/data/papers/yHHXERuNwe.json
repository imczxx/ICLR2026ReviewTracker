{"id": "yHHXERuNwe", "number": 1316, "cdate": 1756870152590, "mdate": 1759898215797, "content": {"title": "Uncovering the Why: Interpretable CLIP Similarity via Dual Modalities Decomposition", "abstract": "The CLIP model has demonstrated strong capabilities in capturing the relationship between images and text through its learned high-dimensional representations. However, these dense features primarily express similarity via cosine distance, offering limited insight into the underlying causes of that similarity. Recent efforts have explored sparse decomposition techniques to extract semantically meaningful components from CLIP features as a form of interpretation. Nevertheless, we argue that these methods treat each modality independently, resulting in inconsistent decompositions that fail to reflect the cross-modal similarity from the aspect of concepts. \nIn this paper, we introduce an explanation method for CLIP similarity via Dual Modalities Decomposition, CLIP-DMD, which employs a Sparse Autoencoder (SAE) to learn sparse decompositions of both CLIP image and text features within a shared concept space. To enhance interpretability, we propose two novel objectives: a Rate Constraint ($RC$) Loss, which promotes the crucial concepts to dominate the overall similarity, and a Corpus Cycle Consistency ($C^3$) Loss, which ensures that the most responsive features are both distinctive and accurately recognized by the encoder. To assess interpretability, we also design an evaluation protocol leveraging Large Language Models (LLMs) to provide automated and human-aligned assessments. Experimental results show that CLIP-DMD not only achieves competitive zero-shot classification, retrieval, and linear probing performance, but also delivers more human-understandable, reasonable, and preferable explanations of CLIP similarity compared to prior methods.", "tldr": "", "keywords": ["Interpretable AI", "Explainable AI", "Feature decomposition"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c430b63a8940c1e92864c8d5eb3574701d40dc6b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present a method to get at interpretability of CLIP features from a dual modality perspective, claiming that prior methods have been too focused on single modalities. The main comparison of methods is with another model called SpLiCE). Overall, the method described claims to more effectively use shared features to gain a deeper level of interpretability. \n\nI will admit that my experience with CLIP feature interpretability research is not the best and It’s been over a year since I last really got into other work on the topic. However, I still feel like a good paper should clearly ground itself and prior research a bit better. There were moments when I felt my lack of understanding was to blame, but upon revisiting the difficult sections I’ve come to see that I just don’t think the paper explains it that well, particularly in explaining the related work and setting the context of the current proposal among other relevant literature. I also found the lack of extensive reporting of results to be quite deficient in backing up the claims. It's not clear to me there is evidence that stretches beyond, \"Look at these images in the figure, and believe us that they are not cherry picked and they extend across all images.\""}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's strengths are the motivating factors in being more encompassing with regard to looking at joint structures when decomposing interpretable elements in CLIP. The modelling process and inspiration on the Concept Bottleneck Framework are good. There is a sound basis building on (limitedly-described) relevant literature and from what I can tell from the often-vague descriptions, there are sparks of interesting ideas, but while some areas of the paper are well-explained, this standard isn't maintained throughout."}, "weaknesses": {"value": "Figure 1.  - What is this and what is it showing me? \nWhy are there no labels for what the values mean? I can see things are labelled as to going up and down but I’m not sure what the numbers actually mean. I’m more confused about the approach after having seen Figure 1 than before, which I think is opposite to the intended effect. The caption is not doing the job it is supposed to and the figure needs to be addressed. I do not see how “sea coast” and “beautiful beach” clearly contribute to explainability.\nThe paper repeatedly points to cherry-picked visual examples and claims that increased interpretability is “obvious” or heavily implied. I also see no sign of a single statistical test or hypothesis being statistically evaluated. For example: ”As shown in Figure 4, our method effectively identifies shared concepts across both modalities, providing a clear and interpretable rationale for the resulting CLIP similarity.”  - This in itself without any further (non-anecdotal) backing is not sufficient to back up the strong claim being made here. I don’t know how this generalises across more examples and I don’t know how cherry picked this example is.\n\nI’m not claiming that the method is incorrect and that it could be truthful, but the burden of proof is on the authors to demonstrate this in a convincing manner and I am concerned that the authors think that these single examples are sufficient to convince readers that broadly, on the whole, across whole datasets, that these benefits must always hold. The evidence isn’t being presented to back up such claims.\n\nThe details of experiments that do seem more quantitative are very under-specified. I followed the trail across references from section 4 back to section 3.3 and then jumped to the appendix for more info, but the details on avoiding positional bias seem to be more suggested at rather than sufficiently documented. The section in the appendix on quantitative evaluation is a very vague set of statements that does not reach anywhere near the level of academic rigour required for acceptance at ICLR unfortunately. You can’t just put 3 images in a figure and make a few comments about them and call this a “quantitative analysis”.\n\nOverall, I think the authors have an interesting idea and they could be onto something and I encourage them to continue this work. However, there are severe, severe shortcomings when it comes to presenting the results successfully in an academic submission. There is only a direct comparison to a single other method (SpLiCE) and many experimental methodological details are completely absent. To pick on one such example, when avoiding presentation order bias it is mentioned there are permutations of method orderings during evaluation. What does this mean if you’re not saying how many permutations were run and not showing averages or variability under these multiple runs? Or do the authors mean they make a permutation and treat the sample results the same. The results for these don’t seem to be anywhere (unless I’ve severely misunderstood something)."}, "questions": {"value": "1. Can you better characterise Figure 1 for me and create a better caption that explains the numbers and why some \"increases\" and some \"decreases\" seem to be all positive news for the proposed method? I just didn't get this part at ll.\n\n2. Can you please revisit and add in some technical details on experiments that you performed that go beyond just looking at specific examples. The LLM evaluation just doesn't have enough detail for me to see that a successful attempt has been made at this.\n\n3. I think you have a high chance at convincing me to increase my rating if you can completely rewrite your experimental section, have someone not familiar with your specific experiment look over it and see if they can fully follow the logic behind the experiments and they can provide feedback as to what is missing. From reading the paper, I fully get the sense that because the authors know all the details of the experiment, they are relying on implicit background knowledge that they haven't realised isn't present in the paper, and this causes confusion that the authors might not be able to appreciate. \n\n4. Would you be able to provide an additional section in the appendix that can flesh out the way you derived the corpus of semantic concepts. This is absolutely a crucial central element to many of the experiments you ran and I need to understand how you create it a bit better because it's not explained in sufficient detail in your paper. A nice Figure would be most welcome, but a sufficient text explanation would also suffice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BJxOWyfTeQ", "forum": "yHHXERuNwe", "replyto": "yHHXERuNwe", "signatures": ["ICLR.cc/2026/Conference/Submission1316/Reviewer_PbAs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1316/Reviewer_PbAs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761386638541, "cdate": 1761386638541, "tmdate": 1762915733239, "mdate": 1762915733239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CLIP-DMD, a sparse encoder–based framework designed to identify and interpret meaningful concepts embedded in CLIP features. Two novel loss functions are introduced to enhance the interpretability of the learned representations. However, the experimental results do not sufficiently validate the effectiveness of the proposed method, and its underlying motivation remains unclear. Moreover, figures and tables require substantial revision to improve clarity, consistency, and integration with the main text."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed corpus cycle consistency loss and rate constraint loss contribute to the model’s ability to learn salient features.\n\n2. The method is streamlined, contributing to implementation clarity."}, "weaknesses": {"value": "1. Figure 1 appears abruptly and lacks corresponding explanation in the main text, making its relevance unclear. In addition, the performance drop of SpLiCE after introducing additional concepts is not addressed.\n\n2. The authors argue that prior works either use CLIP as an auxiliary tool or focus on a single modality. However, methods like Grad-ECLIP already provide joint visual–textual explanations based on CLIP similarity. The authors should clarify what fundamentally distinguishes their approach, beyond simply citing CLIP usage or modality count.\n\n3. Table 1 also appears abruptly without sufficient contextual grounding. The results presented do not clearly support the caption’s claim of being competitive on zero-shot classification, image-text retrieval, and linear probing benchmarks.\n\n4. The architecture of the sparse text and image encoders is not clearly specified in the paper.\n\n5. The proposed method relies on features extracted by CLIP. If these features are biased or flawed, the subsequent process may propagate such errors and compromise the model’s reliability.\n\n6. The validity of the LLM evaluation process and the reliability of the reported results remain questionable. \n\n7. The task agnosticity of concepts experiment lacks comparison with other baselines, which undermines its persuasiveness."}, "questions": {"value": "1. The purpose of Figure 1 is unclear, and it does not illustrate how the interpretations are derived.\n\n2. How are the values of the binary vector determined in the CCC loss?\n\n3. Do all loss functions contribute equally during model training? According to Equation 6, all loss weights are set to 1.\n\n4. The proposed method is mainly evaluated on recognition and classification tasks. It would be interesting to see its applicability to other tasks such as MLLM and image variation.\n\n5. The paper does not analyze the effect of removing sparse text and image encoders, nor does it clarify the respective roles of CE and InfoNCE losses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oesuYLxnoD", "forum": "yHHXERuNwe", "replyto": "yHHXERuNwe", "signatures": ["ICLR.cc/2026/Conference/Submission1316/Reviewer_CQqb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1316/Reviewer_CQqb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789935769, "cdate": 1761789935769, "tmdate": 1762915732990, "mdate": 1762915732990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limited interpretability of CLIP’s cross-modal similarity, which is typically measured via cosine similarity in a high-dimensional space but lacks insight into which semantic concepts drive this similarity. The authors propose CLIP-DMD, a method that uses a Sparse Autoencoder (SAE) to decompose both image and text CLIP features into a shared, sparse concept space. Key innovations include two novel losses: the Corpus Cycle Consistency (C³) loss, which ensures that learned concepts are accurately recognized by the encoder, and the Rate Constraint (RC) loss, which encourages similarity to be dominated by a small set of salient concepts. The authors also introduce an LLM-based evaluation protocol to automatically assess interpretability. Experiments show that CLIP-DMD maintains competitive performance on zero-shot classification, retrieval, and linear probing tasks while providing more human-understandable explanations than prior methods like SpLiCE."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces a unified, dual-modality sparse decomposition framework that enables interpretable cross-modal similarity analysis.\n2. Authors propose two novel losses (C³ and RC) that enhance concept recognition and saliency, supported by ablation studies."}, "weaknesses": {"value": "1. The predefined unigram/bigram corpus limits the granularity and expressiveness of discovered concepts, as noted in the limitations.\n2. LLM-based interpretability evaluation is not validated against human judgments, raising questions about its reliability.\n3. The method does not fully address how to handle compositional or abstract concepts beyond the corpus vocabulary."}, "questions": {"value": "1. How does the LLM-based interpretability score correlate with human judgments? Have the authors conducted any human evaluation to validate this metric?\n2. Could the authors explore more expressive concept naming strategies (e.g., using phrases or LLM-generated descriptions) to overcome the limitations of unigrams/bigrams?\n3. How does CLIP-DMD handle cases where multiple concepts interact or overlap semantically? Is there a risk of over-simplification when only top-k concepts are highlighted?\n4. Could the author provide more intuition or visualization for how C³ loss enables encoders to accurately respond to the learned corpus features?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NCAWQWR2Ja", "forum": "yHHXERuNwe", "replyto": "yHHXERuNwe", "signatures": ["ICLR.cc/2026/Conference/Submission1316/Reviewer_7TfT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1316/Reviewer_7TfT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809308887, "cdate": 1761809308887, "tmdate": 1762915732799, "mdate": 1762915732799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The problem of interpretable CLIP is considered.  The paper introduces CLIP-DMD (CLIP Similarity via Dual Modalities Decomposition), a novel framework that explains the similarity between image and text features in the CLIP model by decomposing them into a shared, sparse concept space using a Sparse Autoencoder (SAE). To enhance interpretability, CLIP-DMD uses two novel objectives: the Rate Constraint (RC) Loss, which ensures that the similarity is dominated by a few critical concepts, and the Corpus Cycle Consistency ($C^3$) Loss, which improves the encoder's recognition of the learned concepts. Quantitative and qualitative results show that CLIP-DMD maintains competitive performance on tasks like zero-shot classification and retrieval while offering superior interpretability and a closer reflection of CLIP's similarity trends compared to previous methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Technical novelties: Corpus Cycle Consistency (C^3) and Rate Constraint (RC) losses\n2. Empirical results validating the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The interpretability evaluation is merely based on a single multi-modal LLM LLaMA-3.2-Vision-11B-Instruction. The current mLLMs are known to can be biased and can make mistakes. The trust-worthiness of the empirical study remains to be questioned. Additional Evaluations by more different mLLMs and human experts are necessary.\n2. Lack of 0-shot evaluation on more datasets. The common practice to evaluate the 0-shot performance of CLIP models is to evaluate on a variety of different datasets (e.g. Pets, Foods, Sun, etc, see VTAB+ for more). Also, why  is the linear probing evaluation only on CUB and TinyImagenet?\n3. Lack of demonstration of downstream application."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6ZoTI9tB8n", "forum": "yHHXERuNwe", "replyto": "yHHXERuNwe", "signatures": ["ICLR.cc/2026/Conference/Submission1316/Reviewer_sH98"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1316/Reviewer_sH98"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921601619, "cdate": 1761921601619, "tmdate": 1762915732658, "mdate": 1762915732658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}