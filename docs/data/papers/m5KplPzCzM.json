{"id": "m5KplPzCzM", "number": 12685, "cdate": 1758209494571, "mdate": 1759897493972, "content": {"title": "Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability", "abstract": "We study the ability of Transformer models to learn sequences generated by Permuted Congruential Generators (PCGs), a widely used family of pseudo-random number generators (PRNGs). PCGs introduce substantial additional difficulty over linear congruential generators (LCGs) by applying a series of bit-wise shifts, XORs, rotations and truncations to the hidden state. We show that Transformers can nevertheless successfully perform in-context prediction on unseen sequences from diverse PCG variants, in tasks that are beyond published classical attacks.  In our experiments we scale moduli up to $2^{22}$ using up to $50$ million model parameters and datasets with up to $5$ billion tokens. Surprisingly, we find even when the output is truncated to a single bit, it can be reliably predicted by the model. \nWhen multiple distinct PRNGs are presented together during training, the model can jointly learn them, identifying structures from different permutations.\nWe demonstrate a scaling law with modulus $m$: the number of in-context sequence elements required for near-perfect prediction grows as $\\sqrt{m}$. \nFor larger moduli, optimization enters extended stagnation phases; in our experiments, learning moduli $m \\geq 2^{20}$ requires incorporating training data from smaller moduli, demonstrating a critical necessity for curriculum learning. \nFinally, we analyze embedding layers and uncover a novel clustering phenomenon: the model spontaneously groups the integer inputs into bitwise rotationally-invariant clusters, revealing how representations can transfer from smaller to larger moduli.", "tldr": "Transformers in-context learn pseudo-random number sequences and curriculum accelerate convergence.", "keywords": ["In-context Learning", "Curriculum Learning", "Interpretability", "Transformers", "Pseudo-Random Number Generators"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/41915f3148185b0a7490e20b50b15a4f5a8fc9c2.pdf", "supplementary_material": "/attachment/a937a9161e6658257c435ae731807be5b9759566.zip"}, "replies": [{"content": {"summary": {"value": "*Disclosure: LLM is used for an initial draft of this review, but significant human effort is made to reflect the human reviewer's understanding and opinion of the paper.*\n\nThis paper investigates the ability of Transformer models to learn and predict sequences from Permuted Congruential Generators (PCGs), a family of pseudo-random number generators (PRNGs) more complex than standard Linear Congruential Generators (LCGs) due to their use of bitwise permutations (shifts, XORs, rotations).\n\nThe authors demonstrate several key findings:\n1.  **In-Context Learning:** Transformers can successfully perform in-context prediction of sequences from various PCG variants, generalizing to unseen generator parameters (a, c). This capability holds even when the output is severely truncated (e.g., to a single bit). A single model trained on a combined dataset of different PCG variants can also learn to identify and predict sequences from all of them.\n2.  **Training Strategy:** Training models on large moduli directly is infeasible due to prolonged stagnation. The authors show that mixing in data from smaller moduli or using a pretrained model are important for overcoming this stagnation and successfully training large-modulus models.\n3.  **Scaling Law:** The authors identify an empirical scaling law: the number of in-context sequence elements L required for near-perfect prediction grows with the modulus m as $L \\propto \\sqrt{m}$. This is a steeper requirement than for LCGs ($L \\propto m^{0.25}$).\n4.  **Interpretability:** By analyzing the token embedding layer, the paper reveals that the model spontaneously organizes representations based on rotation-invariant features (specifically, the number and arrangement of contiguous zero-runs in the tokens' binary form). This learned structure explains why representations transfer effectively from smaller to larger moduli."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- PCGs are practically relevant and designed to be statistically hard to predict. This work effectively establishes a new, ML-based approach to cryptanalysis and could evolve into a practical benchmark or tool for evaluating the security of other PRNGs. The experiments are throughout and comprehensive.\n\n- The paper provides new insights into the expressivity of transformers, demonstrating they can model surprisingly complex, non-linear bitwise operations (not just simple arithmetic). The ablations offer a concrete analysis of the importance of pre-training and curriculum learning.\n\n- The discovery of the $L \\propto \\sqrt{m}$ scaling law is intriguing, potentially related to known attacks such as Sweet32 attacks."}, "weaknesses": {"value": "- The data efficiency of the proposed method is not great. It is unclear that if the accuracy mainly comes from memorizing similar patterns.\n\n- (Minor) The findings around interpretability is not particularly novel, as previous studies on grokking and probing revealed similar geometric structures. The paper will be even better with a more in-depth mechanistic interpretability analysis."}, "questions": {"value": "- I would like to see a power law fit on the dataset size. Is the dataset size (# of sequences to be seen) needed linear w.r.p. to m?\n\n- Is the base-64/128/256 training empirically better than bit-wise tokenization? Would be good if we have some justifications."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cnh4B4ylEX", "forum": "m5KplPzCzM", "replyto": "m5KplPzCzM", "signatures": ["ICLR.cc/2026/Conference/Submission12685/Reviewer_2asJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12685/Reviewer_2asJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798965052, "cdate": 1761798965052, "tmdate": 1762923521597, "mdate": 1762923521597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates whether Transformer models can perform in-context next-state prediction on sequences generated by pseudorandom congruential generators (PCGs).\nThe authors consider four PCG variants (TLCG, XSLRR, XSHRR, XSHRS) and train models both on per-generator datasets and on a combined dataset. \nThe results show that Transformers not only learn each generator’s rule but also can infer which generator produced a given sequence. \nThe authors further study how performance scales with the modulus $m$, dataset size, and model size.\nThe experimental results find that curriculum learning with data-mixing strategies is effective for large moduli. \nFinally, a PCA of the embedding matrix suggests the model leverages structural statistics such as the number of zeros and zero-run patterns to support in-context reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**S1.** The paper is clearly written, and the figures are well-designed.\n\n**S2.**  The authors conduct extensive experiments to support the paper’s claims, and the results convincingly substantiate those claims."}, "weaknesses": {"value": "**W1.**\nThe PCG setup, where a hidden state $s_i$ evolves and the observation $x_i$ is produced via a deterministic function $f$, is conceptually close to HMMs and finite-state automata.\nTo my knowledge, there is already substantial work probing Transformers’ capability on learning HMM/automata-like processes [1,2].\nI think the paper should clarify what is genuinely new here versus what might already follow from known results on those finite-state structures.\n\n**W2.**\nThe PCA-based analysis suggesting the model learns and exploits a “zero-run” pattern is intriguing, but it does not provide a full understanding of how Transformers actually perform in-context learning on PCG instances.\nGiven that PCG sequences are challenging to analyze, a complete theory may be out of scope; however, I feel the analysis remains largely phenomenological.\nAs a result, the contribution read as “pick some convenient task, train a Transformer under various settings, and show that it exhibits in-context learning,” rather than explaining the underlying mechanism.\n\n---\n[1] Hu, Jiachen, Qinghua Liu, and Chi Jin. \"On limitation of transformer for learning hmms.\" arXiv preprint arXiv:2406.04089 (2024).\n\n[2] Liu, Bingbin, et al. \"Transformers learn shortcuts to automata.\" arXiv preprint arXiv:2210.10749 (2022)."}, "questions": {"value": "**Q1.**\nIn lines 263-264, the authors claim that a 1-layer model suffices to solve small-modulus PCG instances.\nHowever, to my knowledge, prior ICL work on Markov-chain-style tasks suggests that at least two layers are required to implement induction heads and achieve ICL [3].\nAlthough PCG is not a Markov chain, it is more complex.\nCould you clarify how PCG can be solved with a shallower model than Markov-chain-style tasks?\n\n**Q2.**\nI personally find the PCA analysis interesting: the first axis aligns with the total number of zero bits, and the second with the number of zero runs.\nHave you examined additional principal components (e.g., 3-component)?\nIf so, does the third component also align with interpretable statistics?\n\n---\n\n[3] Ekbote, Chanakya, et al. \"What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains.\" *arXiv preprint arXiv:2508.07208* (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XwGnxBZTZd", "forum": "m5KplPzCzM", "replyto": "m5KplPzCzM", "signatures": ["ICLR.cc/2026/Conference/Submission12685/Reviewer_i5nj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12685/Reviewer_i5nj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998313993, "cdate": 1761998313993, "tmdate": 1762923521195, "mdate": 1762923521195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper scrutinizes a problem in learning PCG-generated integral sequences in-context and auto-regressively with Transformers, where PCG stands for Permuted Congruential Generators, a class of PRNGs. Empirically, a scaling law for the number of initial elements required to achieve at least 90% accuracy for unseen parameters ($a$ and $c$) is observed/proposed as ~$0.5\\times m^{0.5}$. Moreover, this work offers a wide range of analyses about training methods (e.g., curriculum learning, data mixing) and model representations (e.g., PC analyses of token embeddings and similarity analyses of intermediate features)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well written. The problem setting and experimental results are clearly presented.\n2. I find most of the findings of the paper very interesting (especially the benefits of curriculum learning with the help of a smaller modulus, and the principal component analyses of embedding vectors). Even though the problem scale (e.g., number of bits required to represent the problem) studied here is quite smaller than the practically used PCGs, I believe it will be a stimulating example for many researchers who are interested in the (in)capability of a Transformer model and its learning dynamics. Indeed, it projects several immediate future works. Can we mathematically prove that/why/how a Transformer learn PCG-generated sequences (or, generalized modular arithmetics in general) in-context with a gradient-based optimizer? How far can we get with constant-size models? How about logarithmically-scaled models?"}, "weaknesses": {"value": "1. The observation that transformers can learn PCG-generated sequences auto-regressively may not be a very surprising finding. In fact, the studied problem is not that random since its scale is too small to pass a collection of empirical randomness tests (e.g., BigCrush). Hence, the problem possesses its own auto-regressive nature by its definition, which can be effectively solved with Transformers up to some extent.\n2. Indeed, there has already been a huge literature on in-context learning with Transformers in solving problems with a structure of a hidden Markov model (HMM) [1--6]. However, although the PCG seems to be a special instance of HMM, I cannot find a discussion about any of these previous works.\n3. While being interesting, the principal component analysis seems erroneous and incomplete; I am particularly suspicious about the ‘clustering’ of the embedding vectors. Do they really cluster together? Aren’t they just getting close because of the projection onto a two-dimensional subspace? Since the token embedding matrix is also the weight matrix of the final linear readout (due to weight sharing of GPT-based models), I don’t think the model is learning a “rotationally invariant” features; they must be distinguishable, but the separation between “rotationally invariant” tokens (in binary representation) is just happening in some other subspace(s) orthogonal to the PC1 and PC2. Being a fully empirical work, I believe the paper must provide a complete analysis of the embedding vectors, discovering on which subspace a particular feature of token embedding vectors is getting separated (e.g., PC1 separates tokens based on the number of zero-bits, PC2 separates tokens based on the number of zero-runs, PC3 separates …).\n\n---\n\nReferences:\n\n[1] Wei et al., Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning. NeurIPS 2021.  \n\n[2] Xie et al., An Explanation of In-context Learning as Implicit Bayesian Inference. ICLR 2022.  \n\n[3] Hu et al., On Limitation of Transformer for Learning HMMs. arXiv preprint 2024.  \n\n[4] Zhou et al., Transformers learn variable-order Markov chains in-context. arXiv preprint 2024.  \n\n[5] Dai et al., Pre-trained Large Language Models Learn Hidden Markov Models In-context. NeurIPS 2025.  \n\n[6] Hao et al., Transformers as Multi-task Learners: Decoupling Features in Hidden Markov Models. arXiv preprint 2025."}, "questions": {"value": "1. Why does the observed separation between the learned token embedding vectors really help the model to achieve a near-perfect in-context learning? Yes, I agree that the analysis itself is quite intriguing and visually satisfying, and it gives us some hint about why pre-training enhances the training efficiency for large moduli $m$. However, I don’t think it is a complete analysis of the mechanism of model predictions. One possible way to complete the analysis is to design a pseudo-code that is possibly learned by the model to make correct auto-regressive predictions of PCG-generated sequences.\n1. I hope a future revision of the paper will contain some more in-depth implications of predicting pseudo-random numbers with a deep neural network."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SAoBn1NdpJ", "forum": "m5KplPzCzM", "replyto": "m5KplPzCzM", "signatures": ["ICLR.cc/2026/Conference/Submission12685/Reviewer_yPnh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12685/Reviewer_yPnh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762174300860, "cdate": 1762174300860, "tmdate": 1762923520951, "mdate": 1762923520951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}