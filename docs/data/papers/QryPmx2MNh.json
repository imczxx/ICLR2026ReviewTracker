{"id": "QryPmx2MNh", "number": 3731, "cdate": 1757508033492, "mdate": 1759898072847, "content": {"title": "Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic", "abstract": "The chain of thought, i.e., step-by-step reasoning, is one of the fundamental mechanisms of Transformers. While the design of intermediate reasoning steps has been extensively studied and shown to critically influence performance, the ordering of these steps has received little attention, despite its significant effect on the difficulty of reasoning.\nThis study addresses a novel task of unraveling the chain of thought—reordering decoder input tokens into a learning-friendly sequence for Transformers, for learning arithmetic tasks. The proposed pipeline first trains a Transformer on a mixture of target sequences arranged in different orders and then identifies benign orders as those with fast loss drops in the early stage.  \nAs the search space grows factorially in sequence length, we propose a two-stage hierarchical approach for inter- and intra-block reordering.\nExperiments on four order-sensitive arithmetic tasks show that our method identifies a learning-friendly order out of a few billion candidates. Notably, on the multiplication task, it recovered the reverse-digit order reported in prior studies.", "tldr": "", "keywords": ["chain of thought", "learning-friendly order", "transformer"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3bc23979b14d5f66a6404c2bac78a8645576d5d3.pdf", "supplementary_material": "/attachment/48be1c6e998c09e55e862da7a63bf03ab730c4b3.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses discovering learning-friendly orderings for input tokens when training Transformers on arithmetic tasks. The authors propose training a Transformer on a mixture of target sequences in different orders, then identifying learning-friendly orders as those with fast loss drops in early training. To handle factorial search space growth, they introduce a two-stage hierarchical approach with global block-level reordering followed by local intra-block refinement.\n\nThe method is validated on three custom order-sensitive arithmetic tasks, successfully discovering learning-friendly orders from up to ≈ 6 billion candidates and improving success rates from ~10% to 100%. When applied to multiplication, it automatically rediscovered the previously reported reverse-digit order (least-to-most significant)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The experiments are clearly explained and executed with the results well presented.\n\nThis new approach to prompt optimization can be used to reduce model training effort in applicable use cases - especially cases where the model can’t learn a task without reordering tokens but can with reordered tokens.  \n\nThe token ordering selection mechanism is novel and seems efficient. It is a necessary prerequisite for the new approach to be feasible at non-toy scale."}, "weaknesses": {"value": "CoT typically refers to LMs explicitly generate intermediate reasoning steps before reaching a final answer, whereas this paper uses it to mean \"sequence of generation steps\" or \"order of tokens\" which is a stretch. The paper is about discovering the optimal order in which to arrange output tokens during training. The paper title would be better as just “Discovering Learning-Friendly Orders for Arithmetic”.\n\nAs I understand the paper, in the 4 use cases studied, the optimal ordering is reversing the token ordering. The paper would be stronger if the technique showed that learning a (5th) task is optimized by a previously-unknown optimal token ordering (that is just the “reverse” ordering). Without such an example, the technique’s usefulness over other existing techniques is undemonstrated.\n\nSuppose a novel token ordering for a 5th task is found. A model successfully trained with this technique can now do the task but will permanently need specially ordered prompt tokens. This seems to make the technique pretty specialized. \n\nThe claim “as the learning-friendly orders must be universal” may well be true but is not shown to be true.\n\nMinor:\n- Related Work is short.\n- Change cite usage in “(Arpit et al., 2017) has experimentally”"}, "questions": {"value": "- What is the evidence that this technique reveal insights not provided by other existing techniques? \n\n- In Table 2, are all or some of the “discovered final orders” learnt optimal in some sense? Or only the bolded ones? Or are they just the best ones found during (limited) training? \n\n- This technique seems very specialized. What is cost trade-off / justification: when should we optimize token ordering to reduce training costs or improve performance? \n\n- Once a model has learn a task via this technique, can the model be further refined to also handle the natural token ordering?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nSFh4G0FjL", "forum": "QryPmx2MNh", "replyto": "QryPmx2MNh", "signatures": ["ICLR.cc/2026/Conference/Submission3731/Reviewer_1dB3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3731/Reviewer_1dB3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3731/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761243593070, "cdate": 1761243593070, "tmdate": 1762916952291, "mdate": 1762916952291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a method to automatically discover better output orderings for chain-of-thought reasoning.\nThe authors evaluate three tasks: ReLU, SQUARE-19, and Index.\nThe proposed approach has two components: loss profiling and two-stage hierarchical optimization. Loss profiling trains a Transformer on a mixture of permutations and selects the permutation with the lowest validation loss, based on the intuition that a Transformer learns faster when given a good chain-of-thought ordering.\nThe two-stage hierarchical optimization searches the permutation space and, combined with loss profiling, aims to identify a learning-friendly order from which the model can easily learn the task.\nExperiments on the three arithmetic tasks show that the method can discover learning-friendly orders."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**S1.**\nAutomatic chain-of-thought ordering is important: common practice relies on human priors to design an ordering, which may be suboptimal for Transformers.\n\n**S2.**\nThe paper is well organized and provides clear examples."}, "weaknesses": {"value": "**W1.**\nThe key claim is that the proposed method can find a good ordering that enables a Transformer to learn efficiently. \nTo convincingly support this, the paper should demonstrate that it reliably identifies such orderings across tasks that are not inherently forward- or backward-friendly. \nAs written, ReLU, SQUARE-19, and Index appear forward-friendly, and Prod is backward-friendly.\n\nI would be more convinced if the authors included tasks that are not inherently forward- or backward-friendly.\nFor instance, fix permutation $\\sigma \\in [L] \\rightarrow [L]$ and define:\n(1) $y_{\\sigma(1)} = x\\_1$.\n(2) $y_{\\sigma(i)} = ReLU(y_{\\sigma(i-1)} + x_{i})$ for $i = 2, \\dots, L$.\nSuch constructions could test whether the method can recover a truly task-specific optimal order rather than relying on an obvious forward/backward bias.\n\n**W2.**\nSome important details seem missing (apologies if I missed them):\n\n(1) I don’t see which distribution is used to sample the $x_i$ for each task.\n\n(2) I don’t see how robust your method is.\n\nFor example, in Lines 466-467, you mentioned that “the optimal order is found for both tasks up to $L=30$, and for ReLU even at $L=40$”.\nHowever, I don’t see how often is the optimal order found: always, often, or only occasionally?\nReporting rates (and variance across random seeds) would clarify robustness."}, "questions": {"value": "**Q1.**\nIn figure 5(a), for SQUARE task, the evaluation losses of the optimal and suboptimal permutations are quite close. As $L$ increases and the task becomes harder, loss profiling might discard the optimal permutation. In such cases, can the two-stage optimization rediscover the optimal permutation?\n\n**Q2.**\nIn Table 1, why is the success rate for $(L,d)=(13,4)$ lower than for $(13,8)$? \nIntuitively, a smaller $d$ might seem easier. \nAlso, how many trials were run to compute the success rates in Table 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yTHMXMsxuj", "forum": "QryPmx2MNh", "replyto": "QryPmx2MNh", "signatures": ["ICLR.cc/2026/Conference/Submission3731/Reviewer_tkf6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3731/Reviewer_tkf6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3731/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762064460485, "cdate": 1762064460485, "tmdate": 1762916952042, "mdate": 1762916952042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to discover a learning-friendly ordering of target sequence tokens in a decoder input to solve an arithmetic task with a Transformer. The learning-friendly ordering of target tokens is determined by loss profiling, training a single model on a mixture of target sequences in different orders and choosing the ‘easiest’ one with respect to validation loss. The authors propose a two-stage heuristic combining global and local search of permutations for the sake of factorially large search space. The method is numerically tested with several arithmetic tasks: RELU (outputting a sequence of clipped cumulative sum), SQAURE-19 (modular arithmetic of sum-of-squares), INDEX (input-element pointing based on a partial sum of latest target tokens), and Prod (zero-padded integer multiplication)."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper pinpoints a novel problem that the ordering of target sequence (i.e., answer) tokens is important in learning an arithmetic task with a Transformer.\n2. The paper addresses this problem by proposing a learning-based heuristic to discover a learning-friendly ordering of answer tokens.\n3. The paper provides an ablation study about the design choice of their hierarchical algorithm so that the two-stage method and the structured initializations can much improve the efficiency in finding a learning-friendly permutation for tasks with a longer target length."}, "weaknesses": {"value": "1. The paper proposes an invalid definition of ‘order-sensitivity’ for a task, defined with a ‘non-injective’ mapping $f(x, y)$ w.r.t. $y$. Indeed, in the RELU, SQUARE-19, and INDEX tasks, the mappings of form $y_i = f_i(X, y_{<i})$ used for defining the tasks are not injective in $y_{<i}$, if we only have access to the $i$-th prompt token $x_i$. However, given that we have an entire prompt $X=[x_1, \\cdots, x_L]$, then each $y_i$ is uniquely determined for all $i=1,\\cdots,L$.  For example, for the RELU task, we have $y_1=x_1$ and $y_i = \\max_{k=1, 3, 4, \\ldots, i+1} \\sum_{j=k}^{i} x_i$ by definition. Thus, the authors’ claim that the tested synthetic tasks are using non-injective mapping in $y_{<i}$ seems wrong. The paper should have provided a more sophisticated and valid definition of order-sensitivity. For example, one can compute a minimal number of variables among $x_1, \\ldots, x_L$, $y_1, \\ldots, y_{i-1}$, $y_{i+1}, \\ldots, y_L$ to uniquely determine $y_i$, and then check whether such (minimally required) variables are always appearing before $y_i$ in each ordering of answer tokens.\n2. The paper is not motivating their two-stage search flow well. First of all, they did not rigorously define several jargons, such as ‘block-permutation’, ‘intra-block permutation’ and ‘inter-block permutation’. Second, the overall readability of Section 4 is quite poor because there are quite a lot typos in the description of the search flow. Third, it is not intuitively clear why alternating the searches over intra-block and inter-block permutation is helpful in terms of factorially large search space. Lastly, the numerical performance of their algorithm is quite questionable (e.g., Table 2) without using a prior knowledge (i.e., structured initialization) about the tasks.\n3. The paper should have mentioned that the multiplication task (PROD) with a reversed digit ordering satisfies the recurrence in Equation (5.1). That is, the sentence “Although it does not satisfy the recurrence in (5.1), …” (in line 319) is not true. \n4. The term ‘chain-of-thought’ is misused in this paper. As far as I know, it should refer to an intermediate steps of solving the problem, rather than auto-regressive method of generating the answer itself. However, from section 3, the paper is only focusing on the ordering of answer tokens. \n5. Learning paradigms other than next-token-prediction (e.g., teacher-less learning [1] or full-output prediction [2]) are worth to be discussed. This is mainly because, although the authors argue that the soft-permutation-based order-finding method is not working in a usual teacher-forced next-token-prediction setup because of the information leakage, this claim may not be true in other learning paradigms.\n\n---\n\nReferences:\n\n[1] Bachmann et al., The Pitfalls of Next-Token Prediction, ICML 2024\n\n[2] Fan et al., Looped Transformers for Length Generalization, ICLR 2025"}, "questions": {"value": "1. I think we can improve the efficiency of permutation-searching algorithm much more because the target length is fixed to be $L$ in this paper. For example, we can train/test $L$ different permutations (which are equivalent up to circulation) in parallel. This can be done by expanding the target length to be $2L-1$ as $(y_1, y_2, \\cdots, y_L, y_1, y_2, \\cdots, y_{L-1})$, although you need to manipulate the attention mask manually. I believe a similar idea can be applied in several ways, in both global/local stages of the algorithm. If the authors have enough time, can they numerically test this idea whether it can accelerate finding a learning-friendly permutation?\n2. How can we apply/extend the proposed method to a task having a variable target length? I believe this is a valid question because most of the arithmetic tasks (e.g., addition, multiplication, …) does so.\n3. I wonder whether the authors had a chance to come up with a class of order-sensitive tasks whose learning-friendly order is not straightforward to guess. What will happen if the proposed algorithm is applied to such tasks? I am also looking forward to a further numerical results to answer this question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7pqpGNRiGm", "forum": "QryPmx2MNh", "replyto": "QryPmx2MNh", "signatures": ["ICLR.cc/2026/Conference/Submission3731/Reviewer_qZyA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3731/Reviewer_qZyA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3731/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762074440184, "cdate": 1762074440184, "tmdate": 1762916951427, "mdate": 1762916951427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies reordering the decoder’s target tokens (the chain-of-thought steps) so Transformers learn arithmetic tasks more easily. It trains on a mixture of candidate orders and chooses the “learning-friendly” ones via early-epoch loss profiles. To reduce the factorial search, it uses a two-stage hierarchical search: 1) first choose block order, 2) reorder within blocks. Across several order-sensitive arithmetic tasks including integer multiplication, the method finds orders that increases success from 10% to 100%, and it also recovers the known reverse-digit order for multiplication."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) It's a very interesting paper framing the order of  intermediate steps as a search over permutations.\n\n2) They shows good performance finding the order to simplify the learning on multiple synthetic tasks specially the method rediscovers the reverse-digit order for multiplication tasks corroborating prior findings.\n\n3) The also proposed a simple hierarchical strategy: block-wise first, then within-block reordering reduces the factorial search cost."}, "weaknesses": {"value": "1) They only evaluated it only  on synthetic arithmetic tasks. How well does the approach transfer to natural-language reasoning e.g. GSM8K, etc.?\n\n2) How sensitive are the selected orders to random seeds, positional encoding and model scale choices? Can you provide variance/error bars across runs?\n\n3) Even short-term training over thousands of permutations can be costly. Can you quantify wall-clock/search cost vs. gains both on Full and hierarchical search."}, "questions": {"value": "Look at the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wW9uslyUIk", "forum": "QryPmx2MNh", "replyto": "QryPmx2MNh", "signatures": ["ICLR.cc/2026/Conference/Submission3731/Reviewer_XWqm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3731/Reviewer_XWqm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3731/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762133928039, "cdate": 1762133928039, "tmdate": 1762916951182, "mdate": 1762916951182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}