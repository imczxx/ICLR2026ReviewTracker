{"id": "eaLvpoGOQj", "number": 21077, "cdate": 1758313500903, "mdate": 1762974858451, "content": {"title": "Granularity boosts expressivity in Mixture of Experts", "abstract": "Mixture-of-Experts (MoE) layers are increasingly central to frontier model architectures. By selectively activating parameters, they reduce computational cost while scaling total parameter count. This paper investigates the impact of the number of active experts, termed *granularity*, comparing architectures with many (e.g., 8 per layer in DeepSeek) to those with fewer (e.g., 1 per layer in Llama-4 models). We prove an exponential separation in network expressivity based on this design parameter, suggesting that models benefit from higher granularity. Experimental results corroborate our theoretical findings and illustrate this separation.", "tldr": "We prove that increasing the granularity in Mixture of Experts increases the function class they can represent.", "keywords": ["mixture of experts", "deep learning", "theory", "expressivity", "granularity"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/21af67301c90cdad6ef8f79b489ba38e06042a9c.pdf", "supplementary_material": "/attachment/265b286c6db2379695606ad1bbefecd0b0ba0711.zip"}, "replies": [{"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We are withdrawing the paper. Because there is a public record of the discussions, we have addressed the reviewer comments below."}}, "id": "wiJqfdacEh", "forum": "eaLvpoGOQj", "replyto": "eaLvpoGOQj", "signatures": ["ICLR.cc/2026/Conference/Submission21077/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21077/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762974857157, "cdate": 1762974857157, "tmdate": 1762974857157, "mdate": 1762974857157, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper seeks to answer a fundamental question related to the success of MoE models, namely ``How do specific MoE design choices influence model expressivity?'' - in particular they focus on the notion of granularity. First, the authors present an intuitive argument why granular MoE might be more efficient than non granular: choosing multiple experts might lead to efficient composition of experts' capabilities. The main axis of investigation is a detailed proof of improved expressivity of granular MoE layers when compared to coarse-grained counterparts. The proof follows for three cases of increasing complexity and real-life relevance: (1) for constant activation function, (2) for linear activation function, and (3) for ReLU activation function.\nThe theoretical result is complemented with an experiment, which shows that student granularity needs to be larger than teacher granularity if student is to replicate a function expressed by the teacher."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The work seeks to answer an important question, namely ``why does granular MoE outperform coarse-grained MoE''.\n2. Thorough mathematical of the expressivity gap between coarse-grained and fine-grained MoE.\n3. Availability of the experiment code (notebook)."}, "weaknesses": {"value": "1. The experimental validation is extremely limited: only one setting in a single size is considered. Only \n2. The mathematical model of real architecture is simplified - real models consist of stacks of layers and it is hard to reason about the relation of the results presented in the paper and real MoE Transformers.\n3. Although it is stated that the problem tackled in the paper is ``How do specific MoE design choices influence model expressivity?'', no other other design choices than granularity are considered."}, "questions": {"value": "1. Do authors believe that the expressivity gap is the reason for fine-grained MoE's better performance in real-life settings? Why?\n2. I notice that the experimental validation uses softmax based routing unlike the proofs which rely on linear routing. What is the reason behind this choice?\n3. Could the proof be extended to other activations such as SwiGLU?\n4. Can the expressivity gap be closed in case of multilayer models? Intuitively, making the model deep enough could offset low granularity, as the number of paths a token could take through the model grows very quickly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VGG04fEVlW", "forum": "eaLvpoGOQj", "replyto": "eaLvpoGOQj", "signatures": ["ICLR.cc/2026/Conference/Submission21077/Reviewer_GRZu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21077/Reviewer_GRZu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667417592, "cdate": 1761667417592, "tmdate": 1762999987183, "mdate": 1762999987183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how increasing the number of active experts in a mixture-of-experts model influences the model’s expressivity. The authors present a main theorem demonstrating that expressivity increases with the number of active experts, and in subsequent sections, they provide proofs for various activation functions. The theoretical results are further evaluated through empirical experiments designed to validate the authors’ claims."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Overall, this research increasingly benefits the field by building upon existing empirical findings and theoretically grounding its observations."}, "weaknesses": {"value": "The scope of the experimental evaluation is notably limited. The presented results are constrained to a single model size, a fixed data processed, and a few range of learning rates. The paper would benefit from a scalability analysis to demonstrate how the proposed configurations perform at varying scales."}, "questions": {"value": "- In the cited work by Krajewski et al. (2024), the term granularity was defined as\n>“[…] the multiplier factor for the change in the size of an expert from the original standard model […]”\n\nThis definition emphasizes that the same active parameters are present in both granular and non-granular MoE configurations.\nIn contrast, in this work (see line 043), granularity was defined as\n“[…] the number of experts that activate on a token […]”\nTo align more closely with the original definition, the concept of active parameter matching should be incorporated.\n\n- >Line 453: \"We validate our theory with experiments demonstrating that the effects of granularity are relevant\nat practical scales.\"\n\nThe claim of “practical scales” seems questionable. The total number of model parameters (even in cited works) is orders of magnitude larger than in the experiments reported here. Please clarify what is meant by “practical scales”. \n\n- Could the authors provide more details about the model’s output head and the associated loss function? Specifically, is the head implemented as a simple MLP projecting to a single neuron?\n\nMinor:\n- Summary and Limitations sections are missing. The paper ends rather abruptly right after the Experiments section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Rl0UM5uVvI", "forum": "eaLvpoGOQj", "replyto": "eaLvpoGOQj", "signatures": ["ICLR.cc/2026/Conference/Submission21077/Reviewer_2kUZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21077/Reviewer_2kUZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913456112, "cdate": 1761913456112, "tmdate": 1762940647583, "mdate": 1762940647583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proves that increasing the number of simultaneously active experts (granularity) in MoE layers exponentially enhances their expressivity, even when keeping total and active parameter counts fixed.\nthe authors show that an (m,k)-MoE can represent functions that no smaller-granularity (m′,k′)-MoE can approximate within constant L₂ error.  the paper provides derivations for three activations functions. \nThe paper analyzes linear routing as opposed to softmax weighing but it is still enlighting. \nEmpirical experiments provide some confirmation of the findings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The expressivity separation between low- and high-granularity MoEs is rigorously formalized, providing theoretical analysis of the expressivity of MoEs as a function of granularity\n\nThe authors present formal theorems for three cases (constant output experts, linear activation, and ReLU activation), each proved with careful arguments.\n\nThe fact that, e.g. a granularity-$2$ or $4$ student fails to learn a granularity-$8$ teacher, even with more parameters, reinforces the soundness of the expressivity separation."}, "weaknesses": {"value": "The text is sometimes challenging to follow. The work seems to be written in a rushed way. It lacks conclusion. Experiments are limited. The notation could be better explained (e.g. C and 0.99, 20x in Th 3.1)\n\n\nThe experiment shows the student's granularity must match the teacher's granualrity to learn but does it really prove the paper's thesis about increasing (exponential) expressivity with the increasing granularity? I would add one more ablation argument and one more real data set experiment. \n\nThe paper has much value but I would prefer to be slightly improved, written more clearly before accepting it."}, "questions": {"value": "Is there any observed threshold where increasing granularity no longer improves expressivity\n\nwhere do the numbers in 2.3 come from?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yco4nZfvgf", "forum": "eaLvpoGOQj", "replyto": "eaLvpoGOQj", "signatures": ["ICLR.cc/2026/Conference/Submission21077/Reviewer_U6qX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21077/Reviewer_U6qX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762037160985, "cdate": 1762037160985, "tmdate": 1762940646185, "mdate": 1762940646185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how the granularity parameter (number of active experts k) in Mixture-of-Experts (MoE) layers affects model expressivity. The authors prove that MoE architectures with higher granularity have exponentially better expressivity than those with lower granularity, even when the number of active parameters is held constant. The crux of the argument is that, in the context were we have many more experts relative to the granularity (m ≥ Ck), any MoE that has a lot fewer routing configurations (m' choose k') < c × (m choose k)^0.99 will not be able to approximate the same class of functions. They show that the number of possible expert configurations (m choose k) is the key combinatorial quantity controlling expressivity, and prove separation theorems for constant, linear, and ReLU activation functions. Experiments on synthetic teacher-student tasks are provided."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) This paper provides novel theoretical insights into how the granularity parameter (number of active experts k) in MoE layers affects model expressivity. The authors prove that MoE architectures with higher granularity have exponentially better expressivity than those with lower granularity, even when the number of active parameters is held constant. They show that the number of possible expert configurations (m choose k) is the key combinatorial quantity controlling expressivity, and prove separation theorems for constant, linear, and ReLU activation functions. Experiments on synthetic teacher-student tasks are provided. \n2) The paper seems to be technically sound with the separation theorems appear correct, proving that architectures with substantially fewer routing configurations cannot approximate functions computable by higher-granularity architectures.\n3) The paper provides a clear clear combinatorial intuition of why their results hold. The exponential growth of configurations with k provides an elegant mathematical explanation for potential differences between architectures.\n4) The analysis covers constant, linear, and ReLU activations with increasing technical sophistication, showing the result is robust across different settings."}, "weaknesses": {"value": "My main criticisms of this paper are twofold:\n\n1) The paper is heavily weighted toward theoretical proofs with minimal empirical support. The main body consists primarily of mathematical proofs for the theoretical assertions (Sections 3.1-3.3 occupy most of the paper), with approximately half a page dedicated to experimental findings in Section 4. These experiments are entirely synthetic (random teacher-student imitation on Gaussian data) with no evaluation on real tasks. There is no concluding discussion, limitations section, or future work - the paper simply ends after presenting Figure 3.\n\nTheoretical work can of course be impactful and extremely valuable, but the authors fail to adequately argue why this would be the case here or how their findings translate to real-world impact. Per ICLR reviewer guidelines, papers should \"convincingly demonstrate new, relevant, impactful knowledge.\" While the theoretical contribution may be sound, without either (a) strong empirical validation on real tasks, or (b) a compelling argument for why these specific theoretical insights matter for practice, the paper fails to meet ICLR's standards for impact.\n\n2) The paper provides no justification for why the expressivity separation matters for real applications. How does this finding improve training of currently used MoE models? Modern LLMs are already vastly overparameterized - why would expressivity be the bottleneck rather than optimization, data, or other factors?\n\nThe paper also ignores computational trade-offs, acknowledging routing costs scale linearly with granularity but never quantifying whether exponential expressivity gains justify linear computational overhead. Without establishing that real tasks require the specific functions that benefit from high granularity, this remains a purely theoretical exercise. The authors could address this by: characterizing which types of problems benefit from high granularity, computing the actual constants in their bounds to show effects occur at practical scales, or demonstrating on at least one real task that the expressivity advantage translates to performance gains.\n\nI think the presentation is inadequate for ICLR. I understand the constraints of page limits but currently the paper is 5+ pages of mathematical proofs with limited justification for the significance of these theoretical findings, minimal empirical validation, no limitations section, no discussion of when high granularity helps vs. hurts, and little attempt to connect theory to practice. To make this work suitable, the authors should consider revisions including: experiments on language modeling or vision tasks, empirical evidence that expressivity (not optimization/data) limits current MoE performance, and analysis of compute-performance trade-offs comparing high vs. low granularity."}, "questions": {"value": "1) Do real language/vision tasks actually require the exponentially many distinct modes that high granularity provides? Could you characterize which types of problems benefit?\n2) How does your combinatorial analysis relate to the exponential subnetworks in dropout? Both seem to rely on similar insights about parameter reuse and combinatorial richness.\nSee: \n[1] Dropout: A Simple Way to Prevent Neural Networks from Overfitting, Srivastava et al, 2014\n[2] A Combinatorial Theory of Dropout: Subnetworks, Graph Geometry, and Generalization, Dhayalkar, 2025\n3) What happens as experts become very small (high k, fixed kw)? Is there a sweet spot where granularity benefits plateau?\n4) Your experiments use random fixed routing. How do results change with learned routing on real tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OFPHKsttWz", "forum": "eaLvpoGOQj", "replyto": "eaLvpoGOQj", "signatures": ["ICLR.cc/2026/Conference/Submission21077/Reviewer_CYWC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21077/Reviewer_CYWC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762541914338, "cdate": 1762541914338, "tmdate": 1762940645180, "mdate": 1762940645180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}