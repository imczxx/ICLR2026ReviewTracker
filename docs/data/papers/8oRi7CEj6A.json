{"id": "8oRi7CEj6A", "number": 11121, "cdate": 1758189769594, "mdate": 1759897607034, "content": {"title": "Context Parametrization with Compositional Adapters", "abstract": "Large language models (LLMs) often seamlessly adapt to new tasks through in-context learning (ICL) or supervised fine-tuning (SFT). However, both of these approaches face key limitations: ICL is inefficient when handling many demonstrations, and SFT incurs training overhead while sacrificing flexibility.  \nMapping instructions or demonstrations from context directly into adapter parameters offers an appealing alternative. While prior work explored generating adapters based on a single input context, it has overlooked the need to integrate multiple chunks of information.\nTo address this gap, we introduce CompAs, a meta-learning framework that translates context into adapter parameters with a compositional structure.  \nAdapters generated this way can be merged algebraically, enabling instructions, demonstrations, or retrieved passages to be seamlessly combined without reprocessing long prompts.  \nCritically, this approach yields three benefits: lower inference cost, robustness to long-context instability, and establishes a principled solution when input exceeds the model’s context window.  \nFurthermore, CompAs encodes information into adapter parameters in a reversible manner, enabling recovery of input context through a decoder, facilitating safety and security.\nEmpirical results on diverse multiple-choice and extractive question answering tasks show that \\method outperforms ICL and prior generator-based methods, especially when scaling to more inputs. \nOur work establishes composable adapter generation as a practical and efficient alternative for scaling LLM deployment.", "tldr": "CompAs generates context-specific adapters that can be algebraically combined, enabling efficient, scalable, and stable LLM adaptation.", "keywords": ["LLMs", "context parametrization", "in-context learning", "meta-learning", "adapter generation", "compositionality", "efficiency"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e4f1482a8866336c421f95746da745e1366f9eed.pdf", "supplementary_material": "/attachment/0c0814efcbd51f0d4f93f7f9db64dd5719ef5710.zip"}, "replies": [{"content": {"summary": {"value": "COMPAS encodes each context piece into a small adapter and composes many contexts by adding their adapters in parameter space. A teacher sees the full concatenated context; a student sees only the query plus the summed adapters, and a generator is trained so the student matches the teacher. The paper gives a clean bound that ties output error to a parameter Lipschitz factor and an additivity term, which makes each loss component easy to interpret. On QA and multiple-choice benchmarks, accuracy improves and variance drops as the number of context pieces grows, while latency and memory are kept in check because inference no longer processes long prompts."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Theorem is clear. The compositionality bound neatly decomposes error into an additivity term $\\eta$, a student–teacher term, and a sensitivity constant, so the role of each loss is explicit. \n\n- Method is conceptually straightforward yet principled. Treating context as composable adapters provides a practical alternative to very long prompts and scales naturally with more evidence. \n\n- Empirical results are consistent with the story. gains grow with more context, and the efficiency benefits are tangible in high-shot regimes. \n\n- Analysis and diagnostics are thoughtful. Ablations on the generator and an empirical reconstruction check suggest limited information loss when moving context into adapters."}, "weaknesses": {"value": "- Important configurations remain unexplored. The block size is fixed at four, so we do not know how performance and error terms behave when blocks match shots or when $k$ becomes very large. Varying block granularity and reporting both accuracy and the additive error $\\eta$ as $k$ grows would clarify where composition helps and where it saturates. In addition, it would be useful to track how summing more adapters affects performance itself, for example whether combined adapters introduce interference or yield diminishing returns as their number increases.\n\n- Robustness to conflict and noise is not established. The additive composition is not stress tested when context pieces contradict each other or contain label noise. A light evaluation with controlled conflict rates and off topic passages would show how stable COMPAS remains under realistic retrieval conditions.\n\n- The paper reports accuracy comparisons versus WILDA and GenAda, but its efficiency analysis is limited to COMPAS vs ICL. Efficiency study against WILDA and GenAda would make the compute vs quality trade-off more convincing."}, "questions": {"value": "- How does block granularity matter in practice? If blocks match shots so each demonstration gets its own adapter, do gains over strong prompting hold? \n\n- In very many-shot regimes such as $k>100$, beyond compute and memory savings, does accuracy keep improving roughly with $k$, or does it saturate? Scaling curves for accuracy, latency, and memory versus $k$ would clarify this. \n\n- In Appendix Eq. 7 the second inequality reads like an equality by definition. Is equality intended, or is $\\eta$ meant as a global bound? \n\n- When summing many adapters, do you track collisions between adapters from different contexts, and do you use normalization or orthogonalization to keep adapters identifiable as their number grows?\n\n- Could you provide FLOPs, and peak memory comparisons for COMPAS vs WILDA and GenADa at the same shot counts and model sizes, ideally including adapter training cost for WILDA or GenADa."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fPHH651Hxm", "forum": "8oRi7CEj6A", "replyto": "8oRi7CEj6A", "signatures": ["ICLR.cc/2026/Conference/Submission11121/Reviewer_WtSR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11121/Reviewer_WtSR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905041248, "cdate": 1761905041248, "tmdate": 1762922294530, "mdate": 1762922294530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces COMPAS, a meta-learning framework that maps contextual information into adapter parameters with a compositional structure. The method enables algebraic combination of adapters to approximate the effect of concatenating multiple contexts while maintaining efficient inference. Theoretical analysis and auxiliary objectives support compositionality and reconstruction, and experiments on question answering tasks demonstrate consistent gains over in-context learning and prior methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper defines a structured loss function composed of terms for alignment, compositionality, and reconstruction. The formulation is supported by a theoretical analysis that connects these objectives to compositional consistency.\n\n2. Ablation studies examine the contribution of each loss term and show that removing any component leads to lower performance. This provides empirical evidence for the role of each term in the overall framework.\n\n3. Experiments across multiple backbone models indicate that the method performs competitively and maintains stable performance as the number of demonstrations increases, suggesting potential scalability to high-shot settings."}, "weaknesses": {"value": "1. The paper compares the proposed method only against standard in-context learning (ICL), without including stronger or more recent ICL variants that incorporate retrieval, demonstration selection, or prompt tuning.\n\n2. Although COMPAS avoids concatenating long contexts, the training process still requires multiple forward passes (teacher–student pairs and reconstruction). The computational overhead relative to standard ICL or other adapter-based approaches is not reported.\n\n3. The evaluation focuses primarily on question answering tasks, with limited coverage of other task types such as summarization, generation, or reasoning. As a result, it remains unclear how well the method generalizes beyond QA settings."}, "questions": {"value": "1. The paper compares RNN and linear-based generators, but does not appear to investigate variations in the generator’s scale. Have the authors considered evaluating how changes in the size or capacity of the generator influence performance?\n\n2. Have the authors considered comparing their method to more advanced ICL baselines, such as those using demonstration selection, retrieval-augmented prompting, or prompt tuning?\n\n3. Can the authors provide an analysis of the computational cost of COMPAS, particularly in terms of training or inference time compared to standard ICL or other adapter-based methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TCZdk8aNtO", "forum": "8oRi7CEj6A", "replyto": "8oRi7CEj6A", "signatures": ["ICLR.cc/2026/Conference/Submission11121/Reviewer_jjPy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11121/Reviewer_jjPy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954699993, "cdate": 1761954699993, "tmdate": 1762922294222, "mdate": 1762922294222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses how to adapt LLM to tasks requiring multiple contextual inputs without relying on long prompts or retraining. It introduces COMPAS, a teacher–student meta-learning framework that encodes each context into LoRA adapter parameters and allows algebraic composition of adapters. Experiments show that COMPAS outperforms ICL, Generative Adapter, and WILDA on MMLU, ARC, and HotpotQA, especially as the number of demonstrations increases. It also maintains high context reconstruction fidelity, indicating that contextual information is faithfully captured."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The problem of mapping text to parameter representations is difficult and important. The proposed compositional adapter framework is well-motivated and empirically effective, providing consistent gains across benchmarks and interpretable results."}, "weaknesses": {"value": "1. The paper lacks clarity about the training setup. Specifically, are the learnable parameters optimized on single-task or multi-task data, and how are the train/validation/test splits obtained.\n2. The ICL baselines do not appear to train on the same data as COMPAS, which may give COMPAS an advantage through better task alignment. A supervised baseline trained directly on the same data would strengthen the comparison."}, "questions": {"value": "1. How does the method perform on tasks requiring long or structured outputs?\n2. Can the approach handle heterogeneous contexts that include both instructions and retrieved evidence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GD2NGxt5uY", "forum": "8oRi7CEj6A", "replyto": "8oRi7CEj6A", "signatures": ["ICLR.cc/2026/Conference/Submission11121/Reviewer_UwJk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11121/Reviewer_UwJk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995062998, "cdate": 1761995062998, "tmdate": 1762922293791, "mdate": 1762922293791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}