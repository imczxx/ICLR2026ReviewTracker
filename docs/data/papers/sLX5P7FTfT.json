{"id": "sLX5P7FTfT", "number": 6773, "cdate": 1757995196593, "mdate": 1763485048612, "content": {"title": "Explaining Grokking and Information Bottleneck through Neural Collapse Emergence", "abstract": "The training dynamics of deep neural networks often defy expectations, even as these models form the foundation of modern machine learning.\nTwo prominent examples are grokking, where test performance improves abruptly long after the training loss has plateaued, and the information bottleneck principle, where models progressively discard input information irrelevant to the prediction task as training proceeds.\nHowever, the mechanisms underlying these phenomena and their relations remain poorly understood.\nIn this work, we present a unified explanation of such late-phase phenomena through the lens of neural collapse, which characterizes the geometry of learned representations.\nWe show that the contraction of population within-class variance is a key factor underlying both grokking and information bottleneck, and relate this measure to the neural collapse measure defined on the training set.\nBy analyzing the dynamics of neural collapse, we show that distinct time scales between fitting the training set and the progression of neural collapse account for the behavior of the late-phase phenomena.\nFinally, we validate our theoretical findings on multiple datasets and architectures.", "tldr": "", "keywords": ["deep learning", "grokking", "information bottleneck", "neural collapse", "training dynamics"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/57cef85331de34498051327ba525b31740c9f14e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper investigates learning phaes of neural nets focusing on the later training stages using wihtin-class variance as a key investigative lense."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Important problem: Theory is generally lacking in this area\n* Good alignment of theory and practice.\n* Well written."}, "weaknesses": {"value": "* There are some concerns regarding the suitability of in-class variance, which appear critical   (see questions).\n* As for any theory, there are a number of assumptions.  and lack of common architectural elements\n* There are concerns regarding novelty: Some of the findings appear known, some approaches seem to overlap with prior work, some key statements appear to be known, though not in this context and under a different analytical lense -  but the paper still of interest despite potential overlaps, but clarification is needed in this regard (see questions).\n* Lack of practical implications (though not a must for a theory paper)\n\n(Note, the initial judgement is more on the negative side due to a number of concerns - in particular the in-class variance as metric, but given questions are clarifyable an increase in score is appropriate as this is overall a strong paper)"}, "questions": {"value": "* How is this behavior if you include different kinds of normalization?\n* Could you intuitively state the impact of assumption A.5 (in appendix)? In what sense, does it restrict initialization (e.g. compared to He or other initialization methods)?\n\n* A variance decrease is also possible by simple scaling, e.g., say X are inputs to a layer then we have var[WX] for some weight matrix W, now if we scale X, i.e., we have cX with c<1, we have c^2*var[WX]. \nIf you use regularization on weights you naturally scale weights and push them towards 0. So from this angle the metric appears not very suitable.\n\n\n* You state \"An intriguing observation in the existing information plane work is that, in the late stage of training, DNNs tend to compress I(Z; X) while preserving I(Z; Y ), thereby moving toward a more optimal solution with respect to the IB objective in Equation (1). \n\nThis observation has already been made outside the IB framework, by looking at the reconstruction loss L(X,Z) over time (see [1] below). That is the paper argues that reconstructing the input X given intermediate representation Z gets better (L(X,Z) decreases) before increasing again (L(X,Z) increases), at the same time cross entropy CL(X,Y) loss only reduces, which is qualitatively identical to your Figure 1 showing I(Z,Y), I(Z,X), except that they also show a third phase, i.e., initially L(Z,X) remains constant.  Do you agree?\n\n[1] Schneider, J., & Prabhushankar, M. (2024). Understanding and leveraging the learning phases of neural networks. In Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence (pp. 14886-14893).\n\n\n* Why should in-class variance be a good measure? In particular, you show that it decreases, but it a better measure might something that changes across the phases ? Won't it? \n\n*  [2] (see below) argues that first the mean of the samples are fit, followed by those not well-aligned which might actually strongly impact representations though only minimally impacting the loss, which is fairly intuitive. Does not the initial convergence to the mean (per class) also apply to your formulation?   Essentially you minimize the population within class variance (Definition 3.1) if g(X) ~ Mean_(X,Y=c) g(X|c) ~ E_{X|Y=c}[g(X)].  In particular, the RNC1 score seems to measure just that and directly corresponds to the L2-norm used in [2]. Could you discuss the differences and similarities? \nThe paper [2] argues that after the majority has been fit, learning slows down as but there can still be substanital changes -- the paper says a few (strongly different samples from the mean) can be repsonsible for this using the dot product. This seems to be very different from your approach as you are only concerned with the majority, are you? Or is it also relevant in your case ?\n\n[2] Learning in NN: from fitting most to fitting a few. Neural Computing and Applications, 37(28), 23423-23446."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PZZIjSvttK", "forum": "sLX5P7FTfT", "replyto": "sLX5P7FTfT", "signatures": ["ICLR.cc/2026/Conference/Submission6773/Reviewer_oBWN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6773/Reviewer_oBWN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6773/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835652969, "cdate": 1761835652969, "tmdate": 1762919049444, "mdate": 1762919049444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to provide geometric formulation for grokking’s delayed generalization. They do so by showing that grokking is the combination of a deep neural network following IB dynamics and achieving within-class variance collapse. It derives bounds linking this variance collapse to both generalization error and redundant mutual information, and shows empirically that variance collapse (RNC1) lags training-loss convergence under weight decay. The result is a heuristic unification of IB, grokking, and NC1."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper reframes grokking and IB as natural outcomes of a slowly emerging regularity principle (neural collapse). The paper is well-written, offers clear theorems that formalize variance–generalization connections, and provides empirical breadth.\nThe time-scale separation between fitting and collapse is a genuine insight for grokking dynamics, and the presentation is careful and reproducible."}, "weaknesses": {"value": "However, the analysis treats neural collapse only through NC1, uses an one-sided IB inequality (Thm 3.4), and replaces Papyan 2020’s ETF-based NC2 metric with a condition-number surrogate that cannot ensure ETF geometry. The paper’s main claim is a unification of grokking and information bottleneck (IB) dynamics through neural collapse (NC). This is conceptually interesting but not rigorously established.\n\nThis unification is heuristic not proven: The entire IB connection rests on a one-sided variance-based upper bound (Thm 3.4) that does not prevent degenerate or trivial solutions (when compression is more important than capturing any relevant information). \n\nTheoretical results rely on strong simplifications (smooth activations, squared loss, pyramidal architecture, Assumption A.3, A.4, A.5). The theoretical guarantees in Theorem 4.3 depend on highly restrictive conditions (pyramidal architecture, smooth activations, squared-loss training) that differ from the experimental setups (ReLU + cross-entropy) . Theorem 4.3 and its proof explicitly assume optimization of the squared loss with weight decay, as given in Section 4.2.  All the convergence results refer to that MSE-based objective, not to the cross-entropy loss used in the experiments.\n\nI also think replacing the NC2 metric with a condition-number surrogate without justification is sloppy. The co-occurrence of NC1 and NC2 is critical for the normative statement of neural collapse to hold (theorem 3 in Papyan 2020). NC2 is strictly defined as equal angles and equal norms both satisfied. While when condition number = 1, the perfectly isotropic mean configuration coincides with ETF. Many non-ETF configurations can also yield small condition numbers."}, "questions": {"value": "1) Could flatness or implicit regularization, rather than NC1 dynamics, explain the delayed generalization you observe?\n\n2) Is thm 3.4 a tight bound or a loose ceiling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iBMhHIy2ZN", "forum": "sLX5P7FTfT", "replyto": "sLX5P7FTfT", "signatures": ["ICLR.cc/2026/Conference/Submission6773/Reviewer_ud5o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6773/Reviewer_ud5o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6773/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927416120, "cdate": 1761927416120, "tmdate": 1762919048892, "mdate": 1762919048892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper demonstrates theoretically that the two training late-phase phenomena of grokking and information bottleneck (IB) dynamics are both due mainly to reduced population within-class variance.  It relates the measure of such variance reduction to the neural collapse degree present in the training dataset.  The paper shows that the model training duration ends sooner than the onset of the neural collapse progress, with the former dependent on the time taken to fit the training set and hence resulting in late-phase phenomena because neural collapse is responsible for such phenomena.  Experiments are conducted to validate its theoretical results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper broadens the understanding of training dynamics by revealing the connection between late-phase phenomena and neural collapse.  It analyzes the variance of class-wise representations in the training data and shows that the decrease of empirical within-class variance leads to improved test accuracy, which takes place in the second compression phase to exhibit model grokking behavior after the first fitting phase. \n\nBy considering the grokking and IB dynamics as two representative late-phase phenomena of DNNs, the paper is the very first to demonstrate that both phenomena can be explained in terms of the population within-class variance, offering new insights into their underlying mechanisms. \n\nThe paper conducts a quantitative analysis on the discrepancy between the population within-class variance, allowing to evaluate the progression of neural collapse and implying a corresponding reduction in the population within-class variance to relate\nthe behaviors of grokking and IB dynamics to the development of neural collapse."}, "weaknesses": {"value": "The experimental study for validating theoretical results is limited to DNN models with two classes, like the assumption of theoretical treatment.  It should be evaluated for the models targeting datasets with more classes, for generalization."}, "questions": {"value": "Experiments on more general scenarios for models with more classes are desirable for result validation, since their results are not clear to hold."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "n88IgsSN9M", "forum": "sLX5P7FTfT", "replyto": "sLX5P7FTfT", "signatures": ["ICLR.cc/2026/Conference/Submission6773/Reviewer_gZ1w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6773/Reviewer_gZ1w"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6773/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950882500, "cdate": 1761950882500, "tmdate": 1762919048464, "mdate": 1762919048464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We have revised and uploaded the draft with modifications clearly colored for ease of reference.\n\nIn addition, after the submission of our paper, a related work [Flatness is Necessary, Neural Collapse is Not: Rethinking Generalization via Grokking, Han+, NeurIPS2025] was made public. Since the title may appear to contradict our claims at first glance, we clarify that their findings are fully consistent with ours and highlight the novelty of our contribution.\n\n[Han+, 2025] discuss grokking in relation to neural collapse, which makes their setting similar to ours. Although the title suggests that neural collapse might not be relevant for generalization, this is **not** what it actually argues. Rather, they state that neural collapse appears earlier than the acquisition of generalization. The emergence of neural collapse does not align with that of generalization, and therefore it does not fully explain generalization ability. In contrast, they observe that flatness correlates more closely with the decrease in test loss and thus appears to offer a better explanation in their experiments.  \nHowever, **this empirical observation is entirely consistent with our analysis.** The apparent contradiction arises because [Han+, 2025] measure neural collapse using the *neural collapse clustering (NCC)* metric, which mixes multiple properties associated with neural collapse. In particular, NCC also captures the separation of class mean representations that naturally occurs through fitting the training set, and therefore it decreases earlier than the test loss. As we emphasize in Remark 4.2, our analysis isolates the contribution of within-class variance or RNC1 score, allowing us to separate out the cause of the generalization and correctly focus on the variance decrease that is actually responsible for the emergence of generalization in grokking.  \nFurthermore, the theoretical analysis in [Han+, 2025] is limited to showing that flatness is partially guaranteed under neural collapse. Their work address neither the connection to generalization nor the associated training dynamics. In contrast, as illustrated in Figure 1, our paper clarifies the relationships among several concepts, including information bottleneck, and it further characterizes their training dynamics through a neural-collapse-based analysis. This leads to a unified understanding of the mechanisms underlying grokking."}}, "id": "i38N9M11kG", "forum": "sLX5P7FTfT", "replyto": "sLX5P7FTfT", "signatures": ["ICLR.cc/2026/Conference/Submission6773/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6773/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission6773/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763488150722, "cdate": 1763488150722, "tmdate": 1763488150722, "mdate": 1763488150722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}