{"id": "jv1a8MPymM", "number": 18328, "cdate": 1758286460508, "mdate": 1759897110653, "content": {"title": "Stretching Beyond the Obvious: A Gradient-Free Framework to Unveil the Hidden Landscape of Visual Invariance", "abstract": "Uncovering which feature combinations are encoded by high-level visual units is critical to understanding how images are transformed into representations that support recognition. While existing feature visualization approaches typically infer a unit's most exciting images, this is insufficient to reveal the manifold of transformations under which responses remain invariant, which is critical to generalization in vision.\nHere we introduce Stretch-and-Squeeze (SnS), an unbiased, model-agnostic, and gradient-free framework to systematically characterize a unit’s maximally invariant stimuli, and its vulnerability to adversarial perturbations, in both biological and artificial visual systems. SnS frames these transformations as bi-objective optimization problems. To probe invariance, SnS seeks image perturbations that maximally alter (stretch) the representation of a reference stimulus in a given processing stage while preserving unit activation downstream (squeeze). To probe adversarial sensitivity, stretching and squeezing are reversed to maximally perturb unit activation while minimizing changes to an upstream representation.\nApplied to CNNs, SnS revealed invariant transformations that were farther from a reference image in pixel-space than those produced by affine transformations, while more strongly preserving the target unit's response. The discovered invariant images differed dramatically depending on the stage of the image representation used for optimization: pixel-level changes primarily affected luminance and contrast, while stretching mid- and late-layer CNN representations altered texture and pose respectively. \nBy measuring the extent to which the hierarchical invariant images obtained for robust (i.e., adversarially trained) networks were correctly classified by humans and other observer networks, we discovered a substantial drop in the interpretability of the invariant images when the representation was stretched at increasingly deep layers. Interestingly, the opposite trend was found for standard (i.e., not robustified) models, whose invariant images became increasingly more interpretable when stretching was applied to deeper layers. This indicates that adversarial training fails to increase the interpretability of high-level invariances, despite strong evidence of perceptual alignment between humans and robustified models at the pixel level. This demonstrates how SnS can be used as a powerful new tool to measure the alignment between artificial and biological vision.", "tldr": "Gradient-free optimization uncovers novel invariances in deep convolutional neural nets", "keywords": ["vision", "visual invariance", "feature visualization", "human-AI alignment", "deep convolutional neural networks", "NeuroAI", "psychophysics", "computational neuroscience", "robustness", "adversarial attacks", "evolutionary algorithm", "gradient-free optimization", "machine learning"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d21df94860df9eb9f897b413c6fe7eed6b0a358a.pdf", "supplementary_material": "/attachment/176818723c6f0d11b6439919b5b0911439f98ed2.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new method—Stretch and Squeeze (SnS)—for testing the invariance and adversarial sensitivity of neurons within deep neural networks. The approach can be applied to any two layers of a network to examine both upstream invariance and downstream adversarial sensitivity. The authors demonstrate that the SnS-generated invariant and adversarial images outperform those generated by simple affine transformations, and that these images better align with human perception. Applying this method also reveals differences between robust and standard models. The framework has potential applications for understanding the tuning properties of biological neurons."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The study unifies the measurement of neural invariance and adversarial sensitivity into a single computational framework, which is conceptually novel.\n2. The proposed method could serve as a useful tool for probing biological neural properties in future neuroscience research.\n3. The writing is clear and the structure is easy to follow."}, "weaknesses": {"value": "1. Although the paper is categorized under “Application to Cognitive & Neuroscience,” it does not actually include experiments or validations on biological neural data.\n2. The paper identifies differences between robust and standard networks but does not attempt to explain the origin or meaning of these differences.\n3. The figure quality can be substantially improved."}, "questions": {"value": "1. In the abstract, the authors claim that “adversarial training fails to increase the interpretability of high-level invariances”, but it is unclear how this conclusion was derived. It is just because its alignment with human perception is relatively low. This is interesting but why is the case? I understand the empirical observations, but I do not fully see how the presented results logically imply that adversarial training is ineffective. For example, in Figure 4C and 4E, differences between robust and standard models are apparent, but it remains unclear why these differences demonstrate the failure of adversarial training rather than simply reflecting distinct representational properties.\n\n2. The authors show that SnS can produce invariant and adversarial images that outperform those generated by affine transformations. However, an affine transformation may not be the most informative baseline. Since SnS involves two complementary losses (stretch and squeeze), it would be more convincing to compare it against a single-loss variant, such as using only the stretch term to generate invariant images. This comparison could more clearly highlight the contribution and necessity of the full SnS framework."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zXYNbA7fd1", "forum": "jv1a8MPymM", "replyto": "jv1a8MPymM", "signatures": ["ICLR.cc/2026/Conference/Submission18328/Reviewer_JP16"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18328/Reviewer_JP16"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964683156, "cdate": 1761964683156, "tmdate": 1762928042947, "mdate": 1762928042947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Stretch-and-Squeeze (SnS), a novel, gradient-free, and model-agnostic framework for systematically characterizing the invariance landscape and adversarial sensitivity of units in both artificial and biological visual systems. SnS frames these as bi-objective optimization problems using evolutionary algorithms (specifically CMA-ES) operating on the latent space of a generative model.\n\nFor probing invariance ($\\Xi_{inv}$), SnS seeks to maximize the distance (stretch, $L_{stretch}^{\\kappa}$) between a candidate image and a reference image in a chosen intermediate representation space ($\\kappa$), while simultaneously minimizing the change (squeeze, $L_{squeeze}^{l}$) in the activation of a target unit downstream ($l$). The dual objective is used for generating adversarial examples ($\\Xi_{adv}$).\n\n\nApplying SnS to standard and adversarially trained CNNs (ResNet50), the authors make several significant discoveries:\n\n1. Hierarchical Invariance: The nature of the discovered invariant images varied dramatically depending on the representation layer being stretched: pixel-level changes mainly affected luminance/contrast, mid-layers altered texture, and late-layers altered pose.   \n\n2. Interpretability of Robust vs. Standard Models: Adversarially trained (robust) networks yielded invariant images that were initially more human-interpretable than those from standard models. However, this advantage eroded or reversed at deeper layers: robust network invariances became less interpretable when stretching deeper, while standard network invariances became more interpretable.   \n\n3. Conclusion on Robustness: This suggests that adversarial training, while improving pixel-level perceptual alignment, fails to increase the interpretability of high-level invariances.  \n\nThe framework's versatility is demonstrated by its application to different CNN architectures (ResNet18, VGG16_bn) and Vision Transformers (ViT), and its potential utility in visual neuroscience experiments by showing efficacy even when only a small fraction of units are recorded."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Originality and Technical Novelty: SnS is a gradient-free, model-agnostic framework for systematically exploring the full invariance manifold of a unit, moving beyond local measures or pre-defined transformations. The bi-objective optimization scheme is elegant and powerful, unifying the search for invariant images and adversarial examples.  \n\n* High Quality and Rigor: The experimental design is robust. The authors not only apply the method to a benchmark (ResNet50) but also validate the findings across different architectures (ResNet18, VGG16_bn, ViT) and use a multi-observer and human classification setup for interpretability.  \n\n* Clarity of Insight: The results are not just empirical observations but yield a deep conceptual insight: the stark, hierarchical divergence in interpretability between standard and robust networks at deeper layers. This challenges existing conclusions about adversarial robustness and perceptual alignment.  \n\n* Broad Significance: The technique is directly applicable to visual neuroscience, offering a method to probe biological neurons without relying on perfect \"digital twin\" fidelity, which is a major constraint in current gradient-based neuro-visual studies."}, "weaknesses": {"value": "* Computational Cost: The reliance on evolutionary algorithms (CMA-ES) is a known trade-off for gradient-free and model-agnostic optimization. The computational cost can be very high, especially given the search space dimension ($n=4096$). While the results are excellent, the practical utility of SnS for very large-scale or high-throughput experiments may be limited compared to gradient-based methods.  \n\n* Generative Model Dependency: The quality and expressivity of the invariant images are fundamentally constrained by the generative model ($\\psi$) used. The authors assume a \"powerful prior\" is embodied in the model. The choice of generative model, and its potential bias on the invariant manifold, warrants further discussion or empirical analysis.  \n\n* Convergence Analysis: The paper notes that a significant percentage of runs did not reach the maximum iteration count (up to 86.75% for some standard network conditions). While the authors state the final populations still reached \"functionally relevant activation regimes\", a more detailed discussion on the impact of non-convergence or early stopping on the resulting Pareto front quality would strengthen the completeness of the method's presentation. \n\n* References: The authors also missed citing some critical works in this domain. For e.g. Extreme Image Transforms (EITs) [Crowder et al., 2022; Malik et al., 2023, Biol Cybernetics, Malik et al., 2023, arXiv] which present a novel view of structural changes in the input images, and MIRC [Ullman et al., 2016, PNAS] which present similar experiments with human and network observers. Authors should also mention a comparison of their method to that of EITs given they also make changes to the image space at varying levels and test against VOneResNet [Dapello et al., 2020, NeurIPS] that claims to explain the V1 variance, showing that EITs outperform VOneResNet by a significant margin."}, "questions": {"value": "1. Can the authors elaborate on the choice of the generative model $\\psi$? Specifically, how might a different generative model (e.g., a modern diffusion model vs. the cited Dosovitskiy & Brox, 2016 model) affect the sampled invariant manifold, and have the authors explored this?\n\n2. The paper focuses on an $L_2$ robust ResNet-50 ($\\epsilon=3$). Would the striking hierarchical divergence in interpretability also hold for $L_\\infty$-robust models, which tend to learn different features?\n\n3. Given the bi-objective nature, the results rely on the final solution from the Pareto front. Could the authors provide a more detailed analysis or visualization of the entire Pareto front for a few example units to demonstrate the trade-off space between stretch and squeeze?\n\n4. The method is described as model-agnostic. Have the authors considered or experimented with non-vision applications, for instance, characterizing invariances in audio or text models?\n\n5. Authors should mention the IRB number at the end of the main text and consider releasing code on a public platform. Authors should also provide details on the training and hardware setup. \n\n6. Have the authors tested this approach with larger networks that ResNet and ViTs? The authors should write a remark if they cannot make a direct comparison. \n\n7. Have the authors considered evaluating the human alignment with any fMRI or EEG datasets (like Natural Scenes Dataset (NSD)), especially in case of the digital twin analogy? \n\n8. I understand the author information was redacted for anonymity from line 492 in the reproducibility statement. Just a reminder to fill it in for the final version."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SQ46LIholY", "forum": "jv1a8MPymM", "replyto": "jv1a8MPymM", "signatures": ["ICLR.cc/2026/Conference/Submission18328/Reviewer_ac3Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18328/Reviewer_ac3Z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007799218, "cdate": 1762007799218, "tmdate": 1762928042254, "mdate": 1762928042254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Stretch-and-Squeeze (SnS), a gradient-free, model-agnostic framework to probe visual invariances and adversarial vulnerabilities in both DNNs and biological visual systems. SnS uses an evolutionary optimization strategy to get perturbations which stretch a representation, making it as different as possible, while squeezing a downstream activation, keeping it constant. Reversing these objectives will lead to adversarial examples. The authors claim an insightful finding with robust DNNs – invariant images are more human-recognizable at low levels but less so at deeper layers – which can be useful for further improving model adversarial robustness through human-model alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Well-motivated framework, good practical value.** SnS focuses on understanding neural network representations, a fundamental challenge in the field. In comparison to prior works, SNS doesn’t look at unit activation but explore the space of transformations with well-grounded setting – a generative model, a test network, and a gradient-free optimizer. It is model-agnostic and gradient-free, particularly important with black-box models and neuroscience applications where gradient information is unavailable. It provides insights for model generalization and robustness. \n- **Comprehensive experimental evaluation and clear presentation.** The paper includes experiments with various architectures such as ResNet-18, ResNet-50, VGG16, ViT, as well as human observers with rigorous statistical analysis. This shows the generalizability of SnS. Figures (e.g., figure 1 and 4) are well-presented to illustrate the key concepts and findings. \n- **Interesting findings on adversarial training.** The divergent trends between robust and standard networks across layers (figure 4) is interesting. This hints that adversarial training does not uniformly improves human alignment and suggests that the robustness gain only align low-level, as opposed to high-level, invariances with human perception, which is informative for future investigation."}, "weaknesses": {"value": "We thank the authors for submitting the paper to ICLR 2026! There are a few weaknesses listed below which I believe can make the paper better.\n- **Lack of analysis on computational cost.** The paper uses the covariance matrix adaption evolutionary strategy for d(=4096)-dimentional codes but didn’t provide analysis of computational requirements, convergence properties, or comparisons with other gradient-based methods. How does this scale with network depth or code dimensionality? These considerations can be useful in practice. \n- **Limited direct comparison to metamers or other invariance mapping techniques.** While the paper conceptually compares SnS with prior works such as metamers, empirical side-by-side comparisons would strengthen claims about its competence or unique coverage of the invariance space. \n- **Lack of analysis on invariance manifold geometry.** While SnS generates multiple invariant images (figure 3), there’s limited analysis of the manifold structure itself. Information such as intrinsic dimensionality or whether the discovered invariances concentrated in certain directions or are uniformly distributed would be worth looking into."}, "questions": {"value": "- How do you determine if optimization has converged? Do invariant images stabilize or do they continue to evolve? Some convergence curves would be good to see. \n- For CNN experiments where gradients are available, how do SnS-discovered invariances compare to those found via gradient-based methods? This would help validate the gradient-free approach. \n- Does the invariance found with different model architectures transferable? Are those more dependent on model architectures or training data or some other factors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sajycJJJH6", "forum": "jv1a8MPymM", "replyto": "jv1a8MPymM", "signatures": ["ICLR.cc/2026/Conference/Submission18328/Reviewer_oZyT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18328/Reviewer_oZyT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762139686847, "cdate": 1762139686847, "tmdate": 1762928041719, "mdate": 1762928041719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a framework for examining invariances and adversarial vulnerabilities of a CNN representation, in which\nbase images are modified simultaneously to maximize euclidean distance to the base in one layer while minimizing it in another.\n\nThey demonstrate through examples that the method can generate adversarial and invariant images, and that these are recognizable by humans or other networks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The topic is important: the ML community needs tools for characterizing fundamental aspects of network function.\n\nThe general approach taken by the authors is well motivated and novel (although see below for some related literature that should be cited)."}, "weaknesses": {"value": "I like the conceptuatlization, but the construction of the method is ad hoc and not well explained, and the experimental results are somewhat anectdotal and not fully convincing.\n\nFirst, the method.  I thought I understaood the basic construction after reading abstract/intro, but then found the more precise description in the  Methods section confusing.  Some specifics:\n\n- Eq. (2) is a non-standard notation for a dual objective: Presumably one wants to either trade these two terms off (in which case they should be added, with a multiplier), or one wants to hold one fixed while minimizing the other.  Alternatively, one could take the quotient.  As written, I can't tell what was done for the examples in the papepr (see below).  \n\n- The methods section starts with description of a 3-component implementation, but provides no motivation for this. Specifically, why is there a need for a separate generative model?  Or a separate gradient-free optimizer?  I would think a unified algortihm, in which adjustments to the input are made by backpropagating gradients of the objective through the test network, would be easier to implenent, test, and interpret.\n\nThe empirical results:\n\n- The method is stated in intro/abstract as seeking perturbations that trade off the L2 norm of activations at two different levels {k,l} of a network. But as far as I can tell, all experiments set one of these \"levels\" to be the input (level 0), and it's not clear to me how one would interpret a tradeoff bertween two internal levels.  I think readers would be better served by defining the method with respsect to images, and mentinoning the potential generalization in the Dicussion.\n\n- The methods is defined in terms of the MSE of two layers, but most of the experiments seem to refer to individual readout units.  Why not MSE of the readout layer (or layer before)?\n\n- Figure 2 shows a single example image (an MEI), with a  corresponding adversarial and invarient example.  Although it succeeds in demonstrating that the optimization is pushing the examples in the right direction (the ratios of pixel MSE and readout reduction are opposite), the images do not seem  perceptually convincing. Why was an MEI used for this (rather than a photograph)?  Also, I can't understand why the vertical axis is not Euclidean distance in readout layer (as used in the objective).\n\n- Figure 3 shows examples of invariant images for a single MEI, computed for three different layers.  The text describes these as providing evidence that they corespond to luminance/contrast changes, texture/color changes, and viewpoint or multiplicity of object changes.\nI don't find this very  convincing: The first row includes color changes, size chnages and multiplicity changes.  The third row inclues color chnages\nAlso, not clear why MEI's were used for the base images in this experiment.\n\n- Since the combination of the two objective components is not specified, I could not understand the interpretation of the perceptual study. To make comparisons of performance on invariant images generated for different layers, it would seem essential to match the conserved objective (output layer?), rather than allow them to \"float\" along the pareto front (as described in end of sec 3.1).  Again, I could not find an explanation of how the two objectives were combined or controlled.  \n\n- Related work: the approach bears similarity to two methods that have been proposed for comparing models by searching for stimuli that best discriminate them (in some cases, holding one model fixed while maximizaing/minimizing the other): \"controversial stimuli\" (Golan et al 2020), and \"MAD competition\" (Wang et al 2008).  These should probably be cited in the \"related works\" section."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1917XfAfX4", "forum": "jv1a8MPymM", "replyto": "jv1a8MPymM", "signatures": ["ICLR.cc/2026/Conference/Submission18328/Reviewer_N9Ge"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18328/Reviewer_N9Ge"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762169765085, "cdate": 1762169765085, "tmdate": 1762928041100, "mdate": 1762928041100, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}