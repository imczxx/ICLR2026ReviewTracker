{"id": "odatOcBi61", "number": 3575, "cdate": 1757479017525, "mdate": 1759898080565, "content": {"title": "NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching", "abstract": "Next-generation multimodal foundation models capable of any-to-any cross-modal generation and multi-turn interaction will serve as core components of artificial general intelligence systems, playing a pivotal role in human-machine interaction. However, most existing multimodal models remain constrained by autoregressive architectures, whose inherent limitations prevent a balanced integration of understanding and generation capabilities. Although hybrid and decoupling strategies have been explored to address these tasks within unified frameworks separately, their redundant, non-integrated designs limit their applicability to broader scenarios, such as cross-modal retrieval. In this work, we introduce NExT-OMNI, an open-source omnimodal foundation model that achieves unified modeling through discrete flow paradigms. By leveraging metric-induced probability paths and kinetic optimal velocities, NExT-OMNI natively supports any-to-any understanding and generation with enhanced response efficiency, while enabling broader application scenarios through concise unified representations rather than task-decoupled designs. Trained on large-scale interleaved text, image, video, and audio data, NExT-OMNI delivers competitive performance on multimodal understanding and generation benchmarks, while outperforming prior unified models in multi-turn multimodal interaction and cross-modal retrieval, highlighting its architectural advantages as a next-generation multimodal foundation model. To advance further research, we will release training details, data protocols, and open-source both the code and model checkpoints.", "tldr": "", "keywords": ["Omnimodal", "Multimodal Learning", "Discrete Flow Matching"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5e50c387e04c3124c4bb450763952d513bd7cb0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In all, NExT-OMNI is a well-executed application model that demonstrates strong engineering by effectively integrating Discrete Flow Matching into a 7B-scale, any-to-any multimodal system., But its innovation is combinatorial rather than fundamental and lacks analysis of its slight underperformance in speech tasks and doesn't prove DFM's efficacy on strictly sequential tasks. Moreover, the application of NExT-OMNI on large scale model remains to verify."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.NExT-OMNI first trains a model with \"any to any\" on the 7B scale and achieves SOTA performance on multiple tasks, expanding the application of Discrete Flow Matching(DFM).\n\n2.It does realizes the full integration of DFM, unified discrete representation, lightweight multi-head, parallel decoding, and multi-turn multimodal instruction tuning methods, and provides design details and a series of data synthesis. Relevant ablation experiments prove the effectiveness of each part of the model.\n\n3.For the first time, the paper combines block size padding with dynamically adjusting the preset generation length in steps of block size based on <EOS> confidence, which not only saves computing power but also prevents truncation and loss of generated content."}, "weaknesses": {"value": "1.The paper is a combinatorial innovation at the methodological level, its contributions lie in engineering implementation and scaling up to a 7B full-modal model, without proposing new ideas for addressing the problem of balancing model understanding and generation tasks.\n\n2.In Table 2, NExT-OMNI seems to be slightly inferior in speech-to-speech tasks comparing with auto-regressive models, but no further explanation or analysis of it.\n\n3.The paper does not experimentally demonstrate how DFM, which inherently lacks such an inductive bias, can match or surpass auto regressive models on tasks that strictly require sequential adherence such as code generation and step-by-step reasoning.\n\n4.In Figure 22, the paper only qualitatively illustrates the model’s ability to “think with images”, but it does not systematically evaluate how effectively the model leverages its generation capability for complex reasoning tasks. For example, there is no assessment of whether the model can assist mathematical or logical reasoning by generating images, nor are quantitative metrics such as MMU[1], MMBench[2], or ScienceQA[3].\n\n**References:**\n\n[1] Yue X., Ni Y., Zhang K. et al. “MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI.” arXiv:2311.16502, 2024.\n\n[2] Liu Y., Zhang H., Chen J. et al. “MMBench: Is Your Multi-modal Model an All-around Player?” arXiv:2307.06281, 2023.\n\n[3] Lu P., Mishra S., Xia T. et al. “Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering.” NeurIPS 2022."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5iSgduPam8", "forum": "odatOcBi61", "replyto": "odatOcBi61", "signatures": ["ICLR.cc/2026/Conference/Submission3575/Reviewer_MbBb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3575/Reviewer_MbBb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761477009934, "cdate": 1761477009934, "tmdate": 1762916839294, "mdate": 1762916839294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a DLLM-like model that aims to build a unified, end-to-end architecture for multimodal understanding and generation. The authors conduct detailed experiments and evaluate the model across three modalities—text, audio, and image—demonstrating its overall effectiveness and strong performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed architecture is highly innovative. To the best of my knowledge, this is the first unified model designed for discrete-unit generation and understanding across all modalities.\n\n- In terms of performance, the model outperforms several existing approaches (although many of the baselines are not state-of-the-art)."}, "weaknesses": {"value": "- The comparison with baselines is not sufficiently comprehensive. Important recent models such as Qwen2.5-Omni and Qwen3-Omni, which outperform Next-Omni on OmniBench, should be included for a fair evaluation. Similarly, XCodec2 represents the most recent state-of-the-art in audio tokenization and should be considered as a baseline.\n\n- For audio-related tasks, perceptual audio quality is critical. It would be very helpful if the authors could provide qualitative examples or audio case studies to allow readers to better assess the perceptual quality of the generated audio. I look forward to seeing such demonstrations in future revisions."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PibigRFkzL", "forum": "odatOcBi61", "replyto": "odatOcBi61", "signatures": ["ICLR.cc/2026/Conference/Submission3575/Reviewer_DQzr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3575/Reviewer_DQzr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933099981, "cdate": 1761933099981, "tmdate": 1762916836401, "mdate": 1762916836401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents NExT-OMNI, which is an omnimodal foundation model built on discrete flow matching (DFM), targeting unified multimodal understanding and generation. It addresses the limitations of autoregressive (AR) approaches, which suffer from inherent conflicts between understanding and generation tasks as well as slowed inference due to decoupled designs. Unlike these, DFM introduces parallel information processing with bidirectional integration, enabling both efficient any-to-any cross-modal generation and enhanced multimodal understanding. Besides, NExT-OMNI achieves precise cross-modal retrieval and robust multi-turn multimodal interactions, surpassing prior AR-based and hybrid approaches. The model consistently delivers competitive or superior performance with reduced latency across standard multimodal benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. NExT-OMNI introduces a novel unified modeling approach using DFM to integrate multimodal understanding and generation tasks. Unlike previous work, which requires additional diffusion decoders and increases parameter size, NExT-OMNI achieves compactness by reducing the need for extra modules. Its bidirectional feature fusion design effectively enhances cross-modal interactions, enabling better feature integration across modalities. \n\n2. NExT-OMNI introduces dynamic length generation optimization. By using the EOS token's confidence scores, the model dynamically adjusts text generation lengths. This can effectively improve multimodal understanding and generate more natural outputs in text-based tasks.\n\n3. NExT-OMNI integrates an adaptive caching mechanism to leverage the parallel decoding strengths of DFM, leading to a 1.2× increase in inference speed compared to AR architectures. \n\n4. The model is tested not only on single-turn tasks such as text-to-image and text-to-audio generation but also on real-world scenarios requiring multi-turn interactions. It demonstrates clear improvements in unified understanding and generation capabilities, particularly in dynamic multi-turn exchanges."}, "weaknesses": {"value": "1. While the discrete DFM effectively unifies understanding and generation tasks, its reliance on discrete representations, as opposed to continuous flow-based approaches, may lead to some performance degradation in generation tasks due to information loss during the discretization process.\n2. After completing the encoder warmup phase, the model requires joint optimization with reconstruction losses during subsequent multi-stage training. This additional reconstruction loss, compared to exclusively optimizing with cross-entropy loss, can inevitably lower training efficiency and lengthen the overall training process.\n3. Recent studies [1] show that discrete DFM and diffusion-based models demand more computational resources during training compared to autoregressive architectures. This is attributed to the complexity of learning difficult tasks within the discrete modeling framework.\n\n[1] Training Optimal Large Diffusion Language Models."}, "questions": {"value": "1. Lumina-Dimoo [2] also adopts a similar discrete diffusion modeling approach. Why was there no comparison with it? The authors should further clarify this issue.\n\n2. While unified modeling for both generation and understanding has become a trend, it seems that separating these tasks and modeling them independently yields better performance. Why is it still necessary to persist with a unified model design?\n\n3. The appendix demonstrates that increasing the number of sub-token tables in the warmup encoder improves reconstruction performance. Why is there still a need to aggregate these into high-dimensional features for the backbone, and is there experimental evidence to support this design choice?\n\n   \n\n[2] An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AXOcyOlRNH", "forum": "odatOcBi61", "replyto": "odatOcBi61", "signatures": ["ICLR.cc/2026/Conference/Submission3575/Reviewer_T8p4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3575/Reviewer_T8p4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962888628, "cdate": 1761962888628, "tmdate": 1762916836102, "mdate": 1762916836102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces NExT-OMNI, an open-source omnimodal foundation model that achieves unified modeling of understanding, generation, and retrieval across text, images, video, and audio using discrete flow matching (DFM) techniques. Unlike existing autoregressive (AR) models that struggle with conflicts between understanding and generation tasks, or hybrid architectures that rely on task-specific decoupling, NExT-OMNI employs a streamlined unified architecture. The model leverages metric-induced probability paths and kinetic optimal velocities to enable bidirectional information integration, achieving faster inference through parallel decoding. Useful strategies include reconstruction-enhanced unified representations, dynamic length generation strategies, and vanilla adaptive caching. Experimental results demonstrate competitive performance on standard benchmarks while excelling at multi-turn multimodal interaction and cross-modal retrieval tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Unified Architecture: Successfully demonstrates that a single DFM-based architecture can handle understanding, generation, and retrieval, challenging the dominance of the AR-based paradigm.\n\n- Comprehensive Evaluation: Extensive experiments across 7+ benchmarks covering all major modalities and tasks, with careful ablation studies validating design choices.\n\n- Useful strategies: Provides reconstruction-enhanced unified representations training, dynamic length generation strategies, and vanilla adaptive caching, which are quite helpful to the development of the community.\n\n- Inspiring results: Effectively demonstrates how unified representations enable superior retrieval performance compared to decoupled architectures.\n\n- Open Source Commitment: Authors promise to release code, models, and training protocols, which would significantly benefit the community."}, "weaknesses": {"value": "- I see no other major weakness. Thanks for the authors' hard work."}, "questions": {"value": "- It would be better if the authors could provide detailed training costs (e.g., GPU hours) for the proposed model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2TdNplI5PJ", "forum": "odatOcBi61", "replyto": "odatOcBi61", "signatures": ["ICLR.cc/2026/Conference/Submission3575/Reviewer_nB7e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3575/Reviewer_nB7e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986309433, "cdate": 1761986309433, "tmdate": 1762916835454, "mdate": 1762916835454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}