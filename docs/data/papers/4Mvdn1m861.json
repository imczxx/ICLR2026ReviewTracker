{"id": "4Mvdn1m861", "number": 10378, "cdate": 1758168918070, "mdate": 1759897654951, "content": {"title": "TokenCount: A Training-Free Framework for Object Counting by Interpreting Output Tokens", "abstract": "Object counting is a critical computer vision task with widespread applications in manufacturing, traffic monitoring, and crowd analysis. Recent class-agnostic object counting methods leveraging the Segment Anything Model (SAM) are limited by the inherent uncertainty of the similarity metric derived from its image encoder. While solutions incorporating additional encoders can refine this similarity, they face challenges due to high computational costs. To overcome this challenge, we propose a novel framework that consists of two critical components working in synergy with SAM. We propose a probabilistic prompt generation stage and an output token-based verification stage. The probabilistic prompt generation stage efficiently generates prompts based on probability distributions from SAM's image embedding, while the output token-based verification stage uses SAM's output tokens to effectively distinguish between positive and negative instances. Experimental results show our method achieves superior accuracy with an MAE of 16.25, outperforming existing training-based and training-free counting methods. Notably, our method achieves comparable performance to training-free approaches that require additional models alongside foundation models. Particularly, on the CARPK dataset, our method achieves superior performance, outperforming all supervised methods and demonstrating comparable results against training-free counting methods. Furthermore, ablation studies prove that this performance gain is critically attributed to two key components. This study not only presents an effective solution for object counting but also showcases the potential of applying foundation models to downstream tasks without fine-tuning and additional models.", "tldr": "This paper proposes a novel training-free framework for object counting that uses the SAM without any modifications. The key innovation is directly analyzing SAM's internal 'output tokens' to accurately identify and count objects.", "keywords": ["Few-shot object Counting", "Training-free", "SAM", "Output Token", "Probability Distribution"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/64562a7bc65831c26ce791f4d778ffcac27dbe70.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a method for class-agnostic object counting that fully exploits SAM without any additional training or auxiliary models. It combines probabilistic prompt generation, which adaptively explores image regions based on SAM’s embeddings, with output token-based verification, which directly interprets SAM’s decoder tokens using a geometric similarity metric (TS-SS) to distinguish true object instances. The method achieves competitive or superior accuracy to both training-based and training-free approaches on FSC-147 and CARPK datasets while maintaining computational efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "(1) This paper introduces a training-free framework that leverages SAM’s architecture without any fine-tuning or auxiliary encoders. \n\n(2) It achieves strong results, outperforming prior training-free and some supervised methods, with comprehensive ablation and qualitative analyses validating each component’s effectiveness."}, "weaknesses": {"value": "(1) The probabilistic prompt generation module iteratively classifies masks as positive or negative and generates new prompts accordingly. However, it is unclear how these classifications are determined. For example, in Figure 3, during the second iteration, masks with probabilities 0.127 and 0.055 are marked as positive, whereas the mask with a much higher probability of 0.86 is marked as negative, and the one with 0.071 is not selected at all. Could the authors clarify the criteria or mechanism behind this selection process?\n\n(2) The iterative probabilistic prompt generation process likely increases time complexity. However, despite the method’s emphasis on efficiency and its training-free design, computational scalability and real-time applicability have not been evaluated on large-scale or high-resolution datasets.\n\n(3) It is unclear how the TS-SS metric operates in practice. While it is defined as a similarity measure between two vectors, the paper does not clearly explain how this metric facilitates clustering of similar objects, as mentioned in lines 234–235."}, "questions": {"value": "(1) Since TS-SS combines angular and magnitude components, how is it normalized to handle scale variations in token embeddings? Is there empirical or theoretical justification that TS-SS induces a clustering structure suitable for object instance separation? What is the motivation on designing TS-SS metric?\n\n(2) What is the computational complexity (in terms of prompts or tokens) per iteration, and how does it scale with image size or object density? Have the authors benchmarked inference time or memory usage across varying resolutions (e.g., 512×512 vs. 2048×2048)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rSfB6fI3dN", "forum": "4Mvdn1m861", "replyto": "4Mvdn1m861", "signatures": ["ICLR.cc/2026/Conference/Submission10378/Reviewer_tXFk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10378/Reviewer_tXFk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761518064630, "cdate": 1761518064630, "tmdate": 1762921699553, "mdate": 1762921699553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a training-free, class-agnostic object counting method. Built upon Segment Anything Model (SAM), the proposed pipeline consists of two stages, namely probabilistic prompt generation and output token-based verification. The former samples point prompts according to the probability distribution derived from SAM’s image encoder, while the latter identifies positive and negative instances by analyzing the output tokens of SAM. These two stages operate iteratively until a predefined criterion is met. Evaluations on two benchmarks show that the proposed method performs favorably against training-based and training-free counting methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper demonstrates how SAM can be repurposed as a training-free, class-agnostic object counting model, highlighting SAM’s potential for specialized downstream tasks without additional training.\n* The proposed method comprises two modules, including probabilistic prompt generation and output token-based verification. These modules iteratively generate point prompts and predict objects, achieving competitive counting performance."}, "weaknesses": {"value": "* The significance and benefits of the proposed training-free method require further clarification. Admittedly, training-free SAM-based counting was indeed a promising direction in 2023. However, a large number of advanced methods [1,2,3] have been proposed in 2024, which substantially outperform training-free methods. The paper would benefit from a more nuanced discussion of where the proposed method stands relative to recent methods and what specific advantages it offers.\n* Missing analysis on computational efficiency. As the authors claim that the proposed method could enhance computational efficiency by reducing redundant prompt generation, it would be necessary to analyze the computational cost of each module and compare with vanilla SAM baselines (e.g., inference with a fixed number of point prompts).\n* The necessity of the proposed output token vector space remain questionable. As shown in Table 5, using the original image embedding already achieves similar performance compared to the output token-based variant (16.94 MAE vs. 16.25 MAE). In addition, visualizations in Figure 5 do not necessarily support that the output token space is better than image embedding, as the positive and negative objects are still separable under image embedding setting.\n* The proposed similarity metric, dubbed TS-SS Sim, does not show superiority over existing metrics such as cosine similarity and L2 distance. As shown in Table 3, replacing cosine similarity (16.48 MAE) with TS-SS Sim yields negligible improvement (16.25 MAE). This raises concerns regarding the necessity of TS-SS Sim.\n* While zeroing out negative values in the similar map helps reduce redundant background prompt sampling, it is possible that those discarded regions contain true positive objects, particularly in congested and challenging scenarios. It would be beneficial to analyze when does the probabilistic distribution fail to capture actual objects. \n\n**Reference**\n\n[1] CountGD: Multi-Modal Open-World Counting. NeurIPS 2024.\n\n[2] A Novel Unified Architecture for Low-Shot Counting by Detection and Segmentation. NeurIPS 2024.\n\n[3] DAVE – A Detect-and-Verify Paradigm for Low-Shot Counting. CVPR 2024."}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "veOkHCS4Eg", "forum": "4Mvdn1m861", "replyto": "4Mvdn1m861", "signatures": ["ICLR.cc/2026/Conference/Submission10378/Reviewer_vU7v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10378/Reviewer_vU7v"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761562956551, "cdate": 1761562956551, "tmdate": 1762921698541, "mdate": 1762921698541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TokenCount, a training-free, class-agnostic counting (CAC) framework that reuses Segment Anything (SAM) to bypass the need for additional training or fine tuning. The proposed method leverages SAM decoder tokens for similarity computation to improve interpretability in CAC relative to methods that rely on image-level embeddings, and it introduces the TS-SS geometric metric as the similarity metric for object instance count. In terms of architecture, TokenCount couples probabilistic prompt generation with output token verification. The former generates prompts by iteratively sampling from probability distributions derived from image embeddings of SAM, and the latter uses decoder output tokens of SAM to distinguish positive from negative instances. On FSC 147 and CARPK, TokenCount achieves SOTA performance among methods that rely solely on SAM for counting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Profound insights**: The paper provides compelling evidence where decoder tokens provide a more effective, instance-aware representation for counting than image embeddings. It recasts class-agnostic counting as training-free exemplar matching on SAM decoder tokens and combines this novel representation with probabilistic prompt generation (PPG) with the TS-SS geometric similarity metric.\n- **Robust performance**: The method demonstrates competitive accuracy on FSC-147 and CARPK among training-free baselines that rely solely on SAM. These gains are consistent across shot regimes and target densities, with the effectiveness further validated by ablation studies.\n- **Simple pipeline**: The method uses SAM in an end-to-end manner and does not require external encoders, detectors, or fine-tuning, rendering the algorithmic steps concise and reproducible."}, "weaknesses": {"value": "-\t**Efficiency concerns**: The probabilistic prompt generation iterations require repeated decoder computations, leading to substantial computational overhead and efficiency degradation.\n-\t**Lack of convergence guarantees**: The probabilistic prompt generation iterations lack formal convergence guarantees and do not ensure separability between positive and negative samples in the feature space. This limitation precludes establishing a lower bound on the performance.\n-\t**Unclear method description**: The section on output token verification does not explain how the final count is produced. While the similarity metrics are well discussed, the methodological pipeline remains incomplete. The mechanism and parameters for Non-Maximum Suppression (NMS) or clustering are missing.\n-\t**Inadequate discussion and analysis**: The paper emphasizes token-based counting over embedding-based counting. A rigorous analysis of failure cases and a sound examination, at least an empirical one, of the iteration convergence are essential. However, the Discussion section only briefly addresses these issues, and a deeper exploration would enhance the contribution of this paper."}, "questions": {"value": "-\tIf TokenCount treats a target as a single token, how does it handle larger targets that span multiple tokens? What failure modes are likely to occur due to the aggregation threshold, and how can they be mitigated?\n-\tRegarding Table 4, several questions arise about the definition and use of prompts. The paper does not clearly describe the ablation setup. What does the reported number of prompts denote? For example, if the first row uses 8 iterations with 32 prompts per iteration, the theoretical upper bound is 256 prompts. Why does the observed average number of prompts already approach this bound?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "w0sjdZZHfR", "forum": "4Mvdn1m861", "replyto": "4Mvdn1m861", "signatures": ["ICLR.cc/2026/Conference/Submission10378/Reviewer_9GU5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10378/Reviewer_9GU5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810218874, "cdate": 1761810218874, "tmdate": 1762921698088, "mdate": 1762921698088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TokenCount, a training-free, class-agnostic object counting framework built entirely upon the native architecture of the Segment Anything Model (SAM). The core contributions are twofold: (1) a probabilistic prompt generation stage designed to efficiently sample potential object regions within an image, and (2) an output token-based verification stage that identifies and distinguishes target instances by directly measuring the similarity of SAM's decoder output tokens."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors conduct experiments on the FSC-147 and CARPK benchmarks, demonstrating that their method achieves strong performance among training-free approaches, even outperforming some training-based methods on certain metrics, particularly when using only SAM without auxiliary models. The idea of leveraging output tokens for direct verification to circumvent the ambiguity of similarity metrics in the image embedding space is clever."}, "weaknesses": {"value": "The paper presents probabilistic prompt generation as a key contribution to improve efficiency over grid-based or superpixel sampling. However, the core of this stage is the computation of a cosine similarity map between the exemplar and query image embeddings, followed by sampling from it. This idea is quite common in exemplar-based methods and is closely related to the principle of attention mechanisms. While the proposed iterative update strategy for the probability distribution is a nice implementation detail, the fundamental concept lacks sufficient novelty. The paper would better highlight its unique contribution by providing a more thorough comparison with other adaptive or attention-based sampling strategies.\n\n\nThe paper acknowledges performance degradation in scenarios with \"dense distributions of very small instances,\" attributing it to SAM's failure to generate clean segmentation masks, which in turn hampers the token verification stage . This is a critical limitation for a general-purpose counting method. While the discussion section mentions this, it lacks a deeper analysis. For instance, how does the probabilistic prompt generation strategy behave in these dense regions? Does it over-sample or under-sample? How exactly does token verification fail when masks heavily overlap? Providing more failure cases like those in Figure 8 and dissecting them with respect to each module of the method would help to more clearly define the method's application boundaries .\n\n\nThe paper describes an iterative process where newly identified positive samples are added to the exemplar list, and the probability map is updated to suppress already-verified regions . This process is crucial to the final performance, yet its dynamics are not fully explored."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sxQKHSoD1X", "forum": "4Mvdn1m861", "replyto": "4Mvdn1m861", "signatures": ["ICLR.cc/2026/Conference/Submission10378/Reviewer_WjNt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10378/Reviewer_WjNt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915328550, "cdate": 1761915328550, "tmdate": 1762921697715, "mdate": 1762921697715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}