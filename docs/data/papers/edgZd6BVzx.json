{"id": "edgZd6BVzx", "number": 16728, "cdate": 1758268114917, "mdate": 1759897222464, "content": {"title": "Efficient Fine-Tuning via Behavior-Guided Spectral Alignment", "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has become a practical approach for adapting large vision models with limited data and computational resources. However, existing PEFT methods primarily focus on where to inject trainable parameters, providing little guidance on how internal representations evolve during adaptation. This often results in a passive fine-tuning process that lacks explicit alignment with the target task's structure, especially in settings with limited data or diverse tasks. We propose Behavior-Aligned Fine-Tuning (BAFT), a simple, parameter-free and teacher-free method that introduces behavioral constraints during fine-tuning without changing the model architecture. BAFT extracts the relational structure of model predictions, capturing how samples relate in the output space, and aligns it with intermediate feature representations by minimizing the distance between their cosine similarity matrices. This alignment acts as a lightweight, task-aware regularizer that guides internal representations to better reflect the decision structure of the target task. BAFT requires no additional trainable parameters, adds minimal overhead, and integrates seamlessly with a wide range of PEFT methods including LoRA, AdaptFormer, Bi-LoRA, and Bi-AdaptFormer. On VTAB-1k and few-shot fine-grained classification benchmarks, BAFT consistently improves performance compared to strong PEFT baselines. Analyses of gradient behavior, spectral alignment, and attention dynamics further demonstrate how BAFT promotes more structured and task-aligned representations. By transforming output-space behavior into actionable training signals, BAFT reframes fine-tuning as an active and guided process. This work offers a novel and principled direction for advancing parameter-efficient model adaptation.", "tldr": "BAFT adds behavioral guidance to PEFT by aligning internal features with model predictions", "keywords": ["Behavior alignment", "representation learning", "teacher-free learning", "low-data learning", "internal representation dynamics", "efficient transfer learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b378d47e4ec28f8acaf002fcf9e78a3eb38ed648.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a parameter-efficient fine-tuning approach that creatively guides via the supervision between feature-level output and model output. Results show that without adding additional parameters, this attempt can increase the performance. The method itself is simple and effective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is easy to follow, with the observation that feature-level and output-level supervision can be a useful \"self-improvement\" on downstreams.\n\n2. The experiments are somewhat comprehensive, while more methods (see weakness) should be included for completeness.\n\n3. The discussions on eigenvalues are sound, and they support the reason for using the behavior-guided alignment. However, the motivation for using it is still unclear to me."}, "weaknesses": {"value": "1. Why do the authors think the distance measurement is significant/important? For a design perspective, it is unnatural to think that the distance across different layers (prediction space and feature space) is important and should align with each other, as the layer-wise outputs may specialize in different levels of pattern recognition. The motivation for doing this is not clear to me.\n\n2. More baselines can be reported in the main results. The latest method included in this study is from 2024. Other PEFT approaches [1-3] can be included for completeness. \n\n3. The feature-level supervision idea is very similar to some knowledge distillation approaches, which supervise pair-wise signals between teacher and student. Do you think it is reasonable to add more discussions on knowledge distillation supervision? I notice that the authors use the word \"teacher-free\" to represent their supervision signal. Thus, it makes sense to me to add additional discussions [4-6].\n\n\n[1] Visual Fourier Prompt Tuning\n\n[2] Visual Variational Autoencoder Prompt Tuning\n \n[3] Visual instance-aware prompt tuning\n\n[4] Cross-layer distillation with semantic calibration\n\n[5] Ad-kd: Attribution-driven knowledge distillation for language model compression\n\n[6] AMD: Automatic Multi-step Distillation of Large-scale Vision Models"}, "questions": {"value": "Format problem (does not affect my ratings): Sec. 3 Method title is solely on page 3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1ZAd0li9Ev", "forum": "edgZd6BVzx", "replyto": "edgZd6BVzx", "signatures": ["ICLR.cc/2026/Conference/Submission16728/Reviewer_rM6W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16728/Reviewer_rM6W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760666339864, "cdate": 1760666339864, "tmdate": 1762926778997, "mdate": 1762926778997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Behavior-Aligned Fine-Tuning (BAFT), a parameter-free, teacher-free add-on for Parameter-Efficient Fine-Tuning (PEFT) methods. Instead of adding new trainable parameters, BAFT aligns intermediate feature representations with the relational structure of the model’s own predictions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of this work is interesting -- further exploiting existing training signals as behavioral guidance. \n2. The evaluation is thorough. \n3. BAFT achieves consistently better results compared to PEFT baselines."}, "weaknesses": {"value": "1. BAFT only seem to be effective for Bi-AdaptFormer as Figure 4 shows, contradicting with \"models trained with BAFT consistently produce\n more focused and semantically meaningful activation maps\" in introduction. \n2. The authors seem to claim that the lack of behavioral oversight is a key limitation of PEFT methods, however, it is not well-supported by experiments, as the improvements with BAFT seem minor."}, "questions": {"value": "1. It's commonly acknowledged that the evaluation metric is better aligned with the loss, so the evaluation metric gets directly optimized. However, L_BAFT is included that does not directly reflect the evaluation metric, why is it not a disturbing term? Could you provide discussions why this approach is better than directly optimizing, e.g., cross-entropy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tMTWngbkcv", "forum": "edgZd6BVzx", "replyto": "edgZd6BVzx", "signatures": ["ICLR.cc/2026/Conference/Submission16728/Reviewer_PS61"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16728/Reviewer_PS61"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761200142911, "cdate": 1761200142911, "tmdate": 1762926778221, "mdate": 1762926778221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Behavior-Aligned Fine-Tuning (BAFT), a parameter-free, teacher-free regularization method that improves parameter-efficient fine-tuning (PEFT) of large vision models. Instead of modifying model architecture or adding trainable parameters, BAFT aligns intermediate feature similarities with the relational structure in the model’s prediction space, using cosine similarity matrices as behavioral guidance. The authors provide both theoretical analysis, which shows BAFT as a spectral alignment process, and empirical evidence demonstrating consistent performance gains across VTAB-1k and few-shot fine-grained visual classification tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The behavior-guided process is novel and intuitive.\n2. The paper is well-written and organized, with a clear structure."}, "weaknesses": {"value": "1. The improvements in emperical results seems small, which might raise questions aboy=ut the practical significance.\n2. The method is tested only on ViT-based architectures and classification, maybe it is better to verify on some other applications, such as LLM.\n3. How do you deal with the data with a large scale batch size?\n4. Could you compare with other methods, such as manifold alignment or relational distillation?"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OTKhzWHCAw", "forum": "edgZd6BVzx", "replyto": "edgZd6BVzx", "signatures": ["ICLR.cc/2026/Conference/Submission16728/Reviewer_f3ms"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16728/Reviewer_f3ms"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965274939, "cdate": 1761965274939, "tmdate": 1762926777325, "mdate": 1762926777325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes behavior alignment fine-tuning (BAFT), targeted at parameter-efficient fine-tuning (PEFT) settings. The method uses an alignment loss that tries to match the representation similarity and final output similarity of sample pairs within a batch. The paper also conducts gradient and spectral analysis of the method. Experiments show improvement when used with existing PEFT methods on computer vision fine-tuning datasets using ViT. Ablation studies explore the impact of the loss coefficient lamba and where to apply the BAFT loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of matching internal representation and output representation similarity is novel to my knowledge. \n2. The experiments are relatively extensive within computer vision. Particularly it evaluates the combination of BAFT with various PEFT methods, and show improvement in all cases.\n3. The paper analyzes the effect of the method from a mathematical perspective.\n4. The paper conducts the most essential ablation studies of where and how much to apply the proposed loss. \n5. BAFT adds minor overhead in training."}, "weaknesses": {"value": "1. The proposed BAFT loss does not seem to have a particular applicability to PEFT settings. It could also be used for pre-training. This makes it unclear why the motivation of this method is in particular to PEFT.\n2. All experiments conducted are in computer vision, and use the same ViT-B/16 as the pretrained backbone. This limits our understanding on how useful it is on other modalities/tasks, and whether the effectiveness could hold when the model is larger.\n3. The idea of using intermediate representation to improve training is not entirely new. It dates back as early as FitNet (2014) in a knowledge distillation context. From this perspective, the conceptual contribution of this paper is limited.\n4. The improvement overall is not too significant on VTAB (~0.4%). \n(a) An analysis on whether this improvement is worth the additional effort of tuning lambda is in question. As figure 5 shows, the performance is quite sensitive to lambda and it may require many trials, and it may be specific to each dataset.\n(b) A comparison on how this improvement compares to extending the training time in proportion to the overhead of BAFT is needed."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LLpOZTpFD3", "forum": "edgZd6BVzx", "replyto": "edgZd6BVzx", "signatures": ["ICLR.cc/2026/Conference/Submission16728/Reviewer_WV6S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16728/Reviewer_WV6S"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762112387895, "cdate": 1762112387895, "tmdate": 1762926776682, "mdate": 1762926776682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}