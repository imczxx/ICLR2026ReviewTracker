{"id": "Ea00wC36fn", "number": 3658, "cdate": 1757492420010, "mdate": 1759898076334, "content": {"title": "Bi-level Heterogeneous Learning for Time Series Foundation Models: A Federated Learning Approach", "abstract": "Heterogeneity in time series data is more pronounced than in vision or language, as temporal dynamics vary substantially across domains and tasks. Existing efforts about time series foundation models (TSFMs) are often trained with mixed-batch strategies that merge large-scale datasets, which can cause gradient conflicts and degrade representation quality. To address this, we propose a fine-grained learning framework that distills invariant knowledge from heterogeneous series while reducing cross-domain interference. We characterize heterogeneity at two levels: inter-domain and intra-domain. To tackle this bi-level heterogeneity, we design a federated learning framework that mitigates intra-domain conflicts by enforcing domain-invariant and semantically consistent representations through local regularization, and addresses inter-domain discrepancies by enhancing cross-domain collaboration via domain-aware aggregation. Experiments across diverse benchmarks demonstrate that TSFMs trained with our method consistently outperform both centralized and federated TSFM baselines in point and probabilistic forecasting, while also achieving competitive zero-shot performance at scale, offering a flexible pathway for training TSFMs in heterogeneous environments.", "tldr": "We propose FedTRL, a federated framework for bi-level heterogeneous learning that mitigates inter- and intra-domain heterogeneity, enabling flexible TSFM pretraining with strong zero-shot generalization.", "keywords": ["Time Series Foundation Models", "Time Series Forecasting", "Federated Foundation Models"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f33ddc2b3ad284e2e58bd9e1fe01db85906ab865.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a federated learning framework (FedTRL) to address bi-level heterogeneity, including both inter-domain and intra-domain heterogeneity. FedTRL primarily includes a fine-grained joint optimization–aggregation strategy, which integrates domain-adversarial regularization and domain-aware aggregation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: The paper introduces federated learning into the development of time series foundation models.\n\nS2: Extensive experiments are conducted on multiple benchmarks, including TSLib, GIFT-eval, and the FEV leaderboard.\n\nS3: The experimental descriptions are relatively detailed."}, "weaknesses": {"value": "W1 (Motivation): The authors claim that mixing heterogeneous data during pretraining can obscure domain-specific structures, thereby limiting model generalization. However, this point lacks in-depth explanation and empirical support. On the contrary, why couldn’t combining data from different domains help the model learn cross-domain shared knowledge and enhance generalization? The authors should provide a more detailed discussion and analysis here.\n\nW2 (FedTRL Training): FedTRL’s pretraining on large-scale data requires training separate encoders, decoders, and prediction heads for each domain, and then aggregating encoder parameters across domains to update them. This design may raise several issues: 1. The decoder/prediction head might become incompatible with the updated encoder. 2. The framework must store all models for each domain, resulting in high storage overhead. 3. If downstream data come from a domain not included in the pretraining domains, it is unclear how the model would handle such cases.\n\nW3 (Pretraining Data): The pretraining dataset exhibits severe domain imbalance (i.e., different domains have significantly different data proportions). Such imbalance may substantially affect parameter interactions between the clients and the server in FedTRL. If so, how does the method solve this problem?"}, "questions": {"value": "Q1: In Equation (3), how is ( y_{i}^{dom} ) obtained? Does each patch correspond to a label ( y )?\n\nQ2: The prototype in the paper is simply derived by averaging features, without considering time series characteristics such as seasonality or trends. Would it be better to design prototypes that explicitly encode time series characteristics to better align local and global representations?\n\nQ3: Why design a dual-head architecture instead of using a single probabilistic head that can perform both probabilistic and deterministic predictions simultaneously?\n\nQ4: In Table 1, the federated training results are worse than training on individual datasets (e.g., FedTRL’s results are far below those of PatchTST in its original paper).\n\nQ5: In Table 3, the paper lacks comparisons with recent time series foundation models, such as VisionTS (ICML 2025), Sundial (ICML 2025), and LightGTS (ICML 2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A9gTzbFIqu", "forum": "Ea00wC36fn", "replyto": "Ea00wC36fn", "signatures": ["ICLR.cc/2026/Conference/Submission3658/Reviewer_ug39"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3658/Reviewer_ug39"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635380171, "cdate": 1761635380171, "tmdate": 1762916905155, "mdate": 1762916905155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a time series forecasting modeling method based on federated learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper optimizes the design from two perspectives: cross-domain and intra-domain."}, "weaknesses": {"value": "1. Besides data privacy concerns, what are the differences between federated learning and pre-training and fine-tuning? Why is this approach being considered for the time series field?\n\n2. In order to train a good foundation model, it is important to extract general capabilities, but for each client, specific patterns are beneficial to its own downstream tasks. Using adversarial learning in local optimization will sacrifice specific knowledge.\n\n3. In the experiments corresponding to table 1, methods such as PatchTST are not designed for joint training, so the corresponding experiments are meaningless and cannot explain any problems.\n\n4. In the experimental setting corresponding to Table 2, when there are multiple test datasets for testing, will the local model parameters after fine-tuning the first test dataset be fed back to the base model for model update?\n\n5. A follow-up question: During pre-training, is it better to include more training datasets? Can you provide relevant experiments to make a qualitative judgment?\n\n6. How can we achieve fine-grained data segmentation within a domain?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SatKQ0O1jn", "forum": "Ea00wC36fn", "replyto": "Ea00wC36fn", "signatures": ["ICLR.cc/2026/Conference/Submission3658/Reviewer_qkhe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3658/Reviewer_qkhe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640243585, "cdate": 1761640243585, "tmdate": 1762916904934, "mdate": 1762916904934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FedTRL, a federated framework tackling bi-level heterogeneity in time-series foundation models. It addresses inter-domain differences and intra-domain conflicts through local adversarial regularization and prototype alignment for semantic consistency, and domain-aware aggregation for cross-domain collaboration. Experiments show that FedTRL outperforms centralized and traditional federated baselines in both point and probabilistic forecasting, achieving stronger zero-shot generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper clearly defines and formalizes inter- and intra-domain heterogeneity in time-series foundation model training, providing an insightful perspective.\n\nThe overall design of FedTRL — combining local optimization and global aggregation — is logically sound and systematically organized.\n\nThe experiments cover multiple datasets and both point/probabilistic forecasting tasks, supporting the claims effectively.\n\nThe model demonstrates stable performance on unseen domains, showing effective cross-domain transfer."}, "weaknesses": {"value": "Although FedTRL uses adversarial regularization and prototype alignment, these mainly enforce coarse semantic consistency and may fail to capture continuous or nonlinear sub-domain drift.\n\nThe framework jointly updates local adversarial modules and prototypes while performing domain-aware aggregation each round, leading to heavy overhead in large-scale federations.\n\nDespite claiming “domain awareness,” the paper lacks visualization or empirical analysis showing how representations differ across domains.\n\nAggregation weights are heuristically defined without rigorous justification"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OG3isHafMH", "forum": "Ea00wC36fn", "replyto": "Ea00wC36fn", "signatures": ["ICLR.cc/2026/Conference/Submission3658/Reviewer_JCto"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3658/Reviewer_JCto"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821274313, "cdate": 1761821274313, "tmdate": 1762916904753, "mdate": 1762916904753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **FedTRL**, a federated learning framework designed to train **time series foundation models (TSFMs)** under **bi-level heterogeneity** — both inter-domain and intra-domain differences across clients.\nIt introduces a **dual-level optimization** strategy combining adversarial local regularization and domain-aware global aggregation to achieve domain-invariant and temporally coherent representations.\nExtensive experiments across in-domain, full-shot, and zero-shot forecasting tasks show that FedTRL achieves **state-of-the-art performance**, outperforming both centralized and existing federated baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "## **Strengths**\n\n* The proposed method is technically sophisticated and conceptually “fancy.” The architectural design and figures are very clear and visually appealing.\n* The paper is well written and easy to follow.\n* Experimental results are strong, with the proposed method achieving solid performance across multiple benchmarks."}, "weaknesses": {"value": "## **1. Motivation & Novelty**\n\nThe **motivation is not clearly justified**. I am not fully convinced that *heterogeneity* itself should be viewed as a negative factor. On the contrary, **sufficient heterogeneity and diversity in data are often the key enablers for the success of Time Series Foundation Models (TSFMs)**. The paper criticizes heterogeneity but provides **no empirical evidence** or **quantitative analysis** showing that heterogeneity indeed harms federated training.\n\nFor example, in the discussion of *inter-domain heterogeneity*, the authors claim that it leads to “overfitting to domain-specific signals” and the failure to “capture globally consistent dynamics.” However, no supporting evidence, ablation, or theoretical justification is provided.\n\nMoreover, the paper frequently refers to *“domain-invariant”* and *“temporally coherent patterns”* without giving a clear definition. These notions remain vague. What exactly constitutes a “global dynamic” in time series? In time-series data, the notion of a *domain* is often **weakly constrained**—for example, even within a single domain such as weather, the underlying distributions can vary substantially. This is very different from computer vision, where the concept of domain is more clearly defined. Therefore, **learning domain-invariant representations for time series seems conceptually questionable**, and this undermines the central motivation of the work.\n\nThis is my **primary concern**—without a clearer and empirically grounded motivation, it is difficult to see the necessity of the proposed framework.\n\n---\n\n## **2. Methodological Concerns**\n\n**2.1** Given the above discussion, I suspect that the actual benefit of FedTRL might not stem from addressing heterogeneity per se, but rather from **implicitly improving data diversity and balance during training**. In other words, the observed performance gain may come from a more diversified and higher-quality training process, rather than from the federated or “heterogeneity-resolving” design itself.\n\n**2.2** From the title and framing, the paper claims to propose a *federated learning framework for TSFMs*. However, the proposed FedTRL looks more like a **representation learning model and its training algorithm**, rather than a general-purpose training framework for foundation models. This conceptual mismatch between the claimed goal (framework for TSFMs) and the actual technical contribution (a particular model design) feels somewhat inconsistent.\n\n---\n\n## **3. Experimental Evaluation**\n\n**3.1** The comparison with baselines could be more appropriate. Since the paper emphasizes a *federated* setup, it would be more convincing to compare FedTRL with **federated time-series forecasting baselines**, rather than primarily with representation learning methods. Alternatively, the authors could demonstrate FedTRL as a **general plug-in or enhancement framework** that can consistently improve various federated baselines.\n\n**3.2** Regarding the **GIFT-Eval** evaluation, the authors should clearly state **which specific datasets** from GIFT-Eval were used. It is also important to clarify **whether any of these datasets overlap with those included in the Time-MoE-300B pretraining corpus**, as this could raise concerns about potential data leakage or unfair comparisons."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vDBYFwV0i1", "forum": "Ea00wC36fn", "replyto": "Ea00wC36fn", "signatures": ["ICLR.cc/2026/Conference/Submission3658/Reviewer_wtKL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3658/Reviewer_wtKL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892195481, "cdate": 1761892195481, "tmdate": 1762916903783, "mdate": 1762916903783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}