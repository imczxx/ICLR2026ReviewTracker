{"id": "2w0fZJnv7M", "number": 1952, "cdate": 1756969959821, "mdate": 1762954532113, "content": {"title": "HOW TO TRAIN YOUR DIFFUSION MODELS FOR FEW- SHOT CLASSIFICATION", "abstract": "Generation and classification are two sides of the same coin: a strong generative model can be transformed into a powerful classifier.\nThis is evident when diffusion models (DMs) outperform CLIP-based approaches in fine-grained or customized classification tasks, where a small few-shot training set defines the task on the fly.\nIn this setting, the model is typically fine-tuned to reconstruct the training samples and, at inference, predicts the label with the lowest expected reconstruction error across diffusion time-steps.\nAlthough effective, this approach is computationally expensive, as it requires computing $average$ reconstruction errors for $every \\  class$ over the $ full \\ \\ range \\ \\  of \\ \\ time-steps$ and \\$multiple \\ \\ sampled \\ \\ noises$.\nIn this work, we study techniques to improve both efficiency and accuracy of diffusion classifiers.\nTo accelerate inference, we propose dynamic time-step selection to minimize unnecessary evaluations.\nTo improve the estimation of reconstruction errors, we introduce class-object mask learning, which reduce variance and thereby require fewer noise samples to achieve high precision.\nTo further reduce the number of candidate classes, we explore candidate class selection.\nTogether, these techniques speed up diffusion-based classifiers by over an order of magnitude while simultaneously maintaining or even improving classification performance.\nFinally, we show that DMs and CLIP-based models are complementary, and integrating the two achieves further gains — reinforcing the close connection between generation and classification.", "tldr": "This work greatly accelerates the existing framework of diffusion classifer for few-shot learning while maintaining or even improving the performance.", "keywords": ["Diffusion Classifier", "Few-shot Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/05824cbdea3d886eb3b86a9c1633916e3192e789.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The manuscript proposes FastTiF, an incremental method improved from TiF [1] to accelerate diffusion-based few-shot classification  while maintaining or improving accuracy. The authors address three computational bottlenecks: (1) extensive time-step sampling through time-step learning, (2) uniform noise evaluation through mask learning, and (3) large candidate class pools through CLIP-based filtering. Experiments on three datasets (FGVCAircraft, VeRi-776, New Plant Diseases) show 10-40 time speedups with competitive accuracy compared to TiF baseline and other CLIP-based fine-tuning methods.\n\n[1] Few-shot Learner Parameterization by Diffusion Time-steps, CVPR 2024"}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-motivated with good empirical support. The analysis experiments, especially Figure 2, effectively demonstrate the existence of a \"golden time-step range\" where discriminative ability peaks, making the motivation very clear.\n\n2. The approach addresses the main computational bottlenecks from 3 perspectives (time-steps, noise evaluation, and class candidates) with clear rationale for each component.\n\n3. The baseline selection seems to be adequate, comparing against diverse CLIP-based methods (prompt tuning, adapter-based, multi-modal) and diffusion-based approaches. Achieving 10-40× acceleration is a good practical contribution that could enable real-world deployment."}, "weaknesses": {"value": "1. The writing is imprecise and unpolished throughout. There are numerous typos: \"eg.\", \"dreambooth\", \"Tip-Adaptor\", \"performace\", \"hight\". The formatting is inconsistent with parentheses spacing and dataset names (PlantDiseases vs New Plant Diseases). Some parts are written unclearly, especially in the Experiments and hard to understand the settings such as \"The adopted CLIP-based method is the SOTA Multi-modality variant under the CLIP framework\" and the \"Degraded TiF\" baseline terminology is confusing because it was not described in detail. Why not report the number of noise sample counts directly for this baseline?\n\n2. Several experimental details are missing. Table 2 omits zero-shot results for VeRi-776 when provided for other datasets. The paper tests on different datasets than the original TiF paper, missing comparisons on ISIC2019 and DukeMTMC-reID datasets. Why don't compare on these datasets for a fair comparison?  How sensitive are results to mask learning hyperparameters and architecture choices?\n\n3. Limited Novelty. The works seems to be an incremental improvement of TiF rather than a fundamental methodological advance. The CLIP filtering is a well-known technique. The paper's own ablation (Table 4) reveals that time-step learning achieves nearly identical results to TiF's existing ratio-based time-step estimation, directly undermining one of three claimed contributions. The authors acknowledge this but don't adequately address why it should be considered a novel contribution.\n\n4. The limitation of proposed method was not discussed properly. (see Questions for more detail)."}, "questions": {"value": "1. A failure case for FastTiF is degraded performance on higher shots in Table 1. Could the authors provide an explanation for this phenomenon? Although I agree that CLIP filtering usually yield satisfactory top-k score, utilizing CLIP score may lead to false positives and could hurt performance [2]. Could the authors validate whether it is the case in this scenario?\n2. Given Table 4 shows TiF's time-step estimation works equally well, the need of learning time-step is not convincing. Could the authors clarify more on this?\n3. Why is Full TiF \"super slow\" on Plant Diseases (38 classes) but computable for FGVCAircraft (100 classes) or VeRi-776 (200 classes)?\n4. How many noise samples does Degraded TiF use compared to Full TiF and your method? Why do you need to introduce this intermediate baseline?\n5. What is the training time overhead for mask learning, and when does this complexity pay off compared to simply using more noise samples? \n\n[2] Sieve: Multimodal dataset pruning using image captioning model, CVPR 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "udzFkXDN1x", "forum": "2w0fZJnv7M", "replyto": "2w0fZJnv7M", "signatures": ["ICLR.cc/2026/Conference/Submission1952/Reviewer_NtX7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1952/Reviewer_NtX7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1952/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761328162396, "cdate": 1761328162396, "tmdate": 1762915967995, "mdate": 1762915967995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "l0MiYrDYAD", "forum": "2w0fZJnv7M", "replyto": "2w0fZJnv7M", "signatures": ["ICLR.cc/2026/Conference/Submission1952/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1952/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762954530891, "cdate": 1762954530891, "tmdate": 1762954530891, "mdate": 1762954530891, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FastTiF, a fast and accurate diffusion-based few-shot classification method. FastTiF accelerates standard diffusion classifiers through three key steps: (1) time-step learning, which focuses inference on an optimal “golden” diffusion range; (2) mask learning, which guides the model to compute reconstruction loss only on salient object regions; and (3) class candidate filtering, which leverages CLIP to preselect a small set of likely classes.Together, these steps accelerate diffusion-based classifiers by over an order of magnitude, while maintaining or even improving classification accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Complementary integration: The paper effectively combines the strengths of diffusion and CLIP-based models, showing that they are complementary.\n\nSignificant acceleration: FastTiF achieves up to an order-of-magnitude speedup over TiF while maintaining or even improving accuracy.\nTime-step optimization: The proposed time-step learning efficiently restricts sampling to a golden range,reducing unnecessary computations.\n\nMask learning mechanism: The introduction of class-object masks helps focus on salient regions, improving both efficiency and robustness."}, "weaknesses": {"value": "Limited benchmarks: The evaluation is restricted to only three datasets, lacking experiments on broader or more standard FSL benchmarks such as miniImageNet or CUB.\n\nIncomplete comparison: The paper does not provide a comprehensive comparison with conventional FSL approaches or other foundation-model-based FSL methods.\n\nShallow integration analysis: The integration between CLIP and diffusion models is based mainly on confidence thresholds, without a deeper theoretical or systematic analysis of when each model performs better.\n\nIncremental contribution:The proposed improvements (time-step selection, mask learning, class filtering) are mainly engineering refinements rather than novel theoretical contributions."}, "questions": {"value": "Most prior FSL works report results on miniImageNet, CUB, and tieredImageNet under 5-way 1-shot and 5-way 5-shot settings. Could you evaluate FastTiF on these standard benchmarks to enable a fair comparison with conventional FSL methods?\n\nHave you considered testing cross-domain transfer, for example, training on miniImageNet and evaluating on CUB or tieredImageNet, to assess the model’s generalization capability?\n\nThe CLIP–Diffusion integration is based solely on a confidence threshold. Have you explored more adaptive or probabilistic fusion strategies that might improve performance further?\n\nThe current comparisons mostly focus on CLIP-based methods. Could you include more conventional FSL baselines (e.g., ProtoNet, MatchingNet,MSENet  or Meta-Baseline) to clarify where FastTiF stands relative to traditional paradigms?\n\nIt would be valuable to analyze failure cases, especially where the integration misclassifies samples. For instance, were these cases correctly handled by CLIP or Diffusion individually, and does this suggest that the confidence threshold might not always capture uncertainty effectively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oYRE9fwpL1", "forum": "2w0fZJnv7M", "replyto": "2w0fZJnv7M", "signatures": ["ICLR.cc/2026/Conference/Submission1952/Reviewer_nZMd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1952/Reviewer_nZMd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1952/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798113964, "cdate": 1761798113964, "tmdate": 1762915967545, "mdate": 1762915967545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FastTiF, aiming to improve the inference efficiency and accuracy of Diffusion Models (DMs) in Few-shot Classification (FSL) tasks. The authors argue that generative diffusion models possess complementary potential to discriminative models (such as CLIP) in few-shot settings; however, existing diffusion classifiers suffer from extremely high inference costs, as they require averaging reconstruction errors across all classes, multiple noise samples, and time steps. To address this, the paper introduces three core improvements: Time-step Learning; Pixel-level Saliency Mask Learning; Class Candidate Filtering (CCF). Furthermore, the paper explores the complementarity between CLIP and Diffusion Classifiers, and proposes a hybrid inference strategy to further enhance performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper is clearly written, well-structured, and effectively supported with figures and tables.\n2.\tIt identifies three computational bottlenecks of diffusion classifiers and designs corresponding improvement methods for each."}, "weaknesses": {"value": "1.\tThe literature review is not comprehensive enough, with insufficient references to recent related works.\n2.\tThe authors directly present the mathematical formulation of the diffusion classifier without explaining how these equations relate to the stated problems (low efficiency and high computational cost), nor do they explicitly clarify why the method is computationally expensive.\n3.\tAlthough the proposed time-step learning is empirically supported, it lacks theoretical analysis regarding how discriminative ability changes with diffusion steps.\n4.\tThe ablation studies are not intuitive; Section 5.3 does not systematically analyze the effect of each proposed module or present clear ablation results.\n5.\tThe complementarity of the CLIP + Diffusion hybrid framework is not sufficiently convincing — the integration strategy is merely based on a logit-difference threshold, without prior analysis of the performance gap or complementarity mechanism between the two models."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q1lC4wqyMX", "forum": "2w0fZJnv7M", "replyto": "2w0fZJnv7M", "signatures": ["ICLR.cc/2026/Conference/Submission1952/Reviewer_PEW7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1952/Reviewer_PEW7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1952/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804597059, "cdate": 1761804597059, "tmdate": 1762915967348, "mdate": 1762915967348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FastTiF, a method that improves both the efficiency and accuracy of diffusion classifiers for few-shot classification. The authors introduce three mechanisms: (a) time-step learning to identify the optimal sampling window during inference; (b) mask learning to focus on foreground object regions while suppressing background reconstruction-error noise; and (c) candidate label selection using a CLIP‐based method to reduce the set of potential classes.  Experiments demonstrate that FastTiF outperforms baselines in most cases. They also considers combining FastTiF and CLIP-based methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a clear and well-motivated problem setup, proposing three reasonable modules: time-step learning, mask learning, and class candidate filtering-that intuitively address distinct sources of inefficiency in diffusion-based few-shot classification.\n2. The experiments demonstrate the effectiveness of the proposed model."}, "weaknesses": {"value": "1. The discussion of prior work on accelerating diffusion-based classifiers is too brief. \n2. The explanation of the proposed modules lacks sufficient clarity and detail.\n3. The training cost of the proposed method is not discussed. It is unclear how the speedup is measured and whether the comparison is fair."}, "questions": {"value": "1. Do the three modules require retraining for each dataset or task, or are they trained once and for all? If they need to be trained for each task, how does that fit in the the few-shot learning scenario? How many samples are required and how much time? Will the comparisons in the evaluation be unfair? \n\n2. Lines 319–323 state that the method “filters out fewer than 10% of class candidates per sample,” while Table 1 shows that only 5–10% of the classes remain (e.g., 5 out of 100 for FGVCAircraft), implying that more than 90% are removed. How many classes are actually removed in practice? Moreover, how are the retained candidates defined and how is the additional sampling performed (described in lines 320-322)?\n\n3. Lines 357-359: Larger K makes the problem more challenging but may also give the proposed method more advantage. How do the methods compare when K is small? \n\n4. What samples are used to train the proposed modules in the experiments? Is there information leakage here, making it not a true few-shot learning scenario?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mrhhBspzwB", "forum": "2w0fZJnv7M", "replyto": "2w0fZJnv7M", "signatures": ["ICLR.cc/2026/Conference/Submission1952/Reviewer_hCY8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1952/Reviewer_hCY8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1952/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991868020, "cdate": 1761991868020, "tmdate": 1762915966746, "mdate": 1762915966746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}