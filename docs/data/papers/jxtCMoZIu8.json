{"id": "jxtCMoZIu8", "number": 12922, "cdate": 1758211691543, "mdate": 1759897476366, "content": {"title": "BEP: A Binary Error Propagation Algorithm for Binary Neural Networks Training", "abstract": "Binary Neural Networks (BNNs), which constrain both weights and activations to binary values, offer substantial reductions in computational complexity, memory footprint, and energy consumption. These advantages make them particularly well suited for deployment on resource-constrained devices. However, training BNNs via gradient-based optimization remains challenging due to the discrete nature of their variables. The dominant approach, Quantization-Aware Training, circumvents this issue by employing surrogate gradients. Yet, this method requires maintaining latent full-precision parameters and performing the backward pass with floating-point arithmetic, thereby forfeiting the efficiency of binary operations during training. While alternative approaches based on local learning rules exist, they are unsuitable for global credit assignment and for back-propagating errors in multi-layer architectures. This paper introduces Binary Error Propagation (BEP), the first learning algorithm to establish a principled, discrete analog of the backpropagation chain rule. This mechanism enables error signals, represented as binary vectors, to be propagated backward through multiple layers of a neural network. BEP operates entirely on binary variables, with all forward and backward computations performed using only bitwise operations. Crucially, this makes BEP the first solution to enable end-to-end binary training for recurrent neural network architectures as well. We validate the effectiveness of BEP on both multi-layer perceptrons and recurrent neural networks, demonstrating performance gains of up to +8.70% and +12.30% in test accuracy, respectively. The proposed algorithm is released as an open-source repository.", "tldr": "", "keywords": ["Binary Neural Networks", "Fully binary training", "Binary error backpropagation", "Gradient-free optimization", "Binary Recurrent Neural Networks"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0af2314cf293528fb808c30e3becc1327d2ed7ad.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Binary Error Propagation (BEP), a new training algorithm for binary neural networks (BNNs) where both weights and activations are binary. The key innovation is a fully binary, bitwise analog of the backpropagation chain rule, allowing end-to-end propagation of binary error signals through both feedforward and recurrent architectures. The method is empirically measured against prior binary optimization methods and QAT on several tasks and model architectures, showing test accuracy improvements."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is reasonable. And the formalization of a binary version of global credit assignment for BNNs is clear.\n2. The empirical results are broad, showing the advantage of BEP."}, "weaknesses": {"value": "1. The experiments are not sufficient. Although the authors conduct experiments on both MLP and RNN models to demonstrate the effectiveness of their method, the experimental setup appears somewhat toy. Could the authors perform experiments on larger-scale networks (e.g., comparable to ResNet in size) and datasets (e.g., ImageNet-1k)? In addition, since the BNN field has been studied for a long time, could the authors compare BEP with other BNN methods (like [1-3]) beyond QAT to further validate its effectiveness?\n\n[1] Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm\n\n[2] MeliusNet: Can Binary Neural Networks Achieve MobileNet-level Accuracy?\n\n[3] XNOR-Net++: Improved Binary Neural Networks\n\n2. Have the authors measured the actual training overhead, such as training time and memory usage, to demonstrate that BEP indeed achieves better training efficiency compared to the baseline?\n\n3. The paper format does not appear to follow the ICLR style. The pages seem wider and contain more content, which might be unfair to other submissions."}, "questions": {"value": "1. Similar to Weakness 1: Can BEP be applied to deeper binary CNNs or transformer-style architectures without encountering collapse or stagnation? Have the authors attempted to apply BEP to modern vision or language models beyond the provided MLP/RNN settings?\n\n2. Batch normalization seems to contribute significantly to performance improvement. Have the authors considered adopting a binary analog or an alternative form of normalization within BEP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MKa4dUPKb1", "forum": "jxtCMoZIu8", "replyto": "jxtCMoZIu8", "signatures": ["ICLR.cc/2026/Conference/Submission12922/Reviewer_uRee"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12922/Reviewer_uRee"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747905947, "cdate": 1761747905947, "tmdate": 1762923691349, "mdate": 1762923691349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Binary Error Propagation (BEP), a new training algorithm for binary neural networks that avoids surrogate gradients. Instead of real-valued backpropagation, BEP defines a discrete backward rule that propagates binary \"desired activations\" using only bitwise operations. The method updates integer hidden weights through sparse binary masks that act as an adaptive learning mechanism. BEP is evaluated on MLPs and RNNs across several datasets, showing higher accuracy than quantization-aware training (QAT) and the method proposed in Colombo et al. (2025). The authors claim that this is the first fully binary error Backpropagation (BP) algorithm capable of effectively training BNNs without relying on floating-point gradients."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clearly written and well organized, with notation that is consistent and easy to follow.\n- The presentation is concise and direct, and I found the mathematical derivations correct and well-grounded.\n- The method is novel and well motivated, and the experimental results are consistent with most of the claims."}, "weaknesses": {"value": "- In Section 3.1, the authors obtain binary input representations using a fixed binarization method (median or thermometer encoding) but do not analyze how different binarization functions affect BEP's performance.\n\n- The margin parameter $r$ in Eq. (1) determines when binary updates are triggered, but its value and sensitivity are not analyzed. Since this parameter effectively affects the learning dynamics, the authors should provide an ablation to show how it influences performance.\n\n- The paper asserts that the sparsity mask $M_l$ fulfills the role of the learning rate in classical BP, yet this is only mentioned in the text. Providing empirical evidence would strengthen this claim.\n\n- Experiments are limited to two- and three-layer MLPs. It would be helpful to discuss whether BEP scales to deeper binary networks, as the discrete backward propagation might face stability issues analogous to vanishing gradients.\n\n- The paper should include a limitations section. While the authors briefly mention possible future directions, they should clearly articulate the current constraints of BEP.\n\nMinor comments:\n- The related work section would benefit from including recent works such as \"BiPer: Binary Neural Networks using a Periodic Function (CVPR 2024)\"\n- The small QAT (w/o batchnorm) plots embedded in each subfigure of Figure 2 are difficult to interpret. I recommend improving their presentation or visibility for better clarity.\n- In sections 4.3 and 4.4, the term \"window length\" is not clearly defined."}, "questions": {"value": "- Have you tested other binarization functions, and can you comment on how sensitive BEP is to this choice?\n- The margin parameter $r$ in Eq. (1) controls when updates are triggered. Have you analyzed how different $r$ values affect training stability or accuracy?\n- You state that the sparse mask $M_l$ plays a role similar to a learning rate. Could you provide additional analysis to support this interpretation?\n- Have you tested BEP on deeper binary MLPs? If not, could you comment on potential challenges in extending BEP to deeper networks?\n- Given that most prior work on BNNs targets convolutional architectures, can you elaborate on how BEP might extend to CNNs? \n- Did you test BEP on tasks other than classification? Might it work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0Mt6HdT5zD", "forum": "jxtCMoZIu8", "replyto": "jxtCMoZIu8", "signatures": ["ICLR.cc/2026/Conference/Submission12922/Reviewer_hAH5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12922/Reviewer_hAH5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823365385, "cdate": 1761823365385, "tmdate": 1762923691039, "mdate": 1762923691039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work proposes a novel alternative to the backpropagation algorithm by reformulating it in the binary domain to reduce computational cost. Unlike Quantization-Aware Training, BEP performs both the forward and backward passes entirely in the binary domain, relying solely on bitwise operations. The paper provides theoretical foundations and experimental results that validate this reformulation of backpropagation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Novelty: the paper introduces an alternative approach to regular backpropagation by reformulating the algorithm to work for binary weights.\n\n\nDifferent from other approaches that rely on real value parameters, like a straight-through estimator, this work focuses on the idea of computing forward and backward on the binary domain. \n\n\nThe work displays consistent improvements over the classification task on two datasets."}, "weaknesses": {"value": "1. Lack of organization, it is difficult to read smoothly, as figures, such as Figure 2, are on page 7, and mentioned in page 8, same as Table 1, where it is mentioned in page 9, and it is on page 8.\n\n\n2. The authors claim computational efficiency and memory reduction, but ablation studies over flops, training, and inference time, as well as details over the computational equipment, are not stated.\n\n\n3. The authors propose several hyperparameters, even though in section 4.4 is an analysis. Further ablation studies should be conducted on the sensitivity of parameters like r and pr.\n\n\n4. Lack of training details hinders the reproducibility of the work, as it is not stated the epochs or the number of iterations used in the experiments. \n\n\n5. The update rule should be further clarified. Even though there are references to previous works, it should be explicitly stated where they come from Eq. (8)\n\n\n6. The binary mask from Equation 9 is not theoretically explained, and its purpose is not clear."}, "questions": {"value": "How does BEP scale computationally compared to QAT during training?\n\n\n1. How sensitive is BEP to initialization and hyperparameter selection across datasets?\n\n\n2. Does the algorithm have similar results across different tasks to classification, for instance, binary segmentation?\n\n\n3. How does BEP handle vanishing or exploiting gradients in comparison to the standard backpropagation algorithm?\n\n\n4. How does the algorithm work for larger spatial dimensions and complex feature datasets, such as the celebA dataset with the multilabel classification task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aTOPsNRCsw", "forum": "jxtCMoZIu8", "replyto": "jxtCMoZIu8", "signatures": ["ICLR.cc/2026/Conference/Submission12922/Reviewer_N7bj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12922/Reviewer_N7bj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015950150, "cdate": 1762015950150, "tmdate": 1762923690607, "mdate": 1762923690607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}