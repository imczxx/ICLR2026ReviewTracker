{"id": "LVXc2Jvjme", "number": 1535, "cdate": 1756890461444, "mdate": 1763674690122, "content": {"title": "Q2D2: A geometry-aware audio codec leveraging two-dimensional quantization", "abstract": "Recent neural audio codecs have achieved impressive reconstruction quality, typically relying on quantization methods such as Residual Vector Quantization (RVQ), Vector Quantization (VQ) and Finite Scalar Quantization (FSQ). However, these quantization techniques limit the geometric structure of the latent space, make it harder to capture correlations between features leading to inefficiency in representation learning, codebook utilization and token rate. In this paper we introduce Two-Dimensional Quantization (Q2D2), a quantization scheme in which feature pairs are projected onto structured 2D grids—such as hexagonal, rhombic, or rectangular tiling—and quantized to the nearest grid values, yielding an implicit codebook defined by the product of grid levels, with codebook sizes comparable to conventional methods. Despite its simple geometric formulation, Q2D2 improves audio compression efficiency, with low token rates and high codebook utilization while maintaining state-of-the-art (SOTA) reconstruction quality. Specifically, Q2D2 achieves competitive to superior performance in various objective and subjective reconstruction metrics, across extensive experiments in speech domain compared to SOTA models. Comprehensive ablation studies further confirm the effectiveness of our design choices.", "tldr": "A new geometry-aware audio codec using Two-Dimensional Quantization", "keywords": ["Audio codec", "Quantization", "geometry", "2D", "two-Dimensional"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/91a14185eeff83559b178556728653853d8a8803.pdf", "supplementary_material": "/attachment/af88b74603627aba8edc63365a098bf2e779b19b.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new quantization method for audio codec called Q2D2, and conduct experiments on LibriTTS to show the performance of the proposed method"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. the insight that the channel independent quantization nature of FSQ might limit the expressiveness\n2. the authors tried varies geometric structure for latent space"}, "weaknesses": {"value": "The biggest issue of Q2D2 is that it's actually equivalent to FSQ, which I show below\n \nFSQ on two coordinates quantizes each dimension independently:\n\n$$\nQ_{\\text{FSQ}}(x, y) =\n\\big(\n  \\Delta_x \\, \\mathrm{round}(x / \\Delta_x),\n  \\;\n  \\Delta_y \\, \\mathrm{round}(y / \\Delta_y)\n\\big).\n$$\n\nDecision boundaries are axis-aligned lines  \n$ x = (k+\\tfrac{1}{2})\\Delta_x $ or $y = (\\ell+\\tfrac{1}{2})\\Delta_y $,  \nso the Voronoi cells are rectangles.\n\nFor rectangular grid, it's obvious that it's equivalent to the above:\n\nThe Cartesian lattice is\n\n$$\n\\Lambda_{\\text{rect}} = \\{(k\\Delta_x, \\, \\ell\\Delta_y): k, \\ell \\in \\mathbb{Z}\\}.\n$$\n\nNearest-neighbor quantization is\n\n$$\nQ_{\\text{rect}}(x, y)\n= \\arg\\min_{(k,\\ell)\\in\\mathbb{Z}^2}\n\\big[(x - k\\Delta_x)^2 + (y - \\ell\\Delta_y)^2\\big]\n= Q_{\\text{FSQ}}(x, y),\n$$\n\nfor Hexagonal and Rhombic, they are essentially FSQ with shifted grid on x and y axis. It's easier to spot that from Algorithm 1 and 3. in Algorithm 1 (Hexagonal), the derivation of $x_c$ in mg$(x_c, y)$ is independent of the value of y, and only depend on the value of x; in algorithm 3 (Rhombic), the first grid mg$(c_x, c_y)$ is uniform 1-D grids just like FSQ, the second grid shifted by dx, dx, but still axis separable (e.g. it's not shifted by non-linear combination of dx and dy), which make the final grid a non-2D quantization.\n\nTherefore the proposed methods does not match the motivation - channel dependent quantization.\n\n\nIn addition, in the first paragraph of the introduction, the authors write: \n> By converting high-rate speech signals into compact sequences of discrete tokens, acoustic codec models provide the crucial link between continuous audio and token-based language models, thereby enabling the direct application of LLM architectures to audio.\n\nBut whether the proposed tokenization approach leads to better speech-LLM or NCLM-based TTS performance is not studied at all"}, "questions": {"value": "I'm happy to raise my rating if the authors can show that my understanding of the proposed approach is wrong."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lXPR1CQfS3", "forum": "LVXc2Jvjme", "replyto": "LVXc2Jvjme", "signatures": ["ICLR.cc/2026/Conference/Submission1535/Reviewer_bCXx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1535/Reviewer_bCXx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761422989059, "cdate": 1761422989059, "tmdate": 1762915798874, "mdate": 1762915798874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Q2D2, a novel geometry-aware quantization method for neural audio codecs. The key idea is to group latent channels into pairs and quantize them jointly on structured 2D grids (hexagonal, rhombic, or rectangular), instead of using per-channel scalar quantization (FSQ) or multi-layer residual quantization (RVQ). The proposed scheme constructs implicit codebooks analytically from grid geometry, eliminating learned embeddings, commitment losses, and reseeding heuristics. Empirical results on LibriTTS, LJSpeech, and the ARCH benchmark show that Q2D2 achieves comparable or better perceptual quality and codebook utilization than state-of-the-art codecs such as DAC, Encodec, Vocos, and WavTokenizer, with a drastically lower token rate (e.g., 53–333 tokens s⁻¹)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper offers a clean, geometrically motivated quantization formulation. Pairwise 2D grids bridge the gap between FSQ’s stability and VQ’s expressiveness.\n- The use of Straight-Through Estimators and lightweight projection layers makes the approach compatible with standard training pipelines. The method avoids extra losses or codebook-management tricks, which is a good simplification.\n- Semantic-representation tests on the ARCH benchmark provide an initial indication that the learned codes remain meaningful.\n- Despite training on only LibriTTS (≈ 585 h), Q2D2 rivals or exceeds stronger baselines trained on multi-domain datasets (> 8 k h)."}, "weaknesses": {"value": "- The model is trained solely on LibriTTS (500+h), whereas major baselines (e.g., WavTokenizer, DAC, Encodec) use multi-domain datasets with speech, music, and general audio totaling over 8 k hours. This discrepancy makes it difficult to isolate whether performance gains come from the proposed quantization scheme or from differences in data composition (especially training only on speech is easier than , normalization, and pre-training scope. The paper acknowledges this briefly but still draws strong SOTA claims, which could be misleading. (the major concern of this paper)\n- The core claim is that 2D quantization “captures correlations between channels.” However, the paper provides no quantitative evidence (e.g., correlation coefficients, covariance matrices, or MI statistics) demonstrating that the learned latent pairs are indeed correlated or that Q2D2 decorrelates them better than FSQ. Without such analysis, the geometric intuition remains qualitative.\n- While the codec’s perceptual quality is thoroughly evaluated, downstream utility is not explored. In the current LLM-audio ecosystem, codec quality is increasingly judged by downstream generative modeling (e.g., TTS, S2ST, or instruction-following speech generation).\nWithout such demonstrations, it is unclear whether Q2D2’s gains translate into improved generation or cross-modal performance.\n- Although Table 6 compares hexagonal, rectangular, and rhombic grids, the differences are relatively small (e.g., PESQ ≈ 2.29 – 2.40).\nThe paper claims that rhombic grids “offer higher packing efficiency,” but provides no geometric analysis or visualization to justify this.\nIt remains unclear why rhombic performs better, is it due to isotropy, denser coverage, or numerical convenience?\n- While Q2D2 is claimed to have fewer parameters than VQ or RVQ, there is no detailed analysis of training/inference time, memory usage, or complexity relative to baselines."}, "questions": {"value": "- Have the authors attempted training Q2D2 on the same 8kh dataset as WavTokenizer to provide a fair one-to-one comparison?\n- Conversely, how do baselines trained only on LibriTTS perform? Would Q2D2 still outperform them under equal data conditions?\n- How sensitive is Q2D2 to domain diversity, does performance degrade on non-speech or multilingual datasets?\n- Can the authors show empirical correlation heatmaps or mutual information between paired channels before/after quantization?\n- Is there any measurable improvement in reconstruction loss if the channel pairs are shuffled (i.e., destroying geometric adjacency)?\n- Could the authors relate their 2D lattice structure to lattice quantization theory or product quantization—are there formal efficiency bounds?\n- How is the pairing performed, sequentially, randomly, or learned? Is there any adaptive pairing strategy that further improves correlation capture?\n- Have the authors tested Q2D2 tokens as inputs for an existing audio-LM to verify downstream quality or token predictability?\n- How does Q2D2 affect token entropy or n-gram statistics compared to FSQ/RVQ tokens?\n- Would the structured grids yield smoother latent manifolds that benefit autoregressive or diffusion decoders?\n- Could Q2D2 potentially replace FSQ in multimodal systems like Moshi (or other systems)? If not, what limitations remain?\n- Could the authors visualize quantization error distributions or Voronoi regions for different grid types?\n- Does rhombic quantization yield lower average distortion for isotropic Gaussian inputs compared to rectangular grids?\n- Is the improvement consistent across random seeds or data subsets, or within statistical noise?\n- Would 3D grids (briefly mentioned in Appendix D) further increase utilization or just add complexity?\n- What is the average training time per epoch compared to FSQ or RVQ on the same hardware?\n- Does the grid lookup add noticeable computational overhead during decoding?\n- How do memory and latency scale with grid resolution (e.g., l = 7 -> 11)?\n- Could the 2D quantization be vectorized efficiently for real-time applications?\n- There are some minor presentation issues like:\n    - The related work discussion is a bit messy. You might consider discussing first quantization methods, then neural audio codecs, and end with comparisons.\n    - Consider merging repetitive tables or moving some detailed numeric comparisons to the appendix."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ak7VQ0Ifzt", "forum": "LVXc2Jvjme", "replyto": "LVXc2Jvjme", "signatures": ["ICLR.cc/2026/Conference/Submission1535/Reviewer_oMqv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1535/Reviewer_oMqv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637696029, "cdate": 1761637696029, "tmdate": 1762915798668, "mdate": 1762915798668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Q2D2, a novel geometry-aware quantization method for neural audio codecs. The key idea is to group latent channels into pairs and quantize them jointly on structured 2D grids (hexagonal, rhombic, or rectangular), instead of using per-channel scalar quantization (FSQ) or multi-layer residual quantization (RVQ). The proposed scheme constructs implicit codebooks analytically from grid geometry, eliminating learned embeddings, commitment losses, and reseeding heuristics. Empirical results on LibriTTS, LJSpeech, and the ARCH benchmark show that Q2D2 achieves comparable or better perceptual quality and codebook utilization than state-of-the-art codecs such as DAC, Encodec, Vocos, and WavTokenizer, with a drastically lower token rate (e.g., 53–333 tokens s⁻¹)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper offers a clean, geometrically motivated quantization formulation. Pairwise 2D grids bridge the gap between FSQ’s stability and VQ’s expressiveness.\n- The use of Straight-Through Estimators and lightweight projection layers makes the approach compatible with standard training pipelines. The method avoids extra losses or codebook-management tricks, which is a good simplification.\n- Semantic-representation tests on the ARCH benchmark provide an initial indication that the learned codes remain meaningful.\n- Despite training on only LibriTTS (≈ 585 h), Q2D2 rivals or exceeds stronger baselines trained on multi-domain datasets (> 8 k h)."}, "weaknesses": {"value": "- The model is trained solely on LibriTTS (500+h), whereas major baselines (e.g., WavTokenizer, DAC, Encodec) use multi-domain datasets with speech, music, and general audio totaling over 8 k hours. This discrepancy makes it difficult to isolate whether performance gains come from the proposed quantization scheme or from differences in data composition (especially training only on speech is easier than , normalization, and pre-training scope. The paper acknowledges this briefly but still draws strong SOTA claims, which could be misleading. (the major concern of this paper)\n- The core claim is that 2D quantization “captures correlations between channels.” However, the paper provides no quantitative evidence (e.g., correlation coefficients, covariance matrices, or MI statistics) demonstrating that the learned latent pairs are indeed correlated or that Q2D2 decorrelates them better than FSQ. Without such analysis, the geometric intuition remains qualitative.\n- While the codec’s perceptual quality is thoroughly evaluated, downstream utility is not explored. In the current LLM-audio ecosystem, codec quality is increasingly judged by downstream generative modeling (e.g., TTS, S2ST, or instruction-following speech generation).\nWithout such demonstrations, it is unclear whether Q2D2’s gains translate into improved generation or cross-modal performance.\n- Although Table 6 compares hexagonal, rectangular, and rhombic grids, the differences are relatively small (e.g., PESQ ≈ 2.29 – 2.40).\nThe paper claims that rhombic grids “offer higher packing efficiency,” but provides no geometric analysis or visualization to justify this.\nIt remains unclear why rhombic performs better, is it due to isotropy, denser coverage, or numerical convenience?\n- While Q2D2 is claimed to have fewer parameters than VQ or RVQ, there is no detailed analysis of training/inference time, memory usage, or complexity relative to baselines."}, "questions": {"value": "- Have the authors attempted training Q2D2 on the same 8kh dataset as WavTokenizer to provide a fair one-to-one comparison?\n- Conversely, how do baselines trained only on LibriTTS perform? Would Q2D2 still outperform them under equal data conditions?\n- How sensitive is Q2D2 to domain diversity, does performance degrade on non-speech or multilingual datasets?\n- Can the authors show empirical correlation heatmaps or mutual information between paired channels before/after quantization?\n- Is there any measurable improvement in reconstruction loss if the channel pairs are shuffled (i.e., destroying geometric adjacency)?\n- Could the authors relate their 2D lattice structure to lattice quantization theory or product quantization—are there formal efficiency bounds?\n- How is the pairing performed, sequentially, randomly, or learned? Is there any adaptive pairing strategy that further improves correlation capture?\n- Have the authors tested Q2D2 tokens as inputs for an existing audio-LM to verify downstream quality or token predictability?\n- How does Q2D2 affect token entropy or n-gram statistics compared to FSQ/RVQ tokens?\n- Would the structured grids yield smoother latent manifolds that benefit autoregressive or diffusion decoders?\n- Could Q2D2 potentially replace FSQ in multimodal systems like Moshi (or other systems)? If not, what limitations remain?\n- Could the authors visualize quantization error distributions or Voronoi regions for different grid types?\n- Does rhombic quantization yield lower average distortion for isotropic Gaussian inputs compared to rectangular grids?\n- Is the improvement consistent across random seeds or data subsets, or within statistical noise?\n- Would 3D grids (briefly mentioned in Appendix D) further increase utilization or just add complexity?\n- What is the average training time per epoch compared to FSQ or RVQ on the same hardware?\n- Does the grid lookup add noticeable computational overhead during decoding?\n- How do memory and latency scale with grid resolution (e.g., l = 7 -> 11)?\n- Could the 2D quantization be vectorized efficiently for real-time applications?\n- There are some minor presentation issues like:\n    - The related work discussion is a bit messy. You might consider discussing first quantization methods, then neural audio codecs, and end with comparisons.\n    - Consider merging repetitive tables or moving some detailed numeric comparisons to the appendix."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ak7VQ0Ifzt", "forum": "LVXc2Jvjme", "replyto": "LVXc2Jvjme", "signatures": ["ICLR.cc/2026/Conference/Submission1535/Reviewer_oMqv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1535/Reviewer_oMqv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637696029, "cdate": 1761637696029, "tmdate": 1763675686996, "mdate": 1763675686996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the important field of speech discrete codecs. Building upon the WavTokenizer architecture, the authors primarily enhance the quantization method (replacing standard VQ with Q2D2) to achieve improved reconstruction quality at a lower bitrate. The overall design of Q2D2 shares a strong conceptual similarity with FSQ (Finite Scalar Quantization), in that both employ direct quantization to grid points rather than relying on codebook similarity search. The key distinction, as suggested by its name, is that Q2D2 groups feature channels into pairs and then performs direct quantization onto a two-dimensional geometric coordinate system. The authors perform ablations on different 2D geometric structures, including Hexagonal, Rectangular, and Rhombic tilings. The experimental setup is largely consistent with the original WavTokenizer work, and the results demonstrate the expected performance gains in both reconstruction and semantic evaluation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I find the Q2D2 discretization method to be intriguing. Although fundamentally similar to FSQ, the strategy of grouping channels into pairs for 2D plane quantization represents a novel and constructive modification. Therefore, my overall score is positive."}, "weaknesses": {"value": "1. Given that the core contribution of this work is the proposal of a novel quantization method (Q2D2), a standard and robust experimental configuration should include validation across broader domains (e.g., image, video, and general speech) to verify the resulting reconstruction quality and downstream generation performance.\n\n2. The paper requires additional ablation studies. Specifically, a comparison between an FSQ-based WavTokenizer, the proposed Q2D2-based WavTokenizer, and a WavTokenizer implemented with the latest state-of-the-art quantization methods (beyond VQ/FSQ) is necessary.\n\n3. A more rigorous comparison against more recent audio codec baselines, such as xcodec2 and similar models, is required to properly benchmark the proposed approach.\n\n4. While the methodology is interesting, the authors need to provide a deeper theoretical explanation of why two-channel joint quantization is empirically superior to direct one-dimensional quantization, especially considering the potential for two paired representations to be strongly anticorrelated."}, "questions": {"value": "1. Have the authors considered extending this approach to three-dimensional quantization by grouping three channels together for projection and quantization in 3D space?\n\n2. The paper of wavtokenizer as the backbone was cited incorrectly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ILYtOF4Qp0", "forum": "LVXc2Jvjme", "replyto": "LVXc2Jvjme", "signatures": ["ICLR.cc/2026/Conference/Submission1535/Reviewer_7xrP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1535/Reviewer_7xrP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917926276, "cdate": 1761917926276, "tmdate": 1762915798541, "mdate": 1762915798541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a quantisation framework aiming to combine the robustness of finite scalar quantisation (FSQ) with the expressive capacity of multi-dimensional grids. The authors introduce a rhombic grid quantisation approach and claim that it achieves higher *packing efficiency* than alternative grids. The method is evaluated primarily on speech data and compared with models such as WavTokenizer. While the idea of multi-dimensional quantisation is conceptually interesting, the paper leaves several methodological and interpretative gaps that make it difficult to assess the strength of the contribution.\n\n\nDisclosure: I used a large language model only for grammar, clarity, and structuring; all substantive review content is my own, and the manuscript was not provided to the LLM."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper explores an under-examined direction of combining robustness and expressiveness in quantisation.  \n- The proposal to use rhombic grids is novel and potentially beneficial, as shown in the experimental results.\n- The proposed method delivers results that are at least comparable to baseline models with relatively few numbers of tokens."}, "weaknesses": {"value": "Conceptual clarity and positioning\n\n- The introduction claims the method “combines the robustness of FSQ with the expressive capacity of multi-dimensional grids,” but FSQ is missing from the benchmark, weakening the narrative.\n\nArchitecture and implementation details\n\n- Architecture details are insufficient. It is unclear which parts are inherited from WavTokenizer and what is modified. Learning rate schedule is not specified. \n- Training on a small dataset (LibriTTS) may reduce comparability with other models trained on larger datasets.\n\nExperimental design and benchmarks\n\n- Lack of FSQ baseline prevents a direct evaluation of the claimed improvements.\n\n- Low token count may distort comparisons, since bitrate can be balanced via other parameters.\n\n- Evaluation is limited to speech, whereas many baselines are cross-modal. Performance gains over WavTokenizer are modest.\n\nWriting and presentation\n\n- Packing efficiency is undefined.\n- Figure 3 is not discussed.\n- Grammatical issues remain."}, "questions": {"value": "Conceptual clarity\n\n- Is it correct to say that FSQ appears conceptually similar to the rectangular grid variant of the proposal? Should this be stated explicitly?\n\n- Does “built on the framework of WavTokenizer” refer to the architecture, training pipeline, or something else?\n\n- Why did the authors train on a smaller subset (LibriTTS) instead of the full WavTokenizer dataset? How should the results be interpreted in comparison to models trained on the full dataset?\n\nExperimental design\n\n- Why are FSQ models omitted from the benchmark, even though the proposal method seems to build directly on top of FSQ?\n\n- Why are the numbers of tokens relatively low compared to competing models? Could other parameters have been tuned to maintain a comparable bitrate? What happen if the models compete with similar token counts?\n\nDefinitions / presentation\n\n- What exactly does “packing efficiency” mean in this context?\n\n- Why is Figure 3 not referenced in the text? Comments on how UTMOS does not scale with the token count with the proposed method would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bBIzE5ceSt", "forum": "LVXc2Jvjme", "replyto": "LVXc2Jvjme", "signatures": ["ICLR.cc/2026/Conference/Submission1535/Reviewer_U91h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1535/Reviewer_U91h"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931898099, "cdate": 1761931898099, "tmdate": 1762915798223, "mdate": 1762915798223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Q2D2, a novel geometry-aware quantization scheme for neural audio codecs that organizes latent feature pairs into structured two-dimensional grids, such as hexagonal, rectangular, and rhombic tilings. This approach lies between vector quantization (VQ) and finite scalar quantization (FSQ), aiming to achieve greater latent expressiveness while maintaining the robustness of FSQ. In speech reconstruction tasks, Q2D2 is implemented on top of the WavTokenizer framework and demonstrates improved performance metrics at low token rates compared to other baseline speech codecs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of using two-dimensional quantization to improve the representational capacity of FSQ is novel and well-motivated. \n2. The illustrations and explanations of the method, particularly the comparisons to VQ and FSQ, are clear and easy to follow. \n3. The experiments on audio compression are concise and demonstrate the effectiveness of Q2D2 in improving speech coding performance at low token rates."}, "weaknesses": {"value": "1. The main weakness lies in the experimental design. Although the paper’s primary contribution is a new quantization approach, no experiments directly compare Q2D2 with VQ and FSQ under the same framework. \n2. Q2D2 quantizes pairs of encoder output features using a fixed 2D grid, which is conceptually related to product quantization (PQ). The paper could clarify more explicitly how Q2D2 relates to VQ, FSQ, and PQ, highlighting similarities and differences."}, "questions": {"value": "1. As in Weakness 1, can the authors provide experiments that directly compare Q2D2 with VQ and FSQ under the same framework? This would better demonstrate the effectiveness of Q2D2. \n2. I understand that Q2D2 quantizes neighboring feature pairs, which is a straightforward choice. However, since the quantization grid has a predefined geometry, could alternative grouping strategies better capture inter-channel dependencies? \n3. Would the authors consider evaluating Q2D2 on downstream generative tasks, such as text-to-speech (TTS)? This could further demonstrate its utility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wVy3d7UCL0", "forum": "LVXc2Jvjme", "replyto": "LVXc2Jvjme", "signatures": ["ICLR.cc/2026/Conference/Submission1535/Reviewer_kceZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1535/Reviewer_kceZ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission1535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959590853, "cdate": 1761959590853, "tmdate": 1762915797783, "mdate": 1762915797783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an improvement to Finite Scalar Quantization (FSQ) named Q2D2 for audio tokenization.\nInstead of the independent element-wise quantization in FSQ, Q2D2 treat 2 adjacent element as a pair, then quantize this pair with scalar quantization. The authors believe that the more expressive 2-element feature space will bring advantages over the 1-element feature space in FSQ.\nEvaluation was done on LibriTTS and LJSpeech over a bunch of baselines, showing that Q2D2-WavTokenizer (proposed method) is competitive in many scenarios.\n\nAlthough the paper is well formatted, typos, confusing citations, or confusing technical terms exist. Moreover, as an improved version of FSQ, none of FSQ-based audio tokenizers is evaluated in this paper, making it impossible for the readers to know if Q2D2 really improved FSQ.\n\nAs a reviewer, I tend to reject this paper."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Proposed an interesting scalar quantization method called Q2D2, and used this quanitzation method to train an audio tokenizer that showed competitive performance"}, "weaknesses": {"value": "## Experimental problems\n- Although the proposed Q2D2 is inspired by FSQ, Q2D2 is never compared with FSQ.\n    - It is impossible for a reader to know if Q2D2 really improved FSQ\n- Missing baselines\n    - While the authors say WavTokenizer is the SOTA for single-layer audio tokenizer, BigCodec https://arxiv.org/abs/2409.05377 shall be another important baseline.\n    - StableCodec https://arxiv.org/abs/2411.19842v1, a single-layer audio tokenizer with FSQ, is also ignored in this paper.\n    - XCodec2 https://arxiv.org/abs/2502.04128, a paper came out in February 2025 that presents a single-layer audio tokenizer with FSQ, is also ignored\n    - With all these FSQ baselines missing, the evaluation of this paper is not solid.\n- Improper usage of UTMOS\n    - UTMOS is trained to rate highly for a good speech signal.\n    - LibriTTS test-other is noisy --> the ground truth data will be rated low by UTMOS\n    - Q2D2 producing higher UTMOS than ground truth --> indicating that Q2D2 cannot reconstruct noises, so we cannot say the high UTMOS value of Q2D2 in this dataset is an advantage -- it is actually the disadvantage of Q2D2 model. \n## Paper writing issues\n- I feel the citation for WavTokenizer is confusingm and I cannot find it: Kundan Kumar, Chengyi Wu, Kevin J Shih, Yu Zhang, and Abdelrahman Mohamed. Wavto\u0002kenizer: Learning discrete audio tokens from self-supervised representations. arXiv preprint arXiv:2408.16532, 2024.\n    - The WavTokenizer that I know is this link https://arxiv.org/abs/2408.16532, but the title and author list are totally different.\n- Line 123: To my knowledge, \"EMA\" in the context of VQ-VAE usually means Exponential Moving Average, but the authors say it is Expectation-Maximization Attention. I cannot find this term in the original or other VQVAE paper, please clarify this point."}, "questions": {"value": "- Is the citation for WavTokenizer generated by LLM?\n- Line 123: To my knowledge, \"EMA\" in the context of VQ-VAE usually means Exponential Moving Average, but the authors say it is Expectation-Maximization Attention. I cannot find this term in the original or other VQVAE paper, could you clarify it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yyUPW92nBD", "forum": "LVXc2Jvjme", "replyto": "LVXc2Jvjme", "signatures": ["ICLR.cc/2026/Conference/Submission1535/Reviewer_2e16"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1535/Reviewer_2e16"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission1535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977793494, "cdate": 1761977793494, "tmdate": 1762915797465, "mdate": 1762915797465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}