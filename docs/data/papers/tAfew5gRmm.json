{"id": "tAfew5gRmm", "number": 5439, "cdate": 1757909922020, "mdate": 1759897975179, "content": {"title": "AMLRIS: Alignment-aware Masked Learning for Referring Image Segmentation", "abstract": "Referring Image Segmentation (RIS) aims to segment the object in an image uniquely referred to by a natural language expression. However, RIS training often contains hard-to-align and instance-specific visual signals; optimizing on such pixels injects misleading gradients and drives the model in the wrong direction. By explicitly estimating pixel-level vision–language alignment, the learner can suppress low-alignment regions, concentrate on reliable cues, and acquire more generalizable alignment features.\nIn this paper, we propose Alignment-Aware Masked Learning (AML), a simple yet effective training strategy that quantifies region–referent alignment (PMME) and filters out unreliable pixels during optimization (AFM). Specifically, each sample first computes a similarity map between visual and textual features, and then masks out pixels falling below an adaptive similarity threshold, thereby excluding poorly aligned regions from the training process. AML does not require architectural changes and incurs no inference overhead, directing attention to the areas aligned with the textual description. Experiments on the RefCOCO (vanilla/+/g) datasets show that AML achieves state-of-the-art results across all 8 splits, and beyond improving RIS performance, AML also enhances the model’s robustness to diverse descriptions and scenarios.", "tldr": "", "keywords": ["Reference Image Segmentation， Masked Learning， VLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0c666c499cf73558c6d2f004cec76b916577814d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Alignment-Aware Masked Learning (AML) for referring image segmentation (RIS). AML computes a patch–token alignment map via PMME (random projections per modality) and then applies an Alignment-Aware Filtering Mask (AFM) that suppresses low-alignment image regions during training only; inference is unchanged. Experiments show consistent gains on RefCOCO/+/g when AML is added to CARIS, smaller gains on DETRIS, early-stage experiments on ReLA, and robustness to synthetic perturbations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  **Plug-and-Play Method with Zero Inference Overhead** - Masking is applied exclusively during training. Consequently, the method requires no architectural changes and incurs no additional compute or memory costs during inference.\n\n2.  **Strong Performance** - AML consistently boosts CARIS performance across all eight RefCOCO/RefCOCO+/RefCOCOg splits. It also transfers effectively to other baselines, such as DETRIS, and shows promising results with ReLA.\n\n3.  **Robustness and Interpretability Analysis** - The paper shows improved performance under seven different visual perturbations (e.g., haze, low-light, occlusion), supported by clear qualitative maps that explain where AML helps.\n\n4.  **Thorough Ablation Studies** - The paper contains comprehensive ablations examining the threshold, projection dimension, projection strategy (random vs. learnable), and masking level (image vs. feature), which validate the design choices."}, "weaknesses": {"value": "1. **Gaps in Mathematical Derivations.** The appendix asserts\n   $$\n   \\langle W_i v, W_t u\\rangle = \\tfrac{1}{2}\\langle \\tilde W z, \\tilde W z\\rangle,\\quad\n   \\tilde W = \\tfrac{1}{\\sqrt{2}}\\mathrm{diag}(W_i, W_t),\\quad\n   z = \\begin{bmatrix} v \\ u \\end{bmatrix}.\n   $$\n   But\n   $$\n   \\langle \\tilde W z, \\tilde W z\\rangle\n   = \\left\\langle \\tfrac{1}{\\sqrt{2}}\\begin{bmatrix} W_i v \\ W_t u \\end{bmatrix},\n   \\tfrac{1}{\\sqrt{2}}\\begin{bmatrix} W_i v \\ W_t u \\end{bmatrix}\\right\\rangle\n   = \\tfrac{1}{2}\\big(|W_i v|^2 + |W_t u|^2\\big),\n   $$\n   which is the *sum of squared norms* (no cross term), so it cannot equal $\\langle W_i v, W_t u\\rangle$ except in degenerate cases. Moreover, with *independent* Gaussian projections $W_i, W_t$ (entries $\\sim \\mathcal N(0, 1/D_a)$) and unit $v, u$,\n   $$\n   \\mathbb{E}[\\langle W_i v, W_t u\\rangle] = 0,\\qquad\n   \\mathrm{Var}[\\langle W_i v, W_t u\\rangle] = \\tfrac{1}{D_a},\n   $$\n   so the projected cross dot *concentrates at 0* as $D_a$ grows, making preservation of a nonzero cross-modal inner product *impossible* under the stated construction. This invalidates the paper’s stated Theorem 1 (cross-modal inner-product preservation under block-diagonal projection) and any corollaries that rely on it (e.g., PMME’s “geometry-preserving” property).\n\n2. **Evaluation Gaps**:\n    1. *Lack of Direct Baseline Comparison:* The evaluation is missing a direct comparison against a key alignment-based baseline, Mask Grounding, within the same experimental setting. Since AML is presented as a plug-and-play, alignment-aware strategy similar to Mask Grounding, its claimed universality should be validated by outperforming this baseline under identical, fully trained conditions. Table 2, however, only reports CARIS/DETRIS add-ons and omits Mask Grounding, leaving this critical parity untested. The table should have directly compared Mask Grounding on CARIS/DETRIS with AML on CARIS/DETRIS.\n    2. *Omission of MagNet mIoU Score:* Table 1 omits the mIoU score for MagNet, an important alignment baseline. \n\n3. **Training Efficiency:** Training and memory costs from implementing AML should be put in the main paper, not the appendix.\n\n4. **Generalization / Real-world robustness:** The paper's visualizations are limited to the heavily optimized RefCOCO datasets, making it difficult to assess performance on unconstrained \"in-the-wild\" images. Including qualitative results on such images, mirroring the style of Figure 4(a), is essential to demonstrate the model's real-world generalization\n\n5. **Reproducibility:** Since code is not given, adding some crucial pseudo code in the paper will be tremendously helpful for the broader audience to reproduce this work.\n\n6. **Many Writing & Formatting Issues**: \n    1. *References style:* inconsistent reference format; inconsistent capitalization (e.g., an author name in ALL CAPS on line 594); duplicate/inconsistent entries (e.g., CRIS 2022a/2022b). Standardize to ICLR format.\n    2. *Typos and formatting errors:* e.g. casing of softmax/Softmax is sometimes small (equation 16) and sometimes large (equation 6); no spacing in Theorem1 and Theorem2; PPME in line 1106.\n    3. *Unconventional figure naming:* Figure a, Figure b, Figure c etc.\n    4. *Readability of text in figures:* Some texts in Figure 1, Figure a and Figure b are too small."}, "questions": {"value": "Refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XCxnyjrK57", "forum": "tAfew5gRmm", "replyto": "tAfew5gRmm", "signatures": ["ICLR.cc/2026/Conference/Submission5439/Reviewer_X2d4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5439/Reviewer_X2d4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760862809473, "cdate": 1760862809473, "tmdate": 1762918063236, "mdate": 1762918063236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This work proposes Alignment-Aware Masked Learning (AML), a training strategy that quantifies region–referent alignment (PMME) and filters out unreliable pixels during optimization (AFM), which is validated to improve RIS performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This work proposes Alignment-Aware Masked Learning (AML), a training strategy that quantifies region–referent alignment (PMME) and filters out unreliable pixels during optimization (AFM), which is validated to improve RIS performance.\n\n- The writing overall is good and it is easy for readers to understand the proposed framework.\n\n- The experiments analysis is detailed for readers  to realize the benefits of AML."}, "weaknesses": {"value": "- Motivation clarification: the motivation is not well clarified from figure-1.  I suppose the author's motivation is that a number of regions (especially background regions) dominate the training loss.\n\n- Method contributions\n  - Based on the above motivation, I am more inclined to believe that this work is actually an implementation of curriculum learning in the RIS task. It is also be validated from the efficiency of early-training stage. In view of the originality, it decreases the contributions of this work.\n  - The work claims that the stage-I is forward-only. I am curious how the similarity of these raw visual-language features reflects the degree of alignment.\n  - The whole performance improvement is weak especially on the more strong models, which makes the work seem less significant at present community.\n\n- Some writings are confusing.\n  - The explanation about $B^h$ and  $B^w$ (line-267).\n  - For the `early-stage efficiency', to my knowledge, this is not a common term and it deserves a specific explanation.\n\n- Extra experiments for validation\n  - The author can verify the results of using different models at different stages (e.g., utilize ReLA for stage-1, CARIS for stage-2), which may bring some new observations."}, "questions": {"value": "Refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M4ayErPZzX", "forum": "tAfew5gRmm", "replyto": "tAfew5gRmm", "signatures": ["ICLR.cc/2026/Conference/Submission5439/Reviewer_Vg5H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5439/Reviewer_Vg5H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761760230019, "cdate": 1761760230019, "tmdate": 1762918062931, "mdate": 1762918062931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an alignment-aware masked learning method for referring image segmentation. In particular, each sample is masked out by discarding pixels below an adaptive similarity threshold. The similarity map between visual and textual features is quantified by region-referent alignment. The framework is then trained after the first masking step. The above two steps are conducted in an interleaved fashion. In addition, the region-referent alignment is implemented via a PatchMax Matching Evaluation strategy on randomly projected visual and textual features. Experimental results validated the effectiveness of the proposed method."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "(1) The motivation is well presented of using the proposed alignment-aware masked learning approach for referring image segmentation.\n\n(2) The explanations and illustrations are mostly clear and intuitive of the PatchMax Matching Evaluation, the alignment-aware filtering mask and the training strategy."}, "weaknesses": {"value": "(1) The approach of using a previous-step inference for mask prediction and guide the current learning may face convergence issue. In fact, the initial state of mask is largely incorrect and can result in unexpected learning curves. There is no discussion on this issue.\n\n(2) On the fairness of experimental comparison, since CARIS+AML uses 17.2% more training time than CARIS (according to Appendix G.2), the performance gain in Table 1 is also possibly coming from longer training. There is no ablation study and discussion on this issue."}, "questions": {"value": "No."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "B1alG04U7D", "forum": "tAfew5gRmm", "replyto": "tAfew5gRmm", "signatures": ["ICLR.cc/2026/Conference/Submission5439/Reviewer_P4qH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5439/Reviewer_P4qH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826496954, "cdate": 1761826496954, "tmdate": 1762918062658, "mdate": 1762918062658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a novel framework called AMLRIS, which aims to improve the performance of Referring Image Segmentation (RIS). The framework first introduces a Johnson-Lindenstrauss random projection to measure the similarity between image representations and token features. Then, the image pixels with low similarity are filtered out from the training process to stabilize the training and improve performance. Experimental results demonstrate that AMLRIS achieves superior performance compared to standalone training, and the ablation experiments show the effectiveness of the proposed modules."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "• The projection design provides a novel approach for measuring similarity between representations of different modalities. This method can be extended to more tasks.\n• Experiments demonstrate the effectiveness of the proposed structure, achieving competitive results across multiple downstream datasets.\n• The proposed structure does not significantly increase training overhead while maintaining inference time.\n• The proposed idea is interesting and generally well-motivated, and the experimental evaluation is relatively thorough"}, "weaknesses": {"value": "My primary concern is the method’s sensitivity to small or low-contrast objects. The AML\nframework relies on PMME to generate alignment-based masks by identifying high-confidence visual patches. This mechanism inherently depends on the relative distribution of features within the image. As a result, small objects or objects with low visual saliency may produce low peak alignment scores and be incorrectly masked out during training. Consequently, the model’s performance may degrade on images where the target occupies a very small region or is visually subtle.\n• In Figure 2, the masked pixels appear to be almost exclusively background regions. It is unclear\nwhether masking such areas truly helps the model focus on the target objects. In my view, it would be more important to mask regions corresponding to potentially confusing objects rather than background. This raises some concerns regarding the effectiveness of PMME in guiding the model’s attention.\n• The mechanisms underlying some of the core components of the model remain unclear. I would be willing to consider a higher score if the authors provide clear explanations addressing my concerns."}, "questions": {"value": "See Weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EyDaOoG726", "forum": "tAfew5gRmm", "replyto": "tAfew5gRmm", "signatures": ["ICLR.cc/2026/Conference/Submission5439/Reviewer_pnob"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5439/Reviewer_pnob"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762073221000, "cdate": 1762073221000, "tmdate": 1762918062430, "mdate": 1762918062430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}