{"id": "t3bTQF2Fzy", "number": 14702, "cdate": 1758242074200, "mdate": 1759897354074, "content": {"title": "Exploring the Frontiers of Softmax: Provable Optimization, Applications in Diffusion Model, and Beyond", "abstract": "The softmax activation function plays a crucial role in the success of large language models (LLMs), particularly in the self-attention mechanism of the widely adopted Transformer architecture. However, the underlying learning dynamics that contribute to the effectiveness of softmax remain largely unexplored. As a step towards better understanding, this paper provides a theoretical study of the optimization and generalization properties of two-layer softmax neural networks, providing theoretical insights into their superior performance as other activation functions, such as ReLU and exponential. Leveraging the Neural Tangent Kernel (NTK) framework, our analysis reveals that the normalization effect of the softmax function leads to a good perturbation property of the induced NTK matrix, resulting in a good convex region of the loss landscape. Consequently, softmax neural networks can learn the target function in the over-parametrization regime. To demonstrate the broad applicability of our theoretical findings, we apply them to the task of learning score estimation functions in diffusion models, a promising approach for generative modeling. Our analysis shows that gradient-based algorithms can learn the score function with a provable accuracy. Our work provides a deeper understanding of the effectiveness of softmax neural networks and their potential in various domains, paving the way for further advancements in natural language processing and beyond.", "tldr": "", "keywords": ["Softmax", "Theory", "NTK", "Large Language Models"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d7b7559206365fc49493eb680a82fdaffe987588.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper provides a theoretical study of softmax neural networks, aiming to explain what makes softmax-based architectures successful. Using the Neural Tangent Kernel (NTK) framework, the authors analyze a two-layer softmax neural network and prove convergence and generalization guarantees in the overparameterized regime. The paper argues that the normalization property of softmax induces  a large convex region in the loss landscape. To showcase applicability, the analysis is extended to diffusion models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper takes on an important theoretical question: understanding why softmax works so well in modern architectures. \nconnected to previous NTK results for ReLU and exponential activations.\n\n2. The extension to score-based diffusion models demonstrates practical relevance and bridges theoretical and generative modeling literature."}, "weaknesses": {"value": "1. The paper is mathematically dense but conceptually underexplained. The intuition for why softmax’s normalization leads to better generalization could be emphasized more. The paper is hard to read.\n\n2. The “application” section is purely analytical — no experiments, no empirical validation. While the theoretical link is elegant, it remains disconnected from practice. For ICLR, empirical support or simulations demonstrating the claimed advantage would be expected."}, "questions": {"value": "1. How does the normalization-induced “convex region” in softmax NTK differ quantitatively from the ReLU NTK case?\n\n2. Can you empirically validate the theoretical claims on small synthetic datasets (e.g., compare softmax vs. ReLU NTK behavior)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "pcXzfBLTv6", "forum": "t3bTQF2Fzy", "replyto": "t3bTQF2Fzy", "signatures": ["ICLR.cc/2026/Conference/Submission14702/Reviewer_dU7o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14702/Reviewer_dU7o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761284056020, "cdate": 1761284056020, "tmdate": 1762925067663, "mdate": 1762925067663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies property of two-layer softmax NN. Specifically, the authors exploit NTK tool to analyze convergence of two-layer softmax NN, and extends the result to diffusion model as an application."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper studies properties of two-layer softmax NN. The topic is worth exploring since softmax is playing important role in modern AI systems. \n\nThe authors have established convergence result in Theorem 4.2, based on NTK theoretic tools.\n\nThe authors have also extended their theoretic results to diffusion models, and establish convergence there."}, "weaknesses": {"value": "It seems that the main contribution of the current paper is establishing convergence results for two-layer softmax NN and extends the result to diffusion model. The analysis is standard NTK analysis (though in Section 5.1the authors explicitly explain what are the new challenges in their theoretic derivation).\n\nAlso I personally feel the paper can largely benefit from adding more explanation, discussion, and comparisons. The current presentation is too notation-heavy, and lacks explanation of intuition and general understanding beyond just convergence. Some of highlight points are not well-justified. For example, in the abstract, the authors state \"normalization effect of the softmax function leads to a good perturbation property of the induced NTK matrix, resulting in a good convex region of the loss landscape\". But in main text, there is little verbal explanation beyond notation-heavy theorems. I feel the current presentation can improve a lot if the authors add more explanations about high-level intuition, compare to focusing more on technical details.\n\nSome notation confusions exist."}, "questions": {"value": "In Table 1 and and Lemma 5.1, there is comparison with ReLU and exp activation. The result seems that all three activations: ReLU, exp, and softmax, have same convergence and perturbation properties. Can I interpret it as all three  activations have similar performance from NTK perspective? Then if we replace softmax, by ReLU or exp, we will still get similar results? Also, the authors mention the key to softmax is \"the normalization effect of the denominator\", while ReLU and exp don't have this same denominator, their effectiveness are  drawn by some other mechanisms. A more comprehensive comparison between the underline mechanisms will be interesting.\n\nminor errors?:\n\nline 1033-1035: there are questions marks not rendered correctly\n\nline 83-84: people usually write token embedding dimension as $n'\\times d$ instead of $d\\times n'$, which is a bit hard to follow.\n\nline 199-203: what is $d$ here? is it $d_1=d_2=d$?\n\nline 204: it should be $x_i\\in\\mathbb R^d, y_i\\in\\mathbb R^d$ instead of $x\\in\\mathbb R^d, y\\in\\mathbb R^d$?\n\nline 237: a close bracket is missing in $d\\mathcal L(W(\\tau))$\n\nnot sure why Remark 5.2 is involved..."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "eWI8Gh14AY", "forum": "t3bTQF2Fzy", "replyto": "t3bTQF2Fzy", "signatures": ["ICLR.cc/2026/Conference/Submission14702/Reviewer_rQ1Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14702/Reviewer_rQ1Y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761534607118, "cdate": 1761534607118, "tmdate": 1762925067181, "mdate": 1762925067181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper studies the property of the softmax activation function in the two-layer network setting.\n- Especially, the authors utilize the neural tangent kernel framework to theoretically analyze the perturbation property. \n- To demonstrate their theoretical analysis in the practical settings, the authors apply their work in the diffusion models."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is theoretically grounded."}, "weaknesses": {"value": "- The paper is unnecessarily over-complicated.\n- In fact, LLMs can be the use case of the proposed work, but there seems to be no need to give a long explanation of LLMs. I recommend removing Sections 2.2 and 2.3 as well. Instead, the authors can explain some works, such as  Munteanu et al. (2022), in the related work section, since it is more directly linked to the proposed work. \n- In Table 1, I disagree with Line 57-59: We can see that ... For example, $n^2$ and $n^{2+o(1)}$ can be hugely different.\n- Some synthetic experiments should be conducted to verify how the theoretical analysis connects to the practical setting."}, "questions": {"value": "- Do assumptions in Section 6 always hold for the diffusion models?\n- The authors have assumed a two-layer network, but the noise prediction score network in the diffusion models is very complicated, with U-Net architectures. How does this gap reduce in the (proof of) Theorem 6.6?\n- What can be claimed in the LLMs, similarly to Theorem 6.6?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ShsVpnfm6W", "forum": "t3bTQF2Fzy", "replyto": "t3bTQF2Fzy", "signatures": ["ICLR.cc/2026/Conference/Submission14702/Reviewer_YdFW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14702/Reviewer_YdFW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762195758516, "cdate": 1762195758516, "tmdate": 1762925066755, "mdate": 1762925066755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the convergence of GD-trained transformer with softmax activate under the NTK framework. As an application, the score matching problem in diffusion models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Under the NTK framework, the authors establish the convergence rate for training softmax transformers using gradient descent.\n2. By leveraging the connection with score matching in diffusion models and multi-label regression, the authors obtain the convergence rate of training score functions."}, "weaknesses": {"value": "While technical ideas make sense to me, I still have the following concerns:\n\n1. Some literature shows that softmax transformer has better sequence length dependence compared with ReLU networks. Do you results also support this point? \\\n2. Any technical difficulties when applying the NTK techniques for softmax activation compared to ReLU networks? This is not clearly demonstrated in the manuscript."}, "questions": {"value": "I have minor questions and suggestions:\n\n1. The connection between score matching and regression is not clear. Although you cited the literature Han et al. 2024, I would appreciate it if you could make the writing even more clear. \n2. Assumption 6.1, I understand $ y_i $ can be bounded as $ y_i = \\mathbb{E}[x_0 \\vert x_t] $ and $ x_0 $ can be bounded. However, $ x_i $ is the noisy latent obtained by adding noise. Why could we also assume that is bounded?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EMkEF087H7", "forum": "t3bTQF2Fzy", "replyto": "t3bTQF2Fzy", "signatures": ["ICLR.cc/2026/Conference/Submission14702/Reviewer_uDsP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14702/Reviewer_uDsP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762198846427, "cdate": 1762198846427, "tmdate": 1762925066274, "mdate": 1762925066274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}