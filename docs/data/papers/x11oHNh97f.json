{"id": "x11oHNh97f", "number": 17848, "cdate": 1758281191539, "mdate": 1759897150270, "content": {"title": "When Alignment Hurts: Decoupling Representational Spaces in Multilingual Models", "abstract": "It is often assumed that aligning low-resource varieties with high-resource standards improves modeling in multilingual Large Language Models (LLMs). We challenge this view with the first causal study showing that excessive representational entanglement with dominant varieties can reduce generative quality. We introduce an online variational probing method that continuously estimates the subspace of a dominant variety during fine-tuning on a generative task and penalizes it to reduce its span. Across six language families we find that reducing alignment consistently boosts low-resource translation performance, including +11.7 ChrF++ for European Portuguese, +5.3 for Indonesian, +4.6 for Kven Finnish, and +2.7 for Low German. In Arabic, several dialects improve by up to +4.3 ChrF++ despite sharp drops for cross-lingual tasks such as translation to MSA, English, or French, suggesting that the effect extends beyond simple cross-lingual alignment. Alongside these causal results, we present qualitative and observational evidence from information-theoretic and geometric probing that further supports our hypothesis. Together, our findings establish that disentangling high-resource subspaces can unlock capacity for related low-resource varieties and provide practical tools for controlling representational allocation in multilingual LLMs.", "tldr": "Reducing representational entanglement with high-resource languages improves generative modeling for related low-resource varieties through causal subspace interventions.", "keywords": ["Causal Representation Learning", "Multilingual Language Models", "Representation Disentanglement", "Low-resource Language Modeling", "Subspace Probing", "Generative Modeling"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4dbd5d47262bddbe6097573bc0330868b6571489.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the relationship between the internal language representation alignment and multilingual performance and challenge the assumption that alignment with a high-resource standard is always beneficial. The authors first introduce a training method named online subspace decoupling and use it to fine-tune multilingual large language models on inter-variety machine translation data. They further propose a geometric analysis and information-theoretic probing to evaluate their assumption. Experimental results show that their training method advances the most of language performance excepting some high-resource languages like French and Modern Standard Arabic."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "It is interesting to investigate the relationship between the internal language representation alignment and multilingual performance."}, "weaknesses": {"value": "- Only chrF++ is used to evaluate the performance of machine translation task. Other metrics like COMET[1], which is found better correlation with human judgements, can be adopted to improve the soundness of these findings.\n\n- The writing of this paper is poor. For example, it is hard to follow Figure 4 and its caption. How to infer that Aya exhibits clearer separation between dialectal clusters than other models? Figure 3 is presented at the 7th page but is referred at the 9th page.\n\n- The assumption may not be well supported for missing the statistic of the separation in the model trained by Online Decoupling Training method, and some models like Qwen 3-14B is different to the Aya (Figure 6). \n\n**References**\n\n[1] Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A Neural Framework for MT Evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685–2702, Online. Association for Computational Linguistics."}, "questions": {"value": "1) How about the results of COMET or human evaluation on some language pairs? Are they aligned with the ones of chrF++?\n\n2) (Figure 4) How to infer that Aya exhibits clearer separation between dialectal clusters than other models?\n\n3) Line 428: Given the negative relationship between cosine distance and performance, which means that better alignment (lower cosine distance) in these layers is beneficial, does this contradict your previous assumption?\n\n4) Line 431: Given the different correlation pattern between Aya and other models, how to infer that \"subspaces must be aligned enough for\nknowledge transfer but separate enough to preserve unique dialectal features.\" ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dwVPYLJtaT", "forum": "x11oHNh97f", "replyto": "x11oHNh97f", "signatures": ["ICLR.cc/2026/Conference/Submission17848/Reviewer_HphX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17848/Reviewer_HphX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761380810699, "cdate": 1761380810699, "tmdate": 1762927680459, "mdate": 1762927680459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses representational entanglement in the inter-variety MT setting, and proposes a method to decouple varieties during MT fine-tuning of LLMs. This method projects varieties onto a high-variety subspace, and uses the norm of this projection as a loss penalty. Testing on numerous groups of related languages, the authors find some improvement in some related languages, and some translation degradation in others. Further analysis on Arabic varieties shows that different measures of subspace separation are aligned with performance in the inter-variety MT task."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed online decoupling method is well-motivated and its formulation is well explained. It is also general in its formulation and potentially applicable outside of the inter-variety MT task as well. \n2. The random subspace experiment represented in Table 3, where the question of generic vs targeted hidden space regularization task, is a great analysis to isolate the effect of the specific method proposed. \n3. The paper includes substantial testing across a number of related languauge groups. Different groups may have unique relationships, so testing on a large number helps show the breadth of the current method."}, "weaknesses": {"value": "1. Motivation and contribution mismatch: The introduction to the paper focuses on the problem of \"generative quality\" and makes a claim that cross-lingual alignment sometimes can impede this quality. However, it is revealed much later that the test-bed of this paper is solely the inter-variety translation task. It is much more clearly the case that cross-lingual alignment would indeed harm something like inter-variety translation, but this is not clear from the motivation. Basically, if this were a paper focusing on just inter-variety translation, this would be alright, but the claims early on in the paper make the problem of alignment sound much broader than just in inter-variety translation. \n2. Context in orthogonal subspace projection: The idea of projecting hidden states onto an orthogonal subspace to try to separate subspaces has been previously explored, notably extensible by the continual learning community. While this is a new application of these methods, there is important contextualization to discuss in the related work section. For example, “Gradient Projection Memory for Continual Learning” explore a similar idea, as well as “Orthogonal Subspace Learning for Language Model Continual Learning.” \n3. Does not seem to be consistent in MSA: The degradation on MSA is severe and limits the finding of extensibility across different language varieties. Since bi-directional translation is tested in this paper, as well as claims that the method can help both low- and high-resource varieties within groups, the stark degradation of MSA weakens these claims. Also, it brings into question if the analysis on MSA+varities in section 5.2 onwards is consistent with the subspace decoupling motiation, since MSA performs much more poorly."}, "questions": {"value": "1. Can you further motivate the choice to use English as a pivot for Czech-Slovak and Indonesian-Malay? \n2. What are the components of Figure 4? I am not sure how to read the blue and orange lines, nor the circles in the figures \n3. To be clear, do Sections 5.2 and 5.3 include no subspace decoupling?\n\nTypos: \n\nimabalance -> imbalance (line 99)\n\nprecide -> precise (line 242)\n\nNote:\n\nThe font size in Table 2 is too small to comfortably read, it may be a good idea to change this sizing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sthpmfCRY4", "forum": "x11oHNh97f", "replyto": "x11oHNh97f", "signatures": ["ICLR.cc/2026/Conference/Submission17848/Reviewer_oXfn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17848/Reviewer_oXfn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937433145, "cdate": 1761937433145, "tmdate": 1762927680098, "mdate": 1762927680098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The authors use a probing method that estimates the subspace of given languages during fine-tuning\n- They do this to prevent what they term \"excessive\" alignment of closely related languages and improve dialect performance. \n- They show that doing this improves quality significantly across several high resource-dialect pairs of languages."}, "soundness": {"value": 1}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The specific problem with using multilingual models and co-training with higher resource related languages for dialects is an understudied yet significant problem.\n- I am more familiar with multilingual representation research and think this connection to dialect MT is interesting.\n- The results are reasonable and intuitive"}, "weaknesses": {"value": "- I think a control of a few unrelated languages at least for analysis could strengthen your claims\n- Seeing how these findings change with changing scale would have been great \n\nPresentation etc:\n- You should make it more clear early in the paper that you also introduce a technique \n- nit: From what I understand, you used an existing technique for analysis but introduced a new technique but the abstract and intro reads a bit differently\n- Figure 4 is super unclear"}, "questions": {"value": "- I'd be super curious to see how these findings change at scale, given how much transfer and generalization can change at scale (https://arxiv.org/abs/2403.05530)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HhhjE3hDuq", "forum": "x11oHNh97f", "replyto": "x11oHNh97f", "signatures": ["ICLR.cc/2026/Conference/Submission17848/Reviewer_qrWr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17848/Reviewer_qrWr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762156603686, "cdate": 1762156603686, "tmdate": 1762927679505, "mdate": 1762927679505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new method to simultaneously: (i) measure the extent to which the representations of multiple varieties of a specific language are “aligned”; (ii) use this measure to explicitly train a model to reduce this alignment across training (adding it as a new term in the model’s loss function). The paper finds that adding this extra term to the loss function improves performance on a language’s low-resource varieties, and sometimes also in the high-resource ones. The paper then argues that this shows alignment between a high- and low-resource language’s representation can hurt model performance."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper is relatively well written and nice to read (although I believe key information is left unspecified; see weaknesses below).\n\nThe paper studies an interesting problem: how alignment between languages’ representations affects model performance.\n\nThe paper proposes an interesting method to change the \"amount\" of alignment between languages, which it leverages to study the relationship between alignment and performance."}, "weaknesses": {"value": "I believe this paper is very interesting: it proposes an interesting method, it runs interesting experiments, and has interesting conclusions. Issues in the writing, however, stop me from recommending its acceptance. Specifically:\n\n1. First, one of the key contributions of this paper is unclear from the paper’s description: the online subspace decoupling method. \n\nThe way the paper “Identifies Higher-resource Variety Subspace” is unclear. The paper states:\n> We train a variational linear probe (as in Sec. 4.4) to distinguish the higher-resource variety from all other varieties in a group. We then use Singular Value Decomposition (SVD) on the learned probe weights to extract an orthonormal basis UHR for the higher-resource subspace …\n\nThe paper, however, does not more clearly specify how this variational linear probe is parametrised. If the probe distinguishes the high-resource language from others (one-vs-all), it should be implemented as a single vector. How is SVD applied to this vector? If, alternatively, the model performs a multi-way classification, it can then apply SVD to the probe’s weight matrix. Within the found directions of this multi-way classifier, however, which form the “higher-resource subspace”? Only the one corresponding to the “high-resource logit”? Or all directions, which can also distinguish between low-resource languages? \n\nThe motivation behind “Define Decoupling Loss” is also not very clear. The paper states:\n> This decoupling loss penalizes the magnitude of the projection of the model’s hidden states H onto the higher-resource subspace: $E[|\\mathbf{H}\\mathbf{P}|_2]$\n\nWhere $\\mathbf{P}$ is a projection matrix based on the SVD above. The paper thus minimises the alignment between all hidden states and this projection. What is the intuition behind this? Why are the hidden states of the high-resource variety also being trained this way?\n\n\n2. Second, other methods used in the paper are also underspecified. E.g.,:\n> Furthermore, we compute Subspace Angles (SSA) (Muller-Eberstein et al., 2023) to measure the alignment between subspaces corresponding to different dialects.\n\nThe SSA method is never defined in this paper. Ideally, a paper should be self-contained, and it would be useful if this method were explicitly described here.\n\n\n3. Third, I also believe some interpretations of the plots are unjustified—unless I am misreading these plots. As an example, Figure 3’s caption states:\n\n> Figure 3: (Left) During baseline SFT, the subspace angle (SSA) between MSA and dialects consistently increases, indicating growing representational separation. (Right) This increase in separation correlates directly with improved chrF++ scores. This provides strong evidence that disentangling from MSA is a key mechanism for improving dialectal generation.\n \nHowever, there is no “consistent” increase in SSA on the left plot; I am not even convinced there is an actually increasing trend on it. Similarly, the paper also states:\n\n> As shown in Figure 7, standard fine-tuning causes the code length for all dialects to increase slightly, as the model specializes for generation rather than classification. However, the increase is disproportionately large for MSA.\n\nI see no clear increasing trend in Figure 7 for the low-resource varieties, and even the MSA trend is mixed, with a strong increase followed by a decrease.\n\n4. Finally, the paper makes clear causal claims, which I also believe are unjustified. One of the section’s titles is: “Causal validation: Online subspace decoupling boosts performance”. While I appreciate the proposed method and think it is quite an interesting method to analyse the alignment between language’s subspaces, I do not think it justifies such strong “causal” claims. Formally, a causal method *must* carefully isolate a specific property and remove *all* possible confounders. This is not the case here. The authors themselves are aware of this, as they run an extra experiment to control for one possible confounder:\n\n> To rule out gains from generic hidden space regularization, we tested random subspace shrinking on Arabic dialects. As shown in Table 3, performance consistently dropped below baseline for MSA, the dialects, French, and English, confirming that improvements arise specifically from disentangling oversized higher-resource subspaces rather than from indiscriminate regularization.\n \nWhile this is a good extra experiment, which gives me more confidence that there indeed exists a causal relationship between language alignment and model quality, the proposed method is not enough to measure such a causal effect and only suggests that such an effect exists.\n \n\n\nI want to again highlight that I believe this paper is very interesting and that it has great potential. The issues pointed out above, however, which mostly relate to writing, stop me from recommending its acceptance."}, "questions": {"value": "> Human annotation would be the only true alternative, but is largely infeasible given the small native-speaker populations of many varieties.\n\nI believe human annotation could still be applied to most of the analysed varieties. Even if these are “lower-resource” varieties, there exist plenty of speakers of, e.g., Egyptian Arabic, European Portuguese, and Low German.\n\n\n> To complement the geometric analysis, we employ an information-theoretic variational linear probe (similar to our online subspace decoupling intervention) (Voita & Titov, 2020; Muller-Eberstein et al., 2023). The probe is a sparsity-regularized classifier trained to identify a variety from token-level representations. The resulting negative cross-entropy provides a tight lower bound on the mutual information $I(h; Y)$ between a model’s hidden states and the variety’s identity.\n\nAs argued in Pimentel et al. (2020) and McAllester et al. (2020), to estimate the mutual information you should use the best probe possible, which would typically suggest you should not regularise it. If the goal is estimating the minimum description length of the data (as proposed by Voita et al., 2020), however, then the regularisation might be beneficial.\n\n* Pimentel et al. (2020). Information-Theoretic Probing for Linguistic Structure. https://aclanthology.org/2020.acl-main.420/\n* McAllester et al. (2020). Formal Limitations on the Measurement of Mutual Information. https://proceedings.mlr.press/v108/mcallester20a.html"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RkUY9mpEmP", "forum": "x11oHNh97f", "replyto": "x11oHNh97f", "signatures": ["ICLR.cc/2026/Conference/Submission17848/Reviewer_LwPc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17848/Reviewer_LwPc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762289867452, "cdate": 1762289867452, "tmdate": 1762927678891, "mdate": 1762927678891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}