{"id": "zfkINmO7WZ", "number": 12869, "cdate": 1758211054540, "mdate": 1763443791379, "content": {"title": "Practical Mechanism via Simple Input Control for Fault-Tolerant Spiking Neural Networks", "abstract": "Spiking Neural Networks (SNNs) attract researchers due to their energy-efficient operations in neuromorphic devices. Despite their energy efficiency, hardware-implemented SNNs in neuromorphic devices are vulnerable to hardware faults, which impair the functionality of learnable parameters (e.g., Stuck-At-Faults (SAFs) in synaptic weights). This impairment reduces the capacity to absorb information. When input data contains information exceeding the capacity, SNNs may not absorb information correctly, referred to as **the bottleneck problem**. Existing approaches have relied on complex algorithms or direct modification to most synaptic weights in hardware-implemented SNNs, limiting their practicality in neuromorphic devices. This paper proposes a simple yet effective input control mechanism to address the problem, grounded in a thorough motivation study. Our mechanism divides the input samples into small fragments, following the best fragmentation strategy, derived by analyzing the characteristics of the input samples and diagnosing the current influence of faults. Experimental results demonstrate that our mechanism significantly enhances fault tolerance over existing methods, achieving these gains without complex algorithms or direct weight modification in various SNN models. Additionally, our mechanism improves the fault tolerance of SNN models with actual hardware devices.", "tldr": "", "keywords": ["SNNs", "hardware faults", "bottleneck problem", "practicality", "small fragments"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c4aa1be1d953a7d8759968de2ac27cd3b032a87.pdf", "supplementary_material": "/attachment/18965a5547de99776bc03552d4a2d528cd3bfd67.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposed an input control mechanism to improve the fault tolerance of SNNs. It is claimed that the proposed method is also beneficial to SNNs implemented on FPGA devices."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation study section (section 4) contains an analysis of different aspects."}, "weaknesses": {"value": "1. The paper should clearly establish at the outset that the research focuses on hardware-implemented SNNs and their fault tolerance.\n\n2. The section title “3.2 Mechanisms to Improve the Fault Tolerance of SNNs” is misleading, as SNNs themselves do not suffer the described faults and therefore do not require such fault tolerance. Moreover, the section discusses neuromorphic hardware fault-tolerance research, which does not fit well under this section title.\n\n3. Section 5: The research appears to focus on on-chip SNN learning. This should be stated explicitly earlier in the paper, before detailing the methods.\n\n4. Section 5: It is unclear how the three subsections work together to form the proposed mechanism. A brief summary at the beginning of this section would be helpful.\n\n5. Abstract and Introduction: FPGA is mentioned only four times in the main text, without explaining how the method works on FPGAs or what benefits the proposed method provides to FPGA. Since the main paper does not include any FPGA-relevant analysis, it should not be presented as a primary contribution in the Abstract as well as the Introduction."}, "questions": {"value": "See weakness for questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PmsY6vK5LS", "forum": "zfkINmO7WZ", "replyto": "zfkINmO7WZ", "signatures": ["ICLR.cc/2026/Conference/Submission12869/Reviewer_EvpD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12869/Reviewer_EvpD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841333603, "cdate": 1761841333603, "tmdate": 1762923659415, "mdate": 1762923659415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We would like to thank the AC and Reviewers for their work regarding our submission. In our rebuttal, we have extracted the Reviewer's comments and addressed them subsequently. For ease of reading, the revised manuscript has the changed sections typeset in blue."}}, "id": "TTdMSfxKLt", "forum": "zfkINmO7WZ", "replyto": "zfkINmO7WZ", "signatures": ["ICLR.cc/2026/Conference/Submission12869/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12869/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12869/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762950536711, "cdate": 1762950536711, "tmdate": 1762960183720, "mdate": 1762960183720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed an input control mechanism to improve the fault tolerance of SNNs. It is claimed that the proposed method is also beneficial to SNNs implemented on FPGA devices."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation study section (section 4) contains an analysis of different aspects."}, "weaknesses": {"value": "1. The paper should clearly establish at the outset that the research focuses on hardware-implemented SNNs and their fault tolerance.\n\n2. The section title “3.2 Mechanisms to Improve the Fault Tolerance of SNNs” is misleading, as SNNs themselves do not suffer the described faults and therefore do not require such fault tolerance. Moreover, the section discusses neuromorphic hardware fault-tolerance research, which does not fit well under this section title.\n\n3. Section 5: The research appears to focus on on-chip SNN learning. This should be stated explicitly earlier in the paper, before detailing the methods.\n\n4. Section 5: It is unclear how the three subsections work together to form the proposed mechanism. A brief summary at the beginning of this section would be helpful.\n\n5. Abstract and Introduction: FPGA is mentioned only four times in the main text, without explaining how the method works on FPGAs or what benefits the proposed method provides to FPGA. Since the main paper does not include any FPGA-relevant analysis, it should not be presented as a primary contribution in the Abstract as well as the Introduction."}, "questions": {"value": "See weakness for questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PmsY6vK5LS", "forum": "zfkINmO7WZ", "replyto": "zfkINmO7WZ", "signatures": ["ICLR.cc/2026/Conference/Submission12869/Reviewer_EvpD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12869/Reviewer_EvpD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841333603, "cdate": 1761841333603, "tmdate": 1763057391390, "mdate": 1763057391390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We would like to thank the AC and Reviewers for their work regarding our submission. In our rebuttal, we have extracted the Reviewer's comments and addressed them subsequently. For ease of reading, the revised manuscript has the changed sections typeset in blue. We summarize the rebuttal for each reviewer in the following paragraphs.\n\n- **E666**: The reviewer points out the presentation quality of the explanations in the manuscript. To address the reviewer's comments, we have written detailed explanations about our main idea, the fault models, and the equations of Section 5 in the revised manuscript and rebuttal.\n\n- **Nuth**: \n\n- **Q1C8**: \n\n- **EvpD**: The reviewer raises concerns about the presentation of the paper's scope, explanation of the proposed mechanism, and the FPGA explanation method. To address the reviewer's comments, we have written a clarification of our paper's scope, a brief overview of the proposed mechanism, and a thorough explanation of our FPGA explanation method in the revised manuscript and rebuttal.\n\nAgain, we appreciate the efforts of the AC and reviewers on our paper. Please read our rebuttal and revised manuscript and determine whether we have addressed the reviewers' comments well."}}, "id": "TTdMSfxKLt", "forum": "zfkINmO7WZ", "replyto": "zfkINmO7WZ", "signatures": ["ICLR.cc/2026/Conference/Submission12869/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12869/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12869/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762950536711, "cdate": 1762950536711, "tmdate": 1763084257527, "mdate": 1763084257527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We would like to thank the AC and Reviewers for their work regarding our submission. In our rebuttal, we have extracted the Reviewer's comments and addressed them subsequently. For ease of reading, the revised manuscript has the changed sections typeset in blue. We summarize the rebuttal for each reviewer in the following paragraphs.\n\n- **E666**: The reviewer points out the presentation quality of the explanations in the manuscript. To address the reviewer's comments, we have written detailed explanations about our main idea, the fault models, and the equations of Section 5 in the revised manuscript and rebuttal.\n\n- **Nuth**: The reviewer demands a thorough discussion that compares the proposed mechanism to existing methods to improve fault tolerance of hardware-implemented SNNs. To address the reviewer’s comments, we have discussed the advantages and limitations of the proposed mechanism compared to those of other methods in hardware environments.\n\n- **Q1C8**: \n\n- **EvpD**: The reviewer raises concerns about the presentation of the paper's scope, explanation of the proposed mechanism, and the FPGA explanation method. To address the reviewer's comments, we have written a clarification of our paper's scope, a brief overview of the proposed mechanism, and a thorough explanation of our FPGA explanation method in the revised manuscript and rebuttal.\n\nAgain, we appreciate the efforts of the AC and reviewers on our paper. Please read our rebuttal and revised manuscript and determine whether we have addressed the reviewers' comments well."}}, "id": "TTdMSfxKLt", "forum": "zfkINmO7WZ", "replyto": "zfkINmO7WZ", "signatures": ["ICLR.cc/2026/Conference/Submission12869/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12869/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12869/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762950536711, "cdate": 1762950536711, "tmdate": 1763141324381, "mdate": 1763141324381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We would like to thank the AC and Reviewers for their work regarding our submission. In our rebuttal, we have extracted the Reviewer's comments and addressed them thoroughly. For ease of reading, the revised manuscript has the changed sections typeset in blue. We summarize the rebuttal for each reviewer in the following paragraphs.\n\n- **E666**: The reviewer points out the presentation quality of the explanations in the manuscript. To address the reviewer's comments, we have written detailed explanations about our main idea, the fault models, and the equations of Section 5 in the revised manuscript and rebuttal.\n\n- **Nuth**: The reviewer demands a thorough discussion that compares the proposed mechanism to existing methods to improve fault tolerance of hardware-implemented SNNs. To address the reviewer’s comments, we have discussed the advantages and limitations of the proposed mechanism compared to those of other methods in hardware environments.\n\n- **Q1C8**: \n\n- **EvpD**: The reviewer raises concerns about the presentation of the paper's scope, explanation of the proposed mechanism, and the FPGA explanation method. To address the reviewer's comments, we have written a clarification of our paper's scope, a brief overview of the proposed mechanism, and a thorough explanation of our FPGA-based experiments in the revised manuscript and rebuttal.\n\nAgain, we appreciate the efforts of the AC and reviewers on our paper. Please read our rebuttal and revised manuscript and determine whether we have addressed the reviewers' comments well."}}, "id": "TTdMSfxKLt", "forum": "zfkINmO7WZ", "replyto": "zfkINmO7WZ", "signatures": ["ICLR.cc/2026/Conference/Submission12869/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12869/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12869/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762950536711, "cdate": 1762950536711, "tmdate": 1763202389162, "mdate": 1763202389162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We would like to thank the AC and Reviewers for their work regarding our submission. In our rebuttal, we have extracted the Reviewer's comments and addressed them thoroughly. For ease of reading, the revised manuscript has the changed sections typeset in blue. We summarize the rebuttal for each reviewer in the following paragraphs.\n\n- **E666**: The reviewer points out the presentation quality of the explanations in the manuscript. To address the reviewer's comments, we have written detailed explanations about our main idea, the fault models, and the equations of Section 5 in the revised manuscript and rebuttal.\n\n- **Nuth**: The reviewer demands a thorough discussion that compares the proposed mechanism to existing methods to improve fault tolerance of hardware-implemented SNNs. To address the reviewer’s comments, we have discussed the advantages and limitations of the proposed mechanism compared to those of other methods in hardware environments.\n\n- **Q1C8**: The reviewer comments on the three important points: the environmental assumption of hardware learning, clarity of the FPGA-based experimental settings, and application to other various settings (large SNN models with FPGA, 1D datasets, and unsupervised learning SNNs). To address the reviewer’s comments, we have clarified the assumption of our on-chip learning environment, explained the FPGA settings in detail, and conducted additional experiments with large models based on the FPGA, AudioMNIST, and Diehl&Cook2015 SNN architecture.\n\n- **EvpD**: The reviewer raises concerns about the presentation of the paper's scope, explanation of the proposed mechanism, and the FPGA explanation method. To address the reviewer's comments, we have written a clarification of our paper's scope, a brief overview of the proposed mechanism, and a thorough explanation of our FPGA-based experiments in the revised manuscript and rebuttal.\n\nAgain, we appreciate the efforts of the AC and reviewers on our paper. Please read our rebuttal and revised manuscript and determine whether we have addressed the reviewers' comments well."}}, "id": "TTdMSfxKLt", "forum": "zfkINmO7WZ", "replyto": "zfkINmO7WZ", "signatures": ["ICLR.cc/2026/Conference/Submission12869/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12869/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12869/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762950536711, "cdate": 1762950536711, "tmdate": 1763314341374, "mdate": 1763314341374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies fault tolerance in SNNs, which are prone to performance degradation from faults like Stuck-At-Faults (SAFs) in synaptic weights. The authors identify a bottleneck problem: faults cause pre-activation values to drift outside the surrogate gradient corridor, leading to vanishing gradients and a severe reduction in the network's usable learning capacity.\n\nTo solve this, they propose \n\ni) a novel and simple mechanism inspired by flow control in computer networks. Instead of modifying the SNN's weights or architecture, their method controls the input. The core idea is to fragment input images into smaller pieces based on a sensitivity score that combines image complexity (edges, texture) and fault influence.\nii) They find the optimal cutting angle by minimizing the Gini coefficient of the 1D projection of this score, ensuring each fragment has a balanced information load. These fragments are then fed sequentially to the SNN, and the outputs are aggregated using an entropy-based weighting scheme.\niii) The proposed method is evaluated extensively on various SNN models (MLP, VGG-7/11/15, ResNet-18/34) and datasets (MNIST, FMNIST, CIFAR-10/100, Tiny-ImageNet, UCI-HAR) under different fault types (SAFs, RWFs, CEFs), the mechanism yields the highest accuracy at a given fault ratio versus benchmarks (ECOC, SoftSNN, Routing, Astrocyte, FalVolt, LIFA)."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The authors have touched each dimension of originality, quality, clarity, and significance.\n\nOriginality: The authors focus on input fragmentation plus fault-influence guidance over weight-level scanning, explicitly targeting the surrogate-gradient corridor to prevent gradient bottlenecks. The approach of using input fragmentation controlled by a Gini-optimized strategy is novel and also provides a theoretical analysis (on corridor occupancy/gradient attenuation and capacity thresholds) that explains failure modes under SAF/RWF/CEF and why the mechanism helps. The analogy to network flow control is creative and provides a strong, intuitive foundation.\n\nQuality: The authors do not show just an empirical demonstration but support a thorough motivation study that meticulously shows how faults lead to pre-activation drift and gradient collapse. The appendices provide a rigorous mathematical framework for both the problem and the near-optimality of their solution. The experimental evaluation is extensive, covering multiple models, datasets, fault types, time steps, ablation studies, hyperparameter sensitivity, and even a comparison with DNNs and a real FPGA implementation.\n\nClarity: The paper is well-written and structured. The problem is clearly motivated, the mechanism is explained step-by-step with the help of key points, and the figures and tables effectively support the claims. The use of a simple, high-level analogy (flow control) makes the complex underlying concept more accessible.\n\nSignificance: The proposed mechanism directly targets a critical and common limitation in deployed SNNs, especially for resource-constrained or neuromorphic platforms. Its low complexity and implementation compatibility make it highly relevant for practitioners and researchers seeking robust edge AI solutions."}, "weaknesses": {"value": "1. The explanation of the \"bottleneck problem\" in the paper (at line 60) lacks conceptual clarity and mixes two different learning regimes. The authors first state that \"when faults appear in SNNs' synapses, the weights of the faulty synapses become fixed during training,\" implying that training is happening on-chip, where hardware faults would indeed interfere with plasticity and learning. However, the next line attributes the capacity degradation to surrogate gradient vanishing due to abnormal pre-activation values, this is clearly a reference to offline training using backpropagation through time (BPTT) and surrogate gradients, as implemented in frameworks like snnTorch or SpikingJelly.\n\nThis conflation is problematic. In most practical settings, SNNs are trained offline on fault-free software platforms, and then deployed on neuromorphic hardware. If faults arise, they typically occur after training, during deployment, due to physical issues such as resistance drift, electromigration, peripheral CMOS aging, or read-disturb effects. Thus, during offline BPTT training, the weights are unaffected by hardware faults. On the other hand, if the authors intend to analyze on-chip learning, then the appropriate learning rule would be local, online methods like STDP, not surrogate-gradient-based BPTT. In that case, the \"gradient vanishing\" explanation does not apply.\n\nIn summary, it is implying a vague explanation and needs clarification for accurately motivating the problem and for aligning the theoretical analysis with real-world neuromorphic deployment.\n\n\n2.) The paper focuses solely on synaptic faults (e.g., stuck-at, random-weight), but this overlooks other critical fault modes, especially given that the model is deployed on FPGA hardware. In digital neuromorphic systems, faults can also arise in core arithmetic components such as adders, multipliers, counters, and comparators, which directly impact spiking neuron-level computations. Prior works \nhttps://ieeexplore.ieee.org/abstract/document/10658724\nhttps://ieeexplore.ieee.org/abstract/document/10858960\n\nhave shown that such logic-level faults can significantly degrade SNN performance. A broader fault model or at least a discussion acknowledging these hardware-level vulnerabilities would strengthen the paper's scope and relevance for real-world neuromorphic deployment.\n\n3.) The Section 4. Motivation Study frames the impact of synaptic faults entirely from the perspective of offline software-based BPTT training, which is not representative of how SNNs operate on neuromorphic hardware. In practice, BPTT cannot be used on neuromorphic devices, only online, local learning rules such as STDP are hardware-compatible. Prior works (e.g., Vatajelu et al., 2019; Lee & Lim, 2023) have correctly modeled faults within unsupervised, on-chip STDP-based learning, which reflects real-world behavior. Without grounding the fault impact in such realistic learning settings, the motivation for the proposed input-fragmentation mechanism remains speculative. A more appropriate justification would consider how faults affect STDP-based learning dynamics or inference-time reliability post offline training.\n\n4.) While the authors provide code as supplementary material, there is no accompanying README or documentation explaining how to execute it. This makes it difficult to reproduce the experiments or understand the workflow. Additionally, although the paper reports FPGA-based results, the supplementary materials contain only Python (.py) files, with no Verilog: hardware-specific code required for actual FPGA deployment. Including these files or at least providing a pointer to the hardware implementation would significantly enhance the reproducibility and credibility of the hardware claims.\n\n5.) In Appendix A.7 (line 1296), the authors mention FPGA evaluation using only an MLP model. However, other models used in the paper, such as VGG-7/11/15 and ResNet-18/34 are not included in the hardware experiments. It remains unclear why these deeper architectures were omitted, especially since they form a core part of the software evaluation. Including them would provide a more complete and realistic assessment of the proposed method's hardware applicability.\n\n6.) Hardware results show the pattern but the device/precision configuration dominates performance. A clearer breakdown of numeric formats, bit-width per layer, and resource utilization vs accuracy would strengthen reproducibility claims for hardware.\n\n7.) The proposed approach is heavily designed towards 2D images. How would the approach generalize it to other data modalities, such as audio (1D time series) or text? The paper's significance would be greatly amplified if the core idea could be shown to be more broadly applicable.\n\n8.) Though the method is optimized using sensitivity metrics and Gini coefficients, practical constraints (batch effects, alignment in hardware) may limit its generality. Further discussion of trade-offs in real-world settings (e.g., latency, fragment count vs. accuracy) would be valuable.\n\n9.) Is there potential to dynamically rather than statically per batch adapt the fragmentation strategy during training as fault characteristics evolve, and how might this affect convergence and hardware cost?\n\n10.) The method focuses on gradient-based supervised SNN training. Would similar fragmentation principles benefit unsupervised SNNs?"}, "questions": {"value": "I would request authors to answer all points that are raised in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Woi4oHuJlw", "forum": "zfkINmO7WZ", "replyto": "zfkINmO7WZ", "signatures": ["ICLR.cc/2026/Conference/Submission12869/Reviewer_Q1C8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12869/Reviewer_Q1C8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858781315, "cdate": 1761858781315, "tmdate": 1762923658937, "mdate": 1762923658937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a method for fault tolerance to stuck-at and synaptic faults in spiking neural networks running on neuromorphic hardware using flow control methods based on sensitivity analysis of neurons. The system uses the Gini coefficient to analyse sensitivity and route inputs past problematic nodes that cause gradient collapse due to anomalously large spiking values. This is tested against some other neuromorphic approaches using the Leaky Integrate and Fire method. Overall, the paper presents this as a highly theoretically grounded approach and draws on behavioral models of faults in neuromorphic hardware and tests this in an FPGA."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper's theoretical section is very solid, in particular the appendix proof that provides theoretical guarantees of the system's optimality. Similarly, it presents the use of flow control as a very intuitive approach, comparing it to flow control in computer networks - this makes a complex idea and approach seem much more digestible. I believe that this combination of complex, thorough theory and solid explanation by analogy is what makes the presentation of this paper a very sound addition to the conference provided its weaknesses are addressed.\n\nIts hardware based experimental section is also very good - the use of real hardware for fault tolerance analysis is a mark of good experimentation."}, "weaknesses": {"value": "I am worried about the paper's experimental section, in particular its contrast to prior art in the hardware space that has modelled Leaky Integrate and Fire neurons in non-FPGA formats. The FPGA is not the only cutting-edge accelerator hardware being examined in the field, and resilience approaches in prior art have also looked at:\n\n1) Setting anomalous values to zero based on neuron output statistics in an inference or training episode [1] or using DropOut [2]. This seems a very lightweight approach, as opposed to requirements of routing and flow control that impose interconnect and communication overheads - manageable in an FPGA but may be harder in analog, compute-in-memory or GPU substrates. How does your approach contrast to this? I would like to see a discussion of that.\n\n2) Persistent faults in the network - which are ideally addressed using flow control - have been addressed using testing based approaches (online and offline self-test in [2]; and a signature-based compact test strategy in [3]) which has the benefit of amortizing test overhead over number of inferences, rather than being an always-on strategy. It would be good to show how the online approaches examined here, applied as they are to persistent faults, contrast with test strategies that may be amortizable over the training process.\n\n[1] A. Saha, C. Amarnath and A. Chatterjee, \"A Resilience Framework for Synapse Weight Errors and Firing Threshold Perturbations in RRAM Spiking Neural Networks,\" 2023 IEEE European Test Symposium (ETS), Venezia, Italy, 2023, pp. 1-4, doi: 10.1109/ETS56758.2023.1017422\n\n[2] T. Spyrou, S. A. El-Sayed, E. Afacan, L. A. Camuñas-Mesa, B. Linares-Barranco and H. -G. Stratigopoulos, \"Neuron Fault Tolerance in Spiking Neural Networks,\" 2021 Design, Automation & Test in Europe Conference & Exhibition (DATE), Grenoble, France, 2021, pp. 743-748, doi: 10.23919/DATE51398.2021.9474081.\n\n[3] A. Saha, C. Amarnath, K. Ma and A. Chatterjee, \"Signature Driven Post-Manufacture Testing and Tuning of RRAM Spiking Neural Networks for Yield Recovery,\" 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC), Incheon, Korea, Republic of, 2024, pp. 740-745, doi: 10.1109/ASP-DAC58780.2024.10473874."}, "questions": {"value": "1) Could the authors provide a discussion w.r.t. prior art in the hardware space? A few papers are cited above, but the approaches involving forward-pass resilience seem to be lower overhead than flow-control approaches, especially when applied to non-FPGA hardware.\n\n2) Could the authors provide a short discussion contrasting these on-line approaches with offline or online periodic self-test and repair systems in hardware that would allow for resilience overhead to be amortized over a larger number of computations, at the cost of potentially allowing some faults through? What are the pros and cons, and the application domains, of each approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3kGK7qmSYW", "forum": "zfkINmO7WZ", "replyto": "zfkINmO7WZ", "signatures": ["ICLR.cc/2026/Conference/Submission12869/Reviewer_Nuth"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12869/Reviewer_Nuth"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880127231, "cdate": 1761880127231, "tmdate": 1762923658561, "mdate": 1762923658561, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed an input fragmentation mechanism inspired by flow control in computer networks. It tackles the important issue of fault tolerance in Spiking Neural Networks (SNNs)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The main strength of this paper is that they have done various experiments on several models."}, "weaknesses": {"value": "The main weakness is that the idea presentation could be clearer.\n\nThere could be more information for SAFs and RWFs since they are important characteristics in fault tolerant research.\n\nLine 167-170, this paragraph has a lot of parameters with neither explanation nor citations.\n\nEquations (1)–(4) are very complicated, which needs a more sufficient explanation or derivative. If they are not completely proposed by the authors, some citations would be better.\n\nDatasets could be used with citations."}, "questions": {"value": "What is the Gini coefficient? If it is proposed by the authors, could they add some introductions for this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "H0KEn1C3vV", "forum": "zfkINmO7WZ", "replyto": "zfkINmO7WZ", "signatures": ["ICLR.cc/2026/Conference/Submission12869/Reviewer_E666"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12869/Reviewer_E666"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762567965116, "cdate": 1762567965116, "tmdate": 1762923658368, "mdate": 1762923658368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}