{"id": "b9xDou7uAX", "number": 4286, "cdate": 1757655565805, "mdate": 1759898041652, "content": {"title": "Predict Overcharging: Auditing LLM APIs via Reasoning Length Estimation", "abstract": "Commercial LLM services often conceal internal reasoning traces while still charging users for every generated token, including those from hidden intermediate steps, raising concerns of token inflation and potential overbilling. This gap underscores the urgent need for reliable token auditing, yet achieving it is far from straightforward: cryptographic verification (e.g., hash-based signature) offers little assurance when providers control the entire execution pipeline, while user-side prediction struggles with the inherent variance of reasoning LLMs, where token usage fluctuates across domains and prompt styles.\nTo bridge this gap, we present PALACE (Predictive Auditing of LLM APIs via Reasoning Token Count Estimation), a user-side framework that estimates hidden reasoning token counts from prompt–answer pairs without access to internal traces. PALACE introduces a GRPO-augmented adaptation module with a lightweight domain router, enabling dynamic calibration across diverse reasoning tasks and mitigating variance in token usage patterns. Experiments on math, coding, medical, and general reasoning benchmarks show that PALACE achieves low relative error and strong prediction accuracy, supporting both fine-grained cost auditing and inflation detection. Taken together, PALACE represents an important first step toward standardized predictive auditing, offering a practical path to greater transparency, accountability, and user trust.", "tldr": "", "keywords": ["LLM Auditing", "Reasoning LLM", "LLM API"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8f349d8ce0c63aefad70debee98edd9c13ab13da.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes to train a token count prediction model for auditing LLM APIs with hidden reasoning tokens. The goal is to detect potential overcharging by commercial opaque LLM services. The proposed method, PALACE, involves training a predictive model with supervised finetuning on pre-collected (prompt, output, token count) data and with reinforcement learning to adapt to specific domains, where the domain-specific adapters are selected with a lightweight router. The authors find PALACE outperforms several baselines and constitutes a better method for auditing LLM API token usage."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem they are tackling is important given the widespread usage of COLS. The methodology is simple and clear, directly addressing the task of predicting token counts with problem texts and observed outputs. The proposed method is empirically validated and outperforms baselines across benchmarks."}, "weaknesses": {"value": "## Major weaknesses\n\n**Lack of both theoretical, empirical, and intuitive justification of generalizability**\n- The paper lacks a clear explanation of *why* reasoning length should be possible to be predicated from prompt-answer pairs. The experiments show that it is possible to fit the prediction model to specific domains, but it is unclear whether the method can generalize. It seems to me that the method would require a substantial amount of data in any new domain to be able to predict accurately on them.\n- To make the method practically useful, it needs to be able to generalize OOD to unseen user prompts. For example, can a prediction model trained on code prompt-answers generalize to math and medical problems? Performing leave-one-domain-out experiments and also cross-provider evaluation can show how well the method generalizes.\n\n**Heavy reliance on auxiliary datasets from API providers**\n- The method relies heavily on provider-released public datasets, but it is unclear how large these datasets should be for the method to work on different domains. If each individual domain requires a substantial amount of data, it is unlikely that this method would be used in practice.\n- To better understand this dependence, the authors can analyze accuracy vs. number of training samples across models. Intuitively, RL should require over 10K data points to work, while API providers might not be motivated enough to provide that much data for each domain.\n\n**The proposed method is not robust to simple adversarial attacks**\n- A malicious API provider could train the same model and adjust billing to stay within expected prediction bounds, while claiming token usage higher than the actual count. I suspect that the PALACE breaks down completely under this strategic provider.\n\n**Problematic evaluation metrics and minimal improvements**\n- Definition of accuracy is problematic. The paper counts a prediction \"accurate\" if it is within 33% of ground truth. This choice risks normalizing large billing discrepancies. \n- Looking at Table 2, the AVG error differences between PALACE and the LoRA baseline on Qwen2.5-3B are often <10 tokens. Even for SOTA models like GPT-5-pro (\\\\$120/1M tokens), the per-query dollar difference for 10 tokens is only \\\\$0.0012, which seems economically tiny. This raises questions about whether additional complexity and cost (e.g., training and running the CoT predictive model and the routing model) in PALACE is necessary.\n\n## Minor issues\n\nSection 3\n- There should be explicit definition of what data samples look like in datasets. For example, I assume they are (prompt, solution, reasoning token count) tuples? Concrete definition should be included in this section.\n- Up to this section, there is no discussion on why GRPO is involved in the auditing process / training process of the auditing model. It's a bit hard to understand what the GRPO auditing dataset is used for without reading the later methodology section first.\n\nSection 4\n- Line 256: Why is ground truth different from length from auxiliary data? Also $\\ell_{\\text{reason}}(x)$ is defined but not used."}, "questions": {"value": "Main questions are listed below. Some additional questions / problems are discussed in the weaknesses section. \n1. What number of training samples per domain is needed to reach $\\leq X \\\\%$ AVG error?\n2. Could you include learning curves and leave‑one‑domain‑out evaluations to show whether the model generalizes?\n3. Is there any theoretical justification for why it is possible to train a generalizable token number predictor?\n4. Could you discuss auditor overhead brought by your method? For example, what is the prediction model’s average token usage per query under your final deployed prompt? Could you report cost ratios like (auditor tokens / suspected overcharge)?\n5. If the provider adjusts reported counts using a similar prediction model, can you come up with any defense strategy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sLlFZP4s3S", "forum": "b9xDou7uAX", "replyto": "b9xDou7uAX", "signatures": ["ICLR.cc/2026/Conference/Submission4286/Reviewer_RsSN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4286/Reviewer_RsSN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761511970293, "cdate": 1761511970293, "tmdate": 1762917277748, "mdate": 1762917277748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of hidden reasoning tokens in commercial LLM services that are not shown to the user but still billed, raising the risk of token overcharging. The authors propose PALACE, a user-side auditing framework that estimates the number of hidden reasoning tokens from just the user’s prompt and the final answer, without access to the model’s internal chain-of-thought. PALACE works by first fine-tuning a base language model on general QA data, then adapting it to specific domains using GRPO to better predict token counts for complex reasoning tasks. It also trains a lightweight domain router that classifies each query’s domain and selects the appropriate domain-specific adapted module. During auditing, for each prompt-answer pair, PALACE predicts the hidden reasoning length and flags any large discrepancy between the prediction and the provider’s reported token usage. Experiments across four reasoning-heavy domains show that PALACE achieves low relative error in token count estimation and high accuracy under defined thresholds. This enables fine-grained cost auditing and reliable detection of any token count inflation, representing a first step toward greater transparency in LLM API billing. The key results include strong per-sample accuracy and minimal cumulative bias in predicted token totals compared to ground truth across domains."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Novel Problem: The paper identifies an important and previously under-explored transparency issue in LLM APIs and introduces the first predictive auditing framework to address it. The PALACE approach is innovative in that it requires no access to the model’s internal reasoning trace, instead leveraging a learned model to infer token usage from observable input-output semantics.\n\nTechnical Innovation: PALACE’s design is technically sound and creative. It combines reinforcement learning for fine-grained regression of token counts with domain-specific adapters and a routing mechanism. This modular approach effectively handles the high variance in reasoning lengths across different tasks and domains. The inclusion of GRPO is a strength as it allows the model to be trained with a reward that directly encourages accurate token count predictions within a relative error threshold\n\nStrong Empirical Results: The experimental evaluation is thorough and shows clear advantages of PALACE. On math, coding, medical, and general reasoning benchmarks, PALACE consistently outperforms baseline methods in both accuracy and error measures. For instance, PALACE achieves higher Pass@1 accuracy than baselines for all tested domains and model sizes. It also maintains low average error and low aggregated error in predicted token counts, unlike simpler methods that have large per-sample errors or drift in totals."}, "weaknesses": {"value": "Dependence on Provider Data: A practical concern is that PALACE requires auxiliary training data from the provider to calibrate the auditor for each model/domain. The framework assumes the COLS provider will release a small dataset with examples of prompts, their hidden reasoning traces, and answers. If a provider is uncooperative or if such data is unavailable, users must rely on synthetic data or open-source approximations, which may not perfectly reflect the proprietary model’s behavior. This reliance could limit PALACE’s real-world deployment unless industry standards emerge for providers to supply verifiable audit data. The authors do highlight this need and position it as fulfilling the provider’s obligation, but it remains a potential obstacle to practicality.\n\nComputational Overhead: The PALACE auditing model is non-trivial to train and deploy. It involves fine-tuning a large language model with RL and maintaining multiple domain-specific adapter modules. The largest dataset had over 22 million synthetic examples, and GRPO training requires generating multiple rollouts per query for reward optimization. While the authors use manageable model sizes and mention using LoRA adapters to keep things lightweight, the approach still demands significant compute resources for training. This could hinder adoption by end-users or independent auditors without substantial resources. The paper could discuss more about the inference-time efficiency: running an extra LLM forward pass for each audit is necessary – which might be acceptable, but if the model is large, auditing many queries could be costly.\n\nGenerality to New Domains or Models: PALACE’s modular approach handles the four tested domains well, but it’s unclear how it generalizes to entirely new domains or to rapidly evolving model families. If a query doesn’t cleanly fall into one of the predefined domains, the router might misclassify it, affecting accuracy. The authors trained on specific domains, new reasoning domains would require additional adapter training. Similarly, adapting to a new COLS model might necessitate obtaining new calibration data and performing GRPO fine-tuning for that model. This could be time-consuming whenever providers update their models. In short, the solution might need continuous maintenance to remain effective.\n\nReal Provider Evaluation: The experiments appear to be conducted on simulated data and open-source models where the ground-truth hidden token counts are obtainable. There is no direct test on an actual closed-source APIs, presumably because their hidden traces are not accessible. As a result, the evaluation does not demonstrate an end-to-end audit of a real commercial service’s billing. This is understandable, but it leaves a gap: the paper shows feasibility in a controlled setting, but real-world effectiveness may depend on how closely the surrogate data matches the provider’s true behavior. The authors could strengthen the work by discussing how an auditor might validate their predictions on a live API, perhaps by cross-checking cost on queries where one forces the model to reveal reasoning tokens by special API settings."}, "questions": {"value": "To increase PALACE’s real-world impact, the authors might consider strategies to obtain or generate the necessary calibration data without full provider cooperation. For instance, could a user or auditor query a COLS with specially crafted prompts to indirectly estimate hidden token lengths? Currently, the method assumes a willing provider releasing data. Exploring semi-automated data collection or adaptation to unseen providers would make the approach more practical. Perhaps active learning could be used by querying the API and adjusting the auditor on-the-fly if the API’s behavior drifts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ClRER0AlI6", "forum": "b9xDou7uAX", "replyto": "b9xDou7uAX", "signatures": ["ICLR.cc/2026/Conference/Submission4286/Reviewer_mR1Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4286/Reviewer_mR1Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761513706536, "cdate": 1761513706536, "tmdate": 1762917277305, "mdate": 1762917277305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Although commercial opaque LLM Services do not show internal reasoning traces, users still pay for every generated token, which raises concerns of token inflation and potential over-billing. In light of this, the authors introduce PALACE (Predictive Auditing of LLM APIs via Reasoning Token Count Estimation), a user-side framework that estimates hidden reasoning token counts from prompt–answer pairs without access to internal traces. They demonstrates the efficacy of PALACE in multiple domains such as math, coding, medical, and general reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Estimating hidden reasoning token counts from prompt–answer pairs without access to internal traces seems an interesting and new idea.\n2. PALACE shows the potential for an important first step toward standardized predictive auditing.\n3. It is well-written and easy-to-follow."}, "weaknesses": {"value": "1. It is unclear why PALACE can work well. Can the authors provide more detailed analysis (e.g., a theoretical analysis)?\n2. All the experiments are done with 1B or 3B-sized LLMs. It would be more beneficial if the authors conduct experiments for 8B-sized LLMs such as Llama 3.1 8B or Qwen3 8B.\n3. In Table 2, PALACE usually outperforms LoRA for Pass@1, but underperforms for Pass@5. It is hard to understand why such a phenomenon happens. Can the authors explain this in detail?"}, "questions": {"value": "1. When reporting Pass@5, the authors used a temperature of 0.8, but the recommended temperature is different depending on the chosen model. Is there any reason why the authors used the same temperature for different models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nV0mlEXFjp", "forum": "b9xDou7uAX", "replyto": "b9xDou7uAX", "signatures": ["ICLR.cc/2026/Conference/Submission4286/Reviewer_6xRC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4286/Reviewer_6xRC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735117528, "cdate": 1761735117528, "tmdate": 1762917274945, "mdate": 1762917274945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Nowadays, many commercial opaque LLM services (COLS) need hidden reasoning states which do not reveal to users but could cause additional charge. The authors argue that COLS can lead to potential overcharging, and a user-side auditing framework estimating hidden token usage is necessary. They introduce a GRPO-augmented model combined with a lightweight domain router to predict the number of hidden reasoning tokens. Experiments on several synthetic benchmarks (math, coding, medical, and general reasoning) demonstrate that PALACE achieves lower relative error and higher accuracy compared to baselines such as CoIn, LoRA fine-tuning, and MLP regression. The paper positions predictive reasoning-length estimation as a practical step toward transparent LLM billing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "[S1] Addresses an important and underexplored issue: auditing and transparency of commercial LLM billing.\n\n[S2] The paper is clearly and well written."}, "weaknesses": {"value": "[W1] Lack of generalization. All datasets and reasoning traces come from the DeepSeek family, with no evidence that PALACE generalizes to other LLM families (e.g., OpenAI, Anthropic, Gemini). Without such validation, it is unclear whether PALACE can audit COLS in practice.\n\n[W2] No cross-model or out-of-domain (OOD) robustness evaluation. The paper does not study whether PALACE trained on one reasoning model can handle prompts or outputs from a different one. If not, each new COLS model, or even each model update, would require retraining from scratch, which severely limits practical deployability.\n\n[W3] Evaluation methodology is weak. The 33% relative error threshold for correctness is arbitrary and not rigorous. Metrics such as Average and Aggregated Error are only reported for greedy decoding, yet the results appear sensitive to sampling temperature. A more systematic evaluation of variance and confidence calibration is needed.\n\n[W4] Questionable real-world feasibility. Although PALACE is described as a lightweight and user-side framework, it relies on auditor models with 1.5B–3B parameters (e.g., Qwen2.5, LLaMA3.2), which entail substantial computational and memory demands. This raises concerns about the practicality of deploying PALACE for real-time or large-scale auditing, particularly for individual or enterprise users without significant resources. Furthermore, the approach assumes COLS will provide verified reasoning traces to bootstrap PALACE training. This assumption is unrealistic in most commercial contexts, and without such data the framework cannot be initialized."}, "questions": {"value": "- Could PALACE show high generalization to other model families? It would be better to provide evidence that PALACE can work on other model families and COLS. (see W1)\n\n- Does PALACE have cross-model and OOD robustness? (see W2) If PALACE does not show high cross-model and OOD robustness, the system would require auxiliary datasets every time the model changes. Connected to W4, without cross-model and OOD robustness, it would have very low feasibility in real-world scenarios.\n\n-  Why did you choose the 33% relative error threshold? How do the average and aggregated errors change when the sampling temperature varies? Comparing average and aggregated errors under non-greedy decoding would be more informative than comparing only Pass@1. (see W3)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2FkpWFxbqP", "forum": "b9xDou7uAX", "replyto": "b9xDou7uAX", "signatures": ["ICLR.cc/2026/Conference/Submission4286/Reviewer_e5tK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4286/Reviewer_e5tK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880098291, "cdate": 1761880098291, "tmdate": 1762917274535, "mdate": 1762917274535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PALACE, a framework for auditing commercial reasoning LLM APIs by predicting hidden reasoning token usage from visible prompt–answer pairs. Using GRPO-based modules and a router across domains, PALACE estimates token counts without internal access to provider traces and can detect of overcharging in LLM API services. Experiments on across datasets from different domains show improved accuracy and low error compared to multiple baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses an increasingly relevant problem of auditing hidden reasoning token usage in LLM APIs. It also proposes a new evaluation framework in addition to existing work of CoIn.\n\n2. Innovatively combine a domain router which improving prediction accuracy across different reasoning domains.\n\n3. The experimental setup is systematic, including both regression and classification formulations and clear error analyses."}, "weaknesses": {"value": "1. In real commercial API services, inference-time randomness and determinism settings can vary substantially across providers. How would PALACE perform under such conditions, and can you simulate varied inference environments to demonstrate the robustness of its auditing estimates?\n\n2. The method’s reliance on calibrated reference data implies partial access to provider-side information, which reduces its practicality as an user-side auditing framework.\n\n3. The comparison to CoIn remains mostly empirical; there is little discussion of theoretical guarantees or conditions where PALACE might fail."}, "questions": {"value": "1. Could the model-based estimation in PALACE be integrated with CoIn’s verification to combine predictive power with cryptographic reliability?\n\n2. Have you tested whether PALACE’s GRPO-based length predictor generalizes across model families trained with different reasoning styles?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NwCOXJWhPj", "forum": "b9xDou7uAX", "replyto": "b9xDou7uAX", "signatures": ["ICLR.cc/2026/Conference/Submission4286/Reviewer_wSQr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4286/Reviewer_wSQr"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762243120469, "cdate": 1762243120469, "tmdate": 1762917274266, "mdate": 1762917274266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}