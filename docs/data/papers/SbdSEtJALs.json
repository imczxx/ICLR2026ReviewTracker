{"id": "SbdSEtJALs", "number": 11206, "cdate": 1758193315491, "mdate": 1759897601106, "content": {"title": "In-Context Prompt Optimisation for Knowledge Editing: Enhancing Safety and Coherency in Large Language Models", "abstract": "Large Language Models (LLMs) are often fine-tuned or adjusted at a high cost and complexity. This paper presents a model-agnostic, training-free framework that uses prompt-based learning as a control layer for LLMs. We propose three controllers: (i) a Static baseline, (ii) a Dynamic controller using real-time feedback, and (iii) a Reinforcement Learning (RL)-Enhanced Dynamic controller with a Deep Q-Network for prompt actions based on a multi-objective reward system balancing safety and coherence. Tested on four open-source models (Blacksheep-Llama3.2, Evil-Alpaca, DeepSeek-R1, DialoGPT), Dynamic controllers outperform static prompting; the RL-Enhanced version achieves 84% effectiveness and offers optimal safety-coherence balance. The outcome of this work offers a promising methodology for prompt regulation, practical feedback/RL strategies, and demonstrates real-time improvements without retraining models or accessing their internals, providing a practical solution for LLM output regulation.", "tldr": "Using prompt optimization techniques to improve the safety and coherency of generative models without modifying the model weights.", "keywords": ["NLP", "Optimization", "Generative Models", "LLM", "Rudeness Detection", "Reinforcement Learning", "Prompt Optimization", "AI Safety", "Machine Unlearning", "In Context Unlearning", "AI Alignment", "Knowledge Editing"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b9e860010d1eeaae940bb3320d08b8330f5be74f.pdf", "supplementary_material": "/attachment/f48cf73097f87b30bc8d64feac892d33c14839d8.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a prompt optimization framework that aims to balance the safety and quality of LLM outputs. Three variants of the framework are compared: (1) a static baseline, (2) a dynamic method that iteratively updates prompts based on feedback from a rating model, and (3) an RL-enhanced dynamic method that selects strategies for how to process historical prompt data via a Deep Q-Network. Experiments across multiple LLMs and unsafe prompt datasets show that the dynamic method achieves the most consistent improvements in both safety and response quality, while the RL-enhanced method can reach higher peak performance in certain configurations but exhibits higher variance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a heuristically designed dynamic prompt optimization pipeline that utilizes multiple LLMs (as generator and judge) to steer the model’s safety alignment. Experiments on unsafe prompts demonstrate clear gains over the static baseline in both safety and response coherence of the LLM outputs."}, "weaknesses": {"value": "1. **Limited conceptual novelty.** The core architecture largely instantiates a common generator–evaluator (LLM-as-a-judge) iterative loop for safety steering prompt refinement. While the paper explores several controller variants, the pipeline mainly combines existing ingredients rather than introducing a fundamentally new learning principle or algorithmic mechanism. \n2. **Specific, hand-engineered design with limited generality.** The pipeline leans heavily on human crafted choices: a sophisticated reward formulation with hard thresholds, a hand-specified state representation for the DQN, and a curated set of strategies for processing historical prompt data. These design decisions encode substantial prior knowledge about “safety” and “quality,” suggesting manual specification rather than data-driven generalization and potentially requiring retuning for new models or safety taxonomies.\n3. **Missing comparisons to state-of-the-art safety alignment approaches.** Beyond the authors’ own static prompting baselines, the evaluation omits competitive, modern safety methods such as Constitutional AI / self-critique methods (Bai et al., 2022; Madaan et al., 2023), guarded or classifier-in-the-loop decoding (e.g., Llama Guard; Meta-Llama-Team, 2024), or safety-tuned RLHF-style assistants (Bai et al., 2022).. Without head-to-head comparisons under the same protocol, it is difficult to assess the incremental value of the proposed pipeline.\n4. **The benefit of introducing DQN is not clearly justified.** While the RL-enhanced controller achieves the best single score in one configuration, the gains over the simpler dynamic controller are small and inconsistent, and come with higher variance and added complexity, which weakens the argument that RL is a necessary or central part of the contribution."}, "questions": {"value": "1. Experiments in this paper are conducted on relatively weakly safety-aligned or even deliberately uncensored models (e.g., Blacksheep-Llama3.2, Evil-Alpaca, DialoGPT). How would the proposed control layer behave on strongly safety-aligned frontier models (e.g., GPT-5–class or Claude Sonnet–class systems) whose default policies already suppress most unsafe generations? Do you expect the gains to diminish, or would your method still discover meaningful prompt-level improvements?\n2. Section 3.2 mentions that in the dynamic methods, the buffer keeps “the rating score, the user prompt, the system prompt and the model response” and feeds it back to the strategy module, but the paper does not specify what is the maximum history length used in experiments, and does performance drop if the buffer is shorter?\n3. In Section 4.3, 0.9 is set as the stopping threshold for the automatic optimization loop. Was 0.9 tuned on the evaluation set, or chosen heuristically? Did you try other thresholds (e.g., 0.8)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "swMLHzfPmi", "forum": "SbdSEtJALs", "replyto": "SbdSEtJALs", "signatures": ["ICLR.cc/2026/Conference/Submission11206/Reviewer_YSDp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11206/Reviewer_YSDp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898894201, "cdate": 1761898894201, "tmdate": 1762922357110, "mdate": 1762922357110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Problem:** LLMs often need to be updated or controlled for safety/coherence, but fine-tuning weights is expensive and cumbersome.\n\n**Solution:** Introduce a **training-free**, **model-agnostic** framework that uses *prompt optimisation* as a control layer. Three controller variants are proposed:\n* Static controller (fixed prompts)\n* Dynamic controller (prompt adapts via feedback)\n* RL-Enhanced Dynamic controller (uses a Deep Q-Network and multi-objective reward balancing safety & coherence)\n\n**Results:** On multiple open-source models, the dynamic controllers outperform static prompting; the RL-Enhanced variant achieved ~84% effectiveness in balancing safety/coherence.\n\n**Contribution:** Demonstrates that prompt-based regulation can function as a lightweight governance layer for LLMs, enabling real-time behaviour adjustment **without retraining or accessing internal model weights**.\n\n**Key takeaway:**\nIf you denote a prompt strategy as $p$, model output distribution as $M(\\cdot \\mid p, x)$ for input $x$, the paper’s framework treats prompt-selection as a control policy $\\pi(p \\mid \\cdot)$ that maximises a reward\n$$R = \\alpha\\text{Safety}(M(\\cdot)) + \\beta\\text{Coherence}(M(\\cdot))$$\nwithout changing $M$’s parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper’s framework uses prompt-based controllers (Static, Dynamic, RL-Enhanced) that operate without modifying model weights, enabling compatibility with black-box or service-based LLMs. Prior work like for example, Zheng et al. 2023 [A] also uses in-context demonstration without weight updates to edit factual knowledge. However, the current paper extends this by adding adaptive dynamic and RL controllers rather than static demonstration sets, thus providing a more comprehensive control layer. In contrast, methods like EasyEdit [B] provide a unified editing framework (including parameter update methods) but still rely on model changes rather than purely prompt-based controllers.\n* The inclusion of a Dynamic controller (feedback-based) and an RL-Enhanced controller using a Deep Q-Network to select prompt strategies via a multi-objective reward (safety vs coherence) is a strong contribution. Earlier works like the in-context editing of Zheng et al. [A] mostly treat the editing via demonstration examples and do not integrate RL or multi-objective optimisation of prompt-actions. Other frameworks like EasyEdit [B] provide plug-and-play editing but do not emphasise RL-based prompt policy selection. \n* The paper defines a reward combining both safety (e.g., avoiding undesirable content) and coherence (maintaining high-quality output) in the RL controller. This dual objective is important in real-world regulation of LLM behaviour. Earlier in-context editing papers often focus on factual correctness or updating knowledge, e.g., IKE emphasises editing success and minimal side-effects. But few prior works emphasise safety (toxicity / undesirable content) and coherence (quality of output) together as primary objectives of a prompt-controller. Some editing frameworks focus more narrowly on knowledge updates rather than behaviour regulation for safety.\n\n```\n[A] Can We Edit Factual Knowledge by In-Context Learning?, EMNLP 2023\n[B] EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models, ACL 2024\n```"}, "weaknesses": {"value": "* While the RL-enhanced controller adds a policy-selection layer, the core idea of using prompt manipulation for editing/regulation remains closely aligned with the prior ICL-editing literature. Thus the novelty claim could be strengthened by more clearly distinguishing how the dynamic and RL controllers go beyond what prior ICL editing methods deliver (e.g., in terms of multitask behaviour control, safety objectives, adaptation across tasks).\n* The paper reports that the RL-enhanced dynamic controller \"achieves ~84% effectiveness\" but gives limited detail (in the summary) on how this was measured: which datasets/tasks, how safety/coherence metrics were operationalised, and how strong the baselines were.\n* There is no discussion in the summary of how the controller behaves on out-of-distribution prompts, adversarial prompts, or under varying model architectures. Prior editing work emphasises generalisation/locality measures (e.g., Zheng et al. [A] study both generalisation and specificity).\n* The work does not appear to evaluate \"side-effects\" thoroughly i.e., how many unintended or unrelated knowledge behaviours changed when the prompt controller intervened. In previous research on model editing (e.g., [B]) this is a serious concern.\n* The method proposes an RL-based policy (Deep Q-Network) to select prompts. But the overhead (computation, prompt length, latency) of such a system is not clearly discussed. For real-world deployment in black-box LLM services, control layers must be low-cost and low-latency.\n* While the paper introduces dynamic and RL prompt-controllers, it appears not to benchmark against other automatic prompt optimisation methods (e.g., evolutionary prompt search, gradient‐based discrete prompt tuning, or dynamic retrieval/curriculum of demonstrations).\n```\n[A] Can We Edit Factual Knowledge by In-Context Learning?, EMNLP 2023\n[B] Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue, EMNLP 2024\n```"}, "questions": {"value": "* It would be good to provide a comparative table or analysis showing how the proposed method differs, in concrete measurable terms, from IKE [A] and other prompt-based editing works e.g., in terms of: number of tasks supported, safety/coherence trade-off metrics, adaptability across prompts/models, or RL policy improvements. Also, it would be good to explicitly cite additional prompt-control or RL‐prompt optimisation literature (e.g., [B])\n* It might be worthwhile to include more extensive ablations: (a) behaviour under adversarial / unseen prompt contexts; (b) a \"perturbation\" test to evaluate how robust the controller is when the feedback signal is noisy or missing; (c) evaluation of unrelated knowledge retention (i.e., unintended forgetting) after applying prompt-controllers; (d) comparison with stronger baselines including recent dynamic prompt-optimisation methods.\n* Is it possible to include measurements of prompt-controller overhead: number of feedback rounds, time per inference, prompt length inflation, memory/compute cost? Additionally, also provide guidance or heuristics for when the RL controller is cost‐effective?\n* Is it possible to provide details on the feedback/evaluation module: data used to train it, correlation with human ratings, error rates?\n* What is the controller performance under feedback error/noise (ablation showing sensitivity)?\n* Is it possible to game the metric? e.g., controller choosing prompt strategies that maximise the measured score but degrade actual user experience.\n* Is it possible to add baselines comparing the RL controller to (a) evolutionary prompt search (e.g., PromptQuine [B]), (b) retrieval-based dynamic demonstration selection, (c) static prompt engineering plus simple heuristic search. This will help justify the added complexity of RL. Show whether the RL controller actually delivers significant gains over simpler methods.\n```\n[A] Can We Edit Factual Knowledge by In-Context Learning?, EMNLP 2023\n[B] Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective, ICML 2025\n```"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3jttXavXet", "forum": "SbdSEtJALs", "replyto": "SbdSEtJALs", "signatures": ["ICLR.cc/2026/Conference/Submission11206/Reviewer_kZ43"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11206/Reviewer_kZ43"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914520854, "cdate": 1761914520854, "tmdate": 1762922356609, "mdate": 1762922356609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how to control model outputs for safety through prompt optimization methods similar to in-context learning. The authors explore a wide range of techniques, from static prompts to RL-based prompting, and conduct experiments across various models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The experiments are solid and well support the authors’ claims.\n\nThe writing quality is good.\n\nAnalyzing unlearning from the perspective of control is an interesting approach."}, "weaknesses": {"value": "Lack of comparison with existing unlearning methods.\nThe proposed method is novel and interesting, but it does not sufficiently reflect prior unlearning research. In particular, RL-based unlearning ([1], [2]) and gradient-based unlearning ([3]) are not included as baselines, making it difficult to assess the relative advantages and limitations of the proposed approach. To claim the efficiency of in-context prompt optimization for unlearning, quantitative comparisons with existing methods are needed.\n\nLack of comparison with existing prompt optimization methods.\nThe proposed approach resembles prompt tuning methods based on text gradients ([4], [5]). The paper’s Dynamic Prompt method is conceptually very similar to ORPO [5], and the RL-Enhanced Dynamic Controller resembles GRIPS [6]. The paper should reference or compare against these methods when optimizing prompts for unlearning.\n\nInsufficient justification of efficiency.\nThe paper emphasizes efficiency as a major advantage but does not compare directly against existing unlearning methods.\nWhile gradient-based unlearning methods require a high initial training cost, they allow continuous inference with the same unlearned state once trained. In contrast, the proposed approach must generate long system prompts and repeatedly perform API calls, which can incur high latency and token costs. It remains unclear whether this is more economical than, for example, training a gradient-ascent-based unlearning model for one hour on a single GPU (e.g., RTX 3090). Efficiency should be evaluated not only in terms of short-term computation cost but also long-term operational cost. For this, direct comparisons with existing unlearning methods are necessary.\n\nReferences\n[1] Zhang, Ruiqi, et al. \"Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning.\" arXiv preprint arXiv:2404.05868 (2024).\n\n[2] Mekala, Anmol, et al. \"Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models.\" arXiv preprint arXiv:2409.13474 (2024).\n\n[3] Liu, Bo, Qiang Liu, and Peter Stone. \"Continual Learning and Private Unlearning.\" Conference on Lifelong Learning Agents, PMLR, 2022.\n\n[4] Tang, Xinyu, Xiaolei Wang, Wayne Xin Zhao, Siyuan Lu, Yaliang Li, and Ji-Rong Wen. “Unleashing the Potential of Large Language Models as Prompt Optimizers: Analogical Analysis with Gradient-Based Model Optimizers.” AAAI Conference on Artificial Intelligence (2025).\n\n[5] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. \"Large Language Models as Optimizers.\" ICLR 2024.\n\n[6] Prasad, Archiki, Peter Hase, Xiang Zhou, and Mohit Bansal. \"GRIPS: Gradient-Free, Edit-Based Instruction Search for Prompting Large Language Models.\" (2022)."}, "questions": {"value": "It seems that variance is not reported in the tables or figures. Where is the variance of each method indicated?\n\nIt would be better to match the significant figures in the Combined Score values."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WGMPLTcALS", "forum": "SbdSEtJALs", "replyto": "SbdSEtJALs", "signatures": ["ICLR.cc/2026/Conference/Submission11206/Reviewer_vv45"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11206/Reviewer_vv45"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935920560, "cdate": 1761935920560, "tmdate": 1762922355771, "mdate": 1762922355771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free framework for LLM safety governance through closed-loop prompt optimization. Three methods are evaluated: static prompting, dynamic real-time feedback, and RL-enhanced approach using Deep Q-Networks. The RL method employs a multi-dimensional state vector and five multi-objective reward functions to balance safety and coherence. Tested on four models, it achieves highest effectiveness without modifying parameters, demonstrating strong model-agnostic applicability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work constructs a robust and sophisticated prompt engineering framework ensuring LLM safety and reliability without training. The  design demonstrates effective model governance through systematic external control mechanisms.\n2. It also extensively explores diverse prompt engineering strategies and different novel reward functions across three methods, conducting comprehensive experiments on four models to provide robust empirical validation and practical deployment insights.\n3. The DQN-based dynamic strategy selection is particularly interesting, intelligently adapting prompting approaches through reinforcement learning to achieve 84% effectiveness."}, "weaknesses": {"value": "1. The framework heavily relies on external LLMs with multiple API calls per iteration, significantly escalating operational costs and making practical deployment economically challenging for large-scale applications.\n2. The paper lacks evaluation of time costs from multi-round iterative refinement. No latency metrics or convergence analysis are provided, obscuring the practical impact on response times for real-world deployment.\n3. The paper notes RL methods show higher variance and failure risks than dynamic approaches but lacks analysis of root causes, limiting understanding of failure mechanisms and hindering practical deployment guidance.\n4. Critical DQN training specifics—network architecture, training data setting, hyperparameters, and convergence criteria—are missing despite detailed state vector and reward function descriptions, severely hindering reproducibility."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "c2xPfPiOUS", "forum": "SbdSEtJALs", "replyto": "SbdSEtJALs", "signatures": ["ICLR.cc/2026/Conference/Submission11206/Reviewer_VG2n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11206/Reviewer_VG2n"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978357415, "cdate": 1761978357415, "tmdate": 1762922355084, "mdate": 1762922355084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to employ prompt optimization to perform Knowledge Editing, the act of adaptively regulating the generative outputs of Large Language Models without full retraining. The authors categorize the so-called in-context prompt optimization technique into three types: static, dynamic, and RL-enhnaced prompting. The static prompting method, which is the simplest of the three, encapsulates the user query within a fixed system prompt. The dynamic method incorporates a feedback loop, which is comprised of a rating model, a strategy component, and an improvement model. Lastly, the RL-enhanced dynamic method leverages an RL agent to optimize the prompt generation strategy. Through Deep Q-Network, the RL agent learns the optimal strategy."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- This paper aims to extend RL-based prompt optimization to knowledge editing, which is important for ensuring a safe and reliable deployment of LLMs.\n\n- The authors provide a comprehensive review of related works that span knowledge editing, in-context unlearning, and prompt optimization.\n\n- The proposed method is demonstrated to be quite effective at suppressing unsafe outputs without harming the quality of LLM generation."}, "weaknesses": {"value": "- Prompt optimization via reinforcement learning or evolutionary algorithms has been studied before, but this work does not compare the proposed method against any of the previous works. Here are some works that I recommend the authors include in their experiments as important baselines: [1,2]. I believe doing a literature search starting from these papers should yield more relevant works that deserve to be compared here.\n\n[1] RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning\n\n[2] GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning\n\n- I found the paper quite difficult to follow. In Section 3.3, the authors omit much of the detail regarding DQN training. What are the reward function scores used here? How is the model trained using DQN? *How* does this induce \"meta-learning capabilities\"? A lot of this information is thrown at the audience without sufficient context as to why these claims hold true.\n\n- The authors mainly utilize safety-related benchmarks as a test bed for knowledge editing. However, how can we know for sure that safer outputs are a result of true knoweldge editing? That is, how do we know if the model truly \"forgot\" or \"unlearned\" to generate unsafe outputs, or if this behavior is induced simply by advanced prompting technique? Can these two truly be disentangled? If not, how can we be sure that the authors have induced \"knoweldge editing\" or \"unlearning\" through prompt optimization?\n\n- Lastly, major concerns surrounding the presentation of the paper. Too much material is placed in the appendix. As mentioned above, the methods section is difficult to follow because a lot of the details have either been omitted or are revealed later in the experimental setup. Figures 1-3 are not very helpful either. Only a snapshot of the results is presented in the main paper, and most of them are hastily placed in the Appendix. I suggest the authors work on their delivery and presentation."}, "questions": {"value": "Please refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5zDDlTi9JG", "forum": "SbdSEtJALs", "replyto": "SbdSEtJALs", "signatures": ["ICLR.cc/2026/Conference/Submission11206/Reviewer_D8FP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11206/Reviewer_D8FP"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission11206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762181063423, "cdate": 1762181063423, "tmdate": 1762922354592, "mdate": 1762922354592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}