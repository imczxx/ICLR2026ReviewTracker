{"id": "wHzegHWXCf", "number": 15122, "cdate": 1758248025692, "mdate": 1759897327264, "content": {"title": "Riemannian Stochastic Weakly Convex Optimization Under Heavy-Tailed Noises", "abstract": "Many studies have focused on stochastic optimization methods in Euclidean space under heavy-tailed gradient noises. Recently, some research has observed that gradient noise on Riemannian manifolds may also exhibit heavy-tailed distribution. In this paper, we investigate two types of gradient noise: sub-Weibull-type noise and noise with a bounded $p$-th central moment ($p$-BCM) for $p \\in (1,2]$. The latter is more challenging due to the possibility of infinite variance when $p \\in (1,2]$. Under both gradient noise assumptions, high-probability convergence of stochastic optimization methods in Euclidean spaces has been extensively studied in the contexts of convex (nonconvex) and smooth (nonsmooth) optimization. However, our understanding of the high-probability convergence of stochastic optimization methods on Riemannian manifolds under these two types of noise remains incomplete. We study the high-probability convergence of the ordinary Riemannian stochastic subgradient descent (SsGD) on the Stiefel manifold under weak convex optimization with sub-Weibull-type noise, as well as the convergence of Riemannian clipped-SsGD under $p$-BCM type noise. For weakly convex objective functions that may be nonconvex and nonsmooth, our results show that, compared to the Euclidean case, the theoretical dependence on failure probability and iteration complexity of Riemannian SsGD under sub-Weibull-type noise is not degraded. Under $p$-BCM noise, our findings indicate that the non-smoothness and non-convexity of weakly convex objectives do not affect the theoretical dependence of Riemannian clipped-SGD on failure probability.", "tldr": "", "keywords": ["Stochastic optimization", "Heavy-Tailed noises", "Riemannian optimization", "weakly convex", "Moreau envelope"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6385db97012c9dab31a294529a4fd42a4c2e2bca.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper extends the convergence analysis proposed by Zhu et al. (2025) (Euclidean space) to optimization problems on Stiefel manifolds. The main contributions of this work are summarized as follows:\n\n1)\tFor Riemannian SsGD (Riemannian Stochastic Subgradient Descent) on the Stiefel manifold, the authors derive the first high-probability convergence rate. This result is established under the setting of nonsmooth weakly convex optimization with sub-Weibull type noise.\n\n2)\tFor mini-batch Riemannian clipped SsGD on the Stiefel manifold, the authors also obtain the first high-probability convergence rate. This finding applies to nonsmooth weakly convex optimization with p-BCM type noise.\n\nTianxi Zhu, Yi Xu, and Xiangyang Ji. Stochastic weakly convex optimization under heavy-tailed noises. arXiv preprint arXiv:2507.13283, 2025."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors extend heavy-tailed optimization to Stiefel manifolds and provide a theoretical guarantee for Riemannian stochastic algorithms in some real-world scenarios."}, "weaknesses": {"value": "1.\tThe definition should be presented before the term is used, such as σ-sub-Weibull(θ) distribution and p-BCM condition.\n\n2.\tLack of experimental validation. The main test does not include numerical experiments. To verify the theoretical results, supplementary experiments on the Stiefel manifold under heavy-tailed noise conditions are needed. Additionally, it would be valuable to demonstrate whether the derived result constitutes the tightest possible upper bound.\n\n3.\tLimited manifold generalization. The work only focuses on the Stiefel manifold but does not discuss adaptation to other common Riemannian manifolds. It would be valuable to analyze how manifold geometric properties affect the algorithm’s convergence.\n\n4.\tInsufficient comparison with related work. Given that the authors mention in the abstract that “the dependence on failure probability and iteration complexity matches the best-known Euclidean results up to constants,” it is essential to explicitly highlight the key differences between the proposed work and existing best-known related results in the paper.\n\n\nMinor:\n\n1.\tEquation(17) should be \\(E[||x||^p]\\leq \\sigma^p <+\\infity\\)\n\n2.\tEquation(39) should be \\(E_t[exp\\{(||\\xi_t||/\\sigma)^\\frac{1}{\\theta}\\}]\\)"}, "questions": {"value": "The paper’s title is “RIEMANNIAN STOCHASTIC WEAKLY CONVEX OPTIMIZATION UNDER HEAVY-TAILED NOISES”. Given this focus on Riemannian optimization, two key questions arise:\n\n1.\tCould this work be extended to the common Riemannian manifolds? The paper only focuses on optimization problems on the Stiefel manifolds.\n\n2.\tHow do the geometric properties of the manifold influence the algorithm’s convergence, compared with those in Euclidean space?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aHPGVFHLX2", "forum": "wHzegHWXCf", "replyto": "wHzegHWXCf", "signatures": ["ICLR.cc/2026/Conference/Submission15122/Reviewer_3teB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15122/Reviewer_3teB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761362740277, "cdate": 1761362740277, "tmdate": 1762925441606, "mdate": 1762925441606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the stochastic weakly convex opt over Stiefel manifolds. Assuming the stochastic gradients are unbiased and with heavy-tailed noises, the authors design Riemannian stochastic subgradient methods and clipped-type methods with theoretical guarantees."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "It's interesting to consider the heavy-talied noise setting in the Riemannian weakly cnvex setting."}, "weaknesses": {"value": "1. It is not entirely convincing that the setting (i.e., Riemannian weakly convex opt with heavy-tailed noises on the stochastic gradients) makes sense. Could the authors showcase its practical relevance by giving (several) concrete examples? The only example briefly mentioned in line 58 is not about Stiefel manifolds. Perhaps the the heavy-tailed noises appear in distributed settings (https://arxiv.org/abs/2303.17779) but I am not sure. \n2. When going through this paper, I did not grasp the key technical difficulty compared with the Euclidean case. Could the authors explicitly point out the key technical difficulty?\n3. Line 75 and Line 236, \"weak\" should be \"weakly\". Line 149, \"close\" should be \"closed\"."}, "questions": {"value": "See the two questions in the above weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LWO9i29Ei1", "forum": "wHzegHWXCf", "replyto": "wHzegHWXCf", "signatures": ["ICLR.cc/2026/Conference/Submission15122/Reviewer_kERU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15122/Reviewer_kERU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761426483948, "cdate": 1761426483948, "tmdate": 1762925441256, "mdate": 1762925441256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the Riemannian SGD in a heavy-tailed noise setup. The authors propose two algorithms, that handle sub-Weibull and $p$\n-BCM noises. They derive convergence rates both in expectation and with high-probability. The results obtained do not differ significantly from the ones derived for the Euclidean setup in existing works."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1)Sub-Weibull random variables, as well as random variables with bounded $p$-th central moment, are considered.\n\n2)General convergence rates for $\\eta_t$, satisfying condition at line 241, are derived"}, "weaknesses": {"value": "1)Only Stiefel manifolds are considered.\n\n2)Even though the authors require weak convexity, convergence results are given for $\\Theta(x)$, instead of the function's value.\n\n3)No experiments, analyzing the performance of Algorithm 1 and Algorithm 2."}, "questions": {"value": "1)What is the motivation for considering the weakly-convex setup for Riemannian optimization?\n\n2)Can you provide experiments, validating, that your derived methods handle heavy-tailed noise for Riemannian setup?\n\n3)How can the assumptions on gradient's boundedness be relaxed to the smoothness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No additional ethical concerns."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ePjcAqnLXx", "forum": "wHzegHWXCf", "replyto": "wHzegHWXCf", "signatures": ["ICLR.cc/2026/Conference/Submission15122/Reviewer_eHbK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15122/Reviewer_eHbK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997259256, "cdate": 1761997259256, "tmdate": 1762925440893, "mdate": 1762925440893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Extend the convergence analysis of weakly convex optimization under heavy detail noise from Euclidean space to Stiefel manifold."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Analysis of SGD on manifold under heavy tail noise."}, "weaknesses": {"value": "It seems the extension is fully parallel with the Euclidean case, and some numerical experiments may be helpful."}, "questions": {"value": "* What is the key challenge for the extension from the Euclidean space to Stiefel manifold?\n* The definition of retraction should be properly introduced. In fact, what property should the retraction satisfy in order to establish the convergence? I see from the proof of Lemma B.2 that the retraction based on the polar decomposition is used. Why and whether does the result hold for other retractions?\n* Can the analysis be extended beyond Stiefel manifold?\n* Assumption 3.5 only assumes $\\sigma$-sub-Weilbull($\\theta$), but the $p$-BCM noise is also considered.\n* Any example to show that Assumption 3.3 can be met?\n* Is it possible to obtain strong result for example if $f$ is (locally) geodesic strongly convex?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sByvPRnYYW", "forum": "wHzegHWXCf", "replyto": "wHzegHWXCf", "signatures": ["ICLR.cc/2026/Conference/Submission15122/Reviewer_eu7t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15122/Reviewer_eu7t"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762161401011, "cdate": 1762161401011, "tmdate": 1762925440388, "mdate": 1762925440388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}