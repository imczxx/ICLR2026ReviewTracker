{"id": "nGihWDdQFI", "number": 15143, "cdate": 1758248196838, "mdate": 1759897325369, "content": {"title": "CAED-Agent: an Agentic Framework to Automate Simulation-Based Experimental Design", "abstract": "The adjustment of parameters for expensive computer simulations is a challenging and universal task in the scientific research pipeline.\nWe refer to these problems as Cost-Aware Simulation-Based Experimental Design (CAED). Traditional approaches include: a) brute force search, which is prohibitive for high-dimensional parameter combinations; b) Bayesian optimization, which struggles to generalize across setup variations and does not incorporate prior knowledge; c) case-by-case experts designs, which is effective but difficult to scale. Recent work on language models (LLMs) as scientific agents has shown an initial ability to combine pre-trained domain knowledge with tool calling, enabling workflow automation. Naturally, replacing the expert's manual design with this automation seems to be a scalable remedy to general CAED problems. As will be shown in our empirical evaluations, LLMs lack cost awareness for parameter tuning tasks in scientific simulation, leading to poor and inefficient choices. Inference-time scaling approaches enable better exploration, but the massive additional simulator queries they incur add up to total cost and contradict the target of being efficient. To address this challenge, we propose the Cost-Aware Simulation-Based Experimental Design Agent (CAED-Agent), an agentic framework that combines inference-time scaling with the cost-efficiency feedback from a lightweight surrogate model for solving CAED problems. Our experiments in three different simulation cases show that CAED-Agent outperforms both Bayesian optimization and LLM baselines by significant margins, achieving success rates comparable to inference-time scaling with a ground truth simulator, while being far more cost-efficient.", "tldr": "We present CAED-Agent, a large-language-model agent framework that utilizes inference-time scaling with lightweight surrogate feedback for cost-aware experimental design.", "keywords": ["Cost-Efficienct Experimental Design; Scientific Agents; In-Context Optimization"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7f28ca1fbcb28558638e937478318bd0a131dc88.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CAED-Agent, an agentic framework that aims to improve the cost-efficiency of simulation-based experimental design. The key idea is to pair a large language model’s inference-time reasoning with a lightweight neural network that predicts cost and utility signals, so that the LLM can make informed, cost-aware decisions without excessive simulator calls. The authors test the approach on three physics simulation environments (1D heat conduction, 1D Euler, and 2D Navier–Stokes) and show that CAED-Agent achieves better success rates and efficiency than Bayesian optimization and other LLM-based optimizers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles a well-motivated and practically important problem: how to make LLM-driven scientific agents more cost-efficient and aware of computational budgets. It clearly identifies two major pain points of current approaches: lack of cost awareness and inference-scaling inefficiency.\n- The problem formulation is clean, and the proposed surrogate-signal design is simple yet effective. The experimental setup, while small-scale, convincingly shows that cost-aware signals can meaningfully improve sample efficiency.\n- Overall, the work provides a reasonable step toward more practical LLM-based experiment design agents, and the results are consistent with the claimed motivation."}, "weaknesses": {"value": "- The assumption that “small fully connected neural networks can learn the cost and utility functions well” feels overly strong and only holds because the experiments are limited to very low-dimensional (1D–2D) settings. It’s unclear whether the same idea would scale to higher-dimensional or more realistic simulation problems.\n- The design choice of adding a small neural network to guide the LLM is not fully justified. If the cost and utility mappings are simple enough for a tiny NN to learn, one might question why the LLM itself cannot capture such patterns through proper prompting or fine-tuning. The rationale for separating the learning responsibilities between the LLM and the NN needs clearer theoretical or empirical backing.\n- The related-work discussion could be deeper. There is rich literature in BO that can leverage prior knowledge from related domains. A simple Google search will return many papers. E.g., Pre-trained Gaussian processes for Bayesian optimization (Wang et al. 2024).\n- The clarity of presentation could also improve: a few typos (e.g., “benifit” in line 091), small fonts and dense descriptions in Figure 2 make it hard to follow.\n- Finally, the evaluation is confined to toy problems; no evidence is provided that CAED-Agent can handle realistic scientific simulations where costs and outcomes are high-dimensional or noisy."}, "questions": {"value": "- It would help to clarify the exact claim that “Bayesian optimization cannot generalize across problem variations.” BO’s limitations usually stem from surrogate transfer, not from the BO framework itself.\n- The cost function in Eq. (2) seems to depend on y (simulation outputs), but intuitively cost should depend only on x and θ; please clarify this dependence.\n- Future work could explore whether the cost-aware feedback can be incorporated directly into the LLM through reinforcement-style prompting or few-shot demonstrations, possibly removing the need for a separate NN surrogate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0kA1GI602b", "forum": "nGihWDdQFI", "replyto": "nGihWDdQFI", "signatures": ["ICLR.cc/2026/Conference/Submission15143/Reviewer_toXH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15143/Reviewer_toXH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761420821056, "cdate": 1761420821056, "tmdate": 1762925459322, "mdate": 1762925459322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for automating parameter tuning in computational simulations that must balance accuracy and computational cost. It introduces the concept of cost-aware experimental design (CAED) and proposes CAED-Agent, which combines a large language model with a lightweight neural surrogate trained to predict simulation utility and cost. The surrogate provides feedback signals that the LLM uses in context to iteratively generate improved design parameters without retraining.\n\nThe framework is evaluated on three physics-based simulations, including Heat 1D, Euler 1D, and Navier–Stokes 2D, and compared with Bayesian Optimization and Optimization by Prompting (OPRO). The results show that CAED-Agent generally achieves better cost–utility trade-offs and more stable optimization behavior than these baselines, particularly in multi-turn settings, while requiring fewer full simulator evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The formulation of the cost-aware experimental design problem is novel and meaningful, addressing a problem that has been underexplored.\n2. The use of a smaller surrogate model to provide continuous cost–utility feedback to a large language model is a well-motivated idea, offering an efficient way to utilize prior knowledge or existing data when available.\n3. The writing is generally clear and easy to follow."}, "weaknesses": {"value": "1. The accuracy of the surrogate model is important, as shown in the ablation study. This method requires specific data for each experimental task, which is somewhat unrealistic. In the experiments, more than 4,000 samples were used to train such a model (in 2 out of the 3 tasks), making it unsuitable for few-shot scenarios where experiments are expensive.\n\n2. The statements “outperforms both ... by significant margins” and “Through experiments on three physics simulator environments, each with varying environmental settings and precision requirements, we demonstrated that CAED-Agent consistently outperforms both classical Bayesian optimization baselines and state-of-the-art LLM-based optimizers” are vague. The paper should clarify the metrics on which these claims are based. For example, in terms of pass rate, CAED-Agent does not consistently outperform baseline methods.\n\n3. Some experimental details are missing, such as the hyperparameters of the LLMs and the settings of baseline methods.\n\n4. The experiments are somewhat narrow in scope. All evaluations are conducted on relatively low-dimensional 1D and 2D PDE simulations with a small number of design variables (e.g., grid size, CFL number)."}, "questions": {"value": "1. Please refer to the weaknesses.\n2. How beneficial is it to use a neural network to model the prior knowledge? Would incorporating simple statistics derived from the existing samples (e.g., the possible ranges of the design variables) achieve a similar effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yPQRDcDHgl", "forum": "nGihWDdQFI", "replyto": "nGihWDdQFI", "signatures": ["ICLR.cc/2026/Conference/Submission15143/Reviewer_WNEs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15143/Reviewer_WNEs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761508712371, "cdate": 1761508712371, "tmdate": 1762925458136, "mdate": 1762925458136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CAED-Agent, an agentic framework for simulation-based experimental\ndesign that leverages large language models (LLMs) to optimize simulator configurations\nunder cost constraints. The central idea is to learn a surrogate model that predicts the\nutility (simulation accuracy) and cost (runtime or compute) of running a simulator with\ngiven hyperparameters such as grid size, time step, or solver tolerance. An LLM-based\nagent then iteratively proposes new configurations, queries the surrogate for predicted\ncost and utility, and updates its prompt to improve the trade-off between fidelity and\nefficiency. The authors define single-turn and multi-turn variants of the optimization\nprocess and evaluate CAED-Agent on three physics-based simulators: Heat1D, Euler1D, and\na 2D Navier–Stokes system. They compare against direct LLM prompting, Bayesian\noptimization (BO), and the OPRO framework, showing faster convergence and higher reward\nin several cases."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper explores an emerging and relevant topic: integrating LLMs with surrogate\n  modeling for agentic optimization in scientific computing. The combination of cost\n  modeling, utility shaping, and multi-turn prompting is original and technically\n  interesting.\n- The distinction between single-shot design (analogous to standard hyperparameter\n  tuning) and multi-round design (analogous to iterative experimental design) is\n  conceptually helpful.\n- Although limited in scale, the PDE-based testbeds (Heat1D, Euler1D, NS-2D) provide\n  reproducible and interpretable environments, which is a strength compared to purely\n  synthetic tasks often used in LLM-agent papers.\n- Evaluation: The inclusion of Bayesian optimization and OPRO as comparison methods is\n  appropriate and helps contextualize the agentic behavior.\n- Readable and modular implementation idea: The general pipeline (LLM agent + surrogate\n  model + simulator) is easy to grasp and could, in principle, be extended to other\n  domains (e.g., actual parameter sweeps, i.e., inference)."}, "weaknesses": {"value": "### Problem framing and conceptual clarity\n\n- The problem formulation is ambiguous. The paper presents itself as simulation-based\n  experimental design, yet the optimization targets only simulator settings (grid size,\n  step size, etc.), not experimental variables or physical parameters. This mismatch\n  makes the title and abstract somewhat misleading.\n- The introduction jumps directly into the LLM setup without clearly defining what is\n  being optimized or why. A reader unfamiliar with the specific simulators will struggle\n  to understand the underlying task.\n- The notion of “inference-time scaling” is highlighted but never defined. This should\n  be introduced more explicitly.\n- The downstream purpose of selecting these simulator settings remains unclear. In\n  realistic scenarios, one would care about physical inference or control — not just\n  choosing a grid size. The authors should motivate how this contributes to actual\n  scientific or decision-making goals.\n\n### Relation to prior work and novelty claims\n\n- Overstated novelty in cost modeling. The claim that this is the first approach to\n  estimate simulation cost is inaccurate:\n- Bharti et al. (2024) explicitly model simulator cost within cost-aware SBI, optimizing\n  sampling to minimize total compute.\n- Gorecki et al. (2023) address Bayesian decision making via amortized networks, which\n  inherently learn expected losses that include cost.\n- The paper should clearly position itself relative to these works, correcting the novelty claim.\n  - Although Bharti et al. are cited, the connection to SBI remains opaque. Is the\n    “utility” function intended to represent inference performance, or is inference\n    absent entirely?\n  - The decision-making perspective of Gorecki et al. is highly relevant and missing.\n    Both approaches learn to choose actions under uncertainty given simulator costs;\n    this work could be framed as a heuristic, LLM-driven version of amortized\n    decision-making.\n\n### Methodological and conceptual gaps\n\n- The use of an “LLM agent” is poorly formalized. There is no clear notion of an\n  optimization objective or theoretical grounding (e.g., expected-utility maximization,\n  policy improvement). The surrogate–LLM loop seems heuristic, and no stability or\n  convergence analysis is provided.\n- The argument that existing experimental design benchmarks cannot be used “because they\n  lack cost labels” is weak — costs such as runtime or FLOPs are measurable for any\n  simulator. This choice limits comparability to established approaches.\n- The tasks are self-contained and synthetic. They demonstrate an internal cost–utility\n  trade-off but have no demonstrated downstream relevance or integration with real\n  inference workflows.\n\n### Experimental complexity and scalability\n\n- All three PDE benchmarks (1D heat, 1D Euler, 2D Navier–Stokes) are low-dimensional and\n  deterministic, with smooth cost–fidelity relations. They are suitable as sanity checks\n  but not as evidence of scalability or robustness.\n- The paper does not explore increasing complexity (e.g., 3D, chaotic, or stochastic\n  regimes). Without such tests, it remains unclear how the method behaves when the\n  trade-off surface becomes non-monotonic or discontinuous.\n- In more challenging setups, LLM-based optimization is likely to hallucinate or become\n  unstable, as it lacks calibrated uncertainty or safety mechanisms. The authors should\n  probe these edge cases.\n\n### Missing discussion of uncertainty and trustworthiness\n\n- A major advantage of Bayesian optimization (BO) approaches is that their\n  Gaussian-process surrogates provide uncertainty estimates and theoretical guarantees\n  for exploration and convergence. The LLM-based approach offers no such calibration, at\n  least no principled approaches. Thus, it remains unclear how users can trust or\n  interpret its suggestions.\n- The paper should explicitly compare BO and LLM agents on a more challenging task,\n  evaluating not only final reward but also uncertainty quantification, robustness, and\n  failure detection.\n\n### Presentation and writing quality\n\n- Several stylistic and formatting issues (random bolding, missing spaces, inconsistent\n  punctuation) reduce readability and suggest unedited LLM-generated text.\n- Figures are presented out of order: Figure 1 appears early but is referenced later;\n  Figure 2’s caption misstates that the single-turn agent “calls the simulator” rather\n  than calling the cost-utility surrogate (unless I am missing something general here?)\n- I suggest having Figure 2 as the conceptual Figure 1 early in the paper."}, "questions": {"value": "1. What is the downstream task or use case of optimizing simulator settings? How would\n   this framework contribute to a real scientific or decision-making pipeline?\n2. How does the method compare in practice to Bayesian optimization on more complex\n   problems, particularly in terms of uncertainty estimates and failure detection?\n3. Can the surrogate be integrated into a Bayesian decision-theoretic formulation\n   similar to Gorecki et al. (2023), allowing direct Bayesian expected-loss minimization\n   rather than heuristic LLM guidance?\n4. How does the approach scale computationally when the simulator cost becomes dominant\n   relative to LLM calls?\n5. Have the authors evaluated or observed cases where the LLM proposes invalid or nonsensical\n   simulator configurations? How are such failures detected or handled?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "09SG8E0AX0", "forum": "nGihWDdQFI", "replyto": "nGihWDdQFI", "signatures": ["ICLR.cc/2026/Conference/Submission15143/Reviewer_rHmV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15143/Reviewer_rHmV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923585135, "cdate": 1761923585135, "tmdate": 1762925457548, "mdate": 1762925457548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a method to optimally select parameters for computationally complex computer simulations, comprised of a learned surrogate that models the cost and performance of different parameters and an agentic LLM that queries the surrogate to optimize the parameters."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of training a neural network surrogate for an agentic LLM to query for optimizing simulations is interesting and novel. It is useful to be able to integrate prior knowledge into the optimization process."}, "weaknesses": {"value": "The authors do not present a convincing argument that CAED-agent is consistently better than the state of the art. It is unclear what hyperparameters were used to generate the results for the baseline experiments. It is also unclear if a fair comparison was made, or in what dimension the baseline metrics were equivalent to the CAED-agent metrics. Did the methods have equivalent runtimes, equivalent queries of the simulator, or equivalent computational requirements? \n\nHow does performance for each method change with different constraints? For example, BO is usually sample efficient and would likely still produce reasonable results with a few dozen queries of the simulator, but I would imagine the surrogate neural network would not be able to train on just a few dozen queries. Can the authors run an ablation on these parameters?"}, "questions": {"value": "If you have the neural proxy for the true simulator, does BO work on the neural proxy? On the other hand, is giving the LLM the training data directly and asking for an optimized result possibly more effective than using the data to train a neural proxy then having a separate LLM to query the proxy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9OqkoPLOrX", "forum": "nGihWDdQFI", "replyto": "nGihWDdQFI", "signatures": ["ICLR.cc/2026/Conference/Submission15143/Reviewer_zDSZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15143/Reviewer_zDSZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932693433, "cdate": 1761932693433, "tmdate": 1762925457055, "mdate": 1762925457055, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}