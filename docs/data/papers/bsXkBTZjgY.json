{"id": "bsXkBTZjgY", "number": 18399, "cdate": 1758287206322, "mdate": 1759897105798, "content": {"title": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks", "abstract": "Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. \nHowever, a key drawback of existing VLAs is their extremely high inference costs. \nIn this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training.\nSuccessfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization and action generation strategy. \nWe train HyperVLA on the Open X-Embodiment dataset, and evaluate on the SIMPLER benchmark. \nCompared to existing VLAs, HyperVLA achieves a similar or even higher success rate during evaluation, while significantly reducing inference costs. \nNotably, compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by $90\\times$, and accelerates inference speed by $120\\times$.", "tldr": "", "keywords": ["multi-task learning", "imitation learning", "robotic control", "VLA", "hypernetworks"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07db708ec275cfda257f67bd90c0c23f81ec4245.pdf", "supplementary_material": "/attachment/c7968a07670319fdd880a28d11492f15f3dbbaaa.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents HyperVLA, a vision-language-action (VLA) framework that leverages a hypernetwork to generate lightweight, task-specific policies for efficient inference."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- A clear objective—retaining VLA-level generalization while greatly improving inference efficiency.\n- A concrete HN-based realization with sensible design choices (vision backbone, context-embedding normalization, linear action head) and strong speedups on SIMPLER/LIBERO with competitive success rates."}, "weaknesses": {"value": "- Context dependence & robustness. The HN conditions on only the initial image and language, and the base policy omits language input. Please evaluate robustness to distribution changes like viewpoint/lighting shifts. *I strongly recommend including real-world experiments to further demonstrate the applicability and reliability of the proposed approach.*\n- Accounting for HN cost. Table 3 excludes the one-time HN/encoder computation. Please report amortized wall-clock when tasks switch frequently vs. rarely, and how speedup scales with episode length.  \n- Limitations. A dedicated limitations section would help (e.g., potential brittleness to context errors, limited expressivity of a ~0.1M-parameter base policy for highly multimodal action distributions)\n- Only partial setting on the simplerenv benchmark are presented. Please clarify the reason for not reporting the full results."}, "questions": {"value": "1.\tHypernetworks have rarely been effective as an acceleration mechanism in other AI domains. Could the authors elaborate on what makes the VLA setting different—what specific properties of vision-language-action control allow this approach to succeed where it typically does not? A brief discussion of these distinctions and relevant historical context would strengthen the paper’s motivation.\n2.\tRegarding the question raised in the paper “Can we learn a generalist policy that combines the strong generalization ability of VLAs and the efficient inference of single-task policies?”. Are there any other possible technical routes besides hypernetworks?\nFor instance, can DeeR-VLA be viewed as an alternative realization of this idea? How do dynamic-routing approaches like DeeR-VLA compare with hypernetworks in terms of their advantages and disadvantages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hginhxOBYA", "forum": "bsXkBTZjgY", "replyto": "bsXkBTZjgY", "signatures": ["ICLR.cc/2026/Conference/Submission18399/Reviewer_VKKj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18399/Reviewer_VKKj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666035132, "cdate": 1761666035132, "tmdate": 1762928106826, "mdate": 1762928106826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HyperVLA, an efficient VLA model that leverages a Hypernetwork (HN) to dynamically generate lightweight task-specific policy networks. The design reduces inference latency from hundreds of milliseconds to ~4 ms while maintaining performance comparable to OpenVLA on SIMPLER and LIBERO, with well-organized ablations supporting its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The work proposes a new Hypernetwork-based VLA design that achieves remarkable inference efficiency while preserving zero-shot generalization across diverse manipulation tasks.\n- The work provides well-designed algorithmic components—pretrained vision backbone, HN normalization, and a lightweight action generation strategy—each validated through ablations to be essential for stable and efficient training."}, "weaknesses": {"value": "- Lack of real-world validation: All evaluations are conducted in simulation (SIMPLER, LIBERO). Given the sensitivity of HN to distribution shifts, this omission limits confidence in real-world robustness and transferability to physical robots.\n- Incomplete efficiency comparison: It is encouraged to include comparisons with recent optimized or lightweight variants of existing models that also improve inference efficiency. For example, OpenVLA-OFT demonstrates strong performance on the LIBERO benchmark while achieving notable gains in computational efficiency.\n- Unexplored efficiency–capacity trade-off: The paper lacks analysis of how HN or policy capacity affects performance and scalability. It remains unclear how changing the HN hidden size impacts stability and generalization, whether the 0.1M-parameter base policy is a bottleneck, and how the method scales with larger datasets or more complex task distributions."}, "questions": {"value": "- How consistent are the policies generated by the HN across tasks with similar instructions or visual contexts? For instance, would a policy generated for grasping an apple still succeed when applied to grasping an orange or a cup?\n- The method generates compact task-specific policies without language conditioning at inference. Can this design handle complex or compositional instructions—such as spatial reasoning tasks (e.g., “place the left apple into the right basket”)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4aelsW2wyI", "forum": "bsXkBTZjgY", "replyto": "bsXkBTZjgY", "signatures": ["ICLR.cc/2026/Conference/Submission18399/Reviewer_RdTx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18399/Reviewer_RdTx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726556457, "cdate": 1761726556457, "tmdate": 1762928106479, "mdate": 1762928106479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of high inference costs in existing VLA models. The authors propose a novel hypernetwork-based architecture that activates only a small, task-specific policy during inference. Several key algorithmic designs are introduced to enable the successful training of hypernetwork-based models, including leveraging prior knowledge from vision foundation models, applying hypernetwork normalization, and designing an action generation strategy. The proposed approach is evaluated in simulation environments such as SIMPER and Libero, demonstrating its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel hypernetwork-based VLA model structure that activates only a compact, generated policy at inference, significantly improving efficiency.\n\n2. To address the optimization challenges of hypernetworks, the authors analyze the differences between HN training and standard training, propose normalizing the context embedding, and validate the effectiveness of this strategy through experiments.\n\n3. The paper further highlights practical techniques for training HN-based VLAs, such as utilizing prior knowledge from vision encoders and employing a lightweight linear action head to improve efficiency.\n\n4. The experiments are well-organized and convincingly demonstrate the proposed model’s generalization, adaptability, and inference efficiency compared with mainstream VLAs such as RT1-X, Octo, and OpenVLA."}, "weaknesses": {"value": "1. Hypernetworks for policy generation are not entirely new, with related works in meta-RL and dual-system VLAs sharing conceptual similarities. The paper could clarify more explicitly how HyperVLA advances beyond these prior approaches.\n\n2. The role of the context token as an input to the context encoder seems important, but the current explanation could be elaborated further for clarity.\n\n3. The linear action head combined with action chunking appears to contribute significantly to inference efficiency. However, a more detailed analysis or comparison highlighting its advantages would further strengthen the contribution.\n\n4. The evaluation focuses on simulation environments (SIMPER and Libero). While this provides valuable insights, demonstrating the method’s inference efficiency on real robotic platforms could make the results more compelling."}, "questions": {"value": "1. How does HyperVLA perform in real-robot settings with strict latency constraints? Is the 120× speedup sufficient for high-frequency control (e.g., >50 Hz) in practice?\n\n2. How sensitive is HyperVLA to the choice of backbone? For example, what happens if weaker vision encoders (e.g., EfficientNet) are used instead of DINOv2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kw0nydxDkI", "forum": "bsXkBTZjgY", "replyto": "bsXkBTZjgY", "signatures": ["ICLR.cc/2026/Conference/Submission18399/Reviewer_6UDx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18399/Reviewer_6UDx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972894806, "cdate": 1761972894806, "tmdate": 1762928106116, "mdate": 1762928106116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HyperVLA, an approach to make VLA models more efficient at inference time by introducing a hypernetwork that dynamically generates policy weights conditioned on task inputs. The goal is to reduce inference latency and computational cost without significantly hurting performance. The authors evaluate HyperVLA on simulation benchmarks like SIMPLER and LIBERO, reporting large speedups and comparable success rates to full VLA baselines. The work aims to make VLAs more practical for real-world robotic control by improving efficiency and deployability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper tackles an important and timely problem . VLA models are getting very capable but are often too heavy for real-time control. The motivation to improve inference efficiency is well grounded and practically relevant.\n2. Using a hypernetwork to generate task-specific weights for the policy is a neat idea. Even if not entirely novel, it’s an intuitive way to adapt to diverse tasks without keeping multiple large models in memory.\n3. The paper reads clearly and the technical sections are well-structured. The figures and tables are also clean and informative."}, "weaknesses": {"value": "1. The main idea of using a hypernetwork to make inference faster  is interesting, but it’s not clear why this specific route was chosen over more standard acceleration methods. Approaches like consistency distillation [Consistency Policy], flow matching action head [Pi series or GR000T series], and quantization-based compression are well established and might offer simpler alternatives. The paper doesn’t really compare against them or argue theoretically for why HNs are better suited. As a result, the design choice feels more empirical than principled.\n2. The paper mostly compares against RT-1-X, Octo, and OpenVLA. They are solid but somewhat dated baselines. Recent models like Pi0, MiniVLA, or DeeR-VLA leverage large-scale multimodal pretraining and show strong scaling behavior, which is one of the core advantages of current VLAs. Not including these makes it hard to judge how HyperVLA performs in the context of modern scaling trends. The explanation that those models use different datasets isn’t really sufficient.  \n3. Everything is evaluated in simulation (SIMPLER, LIBERO), and there’s no hardware validation. For a paper that positions itself as making VLAs practical for robotics, that’s a big gap. Real-world deployment involves latency, sensing noise, and control delays that simulation doesn’t capture. Even a small-scale physical experiment, like what Octo did, would make the claims much stronger.\n4. Most of the architectural decisions: freezing the vision backbone, using a linear MLP head and training with MSE, are standard tricks from prior work like Llava and OpenVLA. These choices seem empirical rather than offering new insight. The method works, but it feels more like careful system tuning than a genuinely new idea."}, "questions": {"value": "1. Have you compared HyperVLA’s trade-offs against methods like distillation or quantization, especially in terms of accuracy vs. compute?\n2. The current baselines are weak. How would the method behave when scaling up with hundreds of millions of multimodal samples? Does the hypernetwork remain efficient?\n3. Any evidence on real-robot latency and performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m5lSqnxaxV", "forum": "bsXkBTZjgY", "replyto": "bsXkBTZjgY", "signatures": ["ICLR.cc/2026/Conference/Submission18399/Reviewer_3cYT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18399/Reviewer_3cYT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762071769080, "cdate": 1762071769080, "tmdate": 1762928105628, "mdate": 1762928105628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce HyperVLA, a hypernetwork (HN)-based architecture that activates a small, task-specific policy during inference, which significantly reduces inference costs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "•The paper is well-written with a clear and logical structure.\n•The approach demonstrates a notable improvement in inference speed, effectively achieving a lightweight version of VLA."}, "weaknesses": {"value": "•Baseline Selection and Fairness:\n￮The choice of baselines is questionable. The paper would benefit from comparing against more recent and stronger models, such as OpenVLA-OFT, pi0, pi0-FAST, and pi05-kI. These models, to the best of my knowledge, achieve higher performance on the LIBERO benchmark, which makes it difficult to demonstrate the advantages of the proposed approach.\n•Inference Cost Considerations:\nIn Section 4.4, line 400, the authors claim that the computational cost of HN is negligible since they are only activated once at the beginning of each episode.  However, this assessment may overlook the potential impact of these costs in different scenarios, particularly given that the tasks evaluated are short (e.g., pick-place tasks).\nIt would be helpful if the authors could:\n1. Provide the results of the HN generation cost.\n2.Offer a more detailed analysis and discussion on how the model performs in longer tasks with task-switching.\n•Real-World and Complex Task Evaluation:\n￮The authors should include results on real-world experiments and more complex, longer tasks to better demonstrate the advantages of inference acceleration in practical settings."}, "questions": {"value": "•Have the authors considered alternative methods like soft prompting, LoRA, or MoE (Mixture of Experts) to sparsely activate VLA parameters, which are common in recent approaches?\n•Could the authors comment on the statement in lines 156-158, where it is mentioned that existing VLA models require \"activating the whole model during both training and inference\"? Is this claim still valid, given recent developments in VLAs that decouple tasks more efficiently?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vRiDwch5rD", "forum": "bsXkBTZjgY", "replyto": "bsXkBTZjgY", "signatures": ["ICLR.cc/2026/Conference/Submission18399/Reviewer_1L2x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18399/Reviewer_1L2x"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762683531272, "cdate": 1762683531272, "tmdate": 1762928105021, "mdate": 1762928105021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}