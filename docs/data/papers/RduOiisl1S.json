{"id": "RduOiisl1S", "number": 2892, "cdate": 1757298391364, "mdate": 1759898120847, "content": {"title": "Buffer Matters: Unleashing the Power of Off-Policy Reinforcement Learning in Large Language Model Reasoning", "abstract": "Traditional on-policy Reinforcement Learning Fine-tuning (RFT) frameworks suffer from experience waste and reward homogeneity, which directly hinders learning efficiency on difficult samples during large language models post-training. In this paper, we introduce Batch Adaptation Policy Optimization (BAPO), an off-policy RFT framework to improve the data efficiency in large language models post-training. It dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, while holding a lower bound guarantee for policy improvement. Extensive experiments further demonstrate that BAPO achieves an average 12.5\\% improvement over GRPO across mathematics, planning, and visual reasoning tasks. Crucially, BAPO successfully resolves 40.7\\% of problems that base models consistently fail to solve.", "tldr": "We introduce Batch Adaptation Policy Optimization (BAPO), an off-policy Reinforcement Learning (RL) framework to improve the data efficiency in large language models post-training.", "keywords": ["LLM post-training; off-policy RLVR"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7815fe091f1f714e2ecdbd741ca1215a1b21691e.pdf", "supplementary_material": "/attachment/9667e909878d144517a42d243658a0ee4a7c8f8f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Batch Adaptation Policy Optimization (BAPO), an off-policy reinforcement learning framework to improve the data efficiency of fine-tuning large language models on reasoning tasks. The core problem BAPO addresses is the failure of on-policy methods to learn effectively from difficult samples. BAPO constructs training batches by combining three data sources. It filters fresh rollouts to focus on moderately difficult samples. It also periodically re-evaluates historically difficult samples with the current policy to find newly solved ones. Finally, it reuses high-quality historical samples to ensure batch stability. Experiments show that BAPO improves performance over existing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed batch construction method is well-motivated. Instead of simply mixing old and new data, it treats historical samples differently based on their difficulty. Re-evaluating hard examples and reusing successful ones is a more nuanced approach than standard experience replay and directly targets the stated problem of learning from difficult instances.\n\n2. The evaluation in the paper covers three distinct reasoning domains (mathematics, planning, and visual geometry) using several modern LLM backbones. The comparison against a range of both on-policy and off-policy baselines is thorough, and the inclusion of detailed ablation studies strengthens the authors' claims about their design choices.\n\n3. The work provides a solid theoretical foundation for the proposed method. The inclusion of a policy improvement lower bound in Theorem 3.2 offers a formal guarantee of training stability under certain conditions. This theoretical analysis elevates the work beyond a purely empirical contribution and builds confidence in the method's robustness."}, "weaknesses": {"value": "1. The novelty of the core components could be overstated. The individual ideas of using a replay buffer, re-evaluating samples, and filtering data based on difficulty have precedents in the broader reinforcement learning literature, such as Prioritized Experience Replay. The main contribution lies in the specific combination of these techniques for LLM reasoning with verifiable rewards. The authors should further explain the novelty of this paper. \n\n2. The mechanism for adapting the high-quality sample thresholds c2 and c3 is not clearly defined. The paper states that a linear mapping function is used based on the buffer's average performance, but the details of this function are omitted. This is a critical hyperparameter setting that could significantly impact performance, and its heuristic nature is not fully explored or justified with an ablation study."}, "questions": {"value": "1. Regarding the adaptation of thresholds c2 and c3 mentioned in lines 248 to 251, could the authors provide the exact form of the linear mapping function used? How sensitive is the model's final performance to the specific design of this function?\n\n2. Could the authors elaborate more on the novelty of BAPO in comparison to established methods? Specifically, what are the key distinctions in how BAPO re-evaluates and samples difficult historical data that make it particularly suitable for LLM reasoning tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZJPayLOGrr", "forum": "RduOiisl1S", "replyto": "RduOiisl1S", "signatures": ["ICLR.cc/2026/Conference/Submission2892/Reviewer_x1w9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2892/Reviewer_x1w9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761651186852, "cdate": 1761651186852, "tmdate": 1762916432289, "mdate": 1762916432289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel off-policy reinforcement learning framework called BAPO, which effectively addresses the issues of experience waste and reward homogeneity in traditional on-policy methods through an adaptive batch construction strategy that dynamically re-evaluates historically difficult samples and reuses high-quality ones. The paper provides a theoretical lower-bound guarantee and demonstrates outstanding performance and data efficiency across a series of mathematical, planning, and visual geometry reasoning tasks. Overall, the paper features a well-structured organization, well-justified motivations, and comprehensive experiments. However, several key design aspects in the methodology section are insufficiently explained, which affects the reproducibility and persuasiveness of the work."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis paper proposes a method that systematically integrates the concept of off-policy reinforcement learning into the reinforcement learning with verifiable rewards for large language models. The method establishes a learning framework different from the traditional on-policy training paradigm by introducing experience replay and importance sampling mechanisms.\n\n2.\tThe core innovation of the method lies in the design of a dynamic re-evaluation mechanism, which can reassess historical data according to the model's current capability level. This mechanism transforms previously underutilized training samples into valuable training data, thereby improving data efficiency.\n\n3.\tThe adaptive batch construction strategy proposed in this paper manages the difficulty distribution of training data in a principled manner. By dynamically adjusting the proportion of samples of different difficulties, the method effectively avoids the problem of reward homogeneity that may occur during training.\n\n4. Experiments conducted in the three major reasoning domains of mathematics, planning, and visual geometry, based on models of different scales, show that the method consistently outperforms a series of representative on-policy and off-policy baselines in comprehensive benchmark tests, empirically demonstrating its effectiveness."}, "weaknesses": {"value": "1. **On the necessity of the Gaussian sampling in the $\\mathcal{X}_1$ module.** The paper does not validate the performance impact of removing Gaussian sampling through ablation experiments, nor does it compare other sampling strategies (such as uniform sampling or difficulty-based sampling). Readers cannot determine whether this module is indispensable or merely a redundant design that increases methodological complexity. Therefore, it is recommended to supplement ablation experiments on Gaussian sampling and clarify the advantages of the Gaussian sampling strategy.\n\n2. **Insufficient explanation of the $c_1/c_2/c_3$ threshold settings.** The values of $c_1/c_2/c_3$ in the paper lack adequate theoretical or experimental justification. Thresholds such as $c_1=0.125$, $c_2 \\in [0,0.375]$, and $c_3 \\in [0.25,0.5]$ appear arbitrary, with no explanation of their design inspiration or statistical basis. Moreover, although it is mentioned that $c_2$ and $c_3$ are based on \"linear mapping,\" the specific mapping formula is not provided, making it impossible for readers to reproduce the adaptive process.\n\n3. **Insufficient summarization of the principles behind the $c_1/c_2/c_3$ threshold values.** While the existing ablation experiments effectively demonstrate that \"dynamic adjustment is useful,\" which is highly important, readers cannot derive a clear and transferable parameter design paradigm from them. The current ablation experiments resemble \"effect validation\" rather than \"exploration of principles.\"\n\n4. **Suggestion on terminology standardization and conceptual clarity.** The term \"RFT\" in the abstract is inconsistent with \"RLVR\" in the main text. Given that the method heavily relies on verifiable rewards, unifying the terminology to \"RLVR\" would enhance conceptual clarity and professionalism."}, "questions": {"value": "Please refer to the “Weakness” section for related questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oplgJAW739", "forum": "RduOiisl1S", "replyto": "RduOiisl1S", "signatures": ["ICLR.cc/2026/Conference/Submission2892/Reviewer_Upr9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2892/Reviewer_Upr9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912601713, "cdate": 1761912601713, "tmdate": 1762916432122, "mdate": 1762916432122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a general sample optimization algorithm that ingeniously integrates both on-policy and off-policy data to train the policy model in a targeted manner, adaptively selecting samples based on their learning difficulty assessed from rollouts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper clearly identifies the key limitations of existing on-policy RLVR methods (experience waste and reward homogeneity) and proposes a well-motivated off-policy framework with intuitive design principles that align with RL fundamentals.\n- The paper provides rigorous theoretical analysis (Theorem 3.2) proving that the adaptive batch construction mechanism maintains a lower bound guarantee for policy improvement, ensuring training stability while leveraging off-policy data.\n- The evaluation across diverse reasoning tasks and model backbones demonstrates consistent improvements over baselines, with thorough ablation studies confirming the effectiveness of the proposed components."}, "weaknesses": {"value": "- While BAPO effectively improves learning efficiency through adaptive sample selection, it primarily reorganizes existing experiences rather than fundamentally expanding the model's exploration space. This may limit its ability to solve problems that consistently fail under the current policy distribution.\n- The paper primarily compares rollout counts, but lacks detailed analysis of the actual computational overhead, particularly the costs of forward passes (e.g., computing log probabilities for importance sampling ratios) and backward passes during training. Since BAPO reuses historical samples that do not require new rollouts but still incur training costs, a comprehensive breakdown of these components would better clarify the true efficiency gains."}, "questions": {"value": "- BAPO is specifically designed for GRPO-like group-based advantage estimation, where homogeneous rewards within a group can lead to zero gradients. It remains unclear whether this framework generalizes to other on-policy RL algorithms such as PPO or Reinforce, which does not suffer from the same gradient vanishing issues.\n- Does BAPO incorporate any mechanisms to encourage exploration beyond the existing experience pool, or is performance ultimately bounded by the policy's rollout coverage?\n- BAPO w/o X3 shows a substantial performance gap compared to full BAPO. Since X3 only provides historical high-quality samples that have already been used, could you explain the source of this large gap? Is this primarily due to the mismatch in effective batch sizes (as you configured BAPO w/o X3 to retain all fresh samples X1), or does the historical data contribute beyond simply maintaining batch size consistency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k8sf5tRNgI", "forum": "RduOiisl1S", "replyto": "RduOiisl1S", "signatures": ["ICLR.cc/2026/Conference/Submission2892/Reviewer_SdcC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2892/Reviewer_SdcC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929595966, "cdate": 1761929595966, "tmdate": 1762916431479, "mdate": 1762916431479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Batch Adaptation Policy Optimization (BAPO), an off-policy reinforcement learning fine-tuning framework for LLM reasoning tasks. It incorporates two key off-policy designs into standard RL post-training framework: (1) during rollout process, it delays the update of the inference server to mitigate rollout policy instability that may arise from rapid distribution shifts (a technique from prior work), and (2) during training process, BAPO dynamicaly constructs training batches that combine online generated samples, re-evaluated difficult samples, and historical high-quality samples. Experiments on mathematical reasoning benchmarks demonstrate that BAPO achieves higher accuracy compared to baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The ablation study is comprehensive, including ablations on $\\mathcal{X}_2, \\mathcal{X}_3$, delay steps (v), re-rollout frequency (m), and difficulty thresholds $c_1,c_2,c_3$.\n- Experiments span multiple reasoning domains, demonstrating versatility.\n- The paper structure is clear."}, "weaknesses": {"value": "- Overall, the method appears to be a collection of empirical tricks rather than a broadly generalizable or methodologically novel approach. Specifically, The method introduces a large number of hyperparameters and custom design choices—such as $c_1, c_2, c_3$,  online sample mean ($\\mu$) and standard deviation ($\\sigma$), re-rollout frequency (m), linear mapping function, rollout delay steps (v), and the proportion of different sample types, which greatly limit its plug-and-play usability.\n- The overall method design is highly heuristic, with limited justification on its rational. For example, in Eq. (5), the use of “historical high-quality samples” assumes that samples that were once high-quality remain so under the current policy. Although the method designs a linear mapping function to determine $(c_2, c_3)$ based on the buffer’s average performance, this approach is coarse and lacks solid reasoning to ensure that the selected samples are indeed high-quality under the updated policy.\n- In the $\\mathcal{X}_2$ filtering phase, the method requires periodically re-generating responses for all historically difficult samples using the current policy, leading to substantial computational overhead—especially when the dataset contains numerous difficult samples (a common scenerio). Similarly, in the $\\mathcal{X}_1$ filtering phase, not all online rollouts are utilized, resulting in additional inefficiency.\n- The paper lacks details on the quantities of $|\\mathcal{X}_1|, |\\mathcal{X}_2|, |\\mathcal{X}_3|$, which are crucial. Specifically, for $\\mathcal{X}_1$, how many online rollouts are retained out of the total generated? For $\\mathcal{X}_2$, since the number of such samples is varies considerably, how are the sample sizes distributed and balanced across the three sample types?\n- The theoretical proofs rely on assumptions of small total variation distance, but these assumptions are not sufficiently validated. In particular, assuming $\\mathrm{TV}(\\pi_{\\theta_t}(\\cdot | x), \\alpha_B(\\cdot | x)) \\le \\delta_3$ with sufficiently small $\\delta_3$ could severely limit the improvement of $\\pi_{\\theta_t}$."}, "questions": {"value": "- In the computation analysis results (Line 457), it is counter-intuitive that BAPO is not slower than GRPO, which requires clarification. Given identical training batch sizes, the generation cost before online rollout filtering should be similar to that of GRPO (assuming the pre-filtering sample size is the same, which is not described in the paper). Moreover, constructing $|\\mathcal{X}_2|$ requires periodic large-scale computation of the current policy probabilities over all historically difficult samples. And during training, off-policy samples also require additional probability computation under the current policy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sOwdSaqN76", "forum": "RduOiisl1S", "replyto": "RduOiisl1S", "signatures": ["ICLR.cc/2026/Conference/Submission2892/Reviewer_g1rK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2892/Reviewer_g1rK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977836374, "cdate": 1761977836374, "tmdate": 1762916431223, "mdate": 1762916431223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}