{"id": "ITeWz351rW", "number": 25181, "cdate": 1758365043928, "mdate": 1763723774018, "content": {"title": "Concrete-to-Abstract Goal Embeddings for Self-Supervised Reinforcement Learning", "abstract": "Self-supervised reinforcement learning (RL) aims to train agents without pre-specified external reward functions, enabling them to autonomously acquire the ability to generalize across tasks. A common substitute for external rewards is the use of observational goals sampled from experience, especially in goal-conditioned RL. However, such goals often constrain the goal space: they may be too concrete (requiring exact pixel-level matches) or too abstract (involving ambiguous observations), depending on the observation structure. Here we propose a unified hierarchical goal space that integrates both concrete and abstract goals. Observation sequences are encoded into this partially ordered space, in which a subset relation naturally induces a hierarchy from concrete to abstract goals. This encoding enables agents to disambiguate specific states while also generalizing to shared concepts. We implement this approach in the context of spatial abstraction. We use a recurrent neural network to encode sequences and an energy function to learn the partial order, trained end-to-end with contrastive learning. The energy function then allows to traverse the induced hierarchy to vary the degree of spatial abstraction. In experiments on navigation and robotic manipulation, agents trained with our hierarchical goal space achieve higher task success and greater generalization to novel tasks compared to agents limited to purely observational goals.", "tldr": "", "keywords": ["self-supervised reinforcement learning", "goal representation learning", "goal abstraction"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/61ba4107e2c5321b0d9673f29c193d2613ace299.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a method for learning a hierarchical goal space for self-supervised RL. The core idea is to encode entire sequences of observations into a latent space. A partial order is induced on this space by training an energy function to model the subsequence relation between trajectories. The authors posit that this partial order corresponds to a hierarchy of abstraction, where a goal corresponding to a subsequence is more concrete than the goal for the full sequence. This structure is learned end-to-end using contrastive learning. The learned energy function can then be used to traverse the hierarchy via optimization to find more abstract or concrete goals. Experiments in navigation and manipulation tasks show that this representation can capture abstract reward functions and leads to better generalization on novel tasks compared to using single observations as goals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Originality: The primary strength is the novel idea of using the subsequence relation to induce a partial order and thus a concrete-to-abstract hierarchy in a latent goal space. This is a creative and principled approach to self-supervised structure learning.\n\n- Methodology: The end-to-end learning framework, which combines a sequence encoder with a contrastively trained energy function, is elegant. The ability to traverse the learned hierarchy by optimizing the energy function is a powerful and interesting feature.\n\n- Significance: The work addresses the important problem of goal abstraction in RL. By enabling agents to represent and potentially reason about goals at varying levels of abstraction, this work is a valuable step towards more general and capable autonomous agents. The empirical results clearly show the limitations of standard observational goals for abstract tasks and demonstrate the effectiveness of the proposed representation."}, "weaknesses": {"value": "- Questionable Core Assumption: The paper's foundation rests on the assumption that the subsequence relation is a good proxy for abstraction. This is not always true. A short, specific trajectory (e.g., entering a room and turning left) is concrete, but a very long trajectory covering the entire map might also be viewed as a single, concrete instance of behavior, not an \"abstract\" goal. The paper does not discuss the limitations of this assumption or scenarios where it might fail.\n\n- Limited Baselines: The experiments almost exclusively compare against using single observations as goals. While this is a common baseline, it is insufficient for evaluating a method designed for abstract goals. Stronger baselines would include other latent goal methods, such as those using VAEs to encode observations [1] or methods that encode full trajectories [2], even if for different purposes. Without these comparisons, the performance gains are hard to contextualize.\n\n- Disconnect between Representation and Policy: As acknowledged in the conclusion, the policy is only trained on concrete, single-observation goals. The rich hierarchical structure of the goal space is learned but not directly used during policy pre-training. This is a significant limitation. The agent is never explicitly taught how to achieve an abstract goal, which likely hampers its ability to fully leverage the powerful goal representation.\n\n- Source code: Anonymous link to the source code is missing despite mentioning in the Reproducibility Statement.\n\n\n[1] Nair, Ashvin V., et al. \"Visual reinforcement learning with imagined goals.\" Advances in neural information processing systems 31 (2018).\n\n[2] Co-Reyes, John, et al. \"Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings.\" International conference on machine learning. PMLR, 2018."}, "questions": {"value": "- Could you elaborate on the limitations of using the subsequence relation as a proxy for abstraction? For example, is a trajectory that visits states (A, B, C) always more concrete than one that visits (A, B, C, D, E)? What if the first trajectory represents a complete, short skill, while the second is a longer, meandering one?\n\n- Why was the comparison limited to single-observation goals? A comparison against other latent goal representation learning schemes (e.g., VAEs on states, trajectory auto-encoders) would significantly strengthen the paper's claims.\n\n- The conclusion mentions that the policy is not trained on abstract goals. Have you run any experiments where the policy is trained or fine-tuned on goals generated by traversing the learned hierarchy (i.e., the outputs of the join operation)? It seems this would be a critical step to fully realize the benefits of your proposed goal space.\n\n- The process for finding abstract goals involves optimizing the energy function starting from a random initialization g^0. How sensitive is the final abstract goal to this initialization? Does the process consistently yield semantically meaningful and diverse abstractions, or does it often get stuck in trivial local optima?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WhMIcsRzrU", "forum": "ITeWz351rW", "replyto": "ITeWz351rW", "signatures": ["ICLR.cc/2026/Conference/Submission25181/Reviewer_Xcv6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25181/Reviewer_Xcv6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761607932020, "cdate": 1761607932020, "tmdate": 1762943354165, "mdate": 1762943354165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new way to learn goals representations in a self-supervised way. Instead of treating goals as fixed observations or states, it builds a hierarchical goal space that moves from concrete to abstract goals. The idea is that concrete goals, such as reaching specific locations or achieving low-level actions, can be summarized or combined into more abstract goals that can represent a more general task. For example, in the GridWorld environment, reaching a room’s corner is a concrete goal, while exploring the entire room forms a more abstract goal that encompasses several of those concrete goals. Sequences of observations are encoded with an RNN and a partial order between goals is learned using an energy-based contrastive loss that determines whether one goal is a subset of another. This structure lets the agent move up or down the hierarchy, discovering broader or more specific goals as needed. This representation is then plugged into a contrastive goal-conditioned RL framework, showing that it helps agents generalize to unseen reward functions and tasks in navigation and robotic manipulation settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The idea of viewing goal spaces as partially ordered hierarchies and modeling them as such is conceptually creative and could inspire future work on abstraction in RL.\n\n2) The aim to unify concrete and abstract goal representations is an important step toward generalization in RL.\n\n3) New evaluation metrics shows effort towards more broad validation (Figure 4 and Figure 7). Figure 4 lists the proposed metrics: Attainment, Total Reward, Final Reward, Final Streak, Max Streak, Average Streak, and Goal Speed. Figure 7 then uses these metrics to compare their proposed abstract goal representation method with observational and random goal baselines. These metrics capture performance evaluation dimensions beyond raw success rate, and offer a more nuanced assessment of generalization."}, "weaknesses": {"value": "1) The paper’s main idea of learning subset relations through an energy function lacks theoretical justification. Energy-based and contrastive learning frameworks are typically used to model equivalence or symmetric similarity relations rather than asymmetric hierarchical ones. The proposed formulation fails to demonstrate how the learned energy function satisfies the properties of a true partial order. Additionally, the paper does not clarify how the model handles pairs of trajectories that have no inclusion relation where neither is a subset of the other. Specifically, during the hierarchy traversal step, the optimization can arbitrarily drift toward unrelated goals, as no mechanism is described to ensure that the gradient of the energy function leads to more concrete yet related goals. This issue is particularly concerning when unrelated goals also yield an energy value of zero effectively breaking the intended hierarchical structure of the learned goal space.\n\n2) Missing definitions (e.g., $g^*$ and $p_\\vartheta$ in lines 199–210), inconsistent notation (use of E(⋅,⋅) for both discrete binary ordering and continuous similarity), unnumbered equations, and a confusing flow make the paper difficult to follow. Besides, adding diagrams or pseudo-code could greatly improve clarity by illustrating how the components interact and how the proposed hierarchy is implemented in practice.\n\n3) The experiments lack comparison with baselines (e.g., VAE-based latent goal RL, zero-shot RL methods), making it hard to assess performance improvements. Reported results show modest improvements and lack statistical analysis or ablation."}, "questions": {"value": "1) What guarantees (if any) exist that the energy function truly models a partial order rather than just a similarity metric?\n\n2) In the reward encoding loss, how is $g^*$ obtained and optimized? What is the meaning of $p_\\vartheta$?\n\n3) Could you provide a clearer visualization of concrete-to-abstract goals to show the method’s ability to learn all ranges of abstraction?\n\n4) Why are there no comparisons with VAE-based or other latent-goal models? Could you provide additional results comparison with baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uhU3DenrFN", "forum": "ITeWz351rW", "replyto": "ITeWz351rW", "signatures": ["ICLR.cc/2026/Conference/Submission25181/Reviewer_eU52"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25181/Reviewer_eU52"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814767752, "cdate": 1761814767752, "tmdate": 1762943353736, "mdate": 1762943353736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an algorithm for integrating concrete and abstract goals into a single shared latent goal space. This space is learned using asymmetric contrastive learning, which encourages the representations of related concrete and abstract goals to be similar, which induces a subset relation that enables traversing the latent abstraction to control the level of goal abstraction.\n\nConcrete goals are defined as single observations, whereas abstract goals are sequences of observations. These sequences are encoded using an RNN, and contrastive learning is applied to the resulting representations. Positive pairs consist of related sequences (specifically, when one sequence is a subset of another), while negative pairs consist of unrelated sequences or the inverse of the positive pairs.\nThe paper suggests an optimization-based method to traverse the abstraction level in the latent space, based on gradinet from the contrastive energy function\n\nThe paper further demonstrates that the learned latent goal space can be used to encode a reward function that depends on the observations. The encoded reward can then be provided to the policy in the form of an abstract goal representation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of encoding concrete and abstract goals is really interesting.\n- Encoding reward in an abstract goal space seems to be a promising approach for extending goal-reaching beyond reaching specific goals in the original state/observation space."}, "weaknesses": {"value": "- The paper is hard to follow, and the lack of a background section makes understanding it even more challenging.\n- Coverage assumption is limiting.\n- Lack of baselines, one possible baseline is to train some density model over the diverse data, then sample from that model in proportion to the reward."}, "questions": {"value": "- The complete problem setup is still not entirely clear to me. Could you clarify the setup further? Is the abstract goal model trained offline on diverse data and then applied to downstream goal-reaching tasks? I recommend adding a background section that introduces the problem statement and provides the necessary context to understand the rest of the paper.\n\n- For the positive pairs, what do you mean by “as well as composite trajectories formed from unrelated segments” (line 180)? Could you explain this using the notation introduced in the paper? Does this mean that you compose trajectories and then take subsets of them to construct a positive pair? How are these trajectories composed? Could you elaborate on this process?\n\n- How exactly is the relabeling with abstract goals performed? Do you relabel only a subset of the sampled single-observation goals, or are all of them relabeled? \n\nOverall, I like the idea of the paper, but the writing quality needs improvement to make the paper clearer and less confusing, and at lease one strong baseline should be added."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "F2rYT62RW9", "forum": "ITeWz351rW", "replyto": "ITeWz351rW", "signatures": ["ICLR.cc/2026/Conference/Submission25181/Reviewer_Dy4d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25181/Reviewer_Dy4d"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936673966, "cdate": 1761936673966, "tmdate": 1762943353348, "mdate": 1762943353348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors investigate goal learning in reinforcement learning, with a focus on distinguishing concrete goals (specific target states) from abstract goals (higher-level goal concepts that subsume many concrete target states). They propose a self-supervised method for learning abstract goals from trajectories aimed at concrete goals, enabling an agent to reason at two complementary levels: concrete (state-specific) and abstract (more conceptual). The key idea is that trajectories implicitly contain a successor structure, some states reliably precede and enable others, and by encoding these successor relations the agent can induce a partial order or some hierarchical structure over goals. A learned sequence-to-goal encoder maps states into a latent space where abstract goals emerge as representations that generalize across multiple concrete goal outcomes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors evaluate their approach across three environments, GridWorld, MemoryMaze, and FetchPush, and assess both (i) the ability to represent diverse reward functions via abstract goal embeddings, and (ii) generalization to novel tasks using abstract goal targets for control. Results show that the proposed method better captures spatial and compositional structure in reward functions compared to baselines that operate only on concrete/observation-level goals, and exhibits improved performance when transferring to new tasks."}, "weaknesses": {"value": "The paper offers an interesting perspective but there are some observations and questions:\n\n- The method assumes access to complete state trajectories under full observability during self-supervised training. It would be valuable to discuss how the approach could be adapted to partial observability (e.g., via belief states, recurrent encoders, or trajectory snippets). Moreover,  constructing and organizing abstractions from full trajectories may become prohibitively expensive as state/action dimensionality and horizon grow. The paper would benefit from an analysis (or bounds) of computational/memory complexity, plus experiments or ablations that probe scaling behavior. \n- The paper argues that abstract goals enable more conceptual reasoning than concrete goals; however, the quality and correctness of these abstractions are difficult to assess. Since the abstraction emerges implicitly from trajectories, there is no principled mechanism to verify whether a learned abstract goal is semantically meaningful, sufficiently general, or overly coarse/incorrect. This raises concerns about reliability and fidelity of the abstraction layer: an abstract goal might oversimplify, collapse distinct states, or fail to correspond to a useful higher-level concept, and the current evaluation does not formally quantify “degree of abstraction” or guarantee abstraction soundness.\n- The current evaluation is limited to single-agent, fully observable environments. As a forward-looking direction, the authors could outline how their abstraction mechanism would scale to multi-agent settings, for instance by learning abstractions over joint trajectories, factoring goals across agents (shared vs. individual abstract goals), and adapting successor-based relations to account for other agents’ policies. Even a short roadmap would clarify feasibility and potential challenges."}, "questions": {"value": "- Given the recent progress in LLM-driven abstraction and hierarchical reasoning, it would be interesting to discuss whether the learned abstract goal structure could be compared against abstractions produced by a language-conditioned model. In particular, once the agent induces a successor-based goal hierarchy, could a pretrained LLM (after some light fine-tuning or prompting) derive comparable abstract goal concepts from trajectory descriptions? Such a comparison, even qualitative, could help assess whether the latent hierarchy aligns with human-interpretable abstractions and might provide a complementary validation signal for abstraction quality. While this suggestion may not be perfectly aligned with your current scope, I share it to motivate a potentially fruitful direction: contrasting your latent goal hierarchy with LLM-derived abstractions could be a compelling exploratory angle for future work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JXozrwifdE", "forum": "ITeWz351rW", "replyto": "ITeWz351rW", "signatures": ["ICLR.cc/2026/Conference/Submission25181/Reviewer_xfwQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25181/Reviewer_xfwQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762266442457, "cdate": 1762266442457, "tmdate": 1762943353076, "mdate": 1762943353076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}