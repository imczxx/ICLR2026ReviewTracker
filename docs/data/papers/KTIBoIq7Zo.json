{"id": "KTIBoIq7Zo", "number": 5931, "cdate": 1757947309879, "mdate": 1763729765572, "content": {"title": "Why Do We Need Warm-up? A Theoretical Perspective", "abstract": "Learning rate warm-up - increasing the step size at the beginning of training - has become a ubiquitous heuristic in modern deep learning, yet its theoretical foundations remain poorly understood. In this work, we provide a principled explanation for why warm-up improves training. We rely on a generalization of the $(L_0, L_1)$-smoothness condition, which bounds local curvature as a linear function of the loss sub-optimality and exhibits desirable closure properties. We demonstrate both theoretically and empirically that this condition holds for common neural architectures trained with mean-squared error and cross-entropy losses. Under this assumption, we prove that Gradient Descent with a warm-up schedule achieves faster convergence than with a fixed step-size, establishing upper and lower complexity bounds. Finally, we validate our theoretical insights through experiments on language and vision models, confirming the practical benefits of warm-up schedules.", "tldr": "We propose a smoothness condition for explaining learning-rate warm-up. We show, theoretically and empirically, that it holds for standard architectures and approximates early-training smoothness. We also show convergence and empirical gains.", "keywords": ["Optimization", "(H_0", "H_1) smoothness", "warm-up", "explanation for warm-up"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b0a58efa53c5d39b3afc7a2058ffafa2e0986ad2.pdf", "supplementary_material": "/attachment/4dc3f09569a4b7f2d6782be8aa2f01057fcd5bd2.zip"}, "replies": [{"content": {"summary": {"value": "This work theoretically analyzes learning rate phenomena from a theoretical perspective. Specifically, they study a new constraint on the loss landscape geometry that assumes that the the top eigenvalue of the loss Hessian $\\lambda$ is bounded by the distance to the minima, i.e., $\\lambda \\leq H_0 + H_1 (f(w) - f^*)$. Through examples, the authors show that this condition theoretically holds in multiple cases. Next, the authors analyze the convergence of GD under the learning rate motivated by the $H_0-H_1$ condition. Finally, the authors test their learning rate warmup schedule in practice and show it works on par with linear warmup commonly used in practice."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is clearly written and the main results are easy to follow\n* Strong theoretical analysis of learning rate warmup under the $H_0-H_1$ condition\n* The authors propose a warmup schedule that performs on with linear warmup.\n* The proposed warmup schedule is practical, as it only depends on the current loss and one hparam $C$. This warmup strategy can be perhaps an alternative to the linear schedule used in practice."}, "weaknesses": {"value": "**$H_0-H_1$ condition**: Definition 3.1 states that the largest eigenvalue of the Hessian $\\lambda$ is bounded by $H_0 + H_1 (f - f^*)$. This implies that as the loss decreases the upper bound of $\\lambda$ reduces. The authors show that this condition holds in realistic situations (Figures 1, 2, I.1, I.2). However, there are empirical evidence against it. In full batch setting, its well (empirically) established that $\\lambda$ increases throughout training [1], which is at odds with the submitted work that claims the exact opposite. In mini-batch setting, its known that $\\lambda$ increases during training, albeit with a slower rate.\n\nThere can me multiple reasons why this happens, which I detail below:\n1. The authors use a proxy for the max eigenvalue (line 262), which may have a different behavior than sharpness\n2. Its known that very early in training, $\\lambda$ may decrease early in training (Appendix A of [1]), which is rather short (10 steps) compared to the warmup duration (1000 steps). The authors use a very small learning rate (1e-04 for SGD, 1e-07 for Adam), which restricts the training to this $\\lambda$ decrease phase. I would request the authors to rerun these experiments with a typical learning rate schedule and check if the decrease in curvature is observed during the entire warmup phase (both with their schedule and linear warmup).\n3. For unusually large initializations, $\\lambda$ may decrease throughout training [4], which aligns with the results in the paper. I would like the authors to clarify if the experiments in the submitted work are operating in this regime.\n\n**The gap between theory and practice**: The theoretical analysis of the submitted work assumes $\\lambda$ decreasing through the warmup phase, which corresponds to the 'natural sharpness reduction' phase described in [3]. However, as mentioned above, in realistic settings, $\\lambda$ increases during training. Regardless of whether $\\lambda$ increases or decreases, the effect of increasing learning rate is to reduce $\\lambda$ [2, 3]. \n\nThis creates a causal gap between the theory and practice: \n1. **Theory**: $\\lambda$ decreases during training, so increase $\\eta$\n2. **Practice**: $\\eta$ increases which causes reduction in $\\lambda$. \n\nFurthermore, the proposed schedule does not care about the causality anymore. It increases the learning rate depending on loss change and does not care about whether the curvature increases or decreases. This aligns with the standard linear learning rate schedule used in practice.\n\n[1] Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability, 2021\n\n[2] A Loss Curvature Perspective on Training Instability in Deep Learning, 2021\n\n[3] Why Warmup the Learning Rate? Underlying Mechanisms and Improvements, 2024\n\n[4] Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos, 2023"}, "questions": {"value": "* Line 67: \"we provide empirical guarantees\". I think there is a typo here. It should be empirical evidence rather than a guarantee.\n* Equation 1: where do the constants 10, 20 come from?\n* (Comment, not a question) Line 407: convergence is stochastic setting requires interpolation condition, whereas much of the realistic experiments (language modeling) are in underparameterized setting\n* Can you check Figure 1, 2, I.1, I.2 for standard learning rates? How long the decrease phase lasts?\n* Line 443, how did the authors come this practice schedule. It would be helpful to guide the reader."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2Fc0Gdb1Rj", "forum": "KTIBoIq7Zo", "replyto": "KTIBoIq7Zo", "signatures": ["ICLR.cc/2026/Conference/Submission5931/Reviewer_ryKS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5931/Reviewer_ryKS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761404787852, "cdate": 1761404787852, "tmdate": 1762918595249, "mdate": 1762918595249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why learning-rate warm-up is so effective in large-scale deep learning and proposes a new theoretical explanation based on a novel $(H_{0},H_{1})$-smoothness framework. Unlike the traditional $(L_{0},L_{1})$ condition, which ties curvature to gradient norm and contradicts empirical behavior in early training, the $(H_{0},H_{1})$ condition bounds curvature by the loss suboptimality, matching observed linear relations between curvature and loss. Building on this, the authors derive a theoretically motivated warm-up schedule and compare it against the standard linear warm-up across experiments on language models and vision tasks (ResNet50, ViT-Tiny). Their results show that both linear and $(H_{0},H_{1})$ warm-up improve convergence over no warm-up, with the proposed schedule performing competitively while offering theoretical justification."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel theoretical contribution**: The introduction of $(H_0,H_1)$-smoothness provides a fresh and insightful proxy for curvature, offering a more accurate explanation of warm-up dynamics than prior $(L_0,L_1)$-based analyses.\n\n2. **Balanced theoretical and empirical support**: The paper combines rigorous convergence proofs with clear empirical validation on both language and vision benchmarks, lending credibility to the theoretical claims.\n\n3. **Clear and well-structured presentation**: The exposition is logically coherent and easy to follow, making complex theoretical ideas accessible and enhancing the overall readability of the work."}, "weaknesses": {"value": "1. **Severe formatting issues**: The manuscript does not comply with the official ICLR template. In particular, it uses an incorrect font throughout and shows evidence of space compression (e.g., between Figures 3 and 4), which detracts from professionalism and readability.\n2. **Limited theoretical scope**: While the $(H_0,H_1)$-smoothness analysis shows that warm-up accelerates convergence, it does not establish that warm-up leads to a better final convergence outcome. In principle, similar effects of convergence could be obtained by simply extending training without warm-up. A stronger illustration for the necessity of warm-up still relies on arguments about training instability, which are not captured by this framework.\n3. **Missed validation opportunity**: The proposed theory enables regression of $f^\\ast$ from empirical training trajectories, yet the paper does not attempt such regression or verify the recovered $f^\\ast$ against the true optimum. Including this check would provide a valuable validation of the framework’s practical accuracy."}, "questions": {"value": "1. **Tightness of the bound**: Figures 1–2 display an almost perfect linear relationship between smoothness and loss suboptimality, whereas the $(H_0,H_1)$ framework only provides an upper bound. Is this near equality theoretically expected for certain model classes, or is it an artifact of the optimization trajectory and estimation method? Clarifying this would strengthen the interpretation of the empirical evidence.\n2. **Applicability to attention layers**: Since attention is the fundamental component of Transformer models, does $(H_0,H_1)$-smoothness formally hold for a single attention block? Can the authors provide a proof or at least a theoretical justification beyond empirical observation?\n3. **Combining proxies for curvature**: Curvature is inherently a second-order property. $(L_0,L_1)$-smoothness approximates it via a first-order proxy (gradients), while $(H_0,H_1)$-smoothness uses a zeroth-order proxy (loss values). Would a hybrid framework that leverages both first- and zeroth-order information yield a tighter or more general characterization of smoothness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K31GV7lbfQ", "forum": "KTIBoIq7Zo", "replyto": "KTIBoIq7Zo", "signatures": ["ICLR.cc/2026/Conference/Submission5931/Reviewer_MWmf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5931/Reviewer_MWmf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847384014, "cdate": 1761847384014, "tmdate": 1762918358443, "mdate": 1762918358443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a generalization of the $(L_0, L_1)$ smoothness, they call $(H_0, H_1)$-smoothness. They show that under that assumption warm-up is preferrable to fixed step size. They show that under some assumptions a few neural networks satisfy this property."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and visually pleasing. Addresses a long standing problem in a clean way. Proofs and results are clear.\nI really appreciated reading this.\n\nThe math part is well done, results are proved nicely."}, "weaknesses": {"value": "#### On the Tightness of your Condition\n\nYou prove that under your condition LR-warm up is optimal. It is when your condition is tight! Not in general. If on a Linear Shallow network I start from zero, it does satisfy the condition but the Hessian grows towards the solution, it does not go down, thus fixed step size = the maximal step size you would pick it better. \n\nIn general neural networks seem to show the phenomenon of progressive sharpening, which is conceptually the opposite of your condition. Can you please comment on this? Do you see progressive sharpening in the models you train?\n\n\n#### Explaining Warm-Up?\nHowever, I'm a little dubious that in general warm-up is for convergence purposes. I believe in non-convex cases and large scale ML systems it is for stability. I think it is necessary to comment on that. \n\nThis is not a strong ground for rejection, but I believe one needs to account for/deal with these other explanations. I think the paper is clean and beautiful, just maybe you can conjecture *what else* warm up is needed for. \n\nFor instance, when someone assumes progressive sharpening is happening, warm up is useful to constraint the model to less sharp areas of your landscape. Can you comment on this? Do you see this when applying your theory to neural networks? Do you see this in experiments?\n\nI think my final grade will depend on how you address this. \nPrecisely, making some experiments about and reworking the limitations of your work in analyzing the full picture behing warm up are explicit.\n\n\nAlso, I don't think it is sensible to speak about balanced neural networks for practice, because those are the flatter ones within the parameter space. That said I believe your assumption is also satisfied at standard initialization of linear networks. Maybe you can comment on how $H_0$ needs to grow to be satisfied at standard initialization."}, "questions": {"value": "See weaknesses.\n\nTo what extent is progressive sharpening allowed under your property? Until $H_0$ I guess?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KcOw2rDIi2", "forum": "KTIBoIq7Zo", "replyto": "KTIBoIq7Zo", "signatures": ["ICLR.cc/2026/Conference/Submission5931/Reviewer_k5bA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5931/Reviewer_k5bA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969082730, "cdate": 1761969082730, "tmdate": 1762918357989, "mdate": 1762918357989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response"}, "comment": {"value": "We would like to thank all reviewers for their precious effort and time assessing our manuscript. Below, we provide some general feedback, while we also address reviewers' main points individually. We submit a revised version of our paper, where the changes we made appear in blue.\n\nThe main point of criticism seems to be the incompatibility of our theory with a bulk of research on progressive sharpening and edge of stability (EOS). This is a fair point, and diving deeper into this line of research really helped us to refine our approach. Our new experiments (and a theoretical calculation), detailed in Appendix K, suggest that progressive sharpening does happen in all the models we look at, just at a later stage, while the initial stage is always a stage of progressive flattening. Interestingly, the turning point between flattening and sharpening does not seem to depend on the learning rate. The duration of the initial flattening can depend on the initialization, but it takes about 20% of the training from the loss drop perspective. Our theory applies exactly at this initial stage. See also the discussion we added at the end of Section 3 of the main paper.\n\nAnother major addition we made is analysis for one-layer transformers for in-context learning (Section 3.1.2 and Appendix D for proofs). This is a result we obtained recently, and since Reviewer MWmf asked about it, we thought that it would be a good addition to our revised version.\n\nThanks again for your hard work!"}}, "id": "x466HF8z1E", "forum": "KTIBoIq7Zo", "replyto": "KTIBoIq7Zo", "signatures": ["ICLR.cc/2026/Conference/Submission5931/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5931/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5931/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763726308930, "cdate": 1763726308930, "tmdate": 1763726308930, "mdate": 1763726308930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}