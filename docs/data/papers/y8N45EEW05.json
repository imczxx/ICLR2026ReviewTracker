{"id": "y8N45EEW05", "number": 19302, "cdate": 1758295213340, "mdate": 1759897046824, "content": {"title": "Parameterized Hardness of Zonotope Containment and Neural Network Verification", "abstract": "Neural networks with ReLU activations are a widely used model in machine learning. It is thus important to have a profound understanding of the properties of the functions computed by such networks. Recently, there has been increasing interest in the (parameterized) computational complexity of determining these properties. In this work, we close several gaps and resolve an open problem posted by Froese et al. [COLT '25] regarding the parameterized complexity of various problems related to network verification.\nIn particular, we prove that deciding positivity (and thus surjectivity) of a function $f\\colon\\mathbb{R}^d\\to\\mathbb{R}$ computed by a 2-layer ReLU network is W[1]-hard when parameterized by $d$.\nThis result also implies that zonotope (non-)containment is W[1]-hard with respect to $d$, a problem that is of independent interest in computational geometry, control theory, and robotics.\nMoreover, we show that approximating the maximum within any multiplicative factor in 2-layer ReLU networks, computing the $L_p$-Lipschitz constant for $p\\in(0,\\infty]$ in 2-layer networks, and approximating the $L_p$-Lipschitz constant in 3-layer networks are NP-hard and W[1]-hard with respect to $d$.\nNotably, our hardness results are the strongest known so far and imply that the naive enumeration-based methods for solving these fundamental problems are all essentially optimal under the Exponential Time Hypothesis.", "tldr": "", "keywords": ["Parameterized Complexity", "Exponential Time Hypothesis", "Norm Maximization", "Polyhedral Geometry"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dc5e692ed79efa99121a147ca4a33535efd4b8c2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the parameterized complexity of zonotope containment and some problems related to neural network verification. It extends the prior work and proves the strongest complexity bounds so far. The results shows that under Exponential Time Hypothesis (ETH), brute-force enumeration is asymptotically optimal in complexity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The studies problem is relevant, i.e., providing better complexity characterization for certain neural network certification related problems.\n\nThe paper is clearly and rogirously written.\n\nThe results are novel and extend the scientific boundary."}, "weaknesses": {"value": "It is not clear how these results might influence practice. While knowing the complexities might be interesting, it seems to have minimal impact and guidance on neural network certification practice.\n\nSome related work in the difficulty of neural network is missing: [1] shows that for a two-dimensional simple convex monotone CPWL function, there exists no single-neuron relaxations that can precisely certify any network encoding the function; [2] extends the result and shows that every convex relaxation is incomplete in neural network certification. These works study the theoretical boundary of neural network certification in the practical algorithms, while this work characterizes the difficulty of the general problem.\n\n[1] https://arxiv.org/abs/2311.04015\n\n[2] https://arxiv.org/abs/2410.06816"}, "questions": {"value": "Please address the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZihfErglRo", "forum": "y8N45EEW05", "replyto": "y8N45EEW05", "signatures": ["ICLR.cc/2026/Conference/Submission19302/Reviewer_Lmnz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19302/Reviewer_Lmnz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760964872953, "cdate": 1760964872953, "tmdate": 1762931253163, "mdate": 1762931253163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents new theoretical results on the complexity of the problem of proving properties of ReLU networks. Specifically, the authors study the parameterized complexity of checking for positivity, checking for surjectivity, and computing the Lipschitz constant of ReLU networks and show that these problems are W[1]-hard. The analysis of these properties centers around mapping 2-layer ReLU networks to the multicolored clique problem. The authors also show how these results can be extrapolated to the zonotope containment problem, which has implications in other domains as well."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The theoretical complexity results developed in the paper are the strongest known so far and resolve an open problem.\n- The authors develop results for multiple different neural network verification tasks.\n- The results rely on and have implications for well-studied problems in other domains (multicolored clique and zonotope containment)."}, "weaknesses": {"value": "It is not clear from the paper the practical implications of the derived theory and complexity results. Therefore, it may not be immediately useful for practitioners working on neural network verification tools. Many of the high-performance neural network verification tools used these days rely on overapproximations, and it is unclear how these results might inform decisions for further development in this domain."}, "questions": {"value": "- What are the practical implications of the complexity results?\n- What should we as a community take from these results as we continue to progress the field of neural network verification?\n- Do these results have any implications for approximate neural network verification techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qoiqagi6FH", "forum": "y8N45EEW05", "replyto": "y8N45EEW05", "signatures": ["ICLR.cc/2026/Conference/Submission19302/Reviewer_Druf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19302/Reviewer_Druf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761252986276, "cdate": 1761252986276, "tmdate": 1762931252532, "mdate": 1762931252532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a collection of (parameterised) complexity results related\nto feedforward neural network (FNN) verification tasks, as well as zonotope\ncontainment.\n\nThe paper presents the following core results: \n- 2-Layer ReLU Positivity: The problem of deciding whether a given 2-layer FNN\n  with ReLU activations can produce an output greater than 0 is W[1]-hard.\n- 2-Layer ReLU Surjectivity: The problem of deciding whether a given 2-layer FNN\n  with ReLU activations computes a surjective function is W[1]-hard.\n- Zonotope Non-Containment: The problem of deciding whether, for two given\n  zonotopes, one is not included in the other is W[1]-hard.\n- 2-Layer ReLU $L_p$-Lipschitz Constant: The problem of deciding whether the\n  $L_p$ Lipschitz constant of a given FNN with ReLU activations is greater than\n  some given threshold is NP- and W[1]-hard.\n\nHere, the class W[1] is the first level of the W-hierarchy widely considered in\nthe theory of parameterised complexity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents its results in a rigorous and convincing manner,\nproviding compelling insights into the complexity of FNN verification-related\nproblems.\n\nFurthermore, the structure and craftsmanship of the paper are commendable and feel very polished."}, "weaknesses": {"value": "The paper could improve in several areas:\n- While reading, it sometimes feels like a collection of varied results, somewhat loosely\n  related to FNN verification. Providing more explanation on why these problems are of\n  interest would be beneficial.\n- In the same light, the preliminaries seem to lack a proper definition of the class W[1].\n  It is assumed that many readers may not be fully acquainted with parameterised complexity,\n  which might lead to some confusion."}, "questions": {"value": "- Can you provide more details regarding the implications for actual\n          verification tasks, such as positivity, given your W[1]-hardness result?\n- Could you elaborate on why problems such as positivity, surjectivity,\n          or the Lipschitz constant are of significance to the verification\n          community? They seem somewhat abstract or overly simplistic."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v0IfwuKho6", "forum": "y8N45EEW05", "replyto": "y8N45EEW05", "signatures": ["ICLR.cc/2026/Conference/Submission19302/Reviewer_4wHP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19302/Reviewer_4wHP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729341972, "cdate": 1761729341972, "tmdate": 1762931252072, "mdate": 1762931252072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper uses the tools of parameterized complexity to prove stronger hardness results for several problems arising within the study of ReLU neural networks (for instance, computing the $L_p$-Lipschitz constant of the network), or that can be linked to it (such as Zonotope Containment). \n\nThe paper relies on parameterized reductions, and more precisely on one main reduction using the W[1]-hard problem Multicolored Clique as a starting point and constructing a 2-layer ReLU network such that the maximum value of the function computed by the network determines whether the Multicolored Clique instance had a solution or not.\n\nAfter obtaining the W[1]-hardness and sometimes ETH-conditional lower bounds for some problems, the authors also use known equivalences or reductions between problems to spread these results and get a more complete picture. As they remark in the conclusion, the common denominator to these problems is that they correspond in a way to maximizing a norm over a zonotope."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The theoretical analysis is sound, and yields new results for a variety of problems.\n\nGives a theoretical insight into why the naive enumeration algorithms are somehow optimal for these problems, and fosters more application of parameterized complexity to the field.\n\nFor the $L_p$-Lipschitz constant, the hardness result for the exact computation of 2-layer networks is complemented by a hardness result for the approximation of the  $L_p$-Lipschitz constant of 3-layer networks.\n\nThe paper is well written."}, "weaknesses": {"value": "It is not clear whether some properties of real-world networks make it possible to circumvent these worst-case theorems. However, I agree this is unavoidable for a theoretic analysis, and the identification of such empirical properties is not the topic of the paper."}, "questions": {"value": "l141: this means that for $\\\\ell=1$, no ReLU neuron is involved?\n\nl269: why do you need two extra hidden neurons for the bias of the output?\n\nl426: what is the size of the LPs?\n\n\n\nSuggestions and Typos:\n\nl21, abstract: For readability, consider \"Moreover, we show that (a): ....., (b): ....., and (c): ...... are all NP-hard and W[1]-hard ....\".\n\nl174: $L\\\\in \\\\mathbb{R}$, right?\n\nFigure 1: the use of \"r, l\" as colors is technically fine, but it is on a psychological level confusing since it looks like \"right, left\", with the \"right\" being always on the left, in the figure and in expressions. Not necessary strictly speaking, but it might be nice swapping them.\n\nl404: please reformulate, highlighting that the maximum is over linear regions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ul4xRuQyt2", "forum": "y8N45EEW05", "replyto": "y8N45EEW05", "signatures": ["ICLR.cc/2026/Conference/Submission19302/Reviewer_wktp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19302/Reviewer_wktp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986246347, "cdate": 1761986246347, "tmdate": 1762931251610, "mdate": 1762931251610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}